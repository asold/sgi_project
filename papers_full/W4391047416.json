{
  "title": "Comparing Traditional and LLM-based Search for Image Geolocation",
  "url": "https://openalex.org/W4391047416",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5026232764",
      "name": "Albatool Wazzan",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A5042822346",
      "name": "Stephen MacNeil",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A5057374067",
      "name": "Richard Souvenir",
      "affiliations": [
        "Temple University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3155638432",
    "https://openalex.org/W2974452150",
    "https://openalex.org/W4288359812",
    "https://openalex.org/W1500287723",
    "https://openalex.org/W2163987313",
    "https://openalex.org/W4387993371",
    "https://openalex.org/W4381104201",
    "https://openalex.org/W3160649953",
    "https://openalex.org/W2586726261",
    "https://openalex.org/W4225165463",
    "https://openalex.org/W2030960598",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4318147139",
    "https://openalex.org/W2766843377",
    "https://openalex.org/W2583855849",
    "https://openalex.org/W1946093182",
    "https://openalex.org/W2571310080",
    "https://openalex.org/W3134925693",
    "https://openalex.org/W2970054358",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W1995672491",
    "https://openalex.org/W2996188425",
    "https://openalex.org/W2983585534",
    "https://openalex.org/W2765128264",
    "https://openalex.org/W2284646714",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W2202662853",
    "https://openalex.org/W1502285767",
    "https://openalex.org/W2890250492",
    "https://openalex.org/W2804512604",
    "https://openalex.org/W2116624262",
    "https://openalex.org/W69461734",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W3193084543",
    "https://openalex.org/W2110037758",
    "https://openalex.org/W4383180610",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2811298321",
    "https://openalex.org/W2103163130",
    "https://openalex.org/W2150104072",
    "https://openalex.org/W2145734075",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2404264324",
    "https://openalex.org/W4387156527",
    "https://openalex.org/W4221143046"
  ],
  "abstract": "Web search engines have long served as indispensable tools for information\\nretrieval; user behavior and query formulation strategies have been well\\nstudied. The introduction of search engines powered by large language models\\n(LLMs) suggested more conversational search and new types of query strategies.\\nIn this paper, we compare traditional and LLM-based search for the task of\\nimage geolocation, i.e., determining the location where an image was captured.\\nOur work examines user interactions, with a particular focus on query\\nformulation strategies. In our study, 60 participants were assigned either\\ntraditional or LLM-based search engines as assistants for geolocation.\\nParticipants using traditional search more accurately predicted the location of\\nthe image compared to those using the LLM-based search. Distinct strategies\\nemerged between users depending on the type of assistant. Participants using\\nthe LLM-based search issued longer, more natural language queries, but had\\nshorter search sessions. When reformulating their search queries, traditional\\nsearch participants tended to add more terms to their initial queries, whereas\\nparticipants using the LLM-based search consistently rephrased their initial\\nqueries.\\n",
  "full_text": "Comparing Traditional and LLM-based Search for Image\nGeolocation\nAlbatool Wazzan\nDept of Computer & Info Sciences\nTemple University\nPhiladelphia, USA\nalbatool.wazzan@temple.edu\nStephen MacNeil\nDept of Computer & Info Sciences\nTemple University\nPhiladelphia, USA\nstephen.macneil@temple.edu\nRichard Souvenir\nDept of Computer & Info Sciences\nTemple University\nPhiladelphia, USA\nsouvenir@temple.edu\nABSTRACT\nWeb search engines have long served as indispensable tools for in-\nformation retrieval; user behavior and query formulation strategies\nhave been well studied. The introduction of search engines powered\nby large language models (LLMs) suggested more conversational\nsearch and new types of query strategies. In this paper, we compare\ntraditional and LLM-based search for the task of image geoloca-\ntion, i.e., determining the location where an image was captured.\nOur work examines user interactions, with a particular focus on\nquery formulation strategies. In our study, 60 participants were\nassigned either traditional or LLM-based search engines as assis-\ntants for geolocation. Participants using traditional search more\naccurately predicted the location of the image compared to those\nusing the LLM-based search. Distinct strategies emerged between\nusers depending on the type of assistant. Participants using the\nLLM-based search issued longer, more natural language queries,\nbut had shorter search sessions. When reformulating their search\nqueries, traditional search participants tended to add more terms\nto their initial queries, whereas participants using the LLM-based\nsearch consistently rephrased their initial queries.\nACM Reference Format:\nAlbatool Wazzan, Stephen MacNeil, and Richard Souvenir. 2024. Comparing\nTraditional and LLM-based Search for Image Geolocation. In Proceedings\nof the 2024 ACM SIGIR Conference on Human Information Interaction and\nRetrieval (CHIIR ‚Äô24), March 10‚Äì14, 2024, Sheffield, United Kingdom. ACM,\nNew York, NY, USA, 12 pages. https://doi.org/10.1145/3627508.3638305\n1 INTRODUCTION\nFor decades, web search engines have served as the de facto refer-\nence tool for a wide range of tasks. In fact, it has been demonstrated\nthat humans have been trained to optimize keyword-based search-\ning using query formulations not typically used in natural lan-\nguage [24]. Advancements in artificial intelligence (AI) have driven\nthe emergence of large language models (LLM), such as BERT [9],\nGPT-3 [5], and their successors. These models have served as the\nfoundation for numerous applications, ranging from text genera-\ntion, translation, to question answering, multi-step reasoning, and\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nACM SIGIR, March 10-14, 2024, Sheffield, UK\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0434-5/24/03. . . $15.00\nhttps://doi.org/10.1145/3627508.3638305\ncomplex problem solving [43]. Recently, these tools have been com-\nbined with web search to enable a new mode of LLM-powered con-\nversational search. Unlike keyword-based search, this integration\nallows users to engage in a natural, interactive conversation with\nthe LLM-powered search engine, as if they were interacting with\na knowledgeable assistant. This conversational mode has the po-\ntential to improve user experiences across various domains. While\nprevious work has explored how people engage in sense-making\nand constructing mental models of traditional search engines [40],\nthe adaptation of these models to LLM-based search remains open\nto inquiry.\nTo compare traditional and LLM-based search, we consider the\ntask of image geolocation ‚Äì identifying the location in which an im-\nage was captured, an important task with applications in forensics,\nlaw enforcement, and journalism. This task has historically been\nperformed by expert image analysts, using increasingly sophisti-\ncated reference tools as they became available. Fully automated\ncomputer vision approaches [12, 26, 45] have been developed; these\napproaches typically rely on the visual similarity between the query\nimage and a previously-processed training image and tend to work\nbest when landmarks or other unique features are visible. In the\ngeneral case, accurately localizing images can be challenging. Even\nwith the assistance of a search engine, users not only need to iden-\ntify visual clues, but understand them well enough to translate into\na search query. Because geolocation is a task that requires inves-\ntigation, in that analysts must collect sometimes disparate clues\nto uncover the origin of the image, it can be expected that users\nwill formulate multiple queries as they seek to retrieve information\nabout these clues. This task takes advantage of both the lookup\nabilities of a search engine and the contextual knowledge from\nhumans, making it a compelling task to evaluate how users adapt\ntheir query formulation strategies.\nWe conducted a between-subjects study with 60 participants\nrandomly assigned to use either traditional or LLM-based search to\naid in image geolocation in order to address the following research\nquestions:\nRQ1 How does the use of an LLM-based search tool versus a tradi-\ntional search tool impact participants‚Äô performance in geolo-\ncation tasks?\nRQ2 How do participants adapt their query formulation strategies\nwhen using LLM-based search compared to traditional search\nfor image geolocation?\nRQ3 What are the key challenges encountered by participants\nwhen using LLM-based search for image geolocation?\nOur results indicate that participants using traditional search out-\nperformed those using LLM-based search in terms of accurate image\narXiv:2401.10184v1  [cs.IR]  18 Jan 2024\nACM SIGIR, March 10-14, 2024, Sheffield, UK Albatool Wazzan, Stephen MacNeil, and Richard Souvenir\ngeolocation. This outcome can be explained by our qualitative find-\nings, where participants reported challenges formulating queries\nwhen interacting with the LLM-based search engine. LLM-based\nsearch users issued longer, more conversational queries within\nshorter search sessions. Participants using the traditional search\nengine tended to extend their initial queries with additional terms\nwhen reformulating, while those utilizing the LLM-based search\nconsistently rephrased their initial queries.\n2 RELATED WORK\nSearch engines have evolved into indispensable tools that influ-\nence how information is accessed and problems are solved [ 11].\nHowever, effectively communicating the user‚Äôs search intent has\nbeen a persistent challenge. Much work has been dedicated toward\nunderstanding web search query formulation patterns [3, 18, 30]\nand investigating how users adapt their queries and reformula-\ntion strategies in efforts to uncover search intent [6, 20, 42]. These\nstrategies can be domain-specific. For instance, for health-related\ninformation, Zuccon showed that search results were less help-\nful when users issued complex queries describing their symptoms\nrather than using medical terminology [ 48]. In the educational\nsetting, students heavily rely on search engines for academic pur-\nposes [17, 36]. However, it has been shown that a substantial portion\nof academic search sessions result in null queries, when individu-\nals use vague or complex terms resulting in empty search results\nand obstructing users from achieving their intended search objec-\ntives [25]. Recognizing and understanding these challenges related\nto user behavior and query formulation strategies can enhance\nthe overall search and retrieval experience. Our work builds upon\nexisting research in web search and query formulation and extends\nthe analysis to LLM-based search for the task of image geolocation.\n2.1 LLM-based Search Analysis\nLLMs are trained on large amounts of text corpra, and their ef-\nfectiveness in various applications hinges on the ability to query\nthem effectively [1]. In efforts to optimize LLMs for retrieval tasks,\nseveral works have investigated the process of querying LLMs for\nspecific information. Jiang [21] highlighted the consequences of\npoorly written prompts, yielding failed retrieval results and pro-\nposed a method that used multiple automated paraphrases of the\nquery and an aggregation scheme, mirroring how humans often\nrephrase their queries and provide additional context to make them\nmore informative. Similarly, the work of Petroni [ 32] examined\nenhancing the LLM retrieval by augmenting queries with relevant\ncontext and demonstrated improved performance on various LLMs\non factuality tests.\nWhen humans seek information, they often clarify their queries\nwith examples to obtain better results. To mimic this behavior,\nBrown employed few-shot learning, which involves conditioning\nthe LLM on the task description and just a few examples, and found\nthat this ‚Äúin-context learning‚Äù works best with larger language\nmodels [5]. A more recent effort argued that language models do\nnot learn tasks during runtime from few-shot examples, but lo-\ncate tasks within the model‚Äôs preexisting knowledge; this paper\nproposes 0-shot prompts, which uses an alternative query with\ndifferent phrasings to provide additional task descriptions [35]. Wei\nintroduces chain-of-thought, which aims to replicate the human\nthought process when addressing complex problems [44]. These\nefforts aimed at enhancing the LLM retrieval, but do not address\nthe challenges faced by non-expert users when querying LLMs.\nRecent work [47] involves non-experts issuing prompts for LLM-\nbased chatbots and found that struggles in formulating effective\nprompts resemble issues observed in end-user programming and\ninteractive machine learning systems. Their work emphasizes the\nneed for further research in LLMs and prompt-literacy, specifically\nfor non-expert users. Our work focuses on this challenge for the\nmultifaceted task of image geolocation.\n2.2 Image Geolocation\nImage geolocation is a widely-studied task. One effort used a care-\nfully constructed dataset to investigate the types of clues and strate-\ngies users employ for image geolocation [29]. Several efforts have\naddressed the labor-intensive nature of image geolocation by in-\ncorporating crowdsourcing to improve location identification. One\nstudy introduced a diagramming technique involving visual repre-\nsentations from a bird‚Äôs-eye or satellite perspective, which allowed\nnovice crowd workers to collaborate with experts [23]. In a follow-\nup study, the authors introduced GroundTruth, a system that en-\nhanced image geolocation accuracy through shared representations\nfor crowd-augmented expert work [41].\nOther studies have explored how to improve the accuracy of non-\nexpert workers in image geolocation tasks. One project explicitly\ninstructed novice users to follow a three-step workflow inferred\nfrom expert strategies: collecting image-related clues, deriving po-\ntential coordinates based on these clues, and identifying the image\nlocation on a map [22]. Another method [34] introduced a crowd-\nsourcing platform that leverages existing data mining methods to\nestimate photo and video locations from social media, then used\ncrowdsourcing for verification.\nIn our approach, we focus on how participants articulate visual\nclues into search queries, and whether those query strategies differ\ndepending on the type of search tool available.\n3 METHODS\nWe conducted a between-subjects user study involving 60 par-\nticipants. In this section, we describe the experimental platform,\nrecruitment of participants, task design, and measures.\n3.1 Experimental Platform\nImage geolocation has been well-studied due, in part, to the popular-\nity of gamified versions of the task. The most well-known version is\nGeoGuessr; others include GeoGuess, Geotastic, and City Guesser.\nThe objective of these games is to predict the correct location on\nmap given an image, video, or other information and points are\naccumulated based on speed and/or accuracy. These games can\nserve as useful platforms for evaluating a wide variety of cognitive\ntasks. In this study, we use GeoGuess [4], an open-source image\ngeolocation game. Users are presented a Google Map StreetView\nimage and have two minutes to guess the location by dropping a pin\non a world map. Users can navigate using the StreetView interface\nto virtually zoom and move through the scene. Up to 5,000 points\ncan be earned based on proximity of the prediction to the actual\nComparing Traditional and LLM-based Search for Image Geolocation ACM SIGIR, March 10-14, 2024, Sheffield, UK\nParis, France\n Sydney, Australia\n Chicago, USA\nTokyo, Japan\n Barcelona, Spain\n Ushuaia, Argentina\nFigure 1: Initial viewpoints (with the location indicated) of the six rounds in the experiment.\nlocation. In the instructions for the game, users are encouraged to\nseek out clues that may be useful for localization.\n3.2 Participants\nParticipants, 18 years of age or older who could read and under-\nstand English, were recruited on a university campus. Our sample\nconsisted of 60 participants whose mean age was 25 (SD = 4.95) with\na diverse range of academic backgrounds, majoring in accounting,\nbiology, computer science, systems engineering, physics, chemistry,\nglobal studies, and marketing. The IRB approved study was carried\nout by two members of the research team over the span of three\nweeks. All participants received a $5 gift card to a coffee shop and\nwere informed of the task description, duration, compensation, and\ntheir right to forfeit at anytime before participating.\n3.3 Task Design\nUsers were provided a dual-monitor setup, with the geolocation\ntask on one screen and the search engine on the other. Microsoft\nBing served as the traditional search engine and Microsoft Bing\nChat, which is powered by ChatGPT, served as the LLM-based\nsearch engine.\nWe followed a between-subjects study design, where each par-\nticipant was randomly assigned to either the (Traditional) Search\nor the LLM condition. The experiment consisted of six geolocation\ntasks (shown in Figure 1), which were intended to vary in difficulty.\nParticipants were provided instructions to only use the provided\nsearch engine and not perform image-based search. Participants\nwere asked to confirm their understanding of the instructions by\nclicking on a (I understand) button. After the instructions, par-\nticipants watched a short instructional video on how to use the\ngeolocation interface.\nFor each round, the participant had two minutes to provide a\nguess. They could consult the search tool as often as they needed,\nTable 1: Geolocation performance on the six round experi-\nment.\nEstimate Std. Error t-value p-value\n(Intercept) 2678.5 167.4 15.999 <2e-16 ***\nCondition(Search) 501.3 243.5 2.059 0.0414 *\ngiven the time constraint. Upon completion of the six rounds, par-\nticipants were invited to fill out a post-study survey with questions\nabout familiarity with image geolocation, traditional or LLM-based\nsearch, attitudes toward artificial intelligence, and a set of open-\nended questions for additional feedback. The entire experiment\nwas designed to be completed in approximately 15 minutes per\nparticipant.\n3.4 Measures\nThe primary dependent variable in this study is performance, mea-\nsured by the points earned by each participant per round. The score\nranged from 0 to 5000; the maximum score was obtained when\nthe prediction was within a few kilometers of the actual location.\nThe primary independent variable, type of search, was modeled as\na fixed effect in our linear mixed-effects model. Each participant\nplayed the six rounds in the same order. The round number, which\ncorrelated with difficulty, was modeled as a random effect. For\neach participant, we maintained an event log of timestamped ac-\ntions that included switching between web search and geolocation.\nAdditionally, we recorded the search queries.\n4 ANALYSIS & RESULTS\nWe excluded the five participants that did not engage with the\nsearch engine for any of the rounds, which left 29 for the Search\ncondition and 26 for the LLM condition.\nACM SIGIR, March 10-14, 2024, Sheffield, UK Albatool Wazzan, Stephen MacNeil, and Richard Souvenir\nFigure 2: Performance distribution (points) comparison between Search and LLM conditions on average (left) and per round.\nFigure 3: Percentage of multi-query rounds for Search and\nLLM conditions\nFor the Search condition, the mean performance score was 3189,\nwith a median of 4712 and an IQR of 4262.25. For the LLM condition,\nthe mean performance score was 2725, with a median of 3637.5 and\nan IQR of 4952. Figure 2 shows the distribution of scores across the\ntwo conditions by average and across the six rounds.\nOverall, participants in the Search condition outperformed those\nin the LLM condition. For rounds 1-4, the performance was similar\nfor both conditions, with both groups finding round 2 challenging.\nIn the final two rounds, participants across both conditions per-\nformed poorly, with those in the LLM group performing notably\nworse. We use a Linear mixed-effects Model (LMM) to evaluate\nthe difference in performance between the groups. The results are\nshown in Table 1. There was a significant difference in performance\nbetween the two conditions (ùëù = 0.0414).\n4.1 Query Formulation Patterns\nQuery formulation is a fundamental tool in the analysis of search\nbehavior. Here, we investigate four key query formulation metrics,\ncomparing their differences across the two conditions.\n4.1.1 Number of Queries. We examined the number of queries per\nround. Participants in the Search condition issued an average of\n1.98 queries, whereas those in the LLM condition issued an average\nof 1.04 queries. A Chi-Square test showed this association to be\nsignificant (ùúí2 (6)= 19.71, ùëù= 0.003); users in the Search condition\nissued more queries.\nWe computed the percentage of rounds in which participants\nissued more than one query. As shown in Figure 3, participants\nin both conditions issued more queries as the task increased in\nFigure 4: Comparison of part-of-speech tag counts between\nSearch and LLM conditions\ndifficulty. Participants in the Search condition favored issuing more\nqueries, starting at around 55% and reaching 70% by the end of the\ntask. In the LLM condition, participants issued more queries at a\nlower rate starting at 20%, but reaching 55% by the last round.\n4.1.2 Query Length. Query length, the average number of terms\nin each query, can provide valuable insights into query formula-\ntion patterns [38]. On average, participants in the Search condi-\ntion formulated queries comprising 4.19 terms. In contrast, partici-\npants in the LLM condition issued queries with an average of 6.06\nterms. A Chi-Square test revealed a highly significant association\nbetween the conditions and the differences in query length patterns\n(ùúí2 (28)= 61.78, ùëù< 0.001).\n4.1.3 Part-of-Speech Tagging. To explore potential differences in\nlinguistic characteristics, we performed part-of-speech tagging. Fig-\nure 4 shows the distribution of tags in the queries across conditions.\nAfter adjusting the alpha value using Bonferroni correction, in\nthe LLM condition, several tags, including adverbs (ADV), adposi-\ntions (ADP), determiners (DET), auxiliary verbs (AUX), and (VERB)\nexhibited significantly higher frequencies ( ùëù = 0.051, ùëù = 0.046,\nùëù < 0.001, ùëù < 0.001, ùëù = 0.019) than the Search condition. The\nincreased use of adverbs and auxiliary verbs in the LLM queries\nsuggested a more natural language style, potentially influenced by\nthe conversational nature of interactions with LLMs [31]. On the\nother hand, in the Search condition, usage of proper nouns (PROPN)\nwas significantly higher (ùëù = 0.034) than in the LLM condition. This\nComparing Traditional and LLM-based Search for Image Geolocation ACM SIGIR, March 10-14, 2024, Sheffield, UK\nFigure 5: Average number of query terms for successive\nqueries in a round\nindicates a greater tendency to perform a keyword-based search us-\ning specific entities or locations when interacting with a traditional\nsearch engine.\n4.1.4 Questions. We explore the categorization of question and\nnon-question queries. Following Pang and Kumar [30], we defined\nquestion queries based on the following criteria:\n‚Ä¢Interrogative start: Queries that start with how, what, which,\nwhy, where, when, who, whose .\n‚Ä¢Modal verb start: Queries that start with do, does, did, can,\ncould, has, have, is, was, are, were, should . However, an ex-\nception is made for queries where the second word is not.\n‚Ä¢Queries that end with a question mark (?).\nQueries not meeting the criteria were classified as non-question.\nFor the Search condition, only 17% were question queries. Con-\nversely, for the LLM condition, 73%, were question queries. A Chi-\nsquare analysis yielded statistically significant results between the\ntwo types ( ùúí2 (1)= 6.37, ùëù= 0.012). These findings suggest that\nparticipants in the LLM condition applied a more conversational\nstyle.\n4.2 Query Reformulation Strategies\nWe explore how users reformulate and refine their queries dur-\ning a given round, focusing on two primary aspects: changes in\nquery length and term repeats, which allows us to understand how\nparticipants progressively adapt their queries.\n4.2.1 Number of Terms. We investigate how the length of queries\nchanges within round. For each round, we computed the average\nnumber of terms in each query in order. Figure 5 shows the average\nnumber of terms by query order. Participants in the LLM condition\nissue an initial query of ‚àº6 terms and maintain this length for\nsubsequent queries. Meanwhile, participants in the Search condition\ntend to start with shorter ( ‚àº3) queries and gradually increase.\nLLM users, favoring longer queries, may indicate a tendency for\nconversational interactions. Conversely, participants in the Search\ncondition, may reflect an initial focus on keyword-driven retrieval,\nwith subsequent query expansion.\nFigure 6: Percentages of term repeats for successive queries\nin a round\nFigure 7: Distribution of syntactic-level types across ordered\nqueries for Search (top), and LLM (bottom) conditions\n4.2.2 Term Repeats. We examine term repeats within a round to\nunderstand how often users refined their initial queries. We com-\nputed the Jaccard similarity percentages [27] of consecutive queries\nin a round. Figure 6 shows percentages of queries that share identi-\ncal terms with the previous query in a round. Initially, participants\nin the LLM condition had generally lower term reuse of around\n20%, suggesting a moderate level of query refinement. In the Search\ncondition, participants began with a higher term reuse rate of 30%\nwith a gradual decline as the round progressed, which suggests that\nparticipants initially focused on refining their queries, then shifted\nto queries formulated differently or focused on new clues.\n4.3 Query Reformulation Types\nAnalyzing query reformulation types (QRTs) allows us to infer the\nuser intent in query reformulation. We adopt the QRT taxonomy\nACM SIGIR, March 10-14, 2024, Sheffield, UK Albatool Wazzan, Stephen MacNeil, and Richard Souvenir\nIntent Category Definition Example Query\nSpecification\n(Spec)\nQuery becomes more specific,\nnarrowing down the search in-\ntent\nsauf street sign‚Üísauf street\nsign handicap red x\nGeneralization\n(Gen)\nQuery becomes more general,\nbroadening the search intent\nsaint james peter adam Hamil-\nton‚Üísaint james\nSynonym\n(Syn)\nSubstitution of a term with its\nsynonym while maintaining the\noverall meaning\nRUE CREVAUX street MAP‚Üí\nRUE CREVAUX street location\nSomewhat\nRelevant\n(SR)\nIntent shifts slightly while re-\nmaining somewhat tied to the\noriginal query\nwhat language is SAUF in?‚Üí\nhow about rue crevaux?\nNew Topic\n(New)\nIntent shifts significantly to a\ndifferent subject\nwhat countries have placacen-\ntro masisa?‚Üíwhere is 17 de\nmayo in Chile?\nOthers\n(Oth)\nQueries that do not fit any cate-\ngory\nfederal street boston‚Üífederal\nstreet in (Boston)\nTable 2: Intent-level query reformulation types\nproposed by Chen et al. [6], which characterizes QRTs at both the\nsyntactic and intent level.\nSyntactic changes in consecutive queries, which involve alter-\nations in the structure and composition, are categorized into five\ntypes:\n‚Ä¢Add: New terms are introduced into the query, resulting in\nan expansion of its content.\n‚Ä¢Delete: Terms present in the previous query are removed in\nthe current query.\n‚Ä¢Change: Modifications involve replacing some terms while\nkeeping others unchanged.\n‚Ä¢Repeat: A query remains identical to the preceding one.\n‚Ä¢Others: A combination of different changes within a query\nor the introduction of an entirely new query.\nWe compute the percent of each syntactic category type for\nordered queries in a round. Figure 7 shows the distribution of\nsyntactic-level QRTs for the first, second, etc. query issued in each\nround. For the Search participants (Figure 7 (top)), we observe that\nthe predominant QRTs were ‚ÄúAdd‚Äù, ‚ÄúOthers‚Äù, and ‚ÄúChange. ‚Äù The\nhigh initial rate for ‚ÄúOther‚Äù suggests an exploratory intent at the\nonset. As the task progressed, there was a noticeable rise in the\n‚ÄúAdd‚Äù type, indicating adding details to their queries as they solved\nthe task, suggesting more exploitative behavior.\nFor the LLM condition (Figure 7 (bottom)), the most frequent QRT\nwas ‚ÄúChange‚Äù, showing a steady increase as participants progressed.\nCompared to the Search condition, the ‚ÄúAdd‚Äù type was less common.\nThis lower occurrence of ‚ÄúAdd‚Äù suggests that participants in the\nLLM condition were less inclined to augment their queries with\nadditional terms.\nBeyond the syntactic level, we examined the intent level QRT.\nRather than measuring how users modify queries, the aim is to\nmeasure why the changes were made to uncover the underlying\nmotivations, evolving information needs, and user goals. Table 2\nintroduces the six intent categories along with example queries\nfrom our dataset.\nTwo members of the research team performed intent-level cate-\ngorization. Both researchers individually categorized the queries,\nthen met to reach a consensus on any discrepancies. As shown in\n(Figure 8 (top)), for participants in the Search condition ‚ÄúSpecifica-\ntion‚Äù, and ‚ÄúNew Topic‚Äù were the predominant QRTs. Comparing\nFigure 8: Distribution of intent-level types across ordered\nqueries for Search (top), and LLM (bottom) conditions\nthese findings with the syntactic changes observed in (Figure 7\n(top)), we notice a similar pattern between ‚ÄúSpecification‚Äù and\n‚ÄúAdd‚Äù. This pattern suggests that participants were narrowing down\ntheir search intent by adding more details to their query. A parallel\ntrend is observed between ‚ÄúOthers‚Äù in the syntactic types and ‚ÄúNew\nTopic‚Äù as their percentages initially fluctuate but eventually follow\na similar pattern.\nFor the LLM participants, the distribution of intent-level QRTs is\nshown in (Figure 8 (bottom)). ‚ÄúSpecification‚Äù was the dominant cat-\negory. Analysing similar trends with the syntactic changes shown\nin (Figure 7 (bottom)), there was a steady increase in both ‚ÄòSpec-\nification‚Äù and ‚ÄúAdd‚Äù, however ‚ÄúSpecification‚Äù being much more\nfrequent than ‚ÄúAdd. ‚Äù Interestingly, a parallel trend can be observed\nbetween ‚ÄúSpecification‚Äù and ‚ÄúChange. ‚Äù This suggests that while\nparticipants using the LLM primarily focused on narrowing down\ntheir search intent, they did so without necessarily adding terms\nto their queries. This behavior differs from the Search condition\ntrend, where both ‚ÄúSpecification‚Äù and ‚ÄúAdd‚Äù showed increasing\npercentages and similar frequencies. This contrast showcases the\ndistinctive user interactions between the two assistants. While\nSearch condition ‚ÄúSpecification‚Äù often involves query expansion,\nin the LLM condition, ‚ÄúSpecification‚Äù primarily shows as query\nrephrasing.\n5 QUALITATIVE FINDINGS\nThe qualitative findings derive mainly from the responses to the\npost-study survey and a comparison of the search results returned\nby each search engine for similar queries.\nComparing Traditional and LLM-based Search for Image Geolocation ACM SIGIR, March 10-14, 2024, Sheffield, UK\nFigure 9: Top clues by participants across conditions\n5.1 Open-ended Survey Questions\nWe conducted a post-study survey with open-ended questions to\nbetter understand how participants translated clues into search\nqueries and the challenges they faced.\n5.1.1 Clues Identified by Participants. We coded and categorized\nresponses to the question: What types of clues did you identify\nthat helped you with the task? . As illustrated in Figure 9, the pre-\ndominant clue category was street names, followed closely by busi-\nness names, including store names. Language was also helpful to\nour participants, particularly to identify non-English speaking coun-\ntries. Some participants also referred to geographical features like\nmountains and large bodies of water. The identified clues were con-\nsistent across all participants, regardless of the assigned search tool;\nthis aligns with prior work that explored image geolocation [29].\n5.1.2 Translating Clues into Search Queries. The post-study survey\nasked: How did you translate the clues into search queries? .\nUsing an inductive, open coding approach [39], we coded and cate-\ngorized these responses into distinct strategies:\nLanguage Identification. 10 participants from each condition fo-\ncused on identifying the languages present on signs, buildings, and\nstores. P13 stated, ‚ÄúI would type in the words I saw, and ask the\nhelper, what language is this in?. ‚Äù Similarly, P18 explained, ‚ÄúI trans-\nlated some of the clues I saw to English, this way it shows me what\nthe origin of the language. ‚Äù\nLocating Street Signs. 10 participants from the Search and 7 from\nthe LLM condition utilized this strategy. They focused on finding\nstreet signs in corners and intersections to get closer to the location.\nP31 explained, ‚ÄúI was searching for street names, trying to identify\nwhich neighborhoods the locations were in, for larger cities. ‚Äù P36\nalso said, ‚ÄúI used the road signs to get a general idea of city and\ndirection of city. ‚Äù\nLocating Businesses/Stores. Nine participants from each condition\nfocused on locating businesses, stores, and shops. P37 mentioned,\n‚Äúmy strategy was typing company names into the helper. ‚Äù\nDescribing Geographic Features. A few participants, mostly from\nthe LLM condition, described the geographic features of the loca-\ntion. This included providing details about trees, architecture, and\nmountains. P13 explained, ‚ÄúI tried describing the environment I\nwas in to the chatbot, but the results were often not good. ‚Äù\nLocating Landmarks. In a similar approach to locating businesses,\na small number of participants actively searched for large buildings\nFigure 10: Comparison of challenges faced by our partici-\npants in both the Search and LLM conditions\nand landmarks. P51 noted, ‚ÄúI searched for a landmark around the\narea. ‚Äù\nAlthough these strategies did not differ significantly between\nthe two conditions, analyzing how participants translated visual\nclues into search queries is important for gaining insights into their\napproach to image geolocation.\n5.1.3 Challenges Identified by Participants. We asked the partici-\npants to describe the challenges they faced during the experiment.\nOnly a few participants did not answer or stated they faced no\nchallenges. The responses were coded into in six qualitative classes.\nEach code, with an example participant response, is given in Table 3.\nThe distribution of challenges across the experimental conditions\nis shown in Figure 10. There is notable discrepancy for Efficient\nQuery Formulation . While a few participants from the Search condi-\ntion did mention this challenge, it emerged as the primary obstacle\nfor over half of the participants in the LLM condition. P5 explained,\n‚ÄúMy biggest challenge was getting the chatbot to understand ex-\nactly what I wanted. ‚Äù Similarly, P17 said, ‚ÄúTrying to be concise and\nprecise with my searches using the chatbot was challenging. ‚Äù These\nparticipants encountered difficulties in effectively communicating\ntheir intent to the LLM-based search engine. Others took a more\nstrategic approach to address this challenge. P46 explained, ‚ÄúI real-\nized I needed to ask less specific questions and go more broadly to\nget answers. ‚Äù This adaptive strategy reflects participants‚Äô attempts\nto optimize their interactions with the LLM. Participant P55 men-\ntions, ‚ÄúThe challenging part was figuring out what I was looking at\nand translating it to a question that would narrow down answers\ncoming from the chatbot. ‚Äù\nAnother challenge worth highlighting was the language barrier.\nDespite the advanced language capabilities of modern LLMs, the\nparticipants had difficulties when formulating language-related\nquestions. P49 stated, ‚ÄúI got a response in Spanish when I typed a\nSpanish building name but the chatbot didn‚Äôt answer the question I\nwas asking. ‚Äù Similarly, P28 describes, ‚ÄúI had to rephrase a couple of\ntimes, especially If I was asking about phrases in different language\nthan English the chatbot would shoot back a failed search. ‚Äù\n5.2 Search Results\nWe examined the search queries and the results. For instance, as\nshown in Figure 11a, when participants searched for ‚ÄúStockland\nACM SIGIR, March 10-14, 2024, Sheffield, UK Albatool Wazzan, Stephen MacNeil, and Richard Souvenir\nQualitative Code Description Example Response\nLocation Specificity Precisely specifying and differentiat-\ning streets within a city\nP1(Search): I did have a hard time. There were\nmany First and Second streets I was on, and it is\ndifficult to distinguish those between the first and\nsecond streets of other cities\nLanguage Barriers Identifying Foreign Words with Non-\nEnglish alphabets\nP13(LLM): My strategy of finding word clues\nfailed if the words I saw were in a language that\ndoes not use the English alphabet\nInterpreting Visual Clues Translating visual clues into effective\nquestions or searches\nP58(Search): It is hard to try to search the architec-\nture of a building without using an image search\nEfficient Query Formulation Crafting efficient search queries that\nwould yield precise results\nP38(LLM): It was challenging to find the correct\nwording to get the desired result\nLack of Textual Clues Locations with limited textual clues,\nfor example remote locations\nP53(LLM): Sometimes it‚Äôs very hard to find street\nnames or shop names from the image, especially\nif the images are from remote locations\nEffective Clue Selection Finding clues that can be described or\nwill generate effective results when\nsearched\nP50(LLM): Figuring out what clue to look up,\nfor example, local places were useful, meanwhile\nlarge chains are not as useful\nTable 3: Qualitative codes resultant from the coding of challenges described by participants, and example quotes\n(a) Round 2 clue: Stockland Building\n(b) Round 5 clue: tritecnics\n(c) Round 6 clue: Camuzzi gas del sur\nFigure 11: Comparison of results obtained from asking about\nthe same clue by participants using different search assistants\nbuilding‚Äù, LLM search did not return any results and requested\nclarification, while the search engine quickly located the building\nin Sydney, Australia, in the first few results.\nSimilar disparities were observed when searching in languages\nother than English. For instance, when attempting to locate the\n‚Äútritecnics‚Äù company, as shown in Figure 11b, the LLM-based search\nstruggled to translate or make sense of the word, whereas the search\nengine produced map results for Barcelona, Spain. In Figure 11c,\nwhile the LLM-based search successfully replied that ‚ÄùCamuzzi gas\ndel sur‚Äù was a gas distribution company in Argentina, the search\nengine identified the location as the remote city of Ushuaia and\npresented a map, effectively solving the task for those participants.\n6 DISCUSSION\nThe results demonstrate some key findings: (1) participants us-\ning traditional search for assistance outperformed those using the\nLLM-based search, (2) distinct strategies emerged due to significant\nvariations in the query formulation strategies between the two\nconditions, and (3) the qualitative findings revealed that partici-\npants using the LLM search struggled to effectively formulate their\nqueries.\n6.1 Differences in Performance\nIn response to (RQ1), the results showed that participants using\nthe traditional search outperformed those using the LLM search.\nWe believe that a substantial portion of the performance difference\ncan be explained by the difference in results for similar queries,\nas shown in Section 5.2. Specifically, as shown in Figure 11b and\nFigure 11c, the search engine provided a map, effectively directing\nour participants to the exact location. LLM-based search should not\nonly provide creative answers, but the same features as a traditional\nsearch engine, including maps. As evident from the examples in\nour study, LLM search did not always return results that should\nhave been within its capabilities. Perhaps, the participants needed\nto explicitly request maps or specific directions to effectively uti-\nlize these features; however this was not necessary for traditional\nsearch. This highlights the challenge of crafting effective prompts,\nComparing Traditional and LLM-based Search for Image Geolocation ACM SIGIR, March 10-14, 2024, Sheffield, UK\nQuery LLM Response Score\ncountries in southern hemi-\nsphere with mountains\nThe Southern Hemisphere is home to many mountain\nranges, including the Andes, the Sierra Nevada de Santa\nMarta, and the Cordillera Oriental and Cordillera Occi-\ndental in South America. The highest peak in the Amer-\nicas, Aconcagua, is also located in the Andes. In Africa,\nMount Kilimanjaro is the tallest mountain and one of\nthe world‚Äôs largest volcanoes. The Rwenzori range of\nmountains in Africa were created during the formation\nof the Great Rift Valley. New Zealand‚Äôs South Island\nhas more than 24 mountains with elevations of 9,000\nfeet or more1. I hope this helps!\n605\npatas para arriba location(Original Spanish) La expresi√≥n ‚Äúpatas para arriba‚Äù se\nutiliza para describir una situaci√≥n en la que todo est√°\ndesordenado o confuso. ¬øHay algo m√°s en lo que pueda\nayudarte? (Translated) The expression ‚Äúupside down‚Äù\nis used to describe a situation in which everything is\ndisordered or confusing. Is there anything else I can\nhelp you with?\n1240\n24seventaxi location I‚Äôm sorry but I couldn‚Äôt find any information about\n24seventaxi location. Could you please provide me with\nmore details about what you are looking for?\n1773\nI need to know where the\nstreet H. de Magallanes is lo-\ncated in the world\nI‚Äôm sorry but I couldn‚Äôt find any information about H.\nde Magallanes street. Could you please provide more\ndetails about the location or any other information that\nmight help me find it?\n18\nTable 4: Examples where participants developed inaccurate\nmental models, resulting in the formulation of single, poor-\nquality queries, which led to low performance\nwhich, as research has shown [ 47], significantly influences the\noutput of LLM-based methods. While ‚ÄúLearn more‚Äù links were in-\ncluded with each answer, we found that none of the participants\ntook advantage of this feature. This raises questions about the per-\nceived affordances of LLMs compared to traditional search engines,\nas the integration of similar features in LLMs may not be as in-\ntuitive, as our study suggests. This underutilization of help links\nin LLM responses highlights the challenges of transitioning from\ntraditional search engines to LLM-based search and aligns with\nGibson‚Äôs theory [10] emphasizing the interaction between users\nand their technological environment, which shapes the possibilities\nand constraints for action.\nIn response to (RQ3), our qualitative analysis provided valu-\nable insights into the performance disparities. Over half of the\nparticipants using the LLM-based search expressed difficulties in\nformulating their queries. Some expressed hesitation, while others\nfound it challenging to form queries that effectively communicated\ntheir information needs. Participants also struggled to formulate\nqueries in different languages. Despite stated support of LLM search\nfor many languages, there were reported instances of failed results.\nThis emphasizes the challenge of effectively prompting the LLM\nto comprehend and respond to queries in diverse languages. These\nchallenges were less prevalent when using the search engine. These\nobservations align with our query formulation analysis. In Sec-\ntion 4.1.1, we noted that the average number of queries for LLM\nparticipants consisted of a single query. Figure 3 showed that LLM\nparticipants were less inclined to reformulate or issue more queries\nthroughout the task compared to Search participants.\nWhile it has been shown that individuals can quickly build men-\ntal models when interacting with LLM chatbots [ 16], the quality\nof these models remains uncertain, especially considering the rela-\ntively new nature of LLM technology. Participants may not have\nFigure 12: Summary findings of query formulation and re-\nformulation analysis.\nhad the experience to develop accurate mental models of LLM capa-\nbilities. In the absence of well-defined mental models, users struggle\nto predict outcomes or make sense of their interactions with LLMs,\nleading to instances where users pose vague and poorly defined\nqueries while expecting the LLM to respond appropriately [37, 47].\nTable 4 demonstrates a few examples of this behavior. It is plausible\nthat these mental models influenced the participants to attempt\nonly a single query, potentially resulting in worse performance.\n6.2 Differences in Query Formulation &\nReformulation Strategies\nFigure 12 presents a summary of our findings into query formula-\ntion and reformulation strategies, which is directly related to(RQ2).\nAs described in Section 4.1, our study showed significant differ-\nences across all four formulation metrics. Participants in the Search\ncondition issued shorter queries and a significantly increased use of\nproper nouns including places, store names, and streets. These find-\nings align with prior research [18, 24], indicating that individuals\nare accustomed to keyword-based search from a lifetime of expe-\nrience using traditional search engines. In contrast, participants\nusing the LLM-based search were inclined to issue longer and more\nnatural language queries. These findings suggest that they adhered\nto the perceived norms of a conversational user interface [8].\nSeveral research efforts have explored the topic of query reformu-\nlation for traditional web search [2, 13, 20]. In our study, we adopted\nan existing taxonomy for categorizing both the syntactic and intent-\nbased types of query reformulation, detailed in Section 4.2. Our\ninvestigation revealed a notable trend. Although ‚ÄúSpecification‚Äù or\nACM SIGIR, March 10-14, 2024, Sheffield, UK Albatool Wazzan, Stephen MacNeil, and Richard Souvenir\nthe act of narrowing down the scope and intent of a search, was\nthe dominant category across both conditions, it aligned with dif-\nferent syntactic categories. In the Search condition, participants\noften expanded their queries by adding terms, a common behavior\nobserved in prior research [6, 7]. However, participants using the\nLLM search frequently narrowed down their search intents by para-\nphrasing their initial queries. This observation prompts a critical\nquestion of whether the development of new query reformulation\ntaxonomies specific to LLMs could provide a better framework for\nunderstanding and characterizing user behavior.\n6.3 Geolocation Sensemaking Strategies\nAs described in Section 5.1.2, participants employed diverse strate-\ngies when searching. In geolocation tasks, participants make sense\nof the clues within the images by using internal or external knowl-\nedge representations [29]. Our focus was on external knowledge\nrepresentations cultivated through their searching. Therefore, we\ndid not ask about internal knowledge, such as participants‚Äô cultural\nbackgrounds or travel history.\nIn geolocation tasks, participants engage in sensemaking to inter-\npret visual cues within images. Participants demonstrate adaptabil-\nity in their approach, drawing from Pirolli and Card‚Äôs sensemaking\nmodel [33], which encompasses both top-down and bottom-up ap-\nproaches. The top-down approach involves initiating the process\nwith a theory or a broader concept and then seeking data to sub-\nstantiate it. In this context, an example of the top-down approach\nis the strategy of language identification. Participants effectively\nemploy this strategy by querying the assistant about the language‚Äôs\norigin, which narrows the search scope to specific global regions.\nSimilarly, some participants employed sensory sensemaking as a\ntop-down approach by describing sensory aspects of the location\nto the assistant. However, the limited effectiveness of this strategy\nsuggests that relying solely on sensory cues may not be sufficient\nfor precise geolocation. The bottom-up approach, on the other\nhand, entails gathering data first and progressively forming a the-\nory based on the available information. Examples of this approach\nincluded using street signs, buildings, and landmarks as reference\npoints, facilitating the identification of cities then neighborhoods.\nThe strategies identified in this study provide insights into user\nbehavior and align with the cognitive processes driving geolocation\nsensemaking described by prior work [41]. Understanding how par-\nticipants construct mental models based on image clues and apply\nsensemaking processes, enhances our ability to provide effective\nsupport and guidance in geolocation tasks.\n6.4 Limitations\nAmongst the study limitations was the latency in LLM-based search.\nAlthough the latency was only a few seconds, it could have dis-\nrupted the conversational flow, potentially affecting satisfaction\nand engagement during the task. Our study included participants\nwith diverse backgrounds, education levels, ages, and degrees of\ntechnical literacy. This inherent diversity may have influenced how\nparticipants interacted with both the image geolocation task and\nthe search engines. Furthermore, it‚Äôs essential to acknowledge that\nimage geolocation tasks have historically been conducted by ex-\npert image analysts. In our study, we did not explicitly categorize\nparticipants based on their levels of expertise in geolocation. Lastly,\nour evaluation did not include the assessment of specific metrics\nsuch as search engine result pages (SERPs), clicks, or other perfor-\nmance indicators that could offer a more comprehensive view of\nthe effectiveness of LLM-based search in image geolocation tasks.\n7 CONCLUSION AND FUTURE WORK\nThis study offered valuable insights into differences in strategies\nand user behaviors when using traditional compared to LLM-based\nsearch for image geolocation. We examined the differences in per-\nformance, query formulation, and the sensemaking strategies em-\nployed by participants in these two conditions. Despite the growing\ncapabilities of LLMs, the results reveal that participants using tradi-\ntional search engines outperformed those relying on LLM-based\nsearch. An in-depth exploration of the distinct query formulation\nstrategies utilized by participants mostly explained the performance\ndifference, as evidenced by our qualitative findings, with query for-\nmulation being identified as the most challenging aspect of the\nexperiment. Additionally, we observed a tendency among partici-\npants using LLMs to engage in fewer multi-query search sessions,\npossibly reflecting uncertainties surrounding LLM capabilities and\nthe perceived affordances associated with LLM interface.\nOur findings can extend beyond the geolocation domain, provid-\ning initial insights into user interactions with LLMs in real-world\napplications and prompting more research on human-centered de-\nsign of LLM interfaces, with a focus on understanding how users\nform mental models of LLMs. To achieve more useful LLM inter-\nfaces, it is necessary to first develop a better understanding of query\nformulation strategies and behavior. Extensive prior research about\ntraditional search provides a solid foundation for exploring query\nformulation strategies. Our work presented in this paper begins to\nextend this research based on the novel capabilities and the conver-\nsational nature of LLM-based search; however, more research in this\narea is needed. The second component of making LLM interfaces\nmore usable is to teach novices how to effectively prompt. Emerg-\ning systems like AI Chains [46], MemorySandbox [14], Feedback\nBuffet [28], and PromptMaker [ 19] are at the forefront, making\nLLMs more comprehensible and user-friendly through the use of\ntemplates [19, 28] and procedural guidance [14, 15, 46]. These tools\nare designed to assist novice users in prompt creation by integrat-\ning visual problem representation, incorporating partial prompts,\nand providing user friendly interfaces that facilitate easy iteration\nbased on the LLM output. These advancements represent a leap\ntowards a future in which user interactions with language models\nbecome more intuitive, efficient, and user-friendly.\nACKNOWLEDGMENTS\nThanks to Andrea Brandt for assisting with the user study. This\nresearch was sponsored by the DEVCOM Analysis Center and was\naccomplished under Cooperative Agreement Number W911NF-22-\n2-0001. The views and conclusions contained in this document are\nthose of the authors and should not be interpreted as represent-\ning the official policies, either expressed or implied, of the Army\nResearch Office or the U.S. Government. The U.S. Government is\nauthorized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation herein.\nComparing Traditional and LLM-based Search for Image Geolocation ACM SIGIR, March 10-14, 2024, Sheffield, UK\nREFERENCES\n[1] Leonard Adolphs, Shehzaad Dhuliawala, and Thomas Hofmann. 2021. How to\nQuery Language Models? arXiv:2108.01928 [cs.CL]\n[2] Anne Aula. 2003. Query Formulation in Web Information Search.. In ICWI.\nInternational Conference WWW/Internet, Algarve, Portugal, 403‚Äì410.\n[3] Cory Barr, Rosie Jones, and Moira Regelson. 2008. The Linguistic Structure of\nEnglish Web-Search Queries. In Proceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (Honolulu, Hawaii) (EMNLP ‚Äô08) . Association\nfor Computational Linguistics, USA, 1021‚Äì1030.\n[4] Simon Bilel Jegham, dim fort. 2021. GeoGuess. MIT Licensed. https://github.\ncom/GeoGuess/GeoGuess Accessed: 2023.\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\narXiv:2005.14165 [cs.CL]\n[6] Jia Chen, Jiaxin Mao, Yiqun Liu, Fan Zhang, Min Zhang, and Shaoping Ma.\n2021. Towards a Better Understanding of Query Reformulation Behavior in\nWeb Search. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia)\n(WWW ‚Äô21) . Association for Computing Machinery, New York, NY, USA, 743‚Äì755.\nhttps://doi.org/10.1145/3442381.3450127\n[7] Jia Chen, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2019. Investigating\nquery reformulation behavior of search users. InChina Conference on Information\nRetrieval. Springer, China, 39‚Äì51.\n[8] Leigh Clark, Nadia Pantidi, Orla Cooney, Philip Doyle, Diego Garaialde, Justin\nEdwards, Brendan Spillane, Emer Gilmartin, Christine Murad, Cosmin Munteanu,\nVincent Wade, and Benjamin R. Cowan. 2019. What Makes a Good Conversation?\nChallenges in Designing Truly Conversational Agents. In Proceedings of the 2019\nCHI Conference on Human Factors in Computing Systems (Glasgow, Scotland\nUk) (CHI ‚Äô19) . Association for Computing Machinery, New York, NY, USA, 1‚Äì12.\nhttps://doi.org/10.1145/3290605.3300705\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill\nBurstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-\ntional Linguistics, Minneapolis, USA, 4171‚Äì4186. https://doi.org/10.18653/v1/n19-\n1423\n[10] James J Gibson. 2014. The ecological approach to visual perception: classic edition .\nPsychology press, Online.\n[11] Jutta Haider and Olof Sundin. 2019. Invisible search and online search engines:\nThe ubiquity of search in everyday life . Taylor & Francis, London.\n[12] James Hays and Alexei A Efros. 2008. Im2gps: estimating geographic informa-\ntion from a single image. In 2008 ieee conference on computer vision and pattern\nrecognition. IEEE, CVPR, Alaska, USA, 1‚Äì8.\n[13] Jeff Huang and Efthimis N. Efthimiadis. 2009. Analyzing and Evaluating Query\nReformulation Strategies in Web Search Logs. In Proceedings of the 18th ACM\nConference on Information and Knowledge Management (Hong Kong, China)\n(CIKM ‚Äô09) . Association for Computing Machinery, New York, NY, USA, 77‚Äì86.\nhttps://doi.org/10.1145/1645953.1645966\n[14] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil.\n2023. Memory Sandbox: Transparent and Interactive Memory Management for\nConversational Agents. InAdjunct Proceedings of the 36th Annual ACM Symposium\non User Interface Software and Technology (UIST ‚Äô23 Adjunct) . Association for\nComputing Machinery, New York, NY, USA, Article 97, 3 pages. https://doi.org/\n10.1145/3586182.3615796\n[15] Ziheng Huang, Kexin Quan, Joel Chan, and Stephen MacNeil. 2023. CausalMap-\nper: Challenging Designers to Think in Systems with Causal Maps and Large\nLanguage Model. In Proceedings of the 15th Conference on Creativity and Cognition\n(Virtual Event, USA)(C&C ‚Äô23). Association for Computing Machinery, New York,\nNY, USA, 325‚Äì329. https://doi.org/10.1145/3591196.3596818\n[16] Angel Hsing-Chi Hwang and Andrea Stevenson Won. 2021. IdeaBot: investigating\nsocial facilitation in human-machine team creativity. In Proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems . ACM, Yokohama,Japan,\n1‚Äì16.\n[17] Rahul J Jadhav, Om Prakash Gupta, and Usharani T Pawar. 2011. Significant\nrole of search engine in higher education. International Journal of Scientific &\nEngineering Research 2, 4 (2011), 1‚Äì5.\n[18] Bernard J Jansen and Amanda Spink. 2006. How are we searching the World\nWide Web? A comparison of nine search engine transaction logs. Information\nprocessing & management 42, 1 (2006), 248‚Äì263.\n[19] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach,\nMichael Terry, and Carrie J Cai. 2022. PromptMaker: Prompt-Based Prototyping\nwith Large Language Models. In Extended Abstracts of the 2022 CHI Conference\non Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA ‚Äô22) .\nAssociation for Computing Machinery, New York, NY, USA, Article 35, 8 pages.\nhttps://doi.org/10.1145/3491101.3503564\n[20] Jyun-Yu Jiang, Yen-Yu Ke, Pao-Yu Chien, and Pu-Jen Cheng. 2014. Learn-\ning User Reformulation Behavior for Query Auto-Completion. In Proceedings\nof the 37th International ACM SIGIR Conference on Research & Development\nin Information Retrieval (Gold Coast, Queensland, Australia) (SIGIR ‚Äô14) . As-\nsociation for Computing Machinery, New York, NY, USA, 445‚Äì454. https:\n//doi.org/10.1145/2600428.2609614\n[21] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How Can\nWe Know What Language Models Know? arXiv:1911.12543 [cs.CL]\n[22] Seungun Kim, Masaki Matsubara, and Atsuyuki Morishima. 2022. Image Ge-\nolocation by Non-Expert Crowd Workers with an Expert Strategy. In2022 IEEE\nInternational Conference on Big Data . IEEE xplore, Osaka, Japan, 4009‚Äì4013.\nhttps://doi.org/10.1109/BigData55660.2022.10020932\n[23] Rachel Kohler, John Purviance, and Kurt Luther. 2017. Supporting Image Ge-\nolocation with Diagramming and Crowdsourcing. Proceedings of the AAAI\nConference on Human Computation and Crowdsourcing 5, 1 (Sep. 2017), 98‚Äì107.\nhttps://doi.org/10.1609/hcomp.v5i1.13296\n[24] Dirk Lewandowski. 2008. Search engine user behaviour: How can users be guided\nto quality content? Information Services & Use 28, 3-4 (2008), 261‚Äì268.\n[25] Xinyi Li, Bob J.A. Schijvenaars, and Maarten de Rijke. 2017. Investigating queries\nand search failures in academic search. Information Processing & Management 53,\n3 (2017), 666‚Äì683. https://doi.org/10.1016/j.ipm.2017.01.005\n[26] Tsung-Yi Lin, Yin Cui, Serge Belongie, and James Hays. 2015. Learning deep\nrepresentations for ground-to-aerial geolocalization. In 2015 IEEE Conference\non Computer Vision and Pattern Recognition . IEEE xplore, Boston, MA, USA,\n5007‚Äì5015. https://doi.org/10.1109/CVPR.2015.7299135\n[27] Chang Liu, Xiangmin Zhang, and Wei Huang. 2016. The exploration of objective\ntask difficulty and domain knowledge effects on users‚Äô query formulation. Pro-\nceedings of the Association for Information Science and Technology 53 (12 2016),\n1‚Äì9. https://doi.org/10.1002/pra2.2016.14505301063\n[28] Stephen MacNeil, Andrew Tran, Joanne Kim, Ziheng Huang, Seth Bernstein, and\nDan Mogil. 2023. Prompt Middleware: Mapping Prompts for Large Language\nModels to UI Affordances. arXiv:2307.01142 [cs.HC]\n[29] Sneha Mehta, Chris North, and Kurt Luther. 2016. An exploratory study of\nhuman performance in image geolocation tasks. In GroupSight Workshop on\nHuman Computation for Image and Video Analysis , Vol. 308. HCOMP 2016, Austin,\nTX (USA), 3‚Äì4.\n[30] Bo Pang and Ravi Kumar. 2011. Search in the lost sense of ‚Äúquery‚Äù: Question\nformulation in Web search queries and its temporal changes. InProceedings of\nthe 49th Annual Meeting of the Association for Computational Linguistics: Human\nLanguage Technologies. Association for Computational Linguistics, USA, 135‚Äì140.\n[31] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker,\nand Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces\nThat Elicit Natural Language Queries. In Proceedings of the 2021 Conference on\nHuman Information Interaction and Retrieval (Canberra ACT, Australia)(CHIIR\n‚Äô21). Association for Computing Machinery, New York, NY, USA, 261‚Äì265. https:\n//doi.org/10.1145/3406522.3446035\n[32] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt√§schel, Yuxiang Wu,\nAlexander H. Miller, and Sebastian Riedel. 2020. How Context Affects Language\nModels‚Äô Factual Predictions. arXiv:2005.04611 [cs.CL]\n[33] Peter Pirolli and Stuart Card. 2005. The sensemaking process and leverage\npoints for analyst technology as identified through cognitive task analysis. In\nProceedings of international conference on intelligence analysis , Vol. 5. McLean,\nVA, USA, 2‚Äì4.\n[34] Amudha Ravi Shankar, Jose Fernandez-Marquez, Gabriele Scalia, Maria Rosa Mon-\ndardini, and Giovanna Di Marzo Serugendo. 2019. CROWD4EMS: A CROWD-\nSOURCING PLATFORM FOR GATHERING AND GEOLOCATING SOCIAL ME-\nDIA CONTENT IN DISASTER RESPONSE. ISPRS - International Archives of the\nPhotogrammetry, Remote Sensing and Spatial Information Sciences XLII-3/W8 (08\n2019), 331‚Äì340. https://doi.org/10.5194/isprs-archives-XLII-3-W8-331-2019\n[35] Laria Reynolds and Kyle McDonell. 2021. Prompt Programming for Large Lan-\nguage Models: Beyond the Few-Shot Paradigm. In Extended Abstracts of the 2021\nCHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI\nEA ‚Äô21) . Association for Computing Machinery, New York, NY, USA, Article 314,\n7 pages. https://doi.org/10.1145/3411763.3451760\n[36] S Salehi, J Tina-Du, and H Ashman. 2018. Use of Web search engines and\npersonalisation in information searching for educational purposes. iRInformation\nResearch.\n[37] Hariharan Subramonyam, Christopher Lawrence Pondoc, Colleen Seifert, Ma-\nneesh Agrawala, and Roy Pea. 2023. Bridging the Gulf of Envisioning: Cognitive\nDesign Challenges in LLM Interfaces. arXiv preprint arXiv:2309.14459 none, none\n(2023), 10 pages.\n[38] Jaime Teevan, Daniel Ramage, and Merredith Ringel Morris. 2011. TwitterSearch:\nA Comparison of Microblog Search and Web Search. In Proceedings of the Fourth\nACM International Conference on Web Search and Data Mining (Hong Kong, China)\nACM SIGIR, March 10-14, 2024, Sheffield, UK Albatool Wazzan, Stephen MacNeil, and Richard Souvenir\n(WSDM ‚Äô11) . Association for Computing Machinery, New York, NY, USA, 35‚Äì44.\nhttps://doi.org/10.1145/1935826.1935842\n[39] David R Thomas. 2006. A general inductive approach for analyzing qualitative\nevaluation data. American journal of evaluation 27, 2 (2006), 237‚Äì246.\n[40] Paul Thomas, Bodo Billerbeck, Nick Craswell, and Ryen W White. 2019. Investi-\ngating searchers‚Äô mental models to inform search explanations.ACM Transactions\non Information Systems (TOIS) 38, 1 (2019), 1‚Äì25.\n[41] Sukrit Venkatagiri, Jacob Thebault-Spieker, Rachel Kohler, John Purviance, Ri-\nfat Sabbir Mansur, and Kurt Luther. 2019. GroundTruth: Augmenting expert\nimage geolocation with crowdsourcing and shared representations. Proceedings\nof the ACM on Human-Computer Interaction 3, CSCW (2019), 1‚Äì30.\n[42] Yiwei Wang, Jiqun Liu, Soumik Mandal, and Chirag Shah. 2017. Search successes\nand failures in query segments and search tasks: A field study. Proceedings of the\nAssociation for Information Science and Technology 54, 1 (2017), 436‚Äì445.\n[43] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fe-\ndus. 2022. Emergent Abilities of Large Language Models. arXiv:2206.07682 [cs.CL]\n[44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei\nXia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-Thought Prompt-\ning Elicits Reasoning in Large Language Models. In Advances in Neural In-\nformation Processing Systems , S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-\ngrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., New Orleans,\nUSA, 24824‚Äì24837. https://proceedings.neurips.cc/paper_files/paper/2022/file/\n9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\n[45] Tobias Weyand, Ilya Kostrikov, and James Philbin. 2016. PlaNet - Photo Ge-\nolocation with Convolutional Neural Networks. In Computer Vision ‚Äì ECCV\n2016. Springer International Publishing, Amsterdam, The Netherlands, 37‚Äì55.\nhttps://doi.org/10.1007/978-3-319-46484-8_3\n[46] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. InProceedings of the 2022 CHI Conference on Human Factors in Computing\nSystems (New Orleans, LA, USA)(CHI ‚Äô22). Association for Computing Machinery,\nNew York, NY, USA, Article 385, 22 pages. https://doi.org/10.1145/3491102.\n3517582\n[47] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny Can‚Äôt Prompt: How Non-AI Experts Try (and Fail) to Design\nLLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems (Hamburg, Germany) (CHI ‚Äô23) . Association for Computing\nMachinery, New York, NY, USA, Article 437, 21 pages. https://doi.org/10.1145/\n3544548.3581388\n[48] Guido Zuccon, Bevan Koopman, and Joao Palotti. 2015. Diagnose this if you\ncan: On the effectiveness of search engines in finding medical self-diagnosis\ninformation. In Advances in Information Retrieval: 37th European Conference on\nIR Research, ECIR 2015, March 29-April 2, 2015. Proceedings 37 . Springer, Vienna,\nAustria, 562‚Äì567.",
  "topic": "Geolocation",
  "concepts": [
    {
      "name": "Geolocation",
      "score": 0.8879673480987549
    },
    {
      "name": "Search engine",
      "score": 0.7728581428527832
    },
    {
      "name": "Information retrieval",
      "score": 0.7269341349601746
    },
    {
      "name": "Computer science",
      "score": 0.7268491983413696
    },
    {
      "name": "Web search query",
      "score": 0.5635921359062195
    },
    {
      "name": "Visual search",
      "score": 0.47375044226646423
    },
    {
      "name": "Semantic search",
      "score": 0.4624602496623993
    },
    {
      "name": "Query expansion",
      "score": 0.45866265892982483
    },
    {
      "name": "Search analytics",
      "score": 0.45633453130722046
    },
    {
      "name": "Image (mathematics)",
      "score": 0.41892462968826294
    },
    {
      "name": "World Wide Web",
      "score": 0.37288224697113037
    },
    {
      "name": "Artificial intelligence",
      "score": 0.207925945520401
    }
  ]
}