{
  "title": "Building Hierarchically Disentangled Language Models for Text Generation with Named Entities",
  "url": "https://openalex.org/W3115037886",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2124379589",
      "name": "Yash Agarwal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3018556083",
      "name": "Devansh Batra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A82445930",
      "name": "Ganesh Bagler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963823251",
    "https://openalex.org/W2951309718",
    "https://openalex.org/W2960416371",
    "https://openalex.org/W3025860432",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2096765155",
    "https://openalex.org/W2553397501",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W2740130862",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3128634114",
    "https://openalex.org/W2738136547",
    "https://openalex.org/W2250379752",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2931198394",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2252269235",
    "https://openalex.org/W2561658355",
    "https://openalex.org/W2954801189",
    "https://openalex.org/W2948037078",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W187655585",
    "https://openalex.org/W2962950136",
    "https://openalex.org/W2586756136",
    "https://openalex.org/W2963094819",
    "https://openalex.org/W2950397305",
    "https://openalex.org/W2766570588",
    "https://openalex.org/W1916044692",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W2970990259",
    "https://openalex.org/W2761670104",
    "https://openalex.org/W2125417976",
    "https://openalex.org/W1566256432",
    "https://openalex.org/W2737041163",
    "https://openalex.org/W4288375838",
    "https://openalex.org/W2963676655",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2344444819",
    "https://openalex.org/W2979486033",
    "https://openalex.org/W2971008324",
    "https://openalex.org/W2903902750"
  ],
  "abstract": "Named entities pose a unique challenge to traditional methods of language modeling. While several domains are characterised with a high proportion of named entities, the occurrence of specific entities varies widely. Cooking recipes, for example, contain a lot of named entities — viz. ingredients, cooking techniques (also called processes), and utensils. However, some ingredients occur frequently within the instructions while most occur rarely. In this paper, we build upon the previous work done on language models developed for text with named entities by introducing a Hierarchically Disentangled Model. Training is divided into multiple branches with each branch producing a model with overlapping subsets of vocabulary. We found the existing datasets insufficient to accurately judge the performance of the model. Hence, we have curated 158,473 cooking recipes from several publicly available online sources. To reliably derive the entities within this corpus, we employ a combination of Named Entity Recognition (NER) as well as an unsupervised method of interpretation using dependency parsing and POS tagging, followed by a further cleaning of the dataset. This unsupervised interpretation models instructions as action graphs and is specific to the corpus of cooking recipes, unlike NER which is a general method applicable to all corpora. To delve into the utility of our language model, we apply it to tasks such as graph-to-text generation and ingredients-to-recipe generation, comparing it to previous state-of-the-art baselines. We make our dataset (including annotations and processed action graphs) available for use, considering their potential use cases for language modeling and text generation research.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 26–38\nBarcelona, Spain (Online), December 8-13, 2020\n26\nBuilding Hierarchically Disentangled Language Models for Text\nGeneration with Named Entities\nYash Agarwal*\nNSUT, New Delhi\nyasha.co.17@nsit.net.in\nGanesh Bagler\nIIIT-Delhi, New Delhi\nbagler@iiitd.ac.in\nDevansh Batra*\nNSUT, New Delhi\ndevanshb.it.17@nsit.net.in\nAbstract\nNamed entities pose a unique challenge to traditional methods of language modeling. While\nseveral domains are characterised with a high proportion of named entities, the occurrence of\nspeciﬁc entities varies widely. Cooking recipes, for example, contain a lot of named entities —\nviz. ingredients, cooking techniques (also called processes), and utensils. However, some ingre-\ndients occur frequently within the instructions while most occur rarely. In this paper, we build\nupon the previous work done on language models developed for text with named entities by in-\ntroducing a Hierarchically Disentangled Model. Training is divided into multiple branches with\neach branch producing a model with overlapping subsets of vocabulary. We found the existing\ndatasets insufﬁcient to accurately judge the performance of the model. Hence, we have curated\n158,473 cooking recipes from several publicly available online sources. To reliably derive the\nentities within this corpus, we employ a combination of Named Entity Recognition (NER) as\nwell as an unsupervised method of interpretation using dependency parsing and POS tagging,\nfollowed by a further cleaning of the dataset. This unsupervised interpretation models instruc-\ntions as action graphs and is speciﬁc to the corpus of cooking recipes, unlike NER which is a\ngeneral method applicable to all corpora. To delve into the utility of our language model, we ap-\nply it to tasks such as graph-to-text generation and ingredients-to-recipe generation, comparing it\nto previous state-of-the-art baselines. We make our dataset (including annotations and processed\naction graphs) available for use, considering their potential use cases for language modeling and\ntext generation research.\n1 Introduction\nLanguage Modeling is an important task in Natural Language Processing with several applications (Wise-\nman et al., 2017) including auto complete (Arnold et al., 2017) among others. A key application of\nlanguage modeling is text generation (Semeniuta et al., 2017). Recent advances in language models have\nled to impressive generation of programming code in languages such as Java and C (Hindle et al., 2016;\nYin and Neubig, 2017; Hellendoorn and Devanbu, 2017; Rabinovich et al., 2017). Similarly, efforts\nhave been made towards application of language models in the context of cooking recipes (Parvez et\nal., 2018; Kiddon et al., 2016). Both these tasks, recipe and code generation, deal with corpora heavily\npopulated with named entities. In case of cooking recipes these entities are thousands of ingredients\nused in recipes, hundreds of cooking processes which can be applied upon them and tens of utensils used\nacross the different cuisines. These numerous entities immensely increase the vocabulary of the language\nmodel. Typically, the vocabulary of corpora of cooking recipes is of the order of tens of thousands of\ntokens. The vocabulary of the corpus we prepared (more details will follow later in the manuscript) has\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n*Yash Agarwal and Devansh Batra contributed equally to the work presented in this manuscript.\n27\nabout 54 thousand tokens and 6 thousand ingredients, which form the single largest entity type. It is\namply clear that the language model has to deal with the language semantics comprising of thousands of\nEnglish language tokens along with the way ingredients, utensils and processes interact with each other.\nParvez et al. (2018) provide credence to this argument. For instance, fruits occur 27,260 times in\ntheir corpus but the fruit ‘apple’ occurs only 720 times. And fruit is just one kind of ingredient. To\nsome extent, all fruits can be modeled as one entity, as all of them largely behave in the same manner\nwhen written in a natural language. The processes like cutting and peeling are common to all fruits.\nThus (Parvez et al., 2018) propose using one entity level language model which predicts whether a fruit\nor a vegetable should occur at a given place, followed by a model speciﬁc to the entity type which predicts\nthe probabilistic distribution of which exact fruit ﬁts well. They show that this method greatly inﬂuences\nthe quality of text generation for both recipe and code generation owing to the reduction in vocabulary\nand complexity for the models being trained.\nWe note that programming languages have far less ambiguities and variations than natural language\nbecause they are meant to be compiled using a detailed protocol. Due to this reason, the language models\nperform far better on code generation than natural language generation tasks even though the former has\na lot of data types and variable names (which work like the entity types and their name). These models\nhave already achieved a BLEU score as high as 42.39 (Wei et al., 2019) and models by (Parvez et al.,\n2018) achieved a validation perplexity as low as 2.39 (a perplexity of 1.0 would mean that the cross-\nentropy loss is zero and is theoretically the best result). Thus, we feel that any further improvement\nin the language model would be hard to quantitatively estimate for programming languages by existing\nmetrics, which is why we focused on corpora of cooking recipes.\nThere have been several advances in the ﬁeld of recipe text generation. The ﬁrst implemented system\nwas called EPICURE (Dale, 2019) and was a purely rule based generation algorithm. (Cromwell et al.,\n2015) developed a system for generating salads using statistical search ingredient combinations. (Kiddon\net al., 2016) developed a modern neural recipe generation system by maintaining a state of available and\nused state. Despite that it had a low BLEU score. More recently, the release of Recipe1M+ (Marin et\nal., 2019), has led to the development a related domain of recipe retrieval — retrieving recipes from a\nlarge scale dataset (Zhu et al., 2019; Salvador et al., 2017). In the past year, we have seen two new\napplications — recipe generation from image (Salvador et al., 2019), and personalised recipe generation\n(recommendation) based on user reviews (Majumder et al., 2019). The problem of ingredient to recipe\ngeneration is also tackled by many of these papers.\nAligned with the arguments made earlier, we decided to create our own dataset (which was further\ncleaned and annotated) for the development of the language model. Taking inspiration from (Kiddon\net al., 2015), we apply an unsupervised recipe interpretation pipeline to model the instructions as action\ngraphs along with Named Entity Recognition models which describe in detail in Section 2.\nThe language model is itself described in Section 3. We further study some of the applications of\nour language models, testing it for tasks such as graph-to-recipe generation and ingredients-to-recipe\ngeneration. Image-to-recipe generation as beyond the scope of this paper since (Salvador et al., 2019)\nshows that clues from images are a big help for the language model in predicting the recipe.\n2 Dataset\n2.1 Need for a new dataset\nThe existing research explorations have used a variety of datasets for their experiments. (Kiddon et al.,\n2016) and (Parvez et al., 2018) employ the ‘Now You’re Cooking!’ dataset containing 150,000 recipes.\nHowever, as also pointed out by (Kiddon et al., 2016), many of them do not follow a uniform format,\nhave missing instructions or are duplicates. This leads to only 84,590 usable recipes. Both these papers\nprovide their preprocessed datasets. However, (Parvez et al., 2018) removes all instances of numerals\nwithin the text without citing a reason. While this may been done to avoid the serial number of the\nsteps in recipes, removing numbers takes away important instances of quantities of ingredients. In our\nexperiments with this dataset, we observed several instances like “keep boiling for minutes”. (Kiddon et\nal., 2016) outrightly stripped away all amount information, including the unit, (e.g. ‘1 ts’) Clearly, this\n28\nstep makes the modiﬁed dataset unusable for generation of viable recipes.\nWe believe that the original dataset is heavily based on AllRecipes and is insufﬁcient in itself to get\nall entities. In this paper, we aimed to make a plug and play language model for cooking recipes which\ncan be used for several applications such as ingredient-to-recipe generation, image-to-recipe generation,\naction-graphs-to-recipe generation, translation of recipe between languages etc., such that only the en-\ncoder would differ in all these use cases. For this purpose, we require a dataset with a wide coverage of\ndifferent types of entities as well as the entities themselves. We believe that a dataset of 84k recipes is\nfar too insufﬁcient for such a purpose.\nA recently released related dataset by (Majumder et al., 2019) with 180,000 recipes may be of more\nvalue; but all of these recipes come from a single source - viz. Food.com. We may have used it for our\npurpose. However, by the time the paper and the dataset were made public, we were already done with\nthe gathering and annotation of our dataset.\n2.2 Entities in a Cooking Recipe\nThe modiﬁed dataset provided by (Parvez et al., 2018) set the entity types as categories of ingredients\n- fruits, vegetables, proteins, etc. However, we have strong contentions against such categorisation. All\nof these entities are ingredients, and are likely to behave in similar way. We delve into this more in a\nfollowing section 2.3. In an instruction, generally ingredients are subjected to cooking processes inside\ncooking utensils in some quantity for a speciﬁc duration of time. Thus ingredients, cooking processes,\nutensils, quantity, and duration are the fundamental level of entities we encounter. This is also how\ncooking recipes have been modeled traditionally (Mori et al., 2014; Kiddon et al., 2015; Diwan et al.,\n2020).\nWe also note that the number of ingredient entities (537) considered by (Parvez et al., 2018) is very\nlow compared to the number of ingredients occurring in large scale databases such as RecipeDB (Batra\net al., 2019), FlavorDB (Garg et al., 2018) which record around 3 to 4 thousand ingredients clearly\npointing to incomplete entity recognition.\n2.3 Data collection and processing\nWe compiled 158,473 recipes from 8 websites and 70 geo-cultural cuisines, as detailed in Table 1. To\ncover all entities, we attempted to maximise the number of cuisines as ingredients, processes and utensils\nare often speciﬁc to certain cuisines. This greatly increased the number of entities.\nPre-processing ingredients: Diwan et al. (2020) argue that the Ingredients section may contains addi-\ntional information about the freshness, state, temperature and quantity. Since our focus is on generating\nrecipes from cooking ingredients, we ﬁlter out these extraneous qualiﬁers to keep only ingredient names\nso as to avoid confusing the model. To accomplish this, we used pretrained models made available by\nthe authors which accurately predict freshness, state, temperature and quantity with F1 scores of over\n0.95. This was partly motivated by the fact that the baseline model for our experiments (Majumder et\nal., 2019) also uses only ingredient names (and no other information about freshness, quantity, etc).\nIdentifying entities in instructions: For identifying the ﬁve entities in instructions section, we train\nStanford’s Named Entity Recogntion (NER) Tagger (Finkel et al., 2005) on a manually annotated corpus.\nThe training set comprised of 33,000 words. We tested it on a manually annotated test set containing\n4,236 words. We make the pre-trained models available to use with the evaluation summarised in Table 2.\nInterpreting Action Graphs Another possible use case of the language model could be data-to-\ntext, (Puduppully et al., 2019) or graph-to-text generation (Guo et al., 2019; Koncel-Kedziorski et al.,\n2019). As mentioned before, instructions in cooking recipes can be modeled of as action graphs. Each\naction (or event) is identiﬁed by a cooking process. These actions are applied upon entities such as in-\ngredients and utensils or a combination thereof (several ingredients processed and stored in a utensil).\nThe relationship is usually given by an edge between the process and these entities. Such action graphs\nclosely resemble Resource Description Framework (RDF) Graphs, which are an active research interest\nfor graph-to-text applications.\n29\nSource Number of Recipes\nAllRecipes 17508\nEpicurious 11165\nFood Network 15917\nFood.com 101992\nKraft Recipes 195\nMy Korean Kitchen 203\nTaste (Australia) 7637\nThe Spruce 3856\nTable 1: Breakdown of sources from where cook-\ning recipes were compiled\nEntity F1 score\nIngredients 0.944\nCooking Processes 0.912\nUtensils 0.945\nQuantity 0.962\nDuration 0.968\nTable 2: Evaluation of NER inference\nWe applied a semi-supervised method to interpret our recipes as action graphs, largely inspired by the\nexcellent work done by (Kiddon et al., 2015) with a few adjustments. A detailed explanation of the\nfull method which deals with several probabilistic models is out of the scope of this paper; we highly\nrecommend referring to the mentioned paper. Following are the changes we made to the method:\n1. The paper reports an F1 score of 95.6%, learnt and tested on a limited set of 2456 recipes, for iden-\ntiﬁcation of verbs which were used to deﬁne the actions. On our manually annotated test set for\nidentifying cooking process entities, the method achieves an F1 score of only 78.6%. We swapped\nthe dependency parser for a more recent Shift-Reduce parser in the Segmentation Model. All verbs\nwere identiﬁed using a POS tagger along with all words which were also tagged as cooking pro-\ncesses by our NER model. This raised the F1 score to 93.7%, signiﬁcantly higher than using only\nNER inference (91.2%) for Cooking Processes. These verbs were then used as processes identifying\nthe actions to form the action graphs.\n2. For identiﬁcation of ‘ingredient spans’ in the pre-processing phase, we rely upon the NER tags.\nSome of the missed occurrences of ingredient names were retrieved from the ingredient names\nretrieved from the preprocessed ingredients section, as described above, due to absence of evaluation\nmetrics for the heuristic-based method proposed in the above paper.\n3. While we do provide the action graphs obtained, as DOT (.gv) ﬁles, for all the recipes in the dataset\nto aid further research, the GCN based data-to-text model is trained sentence-by-sentence to match\nthe baseline. So we only make use of ‘partial’ action graphs - those obtained before the sequential\nconnections across consecutive sentences are joined. The action graphs and partial action graphs\nare demonstrated in the supplementary section.\n3 Language Model\nAs mentioned before, we build upon the work done by (Parvez et al., 2018) pertaining to the domain\nof building language models for natural language text containing high proportions of named entities.\nIntuitively, the reason why language models suffer for such text can be understood through the following\nobservations -\n•Named entities can be very high in number, with a small proportion of common English words.\n•The common words in English language follow set patterns with limited deviations. These patterns,\nformalised by the English grammar, do not change with training documents. Thus they are easy to\nmodel if we have a corpus of millions of words of naturally occurring text. Consider the case when\nthe documents on which the model is being trained upon are cooking recipes. Every document\nwould have different entities appearing in them. And the patterns would change too. Very few\nrecipes involve lining up a dough on a baking sheet and putting it in a preheated oven. Fewer\nrecipes would involve spreading a pizza dough on a grill.\n30\n•Thus, a few hundred entities can co-occur together in millions of different ways and complex rela-\ntionships. It can be seen that the problem arises from the fact that the text has a lot of named entities\nto learn about, but not sufﬁcient occurrences of these individual entities to successfully learn all the\npatterns from their context.\nA way to mitigate this problem would be to separate the learning of entity type (ex: fruit) speciﬁc\nknowledge, from entity (ex: apple) speciﬁc knowledge. In more intuitive words - all entities of the\nsame type behave in certain similar ways. While it may be uncommon for a recipe to include “heat\npizza dough on a grill”, if we were to label all ingredients, cooking process and utensils, a pattern like\n“COOKING PROCESS INGREDIENT on a UTENSIL” is far more likely to be found. Thus it would be\neasier for a language model to learn such combinations of patterns because of increased occurrences and\na decreased vocabulary since it doesn’t have to consider the thousands of individual tokens belonging to\nthese categories. (Parvez et al., 2018) introduced a “Type Model” to learn this behaviour.\nIn the subsequent operation, the model (or a separate one) can try to learn what speciﬁc entity should\nappear at a place. Given the context, it is easier to tell what cooking process or utensil or ingredient one\nshould use. Consider a recipe starting with ”COOKING PROCESS a UTENSIL before you begin”. A\nmodel can easily learn by seeing several examples that in such cases when a COOKING PROCESS is\napplied at the start of the instructions section, the most frequent process is “Preheat”. The input sentence\nto the language model now becomes ”Preheat a UTENSIL”. Strictly speaking, an oven would be a\nkitchen appliance, and not a utensil. But in the context of discussion, it has been deliberately marked\nas a utensil and is undoubtedly the best ﬁt here. (Parvez et al., 2018) introduced an “Entity Composite\nModel” to learn this behaviour. During inference, it relies on the output of the “Type Model”.\nFor simplicity, the trivial example above includes only three entities, we later explain our experiments\nwith the higher more number of entities and their effects. Also, only backward context was considered\nhere, which may indeed be the case when, say, a forward LSTM (Hochreiter and Schmidhuber, 1997)\nis used for learning the language model. However a bi-directional LSTM (Graves et al., 2005) allows\nmuch better performance, as also suggested by by (Parvez et al., 2018).\n3.1 Type Disentangled Language Models\nWe appreciate the improvement in perplexity achieved by learning the ‘entity type speciﬁc knowledge’\nand ‘entity speciﬁc knowledge’ separately. However, unlike (Parvez et al., 2018), we attempted to avoid\nlearning two different language models and took a joint inference for the following reasons\n1. Learning two different models from scratch for a single task is computationally expensive.\n2. Also, the entity composite model makes use of the output of type model to give the joint inference.\nSo, the parameters of type model are learnt separately and the loss function (used for learning the\ntype model) does not directly affect this joint inference in any way (which is the desired result to be\noptimised in the ﬁrst place).\n3. A two model framework would be difﬁcult to extend or interpret as a knowledge base in it-\nself (Petroni et al., 2019). Similarly, a rigid framework which requires two models with related\nvocabularies would be difﬁcult to tune for transfer-learning on a different corpus. We believe trans-\nfer learning could be helpful in this context, as a lot of tasks include corpora with named entities.\nRecently, transfer-learning to adapt pretrained representations for different tasks has been highly\nencouraged with the release of algorithms like ELMo (Peters et al., 2019) and benchmarks like\nGLUE (Wang et al., 2018).\nInspired by (V ondrick and Torralba, 2017), we see the entity composite model as a transformation for\npredicting the next word, using the context from previous tokens and the type of the next word being\npredicted (given by the type model). Since the entity composite model itself is differentiable, we decide\nto change the architecture into a single model like in (V ondrick and Torralba, 2017) which would allow\ntraining the whole architecture together.\n31\nWe also change the nomenclature to type prediction network (instead of type model) and entity trans-\nformation network (instead of entity composite model) to further reinforce the fact that we aren’t training\ntwo separate language models. Formally, the two networks work as follows:\nWe want to model the probability distribution of the next wordwwhen we know the previous context\n¯w:\nP(w|¯w; θt,θe) , (1)\nhere, θt and θe are the learnable parameters of the type prediction network and entity transformation\nnetwork respectively.\nMore accurately, the Type Prediction Network network models the probability distribution of types\ns(w) where wis the next word, given the context and type information of the previous words. Thus, the\noverall problem becomes the estimation of\nP(w,s(w)|¯w; θt,θe) . (2)\nIn our model, the prediction of w and s(w) is done by two separate networks. Thus, the problem\nsimpliﬁes as follows:\nP(s(w)|¯w; θt) ×P(w|¯w,s(w); θe) (3)\nWhereas the ﬁrst term in Equation 3 is approximated by the Type Prediction Network, the second term\nis approximated by the Entity Transformation Network.\nNOTE: This is in contrast to (Parvez et al., 2018) who approximate both of these terms differently\nin their “Type Model”. They treat the Type Model as a language model working on types, taking the\nentity types of the previous tokens as input to predict the entity type of the next word. They also use the\ntype of information of all the previous words to predict the next word in the Entity Composite Model\nwhile we only use the predicted type of the next word received from the Type Prediction Network. In\nour evaluations, this change greatly improves the performance of our network 3. The approximation for\nthe ﬁrst term used by (Parvez et al., 2018) is -\nP(s(w)|¯w,s( ¯w); θt,θv) ≈P(s(w)|s( ¯w),θt) (4)\nand that of the second term is -\nP(w|¯w,s( ¯w),s(w),θv) (5)\nwhere θt and θv are the parameters of their type and entity composite models respectively. Interestingly,\nboth the estimations would behave identically for all tokens which do not belong to a type (in most\ncorpora, normal English words). In that case both the methods use the approximation s(w) = w. This\nchange causes subtle difference in all the equations derived below.\nType Prediction Network: This network predicts the entity type of the next word. In case the next\nword does not belong to an entity, we output s(w) = w. It estimates the ﬁrst term in the equation 3.\nP(s(w)|¯w,s( ¯w); θt) ≈P(s(w)|¯w,θt) (6)\nStacked Hierarchical Type Prediction Network:To handle cases where entities may be hierarchi-\ncally classiﬁed into two or more levels, we experimented with a modiﬁcation of the Type Prediction\nNetworks. This will be helpful in dealing with entity types containing large number of entities. A recipe\nmay have entities like Ingredients and Utensils. Our corpus contains 3022 ingredients and 140 utensils.\nIngredients may further be divided into fruits, vegetables, proteins, etc. Fruits may further be divided\ninto sub-entity types and so on until we ﬁnally arrive at a named entity. This way, at each level of cat-\negorisation, the vocabulary of tokens to be considered is decreased. To accomplish this, we stack Type\nPrediction Models with different vocabularies one after the other to go down a level each time and ﬁnally\noutput an entity type at the lowest level. For a 3-layered Entity Hierarchy,\nP(s(w)|¯w; θt) ≈P(s1(w)|¯w,θt1 ) ×P(s2(w)|¯w,s1(w),θt2 ) ×P(s3(w)|¯w,s2(w),θt3 ) (7)\n32\nFigure 1: The inputs and outputs of the proposed networks during pretraining are illustrated. The two\nType Prediction networks together form the Stacked Hierarchical Type Prediction Network. During ﬁne-\ntuning and inference, the tokens are directly fed to the subsequent network as denoted by dotted lines.\nEntity Transformation Network: This network also uses RNN layers (in our experiments, we have\nutilised the QRNNs (Bradbury et al., 2017) ) to predict the next word, given the predicted type for this\nword s(w) and the context ¯w. It estimates the second term in the Equation 3.\nP(w|¯w,s(w); θe)∑\nws∈Ω(s(w)) P(ws|¯w,s( ¯w); θe), (8)\nwhere Ω(s(w)) is the set of words of the same type withw. Figure 1 graphically illustrates the networks.\n3.2 Training\nUnlike (Parvez et al., 2018), all networks use the same vocabulary. In the gathered corpus of cooking\nrecipes, we ﬁrst train both networks separately using Cross Entropy Loss on QRNNs so that the models\nlearn to give the right output individually. For this purpose, we provide datasets tagged with entity\ntype, subtype and ground truth. We ﬁnally ﬁne-tune the whole architecture. For this, we follow the\napproach used by (Xu et al., 2019); training one sub-network at a time at a low learning rate and\nfreezing the other sub-networks. This is done for many iterations. The performance gain is signiﬁcant, as\ndocumented in Table 3. In all experiments involving the proposed dataset, a uniform training-validation-\ntest split was used (126,778 recipes in training set and 15,847 recipes in both validation and test sets).\nFor reproducibility, we have provided more details in the appendix.\n33\nTraining Phase Ppl.\nPretrained Type Prediction N/W 5.20\nPretrained Subtype Prediction N/W 1.34\nPretrained Entity Prediction N/W 1.44\nFine-tuned End-to-end Model 9.25\nTable 3: Test perplexity while training the pro-\nposed stacked hierarchical type model on our\ndataset. Two instances of type prediction models\nwere stacked. The highest level entities are In-\ngredients, Utensils, Cooking Processes, Duration,\nand Quantity. Ingredients and Utensils are further\nclassiﬁed into 22 and 3 different sub types respec-\ntively.\nModel Dataset Number of Perplexity\nNamed Entities\nParvez et al (Baseline) Parvez et al 537 9.67\nOur model Parvez et al 537 9.22\nParvez et al ours 3608 14.89\nOur model ours 3608 9.25\nwith hierarchical types\nTable 4: Comparison of the performance of our\nlanguage model with the baseline. All the results\nare on the test set of the corresponding corpus.\nLower perplexity is better.\nFigure 2: Architecture for the model used for ingredient-to-text generation (Experiment 4.1).\n3.3 Evaluation\nSince our work heavily builds upon (Parvez et al., 2018), we treat it as the baseline. We evaluate both\nmodels on both the datasets - the one provided by them as well as the one we procure. While evaluating\non their dataset, we use the entity types (8 superingredients) they provide. On our dataset, we compare\nit with a model trained on 28 entity types (22 categories of ingredients, 3 categories of utensils, cooking\nprocesses, quantity and duration). No hierarchical types were included since their architecture does not\nsupport hierarchical entity classiﬁcation. The results have been summarised in Table 4. To the best of\nour knowledge, there are no other language models which aim to improve the perplexity for text with\nnamed entities.\n4 Experiments\n4.1 Ingredient to Recipe Generation\nWe apply our language model on the practical task of generating cooking recipes from given ingredients.\nThis task is covered by (Kiddon et al., 2016), (Majumder et al., 2019) and (Salvador et al., 2019).\nThe latter two papers point out that their models, which use user review history and recipe images for\ngenerating the recipe (and not just ingredients), result in a smaller perplexity than simple encoder decoder\nframeworks. Because they require these additional information, we cannot use these models as baselines.\nMajumdar et al. (2019) do not use (Kiddon et al., 2016) as their baseline because an Encoder Decoder\nby (Cho et al., 2014) framework, incorporating attention, achieves a similar performance at a much lower\ncomplexity. Due to this, we have used the baseline (GRU encoder + GRU decoder) provided by them.\nWe modify our architecture, as illustrated in Figure 2, to use the same ingredient GRU encoder as\nthe baseline, followed by the pretrained Type Prediction and Entity Transformation Networks. We then\nincorporate Bahdanau attention (Bahdanau et al., 2015) to add the embeddings from the encoder as an\nadditional weighted input to both the Type Prediction and Entity Transformation Networks and ﬁnally\nﬁne-tune the whole architecture for an ingredient-to-recipe generator. For simplicity, we only used a\n34\nFigure 3: Architecture for the model used for action-graph-to-text generation (Experiment 4.2).\nModel Dataset Number of Perplexity\n(Recipe Corpus) Recipes\nBaseline (EncDec) Ours 158k 10.09\nGRU Encoder Ours 158k 8.99\nwith our model\nPersonalised Recipe FOOD.com 180k 9.52\nImage to Recipe Recipe1M 1M 8.66\nTable 5: Comparison of the performance of\ningredient-to-recipe generation task.\nModel BLEU Score\nSeq-to-seq baseline 43.79\nLSTM EncDec\nGCN Encoder 62.14\nLSTM Decoder\nGCN Encoder 63.57\nOur Decoder\nTable 6: Comparison of the performance of graph\nto recipe generation task. Higher BLEU score is\nbetter.\nsingle Type Prediction Network with the 28 entity types as explained before. This task was chosen as\nit allows us to demonstrate the Transfer Learning ability of our Language Model. Both networks were\nindividually pretrained and have been applied for a different task with some adjustments. As evident from\nTable 5, our model clearly outperforms the baseline on our dataset. We could not incorporate (Salvador\net al., 2019) and (Majumder et al., 2019) on our dataset because of lack of images and user reviews in\nour corpus, but have listed them for fair comparison. We attribute the lower perplexity by (Salvador et\nal., 2019) to the much larger corpus size and clues from images..\n4.2 Action Graph to Recipe Step Generation\nThis experiment builds upon ingredient-to-text generation to explore the problem of data to text gener-\nation. We use the partial Action Graphs obtained during dataset preparation and parse them into XML\nformat, similar to the source dependency graphs from SR11Deep generation task (Belz et al., 2011) and\nthe RDF graphs from WebNLG Task. Each partial Action Graph has a recipe step associated with it. We\nnow attempt to predict the correct recipe step given a partial Action Graph. Due to high training times\neven on a P100 GPU, we could only conduct this experiment using action graphs from 56,676 recipes.\nUnlike other experiments in the manuscript, action graphs of instructions from 51,280 recipes were used\nas training set, 2,698 each as validation and test sets.\nThe proposed model borrows the Graph Convolution Encoder from (Marcheggiani and Perez-\nBeltrachini, 2018), we use the same encoder as the one used in the SR11 task but to delexicalise the\nnamed entities in the input sentence instead of treating multi-word entities as being composed of sep-\narate nodes. During training and inference these are relexicalised to get the same embeddings as the\npretrained on the corpus. The architecture is illustrated in Figure 3.\nWe apply the same training procedure by using pretrained networks on the dataset. Again, a soft\nattention is applied over the GCN induced representation and fed into the Type Prediction and Entity\nTransformation Networks. We use the same baseline as them, a seq-to-seq LSTM encoder decoder\nframework, which takes a linear version of the source graph as input. We also compare our model with\ntheir proposed GCN Encoder + LSTM Decoder model. Although our architecture outperforms both\nthe baselines, as detailed in Table 6, it’s only a marginal improvement when compared with the GCN\nEncoder. Nevertheless, it shows the use of these pretrained models in transfer learning on different tasks.\n35\n4.3 Recipe Generation\nThe language model was used to generate recipes given the ﬁrst sentence of a genuine cooking recipe\nusing beam search with a beam width of 15. We generated 107 such cooking recipes. We collected\nanother 108 genuine recipes which were then tokenised and converted into strings of space separated\ntokens, such that they were indistinguishable from the unformatted generated recipes output by the lan-\nguage model. This set was then shufﬂed and distributed among 13 adults with varied cooking experience.\nThey were told about the composition of the set, that it is evenly comprised of generated and genuine\nrecipes, and were asked to predict which recipes were generated and which were genuine. Out of the 107\ngenerated recipes, 60 were marked correctly as having been generated while 47 were incorrectly marked\nas genuine. Similarly of the 108 genuine recipes, 32 were marked incorrectly as having been generated\nwhile 76 were correctly marked as genuine. We can see that when the recipes are seen as a whole, it\nwas possible to distinguish them from the fact 76 genuine recipes were marked as genuine but only 47\ngenerated recipes were marked as genuine. However, this does not evaluate the model well as a single\ngiveaway is sufﬁcient to identify a fake recipe, as evident from the examples in appendix. We further\nasked the respondents to give a score (from 0 to 10) to the quality of generated text for all the recipes\nthey believed were generated. They were asked to give at least of 4 minutes in making their decisions.\nInterestingly, genuine recipes identiﬁed as generated recipes received an average score of 6.46 but gen-\nerated recipes were close behind with an average score of 6.43. This shows that the quality of text being\ngenerated is of approximately the same quality as that found on online sources.\nExample of Cooking Recipe Generation using Beam Search\nPreheat oven to 350 degrees F. In a large skillet, heat oil over medium heat heat . Add chicken\nand cook until brown on all sides, about 5 minutes per side. Transfer chicken to a baking dish and\nkeep warm in oven oven. Heat oil in the same skillet over medium high heat. Add onion and cook\nuntil translucent, about 3 minutes. Add garlic and cook, stirring, until fragrant, about 1 minute.\nAdd broth and bring to a boil. Add chicken, reduce heat to medium , and simmer until chicken is\ncooked through, about 10 minutes. Transfer chicken to a serving dish and cover with foil to keep\nwarm. Bring to a boil, reduce to a simmer and cook until thickened, about 5 minutes. Season with\nsalt and pepper to taste. Spoon sauce over chicken.\n5 Conclusion\nThe Hierarchically Disentangled Language Models proposed in this manuscript clearly are an incremen-\ntal improvement over (Parvez et al., 2018), across the three settings we evaluated upon.\nFirst, we evaluated it intrinsically in terms of perplexity on two datasets on which it out-performs the\nprimary baseline. We also show that the original dataset with lower number of distinct named entities\nis comparatively easier for both the models. Whereas, the original baseline performed poorly over the\nnewly procured, more complex dataset. This highlights the importance of ﬁne-tuning the subnetworks\nas well as simplifying the input to Entity Transformation network – since knowledge about the previous\ntypes is already present within the RNN, explicity including it is not only redundant, but also makes the\nmodeling task more complex. Secondly, we used our Language Model as a decoder in an ingredient-\nto-recipe generator. Here it got lower perplexity than a baseline encoder-decoder model. Finally, we\nused the language model as the decoder in an action graph-to-recipe model. In this setting the generated\nrecipes had higher BLEU compared with reference recipes than the same model using an LSTM decoder.\nThe latter two experiments together provide the evidence to the utility of such language models by\nTransfer Learning, in more complex tasks.\nAn issue with the proposed language models is the annotation of named entities, which is domain\nspeciﬁc and hard to automate. We also did not see much improvement by increasing levels of entity hier-\narchy, which only made the overall annotation tasks more complex. We encourage future contributions\nin further testing such Language Models in different text domains and improve upon our training strategy\nand architecture - whether speciﬁc to certain domains or for general purpose language modeling.\n36\nReferences\nKenneth Arnold, Kai-Wei Chang, and Adam Kalai. 2017. Counterfactual language model adaptation for sug-\ngesting phrases. In Proceedings of the Eighth International Joint Conference on Natural Language Processing\n(Volume 2: Short Papers) , pages 49–54, Taipei, Taiwan, November. Asian Federation of Natural Language\nProcessing.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning\nto align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\nDevansh Batra, Nirav Diwan, Utkarsh Upadhyay, Jushaan Singh Kalra, Tript Sharma, Aman Kumar Sharma,\nDheeraj Khanna, Jaspreet Singh Marwah, Srilakshmi Kalathil, Navjot Singh, et al. 2019. Recipedb: A resource\nfor exploring recipes. Available at SSRN 3482237.\nAnja Belz, Mike White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The ﬁrst sur-\nface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop\non Natural Language Generation , pages 217–226, Nancy, France, September. Association for Computational\nLinguistics.\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. 2017. Quasi-Recurrent Neural Networks.\nInternational Conference on Learning Representations (ICLR 2017).\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine\ntranslation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1724–1734, Doha, Qatar, October. Association for Computational Linguistics.\nErol Cromwell, Jonah Galeota-Sprung, and Raghuram Ramanujan. 2015. Computational creativity in the culinary\narts.\nRobert Dale. 2019. Generating recipes: An overview of epicure. In Christopher Mellish Robert Dale and Michael\nZock, editors, Current Research in Natural Language Generation. Academic Press, London.\nNirav Diwan, Devansh Batra, and Ganesh Bagler. 2020. A named entity based approach to model recipes. In\n2020 IEEE 36th International Conference on Data Engineering Workshops (ICDEW), pages 88–93. IEEE.\nJenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into\ninformation extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL’05), pages 363–370, Ann Arbor, Michigan, June. Association for\nComputational Linguistics.\nNeelansh Garg, Apuroop Sethupathy, Rudraksh Tuwani, Rakhi Nk, Shubham Dokania, Arvind Iyer, Ayushi Gupta,\nShubhra Agrawal, Navjot Singh, Shubham Shukla, et al. 2018. Flavordb: a database of ﬂavor molecules.\nNucleic acids research, 46(D1):D1210–D1216.\nAlex Graves, Santiago Fern ´andez, and J ¨urgen Schmidhuber. 2005. Bidirectional lstm networks for improved\nphoneme classiﬁcation and recognition. In Proceedings of the 15th International Conference on Artiﬁcial Neu-\nral Networks: Formal Models and Their Applications - Volume Part II , ICANN’05, page 799–804, Berlin,\nHeidelberg. Springer-Verlag.\nZhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for\ngraph-to-sequence learning. Transactions of the Association for Computational Linguistics, 7:297–312, March.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks the best choice for model-\ning source code? In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering ,\nESEC/FSE 2017, page 763–773, New York, NY , USA. Association for Computing Machinery.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. 2016. On the naturalness of\nsoftware. Commun. ACM, 59(5):122–131, April.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–\n1780.\nChlo´e Kiddon, Ganesa Thandavam Ponnuraj, Luke Zettlemoyer, and Yejin Choi. 2015. Mise en place: Unsu-\npervised interpretation of instructional recipes. In Proceedings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pages 982–992, Lisbon, Portugal, September. Association for Computational\nLinguistics.\n37\nChlo´e Kiddon, Luke Zettlemoyer, and Yejin Choi. 2016. Globally coherent text generation with neural checklist\nmodels. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages\n329–339, Austin, Texas, November. Association for Computational Linguistics.\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Gener-\nation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers) , pages 2284–2293, Minneapolis, Minnesota, June. Association for Computational\nLinguistics.\nBodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian McAuley. 2019. Generating personalized\nrecipes from historical user preferences. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5976–5982, Hong Kong, China, November. Association for Computational Linguis-\ntics.\nDiego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to\ntext generation. In Proceedings of the 11th International Conference on Natural Language Generation , pages\n1–9, Tilburg University, The Netherlands, November. Association for Computational Linguistics.\nJavier Marin, Aritro Biswas, Ferda Oﬂi, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Anto-\nnio Torralba. 2019. Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food\nimages. IEEE Trans. Pattern Anal. Mach. Intell.\nShinsuke Mori, Hirokuni Maeta, Yoko Yamakata, and Tetsuro Sasada. 2014. Flow graph corpus from recipe texts.\nIn Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages\n2370–2377, Reykjavik, Iceland, May. European Language Resources Association (ELRA).\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2018. Building language models for\ntext with named entities. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2373–2383, Melbourne, Australia, July. Association for Computational\nLinguistics.\nMatthew E. Peters, Sebastian Ruder, and Noah A. Smith. 2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP\n(RepL4NLP-2019), pages 7–14, Florence, Italy, August. Association for Computational Linguistics.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander\nMiller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November. Association for Computational\nLinguistics.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with entity modeling. In Proceed-\nings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2023–2035, Florence,\nItaly, July. Association for Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract syntax networks for code generation and\nsemantic parsing. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1139–1149, Vancouver, Canada, July. Association for Computational Linguis-\ntics.\nA. Salvador, N. Hynes, Y . Aytar, J. Marin, F. Oﬂi, I. Weber, and A. Torralba. 2017. Learning cross-modal\nembeddings for cooking recipes and food images. In 2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3068–3076.\nAmaia Salvador, Michal Drozdzal, Xavier Giro-i Nieto, and Adriana Romero. 2019. Inverse cooking: Recipe\ngeneration from food images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune.\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. 2017. A hybrid convolutional variational autoen-\ncoder for text generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, pages 627–637, Copenhagen, Denmark, September. Association for Computational Linguistics.\nC. V ondrick and A. Torralba. 2017. Generating the future with adversarial transformers. In2017 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages 2992–3000.\n38\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brus-\nsels, Belgium, November. Association for Computational Linguistics.\nBolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a dual task of code summarization. In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 32, pages 6563–6573. Curran Associates, Inc.\nSam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2017. Challenges in data-to-document generation.\nCoRR, abs/1707.08052.\nKai Xu, Longyin Wen, Guorong Li, Liefeng Bo, and Qingming Huang. 2019. Spatiotemporal cnn for video object\nsegmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.\nPengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. ArXiv,\nabs/1704.01696.\nB. Zhu, C. Ngo, J. Chen, and Y . Hao. 2019. Rgan: Cross-modal recipe retrieval with generative adversarial\nnetwork. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11469–\n11478.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8724309206008911
    },
    {
      "name": "Natural language processing",
      "score": 0.7611569166183472
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7307115793228149
    },
    {
      "name": "Parsing",
      "score": 0.6955709457397461
    },
    {
      "name": "Language model",
      "score": 0.6901890635490417
    },
    {
      "name": "Dependency grammar",
      "score": 0.6506153345108032
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5857605934143066
    },
    {
      "name": "Vocabulary",
      "score": 0.5782158374786377
    },
    {
      "name": "Topic model",
      "score": 0.5419036746025085
    },
    {
      "name": "Entity linking",
      "score": 0.5290008187294006
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.5270830988883972
    },
    {
      "name": "Graph",
      "score": 0.437588095664978
    },
    {
      "name": "Question answering",
      "score": 0.43031707406044006
    },
    {
      "name": "Information retrieval",
      "score": 0.3912787437438965
    },
    {
      "name": "Task (project management)",
      "score": 0.14505350589752197
    },
    {
      "name": "Programming language",
      "score": 0.13421416282653809
    },
    {
      "name": "Theoretical computer science",
      "score": 0.10619956254959106
    },
    {
      "name": "Linguistics",
      "score": 0.08372500538825989
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Knowledge base",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I68891433",
      "name": "Indian Institute of Technology Delhi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I119939252",
      "name": "Indraprastha Institute of Information Technology Delhi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I36090812",
      "name": "Netaji Subhas University of Technology",
      "country": "IN"
    }
  ],
  "cited_by": 5
}