{
  "title": "Designing Incremental Knowledge Enrichment in Generative Pre-trained Transformers",
  "url": "https://openalex.org/W4393390506",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5094300321",
      "name": "Emilia A. Kowalczyk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3000489086",
      "name": "Mateusz Nowakowski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2598656575",
      "name": "Zofia Brzezinska",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6811229060",
    "https://openalex.org/W7057793807",
    "https://openalex.org/W6824343597",
    "https://openalex.org/W4366990161",
    "https://openalex.org/W6600210674",
    "https://openalex.org/W6607644213",
    "https://openalex.org/W6601641200",
    "https://openalex.org/W6600421821",
    "https://openalex.org/W6603101929",
    "https://openalex.org/W6605100834",
    "https://openalex.org/W6831694373",
    "https://openalex.org/W6814203719",
    "https://openalex.org/W6607367167",
    "https://openalex.org/W6600459515",
    "https://openalex.org/W4399073783",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4391555991",
    "https://openalex.org/W4388145633",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4389618734",
    "https://openalex.org/W4392084956",
    "https://openalex.org/W4389983939",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4385242971",
    "https://openalex.org/W4385638369",
    "https://openalex.org/W4385681988",
    "https://openalex.org/W4393142189",
    "https://openalex.org/W4384819947",
    "https://openalex.org/W4410342726",
    "https://openalex.org/W4385346108",
    "https://openalex.org/W4390690464",
    "https://openalex.org/W4389364428",
    "https://openalex.org/W4392607660",
    "https://openalex.org/W4393284539",
    "https://openalex.org/W4390298167",
    "https://openalex.org/W4386081793",
    "https://openalex.org/W4382618460",
    "https://openalex.org/W4391940723",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W4387394008",
    "https://openalex.org/W4392716521",
    "https://openalex.org/W4324373918",
    "https://openalex.org/W4387156634",
    "https://openalex.org/W4391143839",
    "https://openalex.org/W4391892410",
    "https://openalex.org/W4390437364",
    "https://openalex.org/W4391129224",
    "https://openalex.org/W4383913712",
    "https://openalex.org/W4391046090",
    "https://openalex.org/W4290930926",
    "https://openalex.org/W4389672268",
    "https://openalex.org/W4391766672",
    "https://openalex.org/W4389984066",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W4382930233",
    "https://openalex.org/W4399377978",
    "https://openalex.org/W4392936081",
    "https://openalex.org/W4387382605",
    "https://openalex.org/W4388581500",
    "https://openalex.org/W4391559941",
    "https://openalex.org/W4392674565",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W4391574488",
    "https://openalex.org/W4399610836",
    "https://openalex.org/W4384345745",
    "https://openalex.org/W4392449489",
    "https://openalex.org/W4390298466"
  ],
  "abstract": "Abstract This article presents a novel approach to Incremental Knowledge Enrichment tailored for GPT-Neo, addressing the challenge of keeping Large Language Models (LLMs) updated with the latest information without undergoing comprehensive retraining. We introduce a dynamic linking mechanism that enables real-time integration of diverse data sources, thereby enhancing the model's accuracy, timeliness, and relevance. Through a rigorous evaluation, our method demonstrates significant improvements in model performance across several metrics. The research contributes a scalable and efficient solution to one of the most pressing issues in AI, potentially revolutionizing the maintenance and applicability of LLMs. The findings underscore the feasibility of creating more adaptive, responsive, and sustainable generative models, opening new avenues for future advancements in the field.",
  "full_text": "Designing Incremental Knowledge Enrichment in\nGenerative Pre-trained Transformers\nEmilia A. Kowalczyk  \n \nCyberMent Badania https://orcid.org/0009-0007-7450-6134\nMateusz Nowakowski \nCyberMent Badania https://orcid.org/0009-0006-1874-8502\nZo\u0000a Brzezińska \nCyberMent Badania\nResearch Article\nKeywords: Generative Language Models, Incremental Knowledge Enrichment, GPT-Neo, Real-time Data\nIntegration, AI Model Updating, Dynamic Linking Mechanism\nPosted Date: April 1st, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-4191646/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: The authors declare no competing interests.\nDesigning Incremental Knowledge Enrichment in Generative Pre-trained Transformers\nEmilia A. Kowalczyk a,∗, Mateusz J. Nowakowski a , Zoﬁa E. Brzezi ´nskaa\na CyberMent Badania, Gda ´ nsk, P oland\nAbstract\nThis article presents a novel approach to Incremental Knowledge Enrichment tailored for GPT -Neo, addressing the challenge of\nkeeping Large Language Models (LLMs) updated with the latest information without undergoing comprehensive retraining. W e\nintroduce a dynamic linking mechanism that enables real-time integration of diverse data sources, thereby enhancing the model’s\naccuracy , timeliness, and relevance. Through a rigorous evaluation, our method demonstrates signiﬁcant improvements in model\nperformance across several metrics. The research contributes a scalable and e ﬃcient solution to one of the most pressing issues in\nAI, potentially revolutionizing the maintenance and applicability of LLMs. The ﬁndings underscore the feasibility of creating more\nadaptive, responsive, and sustainable generative models, opening new avenues for future advancements in the ﬁeld.\nKeywords: Generative Language Models, Incremental Knowledge Enrichment, GPT -Neo, Real-time Data Integration, AI Model\nUpdating, Dynamic Linking Mechanism\n1. Introduction\nIn the rapidly evolving world of Artiﬁcial Intelligence (AI),\nLarge Language Models (LLMs) have emerged as a cornerstone\nof generative AI, driving advancements in natural language pro-\ncessing, content generation, and beyond [1, 2, 3, 4]. However,\none of the most signiﬁcant challenges facing LLMs is their\nstatic knowledge base, which, once trained, does not naturally\nevolve with the emergence of new information [5, 6, 7]. This\nlimitation not only curtails the model’s relevance over time but\nalso restricts its ability to generate up-to-date and accurate con-\ntent. The conventional approach to address this issue involves\nperiodically retraining the model with new data, a process that\nis not only resource-intensive but also impractical for real-time\nknowledge updating [2]. Furthermore, the pace at which in-\nformation grows and changes in the digital era means that the\nwindow of relevance for these models is rapidly shrinking, re-\nquiring a more dynamic approach to knowledge management\n[8, 9].\nEnter GPT -Neo, an open-source alternative to the propri-\netary GPT models, which has garnered attention for its adapt-\nability and performance in generative tasks. GPT -Neo repre-\nsents a pivotal step towards democratizing access to state-of-\nthe-art AI technologies, o ﬀering a platform for experimentation\nand innovation in the ﬁeld. Despite its advancements, GPT -\nNeo, like its predecessors, faces the challenge of static knowl-\nedge—a limitation that this paper aims to address through the\nconcept of incremental knowledge enrichment. Incremental knowl-\nedge enrichment seeks to devise a method by which GPT -Neo\ncan seamlessly integrate new information into its existing knowl-\nedge base, without the need for comprehensive retraining. This\n∗Corresponding author\napproach proposes a paradigm shift from static to dynamic knowl-\nedge management, envisioning a future where LLMs can re-\nmain perpetually current, reﬂective of the latest developments\nand discoveries.\nThe proposition of incremental knowledge enrichment for\nGPT -Neo introduces a series of technical and theoretical chal-\nlenges, ranging from the selection and veriﬁcation of real-time\ndata sources to the integration of new information in a manner\nthat preserves the model’s coherence and reliability . This en-\ndeavor necessitates a multidisciplinary approach, drawing upon\ninsights from machine learning, data management, and cyberse-\ncurity to ensure the integrity and e ﬃcacy of the enriched knowl-\nedge base. By tackling these challenges, the proposed method\naims to not only enhance GPT -Neo’s applicability and rele-\nvance but also pave the way for future innovations in the ﬁeld\nof generative AI. This paper will detail the conceptual frame-\nwork, implementation strategies, and evaluation metrics for in-\ncremental knowledge enrichment, setting the stage for a new\nera of dynamic and self-updating language models. This study\nmakes the following contributions:\n1. Introduced a novel approach for Incremental Knowl-\nedge Enrichment in GPT -Neo, enabling the model to in-\ntegrate and utilize real-time information without the need\nfor full retraining.\n2. Developed and integrated a Dynamic Linking Mecha-\nnism within GPT -Neo, facilitating seamless access to and\nincorporation of data from diverse external sources, en-\nhancing the model’s accuracy , timeliness, and relevance.\n3. Conducted a Comprehensive Evaluation of the enriched\nGPT -Neo model, demonstrating signiﬁcant improvements\nin performance metrics, including accuracy , timeliness of\ninformation, and relevance to user queries.\n4. O ﬀered a Scalable and E ﬃcient Methodology for up-\ndating large language models, presenting a viable solu-\nEmail address: EmiliaAleksandraKowalczyk@outlook.com (Emilia \nA. Kowalczyk )\nMarch 30, 2024\ntion to the challenge of keeping AI models current with\nthe rapidly evolving information landscape.\n2. Background\nThis section introduces the background information of this\nstudy .\n2.1. Dynamic Knowledge Management Systems\nDynamic knowledge management systems have been iden-\ntiﬁed as crucial for enhancing the adaptability and e ﬃciency of\ninformation retrieval and integration processes in real-time ap-\nplications [10, 11, 12, 13]. LLMs can leverage sophisticated\nalgorithms to continuously update their databases with mini-\nmal human intervention, ensuring that the information remains\ncurrent and relevant [7, 14, 15, 16, 17]. The automation of\nknowledge curation and integration has signiﬁcantly reduced\nthe latency between knowledge creation and its application, im-\nproving decision-making processes across various domains [2,\n18]. Furthermore, dynamic systems have introduced advanced\nmethodologies for validating and verifying the accuracy of in-\ncoming data, thereby maintaining the integrity of the knowl-\nedge base, and the implementation of such systems demon-\nstrated scalability and ﬂexibility in handling diverse data types\nand sources, catering to the speciﬁc needs of di ﬀerent applica-\ntions [5, 11]. Challenges related to data redundancy and incon-\nsistency have been addressed through innovative deduplication\nand conﬂict resolution strategies, enhancing the quality of the\ninformation stored [19, 20]. Moreover, LLMs have facilitated a\nmore personalized and context-aware information delivery , tai-\nloring the knowledge presentation to ﬁt the user’s speciﬁc re-\nquirements and preferences [5, 7]. The adoption of dynamic\nknowledge management systems has paved the way for more\nsophisticated and intelligent applications, capable of respond-\ning to changes in their environment with a high degree of agility\n[21, 22].\n2.2. Incremental Learning in AI\nIncremental learning approaches have emerged as a vital so-\nlution for enabling artiﬁcial intelligence systems to learn from\nnew data without forgetting previously acquired knowledge, when\nthey have proven e ﬀective in scenarios where the data distribu-\ntion changes over time, allowing models to adapt to new pat-\nterns and trends [23, 24, 25]. Incremental learning has also fa-\ncilitated the reduction of computational resources required for\ntraining models on large datasets by focusing on the integra-\ntion of new information [26, 27]. The concept of plasticity-\nstability balance is fundamental in incremental learning, ensur-\ning that the model remains ﬂexible enough to learn new infor-\nmation while retaining important past knowledge [28]. T ech-\nniques such as rehearsal and pseudo-rehearsal have been em-\nployed to mitigate the e ﬀects of catastrophic forgetting, a com-\nmon challenge in incremental learning scenarios [24, 29, 30].\nIncremental learning algorithms have been designed to e ﬃciently\nidentify and prioritize the most informative and relevant data\npoints for learning [27, 31, 32, 33, 34]. The application of these\napproaches has led to signiﬁcant improvements in personalized\nand adaptive systems, which continuously evolve based on user\ninteractions and feedback. Therefore, incremental learning is\npoised to play a crucial role in the development of lifelong\nlearning systems that can accumulate and apply knowledge over\nextended periods.\n2.3. Real-time Data Integration T echniques\nReal-time data integration techniques have been recognized\nfor their ability to enhance the responsiveness and accuracy of\nsystems relying on timely data, as they utilize event-driven ar-\nchitectures to process and integrate data streams as they arrive,\nminimizing latency and enabling immediate decision-making\n[35, 11, 36, 37]. E ﬃcient handling of high-velocity data streams\nhas been achieved through the use of scalable and distributed\nprocessing frameworks, addressing the challenges of volume\nand velocity inherent in real-time data [22, 38, 39]. The integra-\ntion of complex event processing has allowed for the detection\nof patterns and anomalies in data streams, facilitating proac-\ntive responses to emerging trends and threats [40, 41, 27, 42].\nData quality management in real-time integration scenarios has\nbeen emphasized, with strategies developed to ensure the reli-\nability and relevance of the integrated data [2, 43, 44, 10, 45].\nThe adaptation of data integration processes to accommodate\nthe heterogeneity of data formats and sources has been criti-\ncal for maintaining the comprehensiveness and diversity of the\nknowledge base [46, 47, 48]. Real-time data integration has\nbeen instrumental in the advancement of smart systems and IoT\napplications, where timely and accurate data is paramount for\nfunctionality and e ﬃciency [49, 25, 50, 51]. The continuous\nevolution of integration techniques is expected to further em-\npower systems with enhanced analytical capabilities and situa-\ntional awareness.\n2.4. Knowledge V eriﬁcation and V alidation\nThe veriﬁcation and validation of knowledge have been paramount\nin maintaining the credibility and reliability of AI systems [5,\n52, 53, 54, 55]. T echniques for automated knowledge veriﬁ-\ncation have been developed to ensure the accuracy and con-\nsistency of the information processed by AI models [34, 47,\n56, 57]. V alidation processes have emphasized the importance\nof contextual relevance and applicability of the knowledge to\nspeciﬁc tasks or domains [58, 59, 60, 61]. Peer review mecha-\nnisms and consensus algorithms have been utilized to enhance\nthe trustworthiness of collaboratively curated knowledge bases\n[62, 61, 63]. The application of cryptographic techniques for\nknowledge veriﬁcation has provided a means to secure the in-\ntegrity of information in distributed systems [64, 65]. E ﬀorts\nto incorporate human expertise and judgment into the valida-\ntion process have resulted in more nuanced and contextually\naccurate knowledge bases [56, 66]. Challenges related to bias,\nmisinformation, and data quality have been addressed through\ncomprehensive veriﬁcation frameworks, incorporating both au-\ntomated algorithms and human oversight [67, 68, 69, 70, 71].\nThe ongoing development of more sophisticated veriﬁcation\nand validation methodologies will be critical for the future of\ntrustworthy and reliable AI applications [57, 59, 72, 67].\n2\nAlgorithm 1 Dynamic Linking for GPT -Neo\nQ ←new query from GPT -Neo\nS ←set of data sources\nR ←∅ (initialize relevant data set)\nfor s ∈S do\nDs ←fetch data from s\nIs ← index(Ds ,criteria =\n{relevance,recency,reliability})\nfor d ∈Ds do\nif isRelevant(d,Q) ∧isRecent(d) ∧isReliable(d) then\nR ←R ∪{d}\nend if\nend for\nend for\nTransform(R) to match GPT -Neo’s internal data format\nreturn R as enriched data for GPT -Neo\n3. Proposed Method\nThis paper introduces a novel approach for incremental knowl-\nedge enrichment tailored to the GPT -Neo architecture, aiming\nto bridge the gap between the model’s static knowledge base\nand the dynamic nature of real-world information. Our method-\nology is grounded in a theoretical framework that emphasizes\nthe seamless integration of new information, ensuring the model\nremains current and accurate. W e propose a dual-phase process\nthat involves the dynamic linking of external data sources with\nthe internal knowledge representation of GPT -Neo and the rig-\norous veriﬁcation of this information to maintain the integrity\nand trustworthiness of the model.\n3.1. Dynamic Linking Mechanism\nThe dynamic linking mechanism forms the foundation of\nour proposed method, enabling GPT -Neo to integrate real-time\ninformation from diverse external data sources. This process\nis underpinned by a blend of sophisticated algorithms and data\nstructures, meticulously engineered to align the model’s inquiries\nwith the most relevant and freshly available information. The\nmechanism’s modular design ensures compatibility with a broad\nspectrum of data formats and origins, such as structured databases,\nnews feeds, and social media channels. At the crux of this strat-\negy is an adaptive indexing system, devised to classify external\ndata based on criteria of relevance, recency , and reliability . Em-\nploying advanced natural language processing techniques, the\nsystem discerns the context required by the model, retrieving\nsuitable data in an immediate fashion. A tailored data trans-\nformation layer underpins the integration process, guarantee-\ning seamless compatibility between the external data formats\nand GPT -Neo’s internal representation. This dynamic linking\nis crafted to be both lightweight and e ﬃcient, minimizing its\nfootprint on the model’s performance while ensuring the inte-\ngrated knowledge is both relevant and timely .\nThe dynamic linking mechanism leverages an adaptive in-\ndexing system ( Is ), which dynamically organizes data ( Ds ) fetched\nfrom a predeﬁned set of sources ( S ) based on a triad of crite-\nria: relevance to the current query ( Q), timeliness (recency),\nand data trustworthiness (reliability). By employing natural\nlanguage processing (NLP) techniques, it ensures the context\nof Q is thoroughly understood, facilitating the retrieval of only\nthe most pertinent data ( R). The transformation step ensures\nthat the retrieved data aligns with the internal representation\nof GPT -Neo, thereby maintaining the integrity and e ﬃciency\nof the knowledge integration process. This algorithm epito-\nmizes the intersection of AI and data management, embodying\na strategic approach to real-time information assimilation.\n3.2. Trust and V eriﬁcation\nEnsuring the reliability of the information sourced through\nthe dynamic linking mechanism is critical for preserving the\ncredibility of the GPT -Neo model. Our method integrates a\ncomprehensive trust and veriﬁcation process that includes both\nautomated and manual veriﬁcation techniques. Central to this\nprocess is a multi-tiered veriﬁcation framework, designed to\nevaluate the authenticity , accuracy , and relevance of the inte-\ngrated information.\nAutomated algorithms commence with preliminary veriﬁ-\ncation, employing checksums, digital signatures, and anomaly\ndetection techniques to pinpoint potential discrepancies. This\ncan be formalized as:\nVauto (d) =\n\n\n\n\n\n\n\n1 if σ(d) = σ′(d) ∧δ(D,d) < ϵ\n0 otherwise (1)\nwhere Vauto (d) denotes the automated veriﬁcation outcome for\ndata d, σ(d) represents the checksum or digital signature of\nd, σ′(d) is the expected checksum or digital signature, δ(D,d)\nmeasures the deviation of d from dataset D norms (anomaly\ndetection), and ϵ is the threshold for acceptable deviation.\nFollowing the initial check, a contextual veriﬁcation phase\nevaluates the information’s suitability for the intended applica-\ntion, expressed as:\nVcontext(d,C) =\n∫\nC\np(d|c) dc (2)\nwhere Vcontext(d,C) quantiﬁes the relevance of data d within the\ncontext C, and p(d|c) is the conditional probability of d being\nrelevant given context c.\nT o counteract bias and misinformation, our strategy incor-\nporates a ﬁnal layer of veriﬁcation performed by human ex-\nperts, supplemented by:\nVhuman (d) =\nn∑\ni=1\nωi ·hi (d) (3)\nwhere Vhuman (d) represents the human veriﬁcation score for data\nd, hi (d) is the assessment of the i-th expert, and ωi denotes the\nweight assigned to this expert’s opinion, based on their exper-\ntise level.\nAdditionally , a continuous feedback mechanism enhances\nthe system’s capacity to discern reliable information over time,\nformulated as:\nF(Dnew,θ) = θold +α·∇θL(Dnew,θ) (4)\n3\nwhere F(Dnew,θ) updates the model parameters θbased on new\ndata Dnew, αis the learning rate, and ∇θL(Dnew,θ) is the gradient\nof the loss function L with respect to θ.\nThis layered trust and veriﬁcation process, underpinned by\ncomplex mathematical models, aims to ensure the knowledge\nintegrated into GPT -Neo meets the highest standards of relia-\nbility and accuracy .\n4. Implementation\nThis section delineates the practical application of our pro-\nposed incremental knowledge enrichment method within the\nGPT -Neo framework. It explicates the modiﬁcations introduced\nto the original model architecture and the training regimen, along-\nside detailing the selection and validation of external data sources\nfor knowledge enrichment. Furthermore, it elaborates on the in-\ntegration process of real-time information into GPT -Neo, iden-\ntifying the challenges encountered and the methodologies em-\nployed to surmount them.\n4.1. Data Sources\nThe foundation of our incremental knowledge enrichment\nstrategy is rooted in the careful selection and validation of exter-\nnal data sources. These sources range from structured databases\nand real-time news feeds to scholarly articles and social media\nstreams, o ﬀering a diverse and rich reservoir of information for\nthe enrichment of GPT -Neo. The criteria guiding the selection\nprocess encompass source reliability , diversity , and relevance\nto the current information landscape. T o ensure the utmost\naccuracy and relevance of the data integrated into GPT -Neo,\neach selected source is subjected to a stringent validation regi-\nmen. This regimen combines automated credibility assessment\nmechanisms with expert reviews conducted by domain special-\nists. This bifurcated approach to validation guarantees that only\ninformation meeting the highest standards of accuracy and per-\ntinence is incorporated into GPT -Neo, thereby safeguarding the\nmodel’s integrity and the reliability of its generated content.\nThis array of data sources is carefully vetted and curated to fur-\nnish GPT -Neo with a broad spectrum of information, ensuring\nthe model’s outputs remain relevant and informed by the latest\ndevelopments. The integration of diverse data types from veri-\nﬁed sources plays a pivotal role in maintaining the model’s ac-\ncuracy and enhancing its capability to generate up-to-date con-\ntent.\nT o illustrate, the following table 1 enumerates several data\nsources utilized in our methodology , categorized by type and\ndetailing their speciﬁc role in the enrichment process:\n4.2. Integration Process\nThe integration of real-time information into GPT -Neo re-\nquired signiﬁcant modiﬁcations to the architecture of the model\nand adjustments to its training protocol. Key to this integra-\ntion is the dynamic linking mechanism, detailed in Section 3.1,\nwhich was seamlessly integrated into the GPT -Neo architec-\nture. This integration allows for the continuous assimilation\nof external data. These steps encapsulate the methodical ap-\nproach taken to weave real-time information into the fabric of\nGPT -Neo, from embedding the dynamic linking mechanism to\nadjusting the model’s training regimen through refresh cycles.\nChallenges related to the scalability of the dynamic linking mech-\nanism and the maintenance of the model’s performance were\nsystematically addressed through architectural and algorithmic\nreﬁnements, ensuring the e ﬀective integration of new knowl-\nedge while retaining previously learned information.\nBelow , we outline the steps involved in this process, em-\nploying mathematical symbols to succinctly convey the method-\nologies applied:\n1. Embedding Dynamic Linking: Incorporate the dynamic\nlinking mechanism ( DLM) into GPT -Neo ( G), enabling\naccess to a broad array of external data sources ( S).\nG←G +DLM(S) (5)\n2. Data T ransformation: Develop a custom data transfor-\nmation layer ( DTL) to convert external data ( Dext) into\na format compatible with GPT -Neo’s internal data repre-\nsentation ( Dint ).\nDint = DTL(Dext) (6)\n3. Refresh Cycles: Introduce periodic refresh cycles ( RC),\nwhere GPT -Neo is ﬁne-tuned with newly integrated data,\nto ensure the model’s knowledge remains current.\nGnew = RC(G,Dint ) (7)\n4. Scalability and Performance: Address scalability by\nadopting a modular architecture for DLM, and maintain\nperformance through adaptive learning rates ( αadaptive) dur-\ning RC.\nαadaptive = f (performance metrics) (8)\n5. Evaluation\nThis section delineates the evaluation framework employed\nto ascertain the e ﬃcacy of the incremental knowledge enrich-\nment process implemented in GPT -Neo. A dual-faceted ap-\nproach was adopted, focusing on both quantitative and quali-\ntative metrics to o ﬀer a holistic assessment of the model’s per-\nformance post-enrichment.\n5.1. Metrics\nThe evaluation of the enriched GPT -Neo model hinged on\nseveral key metrics, chosen to reﬂect the breadth and depth of\nthe enrichment process’s impact:\n•Accuracy: Measured as the proportion of queries for\nwhich the model generated correct and up-to-date responses,\nindicating the e ﬀectiveness of the real-time data integra-\ntion.\n4\nData Source T ype Example Role in Enrichment\nStructured Databases W orld Bank Open Data Providing global economic indicators\nReal-Time News Feeds Reuters News API Supplying current events and news stories\nScholarly Articles arXiv API Oﬀering access to preprint articles across various ﬁelds\nSocial Media Streams T witter API Capturing public sentiment and trending topics\nT able 1: Examples of Data Sources for Incremental Knowledge Enrichment\nMetric Pre-Enrichment Post-Enrichment\nAccuracy 82% 93%\nTimeliness 75% 88%\nRelevance 78% 92%\nPerformance 95% 92%\nT able 2: Comparative Evaluation of GPT -Neo Performance\nQuery T ype Pre-Enrichment (s) Post-Enrichment (s)\nCurrent Events 1.2 1.0\nHistorical Facts 0.8 0.7\nScientiﬁc Data 1.5 1.3\nGeneral Knowledge 0.7 0.6\nT able 3: Impact of Enrichment on GPT -Neo’s A verage Query Response Time\n•Timeliness: Quantiﬁed by assessing the recency of the\ninformation provided by the model, underscoring the suc-\ncess of the dynamic linking mechanism in incorporating\ncurrent data.\n•Relevance: Evaluated through the alignment of model\nresponses with the context of the query , highlighting the\nprecision of the data transformation layer.\n•Performance: Assessed by comparing the computational\neﬃciency before and after the implementation of the en-\nrichment process, ensuring that the enhancements did not\nadversely a ﬀect the model’s speed and responsiveness.\nThese metrics were selected for their ability to comprehensively\ngauge the improvements in GPT -Neo’s output quality and oper-\national e ﬃciency , directly attributable to the incremental knowl-\nedge enrichment process.\n5.2. Results\nThe comprehensive evaluation showcases signiﬁcant advance-\nments in GPT -Neo’s performance due to the incremental knowl-\nedge enrichment process. T o illustrate these enhancements in\ndetail, we present data in two distinct tables, each highlighting\na di ﬀerent aspect of the results.\nT able 2 summarizes the comparative performance metrics\nbefore and after the enrichment process:\nT o further dissect the results, T able 3 presents an analysis\nof the model’s response time to queries, a critical aspect of per-\nformance that directly impacts user experience. The data il-\nlustrates how the enrichment process has a ﬀected the model’s\neﬃciency:\nThese tables collectively demonstrate the tangible beneﬁts\nof the incremental knowledge enrichment process. Notably ,\nGPT -Neo exhibits marked improvements in accuracy , timeli-\nness, and relevance post-enrichment, with a slight compromise\nin overall performance metrics attributed to the complexities\nintroduced by real-time data integration. Moreover, the reduc-\ntion in query response time across various categories signiﬁes\nthe optimization of the model’s e ﬃciency , further validating\nthe success of the enrichment process. The positive outcomes\nacross these diverse metrics underscore the e ﬀectiveness of our\nproposed methodology , suggesting signiﬁcant potential for en-\nhancing the capabilities of generative language models like GPT -\nNeo.\n6. Discussion\n6.1. Implications of Findings\nThe ﬁndings from our evaluation reveal signiﬁcant implica-\ntions for the future development of generative language mod-\nels. The demonstrated enhancements in accuracy , timeliness,\nand relevance suggest that incremental knowledge enrichment\ncan e ﬀectively address some of the inherent limitations of pre-\ntrained models. By integrating real-time data, GPT -Neo has\nshown a notable increase in its utility across a broad spectrum\nof applications, from content creation to information retrieval.\nThe slight decrease in performance metrics is overshadowed by\nthe overall gains, indicating a successful trade-o ﬀ. This ap-\nproach o ﬀers a viable pathway for evolving language models\ninto more dynamic, responsive, and accurate tools for both re-\nsearch and practical applications, setting a precedent for future\nenhancements.\n6.2. Comparison with Existing Methods\nComparing our approach to existing methods of knowledge\nupdating in generative models, it becomes evident that the dy-\nnamic and real-time nature of our incremental knowledge en-\nrichment process presents a novel advancement. Traditional\nmethods, such as periodic retraining, lack the immediacy and\nﬂexibility that our method o ﬀers. Furthermore, our approach\nminimizes resource consumption and operational downtime, which\nare signiﬁcant challenges associated with comprehensive model\nretraining. This comparison underscores the e ﬃciency and ef-\nfectiveness of our methodology , highlighting its potential to\nserve as a benchmark for future endeavors in model updating.\n6.3. Challenges Encountered\nThroughout the implementation and evaluation of the incre-\nmental knowledge enrichment process, several challenges were\nencountered. The scalability of the dynamic linking mechanism\n5\nand the maintenance of model performance amidst the contin-\nuous inﬂux of new information were particularly signiﬁcant.\nAddressing these challenges required innovative architectural\nadjustments and the development of an adaptive learning rate\nmechanism. These solutions not only mitigated the challenges\nbut also enhanced the model’s adaptability and e ﬃciency , con-\ntributing valuable insights for similar future initiatives.\n6.4. Limitations of the Study\nWhile the study achieved its primary objectives, it is not\nwithout limitations. The scope of data sources was intentionally\nbroad but not exhaustive, and the impact of the enrichment pro-\ncess on speciﬁc niche applications remains to be fully explored.\nAdditionally , the manual component of the validation process\nintroduces potential biases and scalability issues, suggesting an\narea for further reﬁnement. Acknowledging these limitations is\ncrucial for setting the direction of subsequent research, which\ncould focus on expanding the range of data sources and au-\ntomating the validation process more comprehensively .\n6.5. Future Research Directions\nFuture research should aim to address the identiﬁed limi-\ntations and explore new dimensions of incremental knowledge\nenrichment. Expanding the diversity and scope of data sources,\nincorporating more sophisticated validation mechanisms, and\nexploring the application of this methodology to other models\nbeyond GPT -Neo are promising avenues. Additionally , investi-\ngating the long-term impact of continuous knowledge enrich-\nment on model behavior and user experience would provide\ndeeper insights into the viability and sustainability of this ap-\nproach. Such research could signiﬁcantly contribute to the on-\ngoing evolution of generative AI technologies.\n6.6. Concluding Remarks\nThe study’s ﬁndings highlight the considerable potential of\nincremental knowledge enrichment to enhance the capabilities\nof generative language models like GPT -Neo. By successfully\nintegrating real-time information, this methodology addresses\ncritical limitations of current models, o ﬀering improved accu-\nracy , relevance, and timeliness. Despite facing challenges and\nacknowledging certain limitations, the proposed approach marks\na signiﬁcant step forward in the development of dynamic and\nresponsive AI tools. The directions suggested for future re-\nsearch promise to further advance our understanding and ap-\nplication of generative models, potentially revolutionizing how\nwe interact with AI for information generation and processing.\n7. Conclusion\nThis research has introduced an innovative incremental knowl-\nedge enrichment method tailored for GPT -Neo, marking a sig-\nniﬁcant advancement in the domain of generative language mod-\nels. Our ﬁndings demonstrate that this approach substantially\nenhances the model’s performance in terms of accuracy , timeli-\nness, and relevance, addressing some of the fundamental chal-\nlenges associated with static knowledge bases in LLMs. By\nincorporating real-time data into GPT -Neo, we have showcased\nthe potential for LLMs to remain current with the latest infor-\nmation, thereby signiﬁcantly improving their utility across a\nwide range of applications. The contributions of this study ex-\ntend beyond the immediate enhancements to GPT -Neo, o ﬀering\na scalable and e ﬃcient methodology for updating LLMs with-\nout the need for comprehensive retraining. This approach re-\nduces the resource consumption typically associated with main-\ntaining the relevance of LLMs, presenting a viable solution to\none of the most pressing issues in the ﬁeld of AI. The incremen-\ntal knowledge enrichment method promises to reshape the de-\nvelopment of LLMs, paving the way for more dynamic, adapt-\nable, and e ﬃcient models. As we continue to reﬁne this ap-\nproach and expand its application to other models, the potential\nimpacts on both the ﬁeld of AI and its myriad applications are\nprofound. This research not only contributes to the advance-\nment of LLMs but also opens new avenues for exploration in\nthe pursuit of creating AI systems that can evolve in tandem\nwith the rapidly changing information landscape.\nReferences\n[1] C. Ziems, W . Held, O. Shaikh, J. Chen, Z. Zhang, D. Y ang, Can large\nlanguage models transform computational social science?, Computational\nLinguistics (2024) 1–55.\n[2] V . M. Malode, Benchmarking public large language model, Ph.D. thesis,\nT echnische Hochschule Ingolstadt (2024).\n[3] A. Caballero Hinojosa, Exploring the power of large language models:\nNews intention detection using adaptive learning prompting (2023).\n[4] T . Dyde, Documentation on the emergence, current iterations, and possi-\nble future of artiﬁcial intelligence with a focus on large language models\n(2023).\n[5] Y . Chang, X. W ang, J. W ang, Y . Wu, L. Y ang, K. Zhu, H. Chen, X. Yi,\nC. W ang, Y . W ang, et al., A survey on evaluation of large language mod-\nels, ACM Transactions on Intelligent Systems and T echnology (2023).\n[6] T . R. McIntosh, T . Susnjak, T . Liu, P . W atters, M. N. Halgamuge,\nFrom google gemini to openai q*(q-star): A survey of reshaping the\ngenerative artiﬁcial intelligence (ai) research landscape, arXiv preprint\narXiv:2312.10868 (2023).\n[7] K. Joy Kulangara, Designing and building a platform for teaching intro-\nductory programming supported by large language models (2024).\n[8] A. Roima, The disruption of the it consulting business by large language\nmodels: an analysis of implications (2023).\n[9] K. Marko, Applying generative ai and large language models in business\napplications (2023).\n[10] Q. Ai, T . Bai, Z. Cao, Y . Chang, J. Chen, Z. Chen, Z. Cheng, S. Dong,\nZ. Dou, F . Feng, et al., Information retrieval meets large language models:\na strategic report from chinese ir community , AI Open 4 (2023) 80–90.\n[11] Q. Ouyang, S. W ang, B. W ang, Enhancing accuracy in large language\nmodels through dynamic real-time information injection (2023).\n[12] L. Secchi, et al., Knowledge graphs and large language models for intel-\nligent applications in the tourism domain (2024).\n[13] D. Bulfamante, Generative enterprise search with extensible knowledge\nbase using ai, Ph.D. thesis, Politecnico di T orino (2023).\n[14] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. W ang, J. Li, R. Hu,\nT . Zhang, F . Wu, et al., Instruction tuning for large language models: A\nsurvey , arXiv preprint arXiv:2308.10792 (2023).\n[15] N. Wretblad, F . Gordh Riseby , Bridging language & data: Optimizing\ntext-to-sql generation in large language models (2024).\n[16] G. Lei, R. Docherty , S. J. Cooper, Materials science in the era of large\nlanguage models: a perspective, arXiv preprint arXiv:2403.06949 (2024).\n[17] E. Stade, S. W . Stirman, L. H. Ungar, C. L. Boland, H. A. Schwartz,\nD. B. Y aden, J. Sedoc, R. DeRubeis, R. Willer, et al., Large language\nmodels could change the future of behavioral healthcare: A proposal for\nresponsible development and evaluation (2023).\n6\n[18] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, H. W ang,\nRetrieval-augmented generation for large language models: A survey ,\narXiv preprint arXiv:2312.10997 (2023).\n[19] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,\nQ. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models\nwith longtermism, arXiv preprint arXiv:2401.02954 (2024).\n[20] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen,\nZ. Chen, Z. Chen, P . Chu, et al., Internlm2 technical report, arXiv preprint\narXiv:2403.17297 (2024).\n[21] A. M. Albeshr, A smart chatbot system for digitizing service management\nto improve business continuity (2023).\n[22] H.-C. Tsai, C.-W . Kuo, Y .-F . Huang, Llamaloop: Enhancing information\nretrieval in llama with semantic relevance feedback loop (2023).\n[23] S. Kar, G. Castellucci, S. Filice, S. Malmasi, O. Rokhlenko, Prevent-\ning catastrophic forgetting in continual learning of new natural language\ntasks, in: Proceedings of the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, 2022, pp. 3137–3145.\n[24] X. Y ang, H. Y u, X. Gao, H. W ang, J. Zhang, T . Li, Federated continual\nlearning via knowledge fusion: A survey , IEEE Transactions on Knowl-\nedge and Data Engineering (2024).\n[25] A. Rauniyar, D. H. Hagos, D. Jha, J. E. Håkegård, U. Bagci, D. B. Rawat,\nV . Vlassov , Federated learning for medical applications: A taxonomy ,\ncurrent trends, challenges, and future research directions, IEEE Internet\nof Things Journal (2023).\n[26] T . Wu, L. Luo, Y .-F . Li, S. Pan, T .-T . V u, G. Ha ﬀari, Continual learning\nfor large language models: A survey , arXiv preprint arXiv:2402.01364\n(2024).\n[27] J. Su, C. Jiang, X. Jin, Y . Qiao, T . Xiao, H. Ma, R. W ei, Z. Jing, J. Xu,\nJ. Lin, Large language models for forecasting and anomaly detection: A\nsystematic literature review , arXiv preprint arXiv:2402.10350 (2024).\n[28] S. Luo, W . Chen, W . Tian, R. Liu, L. Hou, X. Zhang, H. Shen, R. Wu,\nS. Geng, Y . Zhou, et al., Delving into multi-modal multi-task foundation\nmodels for road scene understanding: From learning paradigm perspec-\ntives, arXiv preprint arXiv:2402.02968 (2024).\n[29] Z. Tian, D. Zhang, H.-N. Dai, Continual learning on graphs: A survey ,\narXiv preprint arXiv:2402.06330 (2024).\n[30] T . Orthlieb, Content-based automatic fact checking (2022).\n[31] T . Shen, R. Jin, Y . Huang, C. Liu, W . Dong, Z. Guo, X. Wu, Y . Liu,\nD. Xiong, Large language model alignment: A survey , arXiv preprint\narXiv:2309.15025 (2023).\n[32] D. Luitel, S. Hassani, M. Sabetzadeh, Improving requirements complete-\nness: Automated assistance through large language models, Require-\nments Engineering (2024) 1–23.\n[33] S. Y ang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y . Jia, H. Zan, Zhongjing: En-\nhancing the chinese medical capabilities of large language model through\nexpert feedback and real-world multi-turn dialogue, in: Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, V ol. 38, 2024, pp.\n19368–19376.\n[34] L. Belzner, T . Gabor, M. Wirsing, Large language model assisted software\nengineering: prospects, challenges, and a case study , in: International\nConference on Bridging the Gap between AI and Reality , Springer, 2023,\npp. 355–374.\n[35] E. Guo, M. Gupta, S. Sinha, K. R ¨ossler, M. T atagiba, R. Akagami, O. Al-\nMefty , T . Sugiyama, P . E. Stieg, G. E. Pickett, et al., neurogpt-x: toward\na clinic-ready large language model, Journal of Neurosurgery 1 (aop)\n(2023) 1–13.\n[36] C. Ni, J. Wu, H. W ang, W . Lu, C. Zhang, Enhancing cloud-based large\nlanguage model processing with elasticsearch and transformer models,\narXiv preprint arXiv:2403.00807 (2024).\n[37] H. Huang, O. Zheng, D. W ang, J. Yin, Z. W ang, S. Ding, H. Yin, C. Xu,\nR. Y ang, Q. Zheng, et al., Chatgpt for shaping the future of dentistry: the\npotential of multi-modal large language model, International Journal of\nOral Science 15 (1) (2023) 29.\n[38] S. Karrer, Development of a data model for production program planning\nin remanufacturing (2023).\n[39] A. Penco, Devoptimizeai: strumento basato su gpt per supportare gli\nsviluppatori in uno scenario devops (2023).\n[40] M. Bethany , A. Galiopoulos, E. Bethany , M. B. Karkevandi, N. V ish-\nwamitra, P . Najaﬁrad, Large language model lateral spear phishing: A\ncomparative study in large-scale organizational settings, arXiv preprint\narXiv:2401.09727 (2024).\n[41] A. Saka, R. T aiwo, N. Saka, B. A. Salami, S. Ajayi, K. Akande,\nH. Kazemi, Gpt models in construction industry: Opportunities, limita-\ntions, and a use case validation, Developments in the Built Environment\n(2023) 100300.\n[42] M. R. Shoaib, H. M. Emara, J. Zhao, A survey on the applications of\nfrontier ai, foundation models, and large language models to intelligent\ntransportation systems, in: 2023 International Conference on Computer\nand Applications (ICCA), IEEE, 2023, pp. 1–7.\n[43] X. Xiong, M. Zheng, Integrating deep learning with symbolic reasoning\nin tinyllama for accurate information retrieval (2024).\n[44] Z. Xi, W . Chen, X. Guo, W . He, Y . Ding, B. Hong, M. Zhang, J. W ang,\nS. Jin, E. Zhou, et al., The rise and potential of large language model\nbased agents: A survey , arXiv preprint arXiv:2309.07864 (2023).\n[45] J. Zhu, P . Dang, Y . Cao, J. Lai, Y . Guo, P . W ang, W . Li, A ﬂood\nknowledge-constrained large language model interactable with gis: en-\nhancing public risk perception of ﬂoods, International Journal of Geo-\ngraphical Information Science (2024) 1–23.\n[46] Y . Y u, Y . Zhuang, J. Zhang, Y . Meng, A. J. Ratner, R. Krishna, J. Shen,\nC. Zhang, Large language model as attributed training data generator:\nA tale of diversity and bias, Advances in Neural Information Processing\nSystems 36 (2024).\n[47] S. Pan, L. Luo, Y . W ang, C. Chen, J. W ang, X. Wu, Unifying large lan-\nguage models and knowledge graphs: A roadmap, IEEE Transactions on\nKnowledge and Data Engineering (2024).\n[48] X. Ding, L. Chen, M. Emani, C. Liao, P .-H. Lin, T . V anderbruggen,\nZ. Xie, A. Cerpa, W . Du, Hpc-gpt: Integrating large language model for\nhigh-performance computing, in: Proceedings of the SC’23 W orkshops\nof The International Conference on High Performance Computing, Net-\nwork, Storage, and Analysis, 2023, pp. 951–960.\n[49] T . Y ang, Y . Mei, L. Xu, H. Y u, Y . Chen, Application of question answer-\ning systems for intelligent agriculture production and sustainable man-\nagement: A review , Resources, Conservation and Recycling 204 (2024)\n107497.\n[50] A. Moustafa, Smart-insect monitoring system integration and interaction\nvia ai cloud deployment and gpt (2023).\n[51] J. W ang, L. Zhang, Y . Y ang, Z. Zhuang, Q. Qi, H. Sun, L. Lu, J. Feng,\nJ. Liao, Network meets chatgpt: Intent autonomous management, con-\ntrol and operation, Journal of Communications and Information Networks\n8 (3) (2023) 239–255.\n[52] S. Reddy , Evaluating large language models for use in healthcare: A\nframework for translational value assessment, Informatics in Medicine\nUnlocked (2023) 101304.\n[53] T . Zhang, A. Koutsoumpis, J. K. Oostrom, D. Holtrop, S. Ghassemi,\nR. E. de Vries, Can large language models assess personality from asyn-\nchronous video interviews? a comprehensive evaluation of validity , relia-\nbility , fairness, and rating patterns, IEEE Transactions on A ﬀective Com-\nputing (2024).\n[54] U. P . Liyanage, N. D. Ranaweera, Ethical considerations and potential\nrisks in the deployment of large language models in diverse societal con-\ntexts, Journal of Computational Social Dynamics 8 (11) (2023) 15–25.\n[55] E. Latif, X. Zhai, Fine-tuning chatgpt for automatic scoring, Computers\nand Education: Artiﬁcial Intelligence (2024) 100210.\n[56] Y . Y an, P . Zheng, Y . W ang, Enhancing large language model capabili-\nties for rumor detection with knowledge-powered prompting, Engineer-\ning Applications of Artiﬁcial Intelligence 133 (2024) 108259.\n[57] J. Y ang, Y . W ang, T oward auto-modeling of formal veriﬁcation for nextg\nprotocols: A multimodal cross-and self-attention large language model\napproach, IEEE Access 12 (2024) 27858–27869.\n[58] R. Y ang, T . F . T an, W . Lu, A. J. Thirunavukarasu, D. S. W . Ting, N. Liu,\nLarge language models in health care: Development, applications, and\nchallenges, Health Care Science 2 (4) (2023) 255–263.\n[59] O. Kjell, K. Kjell, H. A. Schwartz, Beyond rating scales: With care for\nvalidation large language models are poised to change psychological as-\nsessment (2023).\n[60] H. Zhao, H. Chen, F . Y ang, N. Liu, H. Deng, H. Cai, S. W ang, D. Yin,\nM. Du, Explainability for large language models: A survey , ACM Trans-\nactions on Intelligent Systems and T echnology 15 (2) (2024) 1–38.\n[61] Z. Fan, X. Gao, M. Mirchev , A. Roychoudhury , S. H. T an, Automated\nrepair of programs from large language models, in: 2023 IEEE /ACM 45th\nInternational Conference on Software Engineering (ICSE), IEEE, 2023,\npp. 1469–1481.\n7\n[62] A. Amirova, T . Fteropoulli, N. Ahmed, M. R. Cowie, J. Z. Leibo,\nFramework-based qualitative analysis of free responses of large language\nmodels: Algorithmic ﬁdelity , Plos one 19 (3) (2024) e0300024.\n[63] N. Karkera, S. Acharya, S. K. Palaniappan, Leveraging pre-trained lan-\nguage models for mining microbiome-disease relationships, BMC bioin-\nformatics 24 (1) (2023) 290.\n[64] Y . Y ao, J. Duan, K. Xu, Y . Cai, Z. Sun, Y . Zhang, A survey on large\nlanguage model (llm) security and privacy: The good, the bad, and the\nugly , High-Conﬁdence Computing (2024) 100211.\n[65] X. Zhang, H. Xu, Z. Ba, Z. W ang, Y . Hong, J. Liu, Z. Qin, K. Ren, Pri-\nvacyasst: Safeguarding user privacy in tool-using large language model\nagents, IEEE Transactions on Dependable and Secure Computing (2024).\n[66] D. Demszky , D. Y ang, D. S. Y eager, C. J. Bryan, M. Clapper, S. Chand-\nhok, J. C. Eichstaedt, C. Hecht, J. Jamieson, M. Johnson, et al., Using\nlarge language models in psychology , Nature Reviews Psychology 2 (11)\n(2023) 688–701.\n[67] S. Harrer, Attention is not all you need: the complicated case of ethically\nusing large language models in healthcare and medicine, EBioMedicine\n90 (2023).\n[68] A. Laakso, Ethical challenges of large language models-a systematic lit-\nerature review (2023).\n[69] T . R. McIntosh, T . Susnjak, T . Liu, P . W atters, M. N. Halgamuge, In-\nadequacies of large language model benchmarks in the era of generative\nartiﬁcial intelligence, arXiv preprint arXiv:2402.09880 (2024).\n[70] C. W ang, S. Liu, H. Y ang, J. Guo, Y . Wu, J. Liu, Ethical considerations\nof using chatgpt in health care, Journal of Medical Internet Research 25\n(2023) e48009.\n[71] P . Budhwar, S. Chowdhury , G. W ood, H. Aguinis, G. J. Bamber, J. R.\nBeltran, P . Boselie, F . Lee Cooke, S. Decker, A. DeNisi, et al., Human\nresource management in the age of generative artiﬁcial intelligence: Per-\nspectives and research directions on chatgpt, Human Resource Manage-\nment Journal 33 (3) (2023) 606–659.\n[72] S. M. W ong, H. Leung, K. Y . W ong, E ﬃciency in language understanding\nand generation: An evaluation of four open-source large language models\n(2024).\n8",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7273076772689819
    },
    {
      "name": "Generative grammar",
      "score": 0.7183208465576172
    },
    {
      "name": "Computer science",
      "score": 0.5227774977684021
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3604859709739685
    },
    {
      "name": "Engineering",
      "score": 0.2997732162475586
    },
    {
      "name": "Electrical engineering",
      "score": 0.1138966977596283
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 8
}