{
  "title": "Enriching Rare Word Representations in Neural Language Models by Embedding Matrix Augmentation",
  "url": "https://openalex.org/W2925513271",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5047310084",
      "name": "Yerbolat Khassanov",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5034445480",
      "name": "Zhiping Zeng",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5046222772",
      "name": "Van Tung Pham",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5039696635",
      "name": "Haihua Xu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5070872826",
      "name": "Eng Siong Chng",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2058695628",
    "https://openalex.org/W1734538896",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2802422770",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2621404689",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2949563612",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2951553268",
    "https://openalex.org/W2125076245",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W179875071",
    "https://openalex.org/W101286142",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2610242619",
    "https://openalex.org/W2053306448",
    "https://openalex.org/W2040711288",
    "https://openalex.org/W2167419393",
    "https://openalex.org/W4246054309",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2590420008",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W1503259811",
    "https://openalex.org/W2142377809",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W4249773576",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W4386506836"
  ],
  "abstract": "The neural language models (NLM) achieve strong generalization capability by\\nlearning the dense representation of words and using them to estimate\\nprobability distribution function. However, learning the representation of rare\\nwords is a challenging problem causing the NLM to produce unreliable\\nprobability estimates. To address this problem, we propose a method to enrich\\nrepresentations of rare words in pre-trained NLM and consequently improve its\\nprobability estimation performance. The proposed method augments the word\\nembedding matrices of pre-trained NLM while keeping other parameters unchanged.\\nSpecifically, our method updates the embedding vectors of rare words using\\nembedding vectors of other semantically and syntactically similar words. To\\nevaluate the proposed method, we enrich the rare street names in the\\npre-trained NLM and use it to rescore 100-best hypotheses output from the\\nSingapore English speech recognition system. The enriched NLM reduces the word\\nerror rate by 6% relative and improves the recognition accuracy of the rare\\nwords by 16% absolute as compared to the baseline NLM.\\n",
  "full_text": "Enriching Rare Word Representations in Neural Language Models\nby Embedding Matrix Augmentation\nYerbolat Khassanov1, Zhiping Zeng2, Van Tung Pham1,2, Haihua Xu2, Eng Siong Chng1,2\n1School of Computer Science and Engineering, Nanyang Technological University, Singapore\n2Temasek Laboratories, Nanyang Technological University, Singapore\n{yerbolat002,zengzp,vantung001,haihuaxu,aseschng}@ntu.edu.sg\nAbstract\nThe neural language models (NLM) achieve strong generaliza-\ntion capability by learning the dense representation of words\nand using them to estimate probability distribution function.\nHowever, learning the representation of rare words is a chal-\nlenging problem causing the NLM to produce unreliable proba-\nbility estimates. To address this problem, we propose a method\nto enrich representations of rare words in pre-trained NLM and\nconsequently improve its probability estimation performance.\nThe proposed method augments the word embedding matrices\nof pre-trained NLM while keeping other parameters unchanged.\nSpeciﬁcally, our method updates the embedding vectors of rare\nwords using embedding vectors of other semantically and syn-\ntactically similar words. To evaluate the proposed method, we\nenrich the rare street names in the pre-trained NLM and use it\nto rescore 100-best hypotheses output from the Singapore En-\nglish speech recognition system. The enriched NLM reduces\nthe word error rate by 6% relative and improves the recognition\naccuracy of the rare words by 16% absolute as compared to the\nbaseline NLM.\nIndex Terms: rare words, word embeddings, neural language\nmodels, speech recognition\n1. Introduction\nThe neural language models (NLM) have achieved great suc-\ncess in many speech and language processing applications [1,\n2, 3]. Particularly, it is highly employed in automatic speech\nrecognition (ASR) systems to rescore the n-best hypotheses list\nwhere the state-of-the-art results are attained. Different from\nthe traditional count-based N-gram models that suffer from the\ndata sparsity problem [4], the NLMs possess superior gener-\nalization capability. The generalization is mainly achieved by\nlearning dense vector representations of words as part of the\ntraining process and using them to express the probability func-\ntion [1]. As a result, the learned word representations capture\ndifferences and commonalities between words, and thus enable\nNLMs to model different combination of words including the\nones unseen during the training.\nHowever, this concept assumes that each word appears a\nsufﬁcient amount of times in the training data. For rare words\nwith little or no training samples, the learned representations\nwill be poor [5]. Consequently, the NLM will assign them unre-\nliable probability estimates. Moreover, the representation of the\nrare word will be used as a context for the neighbouring words,\nas such, the entire word sequence containing the rare word will\nbe underestimated. The problem exacerbates when a rare word\nis a named entity such as names of persons, locations, organiza-\ntions and so on which are important keywords for downstream\ntasks such as voice search [6].\nCurrently, a common practice in language modeling is to\nignore the rare word problem. For example, by limiting the\nNLM’s vocabulary set to the most frequent words and treat\nthe remaining words as out-of-vocabulary (OOV), i.e. map-\nping them to special <unk> token [7] or train the NLM with\nfull vocabulary as usual. The former approach conﬂates all\nthe meanings of rare words into a single representation, los-\ning the properties of individual words. The latter approach will\nresult in low-quality rare word representations. For both ap-\nproaches, the probability estimates of hypotheses incorporating\nthe rare words will be unreliable, leading to the sub-optimal\nperformance of NLM.\nIn this work, we propose an efﬁcient method to enrich the\nvector representations of rare words in pre-trained NLMs. The\nproposed method augments the word embedding matrices of\npre-trained NLM while keeping other parameters unchanged.\nSpeciﬁcally, our method shifts the rare word representation to-\nwards its semantic landmark in the embedding space using\nrepresentations of other semantically and syntactically similar\nwords. This method has been shown effective for word sim-\nilarity task [8] and vocabulary expansion for NLMs [9]. We\nfurther extend its application to the rare word representation\nenrichment. To evaluate the proposed method, we ﬁrst enrich\nthe representations of rare Singapore street names in pre-trained\nNLM and then use it to rescore the 100-best hypotheses output\nfrom the state-of-the-art Singapore English ASR system. The\nenriched NLM reduces the word error rate by 6% relative and\nimproves the recognition accuracy of the rare words by 16%\nabsolute as compared to the strong baseline NLM.\nThe rest of the paper is organized as follows. Section 2 re-\nviews related approaches designed to deal with the rare word\nproblem. In Section 3, we brieﬂy describe the architecture of\nbaseline NLM. Section 4 presents the proposed embedding ma-\ntrix augmentation technique. In Section 5, we explain the ex-\nperiment setup and discuss the obtained results. Lastly, Section\n6 concludes the paper.\n2. Related works\nThe continuous vector representations of words are typically de-\nrived from large unlabeled corpora using co-occurrence statis-\ntics [10, 11]. They became the dominant feature for many natu-\nral language processing applications achieving the state-of-the-\nart results. To generalize well, however, these tasks require\nmany occurrences of each word and fall short if a word appears\nonly a handful of times [12]. Several approaches have been\nproposed to deal with the rare word problem and most of them\ncan be classiﬁed under one of the three main categories shown\nbelow.\n1) Morphological word representations.A bunch of pro-\nposed works resorts to subword level linguistic units by break-\narXiv:1904.03799v2  [cs.CL]  31 Jul 2019\ning down the words into morphemes [12, 13, 14]. For ex-\nample, [12] represented words as a function of its morphemes\nwhere the recursive neural network is applied over morpheme\nembeddings to obtain the embedding for a whole word. While\nsuch works have been proven effective to deal with the infre-\nquent word variations, they depend on the morphological an-\nalyzer such as Morfessor [15] and unable to model words\nwhose morphemes are unseen during the training stage.\n2) Character-level representations.To alleviate the rare\nword problem, ﬁner level linguistic units such as syllables and\ncharacters have been also studied [16, 17, 18, 19]. For exam-\nple, [16] explored both word-syllable and word-character level\nhybrid NLMs where most frequent words are kept unchanged,\nwhile rare words are split into the syllables and characters, re-\nspectively. In similar fashion, [17] and [18] examined character-\naware NLM architectures which rely only on character level in-\nputs, but predictions are still made at the word level.\nCharacter-level models eliminate the need for morpholog-\nical tagging or manual feature engineering and they comprise\nsubstantially fewer number of parameters compared to word-\nlevel models. Moreover, these approaches success at capturing\nproperties of morphologically related words (e.g. ‘run’ vs ‘run-\nning’), but may fail to capture distinctions between semantically\nunrelated words (e.g. ‘run’ vs ‘rung’) [19, 5].\n3) Knowledge-powered word representations.Another\ndirection of works leverage external knowledge to enhance rep-\nresentations of rare words [5, 20, 21]. For example, [5] em-\nployed word deﬁnitions obtained from the WordNet [22] to\nmodel rare words on the separate network. Alternatively, [20]\nproposed to incorporate external knowledge as a regularization\nterm to the original model’s objective function. Although these\napproaches have shown promising results, they highly depend\non the availability of external hand engineered lexical resources.\nNote that the aforementioned approaches can be also used\njointly. For example, by using factored NLM architecture [23]\nwhere different feature types can be combined.\n3. Baseline NLM architecture\nThe NLM architectures can be generally classiﬁed into two\nmain categories: feedforward [1] and recurrent [2]. Our method\ncan be applied to both of them, but in this paper we will focus on\nrecurrent architecture with LSTM units which has been shown\nto achieve the state-of-the-art results [24].\nThe conventional recurrent LSTM architecture can be de-\ncoupled into three main components as shown in Figure 1: 1)\ninput projection layer, 2) middle layers, and 3) output projec-\ntion layer. The input layer is parameterized by input embedding\nmatrix S used to map one-hot encoding representation of word\nwt ∈R|V |at time t into continuous vector representation st,\nwhere |V |is a vocabulary size:\nst = Swt (1)\nThe embedding vector st and a high-level context feature\nvector from the previous time step ht−1 are then combined by\nnon-linear middle layers, which can be represented as function\nf(), to produce a new context feature vector ht:\nht = f(st, ht−1) (2)\nThe non-linear function f() can employ simple activation units\nsuch as ReLU and hyperbolic tangent or more complex units\nsuch as LSTM and GRU. The middle layers can be also formed\nby composing several such functions.\nFigure 1: NLM architecture decomposed into three components.\nLastly, the context vectorht is fed to the output layer which\nis parameterized by output embedding matrix U to produce a\nhigh-dimensional vector yt ∈R|V |:\nyt = UT ht (3)\nThe entries of output vector yt represent the scores of words\nto follow the context ht. These scores are then normalized by\nsoftmax function to form a probability distribution.\nOur method modiﬁes the embedding matrices S and U\nwhile keeping the middle layer f() intact as will be explained\nin the next section.\n4. Embedding matrix augmentation\nWe start with the assumption that we are given a pre-trained\nNLM which models full vocabulary including both frequent and\nrare words. In such models, the rare words will be poorly rep-\nresented, leading to the sub-optimal performance. Therefore,\nour goal is to enrich the representation of rare words without\ncollecting additional training data and incurring expensive post-\nprocessing procedures.\nTo achieve this goal, we exploit the structure of NLM where\ninput and output layers are parameterized by word embedding\nmatrices (see Figure 1). Particularly, we propose to modify\nboth input and output embedding vectors of the rare words,\nwhile keeping the parameters of middle layers unchanged. The\nembedding vectors of the rare words are modiﬁed using em-\nbedding vectors of other semantically and syntactically similar\nwords. This approach will retain the linguistic regularities en-\ncapsulated within original pre-trained NLM, given that embed-\ndings of the rare words are properly modiﬁed. Our method can\nbe also viewed as a language model adaptation task [25] where\ninstead of topic or speaking style the vocabulary is adapted to\nconform with the words used in the target domain.\nThe proposed method has three main steps: 1) identifying\nthe rare words, 2) ﬁnding similar words and 3) enriching rare\nword representations.\n1) Identifying the rare words.To identify the rare words\nwe can simply count the frequency of words in the training data\nand set a frequency threshold below which all words are con-\nsidered rare. This approach, however, might result in too many\nrare words. To reduce the computation time, we can limit the\nrare words to those which appear in the n-best hypotheses or\nword lattice output.\n2) Finding similar words.Given a subset of rare words\nVrare ⊂V , the next step is to select a list of similar candi-\ndate words Cfor each rare word. The selected candidates will\nbe used to enrich representations of rare words, hence, they\nmust be frequent and present in the vocabulary V of NLM. In\naddition, they should be similar to the target rare word both\nin semantic meaning and syntactic behavior. Note that select-\ning inadequate candidates might deteriorate the performance of\nNLM, thus, they should be carefully inspected.\nSeveral effective methods exist that can ﬁnd appropriate\ncandidate words. For example, using lexical resources that con-\ntain synonyms and related words such as WordNet or employ-\ning pre-trained word embeddings from skip-gram or cbow\nmodels [10] which can also ﬁnd similar words. In our exper-\niments, we use lexical resource containing a list of Singapore\nstreet names where frequent street names will be used to update\nrepresentations of rare street names.\n3) Enriching rare word representations.Let sr be an em-\nbedding vector of some rare word wr in space deﬁned by input\nembedding matrix S and let Cr be corresponding set of similar\nwords. We enrich the representation ofsr using the words in Cr\nby the following formula:\nˆsr =\nsr + ∑\nsc∈Cr mcsc\n|Cr|+ 1 (4)\nwhere ˆsr is the enriched representation of sr, sc is an embed-\nding vector of similar candidate word and mc is a metric used\nto weigh candidates based on importance. The mc can be es-\ntimated using frequency counts or similarity score where most\nfrequent or most similar candidates are given higher weights. In\nour experiments, we weigh the candidates equally.\nThe Eq. (4) typically shifts the embedding of a rare word\ntowards the weighted centroid of its semantic landmark. The\nmotivation is that highly correlated words, in terms of both se-\nmantic meaning and syntactic behavior, should be close to each\nother in the embedding space. We then use the same candidates\nand formula to update the corresponding rare word embedding\nur in the output embedding matrix U. This procedure is then\nrepeated for the remaining words in subset Vrare.\n5. Experiment\nIn this section, we describe experiments conducted to evaluate\nthe effectiveness of the proposed embedding matrix augmenta-\ntion technique. Particularly, we ﬁrst enrich the rare Singapore\nstreet name representations in pre-trained NLM and then use the\nenriched NLM to rescore 100-best hypotheses output from the\nSingapore English ASR. The ASR system is built by Kaldi [26]\nspeech recognition toolkit using Singapore English speech cor-\npus. To highlight the importance of enriching the rare word rep-\nresentations, we used 1 hour recording of 9 read articles about\nSingapore streets as an evaluation set1 (7.3k words).\nWe compare our enriched model against three state-of-\nthe-art language models (LM) including Kneser-Ney smoothed\n4-gram2 (KN4), Kaldi-RNNLM [27] and recurrent LSTM\nLM [3]. Our model is obtained by enriching the representations\nof rare Singapore street names in the recurrent LSTM LM, and\nwe call it E-LSTM. The performance of these four LMs is eval-\nuated on the 100-best rescoring task.\n5.1. Experiment setup\nAcoustic model. The acoustic model (AM) is built using\n‘nnet3+chain’ setup of Kaldi and trained on 270 hours of tran-\nscribed Singapore English data which mostly consist of speech\ntaken from parliament, talk shows and interviews.\nLexicon. The lexicon is constructed by assembling 51k\nunique words which include around 2k Singapore street names.\nTo avoid ambiguity, the street names consisting of more than\n1https://github.com/khassanoff/SG_streets\n2We also examined otherN-gram models and found 4-gram to work\nbest for our case.\none word are joined using the underscore symbol, e.g. ‘Boon\nlay’ is changed to ‘Boon lay’. This lexicon was also used as a\nvocabulary set for LMs.\nLanguage model. To train LMs, we used AM transcripts\nand web crawled Singapore related data which resulted in to-\ntal 1M in-domain sentences (16M words). In addition, we used\nGoogle’s 1 billion word (1BW) benchmark corpus [28] to ac-\ncount for generic English word sequence statistics.\nThe KN4 is trained on combined in-domain (AM\ntranscript+web crawled) and generic 1BW data. It was built us-\ning SRILM toolkit [29] with 51k vocabulary set. We used KN4\nmodel to rescore both word lattice and 100-best list. Its pruned\nversion KN4 pruned was used during the decoding stage.\nThe Kaldi-RNNLM is a word-character level hybrid model\ndesigned to overcome the rare word problem by decomposing\nthe rare words into character n-grams while keeping the most\nfrequent words unchanged. It was trained as a 2-layer LSTM3\nwith 800 units in each layer using only in-domain data. The\ninput and output embedding matrices were tied and embedding\nspace dimension was set to 800. For vocabulary, we tried to\nkeep a different number of most frequent words and found 20k\nto perform best, the remaining 31k words were decomposed.\nThe recurrent LSTM LM is a word level model which was\nbuilt using our own implementation in PyTorch [30]. It was\ntrained as a single layer LSTM with 1k units using in-domain\ndata and 51k vocabulary set. The input and output embedding\nspace dimensions were set to 300 and 1000, respectively. The\nparameters of the model were learned by truncated BPTT [31]\nand SGD with gradient clipping. We also applied dropout for\nregularization [32].\nLastly, our E-LSTM model is obtained by enriching the rare\nword representations in the pre-trained recurrent LSTM LM. As\na case study, we use Singapore street names 4 where frequent\nstreets will be used to enrich the rare street representations. In\nparticular, we ﬁrst count the frequency of each street name in\nthe in-domain data and then divide them into two subsets of\nfrequent and rare streets using some threshold value. Next, we\nrandomly choose words from the subset of frequent streets and\nemploy them to enrich the representations of all rare streets (all-\nStreets) using the Eq. (4). To reduce computation time, we also\ntried to enrich only rare streets present in the 100-best hypothe-\nses output (fromNbest).\n5.2. Experiment results\nThe experiment results are shown in Table 1. In these experi-\nments, we divide the street names into frequent and rare sub-\nsets using the threshold value of 10. To enrich the rare streets\nwe used 5 randomly chosen frequent street names 5. The initial\nword error rate (WER) without any rescoring is 17.07%.\nThe obtained results show that the E-LSTM model outper-\nforms the strong KN4 used to rescore the word lattice by 16%\nrelative WER (from 16.52% to 13.83%). Moreover, it achieves\n6% relative WER improvement over Kaldi-RNNLM and LSTM\nmodels (from 14.73% to 13.83%). We found that enriching\nonly rare streets present in the 100-best hypotheses (fromNbest)\nachieves a similar result as enriching all rare streets (allStreets),\nwhile fromNbest being much faster.\nThe state-of-the-art WER results are usually achieved by in-\nterpolating NLM and count-based N-gram models which have\n3Changing the number of layers and its size didn’t improve WER.\n4https://geographic.org/streetview/singapore/\n5For consistency, we ﬁx the chosen frequent streets to be same.\nTable 1: The perplexity and WER results on evaluation set\nLM Perplexity Rescore WER\nKN4 pruned 436 - 17.07%\nKN4 351 Lattice 16.52%\n100-best 16.84%\nKaldi-RNNLM - 100-best 14.73%\n+KN4 - 100-best 14.10%\nLSTM 295 100-best 14.74%\n+KN4 - 100-best 14.95%\nE-LSTM (allStreets) 242 100-best 13.87%\n+KN4 - 100-best 13.58%\nE-LSTM (fromNbest) 234 100-best 13.83%\n+KN4 - 100-best 13.55%\nbeen shown to complement each other [2, 3]. To this end, we in-\nterpolated6 NLMs with KN4 and achieved further WER reduc-\ntions. Interestingly, the baseline LSTM model doesn’t beneﬁt\nfrom KN47, while E-LSTM gains additional 2% relative WER\nimprovement (from 13.83% to 13.55%).\n5.2.1. Changing the frequency threshold value\nTo determine the effective frequency threshold range, used to\nsplit the street names into frequent and rare subsets, we repeat\nthe experiment with different threshold values as shown in Fig-\nure 2. We observe that setting it between 5 and 50 is sufﬁcient\nto achieve good results. On the other hand, setting it too low or\nhigh will deteriorate WER as can be seen from the left and right\ntails of the plot in Figure 2.\n13.7\n13.9\n14.1\n14.3\n14.5\n1 10 100 1000 10000\nWER\nFrequency threshold\nE-LSTM (fromNbest) E-LSTM (allStreets)\nFigure 2: WER performance of E-LSTM at different frequency\nthreshold values used to split frequent and rare street names.\n5.2.2. Changing the number of frequent words\nWe also repeat the experiment to determine the optimal number\nof frequent words to use to enrich the rare words. We observed\nthat for all cases the WER results are similarly good. For fast\ncomputation, we recommend to use around 3-20 most frequent\nwords. This experiment is incomplete as we didn’t examine the\nquality of selected frequent words which requires more substan-\ntial analysis. Due to the space limitations, we leave the further\nanalysis for future work.\n6Interpolation weight for KN4 is set to 0.3.\n7Changing the interpolation weight didn’t help.\nTable 2: Recognition accuracy of 265 rare street names\nLM Rescore Accuracy\nKN4 pruned - 37.36%\nKN4 Lattice 36.98%\nKaldi-RNNLM 100-best 43.40%\nLSTM 100-best 43.02%\nE-LSTM (fromNbest) 100-best 59.25%\n5.2.3. Recognition accuracy of enriched rare words\nTo ensure that WER improvements are achieved as a result of\ncorrectly recognizing the enriched rare street names, we com-\npute the recognition accuracy of 265 rare street names (see Ta-\nble 2). The experiment results show that after enriching the\nbaseline recurrent LSTM LM, the recognition accuracy is in-\ncreased by 16.23% (from 43.02% to 59.25%) achieving the\nbest result among all LMs. Furthermore, we observe that cor-\nrectly recognizing the rare street names also helps to recover\nneighbouring words (see Table 3). These results conﬁrm the\neffectiveness of the proposed method.\nTable 3: Examples of correctly recovered neighbouring words\nafter rescoring with E-LSTM\nLM Example\nKN4 pruned\n1) a hawker centre and market began operations at\nbully playsin nineteen seventy six\n2) by nineteen ninety four when the book develop-\nment guide plan was announced\nLSTM 1) a hawker centre and market began operation at\nbully policein nineteen seventy six\n2) by nineteen ninety four when the product devel-\nopment gap plan was announced\nE-LSTM 1) a hawker centre and market began operations at\nboon lay placein nineteen seventy six\n2) by nineteen ninety four when thebedok develop-\nment guide plan was announced\n6. Conclusions\nIn this work, we proposed an effective method to enrich the\nrepresentations of rare words in pre-trained NLM. The pro-\nposed method augments the embedding matrices of pre-trained\nNLM while keeping other parameters unchanged. Importantly,\nit doesn’t require additional in-domain data and expensive post-\ntraining procedures. We applied our method to enrich the rare\nSingapore street names in pre-trained LSTM LM and used it to\nrescore the 100-best list generated by the state-of-the-art Singa-\npore English ASR system. The enriched LSTM LM achieved\n6% relative WER improvement over the baseline LSTM LM. In\ncomparison to other strong baseline LMs, our method achieves\nsigniﬁcant WER improvements, i.e. 6% and 16% improvement\nover Kaldi-RNNLM and KN4, respectively. Moreover, the en-\nriched LSTM increased the recognition accuracy of rare street\nnames by 16% absolute. We believe that the proposed method\ncan beneﬁt other models with similar network architecture and\nbe easily adapted to other scenarios.\n7. Acknowledgements\nThis work is supported by the project of Alibaba-NTU Singa-\npore Joint Research Institute.\n8. References\n[1] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural\nprobabilistic language model,” Journal of Machine Learning Re-\nsearch, vol. 3, pp. 1137–1155, 2003.\n[2] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and S. Khudan-\npur, “Recurrent neural network based language model,” in 11th\nAnnual Conference of the International Speech Communication\nAssociation, INTERSPEECH, 2010.\n[3] M. Sundermeyer, R. Schl ¨uter, and H. Ney, “Lstm neural networks\nfor language modeling,” in 13th Annual Conference of the In-\nternational Speech Communication Association, INTERSPEECH,\n2012.\n[4] S. F. Chen and J. Goodman, “An empirical study of smoothing\ntechniques for language modeling,” Computer Speech & Lan-\nguage, vol. 13, no. 4, pp. 359–394, 1999.\n[5] D. Bahdanau et al., “Learning to compute word embeddings on\nthe ﬂy,”arXiv preprint arXiv:1706.00286, 2017.\n[6] J. Schalkwyk, D. Beeferman, F. Beaufays, B. Byrne, C. Chelba,\nM. Cohen, M. Kamvar, and B. Strope, “Your word is my com-\nmand: google search by voice: A case study,” in Advances in\nspeech recognition. Springer, 2010, pp. 61–90.\n[7] J. Park, X. Liu, M. J. Gales, and P. C. Woodland, “Improved neu-\nral network based language modelling and adaptation,” in 11th\nAnnual Conference of the International Speech Communication\nAssociation, INTERSPEECH, 2010.\n[8] M. T. Pilehvar and N. Collier, “Inducing embeddings for rare and\nunseen words by leveraging lexical resources,” in Proceedings of\nthe 15th Conference of the European Chapter of the Association\nfor Computational Linguistics, EACL, 2017, pp. 388–393.\n[9] Y . Khassanov and C. E. Siong, “Unsupervised and efﬁcient vocab-\nulary expansion for recurrent neural network language models in\nasr,” in19th Annual Conference of the International Speech Com-\nmunication Association, INTERSPEECH, 2018, pp. 3343–3347.\n[10] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient esti-\nmation of word representations in vector space,” arXiv preprint\narXiv:1301.3781, 2013.\n[11] J. Pennington, R. Socher, and C. Manning, “Glove: Global vec-\ntors for word representation,” in Proceedings of the Conference\non Empirical Methods in Natural Language Processing, EMNLP,\n2014, pp. 1532–1543.\n[12] T. Luong, R. Socher, and C. Manning, “Better word representa-\ntions with recursive neural networks for morphology,” inProceed-\nings of the 17th Conference on Computational Natural Language\nLearning, CoNLL, 2013, pp. 104–113.\n[13] A. Lazaridou et al., “Compositional-ly derived representations of\nmorphologically complex words in distributional semantics,” in\nProceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics, ACL, Volume 1: Long Papers , 2013,\npp. 1517–1526.\n[14] S. Qiu, Q. Cui, J. Bian, B. Gao, and T.-Y . Liu, “Co-learning of\nword representations and morpheme representations,” in 25th In-\nternational Conference on Computational Linguistics, COLING ,\n2014, pp. 141–150.\n[15] M. Creutz and K. Lagus, “Unsupervised models for morpheme\nsegmentation and morphology learning,”TSLP, vol. 4, no. 1, p. 3,\n2007.\n[16] T. Mikolov, I. Sutskever, A. Deoras, H.-S. Le, and S. Kombrink,\n“Subword language modeling with neural networks,” 2012.\n[17] W. Ling et al., “Finding function in form: Compositional char-\nacter models for open vocabulary word representation,” in Pro-\nceedings of the Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP, 2015, pp. 1520–1530.\n[18] Y . Kim, Y . Jernite, D. Sontag, and A. M. Rush, “Character-aware\nneural language models.” in Proceedings of the 13th AAAI Con-\nference on Artiﬁcial Intelligence, 2016, pp. 2741–2749.\n[19] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching\nword vectors with subword information,” TACL, vol. 5, pp. 135–\n146, 2017.\n[20] C. Xu et al. , “Rc-net: A general framework for incorporating\nknowledge into word representations,” inProceedings of the 23rd\nACM International Conference on Information and Knowledge\nManagement, CIKM, 2014, pp. 1219–1228.\n[21] M. Faruqui et al. , “Retroﬁtting word vectors to semantic lexi-\ncons,” in Proceedings of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Tech-\nnologies, NAACL HLT, 2015, pp. 1606–1615.\n[22] G. A. Miller, “Wordnet: a lexical database for english,” Commu-\nnications of the ACM, vol. 38, no. 11, pp. 39–41, 1995.\n[23] A. Alexandrescu and K. Kirchhoff, “Factored neural language\nmodels,” in Human Language Technology Conference of the\nNorth American Chapter of the Association of Computational Lin-\nguistics. ACL, 2006.\n[24] M. Sundermeyer et al., “Comparison of feedforward and recurrent\nneural network language models,” in IEEE International Confer-\nence on Acoustics, Speech and Signal Processing, ICASSP, 2013,\npp. 8430–8434.\n[25] Y . Khassanov et al., “Unsupervised language model adaptation by\ndata selection for speech recognition,” inAsian Conference on In-\ntelligent Information and Database Systems, ACIIDS. Springer,\n2017, pp. 508–517.\n[26] D. Povey et al., “The kaldi speech recognition toolkit,” in IEEE\nSignal Processing Society, 2011.\n[27] H. Xu et al., “Neural network language modeling with letter-based\nfeatures and importance sampling,” inIEEE International Confer-\nence on Acoustics, Speech and Signal Processing, ICASSP, 2018,\npp. 6109–6113.\n[28] C. Chelba et al. , “One billion word benchmark for measuring\nprogress in statistical language modeling,” in 15th Annual Con-\nference of the International Speech Communication Association,\nINTERSPEECH, 2014, pp. 2635–2639.\n[29] A. Stolcke, “Srilm-an extensible language modeling toolkit,” in\n7th International Conference on Spoken Language Processing,\nICSLP-INTERSPEECH, 2002.\n[30] A. Paszke et al., “Automatic differentiation in pytorch,” 2017.\n[31] P. J. Werbos, “Backpropagation through time: what it does and\nhow to do it,”Proceedings of the IEEE, vol. 78, no. 10, pp. 1550–\n1560, 1990.\n[32] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural net-\nwork regularization,”arXiv preprint arXiv:1409.2329, 2014.",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.7414171695709229
    },
    {
      "name": "Generalization",
      "score": 0.6907175779342651
    },
    {
      "name": "Word (group theory)",
      "score": 0.6816262006759644
    },
    {
      "name": "Computer science",
      "score": 0.6816241145133972
    },
    {
      "name": "Word embedding",
      "score": 0.6030886173248291
    },
    {
      "name": "Representation (politics)",
      "score": 0.5946390628814697
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5646119713783264
    },
    {
      "name": "Natural language processing",
      "score": 0.5200324058532715
    },
    {
      "name": "Language model",
      "score": 0.4876490831375122
    },
    {
      "name": "Word error rate",
      "score": 0.4708663523197174
    },
    {
      "name": "Matrix (chemical analysis)",
      "score": 0.451930433511734
    },
    {
      "name": "Speech recognition",
      "score": 0.41978731751441956
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3792003393173218
    },
    {
      "name": "Mathematics",
      "score": 0.2635842561721802
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ]
}