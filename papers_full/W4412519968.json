{
  "title": "LLM-as-a-Judge: automated evaluation of search query parsing using large language models",
  "url": "https://openalex.org/W4412519968",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Mehmet Selman Baysan",
      "affiliations": [
        "Turkish Society of Hematology"
      ]
    },
    {
      "id": "https://openalex.org/A2146362776",
      "name": "Serkan Uysal",
      "affiliations": [
        "Aselsan (Turkey)"
      ]
    },
    {
      "id": "https://openalex.org/A4296348237",
      "name": "İrem İşlek",
      "affiliations": [
        "Turkish Society of Hematology"
      ]
    },
    {
      "id": "https://openalex.org/A5072188543",
      "name": "Çağla Çığ Karaman",
      "affiliations": [
        "Turkish Society of Hematology"
      ]
    },
    {
      "id": "https://openalex.org/A2104501514",
      "name": "Tunga Gungor",
      "affiliations": [
        "Boğaziçi University"
      ]
    },
    {
      "id": null,
      "name": "Mehmet Selman Baysan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146362776",
      "name": "Serkan Uysal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296348237",
      "name": "İrem İşlek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5072188543",
      "name": "Çağla Çığ Karaman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104501514",
      "name": "Tunga Gungor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6691556129",
    "https://openalex.org/W6859606051",
    "https://openalex.org/W4394708144",
    "https://openalex.org/W4402482103",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W6794881632",
    "https://openalex.org/W4404782099",
    "https://openalex.org/W4405253355",
    "https://openalex.org/W4384625834",
    "https://openalex.org/W4404344675",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W6966639963",
    "https://openalex.org/W4392305811",
    "https://openalex.org/W4405562067",
    "https://openalex.org/W6865215264",
    "https://openalex.org/W6871252035",
    "https://openalex.org/W6852874933",
    "https://openalex.org/W1593271688",
    "https://openalex.org/W4412230201",
    "https://openalex.org/W4399836654",
    "https://openalex.org/W1493697127"
  ],
  "abstract": "Introduction The adoption of Large Language Models (LLMs) in search systems necessitates new evaluation methodologies beyond traditional rule-based or manual approaches. Methods We propose a general framework for evaluating structured outputs using LLMs, focusing on search query parsing within an online classified platform. Our approach leverages LLMs' contextual reasoning capabilities through three evaluation methodologies: Pointwise, Pairwise, and Pass/Fail assessments. Additionally, we introduce a Contextual Evaluation Prompt Routing strategy to improve reliability and reduce hallucinations. Results Experiments conducted on both small- and large-scale datasets demonstrate that LLM-based evaluation achieves approximately 90% agreement with human judgments. Discussion These results validate LLM-driven evaluation as a scalable, interpretable, and effective alternative to traditional evaluation methods, providing robust query parsing for real-world search systems.",
  "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/one.tnum July /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nOPEN ACCESS\nEDITED BY\nYongqi Li,\nHong Kong Polytechnic University,\nHong Kong SAR, China\nREVIEWED BY\nHongru Cai,\nNational University of Singapore, Singapore\nZhen Zhang,\nNanyang Technological University, Singapore\n*CORRESPONDENCE\nMehmet Selman Baysan\nmehmet.baysan@sahibinden.com\nRECEIVED /one.tnum/four.tnum April /two.tnum/zero.tnum/two.tnum/five.tnum\nACCEPTED /three.tnum/zero.tnum June /two.tnum/zero.tnum/two.tnum/five.tnum\nPUBLISHED /two.tnum/one.tnum July /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nBaysan MS, Uysal S, ˙I¸ slek˙I, Çı ˘g Karaman Ç and\nGüngör T (/two.tnum/zero.tnum/two.tnum/five.tnum) LLM-as-a-Judge: automated\nevaluation of search query parsing using large\nlanguage models. Front. Big Data/eight.tnum:/one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Baysan, Uysal,˙I¸ slek, Çı˘g Karaman and\nGüngör. This is an open-access article\ndistributed under the terms of the\nCreative\nCommons Attribution License (CC BY) . The\nuse, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nLLM-as-a-Judge: automated\nevaluation of search query\nparsing using large language\nmodels\nMehmet Selman Baysan /one.tnum*, Serkan Uysal /two.tnum, ˙Irem ˙I¸ slek/one.tnum,\nÇa˘gla Çı ˘g Karaman /one.tnumand Tunga Güngör /three.tnum\n/one.tnumsahibinden.com, Istanbul, Türkiye, /two.tnumsahibinden.com, Ankara, Türkiye, /three.tnumDepartment of Computer\nEngineering, Faculty of Engineering, Bo ˘gaziçi University, Istanbul, Türkiye\nIntroduction: The adoption of Large Language Models (LLMs) in search systems\nnecessitates new evaluation methodologies beyond tradition al rule-based or\nmanual approaches.\nMethods: We propose a general framework for evaluating structured outpu ts\nusing LLMs, focusing on search query parsing within an online class iﬁed platform.\nOur approach leverages LLMs’ contextual reasoning capabilitie s through three\nevaluation methodologies: Pointwise, Pairwise, and Pass/Fail assessments.\nAdditionally, we introduce a Contextual Evaluation Prompt Ro uting strategy to\nimprove reliability and reduce hallucinations.\nResults: Experiments conducted on both small- and large-scale datasets\ndemonstrate that LLM-based evaluation achieves approximate ly /nine.tnum/zero.tnum% agreement\nwith human judgments.\nDiscussion: These results validate LLM-driven evaluation as a scalable,\ninterpretable, and eﬀective alternative to traditional evalua tion methods,\nproviding robust query parsing for real-world search systems.\nKEYWORDS\nLLM-as-a-Judge, structured output evaluation, search query p arsing, large language\nmodels, evaluation framework, generative search, automatic eva luation, query\nunderstanding\n/one.tnum Introduction\nThe adoption of large language models (LLMs) in search systems is fundamentally\nreshaping how these systems function, driving the emergence of generative search beyond\ntraditional retrieval methods. This shift introduces new challenges in the evaluation of\nsearch system performance, especially in real-world applications such as online classiﬁed\nads platforms, where accurately interpreting user search queries is essential to improve the\nretrieval and ranking quality (\nLuo et al., 2023 ).\nFigure 1 shows the parsing of an example search query in an online ads platform\nand a sketch of the evaluation process proposed in this work. The search system extracts\nstructured information from the user query given in natural language, including query\ncategory, search ﬁlters, location, explicit and implicit keywords, synonyms, and other\nrelevant attributes. The extracted elements are represented in a structured form which is\nused as the basis for evaluation. Unlike traditional syntactic parsing, search systems parse\nbased not only on the textual features of the query but also on its semantics and contexts.\nThis implies that the system infers implicit intentions, resolves ambiguities, and maps the\nquery to a structured representation that aligns with its underlying meaning rather than\njust its surface form.\nFrontiers in Big Data /zero.tnum/one.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nFIGURE /one.tnum\nOverview of the LLM-as-a-Judge evaluation process for a sample se arch query.\nTraditional evaluation methods for search query parsing such\nas exact match, precision, recall, and rule-based heuristics like the\nnumber of search results returned often struggle to fully reﬂect\nthe query understanding capability of the systems used in complex\napplications (\nLee et al., 2021 ). Additionally, these approaches may\nnot eﬀectively capture the semantic nuances of the parsed user\nqueries, leading to limited generalizability. Task-speciﬁc evaluation\nmethods, such as checking whether a query retrieves the correct\nresults or measuring how users interact with search results, can\nprovide better assessments. However, these methods are highly\ndependent on domain speciﬁc rules and are diﬃcult to apply\nacross diﬀerent search systems, making them less ﬂexible (\nJiang\nand Cai, 2024 ). Moreover, manual evaluation methods, which\ninvolve human annotators assessing query parsing results, are\nboth time-consuming and expensive, making them impractical\nfor large-scale, real-time systems. Due to the ambiguity inherent\nin natural language, there is often no single correct output for\na given query, as multiple valid structured representations may\nexist depending on subtle diﬀerences in context and user intent.\nTraditional evaluation methods rely on rigid comparisons against\npredeﬁned reference outputs and thus cannot handle cases where\nmultiple outputs are equally valid or where errors are subtle\nyet impactful. Consequently, a more context-aware and ﬂexible\nevaluation approach is necessary to assess the eﬀectiveness of the\nsearch system in understanding and structuring user queries.\nThe challenge is particularly evident in high-traﬃc\nenvironments such as e-commerce platforms, where the ability\nto accurately parse user queries into structured representations is\ncrucial for delivering relevant search results. In these scenarios,\ntraditional evaluation methods face diﬃculties in scaling to the\nvast number of user queries, ensuring real-time adaptability,\nand mitigating inconsistencies in human-labeled reference\ndatasets. Beyond these commonly-used methods, other automated\nevaluation techniques, such as BLEU (\nPapineni et al., 2002 ) for\nn-gram overlap or Smatch ( Cai and Knight, 2013 ) for Abstract\nMeaning Representation (AMR), have shown promise but cannot\nfully address the complexity of evaluating structured outputs like\nsearch query parsing results.\nThe LLM-as-a-Judge framework initially emerged as a\npromising approach for evaluating various natural language\nprocessing (NLP) tasks, providing an automated and scalable\nalternative to human assessments. Over time, its application\nhas expanded to include the evaluation of structured outputs\nof various systems. While still an evolving method, it oﬀers the\npotential for more scalable and consistent assessments compared\nto traditional techniques. Recent research (\nSchneider et al., 2024 )\nhas demonstrated that LLMs can eﬀectively evaluate semantic\nparsing tasks by leveraging their ability to understand natural\nlanguage nuances and assess the correctness of structured outputs\nbeyond surface-level lexical matching.\nIn this work, we employ automated LLM-based evaluation\nmethods for assessing search query parsing systems, which leverage\nthe reasoning and contextual understanding capabilities of LLMs.\nUnlike rule-based or heuristic evaluation methods, which may\nfail to generalize across diﬀerent query formulations, LLMs oﬀer\na more nuanced and adaptable evaluation framework. These\nmethods enable the system to be assessed not just on surface-\nlevel correctness but also on semantic ﬁdelity, intent alignment,\nand contextual appropriateness. By integrating LLM-as-a-Judge\nmethods, we ensure a more robust and scalable evaluation of search\nquery parsing in large-scale, real-world search systems, where\naccuracy and eﬃciency are critical. However, integrating LLMs into\nevaluation pipelines introduces new complexities, necessitating\ncareful design to ensure reliability, eﬃciency, and alignment\nwith human judgment. In this work, we explore strategies to\nmitigate these challenges, reﬁning LLM-based evaluations to\nenhance consistency and reduce hallucinations, ultimately making\nthem a viable alternative to both costly and labor-intensive\nhuman evaluations and evaluations made by traditional syntax-\nbased/word-based metrics.\nWe explore three distinct LLM-as-a-Judge methodologies:\nPointwise Evaluation, Pairwise Evaluation, and Pass/Fail Evaluation.\nOur approach builds upon existing research in semantic parsing\nevaluation, incorporating elements from both traditional metrics\nlike Smatch and newer LLM-based assessment techniques. We\nconduct extensive experiments using various LLMs, prompting\nstrategies, and evaluation techniques, and demonstrate that our\nLLM-as-a-Judge framework achieves over 90% alignment with\nhuman judgments across evaluation types. Furthermore, we\nintroduce a Contextual Evaluation Prompt Routing strategy within\nLLM evaluation to enhance the eﬃciency of the evaluation and\nmitigate LLM hallucinations. Our ﬁndings validate the eﬀectiveness\nof LLM-driven automated evaluation for search query parsing\nin large-scale, real-world search systems, oﬀering a scalable and\nadaptable evaluation pipeline that minimizes manual eﬀort. Beyond\nstructured output evaluation, we also examine the reliability of our\nLLM evaluator framework using statistical agreement metrics to\nensure the robustness of LLM-based assessments.\nFrontiers in Big Data /zero.tnum/two.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nThe contributions in this work are as follows:\n• We propose an evaluation framework that leverages large\nlanguage models for context-aware, interpretable, and scalable\nevaluation of structured outputs.\n• We apply the proposed evaluation framework to search\nquery parsing by adapting the Pointwise, Pairwise, and\nPass/Fail evaluation strategies to address various assessment\nrequirements.\n• We introduce the Contextual Evaluation Prompt Routing\nstrategy as a domain-speciﬁc solution for dynamically\nadjusting evaluation prompts based on query categories,\nenabling more accurate and context-aware assessment of\nstructured search query parsing outputs.\n• We show that the proposed framework, particularly the\nContextual Evaluation Prompt Routing strategy, substantially\nimproves the evaluation accuracy and reliability compared to\nbaseline methods.\n/two.tnum Related work\nThe evaluation of search query parsing and semantic parsing\nsystems has been a longstanding challenge in NLP and information\nretrieval (IR). Various methods ranging from rule-based systems to\nneural models have been proposed to improve parsing accuracy and\nassessment. Recently, LLM-as-a-Judge has emerged as a promising\napproach for evaluating structured outputs, oﬀering scalability\nand adaptability across diﬀerent domains. In this section, we\nreview traditional evaluation methods for search query parsing,\ndiscuss general LLM-as-a-Judge techniques, and explore domain-\nspeciﬁc applications of LLM-as-a-Judge, particularly in search and\nsemantic parsing systems.\n/two.tnum./one.tnum Traditional evaluation methods\nTraditional evaluation methods for search query parsing and\nsemantic parsing rely on exact match accuracy, precision-recall\nmetrics, and rule-based heuristics. While these approaches measure\nthe correctness by comparing system outputs to reference query\noutputs, they often fail to capture semantic equivalence, penalizing\nvalid variations in structured outputs (\nLee et al., 2021 ). Early\nmethods assessed correctness through query execution accuracy,\nwhere system-generated queries are executed against a database\nand the returned results determine the accuracy (\nJiang and Cai,\n2024). While this approach is applicable in search query parsing,\nevaluating the correctness of the results still requires manual\njudgments.\nLuo et al. (2023) introduced precision and coverage\nmetrics to evaluate attribute extraction, but these methods require\nmanual judgments, making them less scalable.\nWith the rise of neural semantic parsers, evaluation techniques\nincreasingly incorporated denotation-based methods while\nretaining other approaches. Denotation-based evaluation, which\ncompares execution results rather than output structures, was\nalready used in non-neural settings and gained further prominence\nwith neural models. Additionally, though initially designed for\nmachine translation, statistical metrics like BLEU (\nPapineni et al.,\n2002) continued to be used for assessing semantic parsing outputs.\nHowever, these approaches face some challenges:\n• Spurious matches—Incorrect queries may produce correct\nresults by chance.\n• Over-penalization—Semantically correct but syntactically\ndiﬀerent outputs are unfairly penalized.\n• Lack of semantic awareness—BLEU and similar metrics fail to\ncapture deep semantic understanding.\nGraph-based evaluation metrics like Smatch (\nCai and Knight,\n2013) attempt to address these issues by measuring semantic\nstructure similarity rather than strict string matching. While\neﬀective, these methods are computationally expensive and not\nwidely used in search query parsing.\nOverall, traditional evaluation methods struggle with\ngeneralization, adaptability, and deeper semantic reasoning.\nThis highlights the need for more semantically oriented evaluators,\nwhich can assess semantic correctness beyond surface-level\ncomparisons. Our work builds on these ﬁndings by employing\nLLM-as-a-Judge to evaluate structured search query parsing,\nensuring alignment with user intent and domain-speciﬁc accuracy.\n/two.tnum./two.tnum LLM-as-a-Judge for automated\nevaluation\nLLM-as-a-Judge has gained traction as a scalable alternative\nto human evaluations across various NLP tasks, including text\nsummarization, dialogue evaluation, and semantic parsing. The\nconcept leverages the reasoning capabilities of large language\nmodels to assess system outputs, reducing reliance on costly\nhuman annotations while maintaining evaluation consistency\nand scalability.\nA comprehensive survey by\nGu et al. (2025) explores how\nLLM-as-a-Judge can enhance evaluation reliability by addressing\nchallenges such as bias mitigation, prompt engineering, and\nstandardization of evaluation methodologies. The study highlights\nthat LLMs oﬀer more nuanced assessments compared to rule-\nbased or heuristic metrics, particularly in tasks requiring semantic\nalignment rather than syntactic matching. Similarly,\nLi H. et al.\n(2024) provide a structured framework for constructing LLM-\nbased evaluation pipelines, addressing how LLMs can be utilized\neﬀectively, where they perform best and how they should be\nassessed.\nAnother critical aspect is the evaluation process itself.\nChiang\nand Lee (2023) analyze diﬀerent strategies, revealing that forcing\nLLMs to output only a single numeric rating is suboptimal, while\nprompting LLMs to explain their ratings signiﬁcantly improves\nalignment with human judgments. These ﬁndings emphasize the\nimportance of prompt engineering and structured evaluation\nprompts to enhance the reliability of LLM-generated assessments.\nDespite their advantages, LLM-based evaluators are prone\nto stochastic variability, position bias, verbosity bias, and self-\nenhancement bias.\nZheng et al. (2023) highlight these limitations\nin their study on MT-Bench and Chatbot Arena, demonstrating\nthat while LLM judges such as GPT-4 align with human preferences\nFrontiers in Big Data /zero.tnum/three.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nover 80% of the time, they still require bias mitigation techniques to\nensure fair assessments. To address reliability concerns, researchers\nhave explored adaptive evaluation techniques.\nShankar et al. (2024)\nintroduce EvalGen, a system that reﬁnes evaluation prompts\nthrough human-in-the-loop feedback. They identify criteria drift,\nwhere evaluation criteria evolve as human reviewers assess more\noutputs. This aligns with our prompt engineering with iterative\nreﬁnement approach, which systematically optimizes evaluation\nprompts to enhance consistency and reduce hallucinations. In our\nexperiments, since we use evaluators from the same model family\nwe do not consider self-enhancement bias a signiﬁcant threat.\nHowever, position bias and stochastic variability remain critical\nchallenges in our context. This study explicitly addresses these\ntwo sources of evaluation instability through experimental controls\nsuch as randomized response ordering and repeated runs with\nmajority voting or averaging to enhance robustness.\nA major challenge in LLM-based evaluation is result\nconsistency across multiple replications.\nSchroeder and Wood-\nDoughty (2024) introduce McDonald’s omega as a measure of\nevaluation reliability, assessing how sensitive LLM evaluators are to\nsmall variations in input conditions. Their study emphasizes that\nsingle-shot evaluations may introduce inconsistencies, reinforcing\nthe need for multiple evaluation iterations and statistical\nreliability measures, which is an approach we integrate into\nour study.\nWhile these studies focus on LLM-based evaluation across\ngeneral NLP tasks, their insights inform our approach to evaluating\nstructured outputs in search query parsing. Our study extends this\nparadigm by adapting LLM-as-a-Judge to structured JSON-like\nevaluations, ensuring that search query parsing accuracy is assessed\nthrough semantic correctness, intent alignment, and contextual\nappropriateness rather than surface-level comparisons.\n/two.tnum./three.tnum Domain-speciﬁc LLM-as-a-Judge and\nits application in search query parsing\nWhile LLM-as-a-Judge has been widely explored in general text\ngeneration and evaluation tasks, its application in domain-speciﬁc\nstructured evaluation, such as search query parsing and semantic\nparsing, presents additional challenges. Unlike free-text evaluation,\nquery parsing evaluation requires assessing structured outputs,\nincluding logical forms, database queries, or graph representations.\nRecent research has focused on adapting LLM-based evaluation\nframeworks to domain-speciﬁc tasks, ensuring that evaluations\nalign with business needs, structured information retrieval, and\nreasoning-intensive applications.\nA key challenge in domain-speciﬁc LLM-based evaluation\nis the need for custom evaluation criteria.\nZhang et al. (2024)\nintroduce TALEC, a model-based evaluation method that enables\nusers to ﬂexibly deﬁne their own evaluation criteria based\non domain requirements. Their approach leverages zero-shot\nand few-shot in-context learning (ICL) to teach LLMs in-\nhouse evaluation rules, improving adaptability across diﬀerent\nbusiness scenarios. By combining prompt engineering with\niterative reﬁnement, TALEC achieves over 80% correlation\nwith human judgments, demonstrating that LLM-as-a-Judge can\naccurately reﬂect domain-speciﬁc quality standards. Our work\nbuilds upon these ﬁndings by applying the Contextual Evaluation\nPrompt Routing strategy to search query parsing evaluation,\nwhere domain-speciﬁc prompts are dynamically selected based\non query categories, ensuring that evaluation criteria remain\ncontextually relevant.\nAnother critical aspect of LLM-based evaluation in domain-\nspeciﬁc applications is the construction of specialized evaluation\ndatasets.\nRaju et al. (2024) propose a data pipeline for curating\ndomain-speciﬁc evaluation sets, addressing the limitations\nof general-purpose benchmarks like AlpacaEval 2.0 LC\n(\nDubois et al., 2024 ) and Arena-Hard v0.1 ( Li T. et al., 2024 ).\nTheir method integrates manual curation, semi-supervised\nclustering, and stratiﬁed sampling to create balanced evaluation\ndatasets covering diverse domains such as law, medicine, and\nmultilingual contexts. This approach signiﬁcantly improves\nbenchmark separability (84%) and agreement with human\npreferences (84%), demonstrating the importance of tailored\nevaluation datasets for LLM-as-a-Judge frameworks. Our study\naligns with this research by constructing manually labeled\nvalidation sets for search query parsing evaluation, ensuring that\nassessments align with human preferences and domain-speciﬁc\naccuracy requirements.\nBeyond domain-speciﬁc benchmarks, LLM-as-a-Judge has also\nbeen explored in reasoning-intensive retrieval tasks. JudgeRank\n(\nNiu et al., 2024 ) introduces a three-step agentic reranking\napproach, where query analysis, document summarization, and\nrelevance judgment are performed sequentially to improve\nretrieval accuracy in retrieval-augmented generation (RAG)\nsystems. Their method outperforms dense retrieval baselines on\nreasoning-intensive tasks, highlighting the potential of LLMs in\nstructured evaluation. While JudgeRank focuses on ranking search\nresults, its stepwise reasoning approach informs our multi-step\nquery parsing evaluation framework, where LLMs assess query\nunderstanding based on extracted structured attributes rather than\ndocument rankings.\nFinally, evaluating semantic parsing for knowledge-based\nconversational question answering has revealed important insights\ninto LLM performance on structured outputs.\nSchneider et al.\n(2024) evaluate LLMs in generating structured graph queries\nfrom natural language, demonstrating that few-shot prompting\nand ﬁne-tuning techniques improve performance on structured\nparsing tasks. Their ﬁndings suggest that zero-shot performance\nis often inadequate for complex structured outputs, reinforcing\nour decision to incorporate few-shot prompting and iterative\nreﬁnements in LLM-based search query parsing evaluation.\nOverall, these studies highlight the importance of domain-\nspeciﬁc criteria, specialized benchmarks, reasoning-driven\nevaluation strategies, and structured query assessment in adapting\nLLM-as-a-Judge to search query parsing and semantic parsing\napplications. Our work extends these eﬀorts by introducing\na scalable, structured evaluation pipeline, leveraging LLM-\nas-a-Judge for assessing query parsing outputs in real-world\nsearch systems.\n/three.tnum Methodology\nIn this section, we propose a general framework for evaluating\nstructured outputs using the LLM-as-a-Judge approach. Structured\nFrontiers in Big Data /zero.tnum/four.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nFIGURE /two.tnum\nLLM-as-a-Judge framework.\noutputs, such as those in semantic and search query parsing, require\nboth semantic understanding and structural consistency, making\ntheir evaluation more complex than rule-based assessments. We\ndemonstrate our framework through the evaluation of search\nquery parsing in an online advertisement platform. However,\nthe proposed approach is not limited to this task. Instead,\nit provides a scalable and adaptable evaluation methodology\nfor assessing structured outputs across various domains. To\nensure reliable and interpretable evaluations, our framework\nincorporates structured evaluation prompts. Additionally, we\nintroduce a Contextual Evaluation Prompt Routing strategy to\nimprove evaluation eﬃciency and mitigate hallucinations in\nLLM-based assessments.\nThe LLM-as-a-Judge framework proposed in this work\nis depicted in\nFigure 2. The framework follows a structured\napproach to assess the quality of the results of the parsed search\nquery. The process begins with a user query that is given to\nthe search query parser. The search query parser converts\nthe query into a structured format, capturing attributes such\nas category, ﬁlters, location, and keywords by making use of\ninformation about categories (vehicle, real estate, etc.) and\nﬁlters (vehicle condition, room size, etc.) encoded in trees.\nThis parsed output is then evaluated through three distinct\nmethods: pointwise evaluation, pairwise evaluation, and pass/fail\nevaluation. To perform evaluations, an evaluation prompt\nis given to the LLM-as-a-Judge as input. This evaluation\nprompt consists of domain-speciﬁc evaluation criteria, rating\nrubrics customized for each evaluation method, enough few-\nshot examples so that the LLM can learn how to evaluate\nfrom context, and the search query parsing system prompt\nto understand the category and ﬁlter information. Each\nevaluation method ensures transparency by providing a\njustiﬁcation for its evaluation, reducing the reliance on human\nintervention while maintaining high reliability. By leveraging\nthese techniques, the LLM-as-a-Judge framework oﬀers a robust\nand scalable solution for evaluating structured outputs across\ndiverse applications.\n/three.tnum./one.tnum Search query parsing system\nIn this study, we evaluate a search query parsing system\ndesigned for an online classiﬁed advertisement platform. The\nparsing system plays a crucial role in understanding user queries\nand transforming them into a structured representation that\nenhances search accuracy and ﬁltering capabilities. In this section,\nwe describe brieﬂy the query parsing system used in this work to\nunderstand the evaluation process clearly.\n/three.tnum./one.tnum./one.tnum Query types and user intent\nThe platform handles a diverse set of user search queries,\nranging from simple keyword-based searches to more complex,\nmulti-faceted queries that include speciﬁc ﬁlters and conditions.\nThe queries typically fall into the following categories:\n• Generic queries: broad search terms without speciﬁc ﬁlters\n(e.g., “cars for sale, ” “rental apartments”).\n• Feature-speciﬁc queries: queries that include attributes such\nas price range, brand, or room count (e.g., “red BMW under\n500,000 TL, ” “2 + 1 apartments for rent in Be¸ sikta¸ s”).\n• Location-based queries: queries that explicitly mention cities,\ndistricts, or neighborhoods (e.g., “houses for sale in Kadıköy, ”\n“oﬃce space for rent in Levent”).\n• Implicit intent queries: queries where certain attributes (e.g.,\nprice expectations such as “bargain price, ” conditions like\n“urgent sale”) are implied rather than explicitly stated.\nTo ensure an optimal search experience, the search query\nparsing system must eﬀectively interpret these queries, extract\nmeaningful attributes, and represent them in a structured format.\n/three.tnum./one.tnum./two.tnum Structured output representation\nThe parsing system converts each query into a structured JSON\noutput, ensuring that the search criteria in the query are properly\nFrontiers in Big Data /zero.tnum/five.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\ncategorized and formatted for the retrieval engine. All categories\nand ﬁlters used to extract structured output in this system are\nspeciﬁc to the online classiﬁed platform domain. This structured\noutput consists of the following key components:\n• The search query parsing system classiﬁes each query\ninto a multi-level category hierarchy, ensuring that the\nsearch intent is accurately captured and aligned with\nthe platform’s structured taxonomy. At the highest level,\ncategory_level_1 represents the broad category, such as\nreal estate. This is further reﬁned into category_level_2,\nwhich indicates the general classiﬁcation within the broader\ncategory, such as apartments or commercial properties.\ncategory_level_3 provides a more speciﬁc classiﬁcation,\nidentifying distinctions like for sale or for rent. In some cases,\nan additional level, category_level_4, is included for further\nreﬁnements. Accurately determining the category of a query\nis essential, as it ensures that relevant ﬁlters and retrieval\nmechanisms are applied correctly, improving the precision of\nsearch results.\n• The system also performs ﬁlter extraction, identifying\nboth explicit and implicit ﬁlters within the query. Filters\ncapture essential attributes that reﬁne the search results\nand enhance user experience. These include numerical\nﬁlters such as price ranges, mileage, and room counts\n(e.g., “houses under 5 million TL ”); boolean ﬁlters which\nindicate conditions (e.g., “furnished” or “new”); and\nenumerated ﬁlters which deﬁne speciﬁc values such as brand\nnames, fuel types, or transmission types. For reliable query\ninterpretation, the extracted ﬁlters must be mapped accurately\nto the predeﬁned system ﬁlters, ensuring structured and\nmeaningful retrieval.\n• The system also extracts location information, if present in the\nquery. Location data is mapped to structured ﬁelds such as\ncity (e.g., “Istanbul”) and district (e.g., “Kadıköy”). If the query\nlacks explicit location details, the system may infer relevant\nlocation attributes based on user behavior, default preferences,\nor additional context. Proper location extraction ensures that\nthe results are relevant to the user’s intent and geographical\nconstraints.\n• Keyword and synonym recognition plays a crucial role in\nenhancing search coverage and query understanding. The\nsystem identiﬁes explicit keywords that appear in the user\nquery, while also generating synonyms to improve search\nrecall (e.g., “ﬂat” for “apartment, ” “auto” for “car”). However,\nif a keyword is already categorized under the category or\nﬁlter ﬁelds, it is not duplicated as an explicit keyword to\navoid redundancy. This structured approach to keyword\nand synonym recognition helps reﬁne search accuracy while\nmaintaining query clarity.\nBy converting unstructured natural language queries into\nstructured data, the search query parsing system enhances the\neﬃciency of the search engine. However, ensuring the accuracy of\nthese parsed outputs requires a robust evaluation framework. This\nis where the LLM-as-a-Judge evaluation methodology is applied,\nassessing the correctness of structured outputs using various\nevaluation techniques described in the following section.\nTABLE /one.tnum Evaluation criteria for search query parsing.\nEvaluation\ncriteria\nDescription\nCategory accuracy Ensures that the assigned category levels (1–4) are\nconsistent with the category hierarchy. Level 4 can be\nset to “None” but must be accurate if present\nFilter accuracy Ensures the accuracy and completeness of extracted\nﬁlters, including numerical ranges and boolean ﬂags.\nFilters must be explicitly stated or strongly implied in\nthe query and must adhere to the provided ﬁlter tree\nLocation accuracy Checks whether location extraction is accurate,\nensuring that the ﬁeld is left as “None” if location\ndetails are absent or ambiguous in the query\nKeyword accuracy Assesses whether explicit and implicit keywords are\ncorrectly identiﬁed. Keywords that match a category\nname or ﬁlter should not be counted as explicit\nkeywords. Implicit keywords should be judged in\ncontext but should not be penalized if absent\nSynonym accuracy Evaluates the correctness and relevance of synonyms.\nThere should be no more than one synonym per\nkeyword. If synonyms are absent, it should not be\npenalized. Synonyms should only be provided when\nthey improve search clarity. Minor inaccuracies in\nsynonyms should not be penalized\nCompleteness Ensures that the JSON response contains all required\nﬁelds (categories, ﬁlters, keywords, synonyms, and\nlocation where applicable). “None” values are allowed if\nthe information is not in the query\n/three.tnum./two.tnum Evaluation methods\nTo systematically assess the quality of structured search\nquery parsing outputs, we employ the LLM-as-a-Judge framework\nwith three distinct evaluation methods: Pointwise, Pairwise, and\nPass/Fail evaluation. Each of these methods leverages predeﬁned\nevaluation criteria and structured rating rubrics to ensure\nconsistency, transparency, and alignment with human assessments.\nFor all evaluation methods, we use the evaluation criteria\noutlined in\nTable 1 which ensure that the key aspects—category\naccuracy, ﬁlter accuracy, location accuracy, keyword accuracy,\nsynonym accuracy, and completeness—are systematically assessed.\nFurthermore, each evaluation prompt incorporates few-shot\nexamples to provide the LLM with contextual understanding and\nenable it to generate well-grounded assessments.\nThe general evaluation pipeline is as follows:\n1. Query parsing: a user query is parsed into a structured JSON\nformat as explained in Section 3.1.2.\n2. Evaluation prompt construction: an evaluation prompt is\ngenerated, incorporating the user query, the parsed output, the\nevaluation criteria, the rating rubrics, and few-shot examples.\n3. LLM-based assessment: the LLM-as-a-Judge evaluates the\nparsed output using the selected evaluation method. The LLM\nassigns a score (for Pointwise Evaluation), selects a preferred\nresponse (for Pairwise Evaluation), or classiﬁes the response\nas pass/fail (for Pass/Fail Evaluation), accompanied by an\nexplanation.\n4. Consistency: each evaluation is repeated multiple times with\ndiﬀerent runs. The average value for Pointwise Evaluation and\nFrontiers in Big Data /zero.tnum/six.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nTABLE /two.tnum Evaluation rating rubrics.\nRating Description\nPointwise evaluation rating rubric\n4 (Excellent) The JSON response is completely accurate and\ncomplete, correctly identifying all categories, ﬁlters,\nkeywords (explicit and implicit), synonyms, and\nlocation information (if applicable). It perfectly reﬂects\nthe user’s search intent.\n3 (Good) The JSON response is mostly accurate and complete,\nwith only minor inaccuracies or omissions.\n2 (Fair) Several inaccuracies or omissions impact the overall\nunderstanding of the user’s search intent.\n1 (Poor) The JSON response is largely inaccurate and\nincomplete, failing to capture the essence of the query.\n0 (Unacceptable) The response is completely incorrect or empty.\nPairwise evaluation rating rubric\nA Response A is signiﬁcantly better than Response B.\nB Response B is signiﬁcantly better than Response A.\nSAME Both responses are nearly identical in performance.\nUNCLEAR Neither response accurately reﬂects the user’s search\nintent.\nPass/fail evaluation rating rubric\nPASS The response meets all correctness criteria.\nFAIL The response contains substantial errors or omissions.\nthe majority voting value for the other two evaluation methods\nis used to obtain reliable assessments.\n/three.tnum./two.tnum./one.tnum Use of few-shot examples in evaluation\nprompts\nTo enhance the reliability and reasoning capabilities of\nthe LLM-as-a-Judge framework, we iteratively constructed and\nincorporated few-shot examples into the evaluation prompts. These\nfew-shot examples are entirely independent of the validation\ndataset and were manually crafted to reﬂect a diverse range of user\nqueries and parsing scenarios.\nEach few-shot example consists of a user query, a\ncorresponding structured output (either fully correct, partially\ncorrect, or incorrect), a detailed evaluation of that output based\non the established evaluation criteria, and a ﬁnal judgment (score,\npreference, or pass/fail decision) with justiﬁcation. This structure\nprovides the LLM with contextual grounding and helps calibrate\nits evaluation behavior across diverse parsing outcomes.\nExperiments were conducted using both zero-shot and few-\nshot versions of the evaluation prompts. Initially, evaluation\nwas performed using zero-shot prompts to identify common\nfailure patterns. These insights were then used to iteratively\ndesign targeted few-shot examples that address speciﬁc weaknesses\nobserved in the model’s judgments. This adaptive reﬁnement\ncontinued across multiple iterations, with new few-shot cases\nadded in response to the evaluator errors, particularly focusing\non challenging or ambiguous cases. Separate sets of few-shot\nexamples were maintained and updated for each evaluation method\n(Pointwise, Pairwise, and Pass/Fail), ensuring that the prompting\nremained method-speciﬁc and aligned with the underlying rating\nrubrics.\nThe number of few-shot examples varied by evaluation type\nand category but generally started with around ﬁve examples per\nprompt. As iterations progressed and the evaluator’s weaknesses\nbecame better understood, the number of few-shot examples\nincreased, culminating in ∼ 30 examples in the ﬁnal prompt\nconﬁgurations. This iterative augmentation process signiﬁcantly\nimproved evaluator consistency and performance, as evidenced by\nthe quantitative results reported in Section 4.4.\n/three.tnum./two.tnum./two.tnum Pointwise evaluation\nPointwise evaluation assesses the parsed query outputs by\nassigning a numerical score based on the predeﬁned evaluation\ncriteria and rating rubrics. The rating rubric we used for this\nmethod is shown in\nTable 2. The LLM evaluates the correctness of\nthe parsed query with respect to the user query, assigns a score on\na Likert scale (0–4), and also provides a textual justiﬁcation for its\nrating. A running example of the pointwise evaluation process is\nillustrated in\nFigure 3A. An example evaluation prompt used in the\npointwise evaluation can be found in Supplementary material.\n/three.tnum./two.tnum./three.tnum Pairwise evaluation\nPairwise evaluation compares two diﬀerent parsed outputs for\nthe same query, enabling a direct comparison between diﬀerent\nsearch query parsing models. This method may be particularly\nuseful for assessing performance improvements between traditional\nrule-based systems and LLM-based systems. The LLM determines\nwhich output better meets the evaluation criteria or declares them\nequivalent if both are equally valid or unsatisfactory. The rating\nrubric used for pairwise evaluation is given in\nTable 2. Figure 3B\nshows an example that compares the outputs of two parsing systems\nfor the same query. An example evaluation prompt used in pairwise\nevaluation can be found in\nSupplementary material.\n/three.tnum./two.tnum./four.tnum Pass/fail evaluation\nThe Pass/Fail evaluation simpliﬁes the assessment by\nconverting the evaluation process into a binary classiﬁcation task.\nInstead of assigning scores or making comparative judgments,\nthe LLM assesses whether the parsed output meets the evaluation\ncriteria and classiﬁes it as either “PASS” or “FAIL.” The rating\nrubric is given in\nTable 2. Figure 3C presents a sample Pass/Fail\nevaluation scenario. An example evaluation prompt used in\nPass/Fail evaluation can be found in\nSupplementary material.\n/three.tnum./three.tnum Contextual evaluation prompt routing\nWe propose a method called Contextual Evaluation Prompt\nRouting to improve both the reliability and eﬃciency of LLM-based\nevaluation in structured output tasks. This approach dynamically\nroutes evaluation prompts based on the category of the user query,\nenabling the use of tailored evaluation criteria, category-speciﬁc\nFrontiers in Big Data /zero.tnum/seven.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nFIGURE /three.tnum\nRunning examples for each evaluation method. (A) Pointwise evaluation for an example query. (B) Pairwise evaluation for an example query. (C)\nPassFail evaluation for an example query.\nrating rubrics, and customized few-shot examples aligned with\ndomain-speciﬁc parsing expectations.\nPrior to this approach, we used a single uniﬁed evaluation\nprompt for all query categories as detailed in Section 3.2. The\nprompt included a comprehensive set of evaluation rubrics,\ncriteria, and few-shot examples covering all categories. However,\nexperimental analysis revealed that this one-size-ﬁts-all design\nintroduces several key limitations:\n• First, criteria that are important in one domain (e.g., Location\nAccuracy in real estate category) are not that much important\nin other domains, leading the LLM evaluator to misinterpret\nirrelevant or inapplicable instructions.\n• Second, the inclusion of few-shot examples from unrelated\ndomains increases hallucination risk, as the model might align\nthe evaluation with incorrect reference structures.\n• Third, as we expand the number of few-shot examples to\nimprove performance, the token length of the prompts exceeds\npractical limits (up to 100k tokens), resulting in degraded\nperformance and higher computational cost.\nTo mitigate these issues, the Contextual Evaluation Prompt\nRouting strategy segments the evaluation process into two stages.\nFirst, the structured output’s category_level_1 value is used to\ndetermine the main category of the query. Then, a category-speciﬁc\nevaluation prompt is constructed with\n• Only the relevant evaluation criteria and rating rubric,\n• Tailored few-shot examples that reﬂect the annotation\nstandards of that speciﬁc category, and\n• A more compact and focused prompt length, improving\nLLM interpretability.\nThis strategy oﬀers several advantages. By eliminating\nirrelevant instructions and examples, it reduces hallucinations and\nimproves evaluation accuracy through domain-speciﬁc guidance.\nFrontiers in Big Data /zero.tnum/eight.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nFIGURE /four.tnum\nContextual evaluation prompt routing pipeline.\nThe modular nature of the prompts also signiﬁcantly lowers the\ninference time and computational costs due to shorter input\nlengths. Moreover, isolating category-speciﬁc conﬁgurations allows\nfor the inclusion of a larger number of relevant few-shot examples,\nfurther enhancing the evaluation performance. The overview of the\nrouting mechanism is depicted in\nFigure 4, and a sample routed\nprompt is provided in Supplementary material.\nThe proposed method is conceptually inspired by advances\nin task decomposition and prompt specialization. For instance,\nMulti-Trait Specialization (MTS) (\nLee et al., 2024 ) applies trait-\nspeciﬁc prompts to improve zero-shot performance in essay\nscoring. While MTS addresses unstructured generation tasks,\nour Contextual Evaluation Prompt Routing strategy adapts\nthe underlying principles to the structured output domain of\nsemantic search query parsing.\nKhot et al. (2023) introduce\ndecomposed prompting that modularizes complex tasks into\nsimpler, interpretable components.\nDun et al. (2025) propose\nthe Mixture of Prompts (MoPs) framework, which dynamically\nselects specialized prompt modules based on input characteristics,\nimproving adaptability across heterogeneous tasks. Building on\nthese insights, our routing method oﬀers a scalable, interpretable,\nand eﬃcient alternative to monolithic prompts for evaluation in\nmulti-domain structured output settings.\nWhile related to prior prompt specialization approaches,\nthe proposed method is uniquely designed for structured\nevaluation scenarios that require strict rubric alignment,\ninterpretability, and category consistency. Thus, Contextual\nEvaluation Prompt Routing is not just a heuristic routing solution,\nit also serves as a foundational mechanism for scalable and\nreliable deployment of LLM-as-a-Judge systems in real-world,\nmulti-domain search applications.\nTo validate its eﬀectiveness, we constructed category-speciﬁc\nevaluation subsets, each annotated by domain experts. These\ncontrolled benchmarks revealed that category-specialized routing\nimproves both alignment with human judgments and consistency\nacross repeated assessments. Importantly, the strategy generalizes\nwell to unseen inputs without requiring model ﬁne-tuning,\nsupporting its modularity and adaptability to diverse semantic\nparsing domains. Furthermore, we applied a range of statistical\nmethods, including agreement metrics and error analysis, to\nquantify the reliability and stability of our evaluation framework.\nThe results indicate that using category-specialized prompts\nnot only improves alignment with human assessments but also\nenhances evaluation robustness without introducing the cost\noverhead associated with full-scale model ﬁne-tuning.\nThrough the methodology explained in Sections 3.2 and 3.3, the\nproposed LLM-as-a-Judge framework oﬀers a modular, scalable,\nand semantically accurate evaluation solution for structured search\nquery parsing. By integrating multiple evaluation strategies and\nenhancing them with domain-aware prompt routing, our approach\nachieves high interpretability and consistency while signiﬁcantly\nreducing the need for human evaluation in critical systems.\n/four.tnum Experiments and results\nIn this section, we present the datasets, the evaluation metrics,\nand the experimental setup used for assessing the eﬀectiveness\nof the LLM-as-a-Judge framework in evaluating search query\nparsing systems. Our goal is to systematically compare diﬀerent\nevaluation methods (Pointwise, Pairwise, and Pass/Fail) and\nevaluation techniques (in-context learning, prompt engineering\nwith iterative reﬁnement, human-in-the-loop evaluation, etc.) and\nvalidate their reliability in capturing the accuracy and completeness\nof structured query parsing outputs. In addition, we investigate the\nimpact of the Contextual Evaluation Prompt Routing strategy on\nthe consistency and eﬃciency of the evaluation. The results are\npresented accompanied with ablation studies. We also conduct an\nerror analysis and discuss the limitations of the proposed approach.\n/four.tnum./one.tnum Datasets\nIn this work, we compiled two datasets and conducted the\nexperiments on these datasets.\n/one.tnum\n/one.tnum Since the datasets are at the proprietary of the company, we cannot make\nthe datasets publicly available.\nFrontiers in Big Data /zero.tnum/nine.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\n/four.tnum./one.tnum./one.tnum Small-scale evaluation dataset\nThe ﬁrst dataset consists of 66 queries covering various\nsearch scenarios. These queries were manually created by domain\nexperts and designed to represent diﬀerent category levels, ﬁltering\nattributes, and query complexities commonly encountered in\nsearch systems. Each query was processed by two diﬀerent search\nquery parsers:\n• Rule-based parser: a traditional query parsing system that\nrelies on predeﬁned rules and heuristics to extract structured\ninformation.\n• LLM-assisted parser: a more ﬂexible, context-based parser that\nleverages a large language model to interpret and generate\nstructured query representations.\nFor each query, the structured outputs generated by both\nparsers are manually evaluated and labeled by a human annotator\nwith domain knowledge and background in computer science.\nThe annotator assigns scores to the search query parser outputs\nfor Pointwise evaluations, makes pass/fail decisions for Pass/Fail\nevaluations, and determines which of the two parsing outputs better\ncaptures the search intent for Pairwise evaluations. To address the\nknown position bias issues in Pairwise evaluation, in half of the\ncases the ﬁrst parser output and in the other half of the cases the\nsecond parser output are presented ﬁrst to the LLM evaluator.\nThe annotations are then reviewed by ﬁve domain experts who\nprovide feedback and additional notes to ensure accuracy and\nconsistency. Based on this expert feedback, the annotator revisits\nand reﬁnes the labels, ensuring that the ﬁnal annotations align with\ndomain-speciﬁc expectations and serve as reliable ground-truth\nreferences.\n/four.tnum./one.tnum./two.tnum Large-scale evaluation dataset\nTo further analyze the performance of the LLM-as-a-Judge\nframework at scale, we constructed a more extensive dataset\nconsisting of 600 queries spanning multiple domain-speciﬁc\ncategories. These queries were selected according to search\nfrequency after removing duplicate and highly similar queries\namong the most frequently searched queries on the online classiﬁed\nplatform. These categories align with the main categories used\nin the routing strategy and include “Real Estate, ” “Vehicles, ”\n“Used & Brand New Items, ” “Vehicle Parts, Accessories &\nTuning, ” “Other Categories, ” and “No Category” (Uncategorized\nQueries) categories.\nOne hundred queries were collected for each category, ensuring\na balanced representation of search intents and parsing challenges.\nGiven the high annotation cost of the large-scale dataset and based\non the experimental ﬁndings in the small-scale dataset that show\nthat Pass/Fail evaluation yields more consistent results, the large-\nscale dataset was only annotated for the outputs of the LLM-based\nparser for Pass/Fail evaluation. The annotation process followed a\nsimilar labeling procedure as the small-scale dataset. However, to\naccelerate the manual annotation process, a preliminary annotation\nwas ﬁrst conducted using the best-performing techniques identiﬁed\nin the small-scale dataset. The outputs were then manually\nreviewed and reﬁned by human annotators to ensure high-quality\nground truth labels.\nThis larger dataset enabled a more comprehensive assessment\nof the Contextual Evaluation Prompt Routing strategy, allowing\nus to evaluate how well domain-speciﬁc prompts improve the\naccuracy and reliability of search query parsing evaluations across\ndiﬀerent query characteristics, such as query length, complexity,\nand category-speciﬁc constraints.\n/four.tnum./two.tnum Evaluation metrics\nTo assess the eﬀectiveness of the LLM-as-a-Judge framework,\nwe employed a range of evaluation metrics that measure the\nalignment between LLM-based evaluations and human judgments.\nWe selected a number of proper metrics for each evaluation\nmethod. The metrics were used to quantify both the agreement\nwith human evaluations and the reliability of the LLM-based\nassessment process.\n/four.tnum./two.tnum./one.tnum Agreement metrics\nWe utilized diﬀerent agreement metrics depending on the\nevaluation methodology.\n/four.tnum./two.tnum./one.tnum./one.tnum Pointwise evaluation\nSince Pointwise evaluation involves assigning numerical scores\nto structured query parsing outputs, we used correlation metrics\nbelow to measure the alignment between LLM-generated scores\nand human ratings:\n• Spearman’s rank correlation ( ρ ) was used to assess the\nmonotonic relationship between the rankings of human and\nLLM evaluations. This metric evaluates whether higher scores\nassigned by humans correspond to higher scores assigned by\nthe LLM.\n• Standard deviation across runs reﬂects the average variability\nof scores assigned by the LLM across evaluation runs per\nquery, indicating the consistency of the LLM-as-a-Judge\nsystem for a given setup.\n/four.tnum./two.tnum./one.tnum./two.tnum Pairwise and pass/fail evaluation\nSince these evaluation methods involve categorical decisions\nrather than numerical scores, we used classiﬁcation-based\nagreement metrics:\n• Exact match accuracy was used to measure the percentage\nof instances where the LLM’s decision matched the human\nannotated label. This metric is a simple but eﬀective way to\ncalculate the agreement of categorical evaluations.\n• Cohen’s Kappa ( κ ) was utilized to account for agreement\nbeyond chance, measuring the level of consistency between\nhuman and LLM evaluations while considering the possibility\nof random agreement. This metric is particularly useful for\ncategorical classiﬁcation.\n/four.tnum./two.tnum./two.tnum Reliability metrics\nIn addition to aligning with human annotations, we assessed\nthe internal reliability of the LLM evaluator using statistical\nmeasures that quantify the consistency of its evaluations across\nFrontiers in Big Data /one.tnum/zero.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\ndiﬀerent subsets. Unlike conventional inter-rater agreement\nmetrics, which evaluate the consensus among diﬀerent raters,\nour approach measures the variability of LLM judgments using\nstandard deviation, coeﬃcient of variation, and mean absolute\ndeviation. These metrics provide a robust assessment of how\nconsistently the LLM applies its evaluation criteria across diverse\nquery distributions.\nThe standard deviation (SD) measures how much the\nagreement scores deviate from their mean. A lower standard\ndeviation indicates that the evaluations are consistent and stable,\nwhile a higher value suggests ﬂuctuations in agreement scores,\npointing to inconsistencies in LLM assessments. This metric is\nuseful for understanding the overall dispersion of scores across\ndiﬀerent evaluation runs.\nThe coeﬃcient of variation (CV) normalizes the standard\ndeviation by expressing it as a percentage of the mean, making it\nuseful for comparing variability across diﬀerent datasets. A lower\nCV percentage suggests more stable evaluations, while a higher\npercentage indicates greater variability. Since CV accounts for\ndiﬀerences in scale, it helps in making meaningful comparisons\nbetween categories with varying levels of agreement scores.\nThe mean absolute deviation (MAD) measures the average\nabsolute diﬀerence between the agreement scores and their mean.\nUnlike standard deviation, MAD is less sensitive to extreme\noutliers, making it a robust alternative for measuring variability.\nA lower MAD value suggests that evaluations remain closely\ndistributed around the mean, while a higher value indicates larger\nﬂuctuations and potential inconsistency in LLM-based judgments.\nThis combination of agreement and reliability metrics provides\na comprehensive assessment of the LLM-as-a-Judge framework,\nensuring that the evaluation process is both aligned with human\njudgments and reliable.\n/four.tnum./three.tnum Experimental setup\nIn the experiments, we evaluated the LLM-as-a-Judge\nframework using a range of Gemini\n/two.tnummodels: gemini-1.5-\nflash-001, gemini-1.5-flash-002, gemini-1.5-\npro-001, and gemini-1.5-pro-002. The flash models\nare optimized for eﬃciency, being smaller and faster than the pro\nmodels. Speciﬁcally, gemini-1.5-flash-002 represents an\nupdate over 001. We initially began with Pointwise evaluation,\nwhich is relatively more complex than the other evaluation\nmethods. For this task, we primarily employed the pro models due\nto their higher capacity, and ran each prompt setup multiple times\nto ensure the stability of the results. Given the high computational\ncost of the pro models, we continued with the more cost-eﬃcient\nflash models in the later stages. Additionally, as new Gemini\nmodels were released during the study, we updated the models\nused in our experiments accordingly. As a result, we used the most\nsuitable models in diﬀerent stages of the experiments based on\nthe complexity of the evaluation and the cost considerations. By\ncomparing these models, we aimed to understand the impact of\n/two.tnum Google Gemini:https://deepmind.google/technologies/gemini/, Last\nAccess Date: /zero.tnum/seven.tnum./zero.tnum/four.tnum./two.tnum/zero.tnum/two.tnum/five.tnum.\nmodel size, optimization strategies, and generational advancements\non the framework’s performance.\nWe conﬁgure the temperature and seed parameters during\ninference to ensure controlled and reproducible evaluations.\nThe temperature is set to 0.7, allowing for a balanced degree\nof randomness in token selection while maintaining response\nconsistency. The seed parameter is speciﬁed to enforce\ndeterministic outputs. However, due to the probabilistic nature of\nLLMs, setting a ﬁxed seed does not entirely eliminate the variance\nin responses across multiple runs.\nAlthough LLM evaluation is inherently a deterministic task,\nvariability in token selection may lead to slight inconsistencies\nin the generated evaluations. We perform multiple evaluation\niterations for each experimental setup to mitigate this issue.\nBy running multiple iterations, we ensure the stability of\nLLM evaluations and reduce the impact of randomness\non the assessment metrics. This iterative approach helps\nquantify the robustness of the LLM evaluators across diﬀerent\nevaluation methodologies.\n/four.tnum./four.tnum Results\nThis section presents the results of the experiments for the\nLLM evaluation of search query parsing outputs. We analyze the\nimpact of various evaluation methods, prompt designs, evaluator\nmodels, and reference values on alignment with human judgments\nand evaluation consistency.\nAs stated in Section 4.1, the queries in the small-scale dataset\nwere parsed by both the rule-based parser and the LLM-based\nparser, and the results of both parsers were manually annotated.\nWe thus evaluate the outputs of both parsers using the LLM-\nas-a-Judge framework for the small-scale dataset and compute\nthe agreement scores with the human labels. For Pointwise and\nPass/Fail evaluations the parsed outputs of the two parsers are\nassessed separately, while for Pairwise evaluation the two parsers\nare compared in the same experiment. Since the Contextual\nEvaluation Prompt Routing strategy was tested across all three\nevaluation methodologies (Pointwise, Pairwise, and Pass/Fail), the\nexperiments leveraging this strategy were conducted using the\nsmall-scale validation dataset where both parsers’ outputs were\nannotated. Due to the annotation cost, the large-scale dataset was\nonly used to evaluate the eﬀectiveness of the strategy in the Pass/Fail\nsetting, where only the LLM-based parser’s outputs were labeled.\n/four.tnum./four.tnum./one.tnum Pointwise evaluation results\nTables 3, 4 show the evaluation results of the parsed queries for\nthe two parsers. In all the tables in this section, we include the\nresults only for the gemini-1.5-flash-002 and gemini-\n1.5-pro-002 models which are newer versions of the flash\nand pro models in order not to clutter the tables. The complete\nresults for all the models and conﬁgurations are provided in\nSupplementary material. The LLM evaluation for each query and\nsetting is repeated 10 times and averaged to increase the reliability\nof the scores. The Spearman’s correlation column is the correlation\nbetween the human scores and the LLM scores (averaged). In\nFrontiers in Big Data /one.tnum/one.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\naddition to Spearman’s correlation, Pearson and Kendall’s Tau\ncorrelation metrics were also computed. However, as their results\nwere consistent with the Spearman correlation values and did\nnot provide additional insights, we do not include them in\nthe tables.\nTable 5 shows the interpretation of the Spearman’s\ncorrelation values. The standard deviation column is the standard\ndeviation of the 10 runs. Having multiple runs helps account for\nthe variability in the LLM outputs, ensuring that our evaluation\ncaptures consistency across diﬀerent test cases.\nBasic prompt denotes the prompt in its basic form and is shown\nin\nSupplementary material. Few-shot shows the eﬀect of including\nquery examples in the prompt (Section 3.2.1) to guide the LLM\nevaluation. While evaluating a parser output, the LLM gives a\nscore which is followed by an explanation. Explain ﬁrst shows\nthe results when the LLM is guided to explain the score before\npresenting the score, which was observed to increase the evaluation\nperformance in some works. Separate system prompt refers to a\nprompt design in which the search query parser’s original system\nprompt and the user query are presented in separate sections, rather\nthan being concatenated and given as a single input. This aims to\nmake the evaluation criteria clearer by structurally distinguishing\nbetween system behavior and user input. In order to apply the\nseparate system prompt, the user query should be removed from\nthe Search Query Parser Prompt section in the evaluation prompt\nand written under a separate heading. Reference values indicates\nwhether the gold (human-annotated) reference parsing output is\nprovided within the evaluation prompt. When reference values are\nincluded, the LLM-as-a-Judge can see what the correct structured\noutput should look like, allowing it to make more informed and\naccurate evaluations.\nIt is important to note that there is a signiﬁcant score gap\nbetween the evaluation of the rule-based and LLM-based parser\noutputs. This discrepancy primarily stems from the nature of the\nrule-based parser, which either parses a query very well or very\npoorly due to extensive manual mappings and highly domain-\nspeciﬁc rules. As a result, the LLM evaluator often assigns either\nthe highest or lowest score, which simpliﬁes its decision-making\nand leads to higher agreement with human ratings. In contrast, the\nLLM-based parser tends to produce more nuanced outputs with\npartial correctness. In these borderline cases, the LLM evaluator\nmay either overlook minor errors and assign a high score or\ninterpret them as critical issues and give a low score, both of which\nhinder alignment with human judgment.\n/four.tnum./four.tnum./one.tnum./one.tnum Impact of adding few-shot examples\nTables 3, 4 reveal diﬀerent impacts of few-shot prompting\nacross the two types of parser. For the LLM-based parser, adding\nfew-shot examples does not improve alignment with human scores;\nin fact, a slight drop is observed. For example, in gemini-\n1.5-flash-002, Spearman’s correlation decreases from 0.381\nto 0.364, and in gemini-1.5-pro-002 from 0.490 to 0.445.\nThis decline can be attributed to the fact that the LLM-based parser\nalready produces semantically nuanced outputs. Introducing few-\nshot examples may have led the LLM evaluator to overﬁt to speciﬁc\nreference patterns in the prompt, resulting in misalignment for\nmore ambiguous or borderline cases.\nIn contrast, for the rule-based parser, few-shot prompting\nyields substantial improvements. Correlation increases from 0.564\nTABLE /three.tnum Pointwise evaluation results for LLM-based parser outputs.\nEvaluator\nmodel\nPrompt\ntype\nSpearman’s\ncorrelation\nStandard\ndeviation\nacross runs\ngemini-1.5-\nﬂash-002\nBasic prompt 0.381 0.023\nBasic prompt\n+ few shot\n0.364 0.023\nBasic prompt\n+ few shot\n+ Explain ﬁrst\n0.350 0.184\nBasic prompt\n+ few shot\n+ Separate\nsystem prompt\n0.565 0.020\nBasic prompt\n+ few shot\n+ Reference\nvalues\n0.898 0.024\nPrompt\nrouting\n0.402 0.092\nPrompt\nrouting\n+ few shot\n(initial)\n0.664 0.081\ngemini-1.5-\npro-002\nBasic prompt 0.490 0.061\nBasic prompt\n+ few shot\n0.445 0.464\nBasic prompt\n+ few shot\n+ explain ﬁrst\n0.461 0.453\nBasic prompt\n+ few shot\n+ separate\nsystem prompt\n0.354 0.402\nBasic prompt\n+ few shot\n+ reference\nvalues\n0.858 0.209\nPrompt\nrouting\n0.481 0.181\nPrompt\nrouting\n+ few shot\n(initial)\n0.671 0.151\nto 0.793 in gemini-1.5-flash-002 and from 0.677 to\n0.853 in gemini-1.5-pro-002, representing a clear boost in\nevaluation accuracy. This eﬀect is likely due to the more rigid\nand deterministic nature of rule-based outputs, which align better\nwith the explicit decision templates presented in few-shot examples.\nThe examples provide clear guidance that helps the LLM evaluator\nassess structured, rule-derived outputs more eﬀectively.\n/four.tnum./four.tnum./one.tnum./two.tnum Impact of contextual evaluation prompt routing\nThe addition of Contextual Evaluation Prompt Routing yields\ndiﬀerent eﬀects depending on the parser type. For the LLM-based\nparser, routing alone results in a modest improvement or similar\nperformance over the basic prompt (0.402 and 0.481 Spearman\ncorrelation for flash-002 and pro-002, respectively). When\nFrontiers in Big Data /one.tnum/two.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nTABLE /four.tnum Pointwise evaluation results for rule-based parser outputs.\nEvaluator\nmodel\nPrompt\ntype\nSpearman’s\ncorrelation\nStandard\ndeviation\nacross runs\ngemini-1.5-\nﬂash-002\nBasic prompt 0.564 0.082\nBasic prompt\n+ few shot\n0.793 0.057\nBasic prompt\n+ few shot\n+ separate\nsystem prompt\n0.785 0.015\nBasic prompt\n+ few shot\n+ reference\nvalues\n0.870 0.007\nPrompt\nrouting\n0.371 0.127\nPrompt\nrouting\n+ few shot\n(initial)\n0.309 0.102\ngemini-1.5-\npro-002\nBasic prompt 0.677 0.062\nBasic prompt\n+ few shot\n0.853 0.508\nBasic prompt\n+ few shot\n+ separate\nsystem prompt\n0.711 0.458\nBasic prompt\n+ few shot\n+ reference\nvalues\n0.832 0.492\nPrompt\nrouting\n0.802 0.497\nPrompt\nrouting\n+ few shot\n(initial)\n0.815 0.186\nTABLE /five.tnum Interpretation of Spearman’s rank correlation (adaptedfrom\nDancey and Reidy, /two.tnum/zero.tnum/zero.tnum/seven.tnum).\nSpearman’s rank correlation Interpretation\n≥ 0.70 Very strong relationship\n0.40–0.69 Strong relationship\n0.30–0.39 Moderate relationship\n0.20-0.29 Weak relationship\n0.01–0.19 No or negligible\nrelationshipcombined with category-speciﬁc few-shot examples, the alignment\nfurther improves to 0.664 and 0.671 for the two models,\nrespectively. This reﬂects a transition from moderate to strong\ncorrelation (\nTable 5), suggesting that contextual prompt routing\nhelps the LLM evaluator better understand structured outputs\nthrough more targeted domain-speciﬁc guidance.\nFor the rule-based parser, however, the eﬀect is more nuanced.\nIn the flash-002 model, routing alone or with few-shot\nexamples does not improve performance and in fact leads to a\ndecline (0.371 and 0.309 vs. 0.564 with the basic prompt). This may\nbe due to the simpler and highly rigid outputs of the rule-based\nsystem, which align better with generic evaluation instructions\nrather than segmented category-speciﬁc ones. However, in the\npro-002 model, prompt routing continues to be beneﬁcial,\nachieving 0.802 correlation without few-shot and 0.815 with\ncategory-speciﬁc few-shot examples, both signaling improvement\nover the basic prompt (0.677). These results suggest that routing is\nmore eﬀective when combined with stronger evaluator models that\ncan make use of nuanced prompt variations.\nOverall, the ﬁndings indicate that Contextual Evaluation\nPrompt Routing, especially when paired with few-shot examples, is\na promising approach for increasingrere alignment in LLM-based\nevaluations, particularly for LLM-based parser outputs.\n/four.tnum./four.tnum./one.tnum./three.tnum Impact of evaluator model\nThe choice of the evaluator model signiﬁcantly aﬀects the\nreliability and quality of LLM-based evaluations. Across our\nexperiments, we observed that diﬀerent sizes of the Gemini family\nmodels ( flash-002 and pro-002) demonstrate varying levels\nof correlation with human judgments and stability across runs, even\nunder identical prompting conditions.\nInterestingly, the smaller model gemini-1.5-flash-002\nachieves the highest Spearman correlation among all conﬁgurations\nfor LLM-based parser outputs when reference values are provided\n(0.898), outperforming the larger pro-002 model (0.858).\nSimilarly, in the rule-based parser evaluation, flash-002 reaches\na peak correlation of 0.870, slightly higher than pro-002 (0.832).\nThese results suggest that smaller models can be more eﬀective than\nlarger ones in alignment with human ratings when given strong\nreference cues.\nHowever, flash-002 also shows more pronounced\nvariability across diﬀerent prompting strategies. For example, in\nthe LLM-based parser, Spearman correlation drops to 0.350 with\nthe Explain First prompt, compared to 0.565 with Separate System\nPrompt, indicating greater sensitivity to prompt format. On the\nother hand, pro-002 tends to oﬀer more stable performance\nacross conﬁgurations, with smaller ﬂuctuations in correlation\nvalues between prompt types.\nStandard deviation results further support this observation.\nUnder basic prompt conditions, pro-002 exhibits low variability\n(0.061 and 0.062 for LLM-based and rule-based parsers), whereas\nflash-002 shows very low variability only in its simplest\nconﬁgurations (0.023) but higher variance in others (e.g., 0.184\nin Explain First). This implies that while both models can reach\nstrong alignment with humans under optimal prompting, the\npro-002 model tends to produce more consistent evaluations\nacross prompt types.\nOne notable and consistent observation is that individual\nevaluator models behave similarly when scoring both the LLM-\nbased and rule-based parser outputs. That is, if a model performs\nwell in evaluating the rule-based system, it tends to also\nperform well in the LLM-based system, under the same prompt\nconﬁguration. This suggests that model behavior is inﬂuenced more\nFrontiers in Big Data /one.tnum/three.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nby the evaluator’s internal alignment mechanisms than by the type\nof the parser being evaluated.\nPrompt length is another critical factor. Adding more few-\nshot examples or reference values generally increases prompt\ncomplexity, which can in turn lead to higher output variance.\nFor instance, standard deviation reaches 0.464 with few-shot\nprompts in pro-002 (LLM-based parser), suggesting that model\noutputs become less stable when overwhelmed with too much\ncontextual information.\nNotably, prompt routing appears eﬀective in reducing this\nvariance. In the LLM-based parser evaluations, using prompt\nrouting combined with few-shot examples yields standard\ndeviations of 0.081 ( flash-002) and 0.151 ( pro-002),\nsubstantially lower than in their respective monolithic few-shot\nsetups. These ﬁndings validate our hypothesis that modular,\ncategory-speciﬁc routing mitigates prompt overload and improves\nevaluation stability, especially when tailored few-shot examples are\nsupplied for each domain.\nOverall, these results highlight that while larger models like\npro-002 may oﬀer more predictable performance across settings,\nsmaller models like flash-002 can deliver superior alignment\nwhen supported by strong reference guidance. Ultimately, careful\nbalancing of model size, prompt strategy, and task-speciﬁc routing\nis key to achieving both accurate and consistent LLM-as-a-\nJudge evaluations.\n/four.tnum./four.tnum./one.tnum./four.tnum Impact of reference values\nProviding reference values within the prompt mostly enhances\nalignment with human evaluations. In the LLM-assisted parser\nevaluations, the correlation improved from 0.445 to 0.858, moving\nfrom a strong relationship to a very strong relationship (\nTable 5),\nhighlighting the importance of reference values in guiding the\nevaluation. Additionally, standard deviation values decreased from\n0.464 to 0.209, reinforcing that reference values contribute to more\nstable evaluations.\n/four.tnum./four.tnum./one.tnum./five.tnum Impact of scoring ﬁrst vs. explaining ﬁrst\nWhen the LLM is requested to provide a justiﬁcation for the\nscore before giving the score, the correlation with the human\nscore increases.\nTable 3 shows that correlation increases from 0.445\nto 0.461 for the stronger gemini-1.5-pro-002 model. The\nincrease in correlation suggests that allowing the LLM to rationalize\nits decisions before scoring leads to more thoughtful and accurate\nevaluations. However, the variability in the evaluation runs does not\nfollow a particular pattern.\nAfter observing that the explain-ﬁrst strategy yielded\nconsistently better results than the score-ﬁrst prompting for the\nLLM-based parser, all subsequent evaluations in this study were\nconducted using the explain-ﬁrst strategy.\n/four.tnum./four.tnum./one.tnum./six.tnum Impact of providing system prompt separately\nWe observe that separating the system prompt from the user\nquery, i.e. making the evaluation criteria distinct from the user\ninput, results in lower Spearman correlation scores compared to\nother setups but yields more stable evaluations. This suggests that\nintegrating the system prompt with the evaluation prompt provides\nadditional context that aids assessment and keeping them separate\nimproves consistency across multiple runs.\nTABLE /six.tnum Pairwise evaluation results.\nEvaluator\nmodel\nPrompt\ntype\nExact match\naccuracy\nCohen’s\nKappa\ngemini-1.5-\nﬂash-002\nBasic prompt 0.773 0.635\nBasic prompt\n+ few shot\n(initial)\n0.773 0.632\nBasic prompt\n+ few shot\n(ﬁnal)\n0.879 0.807\nPrompt\nrouting\n0.697 0.508\nPrompt\nrouting\n+ few shot\n(initial)\n0.758 0.607\ngemini-1.5-\npro-002\nBasic prompt 0.758 0.610\nBasic prompt\n+ few shot\n(initial)\n0.758 0.610\nBasic prompt\n+ few shot\n(ﬁnal)\n0.879 0.816\nPrompt\nrouting\n0.742 0.583\nPrompt\nrouting\n+ few shot\n(initial)\n0.803 0.687\nIn particular, for rule-based parser outputs evaluated with\ngemini-1.5-pro-002, using the separated system prompt\nalong with few-shot examples yielded a Spearman correlation of\n0.711. This is lower than other conﬁgurations except the basic\nprompt, indicating that structurally separating prompt sections\nnegatively impacts accurate evaluations.\n/four.tnum./four.tnum./two.tnum Pairwise evaluation results\nTable 6 shows the evaluation results for Pairwise evaluation\nwhere we compare the outputs of the two parsers. As stated in\nSection 4.1.1, the dataset is randomly shuﬄed such that the output\nof the rule-based parser appears in the ﬁrst position in half of\nthe examples and the output of the LLM-based parser appears\nin the ﬁrst position in the other half. The LLM evaluation for\neach query and setting is repeated 5 times and majority voting\nis applied. In Few-Shot (Initial) , we include 3 examples in the\nprompt. After the initial experiment with few-shot examples,\nwe iteratively reﬁne the prompt by analyzing the decisions of\nthe LLM and adding new examples to the prompt according\nto the results of the analysis (Section 3.2.1). Few-Shot (Final)\ndenotes the ﬁnal form of the prompt in which 24 examples are\nused. We used the exact match and Cohen’s Kappa to measure\nthe alignment between human evaluations and LLM evaluations.\nFull evaluation results including experiments with gemini-\n1.5-pro-001 and gemini-1.5-flash-001 are provided in\nSupplementary material for completeness and reference.\nFrontiers in Big Data /one.tnum/four.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nTABLE /seven.tnum The eﬀect of position bias in pairwise evaluation.\nDataset Exact match\naccuracy\nCohen’s Kappa\nDataset with\nshuﬄed pairs\n0.879 0.807\nDataset with\nno-shuﬄed pairs\n0.833 0.639\n/four.tnum./four.tnum./two.tnum./one.tnum Impact of few-shot prompting\nTable 6 shows that, for the gemini-1.5-flash-002\nmodel, using the basic prompt yields an exact match accuracy\nof 0.773 and a Cohen’s Kappa of 0.635. While these results\nseem reasonable, they indicate room for improvement for the\nLLM evaluator in capturing nuanced diﬀerences between the\nsystem outputs. When a large number of examples is included\nin the prompt, the exact match accuracy is increased to 0.879\nand the Cohen’s Kappa to 0.807, demonstrating the eﬀectiveness\nof few-shot learning in reﬁning LLM-based evaluation. When\nswitching from the gemini-1.5-flash-002 model to the\nmore advanced gemini-1.5-pro-002 model using the same\nreﬁned prompt, Cohen’s Kappa increased slightly from 0.807 to\n0.816, suggesting better agreement between the human and the\nLLM compared to random agreement.\nThese results conﬁrm that adding more diverse few-shot\nexamples enhances the evaluation accuracy, allowing the\nmodel to better diﬀerentiate between subtle variations in query\nparsing performance.\n/four.tnum./four.tnum./two.tnum./two.tnum Impact of contextual evaluation prompt routing\nIn addition to standard few-shot prompting, we evaluated the\nimpact of Contextual Evaluation Prompt Routing in the pairwise\nsetting. This strategy routes evaluation prompts based on the\ncategory of the search query to reduce prompt length and improve\nsemantic relevance (see Section 3.3).\nAs shown in\nTable 6, prompt routing without few-shot\nexamples results in lower performance than the basic prompt\nin both flash-002 and pro-002 models. For instance, exact\nmatch accuracy drops from 0.773 to 0.697 in flash-002, and\nCohen’s Kappa decreases from 0.635 to 0.508. This suggests that\nrouting alone, without adequate in-context examples, may reduce\nthe LLM’s ability to generalize comparisons across domains.\nHowever, when routing is combined with even a small number\nof few-shot examples [prompt routing + few shot (initial)],\nperformance may improve substantially. For example, in pro-\n002, exact match accuracy increases to 0.803 and Cohen’s Kappa\nto 0.687, outperforming the basic prompt setup. This conﬁrms\nthat routing and few-shot prompting are complementary in the\nsense that routing helps delivering category-relevant context while\nexamples help guiding ﬁne-grained decision boundaries in the\npairwise comparison task.\nThese ﬁndings align with earlier trends observed in pointwise\nevaluations; routing by itself reduces prompt cluttering and\nimproves stability, but gains are best realized when domain-speciﬁc\nfew-shot guidance is also provided.\nTABLE /eight.tnum Pass/Fail evaluation results for LLM-based parser outputs.\nEvaluator\nmodel\nPrompt type Exact\nmatch\naccuracy\nCohen’s\nkappa\ngemini-1.5-\nﬂash-002\nBasic prompt 0.742 0.282\nBasic prompt\n+ few shot (initial)\n0.727 0.250\nBasic prompt\n+ few shot (ﬁnal)\n0.848 0.659\nPrompt routing 0.894 0.747\nPrompt routing\n+ few shot (initial)\n0.939 0.861\ngemini-1.5-pro-\n002\nBasic prompt 0.727 0.250\nBasic prompt\n+ few shot (initial)\n0.727 0.270\nBasic prompt\n+ few shot (ﬁnal)\n0.848 0.625\nPrompt routing 0.818 0.526\nPrompt routing\n+ few shot (initial)\n0.909 0.780\n/four.tnum./four.tnum./two.tnum./three.tnum Impact of position bias\nPosition bias is a well-known issue in pairwise evaluation,\nwhere models tend to favor responses in a particular position (ﬁrst\nposition or second position). To analyze the eﬀect of position\nbias, we conducted an additional experiment using an unshuﬄed\ndataset. The output of one of the parsers always appears in the ﬁrst\nposition and the LLM is asked to select the better one among the\ntwo outputs. We used the gemini-1.5-flash-002 model and\nthe reﬁned prompt with large number of few-shot examples.\nTable 7\nshows the results. Compared to the original setting where the\norder of the pairs are shuﬄed, the exact match accuracy decreased\nfrom 0.879 to 0.833 and Cohen’s Kappa dropped from 0.807\nto 0.639.\nThis signiﬁcant drop in inter-rater agreement highlights the\nimportance of randomizing the response order in pairwise\nevaluation setups to prevent systematic biases. Without\nshuﬄing, the model may develop an unintended preference\nfor responses in a speciﬁc position, leading to skewed\nevaluation results.\n/four.tnum./four.tnum./three.tnum Pass/fail evaluation results\nTables 8, 9 show the evaluation results for Pass/Fail evaluation\nfor the outputs of the two parsers. The LLM evaluation for each\nquery and setting is repeated 5 times and majority voting is\napplied. Similar to Pointwise evaluation, we started with a basic\nprompt without few shot examples and then used 15 examples\nin the initial few-shot setting. By analyzing the evaluations of\nthe LLM, we reﬁned the prompt to include more examples and\nused 30 examples in the ﬁnal few-shot setting (Section 3.2.1).\nFull evaluation results including experiments with gemini-\n1.5-pro-001 and gemini-1.5-flash-001 are provided in\nSupplementary material for completeness and reference.\nFrontiers in Big Data /one.tnum/five.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nTABLE /nine.tnum Pass/fail evaluation results for rule-based parseroutputs.\nEvaluator\nmodel\nPrompt type Exact\nmatch\naccuracy\nCohen’s\nkappa\nGemini-1.5-ﬂash-\n002\nBasic prompt 0.485 0.040\nBasic prompt\n+ few shot (initial)\n0.894 0.787\nBasic prompt\n+ few shot (ﬁnal)\n0.803 0.593\nPrompt routing 0.909 0.816\nPrompt routing\n+ few shot (initial)\n0.939 0.878\nGemini-1.5-pro-\n002\nBasic prompt 0.848 0.692\nBasic prompt\n+ few shot (initial)\n0.909 0.815\nBasic prompt\n+ few shot (ﬁnal)\n0.955 0.907\nPrompt routing 0.773 0.544\nPrompt routing\n+ few shot (initial)\n0.909 0.816\n/four.tnum./four.tnum./three.tnum./one.tnum Impact of few-shot prompting\nTables 8, 9 show that using a basic prompt without few-shot\nexamples yields low agreement with human labels. When few-\nshot examples are included in the prompt, the agreement scores\nincrease signiﬁcantly for both parsers. Reﬁning the prompt and\nadding more targeted examples further improves alignment with\nhuman decisions. We also tested whether increasing the number\nof examples beyond 30 leads to higher scores, but observed that\nprompt length issues began to reduce eﬀectiveness. This aligns with\nprior ﬁndings indicating that overly long prompts may introduce\nconfusion and hallucination in LLM-based evaluators (\nZhang et al.,\n2024).\n/four.tnum./four.tnum./three.tnum./two.tnum Impact of ﬁne-tuning the LLM evaluator\nTo address the prompt length and inference overhead\nchallenges in few-shot prompting setups, we explored ﬁne-tuning\nas an alternative strategy for enhancing the reliability of LLM-based\nevaluations in an additional experiment. Instead of incorporating\nfew-shot examples directly into the prompt which may lead to\nprompt length constraints, we constructed a supervised dataset\ncontaining 57 manually curated examples, aggregated from the\nfew-shot samples used in earlier prompting experiments. These\nexamples span a range of query parsing outputs across multiple\ncategories and were used to ﬁne-tune the gemini-1.5-flash-\n002 model.\nAlthough the ﬁne-tuned models achieved improvements over\nthe basic prompt setting by raising the exact match accuracy from\n0.742 to 0.794 for the LLM-based parser and from 0.485 to 0.755\nfor the rule-based parser, they could not outperform the few-\nshot prompting conﬁguration.\nTables 8, 9 show that the ﬁnal few-\nshot prompt setup achieved Exact Match Accuracy scores of 0.848\nfor the LLM-based parser and 0.803 for the rule-based parser\nin the gemini-1.5-flash-002 model, surpassing the ﬁne-\ntuned model performance. This indicates that while ﬁne-tuning\nprovides a stable baseline improvement over naive prompting, few-\nshot prompting remains a more eﬀective method, especially when\nsuﬃcient in-context examples can be supplied.\nTo further explore this, we conducted additional experiments\nby combining the ﬁne-tuned models with 30 new few-shot\nexamples (excluded from the ﬁne-tuning set) during evaluation.\nThis hybrid approach oﬀered marginal improvements, suggesting\nthat prompt-based guidance can still help the ﬁne-tuned models\ncontextualize and reﬁne their judgments. However, the overall\nﬁndings reinforce that direct few-shot prompting outperforms\nﬁne-tuning in terms of evaluation accuracy, especially when\ncomputational resources allow for longer prompt lengths.\n/four.tnum./four.tnum./three.tnum./three.tnum Impact of contextual evaluation prompt routing\nGiven that few-shot prompting gives rise to long prompts\nand ﬁne-tuning requires training large models on task-speciﬁc\ndata, we experimented also with prompt routing which is a\nmore scalable approach.\nTable 8 shows that even without few-\nshot examples, routing the prompts based on context improved\nthe exact match accuracy from 0.742 to 0.894 for the LLM-\nbased parser and from 0.485 to 0.909 for the rule-based parser\nfor gemini-1.5-flash-002. When few-shot examples are\nincluded in the prompt, the accuracy further improved to 0.939 for\nboth LLM-based parser and rule-based parser. This conﬁrms that\nprompt routing is a highly eﬀective approach for structured query\nparsing evaluation, even when applied to traditional rule-based\nparsing outputs.\nThese results suggest that prompt routing helps maintain\nprompt clarity while avoiding hallucination issues associated with\nlong few-shot prompts.\n/four.tnum./four.tnum./three.tnum./four.tnum Scaling prompt routing to a larger dataset\nSince Contextual Evaluation Prompt Routing yielded\npromising results, we evaluated its performance on a larger dataset\nthat covers multiple search query categories. Up to this point, all\nthe experiments had been conducted on the small-scale dataset\nconsisting of 66 manually crafted queries. This small dataset was\nused to observe the eﬀect of iterative improvements of the prompt\non a controlled set of complex, long-form queries containing\nexplicit ﬁlters and implicit keywords. By focusing on a small but\ndiverse dataset, we were able to systematically determine the most\neﬀective evaluation approach for the problem. However, given the\nlimited sample size, we need to validate the ﬁndings on a larger\nand more representative test set, ensuring the method’s robustness\nacross real-world search queries.\nTable 10 presents the results on the large-scale dataset which\nconsists of six categories with 100 queries in each category.\nThe column labeled “Initial Few-Shot Prompt” gives the results\nusing the reﬁned prompt obtained in the small-scale dataset\nexperiments. These results show how well the prompt optimized\nfor a smaller manually curated dataset generalizes to a broader set\nof search queries.\nThe small-scale dataset contains long and complex queries\ndesigned to test the system’s ability to extract structured attributes\neﬀectively. However, the larger dataset was constructed from\nthe most frequently searched queries on the online classiﬁed\nads platform and thus the queries are mostly shorter and more\nambiguous, creating a slight distributional shift between the\nFrontiers in Big Data /one.tnum/six.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nTABLE /one.tnum/zero.tnum Contextual evaluation prompt routing evaluation results on\nlarge-scale dataset.\nCategory Initial few shot\nprompt\nImproved few\nshot prompt\nReal estate 0.66 0.91\nVehicles 0.93 0.95\nUsed and brand new\nitems\n0.88 0.97\nVehicle parts,\naccessories and tuning\n0.40 0.94\nOther categories 0.94 0.95\nNo category 0.81 0.87\nEvaluations are made by gemini-1.5-flash-002 model.\ntwo datasets. Due to this shift, evaluations on the large dataset\nusing the original prompt revealed discrepancies in performance,\nhighlighting the need for further reﬁnements.\nTo address this, the prompt routing strategy was adjusted\nto better align with the characteristics of the larger dataset.\nThe column labeled “Improved Few-Shot Prompt” represents the\nresults obtained after modifying the prompts to accommodate for\nshorter and more ambiguous queries. These adjustments involved\nreﬁning the category-speciﬁc few-shot examples and optimizing the\nevaluation instructions to account for real-world search behavior.\nThe largest performance improvement was observed in the\nVehicle Parts, Accessories & Tuning category, where the accuracy\nincreased from 0.40 to 0.94. Similarly, Real Estate queries\nsaw a notable improvement from 0.66 to 0.91, indicating that\ncategory-speciﬁc prompt reﬁnements signiﬁcantly enhance the\nevaluation quality. However, No Category queries remained the\nmost challenging with accuracy peaking at 0.87. This suggests that\nimplicit category assignments are inherently harder to evaluate, as\nthey rely more heavily on contextual inference rather than explicit\nquery signals.\nOverall, these ﬁndings conﬁrm that prompt routing,\nparticularly when combined with category-speciﬁc few-shot\nexamples, provides a robust and scalable evaluation approach.\nThe ability to adapt the prompts to diﬀerent query distributions\nensures that the LLM-as-a-Judge framework remains eﬀective\nacross diverse search environments.\n/four.tnum./four.tnum./three.tnum./five.tnum Reliability measuring\nWe conducted an experiment on the large-scale dataset\nto assess the reliability of the LLM-as-a-Judge system. For\neach category, we randomly selected ﬁve diﬀerent subsets, each\ncontaining 20 samples. Agreement metrics were calculated across\nthese subsets to capture the variability in LLM evaluations. Using\nthese agreement scores, we computed the standard deviation\n(SD), coeﬃcient of variation (CV), and mean absolute deviation\n(MAD) metrics.\nTable 11 presents the variability in the LLM-based agreement\nscores across diﬀerent categories. Lower values indicate more\nconsistent evaluations across the subsets, while higher values\nsuggest greater variability.\nTABLE /one.tnum/one.tnum Reliability scores across categories.\nCategory SD CV (%) MAD\nReal estate 0.042 4.358 0.050\nVehicles 0.060 6.433 0.072\nUsed and brand new items 0.048 5.082 0.058\nVehicle parts, accessories & Tuning 0.050 6.013 0.060\nOther categories 0.075 7.711 0.067\nNo category 0.042 4.450 0.050\nThe results reveal that the Real Estate and No Category\ncategories exhibit the lowest variability, implying that LLM\nevaluations in these domains are relatively stable. This is likely\ndue to well-structured queries in the real estate sector and\nlimited complexity in non-categoric queries, queries without any\ncategory assignment, such as “urgent.” In such queries, it is usually\nsuﬃcient to extract only the explicit keyword, which simpliﬁes the\nproblem. Conversely, the Other Categories category demonstrates\nthe highest variability, indicating signiﬁcant inconsistencies in\nLLM evaluations across the subsets. This can be expected as\nthe category aggregates diverse and sparsely represented queries,\nmaking structured evaluation more challenging.\n/four.tnum./five.tnum Comparisons with related works\nIn the proposed approach, a search query is parsed into a\nstructured form (search query parser output) that is used to\nretrieve the search results from a database. The quality of the\nstructured output cannot be evaluated by executing it and counting\nthe number of returned items, as a large set of results does\nnot necessarily signal relevancy. For instance, omitting parsing\naltogether and performing a simple keyword search may yield\na broad set of results, but many of them would not reﬂect the\nuser’s actual intent. In such a setup, the only reliable way to\nassess the quality of a parsed output is through human evaluation,\nexamining whether the structured output accurately represents the\nuser’s semantic intent. However, manual evaluation of thousands\nof structured outputs is prohibitively time-consuming and costly.\nTherefore, we adopt the LLM-as-a-Judge framework as an eﬃcient\nand semantically robust alternative. In assessing the performance of\nthe proposed evaluation framework, we use correlation with human\njudgments as the evaluation metric. Traditional automated metrics\n(e.g., BLEU) or heuristic-based baselines are not directly applicable\nin our setting due to the nature of the search pipeline and the\nsparsity of ground truth labels at scale.\nWhile our primary comparison is against expert human\nannotation on a small-scale validation set, we acknowledge the\nneed to contextualize our results within the broader literature.\nSeveral recent studies have validated the eﬀectiveness of LLM-based\nevaluation methods against human preferences and established\ntheir superiority over traditional baselines:\n•\nRaju et al. (2024) introduce a domain-speciﬁc benchmark to\nevaluate LLMs as judges, achieving a Spearman correlation\nFrontiers in Big Data /one.tnum/seven.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nof 0.915 with human judgments, substantially outperforming\nexisting baselines such as AlpacaEval 2.0 LC (0.297). Their\nstudy also reports 84% agreement with Chatbot Arena results,\ndemonstrating that well-designed evaluation prompts tailored\nto the task domain can lead to highly reliable assessments.\n• TALEC (\nZhang et al., 2024 ) proposes a framework for\ntraining LLM evaluators with task-speciﬁc criteria, reporting\nSpearman correlations of 0.96–0.97 for tasks such as sentiment\nanalysis and title generation. The study also highlights that\naverage correlation with human judgment exceeds 0.80, often\nsurpassing inter-human agreement in subjective tasks.\n•\nZheng et al. (2023) , use the LLM-as-a-Judge strategy with\nMT-Bench and Chatbot Arena and report that GPT-4\nachieves 85% agreement with human experts, which is\nhigher than the 81% agreement among human annotators\nthemselves. This reinforces the reliability of advanced LLMs\nlike GPT-4 as scalable surrogates for human judgment in\nevaluation pipelines.\nIn our study, we similarly observe around 90% agreement\nbetween the LLM-based evaluations and human annotations using\ndiﬀerent prompt conﬁgurations. These results are in line with the\nﬁndings in the literature and conﬁrm that LLM-as-a-Judge can\nserve as a reliable and scalable alternative to manual evaluation for\nstructured output tasks in search systems.\nFinally, we emphasize that while real-time A/B testing metrics\nsuch as click-through rate (CTR) and exit rate will eventually\nserve as automated feedback signals in production, these signals\nare not currently accessible during the oﬄine development phase.\nAs such, traditional automated baselines cannot be employed\nto evaluate the eﬀectiveness of query parsing outputs at scale\nprior to deployment. In this setting, LLM-as-a-Judge serves as\nan indispensable evaluation mechanism, oﬀering a scalable and\nsemantically grounded alternative to manual annotation.\n/four.tnum./six.tnum Error analysis\nIn this section, we brieﬂy mention some error cases and\nlimitations of the proposed framework. One issue is the tendency\nof the search query parser to generate hallucinated outputs. The\nparser may assign categories that do not exist in the predeﬁned\ncategory taxonomy or it may fail to make category assignments\nat the appropriate hierarchical level and assigns categories at a\nmore granular level than required. Although LLM-as-a-Judge is\ngenerally eﬀective in assessing the accuracy of the structured\noutputs, it exhibits improper evaluations in cases where such\nhallucinations occur. The main reason for these errors is that LLM-\nas-a-Judge lacks reference ground truth values, relying solely on the\ninstructions provided in the system prompt of the search query\nparser. When the prompt does not contain suﬃciently detailed\nexplanations, the evaluation process becomes susceptible to errors\nas the model has no alternative means of verifying correctness.\nAnother limitation of the framework involves domain-speciﬁc\nsearch requirements. In cases of incorrect category matching, the\nmodel for instance may misclassify a term such as “golf” under\nsports rather than identifying it as a car model (Volkswagen Golf).\nIn such cases, LLM-as-a-Judge struggles to detect misclassiﬁcations,\nleading to erroneous evaluations. Furthermore, the model exhibits\ndiﬃculty in recognizing implicit information within the search\nqueries. For example, the word “paint-free” implies that a vehicle\nis undamaged and the ﬁlters should be extracted according to this.\nHowever, the search query parser fails to infer this meaning and\nLLM-as-a-Judge does not correctly ﬂag the response as erroneous.\nThese ﬁndings suggest that enhancing the system prompt\nwith more detailed explanations and incorporating domain-\nspeciﬁc knowledge 5improve the reliability of LLM-as-a-Judge in\nevaluating search query parser outputs.\n/five.tnum Conclusion\nIn this paper, we introduced LLM-as-a-Judge, a general\nframework for evaluating structured outputs, with a speciﬁc\nfocus on search query parsing in an online classiﬁeds platform.\nUnlike traditional evaluation methods, the proposed approach\nleverages LLMs’ reasoning abilities to assess structured outputs\nmore eﬀectively, ensuring context-aware, interpretable, and\nscalable evaluations. We proposed three evaluation methodologies,\nPointwise, Pairwise, and Pass/Fail, to cover diﬀerent assessment\nneeds, and we further enhanced reliability and eﬃciency with the\nContextual Evaluation Prompt Routing strategy which dynamically\nadjusts evaluation prompts based on query categories. To validate\nthe framework, we conducted experiments on two datasets which\nare a small, manually curated dataset and a large, real-world dataset.\nThe small dataset enabled us to iteratively reﬁne our evaluation\nprompts and methodologies, while the large dataset allowed us to\ntest the scalability of the prompt routing approach. The ﬁndings\nconﬁrmed that routing the prompts based on context of the query\nsigniﬁcantly improves the evaluation accuracy, particularly in\ncategory-speciﬁc few-shot prompting. Also, the reliability analysis\nshowed that this approach is highly eﬀective for well-deﬁned, high-\ntraﬃc categories, while more ambiguous queries require further\noptimization to achieve consistent evaluations.\nThe experimental results highlighted key insights into the\nperformance of diﬀerent evaluation techniques, some of which are\noutlined below:\n• Pointwise evaluation: across both LLM-based and rule-\nbased parser outputs, few-shot prompting generally improved\nalignment with human scores, as reﬂected by increased\nSpearman’s correlation. For example, in the rule-based parser,\ncorrelation rose from 0.564 to 0.793 ( flash-002) and from\n0.677 to 0.853 ( pro-002). Incorporating reference values\nfurther boosted alignment, achieving correlation scores of up\nto 0.898 for the LLM-based parser and 0.870 for the rule-\nbased parser.\n• Pairwise evaluation: by adding few shot examples to the\nprompt we improved the alignment from 0.773/0.758 to\n0.879 for both models. Also, we mitigated position bias by\nrandomizing the order of the outputs of the two parsers,\nimproving exact match accuracy from 0.833 to 0.879.\n• Pass/fail evaluation: the prompt routing method achieved\n0.939 exact match accuracy and 0.861 Cohen’s Kappa\n(flash-002) on the small-scale dataset and 0.87–0.97 exact\nFrontiers in Big Data /one.tnum/eight.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nmatch accuracy on the large-scale real world dataset for the\nLLM-based parser, demonstrating its eﬀectiveness in binary\nclassiﬁcation tasks.\nAs future work, we aim at improving the reliability of the\nevaluation of ambiguous queries, speciﬁcally those without a clear\ncategory and those outside the four main categories. This will\ninvolve expanding the prompt diversity by incorporating a broader\nrange of query formulations and optimizing prompt routing\nstrategies for these underrepresented categories. Additionally, we\nplan to explore alternative LLM architectures and ﬁne-tune the\nmodels with domain-speciﬁc evaluation data to further enhance\nalignment with human judgments. Another key direction is\nautomating the reﬁnement process by iteratively identifying failure\ncases and adapting the evaluation prompts dynamically. Also, we\nwill investigate cross-domain generalization, applying the LLM-\nas-a-Judge framework to other structured output evaluation tasks\nbeyond search query parsing. Lastly, once the system is live in\nthe future, we plan to use live A/B testing metrics as a post-\ndeployment baseline to measure the practical eﬀectiveness of the\nLLM-as-a-Judge evaluations. These real-world metrics will allow\nus to retrospectively validate and calibrate the judgments made\nby our LLM-based evaluation framework, providing a closed-loop\nmechanism that combines human-aligned semantic assessment\nwith behavioral user signals from production.\nData availability statement\nThe datasets presented in this article are not readily\navailable because the search datasets are at the proprietary\nof the company, we cannot make the datasets publicly\navailable. Requests to access the datasets should be directed\nto\nmehmet.baysan@sahibinden.com.\nAuthor contributions\nMB: Writing – original draft, Conceptualization, Methodology,\nSoftware. SU: Writing – original draft, Conceptualization,\nMethodology, Software. ˙I˙I: Writing – review & editing. ÇÇ: Writing\n– review & editing. TG: Writing – review & editing, Supervision.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research and/or publication of this article.\nConﬂict of interest\nMB, ˙I˙I and ÇÇ were employed at sahibinden.com, Istanbul. SU\nwas employed at sahibinden.com, Ankara.\nThe remaining author declares that the research was conducted\nin the absence of any commercial or ﬁnancial relationships that\ncould be construed as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Gen AI was used in the creation\nof this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/fdata.2025.\n1611389/full#supplementary-material\nReferences\nCai, S., and Knight, K. (2013). “Smatch: an evaluation metric for semantic\nfeature structures, ” in Proceedings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers) , eds. H. Schuetze,\nP. Fung, and M. Poesio (Soﬁa: Association for Computational L inguistics),\n748–752.\nChiang, D. C., and Lee, H. (2023). “A closer look into using large\nlanguage models for automatic evaluation, ” in Findings of the Association\nfor Computational Linguistics: EMNLP 2023, Singapore, December 6-1 0,\n2023, eds. H. Bouamor, J. Pino, and K. Bali (Singapore: Association for\nComputational Linguistics), 8928–8942. doi: 10.18653/v1/ 2023.ﬁndings-\nemnlp.599\nDancey, C. P., and Reidy, J. (2007). Statistics Without Maths for Psychology. London:\nPearson education.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. (20 24). Length-\ncontrolled alpacaeval: a simple way to debias automatic evaluators. arXiv [Preprint] .\narXiv:2404.04475. doi: 10.48550/arXiv.2404.04475\nDun, C., Garcia, M. H., Zheng, G., Awadallah, A. H., Kyrillidis, A. , and Sim, R.\n(2025). Sweeping heterogeneity with Smart MoPs: mixture of pr ompts for LLM task\nadaptation. arXiv [Preprint]. arXiv:2310.02842. Available online at: https://arxiv.org/\nabs/2310.02842\nGu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., et al. (2025 ). A survey on LLM-as-\na-Judge. arXiv [Preprint]. arXiv:2411.15594. Available online at: https://arxiv.org/abs/\n2411.15594\nJiang, P., and Cai, X. (2024). A survey of semantic parsing tec hniques. Symmetry\n16:1201. doi: 10.3390/sym16091201\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., et al. (2023).\n“Decomposed prompting: a modular approach for solving complex tasks, ” in The\nEleventh International Conference on Learning Representations . Available online at:\nhttps://openreview.net/forum?id=_nGgzQjzaRy\nLee, C., Gottschlich, J., and Roth, D. (2021). Toward code gene ration: a\nsurvey and lessons from semantic parsing. arXiv [Preprint]. arXiv:2105.03317.\ndoi: 10.48500/arXiv.2105.03317\nFrontiers in Big Data /one.tnum/nine.tnum frontiersin.org\nBaysan et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fdata./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/one.tnum/three.tnum/eight.tnum/nine.tnum\nLee, S., Cai, Y., Meng, D., Wang, Z., and Wu, Y. (2024). “Unleas hing large\nlanguage models’ proﬁciency in zero-shot essay scoring, ” in Findings of the Association\nfor Computational Linguistics: EMNLP 2024 , eds. Y. Al-Onaizan, M. Bansal, and\nY.-N. Chen (Miami, FL: Association for Computational Lingui stics), 181–198.\ndoi: 10.18653/v1/2024.ﬁndings-emnlp.10\nLi, H., Dong, Q., Chen, J., Su, H., Zhou, Y., Ai, Q., et al. (2024 ). Llms-as-\njudges: a comprehensive survey on LLM-based evaluation metho ds. arXiv [Preprint] .\narXiv:2412.05579. doi: 10.48550/arXiv.2412.05579\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., et al. (2024).\nFrom Crowdsourced Data to High-quality Benchmarks: Arena-hard and Benc hbuilder\nPipeline.\nLuo, C., Goutam, R., Zhang, H., Zhang, C., Song, Y., Yin, B., et al. (2023).\n“Implicit query parsing for product search, ” in SIGIR ’23: Proceedings of the\n46th International ACM SIGIR Conference on Research and Development\nin Information Retrieval (New York, NY: ACM). doi: 10.1145/3539618.35\n91858\nNiu, T., Joty, S., Liu, Y., Xiong, C., Zhou, Y., Yavuz, S., et a l. (2024).\nJudgerank: leveraging large language models for reasoning-int ensive\nreranking. arXiv [Preprint]. arXiv:2411.00142. doi: 10.48550/arXiv.2411.\n00142\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). “Ble u: a\nmethod for automatic evaluation of machine translation, ” in Proceedings\nof the 40th Annual Meeting of the Association for Computational\nLinguistics, eds. P. Isabelle, E. Charniak, and D. Lin (Philadelphia, PA:\nAssociation for Computational Linguistics), 311–318. doi: 10.3115/1073083.10\n73135\nRaju, R., Jain, S., Li, B., Li, J., and Thakker, U. (2024). Con structing domain-\nspeciﬁc evaluation sets for llm-as-a-judge. arXiv [Preprint] . arXiv:2408.08808.\ndoi: 10.48550/arXiv.2408.08808\nSchneider, P., Klettner, M., Jokinen, K., Simperl, E., and Matt hes, F. (2024).\n“Evaluating large language models in semantic parsing for conver sational question\nanswering over knowledge graphs, ” in Proceedings of the 16th International Conference\non Agents and Artiﬁcial Intelligence, ICAART 2024, Volume 3, Rome, It aly, February 24–\n26, 2024 , eds. A. P. Rocha, L. Steels, and H. J. van den Herik (SCITEPRES S), 807–814.\ndoi: 10.5220/0012394300003636\nSchroeder, K., and Wood-Doughty, Z. (2024). Can you trust LL M\njudgments? Reliability of Llm-as-a-judge. arXiv [Preprint] . arXiv:2412.12509.\ndoi: 10.48550/arXiv.2412.12509\nShankar, S., Zamﬁrescu-Pereira, J. D., Hartmann, B., Param eswaran, A. G., and\nArawjo, I. (2024). “Who validates the validators? Aligning LLM -assisted evaluation\nof LLM outputs with human preferences, ” in Proceedings of the 37th Annual ACM\nSymposium on User Interface Software and Technology, UIST 2024, Pi ttsburgh, PA, USA,\nOctober 13-16, 2024 , eds. L. Yao, M. Goel, A. Ion, and P. Lopes (New York, NY: ACM),\n131:1–131:14. doi: 10.1145/3654777.3676450\nZhang, K., Yuan, S., and Zhao, H. (2024). TALEC: teach your LL M to evaluate in\nspeciﬁc domain with in-house criteria by criteria division a nd zero-shot plus few-shot.\narXiv [Preprint]. arXiv:2407.10999. doi: 10.48550/arXiv:2407.10999\nZheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., et al.\n(2023). “Judging LLM-as-a-judge with mt-bench and chatbot arena, ” in Advances in\nNeural Information Processing Systems 36: Annual Conference on Neural Info rmation\nProcessing Systems 2023, NeurIPS 2023 , eds. A. Oh, T. Naumann, A. Globerson, K.\nSaenko, M. Hardt, and S. Levine (New Orleans, LA).\nFrontiers in Big Data /two.tnum/zero.tnum frontiersin.org",
  "topic": "Parsing",
  "concepts": [
    {
      "name": "Parsing",
      "score": 0.8001078367233276
    },
    {
      "name": "Computer science",
      "score": 0.7418822050094604
    },
    {
      "name": "Natural language processing",
      "score": 0.5361843705177307
    },
    {
      "name": "Web search query",
      "score": 0.522726833820343
    },
    {
      "name": "Information retrieval",
      "score": 0.503462016582489
    },
    {
      "name": "RDF query language",
      "score": 0.4828372597694397
    },
    {
      "name": "Query language",
      "score": 0.46756917238235474
    },
    {
      "name": "Programming language",
      "score": 0.4440130889415741
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4438815116882324
    },
    {
      "name": "Query optimization",
      "score": 0.426119863986969
    },
    {
      "name": "Query expansion",
      "score": 0.4250248074531555
    },
    {
      "name": "Search engine",
      "score": 0.2846975326538086
    },
    {
      "name": "Web query classification",
      "score": 0.21849769353866577
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210088664",
      "name": "Turkish Society of Hematology",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I4405392",
      "name": "Boğaziçi University",
      "country": "TR"
    }
  ]
}