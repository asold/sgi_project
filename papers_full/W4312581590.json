{
  "title": "Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models",
  "url": "https://openalex.org/W4312581590",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A275481428",
      "name": "Kamel Gaanoun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2914288810",
      "name": "Mohammed Alsuhaibani",
      "affiliations": [
        "Qassim University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6777222205",
    "https://openalex.org/W3030030185",
    "https://openalex.org/W349534049",
    "https://openalex.org/W2096451472",
    "https://openalex.org/W3152503596",
    "https://openalex.org/W2922081354",
    "https://openalex.org/W2912185421",
    "https://openalex.org/W6769263558",
    "https://openalex.org/W6790837184",
    "https://openalex.org/W6791397626",
    "https://openalex.org/W3116641301",
    "https://openalex.org/W2796251561",
    "https://openalex.org/W2306024562",
    "https://openalex.org/W2070493638",
    "https://openalex.org/W2020278883",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W6745609711",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6770212971",
    "https://openalex.org/W6774526564",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W6803495051",
    "https://openalex.org/W2141014056",
    "https://openalex.org/W2135790056",
    "https://openalex.org/W3038050353",
    "https://openalex.org/W2626112078",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W3003681195",
    "https://openalex.org/W3016441316",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2076063813",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W1972949070",
    "https://openalex.org/W6748922080",
    "https://openalex.org/W6758127462",
    "https://openalex.org/W6796797796",
    "https://openalex.org/W6792882139",
    "https://openalex.org/W4210432041",
    "https://openalex.org/W2233875610",
    "https://openalex.org/W3160185768",
    "https://openalex.org/W2971943357",
    "https://openalex.org/W2896092725",
    "https://openalex.org/W2572282666",
    "https://openalex.org/W3086863200",
    "https://openalex.org/W4226204329",
    "https://openalex.org/W2766044727",
    "https://openalex.org/W2981052445",
    "https://openalex.org/W2517045473",
    "https://openalex.org/W2063904635",
    "https://openalex.org/W2933299603",
    "https://openalex.org/W2737990573",
    "https://openalex.org/W2974335209",
    "https://openalex.org/W6607259140",
    "https://openalex.org/W3000973331",
    "https://openalex.org/W2165612380",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2792365648",
    "https://openalex.org/W2544841572",
    "https://openalex.org/W1558982577",
    "https://openalex.org/W2142827986",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4288640753",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3166673781",
    "https://openalex.org/W2187420056",
    "https://openalex.org/W3146946303",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2768348081",
    "https://openalex.org/W1535736950",
    "https://openalex.org/W4393839444",
    "https://openalex.org/W3149006903",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3133440961",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3134155512",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2790563929",
    "https://openalex.org/W3026118719",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W179179905"
  ],
  "abstract": "Muslims rely primarily on the Quran and the Hadiths in all their spiritual life and consider them as sacred sources. If the Quran is God&#x2019;s word, then the Hadiths are God&#x2019;s instructions in the words of the Prophet Muhammad. Since Hadiths are transmitted through multiple narrators, they have been extensively studied to ensure their authenticity. The purpose of this study is to detect fabricated Hadiths, or Mawdu, which is the type of Hadith most rejected by Muslim scholars. The study utilises the central text and content of Hadith, Matn, rather than solely focusing on Hadith chain of narrators, Sanad. In order to accomplish this, we create and release the first dataset dedicated to Mawdu Hadiths, called MAHADDAT. Furthermore, we set up a Mawdu Hadith (MH) detection system based on a transformer language model, BERT, achieving a 92.47&#x0025; <inline-formula> <tex-math notation=\"LaTeX\">$\\text{F}1_{MH}$ </tex-math></inline-formula> score.",
  "full_text": "Received 7 October 2022, accepted 21 October 2022, date of publication 26 October 2022, date of current version 3 November 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3217457\nFabricated Hadith Detection: A Novel Matn-Based\nApproach With Transformer Language Models\nKAMEL GAANOUN\n 1 AND MOHAMMED ALSUHAIBANI\n 2\n1Association for Business Intelligence (AMID), Beni Mellal 23000, Morocco\n2Department of Computer Science, College of Computer, Qassim University, Buraydah 52571, Saudi Arabia\nCorresponding author: Mohammed Alsuhaibani (m.suhibani@qu.edu.sa)\nThe researchers would like to thank the Deanship of Scientiﬁc Research, Qassim University for funding the publication of this project.\nABSTRACT Muslims rely primarily on the Quran and the Hadiths in all their spiritual life and consider them\nas sacred sources. If the Quran is God’s word, then the Hadiths are God’s instructions in the words of the\nProphet Muhammad. Since Hadiths are transmitted through multiple narrators, they have been extensively\nstudied to ensure their authenticity. The purpose of this study is to detect fabricated Hadiths, or Mawdu,\nwhich is the type of Hadith most rejected by Muslim scholars. The study utilises the central text and content\nof Hadith, Matn, rather than solely focusing on Hadith chain of narrators, Sanad. In order to accomplish\nthis, we create and release the ﬁrst dataset dedicated to Mawdu Hadiths, called MAHADDAT. Furthermore,\nwe set up a Mawdu Hadith (MH) detection system based on a transformer language model, BERT, achieving\na 92.47% F1 MH score.\nINDEX TERMS Arabic NLP, BERT, fabricated Hadiths, Hadtih authentication, Mawdu, transformers.\nI. INTRODUCTION\nThe development of automated classiﬁcation systems has\nbeen necessitated recently due to the rapid increase in data.\nThe majority of the data is presented as text. Hence the need\nfor text classiﬁcation which involves grouping texts into one\nor more pre-established categories or classes. Such a process\nmay organize, arrange, and categorize virtually any sort of\ntext, including ﬁles, documents, and text from the internet.\nFor instance, news can be classed by its authenticity [1],\narticles by topic [2], service requests by urgency [3], and\nsocial media status by sentiment [4], to name a few examples.\nIn reality, the work in the area of text classiﬁcation is\nmostly applied to English language texts using Machine\nLearning (ML) algorithms, with relatively much fewer works\nin other natural languages [5]. Even though Arabic is among\nthe top ﬁfth spoken languages in the world with more than\n20 countries having it as the ofﬁcial language and more\nthan 400 million native and non-native speakers [6], there\nare fewer attempts of classifying Arabic texts. It is even\nconsiderably less when it comes to religious-related Arabic\ntexts. One of the two pillars of Islam, together with the holy\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Yiming Tang\n.\nQuran (i.e. the sacred scripture of Islam), is Hadith (i.e. the\nsayings of Prophet Muhammad Peace Be Upon Him (PBUH))\nand both are the principal references for more than 1.5 billion\nMuslims around the world [7].\nArabic word Hadith literally means the discourse of a\nperson. A recount of the teachings, acts, and sayings of the\nProphet Muhammad PBUH is what Hadith means in the\ncontext of religion [8], [9]. Sanad (chain of narrators back\nto the Prophet) and Matn (the actual content and central text)\nare the names of the two Hadith’s primary components. They\ncombine to form the fundamental elements of each Hadith.\nTo organise Hadiths according to their topics (a.k.a Hadith\nclassiﬁcation), scholars began classifying Islamic literature\nin antiquity [10]. Scholars have also paid a great deal of\nthought to deciding the authenticity of Hadith. Consequently,\nrules and procedures were devised for achieving that goal of\ndetermining the authenticity degree of Hadith (a.k.a. Hadith\nauthentication) as Sahih (accurate or correct), Hasan (good),\nDaif (weak) or Mawdu (Fabricated) [11]. Mawdu Hadiths\n(MHs) are considered to be the worst among non-authentic\nHadiths, because they are made-up, manipulated or fabricated\nHadiths that are falsely attributed to the Prophet PBUH [12].\nFrom ML and Natural Language Processing (NLP) per-\nspectives, Hadith classiﬁcation and authentication can be\n113330 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 10, 2022\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nviewed as text classiﬁcation practices, since they are both\nconcentrating on classifying Arabic text into some pre-\ndeﬁned classes. However, in Hadith science, the classiﬁca-\ntion of Hadith according to the topics known as Tasneef\nAl Hadith whereas Takhreeg Al Hadith is the reference for\nthe authentication of Hadith and assigning its authenticity\ndegree. As such, the work presented in this paper will be\nsolely on Hadith authentication with a particular focus on\ndetecting MHs.\nHadith scholars consult both the Sanad and Matn to estab-\nlish the reliability of a certain Hadith. Because the narrators\nin the Sanad must not be disconnected, researchers examine\neach narrator’s status to determine whether or not they are\nconsistently trustworthy and connected [13]. Hadith scholars\nalso examine Hadith’s Matn to see if it agrees or disagrees\nwith the grammar of Arabic, authentic Hadiths, or what is\nbeing said in the Quran. Because there is a possibility that\nMatn uses unsuitable language or expressions that do not\nalign with Muslims beliefs, authentic Hadiths or the discourse\nof the Prophet Muhammad PBUH [14], [15], [16] which in\nthat case falls under the deﬁnition of MHs.\nThe last few years have witnessed some efforts in the liter-\nature in regards to automatically authenticating Hadiths [15],\n[16], [17], [18], [19], [20]. Such efforts solely focus on the\nSanad in order to assign the Hadith’s degree of authenticity.\nMoreover, almost all the available studies focus on authen-\nticating whether a Hadith is Sahih or not, none were solely\nfocusing on studying the nature of the MHs nor building the\nauthentication process around it, which, we argue, is a very\nvital way to look into the problem.\nIndeed, the Sanad is there to explain the validity of Hadith.\nNonetheless, nowadays, the majority of Muslims reference\nHadiths without stating their Sanad, in contrast to the early\nIslamic era [14]. Additionally, with more people having\naccess to the internet and social media nowadays, the prob-\nlem has grown because the number of fabricated Hadiths\nis steadily increasing. Therefore, it is critically important to\nstudy the problem of Hadith authentication considering the\nMatn, which is the gap that is being addressed in this paper.\nML algorithms have been the foremost and sole tech-\nnique in all the previously presented studies related to\nHadith authentication as highlighted in the related work\nsection next. However, most recently, ML and NLP have\nseen unprecedented breakthroughs with the introduction\nof Transformer Language Models (TLMs). Such TLMs\ninclude Bidirectional Encoder Representations from Trans-\nformers (BERT) [21], Generative Pre-trained Transformer\n(GPT-3) [22], XLNet [23] and Robustly Optimized BERT\n(RoBERTa) [24]. TLMs report the state-of-the-art results in a\nlarge number of ML and NLP tasks [25], [26], [27], [28].\nStill, there is yet to be any study utilising TLMs to deal\nwith Hadith authentication. Therefore, the work presented in\nthis paper takes the advantage of utilising all available Arabic\nTLMs to study Hadith authentication for the ﬁrst time.\nWe propose an approach using various Arabic TLMs\nto deal with Hadith authentication using the central\ntext of Hadiths, the Matn. The aim is to automati-\ncally detect MHs using the Matn. In particular, we have\nutilised AraBERTv2 [29], Arabic-BERT [30], QARIB [31],\nCAMeLBERT MSA [32], CAMeLBERT CA [32], mBERT\nbase [21] and XLM-RoBERTa base [33] centring to identify\nthe MHs. The contribution of the paper can be summarised as\nfollows:\n• A Matn-based MH detection system taking into account\nstudying and understanding the central text and content\nof Hadith, rather than solely focusing on the chain of\nnarrators.\n• Exploiting all the available Arabic TLMs to automati-\ncally authenticate Hadiths. To the best of our knowledge,\nthis is the very ﬁrst work considering TLMs for such a\ntask.\n• Proposing two new datasets, called NAH Plus and\nMAHADDAT, with broad discussion of the creation\nphases, analysis and statistics prior to the thorough\nexperiments. Both datasets are released and publicly\navailable.\n• A main focus to detect MHs and studying their natures\namong all Hadiths rather than only concentrating on\nclassifying Hadith as Sahih or Daif.\n• A comprehensive comparison study between numerous\nclassical ML algorithms and Arabic TLMs in Hadith\nauthentication performance.\nThe proposed model together with the thorough experiments\nreveals that employing Arabic TLMs for Hadith authentica-\ntion and detecting MHs is fully justiﬁed by reporting the\nstate-of-the-art results with several metrics and evaluation\nmethods.\nThe rest of the paper is organized as follows: section II\npresents the related work, section III mainly describes the\ndata creation, collection and processing steps, section IV is\ndedicated to the classiﬁcation methodology, and section V\nintroduces the experiments and results. The results are dis-\ncussed in section VI. Finally we present our conclusions\nalong with some future directions in section VII.\nII. RELATED WORK\nHadiths, together with the Holy Quran, serve as Muslims’\nprimary source of law, hence authenticating Hadiths is essen-\ntial. It is as well equally crucial to classify Hadiths into\ngroups or topics to make them simpler to search for and\nidentify. The problems of Hadith classiﬁcation and authen-\ntication can be resolved using a variety of NLP techniques.\nNevertheless, relatively not many have looked into this in the\nliterature. Although this paper’s work focuses primarily on\nHadith authentication and detecting MHs, we sought to take\na broader approach and review some prior research in Hadith\nclassiﬁcation.\nOne of the early works to computationalise the Hadith\nclassiﬁcation process is backdated when Jbara et al. [34]\npresented an approach for classifying Hadiths’ topics into\n13 classes (books) of Sahih Al-Bukhari. Such classes include\nVOLUME 10, 2022 113331\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nfaith, knowledge, praying, hajj (pilgrimage), eclipse, alms-\ngiving, fasting, and medicine. Similarly, Alkhatib [35] study\nthe effectiveness of categorising Hadiths into 8 differ-\nent classes (books) using ML classiﬁers. Later, with a\nmuch focus on extracted quote Hadiths from four dif-\nferent books, Al-Kabi et al. [8] train and compare three\nML classiﬁers to predict the four classes. This work was\nafterwards extended by Al-Kabi et al. [36] with enlarging\nthe Hadiths dataset. Besides, Aﬁanto et al. [37] presented\nan approach to categorises Hadiths into three predeﬁned\ncategories: suggestion, prohibition, and information. Hav-\ning Arabic Sahih Al-Bukhari’s translated to the Indonesian\nlanguage, two classiﬁcation models with backpropagation\nNeural Network (NN) were proposed by Bakar et al. [38].\nRostam and Malim [39] followed an alternative mode and\nsuggested a technique that uses text categorization to classify\nparticular classes by ﬁguring out how the resources relate to\none another. The authors combined various resources com-\nprising Quran and Hadith. Mediamer [40] shifted the focus\nto the impact of feature extraction and preprocessing towards\nHadith classiﬁcation.\nIt is obvious that the Matn is used in all of the aforemen-\ntioned studies on Hadith classiﬁcation because it functions\nrather like topic modelling. However, as we will demonstrate\nin the following paragraphs, such a thing is conspicuously\nabsent from Hadith authentication.\nOver a decade ago, Zahedi et al. [41] presented a fuzzy\nexpert system with an ambition to authenticate Hadiths with\nits rate of validity. The system initiates with domain experts’\ninputs for developing a knowledge base with some essential\nrules taking into account the narrators’ names, particularly the\nSanad, as a main focus for the rating process. Analogously,\na combination of expert system and ML techniques was\nemployed by Aldhlan et al. [42] as a new classiﬁcation\nmethod to authenticate 999 Hadiths to their validity degree\n(e.g. Sahih or Hasan). In particular, a tree structure model\nwith a Decision Tree (DT) [43] classiﬁer along with selected\nattributes of the instances extracted from Hadith books.\nRather than relying on building or training a model, Shatnawi\net al. [44] presented a technique for extracting hadith phrases\nfrom web pages and using a positional index created from a\ndatabase of Hadiths to authenticate Hadiths as Sahih or Daif.\nMoreover, Najiyah et al. [17] opted for building an expert\ntable of Hadith comprising various characteristics and codes\nbased on consultation with domain experts. The intention was\nto authenticate and classify Hadiths into Sahih, Mawdu or\nDaif according to such characteristics and codes which will\nbe then used for creating a decision tree and a rule degree of\nhadith. Similarly motivated, Abdelaal and Youness [7] intro-\nduced a ML-based algorithm to authenticate Hadiths based\non the characteristics of the narrator such as reliability and\nmemory. Furthermore, Ghanem et al. [19] represents Hadith\nas vectors in the Vector Space Model (VSM) [45] and Term\nFrequency-Inverse Document Frequency (TF-IDF) as term\nweighting indicating its importance in order to classify 160\nHadiths into an authenticity grade.\nAs opposed to exploiting expert systems or ML techniques,\na simple method was proposed by Azmi and AlOfaidly [46].\nThe scheme essentially automates the process by formulating\nthe rules used by Hadith scholars to authenticate and rate the\nvalidity of 2800 Hadiths from Sunan Al-Tirmizi based on\nthe Sanad. Similarly, based on four main criteria concerning\nonly Sanad, namely the reliability and preservation of the\nnarrators, the ﬂaw in the chain of transmission, and connected\nchain, Ibrahim et al. [13] offered a theoretical authentication\nframework that would determine if a Hadith is Sahih or not.\nTaking a different tack at authenticating Hadiths using the\nSanad, Balgasem and Zakaria [47] addresses the problem by\nrecognising the Arabic names in the chain of narrators using\nPart-of-Speech (POS) and Named Entity Recognition (NER).\nAlthough Hadith scholars consult both the Sanad and Matn\nto establish the authenticity of a certain Hadith [11], all the\nabove-mentioned studies were restricted to using Sanad. The\nimportance of using Matn for Hadith aligns with Hadith\nscholars examining Hadith’s Matn to see if it agrees or\ndisagrees with other authentic Hadiths or what the Quran\nsays [16]. Matn on occasions uses unsuitable language or\nexpressions that do not align with Muslims beliefs or the\ndiscourse of the Prophet Muhammad PBUH. To the best of\nour knowledge, only two previous work has merely focused\non Matn for Hadith authentication which will be discussed in\nthe following paragraph.\nFirstly, Hassaine et al. [20] explored the possibility of a\nHadiths authentication process based solely on the Matn. This\nwas accomplished by maintaining a binary relation (for each\nclass, authentic and non-authentic) approach. Precisely, the\nproposed work begins with manually extracting keywords of\neach Hadith, authentic and non-authentic, using hyper rectan-\ngular decomposition, and these extracted keywords are then\nfed into ML algorithms for authentication. Secondly, compre-\nhensive experiments for the evaluation of Hadith authenticity\nwith various ML and deep learning classiﬁers were lately\nconducted by Tarmom et al. [16]. For example, Support\nVector Machine (SVM) [48], Naïve Bayes (NB) [49] and\nDT classiﬁers and Long Short-Term Memory (LSTM) [50],\nConvolutional Neural Network (CNN) [51] and CNN-LSTM\ndeep learning classiﬁers. Both Sanad and Matn were utilized\nin the proposed experiments.\nMost recently, numerous AI, ML, and NLP tasks have\nseen unprecedented and extraordinary results with the help of\nTLMs. However, no attempts have yet been made to explore\nthe usefulness of exploiting TLMs in Arabic for Hadith\nauthentication or classiﬁcation. The single attempt for all\nwe know to deal with Hadith using TLMs came lately by\nEmha et al. [52] dealing with Indonesian Hadiths translated\nfrom Arabic. In particular, a semi-supervised BERT with\nan additional feed-forward neural network was proposed to\nclassify the Indonesian Hadiths. The feed-forward network\nespecially operates on the narrators for Hadith for the execu-\ntion of NER, for Indonesian Hadith texts in particular. The\nexperiments show that the proposed model utilizing BERT\nwith NER was exceedingly effective.\n113332 VOLUME 10, 2022\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nTABLE 1. NAH and LK corpora content description.\nTo overcome the various limitations that exist in the lit-\nerature concerning Hadith authentication, we propose the\nﬁrst thorough study exploiting Arabic TLMs for Hadith\nauthentication and MHs detection using the central content\nof Hadiths, the Matn.\nIII. DATA\nA. AVAILABLE CORPORA\nIn the systematic review proposed by Binbeshr [11],\nit was concluded that Sahih Al-Bukhari is the most widely\nemployed corpus in Hadith studies. It was also stated that\nalmost all the datasets used in Hadiths literature are not\npublicly available. Although the number of books devoted\nto Hadith narration is considerable, we rarely dispose of a\nstructured digital version ready to process. While most of the\nefforts have been directed at Sahih Al-Bukhari, the existing\nworks do not give access to the used subsets of the book.\nA second disadvantage is that Sahih Al-Bukhari focuses\nsolely on Sahih Hadiths (SHs), which does not entirely meet\nthe objective of our study. It is only newly that Non-Authentic\nHadiths (NAH) [53] corpus was created and made public,\nbecoming the ﬁrst corpus dedicated to non-authentic Hadiths.\nIn addition, Leeds and King Saud University (LK) corpus [54]\nwas also lately published. The latter gathers the Hadiths\nfrom the six most well-known books of Hadiths concerning\nSH; known as Al-Sihah Al-Sittah, or ‘‘The Authentic Six’’.\nAlthough not all of the Hadiths in these books are authentic,\ntheir name derives from the fact that most of them are con-\nsidered authentic.\nAfresh Hadith corpus covering 9 books of Hadith was\nalso made public [55], which includes the same books of\nthe LK corpus as well as the contents of Musnad Ahmad\nIbn Hanbal, Malik Muwatta, and Sunan Al Darimi. It con-\ntains more Hadiths than LK, however, it does not distinguish\nbetween the Matn and the Sanad of Hadith. Indeed, LK and\nNAH corpora explicitly distinguish between Sanad and Matn\nto facilitate the work on them. For these reasons, we opt to\nbase our work on these two corpora by adapting them to our\nproblem, as described in the next section. We demonstrate the\ndetails of LK and NAH corpora in Table 1.\nB. CORPUS CREATION PROCESS\nTo mimic the actual preponderance of MHs, we used the\nLK and NAH corpora as starting point to obtain an unbal-\nanced ﬁnal corpus for Mawdu and authentic Hadiths. For\nthis purpose, we apply the processing steps described in the\nfollowing subsections. The whole corpus creation process is\ndemonstrated in Fig. 1.\n1) CLEANING PHASE\nAs a means of guaranteeing an optimal quality of input for\nthe models, we clean the variables concerning the Matn and\nthe degree of authenticity of the Hadiths as follows:\nLK corpus cleaning relies on the Arabic_Matn,\nEnglish_Grade, and Arabic_Grade ﬁelds, and apply cleaning\ndecisions below:\n• Using Dorar1 to check Hadiths when Arabic and English\ngrades differ →2 Hadiths\n• Removing Hadiths with empty Arabic Matn →826\nHadiths\n• Removing Hadiths with empty English and Arabic\ngrades →380 Hadiths\n• Removing Hadiths with no English grade and an Arabic\ngrade that we were not able to classify as authentic or\nnot →32 Hadiths\n• Keep the following grades: Sahih - Authentic, Daif -\nWeak, Hassan - Good, Hassan - Sahih, Mawdu -\nFabricated, Munkar →removed 708 Hadiths\nFor the NAH corpus we use Matn and Degree ﬁelds for the\nsubsequent cleaning steps:\n• Removal of Hadiths without Matn →1,246 Hadiths\n• For Hadiths without degree information, we scrap the\ndegree from Hdith website, 2 see next section III-B2 →\n3,352 Hadiths\n• Removal of authentic Hadiths →359 Hadiths\n• Grouping Hadiths with degrees meaning Mawdu and\nleave the remaining as Daif. Degrees classiﬁed as\n1https://dorar.net/\n2http://hdith.com/\nVOLUME 10, 2022 113333\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nFIGURE 1. Corpus creation process.\nMawdu are :\n (false,\nfabricated, a lie, slanderous).\n2) SCRAPPING PHASE\nIn light of the very limited number of MHs available,\nwe decided to further add additional MHs. To accomplish\nthis, we scraped the two well known websites specialized\nin Hadith indexing, Dorar and Hdith. Compared to Dorar,\nHdith has a better search engine that can identify Hadiths\neven when they are misspelled. Hdith was used to recover\nthe authenticity degrees of 3,552 Hadiths that lacked this\ninformation in the NAH corpus. While Dorar was utilized to\nscrape additional MHs not present in the NAH corpus. 3 This\nmethod recognized 900 Hadiths out of the 3,552 Hadiths as\nMawdu from Hdith. Dorar has a section titled ‘‘Widespread\nbut unauthentic Hadiths’’, from which we scraped all 1,315\nHadiths, including 366 MHs after ﬁltration.\nAfter the cleaning and scrapping phases, we obtain an\nenhanced version of the NAH corpus, which we call NAH\nPlus, which is now publicly available. 4 NAH Plus contains\nonly non-authentic Hadiths, with available Matn in addition\nto the Hadiths for which we have recovered the degrees\nof authenticity. This corpus contains a total of 3,660 non-\nauthentic Hadiths.\n3) FILTER PHASE\nWe ﬁlter the NAH Plus dataset to extract the MHs, and the\nsame process was applied to the LK corpus, which con-\ntains 29 MHs. In addition, we add the 366 MHs scraped\nfrom Dorar and delete any duplicates. As a result, we cre-\nate a new corpus dedicated to MHs, with a total of 2,452\nHadiths. We call this dataset MAwdu HADith DATaset or\n3Python selenium 4.1.5 library was used for the scraping phase.\n4https://github.com/kamelgaanoun/mhdetection/tree/main/nah_plus\nMAHADDAT meaning ‘‘Never narrated’’ in Arabic. To the\nbest of our knowledge, this is the ﬁrst dataset dedicated\nto MHs. We have released and made the dataset publicly\navailable to encourage more research on this ﬁeld. 5\nFor the ﬁnal dataset, we mix MAHADDAT with a ﬁltered\nversion of LK corpus retaining only authentic SHs, and we\nremove the duplicated Hadiths. In addition, Hadiths with a\nMatn indicating their authenticity degree were identiﬁed and\nremoved from the LK corpus. In order to avoid any indication\nfor the classiﬁcation models, we removed Hadiths containing\nthe words\nauthentic or\n good.\nIn addition to the previous steps, we applied an extra\nstep for the LK corpus. In fact, the creators of this cor-\npus state in their paper the following: ‘‘in our corpus we\nincorporated the Prophet in the Matn instead of Isnad’’. The\nIsnad/Sanad usually includes the chain of successive narra-\ntors that ends with an expression related to the Prophet, such\nas\n‘‘The prophet Peace\nBe Upon Him said.’’,\n‘‘That the Messenger of God, may God bless him and grant\nhim peace, said’’,\n ‘‘On\nthe authority of the Prophet, may God bless him and grant\nhim peace, that he said’’. It is worth mentioning that including\nthese expressions in the Sanad creates a bias for the models,\nsince these expressions are not part of the Matn. Moreover,\nsince the NAH corpus does not include these expressions, the\nmodels will have an indication towards the Hadiths Sahih.\nWe therefore decided to eliminate these expressions in order\nto eliminate any potential bias. That said, the results without\nthis cleaning step were also obtained (and presented in the\nsupplementary ﬁle submitted alongside this paper), where we\n5https://github.com/kamelgaanoun/mhdetection/tree/main/mahaddat\n113334 VOLUME 10, 2022\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nTABLE 2. Description of the final corpus (MAHADDAT combined with the enhanced LK).\nFIGURE 2. Matn length distribution (with extreme length omitted).\nobserve better results than our ﬁnal scores due to the bias\nraised above.\nC. DESCRIPTION OF USED CORPUS\nThere are 26,561 Hadiths in the ﬁnal corpus, of which 24,109\nare Sahih and 2,452 are Mawdu, representing 9.23% of the\ntotal. We describe the details of this dataset in Table 2. While\nthe average number of words is quite similar for Sahih and\nMHs, 42 and 45 respectively, the median number is not.\nIn fact, if we omit the extremes, the MHs have much lower\nwords than the SHs (see Fig. 2). Parallel to this, for longer\nHadiths, Mawdu ones can be three times longer than SHs,\nattaining a maximum of 5,608 words.\nWe also analyzed the most frequent words for both types\nof Hadiths, Sahih and Mawdu. For this purpose, we ﬁrst\neliminated the Arabic-speciﬁc stopwords derived from the\nNLTK 3.7 library. 6 In addition, we built a list of stopwords\nspeciﬁc to Hadiths, including words like\n: Almighty,\n: God,\n : and he said,\n : I heard,\n : The prophet.\nThe complete list of the 515 additional stopwords is released\nand publicly available. 7\nWe visualize the result of this analysis with the help of the\nwordcloud in Fig. 3a for the SHs and Fig. 3b for the MHs.\n6https://www.nltk.org/\n7https://github.com/kamelgaanoun/mhdetection/tree/main/stopwords\nFIGURE 3. Corpus wordcloud.\nFor the SHs, we notice rather the recurrence of names of\ncompanions who were very intimate with the Prophet PBUH\nand who are known by the important number of Hadiths\nthat they reported. We see for example the name of\nAisha, one of the wives of the prophet PBUH and\nAbu Hurairah, a companion who rarely separated from the\nprophet.\nWhile for the MHs, we note the recurrence of words relat-\ning encouragement towards good deeds and rewards hoped\nfor by the faithful, such as\n Prayer,\n Paradise, and\nalso words serving to frighten towards\n the hell or\nthe day of resurrection for example. In addition we also note\na fairly high frequency for the word\n which can mean ‘‘on\nme’’ or the ﬁrst name of a companion among the four rightly\nguided caliphs, and who is also the ﬁrst Imam followed\nby the Shia group. In order to estimate the prevalence of\nthe word\nrelated to the companion whose full name is\nAli Ibn Abi Talib and since the MHs do\nnot have diacritics, we have relied on the context in which\nVOLUME 10, 2022 113335\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nthis word is quoted. Indeed, we deﬁne a window of 10 words\nbefore and after the word\n whenever it is encountered.\nThen, we consider it to be the companion when one of the\nfollowing rules is veriﬁed:\n• The delimited window includes one of the follow-\ning words:\n which are\nglobally qualiﬁers or words strongly related to the\ncompanion\n• The word\n is directly preceded by one of the fol-\nlowing words:\n (Hey, To, With, Where is)\nwhich are prepositions that cannot occur before the word\nmeaning ‘‘on me’’\nBased on this method, we concluded that at least 47% of\ninstances of the word\n concern the companion in the MHs\nand only 20% in the SHs.\nIV. CLASSIFICATION METHODOLOGY\nA. ML MODELS\n1) TEXT REPRESENTATION\nAs ML algorithms cannot process texts directly, a preliminary\nstep called text representation or vectorization is required.\nAs part of this step, each document is represented by a vector,\nwhose components are, for example, its words, so that the\nlearning algorithms can exploit them [56]. As a result, a col-\nlection of texts can be represented by a matrix whose rows are\nthe terms that appear at least once and whose columns are the\ndocuments. This matrix generally contains weights assigned\nto each word, depending on the method used to calculate these\nweights, we get different matrices. These weights correspond\nto the contribution of each word to the semantics of its\ndocument. A commonly used approach in this ﬁeld is TF-IDF.\nThis method weights each word based on its frequency in all\ndocuments, while giving advantage to rare words. Thus for a\nword w and a document d, the TF-IDF is calculated according\nto the following equation:\nTF −IDF(w,d) =frequency(w,d) ×log D\nDw\n(1)\nwith:\n• frequency(w,d): Number of occurrences of w in d\n• D: Total number of documents\n• Dw: Number of documents containing the word w\nThe expression in (1) may differ slightly from one imple-\nmentation to another. We use the one from the Pyhton Sklearn\nlibrary8 which is written as follows:\nTF −IDF(w,d) =frequency(w,d) ×(log D\nDw\n+1) (2)\nIn addition, we also experiment with another varia-\ntion of this method using a logarithmic transformation of\nfrequency(w,d) in order to reduce the importance of terms\n8https://scikit-learn.org/\nwith high frequency [57], and we note this variation as\nLogTF-IDF.\nThe large number of words in the corpus can lead to\nlarge matrices, affecting the complexity and accuracy of the\nmodels. By using a dimension reduction method, we can\nkeep the most important features while restricting the number\nof features. The method used here is the Singular Value\nDecomposition (SVD) [58].\n2) EXPERIMENTED ML MODELS\nWe present below an overview of the different ML models\nused in our experiments:\nRandom Forest (RF)[59] is a decision tree-based ensem-\nble learning technique. Multiple decision trees are created\nusing data sets that have been split from the original data.\nDuring each stage of the decision tree, a subset of variables\nis randomly selected. The model then selects the mode for all\npredictions in each decision tree.\nLogistic Regression (LR)[60] is based on the concept of\nlinear regression but adapted to the case where the explained\nvariable takes discrete values. It also has the particularity of\npredicting not the value of the variable itself, but rather the\nprobability of occurrence of an event. For the case of a vari-\nable with two values, we refer to it as binary logistic regres-\nsion and the outcome will be a probability of occurrence of\nthe event bounded between 0 and 1. Logistic regression is\nmodeled according to the following equation:\nlog( p\n1 −p) =β0 +β1X1 +β2X2 +... +βnXn (3)\nwhere:\n• p is the probability of event occurrence and 1 −p is the\nprobability of failure.\n• β0 to βn are the regression coefﬁcients.\n• X1 to Xn are the independent variables.\nNaïve Bayes (NB)is a probabilistic classiﬁer based on the\nBayes theorem [61]. Assuming all explanatory variables are\nconsidered independently, this algorithm relies on a strong\nassumption. The term naive comes from the fact that we\nassume this independence of the variables. In our binary\nclassiﬁcation, for example, NB will assume that the words\nin a document appear independently of each other.\nSupport Vector Machine (SVM)will ﬁnd a hyperplane or\nboundary between the two classes of data (for a binary clas-\nsiﬁcation problem) that will maximize the margin between\nthe two classes. There are many planes that can separate the\ntwo classes, but only one plane can maximize the margin or\ndistance between the classes.\nGradient Boosting (GB)[62] as the name suggests, it uti-\nlizes two main concepts, Gradient and Boosting. Boosting\nis an iterative method consisting in reinforcing successive\nmodels at each iteration by giving more weight to cases with\nhigh values with respect to the loss function, these cases are\ncalled difﬁcult cases. Boosting is a kind of method allow-\ning the model to learn from its previous errors. Gradient is\n113336 VOLUME 10, 2022\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nFIGURE 4. Transformer model (as described in the original paper [65]).\nthe optimization method that allows to minimize the loss\nfunction.\nXtreme Gradient Boosting (XGBoost)[63] is a particular\nimplementation of the GB algorithm with more advanced\napproximation methods, such as the use of second order\ngradients, as well as a better generalization using the L1 and\nL2 regularization methods.\nLight Gradient Boosting Machine (LGBM) [64] is\nanother decision tree-based algorithm that is faster and uses\nless memory than XGBoost. Additionally, it splits the deci-\nsion tree differently from XGBoost. As a matter of fact,\nLGBM splits the tree leaf-wise, unlike XGBoost, which splits\nthe tree level-wise.\nB. BERT MODELS\n1) DEFINITION\nBERT is based on Transformers, which consists of two dis-\ntinct blocks: an encoder that reads the input text and a decoder\nthat predicts the task. In BERT, only the encoder block is\ninvolved since the goal is to generate a language model\nwith the main objective of creating an attention mechanism\nthat learns the contextual relationships between words (or\nsubwords) in a text. The architecture of this encoder block\nis composed of several attention layers, and the number of\nthe latter differs according to the version of BERT. Indeed,\nBERT comes in two architectural variations, BERT base and\nBERT large, with the ﬁrst containing 12 attention layers and\nthe latter having 24. Fig. 4 illustrates this architecture.\nFor the training purpose, BERT relies on two training\ntasks, namely the Masked Language Model (MLM) and Next\nSentence Prediction (NSP). In MLM, 15% of the words\nare randomly masked with the [MASK] token, followed by\nreplacement of 10% of these tokens by random words, and\n10% by the original word. The model aims to predict the\nTABLE 3. Selected BERT models configurations.\nmasked tokens iteratively until convergence is reached. As for\nNSP, it uses pairs of sentences as inputs, the model predicts\nif the second sentence is the next sentence in the original\ndocument. The inputs consist of 50% pairs where the second\nsentence is the actual next sentence, and 50% pairs where a\nrandom sentence is used.\nThe aforementioned process is called pre-training phase.\nDuring this stage, BERT acquires knowledge related to the\nexamined language, constructing a language model capa-\nble of understanding the relationships between the words\nof the language in question. It is a major advance in NLP\nin that, based on the knowledge acquired earlier by BERT,\nit is possible to apply transfer learning in a second phase\ncalled ﬁne-tuning using a limited volume of data in a speciﬁc\ndomain.\n2) AVAILABLE BERT MODELS\nWhen BERT was initially designed only for English, the ﬁrst\nmultilingual model based on the BERT architecture, called\nmBERT, soon followed. It used Wikipedia from the 104 most\ncommonly represented languages for its training. Follow-\ning that, Facebook AI researchers released XLM-RoBERTa,\na second multilingual model based on CommonCrawl [66].\nWhile both models partially support the Arabic language,\nthey were limited concerning Arabic-related downstream\ntasks, and the need for Arabic-speciﬁc models became\nincreasingly persistent. This need for a specialized model\ntrained uniquely on the Arabic language, and able to achieve\nbetter performances gave birth to several successful models\nsuch as AraBERT [29], ArabicBERT [30], ARBERT [67],\nQARIB [31], CAMeLBERT [32]. As the Hadiths are writ-\nten in Classical Arabic (CA) and Modern Standard Arabic\n(MSA), we only mention the BERT models that apply to these\ntwo variants and ignore models for Dialectal Arabic (DA).\nAs illustrated in Table 3, CAMeLBERT_CA [68] is\nthe only model trained on a CA corpus, with the rest\nbeing mainly based on MSA. Aside from the multilingual\nmodels, all the MSA models use similar sources, unlike\nCAMeLBERT_CA, which uses Open Islamicate Texts Ini-\ntiative Corpus (OpenITI) [68], with more than 11,000 Islamic\nbooks and 7.5M pages.\nVOLUME 10, 2022 113337\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nV. EXPERIMENTS AND RESULTS\nA. EVALUATION\nIn order to make this work comparable and encourage more\nresearch in this ﬁeld, in addition to making our dataset pub-\nlicly available, we present our results using various metrics\ncommonly employed for supervised classiﬁcation. Along\nwith the standard metrics of accuracy, precision, and recall,\nwe also use Area Under ROC Curve (AUC) and F1-measure\nthat are more appropriate to the problem of unbalanced data.\nWe introduce below each of the used metrics.\nAccuracy describes the effectiveness of a model in cor-\nrectly predicting both positive and negative individuals in a\nsymmetrical way. It measures the rate of correct predictions\nfor all individuals, and it is generally presented in the form of\nthe following ratio:\nAccuracy = TP +TN\nTP +FP +TN +FN (4)\nWith:\n• TP: the individuals that the model was able to predict as\npositive (individuals concerned by the studied event)\n• TN: the individuals that the model was able to predict as\nnegative (individuals not concerned by the studied event)\n• FP: the individuals that the model wrongly predicted as\npositive\n• FN: the individuals that the model wrongly predicted as\nnegative\nFor the case of our study, TP are the Hadiths correctly\npredicted as Mawdu, while TN are the Hadiths correctly\npredicted as Sahih, and this deﬁnition holds for the rest of\nmetrics using Positive and Negative terms.\nOne of the limitations of this metric is that it is only\nmeaningful for datasets with equal distribution of classes.\nPrecision helps answer the question, ‘‘What proportion of\nMH predictions, were actually correct?’’, and is calculated\nas the ratio between TP to the total number of individuals\npredicted as Mawdu as in (5):\nPrecision = TP\nTP +FP (5)\nRecall allows to answer the question, ‘‘What proportion of\ntrue MH results were correctly identiﬁed?’’, and is calculated\nas the ratio between TP to the total number of MH as in (6):\nRecall = TP\nTP +FN (6)\nF1-measure evaluates the ability of a classiﬁcation model\nto efﬁciently predict positive individuals (MH in our case),\nby making a trade-off between precision and recall. It is\nparticularly used for tasks dealing with unbalanced data. The\nF1-score summarizes the precision and recall values in a\nunique metric as expressed in (7):\nF1 =2 ×Precision ×Recall\nPrecision +Recall (7)\nWe use the F1-score as a reference metric to compare the\ndifferent models since it is a robust metric concerning unbal-\nanced datasets and offers a summary of the precision and\nrecall. Nevertheless, because this study is more concerned\nwith detecting Mawdu Hadith than Sahih Hadith, the metric\nadopted is the F1 MH speciﬁc to Mawdu Hadith. In parallel,\nwe provide the F1 to summarize the results of the F1 measure\nfor the two classes of Hadiths.\nAUC is based on the Receiver Operating Characteristics\n(ROC), a probability curve where True Positive Rates (TPR)\nare plotted against False Positive Rates (FPR) for different\nthresholds. The AUC is then the area under this ROC curve,\nand summarizes the curve with a range of threshold values\nas a single score. AUC is an other used metric for unbal-\nanced data, that said, this metric remains controversial in the\nliterature [69], [70], we therefore use it as complementary\ninformation to the F1-score.\nB. CONFIGURATION\n1) ML MODELS\nWe conducted experiments with four different scenarios\ndepending on the settings of the TF-IDF and SVD. We list\nthese four scenarios below:\n• TF-IDF 1 : TF-IDF maximum features=5000, SVD\nnumber of components =15\n• TF-IDF 2 : TF-IDF maximum features=8000, SVD\nnumber of components =20\n• LogTF-IDF 1 : LogTF-IDF maximum features=5000,\nSVD number of components =15\n• LogTF-IDF 2 : LogTF-IDF maximum features=8000,\nSVD number of components =20\nThe rest of the parameters are kept at their default value\ndeﬁned by the Sklearn library and we consider unigrams,\nbigrams and trigrams by setting the ngram_range parameter\nto the value (1,3). In addition, each of these four scenarios was\nevaluated using light-stemming [71] to assess the impact of\nthis method. The whole parameterizations details of the ML\nmodels is presented in the supplementary ﬁle attached with\nthe paper.\nWe split the dataset into a training set (Train), reserved\nfor the training process, a development set (Dev) to select\nand validate the best system, and a test set (Test) to evaluate\nthe adopted system and produce ﬁnal results. We chose an\n80/10/10 partition scenario for this split, while maintaining\nthe distribution of the two labels. We openly share 9 these\npartitions for future researchers to ensure comparability of\nour results.\n2) BERT MODELS\nDue to the fact that the selected pre-trained models aren’t\noriginally intended to classify Hadiths, they have to be\nﬁne-tuned to ﬁt our problem. To do so, we add a classiﬁca-\ntion layer on top of the model architecture combined with a\nsoftmax function and feed it with the [CLS] token embedding\nof the model.\nWe use the Trainer class from the HuggingFace Transform-\ners 4.5.1 library to train and evaluate the models. As the\n9https://github.com/kamelgaanoun/mhdetection/tree/main/corpus\n113338 VOLUME 10, 2022\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nTABLE 4. Classification results on Dev set using ML models and second\nLog TF-IDF/SVD configuration.\nproblem is a single label classiﬁcation one, the used loss\nfunction is cross-entropy.\nExcept for the batch size, all the models share the same\nhyperparameters. Due to GPU memory limitations, the batch\nsize is set to 64 for base models and 16 for large models.\nAs for the remaining hyperparameters, we set the learning\nrate to 2e −5 and the maximum input length to 128 tokens.\nNVIDIA Tesla P100-PCIE-16GB and NVIDIA Tesla T4\nGPUs were used in these experiments.\nIn the following subsection, we present the best results\nobtained on the Dev set, as well as Test set results with the\nretained conﬁguration. Results for the other conﬁgurations\nwere also obtained and are presented in the supplementary\nﬁle attached with this manuscript. The results of ML models\nare shown ﬁrst, followed by the results of BERT models.\nC. RESULTS\n1) ML MODELS RESULTS\nTable 4 presents results for second LogTF-IDF/SVD without\nstemming conﬁguration, which produces the best scores on\nthe Dev set. We evaluate this setting on the Test set, and\nillustrate the results in Table 5.\n2) BERT MODELS RESULTS\nWe evaluate all BERT models on the Dev set with 1 and\n3 epochs combined with the application of light-stemming.\nIn this section, we present results obtained with the best con-\nﬁguration both on Dev and Test sets. The remaining results\nare available in the supplementary ﬁle.\nTable 6 present results on the original Dev set (without\nstemming) after ﬁne-tuning Models for 3 epochs. As the\nconﬁguration with no stemming and 3 epochs is providing\nbest results, we apply this setting on the Test set, and present\nﬁnal results in Table 7.\nWhile this work focuses on training and evaluating\non unbalanced datasets, we present three other scenarios\nbased on training and evaluation dataset type in Table 8.\nThe balanced datasets are obtained by downsampling the\nSahih Hadiths to equal the number of Mawdu Hadiths.\nWe make these datasets available 10 to researchers for future\ncomparison.\n10https://github.com/kamelgaanoun/mhdetection/tree/main/corpus\nTABLE 5. Classification results on non Stemed Test set using ML models\nand second LogTF-IDF/SVD configuration.\nTABLE 6. Classification results on Dev set without stemming using BERT\nmodels and 3 epochs.\nTABLE 7. Classification results on Test set with BERT models.\nTABLE 8. CAMeLBERT_CA results on Balanced and Unbalanced Test sets.\nD. QUALITATIVE ANALYSIS\nThe confusion matrix on Fig. 5 shows that only 5 SHs were\npredicted as Mawdu, and 30 Hadiths were falsely predicted\nas Sahih to make a total of 35 wrong predictions out of all\n2,657 Test Hadiths.\nTo deepen the evaluation of our MH detection system,\nwe created a new dataset of simulated MHs. The texts of\nthose simulated MHs come from Muslim scholars religious\nopinions (Fatwa), 11 which are texts dealing with religious\ntopics, containing semantics very similar to Hadiths, and\nquoting prophetic personalities. Considering that these three\n11We scrapped 45 Fatwas from https://binbaz.org.sa\nVOLUME 10, 2022 113339\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nFIGURE 5. Confusion matrix for Test set results.\nelements are omnipresent in the MHs, the integration of these\ntexts may be seen as a simulation of MHs. Here, we create\na dataset containing those Fatwas along with a set of SHs,\nand evaluate our system against this dataset to see if it can\ndifferentiate the simulated MHs from Hadith.\nAs for the sources, we used the 380 LK corpus Hadiths\nthat do not have authenticity degrees (see subsection III-B).\nTo obtain the degrees of authenticity, we follow the same\nmethod as in subsection III-B2. We retain 98 Hadiths that\nwere identiﬁed as Sahih. As such, we are guaranteed that\nthese Hadiths have not been seen by the model during\ntraining.\nThe model, as shown in Fig. 6, classiﬁes almost all the\nexamples correctly, except for one SH predicted as Mawdu\nand three Fatwas predicted as SHs. Based on this evaluation,\nF1Fatwa is 95.45% and Accuracy is 97.20%, which are very\nclose to the scores obtained in the previous evaluations. This\nclearly further shows the ability of the model to detect MHs.\nVI. DISCUSSION\nBERT models considerably outperform ML models regard-\nless of any conﬁguration settings. In fact, the best BERT\nmodels, CAMeLBERT_CA, outperforms the best ML model,\nRF, by more than 42 percentage points in terms of F1 MH\nscore. The same is true for the lowest BERT score achieved\nby multilingual BERT (80.62%), which is 31 points higher\nthan the ML highest score. Furthermore, the BERT models\noutperform the best ML model by an average of approxi-\nmately 38 points. This ﬁnding justiﬁes the proposal in this\npaper and is consistent with the literature conclusions about\nthe superiority of BERT models in text classiﬁcation [72],\n[73], [74].\nRegarding ML models, the second LogTF-IDF/SVD con-\nﬁguration without light stemming produce the best results on\nthe Dev set, with an F1 MH score of 51.93% for the RF model\nand an average of 33% for all models. RF proves to be the best\nmodel for this problem, scoring highest in 5 of the 8 setups\nstudied. LGBM comes second with an average of 47.99%\nFIGURE 6. Confusion matrix for simulated MHs (Fatwa).\nagainst 48.56% for RF on all experimented conﬁgurations.\nThe evaluation of the best conﬁguration on the Test set gives\nthe advantage to RF with an F1 MH of 49.60%, outperforming\nLGBM, which achieves a score of 47.34%.\nFor the Dev set, BERT models achieve their highest\nscores with three epochs and without light stemming, with\nCAMeLBERT_CA attaining an F1 MH of 94.27%, barely\nbetter than ArabicBERT_large, which achieves a score of\n92.01%. When applying this conﬁguration to the Test set,\nCAMeLBERT_CA gets an F1 MH score of 92.47%, fol-\nlowed by ArabicBERT_large and ARBERT, with 92.01% and\n90.99% F1MH scores, respectively.\nWhether it is for the Dev set or the Test set,\nCAMeLBERT_CA achieves important results, and out-\nperforms models trained on larger datasets. For example,\nCAMeLBERT_CA obtains better results than models trained\non datasets 18 times larger such as CAMeLBERT_MSA\nand 16 times larger like ArabicBERT, and also AraBERT\nand ARBERT trained on 13 and 10 times larger datasets,\nrespectively. This performance is attributable to the model’s\nfocus on the task at hand, as it is trained on classical Arabic\ntexts unlike the other models. The other models were trained\non MSA text, whereas Hadiths are mainly written in classical\nArabic. Using a smaller model specialized on the treated text\nis more relevant than larger models trained on less specialized\ntexts. Moreover, CAMeLBERT_CA also outperforms models\nwith more complex architecture, like AraBERTv2_large and\nArabicBERT_large based on the Large architecture BERT\nversion, whereas CAMeLBERT is based on BERT base\nmodel.\nEven though the dataset is unbalanced, the method\nachieves very high scores, including for the F1 metric,\nwhich is sensitive to this type of dataset. This result is\nimportant since the used dataset mimics the actual pre-\nponderance of MHs. By taking a balanced training and\nunbalanced evaluation datasets as a baseline, our results\nexceed this score by over 6 percentage points. The other two\nscenarios with a balanced evaluation dataset are considered\n113340 VOLUME 10, 2022\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\nas maximum comparison scores to encourage future\nresearch.\nFurthermore, these results address a very present need in\nthis ﬁeld, which is either the absence of works addressing\nunbalanced datasets, or the use of metrics not appropriate for\nunbalanced datasets, like accuracy metric.\nVII. CONCLUSION AND FUTURE DIRECTIONS\nHadiths are the second source of Islamic law after the Quran.\nHowever, some Hadiths may be fabricated and mislead the\nfaithful. These Hadiths are called Mawdu Hadiths (MHs).\nIn this work, we have developed a system for detecting fabri-\ncated Hadiths. For this purpose, we have created and released\nthe ﬁrst dataset speciﬁc to MHs, called MAHADDAT along\nwith releasing a new enhanced version of an existed dataset\n(NAH), called NAH Plus. The work presented in this paper\nalso study and understand the central text and content of\nHadith, Matn, rather than solely focusing on the Sanad.\nDespite being trained in much smaller dataset as compared\nto other Arabic BERT models, our best system is based\non CAMeLBERT_CA, a BERT-based model specializing in\nthe classical Arabic variant. The proposed model achieves\nstate-of-the-art results with an F1 MH score of 92.47%. More-\nover, a thorough comparison study in Hadith authentication\nbetween numerous classical ML algorithms and all available\nArabic TLMs was also performed. Such comparison reveal\nthat all Arabic TLMs are superior to all classical ML models.\nFuture studies could reﬁne the automatic authentication of\nHadiths by improving the method used in this paper. Fine-\ngrained authentication would be possible by detecting both\nthe degree and type of Hadith. The Daif Hadith, for example,\ncan be classiﬁed into up to ten types. In addition, the ﬁndings\nabout the superiority of a BERT model specializing in CA\nas demonstrated by CAMeLBERT_CA could be explored\nby developing more specialized models trained on Hadith\ncorpora, in order to create more effective Hadith analysis\nsystems.\nACKNOWLEDGMENT\nThe researchers would like to thank the Deanship of Scientiﬁc\nResearch, Qassim University for funding the publication of\nthis project.\nREFERENCES\n[1] L. Bozarth and C. Budak, ‘‘Toward a better performance evaluation frame-\nwork for fake news classiﬁcation,’’ in Proc. Int. AAAI Conf. Web Social\nMedia, vol. 14, 2020, pp. 60–71.\n[2] C. Wang and D. M. Blei, ‘‘Collaborative topic modeling for recommending\nscientiﬁc articles,’’ in Proc. 17th ACM SIGKDD Int. Conf. Knowl. Discov-\nery Data Mining (KDD), 2011, pp. 448–456.\n[3] F. Al-Hawari and H. Barham, ‘‘A machine learning based help desk system\nfor IT service management,’’ J. King Saud Univ. Comput. Inf. Sci., vol. 33,\nno. 6, pp. 702–718, Jul. 2021.\n[4] L. Hong and B. D. Davison, ‘‘Empirical study of topic modeling in\nTwitter,’’ in Proc. 1st Workshop Social Media Analytics (SOMA), 2010,\npp. 80–88.\n[5] A. Elnagar, R. Al-Debsi, and O. Einea, ‘‘Arabic text classiﬁcation using\ndeep learning models,’’ Inf. Process. Manage., vol. 57, no. 1, Jan. 2020,\nArt. no. 102121.\n[6] N. Boudad, R. Faizi, R. O. H. Thami, and R. Chiheb, ‘‘Sentiment analysis\nin Arabic: A review of the literature,’’ Ain Shams Eng. J., vol. 9, no. 4,\npp. 2479–2490, 2017.\n[7] H. M. Abdelaal and H. A. Youness, ‘‘Hadith classiﬁcation using machine\nlearning techniques according to its reliability,’’ Rom. J. Inf. Sci. Technol.,\nvol. 22, nos. 3–4, pp. 259–271, 2019.\n[8] M. N. Al-Kabi, H. A. Wahsheh, and I. M. Alsmadi, ‘‘A topical classiﬁca-\ntion of Hadith Arabic text,’’ in Proc. IMAN, 2014, pp. 1–8.\n[9] M. M. A. Najeeb, ‘‘A novel Hadith processing approach based on genetic\nalgorithms,’’IEEE Access, vol. 8, pp. 20233–20244, 2020.\n[10] M. A. Saloot, N. Idris, R. Mahmud, S. Jaafar, D. Thorleuchter, and A. Gani,\n‘‘Hadith data mining and classiﬁcation: A comparative analysis,’’ Artif.\nIntell. Rev., vol. 46, no. 1, pp. 113–128, Jun. 2016.\n[11] F. Binbeshr, A. Kamsin, and M. Mohammed, ‘‘A systematic review on\nHadith authentication and classiﬁcation methods,’’ ACM Trans. Asian\nLow-Resource Lang. Inf. Process., vol. 20, no. 2, pp. 1–17, Apr. 2021.\n[12] A. H. Usman and R. Wazir, ‘‘The fabricated Hadith: Islamic ethics and\nguidelines of Hadith dispersion in social media,’’ Turkish Online J. Design\nArt Commun., vol. 8, pp. 804–808, Sep. 2018.\n[13] N. K. Ibrahim, S. Samsuri, M. Sadry Abu Seman, A. E. B. Ali, and\nM. Kartiwi, ‘‘Frameworks for a computational isnad authentication and\nmechanism development,’’ in Proc. 6th Int. Conf. Inf. Commun. Technol.\nMuslim World (ICT4M), Nov. 2016, pp. 154–159.\n[14] J. A. C Brown, Hadith: Muhammad’s Legacy Medieval Modern World.\nNew York, NY , USA: Simon and Schuster, 2017.\n[15] S. Hakak, A. Kamsin, W. Z. Khan, A. Zakari, M. Imran, K. B. Ahmad,\nand G. A. Gilkar, ‘‘Digital Hadith authentication: Recent advances, open\nchallenges, and future directions,’’ Trans. Emerg. Telecommun. Technol.,\nvol. 33, no. 6, Jun. 2022, Art. no. e3977.\n[16] T. Tarmom, E. Atwell, and M. Alsalka, ‘‘Deep learning vs compression-\nbased vs traditional machine learning classiﬁers to detect Hadith authentic-\nity,’’ in Proc. Annu. Int. Conf. Inf. Manage. Big Data. Cham, Switzerland:\nSpringer, 2022, pp. 206–222.\n[17] I. Najiyah, S. Susanti, D. Riana, and M. Wahyudi, ‘‘Hadith degree classiﬁ-\ncation for Shahih Hadith identiﬁcation web based,’’ in Proc. 5th Int. Conf.\nCyber IT Service Manage. (CITSM), Aug. 2017, pp. 1–6.\n[18] H. M. Abdelaal, B. R. Elemary, and H. A. Youness, ‘‘Classiﬁcation of\nHadith according to its content based on supervised learning algorithms,’’\nIEEE Access, vol. 7, pp. 152379–152387, 2019.\n[19] M. Ghanem, A. Mouloudi, and M. Mourchid, ‘‘Classiﬁcation of Hadiths\nusing LVQ based on VSM considering words order,’’ Int. J. Comput. Appl.,\nvol. 148, no. 4, pp. 25–28, Aug. 2016.\n[20] A. Hassaine, Z. Saﬁ, and A. Jaoua, ‘‘Authenticity detection as a binary text\ncategorization problem: Application to Hadith authentication,’’ in Proc.\nIEEE/ACS 13th Int. Conf. Comput. Syst. Appl. (AICCSA), Nov. 2016,\npp. 1–7.\n[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[22] T. B. Brown, ‘‘Language models are few-shot learners,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 33, 2020, pp. 1877–1901.\n[23] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le,\n‘‘XLNet: Generalized autoregressive pretraining for language understand-\ning,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 32, 2019, pp. 1–18.\n[24] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\npretraining approach,’’ 2019, arXiv:1907.11692.\n[25] J. Á. González, L.-F. Hurtado, and F. Pla, ‘‘Transformer based contextu-\nalization of pre-trained word embeddings for irony detection in Twitter,’’\nInf. Process. Manage., vol. 57, no. 4, Jul. 2020, Art. no. 102262.\n[26] D. Meškel ˙e and F. Frasincar, ‘‘ALDONAr: A hybrid solution for sentence-\nlevel aspect-based sentiment analysis using a lexicalized domain ontology\nand a regularized neural attention model,’’ Inf. Process. Manage., vol. 57,\nno. 3, May 2020, Art. no. 102211.\n[27] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nNAACL, 2019, pp. 4171–4186.\n[28] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog ,\nvol. 1, no. 8, p. 9, 2019.\n[29] W. Antoun, F. Baly, and H. Hajj, ‘‘AraBERT: Transformer-based model for\nArabic language understanding,’’ 2020, arXiv:2003.00104.\nVOLUME 10, 2022 113341\nK. Gaanoun, M. Alsuhaibani: Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models\n[30] A. Safaya, M. Abdullatif, and D. Yuret, ‘‘KUISAIL at SemEval-2020 task\n12: BERT-CNN for offensive speech identiﬁcation in social media,’’ in\nProc. 14th Workshop Semantic Eval., Dec. 2020, pp. 2054–2059.\n[31] A. Abdelali, S. Hassan, H. Mubarak, K. Darwish, and Y . Samih,\n‘‘Pre-training BERT on Arabic tweets: Practical considerations,’’ 2021,\narXiv:2102.10684.\n[32] G. Inoue, B. Alhafni, N. Baimukan, H. Bouamor, and N. Habash, ‘‘The\ninterplay of variant, size, and task type in Arabic pre-trained language\nmodels,’’ 2021, arXiv:2103.06678.\n[33] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov,\n‘‘Unsupervised cross-lingual representation learning at scale,’’ 2019,\narXiv:1911.02116.\n[34] K. M. A. Jbara, A. T. Sleit, and B. H. Hammo, Knowledge Discovery in Al-\nHadith Using Text Classiﬁcation Algorithm. Amman, Jordan: Univ. Jordan,\n2009.\n[35] M. Alkhatib, ‘‘Classiﬁcation of Al-Hadith Al-Shareef using data mining\nalgorithm,’’ inProc. Eur., Medit. Middle Eastern Conf. Inf. Syst., (EMCIS),\nAbu Dhabi, UAE, 2010, pp. 1–23.\n[36] N. M. Al-Kabi, A. H. Wahsheh, M. I. Alsmadi, and A. M. A. Al-Akhras,\n‘‘Extended topical classiﬁcation of Hadith Arabic text,’’ Int. J. Islamic\nAppl. Comput. Sci. Technol., vol. 3, no. 3, pp. 13–23, Sep. 2015.\n[37] M. F. Aﬁanto and S. Al-Faraby, ‘‘Text categorization on Hadith Sahih Al-\nBukhari using random forest,’’ J. Phys., Conf. Ser., vol. 971, Mar. 2018,\nArt. no. 012037.\n[38] M. Y . A. Bakar and S. A. Faraby, ‘‘Multi-label topic classiﬁcation of Hadith\nof Bukhari (Indonesian language Translation)using information gain and\nbackpropagation neural network,’’ in Proc. Int. Conf. Asian Lang. Process.\n(IALP), Nov. 2018, pp. 344–350.\n[39] N. A. P. Rostam and N. H. A. H. Malim, ‘‘Text categorisation in Quran and\nHadith: Overcoming the interrelation challenges using machine learning\nand term weighting,’’ J. King Saud Univ. Comput. Inf. Sci., vol. 33, no. 6,\npp. 658–667, Jul. 2021.\n[40] Gugun Mediamer, Adiwijaya, and Said Al Faraby, ‘‘Development of rule-\nbased feature extraction in multi-label text classiﬁcation,’’ Int. J. Adv. Sci.\nEng. Inf. Technol., vol. 9, no. 4, pp. 1460–1465, 2019.\n[41] M. Ghazizadeh, M. H. Zahedi, M. Kahani, and B. M. Bidgoli, ‘‘Fuzzy\nexpert system in determining Hadith validity,’’ in Advances in Computer\nand Information Sciences and Engineering, 2008, pp. 354–359.\n[42] K. A. Aldhlan, A. M. Zeki, A. M. Zeki, and H. A. Alreshidi, ‘‘Novel\nmechanism to improve Hadith classiﬁer performance,’’ in Proc. Int. Conf.\nAdv. Comput. Sci. Appl. Technol. (ACSAT), Nov. 2012, pp. 512–517.\n[43] X. Wu, V . Kumar, J. R. Quinlan, J. Ghosh, Q. Yang, H. Motoda,\nG. J. McLachlan, A. Ng, B. Liu, P. S. Yu, Z.-H. Zhou, M. Steinbach,\nD. J. Hand, and D. Steinberg, ‘‘Top 10 algorithms in data mining,’’ Know.\nInf. Sys., vol. 14, pp. 1–37, Dec. 2008.\n[44] M. Q. Shatnawi, Q. Q. Abuein, and O. Darwish, ‘‘Veriﬁcation Hadith\ncorrectness in Islamic web pages using information retrieval techniques,’’\nin Proc. Int. Conf. Inf. Commun. Syst., 2011, pp. 164–167.\n[45] G. Salton, A. Wong, and C. S. Yang, ‘‘A vector space model for automatic\nindexing,’’Commun. ACM, vol. 18, no. 11, pp. 613–620, 1975.\n[46] A. M. Azmi and A. M. AlOfaidly, ‘‘A novel method to automatically pass\nhukm on Hadith,’’ in Proc. 5th Int. Conf. Arabic Lang. Process. (CITALA) ,\n2014, pp. 118–124.\n[47] S. S. Balgasem and L. Q. Zakaria, ‘‘A hybrid method of rule-based\napproach and statistical measures for recognizing narrators name in\nHadith,’’ in Proc. 6th Int. Conf. Electr. Eng. Informat. (ICEEI), Nov. 2017,\npp. 1–5.\n[48] C. Cortes and V . Vapnik, ‘‘Support-vector networks,’’ Mach. Learn.,\nvol. 20, no. 3, pp. 273–297, 1995.\n[49] I. Rish, ‘‘An empirical study of the naive Bayes classiﬁer,’’ in Proc. IJCAI\nWorkshop Empirical Methods Artif. Intell., vol. 3, no. 22, 2001, pp. 41–46.\n[50] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[51] J. Schmidhuber, ‘‘Deep learning in neural networks: An overview,’’ Neural\nNetw., vol. 61, pp. 85–117, Oct. 2014.\n[52] E. T. Luthﬁ, Z. I. M. Yusoh, and B. M. Aboobaider, ‘‘BERT based named\nentity recognition for automated Hadith narrator identiﬁcation,’’ Int. J. Adv.\nComput. Sci. Appl., vol. 13, no. 1, pp. 1–9, 2022.\n[53] T. Tarmom, E. Atwell, and M. Alsalka, ‘‘Non-authentic Hadith corpus:\nDesign and methodology,’’ in Proc. IMAN. Leeds, U.K., 2019, pp. 13–19.\n[54] S. Altammami, E. Atwell, and A. Alsalka, ‘‘The Arabic-english parallel\ncorpus of authentic Hadith,’’ Int. J. Islamic Appl. Comput. Sci. Technol.,\nvol. 8, no. 2, 2020, pp. 1–10.\n[55] A. Mohamed and M. A. Jamaoui. Hadith-Data-Sets. Accessed:\nJun. 26, 2022. [Online]. Available: https://github.com/abdelrahmaan/\nHadith-Data-Sets\n[56] K. Grzegorczyk, ‘‘Vector representations of text data in deep learning,’’\n2019, arXiv:1901.01695.\n[57] A. I. Kadhim, Y .-N. Cheah, I. A. Hieder, and R. A. Ali, ‘‘Improving TF-\nIDF with singular value decomposition (SVD) for feature extraction on\nTwitter,’’ in Proc. 3rd Int. Eng. Conf. Develop. Civil Comput. Eng. Appl.,\n2017, pp. 1–9.\n[58] G. W. Stewart, ‘‘On the early history of the singular value decomposition,’’\nSIAM Rev., vol. 35, no. 4, pp. 551–566, 1993.\n[59] L. Breiman, ‘‘Random forests,’’ Mach. Learn., vol. 45, no. 1, pp. 5–32,\n2001.\n[60] J. S. Cramer, ‘‘The origins of logistic regression,’’ Tinbergen Inst. Discuss.\nPaper 02-119/4, 2002.\n[61] E. S. Pearson, ‘‘Bayes’ theorem, examined in the light of experimental\nsampling,’’Biometrika, vol. 17, nos. 3–4, pp. 388–442, Dec. 1925.\n[62] J. H. Friedman, ‘‘Stochastic gradient boosting,’’ Comput. Statist. Data\nAnal., vol. 38, no. 4, pp. 367–378, 2002.\n[63] T. Chen and C. Guestrin, ‘‘XGBoost: A scalable tree boosting system,’’\nin Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining,\nAug. 2016, pp. 785–794.\n[64] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y . Liu,\n‘‘LightGBM: A highly efﬁcient gradient boosting decision tree,’’ in Proc.\nAdv. Neural Inf. Process. Syst., vol. 30, 2017, pp. 3149–3157.\n[65] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, N. A. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. 31st\nNIPS, Red Hook, NY , USA: Curran Associates, 2017, pp. 6000–6010.\n[66] G. Wenzek, M.-A. Lachaux, A. Conneau, V . Chaudhary, F. Guzmán,\nA. Joulin, and E. Grave, ‘‘CCNet: Extracting high quality monolingual\ndatasets from web crawl data,’’ 2019, arXiv:1911.00359.\n[67] M. Abdul-Mageed, A. Elmadany, and E. M. B. Nagoudi, ‘‘ARBERT\nMARBERT: Deep bidirectional transformers for Arabic,’’ in Proc. 59th\nAnnu. Meeting Assoc. Comput. Linguistics 11th Int. Joint Conf. Natural\nLang. Process. (Long Papers), vol. 1, 2021.\n[68] L. Nigst, M. Romanov, S. B. Savant, M. Seydi, and P. Verkinderen,\n‘‘OpenITI: A machine-readable corpus of islamicate texts,’’ Oct. 2020, doi:\n10.5281/zenodo.4075046.\n[69] J. M. Lobo, A. Jiménez-Valverde, and R. Real, ‘‘AUC: A misleading\nmeasure of the performance of predictive distribution models,’’ Global\nEcol. Biogeogr., vol. 17, no. 2, pp. 145–151, Mar. 2008.\n[70] J. Huang and C. X. Ling, ‘‘Using AUC and accuracy in evaluating learning\nalgorithms,’’ IEEE Trans. Knowl. Data Eng., vol. 17, no. 3, pp. 299–310,\nMar. 2005.\n[71] M. A. H. Omer and S.-L. Ma, ‘‘Stemming algorithm to classify Arabic\ndocuments,’’J. Commun. Comput., vol. 7, no. 9, 2010.\n[72] S. González-Carvajal and E. C. Garrido-Merchán, ‘‘Comparing\nBERT against traditional machine learning text classiﬁcation,’’ 2020,\narXiv:2005.13012.\n[73] D. Patel, P. Raval, R. Parikh, and Y . Shastri, ‘‘Comparative\nstudy of machine learning models and BERT on SQuAD,’’ 2020,\narXiv:2005.11313.\n[74] Q. G. To, K. G. To, V .-A.-N. Huynh, N. T. Q. Nguyen, D. T. N. Ngo,\nS. J. Alley, A. N. Q. Tran, A. N. P. Tran, N. T. T. Pham, T. X. Bui, and\nC. Vandelanotte, ‘‘Applying machine learning to identify anti-vaccination\ntweets during the COVID-19 pandemic,’’ Int. J. Environ. Res. Public\nHealth, vol. 18, no. 8, p. 4069, Apr. 2021.\nKAMEL GAANOUNreceived the M.Sc. degree in data science and statisti-\ncal modeling from Université Bretagne Sud, France. He is currently a Data\nScientist and a member of the Association of Business Intelligence (AMID).\nHis current research interests include artiﬁcial intelligence, Arabic natural\nlanguage processing, transformers models, and data-centric approaches.\nMOHAMMED ALSUHAIBANIreceived the M.Sc. degree in advance com-\nputer science and the Ph.D. degree in computer science from The University\nof Liverpool, U.K. He is currently an Assistant Professor and the Head of the\nDepartment of Computer Science, College of Computer, Qassim University,\nSaudi Arabia. His research interests include artiﬁcial intelligence, machine\nlearning, computational linguistics, and natural language processing ﬁelds.\n113342 VOLUME 10, 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.718218207359314
    },
    {
      "name": "Transformer",
      "score": 0.5408303141593933
    },
    {
      "name": "Electrical engineering",
      "score": 0.16565266251564026
    },
    {
      "name": "Voltage",
      "score": 0.09681621193885803
    },
    {
      "name": "Engineering",
      "score": 0.08654522895812988
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I156216236",
      "name": "Qassim University",
      "country": "SA"
    }
  ],
  "cited_by": 11
}