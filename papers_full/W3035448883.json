{
  "title": "Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer",
  "url": "https://openalex.org/W3035448883",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2124772336",
      "name": "Jianfei Yu",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2007802817",
      "name": "Jing Jiang",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A1993907199",
      "name": "Li Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098083858",
      "name": "Rui Xia",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1502473750",
    "https://openalex.org/W2143933463",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W2798298921",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W3011594683",
    "https://openalex.org/W2045993505",
    "https://openalex.org/W2757931374",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963641259",
    "https://openalex.org/W2588986918",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2906635035",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2857028992",
    "https://openalex.org/W2250729567",
    "https://openalex.org/W2008830554",
    "https://openalex.org/W2096765155",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2153848201",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2004763266",
    "https://openalex.org/W2808393080",
    "https://openalex.org/W2962982907",
    "https://openalex.org/W2123661878",
    "https://openalex.org/W2788647998",
    "https://openalex.org/W2964266863",
    "https://openalex.org/W2056451646",
    "https://openalex.org/W2251559320",
    "https://openalex.org/W2168596788",
    "https://openalex.org/W2018268077",
    "https://openalex.org/W2046240631",
    "https://openalex.org/W2123512824",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts. Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context. To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations. To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions. Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3342–3352\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n3342\nImproving Multimodal Named Entity Recognition via Entity Span\nDetection with Uniﬁed Multimodal Transformer\nJianfei Yu1, Jing Jiang2, Li Yang3, and Rui Xia1,∗\n1 School of Artiﬁcial Intelligence, Nanjing University of Science & Technology, China\n2 School of Information Systems, Singapore Management University, Singapore\n3 DBS Bank, Singapore\n{jfyu, rxia}@njust.edu.cn, jingjiang@smu.edu.sg, liyang@dbs.com\nAbstract\nIn this paper, we study Multimodal Named\nEntity Recognition (MNER) for social media\nposts. Existing approaches for MNER mainly\nsuffer from two drawbacks: (1) despite gener-\nating word-aware visual representations, their\nword representations are insensitive to the vi-\nsual context; (2) most of them ignore the bias\nbrought by the visual context. To tackle the\nﬁrst issue, we propose a multimodal interac-\ntion module to obtain both image-aware word\nrepresentations and word-aware visual repre-\nsentations. To alleviate the visual bias, we fur-\nther propose to leverage purely text-based en-\ntity span detection as an auxiliary module, and\ndesign a Uniﬁed Multimodal Transformer to\nguide the ﬁnal predictions with the entity span\npredictions. Experiments show that our uni-\nﬁed approach achieves the new state-of-the-art\nperformance on two benchmark datasets.\n1 Introduction\nRecent years have witnessed the explosive growth\nof user-generated contents on social media plat-\nforms such as Twitter. While empowering users\nwith rich information, the ﬂourish of social media\nalso solicits the emerging need of automatically ex-\ntracting important information from these massive\nunstructured contents. As a crucial component of\nmany information extraction tasks, named entity\nrecognition (NER) aims to discover named enti-\nties in free text and classify them into pre-deﬁned\ntypes, such as person (PER), location (LOC) and\norganization (ORG). Given its importance, NER\nhas attracted much attention in the research com-\nmunity (Yadav and Bethard, 2018).\nAlthough many methods coupled with either dis-\ncrete shallow features (Zhou and Su, 2002; Finkel\net al., 2005; Torisawa et al., 2007) or continuous\ndeep features (Lample et al., 2016; Ma and Hovy,\n∗Corresponding author.\n(a). [ Kevin Durant PER ] enters\n[Oracle Arena LOC ] wearing off\n— White x [ Jordan MISC]\n(b). V ote for [ King of the Jungle\nMISC] — [ Kian PER] or [ David\nPER] ?\nFigure 1: Two examples for Multimodal Named Entity\nRecognition (MNER). Named entities and their entity types\nare highlighted.\n2016) have shown success in identifying entities in\nformal newswire text, most of them perform poorly\non informal social media text (e.g., tweets) due\nto its short length and noisiness. To adapt existing\nNER models to social media, various methods have\nbeen proposed to incorporate many tweet-speciﬁc\nfeatures (Ritter et al., 2011; Li et al., 2012, 2014;\nLimsopatham and Collier, 2016). More recently,\nas social media posts become increasingly multi-\nmodal, several studies proposed to exploit useful\nvisual information to improve the performance of\nNER (Moon et al., 2018; Zhang et al., 2018; Lu\net al., 2018).\nIn this work, following the recent trend, we focus\non multimodal named entity recognition (MNER)\nfor social media posts, where the goal is to detect\nnamed entities and identify their entity types given\na {sentence, image}pair. For example, in Fig. 1.a,\nit is expected to recognize that Kevin Durant, Or-\nacle Arena, and Jordan belong to the category of\nperson names (i.e., PER), place names (i.e., LOC),\nand other names (i.e., MISC), respectively.\nWhile previous work has shown success of fus-\ning visual information into NER (Moon et al., 2018;\nZhang et al., 2018; Lu et al., 2018), they still suf-\nfer from several limitations: (1) The ﬁrst obstacle\n3343\nlies in the non-contextualized word representations,\nwhere each word is represented by the same vector,\nregardless of the context it occurs in. However,\nthe meanings of many polysemous entities in so-\ncial media posts often rely on its context words.\nTake Fig. 1.a as an example, without the context\nwords wearing off, it is hard to ﬁgure out whether\nJordan refers to a shoe brand or a person. (2) Al-\nthough most existing methods focus on modeling\ninter-modal interactions to obtain word-aware vi-\nsual representations, the word representations in\ntheir ﬁnal hidden layer are still based on the tex-\ntual context, which are insensitive to the visual\ncontext. Intuitively, the associated image often pro-\nvides more context to resolve polysemous entities,\nand should contribute to the ﬁnal word representa-\ntions (e.g., in Fig. 1.b, the image can supervise the\nﬁnal word representations of Kian and David to be\ncloser to persons than animals). (3) Most previous\napproaches largely ignore the bias of incorporating\nvisual information. Actually, in most social media\nposts, the associated image tends to highlight only\none or two entities in the sentence, without men-\ntioning the other entities. In these cases, directly\nintegrating visual information will inevitably lead\nthe model to better recognize entities highlighted\nby images, but fail to identify the other entities (e.g.,\nOracle Arenaand King of the Junglein Fig. 1).\nTo address these limitations, we resort to ex-\nisting pre-trained contextualized word representa-\ntions, and propose a uniﬁed multimodal architec-\nture based on Transformer (Vaswani et al., 2017),\nwhich can effectively capture inter-modality inter-\nactions and alleviate the visual bias. Speciﬁcally,\nwe ﬁrst adopt a recently pre-trained contextualized\nrepresentation model (Devlin et al., 2018) as our\nsentence encoder, whose multi-head self-attention\nmechanism can guide each word to capture the\nsemantic and syntactic dependency upon its con-\ntext. Second, to better capture the implicit align-\nments between words and images, we propose a\nmultimodal interaction (MMI) module, which es-\nsentially couples the standard Transformer layer\nwith cross-modal attention mechanism to produce\nan image-aware word representation and a word-\naware visual representation for each input word,\nrespectively. Finally, to largely eliminate the bias\nof the visual context, we propose to leverage text-\nbased entity span detection as an auxiliary task,\nand design a uniﬁed neural architecture based on\nTransformer. In particular, a conversion matrix is\ndesigned to construct the correspondence between\nthe auxiliary and the main tasks, so that the entity\nspan information can be fully utilized to guide the\nﬁnal MNER predictions.\nExperimental results show that our Uniﬁed Mul-\ntimodal Transformer (UMT) brings consistent per-\nformance gains over several highly competitive uni-\nmodal and multimodal methods, and outperforms\nthe state-of-the-art by a relative improvement of\n3.7% and 3.8% on two benchmarks, respectively.\nThe main contributions of this paper can be sum-\nmarized as follows:\n•We propose a Multimodal Transformer model\nfor the task of MNER, which empowers Trans-\nformer with a multimodal interaction mod-\nule to capture the inter-modality dynamics\nbetween words and images. To the best of\nour knowledge, this is the ﬁrst work to apply\nTransformer to MNER.\n•Based on the above Multimodal Transformer,\nwe further design a uniﬁed architecture to in-\ncorporate a text-based entity span detection\nmodule, aiming to alleviate the bias of the\nvisual context in MNER with the guidance\nof entity span predictions from this auxiliary\nmodule.\n2 Methodology\nIn this section, we ﬁrst formulate the MNER task,\nand give an overview of our method. We then delve\ninto the details of each component in our model.\nTask Formulation: Given a sentence Sand its\nassociated image V as input, the goal of MNER is\nto extract a set of entities from S, and classify each\nextracted entity into one of the pre-deﬁned types.\nAs with most existing work on MNER, we for-\nmulate the task as a sequence labeling problem.\nLet S = (s1,s2,...,s n) denote a sequence of in-\nput words, and y = (y1,y2,...,y n) be the corre-\nsponding label sequence, where yi ∈ Yand Y\nis the pre-deﬁned label set with the BIO2 tagging\nschema (Sang and Veenstra, 1999).\n2.1 Overall Architecture\nFig. 2.a illustrates the overall architecture of our\nUniﬁed Multimodal Transformer, which contains\nthree main components: (1) representation learning\nfor unimodal input; (2) a Multimodal Transformer\nfor MNER; and (3) a uniﬁed architecture with aux-\niliary entity span detection (ESD) module.\n3344\n[CLS]\n Kevin\n Durant\n enters\n ......\n \nJordan\n [SEP]\nResNet\nt0\n t1\n t2\n t3 ......\n tn\n tn+1\n r0\n r1\n r2\n r3 ......\n rn\nF1\n F2\n F3 ......\n Fn\nTransformer Layer  \nwith Self-Attention\nQ              K             V\nTransformer Layer \nwith Self-Attention\nQ              K              V\nB\n I\n O ......\n B\nh0\n h1\n h2\n h3 ......\n hn\n hn+1\nE1\n E2\n E3 ......\n En\nI-PER\n O ......\n B-MISC\nVisual Input Textual Input \n \n  \n v49......v2v1\nMultimodal Transformer for MNER\nText \nModality\nVisual \nModality\nConversion\nMatrix\nMultimodal \nInteraction \nModule\nBERT Encoder\n+\n+\nAuxiliary Entity Span\n Detection Module\nB-PER\nrn+1\nE3E2E1E0 En En+1\nEAEAEAEA EA EA\n......\n......\n...... E[SEP]EJordanEenters EDurantEKevinE[CLS]\nc0\n c1\n c2\n c3\n cn\n cn+1\n......\nCross-Modal \nAttention\nQ         K         V\nAdd & Norm\nAdd & Norm\nFeed Forward\n \n  \n v49v2v1\nVisual Modality\nCross-Modal \nAttention\nAdd & Norm\nFeed Forward\n \n  \n rn+1......r1r0\nText Modality\nQ         K         V\nCross-Modal \nAttention\nQ         K         V\nAdd & Norm\nAdd & Norm\nFeed Forward\nh0\n h1\n hn+1\np1\n p2 ......\n p49\n \n  \n an+1......a1a0\n  \n  \n bn+1......b1b0\n+\nσ\n \n  \n qn+1......q1q0\nAdd & Norm\n......\n......\nVisual\nGate\nFigure 2: (a). Overall Architecture of Our Uniﬁed Multimodal Transformer. (b). Multimodal Interaction (MMI) Module.\nAs shown at the bottom of Fig. 2.a, we ﬁrst ex-\ntract contextualized word representations and vi-\nsual block representations from the input sentence\nand the input image, respectively.\nThe right part of Fig. 2.a illustrates our Multi-\nmodal Transformer model for MNER. Speciﬁcally,\na Transformer layer is ﬁrst employed to derive each\nword’s textual hidden representation. Next, a multi-\nmodal interaction (MMI) module is devised to fully\ncapture the inter-modality dynamics between the\ntextual hidden representations and the visual block\nrepresentations. The hidden representations from\nMMI are then fed to a conditional random ﬁeld\n(CRF) layer to produce the label for each word.\nTo alleviate the visual bias in MNER, we further\nstack a purely text-based ESD module in the left\npart of Fig. 2.a, where we feed its hidden represen-\ntations to another CRF layer to predict each word’s\nentity span label. More importantly, to utilize this\nfor our main MNER task, we design a conversion\nmatrix to encode the dependency relations between\ncorresponding labels from ESD to MNER, so that\nthe entity span predictions from ESD can be inte-\ngrated to get the ﬁnal MNER label for each word.\n2.2 Unimodal Input Representations\nWord Representations: Due to the capability of giv-\ning different representations for the same word in\ndifferent contexts, we employ the recent contextu-\nalized representations from BERT (Devlin et al.,\n2018) as our sentence encoder. Following Devlin\net al. (2018), each input sentence is preprocessed by\ninserting two special tokens, i.e., appending [CLS]\nto the beginning and [SEP] to the end, respectively.\nFormally, let S′= (s0,s1,...,s n+1) be the modi-\nﬁed input sentence, where s0 and sn+1 denote the\ntwo inserted tokens. Let X = (x0,x1,..., xn+1)\nbe the word representations of S′, where xi is the\nsum of word, segment, and position embeddings\nfor each token si. As shown in the bottom left of\nFig. 2.a, X is then fed to the BERT encoder to ob-\ntain C = (c0,c1,..., cn+1), where ci ∈Rd is the\ngenerated contextualized representation for xi.\nVisual Representations: As one of the state-of-\nthe-art CNN models for image recognition, Resid-\nual Network (ResNet) (He et al., 2016) has shown\nits capability of extracting meaningful feature rep-\nresentations of the input image in its deep layers.\nWe therefore keep the output from the last convo-\nlutional layer in a pretrained 152-layer ResNet to\nrepresent each image, which essentially splits each\ninput image into 7 ×7=49 visual blocks with the\nsame size and represents each block with a 2048-\ndimensional vector. Speciﬁcally, given an input\nimage V, we ﬁrst resize it to 224×224 pixels, and\nobtain its visual representations from ResNet, de-\nnoted as U = (u1,u2,..., u49), where ui is the\n2048-dimensional vector representation for the i-th\nvisual block. To project the visual representations\ninto the same space of the word representations,\n3345\nwe further convert U with a linear transformation:\nV = W⊤\nu U, where Wu ∈R2048×d is the weight\nmatrix1. As shown in the bottom right of Fig. 2.a,\nV = (v1,v2,..., v49) is the visual representa-\ntions generated from ResNet.\n2.3 Multimodal Transformer for MNER\nIn this subsection, we present our proposed Multi-\nmodal Transformer for MNER.\nAs illustrated on the right of Fig. 2.a, we ﬁrst\nadd a standard Transformer layer over C to ob-\ntain each word’s textual hidden representation:\nR = (r0,r1,..., rn+1), where ri ∈Rd denotes\nthe generated hidden representation for xi.\nMotivation: While the above Transformer layer\ncan capture which context words are more rele-\nvant to the prediction of an input word xi, they fail\nto consider the associated visual context. On the\none hand, due to the short length of textual con-\ntents on social media, the additional visual context\nmay guide each word to learn better word repre-\nsentations. On the other hand, since each visual\nblock is often closely related to several input words,\nincorporating the visual block representation can\npotentially make the prediction of its related words\nmore accurately. Inspired by these observations,\nwe propose a multimodal interaction (MMI) mod-\nule to learn an image-aware word representation\nand a word-aware visual representation for each\nword.\n2.3.1 Image-Aware Word Representation\nCross-Modal Transformer (CMT) Layer:As shown\non the left of Fig. 2.b, to learn better word rep-\nresentations with the guidance of associated im-\nages, we ﬁrst employ an m-head cross-modal at-\ntention mechanism (Tsai et al., 2019), by treating\nV ∈Rd×49 as queries, and R ∈Rd×(n+1) as keys\nand values:\nCAi(V,R) =softmax([Wqi V]⊤[Wki R]√\nd/m\n)[Wvi R]⊤;\nMH-CA(V,R) =W′[CA1(V,R),..., CAm(V,R)]⊤,\nwhere CAi refers to the i-th head of cross-modal at-\ntention, {Wqi,Wki,Wvi}∈ Rd/m×d, and W′∈\nRd×d denote the weight matrices for the query, key,\nvalue, and multi-head attention, respectively. Next,\nwe stack another three sub-layers on top:\n˜P = LN(V + MH-CA(V,R)); (1)\nP = LN(˜P + FFN(˜P)), (2)\n1Bias terms are omitted to avoid confusion in this paper.\nwhere FFN is the feed-forward network (Vaswani\net al., 2017), LN is the layer normalization (Ba\net al., 2016), and P = (p1,p2,..., p49) is the\noutput representations of the CMT layer.\nCoupled CMT Layer: However, since the visual\nrepresentations are treated as queries in the above\nCMT layer, each generated vectorpi is correspond-\ning to the i-th visual block instead of the i-th input\nword. Ideally, the image-aware word representation\nshould be corresponding to each word.\nTo address this, we propose to couple P with an-\nother CMT layer, which treats the textual represen-\ntations R as queries, and P as keys and values. As\nshown in the top left of Fig. 2.a, this coupled CMT\nlayer generates the ﬁnal image-aware word repre-\nsentations, denoted by A = (a0,a1,..., an+1).\n2.3.2 Word-Aware Visual Representation\nTo obtain a visual representation for each word,\nit is necessary to align each word with its closely\nrelated visual blocks, i.e., assigning high/low atten-\ntion weights to its related/unrelated visual blocks.\nHence, as shown in the right part of Fig. 2.b, we\nuse a CMT layer by treating R as queries and V\nas keys and values, which can be considered as a\nsymmetric version of the left CMT layer. Finally,\nit generates the word-aware visual representations,\ndenoted by Q = (q0,q1,..., qn+1).\nVisual Gate: As pointed out in some previous\nstudies (Zhang et al., 2018; Lu et al., 2018), it is\nunreasonable to align many function words such\nas the, of, and well with any visual block. There-\nfore, it is important to incorporate a visual gate to\ndynamically control the contribution of visual fea-\ntures. Following the practice in previous work, we\ndesign a visual gate by combining the information\nfrom the above word representations A and visual\nrepresentations Q as follows:\ng = σ(W⊤\na A + W⊤\nq Q), (3)\nwhere {Wa, Wq}∈ Rd×d are weight matrices,\nand σis the element-wise sigmoid function. Based\non the gate output, we can obtain the ﬁnal word-\naware visual representations as B = g ·Q.\n2.3.3 CRF Layer\nTo integrate the word and the visual representations,\nwe concatenate A and B to obtain the ﬁnal hidden\nrepresentations H = (h0,h1,..., hn+1), where\nhi ∈R2d. Following Lample et al. (2016), we then\nfeed H to a standard CRF layer, which deﬁnes the\n3346\nprobability of the label sequence y given the input\nsentence Sand its associated image V:\nP(y|S,V ) = exp(score(H,y))∑\ny′ exp(score(H,y′)); (4)\nscore(H,y) =\nn∑\ni=0\nTyi,yi+1 +\nn∑\ni=1\nEhi,yi ; (5)\nEhi,yi = wyi\nMNER ·hi, (6)\nwhere Tyi,yi+1 is the transition score from the label\nyi to the label yi+1, Ehi,yi is the emission score of\nthe label yi for the i-th word, and wyi\nMNER ∈R2d is\nthe weight parameter speciﬁc to yi.\n2.4 Uniﬁed Multimodal Transformer\nMotivation: Since the Multimodal Transformer pre-\nsented above mainly focuses on modeling the inter-\nactions between text and images, it may lead the\nlearnt model to overemphasize the entities high-\nlighted by the image but ignore the remaining enti-\nties. To alleviate the bias, we propose to leverage\ntext-based entity span detection (ESD) as an aux-\niliary task based on the following observation. As\nResNet is pre-trained on ImageNet (Deng et al.,\n2009) for the image recognition task, its high-\nlevel representations are closely relevant to the\nﬁnal predictions, i.e., the types of contained ob-\njects. This indicates that the visual representations\nfrom ResNet should be quite useful for identifying\ntypes of the detected entities, but are not necessar-\nily relevant to detecting entity spans in the sentence.\nTherefore, we use purely text-based ESD to guide\nthe ﬁnal predictions for our main MNER task.\nAuxiliary Entity Span Detection Module: For-\nmally, we model ESD as another sequence labeling\ntask, and use z = (z1,...,z n) to denote the se-\nquence of labels, where zi ∈Z and Z= {B,I,O}.\nAs shown in the left part of Fig. 2.a, we employ\nanother Transformer layer to obtain its speciﬁc hid-\nden representations as T = (t0,t1,..., tn+1), fol-\nlowed by feeding it to a CRF layer to predict the\nprobability of the label sequence z given S:\nP(z|S) = exp(∑n\ni=0 Tzi,zi+1 + ∑n\ni=1 wzi\nESD ·ti)\n∑\nz′ exp(∑n\ni=0 Tz′\ni,z′\ni+1\n+ ∑n\ni=1 w\nz′\ni\nESD ·ti)\n,\nwhere wzi\nESD ∈Rd is the parameter speciﬁc to zi.\nConversion Matrix: Although ESD is modeled\nas an auxiliary task separated from MNER, the\ntwo tasks are highly correlated since each ESD\nlabel should be only corresponding to a subset of\nlabels in MNER. For example, given the sentence\nin Fig. 2.a, if the ﬁrst token is predicted to be the\nTWITTER-2015 TWITTER-2017\nEntity Type Train Dev Test Train Dev Test\nPerson 2217 552 1816 2943 626 621\nLocation 2091 522 1697 731 173 178\nOrganization 928 247 839 1674 375 395\nMiscellaneous 940 225 726 701 150 157\nTotal 6176 1546 5078 6049 1324 1351\nNum of Tweets 4000 1000 3257 3373 723 723\nTable 1: The basic statistics of our two Twitter datasets.\nbeginning of an entity in ESD (i.e., have the label\nB), it should be also the beginning of a typed entity\nin MNER (e.g., have the label B-PER).\nTo encode such inter-task correspondence, we\npropose to use a conversion matrixWc ∈R|Z|×|Y|,\nwhere each element Wc\nj,k deﬁnes the conversion\nprobability from Zj to Yk. Since we have some\nprior knowledge (e.g., the label B can only con-\nvert to a label subset {B-PER, B-LOC, B-ORG, B-\nMISC}), we initialize Wc as follows: if Zj is not\ncorresponding to Yk, Wc\nj,k is set to 0; otherwise,\nWc\nj,k is set to 1\n|Cj|, where Cj denotes a subset of\nYthat is corresponding to Zj.\nModiﬁed CRF Layer for MNER: After obtaining\nthe conversion matrix, we further propose to fully\nleverage the text-based entity span predictions to\nguide the ﬁnal predictions of MNER. Speciﬁcally,\nwe modify the CRF layer for MNER by incorporat-\ning the entity span information from ESD into the\nemission score deﬁned in Eqn. (6):\nEhi,yi = wyi\nMNER ·hi + wzi\nESD ·ti ·Wc\nzi,yi . (7)\n2.5 Model Training\nGiven a set of manually labeled training samples\nD = {Sj,V j,yj,zj}N\nj=1, our overall training ob-\njective function is a weighted sum of the sentence-\nlevel negative log-likelihood losses for our main\nMNER task and the auxiliary ESD task2:\nL= − 1\n|D|\nN∑\nj=1\n(\nlog P(yj|Sj,V j) +λlog P(zj|Sj)\n)\n,\nwhere λis a hyperparameter to control the contri-\nbution of the auxiliary ESD module.\n3 Experiments\nWe conduct experiments on two multimodal NER\ndatasets, comparing our Uniﬁed Multimodal Trans-\nformer (UMT) with a number of unimodal and\nmultimodal approaches.\n2We obtain zj by removing the type information in yj.\n3347\nTWITTER-2015 TWITTER-2017\nSingle Type (F1) Overall Single Type (F1) Overall\nModality Methods PER. LOC. ORG. MISC. P R F1 PER. LOC. ORG. MISC. P R F1\nBiLSTM-CRF 76.77 72.56 41.33 26.80 68.14 61.09 64.42 85.12 72.68 72.50 52.56 79.42 73.43 76.31\nCNN-BiLSTM-CRF 80.86 75.39 47.77 32.61 66.24 68.09 67.15 87.99 77.44 74.02 60.82 80.00 78.76 79.37\nText HBiLSTM-CRF 82.34 76.83 51.59 32.52 70.32 68.05 69.17 87.91 78.57 76.67 59.32 82.69 78.16 80.37\nBERT 84.72 79.91 58.26 38.81 68.30 74.61 71.32 90.88 84.00 79.25 61.63 82.19 83.72 82.95\nBERT-CRF 84.74 80.51 60.27 37.29 69.22 74.59 71.81 90.25 83.05 81.13 62.21 83.32 83.57 83.44\nGV ATT-HBiLSTM-CRF 82.66 77.21 55.06 35.25 73.96 67.90 70.80 89.34 78.53 79.12 62.21 83.41 80.38 81.87\nAdaCAN-CNN-BiLSTM-CRF 81.98 78.95 53.07 34.02 72.75 68.74 70.69 89.63 77.46 79.24 62.77 84.16 80.24 82.15\nText+Image GV ATT-BERT-CRF 84.43 80.87 59.02 38.14 69.15 74.46 71.70 90.94 83.52 81.91 62.75 83.64 84.38 84.01\nAdaCAN-BERT-CRF 85.28 80.64 59.39 38.88 69.87 74.59 72.15 90.20 82.97 82.67 64.83 85.13 83.20 84.10\nMT-BERT-CRF(Ours) 85.30 81.21 61.10 37.97 70.48 74.80 72.58 91.47 82.05 81.84 65.80 84.60 84.16 84.42\nUMT-BERT-CRF(Ours) 85.24 81.58†63.03† 39.45† 71.67 75.23 73.41 † 91.56†84.73†82.24 70.10† 85.28 85.34 85.31 †\nTable 2: Performance comparison on our two TWITTER datasets. †indicates that UMT-BERT-CRFis signiﬁcantly better than\nGV ATT-BERT-CRF and AdaCAN-BERT-CRF with p-value<0.05 based on paired t-test.\n3.1 Experiment Settings\nDatasets: We take two publicly available Twit-\nter datasets respectively constructed by Zhang\net al. (2018) and Lu et al. (2018) for MNER.\nSince the two datasets mainly include multi-\nmodal user posts published on Twitter during\n2014-2015 and 2016-2017, we denote them as\nTWITTER-2015 and TWITTER-2017 respec-\ntively. Table 1 shows the number of entities for\neach type and the counts of multimodal tweets in\nthe training, development, and test sets of the two\ndatasets3. We have released the two datasets pre-\nprocessed by us for research purpose via this link:\nhttps://github.com/jefferyYu/UMT.\nHyperparameters: For each unimodal and mul-\ntimodal approach compared in the experiments,\nthe maximum length of the sentence input and the\nbatch size are respectively set to 128 and 16. For\nour UMT approach, most hyperparameter settings\nfollow Devlin et al. (2018) with the following ex-\nceptions: (1) the word representations C are ini-\ntialized with the cased BERTbase model pre-trained\nby Devlin et al. (2018), and ﬁne-tuned during train-\ning. (2) we employ a pre-trained 152-layer ResNet4\nto initialize the visual representations U and keep\nthem ﬁxed during training. (3) For the number of\ncross-modal attention heads, we set it as m=12. (4)\nThe learning rate, the dropout rate, and the tradeoff\nparameter λare respectively set to 5e-5, 0.1, and\n0.5, which can achieve the best performance on the\ndevelopment set of both datasets via a small grid\nsearch over the combinations of [1e-5, 1e-4], [0.1,\n0.5], and [0.1, 0.9].\n3The TWITTER-2017 dataset released by Lu et al. (2018)\nis slightly different from the one used in their experiments, as\nthey later remove a small portion of tweets for privacy issues.\n4https://download.pytorch.org/models/resnet152-\nb121ed2d.pth.\n3.2 Compared Systems\nTo demonstrate the effect of our Uniﬁed Multi-\nmodal Transformer (UMT) model, we ﬁrst con-\nsider a number of representative text-based ap-\nproaches for NER: (1) BiLSTM-CRF (Huang et al.,\n2015), a pioneering study which eliminates the\nheavy reliance on hand-crafted features, and sim-\nply employs a bidirectional LSTM model followed\nby a CRF layer for each word’s ﬁnal prediction;\n(2) CNN-BiLSTM-CRF (Ma and Hovy, 2016), a\nwidely adopted neural network model for NER,\nwhich is an improvement of BiLSTM-CRF by re-\nplacing each word’s word embedding with the\nconcatenation of its word embedding and CNN-\nbased character-level word representations; (3)\nHBiLSTM-CRF (Lample et al., 2016), an end-to-\nend hierarchical LSTM architectures, which re-\nplaces the bottom CNN layer in CNN-BiLSTM-\nCRF with an LSTM layer to obtain the character-\nlevel word representations; (4) BERT (Devlin et al.,\n2018), a multi-layer bidirectional Transformer en-\ncoder, which gives contextualized representations\nfor each word, followed by stacking a softmax layer\nfor ﬁnal predictions; (5) BERT-CRF, a variant of\nBERT by replacing the softmax layer with a CRF\nlayer.\nBesides, we also consider several competitive\nmultimodal approaches for MNER: (1) GVATT-\nHBiLSTM-CRF (Lu et al., 2018), a state-of-the-art\napproach for MNER, which integrates HBiLSTM-\nCRF with the visual context by proposing a vi-\nsual attention mechanism followed by a visual\ngate to obtain word-aware visual representations;\n(2) AdaCAN-CNN-BiLSTM-CRF (Zhang et al.,\n2018), another state-of-the-art approach based on\nCNN-BiLSTM-CRF, which designs an adaptive co-\nattention network to induce word-aware visual rep-\nresentations for each word; (3) GVATT-BERT-CRF\n3348\nTWITTER-2015 TWITTER-2017\nMethods P R F1 P R F1\nUMT-BERT-CRF 71.67 75.23 73.41 85.28 85.34 85.31\nw/o ESD Module 70.48 74.80 72.58 84.60 84.16 84.42\nw/o Conversion Matrix 70.43 74.98 72.63 84.72 84.97 84.85\nw/o Image-Aware WR 70.33 75.44 72.79 83.83 85.94 84.87\nw/o Visual Gate 71.34 75.15 73.19 85.31 84.68 84.99\nTable 3: Ablation Study of Uniﬁed Multimodal Transformer.\nand AdaCAN-BERT-CRF, our two variants of the\nabove two multimodal approaches, which replace\nthe sentence encoder with BERT; (4) MT-BERT-\nCRF, our Multimodal Transformer model intro-\nduced in Section 2.3; (5) UMT-BERT-CRF, our uni-\nﬁed architecture by incorporating the auxiliary en-\ntity span detection module into Multimodal Trans-\nformer, as introduced in Section 2.4.\nAll the neural models are implemented with Py-\nTorch, and all the experiments are conducted on\nNVIDIA RTX 2080 TiGPUs.\n3.3 Main Results\nIn Table 2, we report the precision (P), recall (R),\nand F1 score ( F1) achieved by each compared\nmethod on our two Twitter datasets.\nFirst, comparing all the text-based approaches,\nwe can clearly observe that BERT outperforms the\nother compared methods with a signiﬁcant margin\non both datasets. Moreover, it is easy to see that\nempowering BERT with a CRF layer can further\nboost the performance. All these observations in-\ndicate that the contextualized word representations\nare indeed quite helpful for the NER task on social\nmedia texts, due to the context-aware characteris-\ntics. This agrees with our ﬁrst motivation.\nSecond, comparing the state-of-the-art multi-\nmodal approaches with their corresponding uni-\nmodal baselines, we can ﬁnd that the multimodal\napproaches can generally achieve better perfor-\nmance, which demonstrates that incorporating the\nvisual context is generally useful for NER. Besides,\nwe can see that although GVATT-HBiLSTM-CRF\nand AdaCAN-CNN-BiLSTM-CRF can signiﬁcantly\noutperform their unimodal baselines, the perfor-\nmance gains become relatively limited when re-\nplacing their sentence encoder with BERT. This\nsuggests the challenge and the necessity of propos-\ning a more effective multimodal approach.\nThird, in comparison with the two existing mul-\ntimodal methods, our Multimodal Transformer\nMT-BERT-CRF outperforms the state-of-the-art by\n2.5% and 2.8% respectively, and also achieves bet-\nFigure 3: The number\nof entities (shown in y-\naxis) that are incorrectly\npredicted by BERT-CRF,\nbut get corrected by each\nmultimodal method\nFigure 4: The number\nof entities (shown in y-\naxis) that are correctly\npredicted by BERT-CRF,\nbut wrongly predicted by\neach multimodal method\nter performance than their BERT variants. We con-\njecture that the performance gains mainly come\nfrom the following reason: the two multimodal\nmethods only focus on obtaining word-aware vi-\nsual representations, whereas our MT-BERT-CRF\napproach targets at generating both image-aware\nword representations and word-aware visual repre-\nsentations for each word. These observations are\nin line with our second motivation.\nFinally, comparing all the unimodal and multi-\nmodal approaches, it is clear to observe that our\nUniﬁed Multimodal Transformer (i.e., UMT-BERT-\nCRF) can achieve the best performance on both\ndatasets, outperforming the second best methods\nby 1.14% and 1.05%, respectively. This demon-\nstrates the usefulness of the auxiliary entity span\ndetection module, and indicates that the auxiliary\nmodule can help our Multimodal Transformer al-\nleviate the bias brought by the associated images,\nwhich agrees with our third motivation.\n3.4 Ablation Study\nTo investigate the effectiveness of each component\nin our Uniﬁed Multimodal Transformer (UMT) ar-\nchitecture, we perform comparison between the\nfull UMT model and its ablations with respect to\nthe auxiliary entity span detection (ESD) module\nand the multimodal interaction (MMI) module.\nAs shown in Table 3, we can see that all the\ncomponents in UMT make important contributions\nto the ﬁnal results. On the one hand, removing\nthe whole ESD module will signiﬁcantly drop the\nperformance, which shows the importance of alle-\nviating the visual bias. In particular, discarding the\nconversion matrix in the ESD module also leads to\nthe performance drop, which indicates the useful-\nness of capturing the label correspondence between\nthe auxiliary module and our main MNER task.\nOn the other hand, as the main contribution of\n3349\nImportance of the MMI Module Importance of the ESD Module Importance of Associated Images Noise of Associated Images\nA. Review of [Wolf Hall MISC]1, Episode\n1 : Three Card Trick (bit.ly/1BHnWNb)\n#[WolfHall MISC]2\nB. [Kevin Love PER ]1 was more ex-\ncited about [GameofThrones MISC]2\nthan beating the [Hawks ORG]3\nC. My mum took some awesome\nphotos of @ [iamrationale PER]1\nand @ [bastilledan PER]2.\nD. Ask [ Siri MISC]1 what\n0 divided by 0 is and watch\nher put you in your place.\nBERT-CRF: 1-LOC \u0017, 2-LOC \u0017 1-PER\u0013, 2-MISC\u0013, 3-ORG \u0013 1-MISC\u0017, 2-ORG\u0017 1-MISC\u0013\nAdaCAN-BERT-CRF: 1-LOC\u0017, 2-LOC \u0017 1-PER\u0013, 2-NONE\u0017, 3-ORG\u0013 1-PER\u0013, 2-PER \u0013 1-PER\u0017\nMT-BERT-CRF: 1-MISC \u0013, 2-MISC\u0013 1-PER\u0013, 2-NONE\u0017, 3-ORG\u0013 1-PER\u0013, 2-PER \u0013 1-PER\u0017\nUMT-BERT-CRF: 1-MISC \u0013, 2-MISC\u0013 1-PER\u0013, 2-MISC\u0013, 3-ORG \u0013 1-PER\u0013, 2-PER \u0013 1-PER\u0017\nTable 4: The second row shows several representative samples together with their manually labeled entities in the test set of our\ntwo TWITTER datasets, and the bottom four rows show predicted entities of different methods on these test samples.\nour MMI module, Image-Aware Word Representa-\ntions (WR) demonstrates its indispensable role in\nthe ﬁnal performance due to the moderate perfor-\nmance drop after removal. Besides, removing the\nvisual gate also results in minor performance drop,\nindicating its importance to the full model.\n3.5 Further Analysis\nImportance of MMI and ESD Modules: To bet-\nter appreciate the importance of two main contribu-\ntions (i.e., MMI and ESD modules) in our proposed\napproaches, we conduct additional analysis on our\ntwo test sets. In Fig. 3 and Fig. 4, we show the num-\nber of entities that are wrongly/correctly predicted\nby BERT-CRF, but correctly/wrongly predicted by\neach multimodal method5.\nFirst, we can see from Fig. 3 that with the MMI\nmodule, our MT-BERT-CRF and UMT-BERT-CRF\napproaches correctly identify more entities, com-\npared with the two multimodal baselines. Table 4.A\nshows a speciﬁc example. We can see that our two\nmethods correctly classify the type of Wolf Hall\nas MISC whereas the compared systems wrongly\npredict its type as LOC, probably because our MMI\nmodule enforces the image-aware word representa-\ntions of Wolf Hallto be closer to drama names.\nSecond, in Fig. 4, it is clear to observe that com-\npared with the other three methods, UMT-BERT-\nCRF can signiﬁcantly decrease the bias brought by\nthe visual context due to incorporating our auxiliary\nESD module. In Table 4.B, we show a concrete\nexample: since Game of Thronesis ignored by the\nimage, the two multimodal baselines fail to iden-\ntify them; in contrast, with the help of the auxiliary\n5Note that here we use strict matches (i.e., correct span\nand type predictions).\nESD module, UMT-BERT-CRF successfully elimi-\nnates the bias.\nEffect of Incorporating Images: To obtain a\nbetter understanding of the general effect of incor-\nporating associated images into our MNER task,\nwe carefully examine our test sets and choose two\nrepresentative test samples to compare the predic-\ntion results of different approaches.\nFirst, we observe that most improvements gained\nby multimodal methods come from those samples\nwhere the textual contents are informal or incom-\nplete but the visual context provides useful clues.\nFor example, in Table 4.C, we can see that without\nthe visual context, BERT-CRF fails to identify that\nthe two entities refer to two singers in the concert,\nbut all the multimodal approaches can correctly\nclassify their types after incorporating the image.\nSecond, by manually checking the test set of our\ntwo datasets, we ﬁnd that in around 5% of the so-\ncial media posts, the associated images might be\nirrelevant to the textual contents due to two kinds\nof reasons: (1) these posts contain image memes,\ncartoons, or photos with metaphor; (2) their im-\nages and textual contents reﬂect different aspects\nof the same event. In such cases, we observe that\nmultimodal approaches generally perform worse\nthan BERT-CRF. A speciﬁc example is given in Ta-\nble 4.D, where all the multimodal methods wrongly\nclassify Siri as PER because of the unrelated face\nin the image.\n4 Related Work\nAs a crucial component of many information ex-\ntraction tasks including entity linking (Derczynski\net al., 2015), opinion mining (Maynard et al., 2012),\nand event detection (Ritter et al., 2012), named\n3350\nentity recognition (NER) has attracted much at-\ntention in the research community in the past two\ndecades (Li et al., 2018).\nMethods for NER: In the literature, various su-\npervised learning approaches have been proposed\nfor NER. Traditional approaches typically focus\non designing various effective NER features, fol-\nlowed by feeding them to different linear classi-\nﬁers such as maximum entropy, conditional random\nﬁelds (CRFs), and support vector machines (Chieu\nand Ng, 2002; Florian et al., 2003; Finkel et al.,\n2005; Ratinov and Roth, 2009; Lin and Wu, 2009;\nPassos et al., 2014; Luo et al., 2015). To re-\nduce the feature engineering efforts, a number of\nrecent studies proposed to couple different neu-\nral network architectures with a CRF layer (Laf-\nferty et al., 2001) for word-level predictions, in-\ncluding convolutional neural networks (Collobert\net al., 2011), recurrent neural networks (Chiu and\nNichols, 2016; Lample et al., 2016), and their hier-\narchical combinations (Ma and Hovy, 2016). These\nneural approaches have been shown to achieve the\nstate-of-the-art performance on different bench-\nmark datasets based on formal text (Yang et al.,\n2018).\nHowever, when applying these approaches to\nsocial media text, most of them fail to achieve\nsatisfactory results. To address this issue, many\nstudies proposed to exploit external resources (e.g.,\nshallow parser, Freebase dictionary, and ortho-\ngraphic characteristics) to incorporate a set of\ntweet-speciﬁc features into both traditional ap-\nproaches (Ritter et al., 2011; Li et al., 2014; Bald-\nwin et al., 2015) and recent neural approaches (Lim-\nsopatham and Collier, 2016; Lin et al., 2017),\nwhich can obtain much better performance on so-\ncial media text.\nMethods for Multimodal NER (MNER): As\nmultimodal data become increasingly popular on\nsocial media platforms, several recent studies focus\non the MNER task, where the goal is to leverage\nthe associate images to better identify the named\nentities contained in the text. Speciﬁcally, Moon\net al. (2018) proposed a multimodal NER network\nwith modality attention to fuse the textual and vi-\nsual information. To model the inter-modal interac-\ntions and ﬁlter out the noise in the visual context,\nZhang et al. (2018) and Lu et al. (2018) respectively\nproposed an adaptive co-attention network and a\ngated visual attention mechanism for MNER. In\nthis work, we follow this line of work. But different\nfrom them, we aim to propose an effective multi-\nmodal method based on the recent Transformer\narchitecture (Vaswani et al., 2017). To the best\nof our knowledge, this is the ﬁrst work to apply\nTransformer to the task of MNER.\n5 Conclusion\nIn this paper, we ﬁrst presented a Multimodal Trans-\nformer architecture for the task of MNER, which\ncaptures the inter-modal interactions with a multi-\nmodal interaction module. Moreover, to alleviate\nthe bias of the visual context, we further proposed a\nUniﬁed Multimodal Transformer (UMT), which in-\ncorporates an entity span detection module to guide\nthe ﬁnal predictions for MNER. Experimental re-\nsults show that our UMT approach can consistently\nachieve the best performance on two benchmark\ndatasets.\nThere are several future directions for this work.\nOn the one hand, despite bringing performance\nimprovements over existing MNER methods, our\nUMT approach still fails to perform well on so-\ncial media posts with unmatched text and images,\nas analyzed in Section 3.5. Therefore, our next\nstep is to enhance UMT so as to dynamically ﬁlter\nout the potential noise from images. On the other\nhand, since the size of existing MNER datasets\nis relatively small, we plan to leverage the large\namount of unlabeled social media posts in different\nplatforms, and propose an effective framework to\ncombine them with the small amount of annotated\ndata to obtain a more robust MNER model.\nAcknowledgments\nWe would like to thank three anonymous reviewers\nfor their valuable comments. This research is sup-\nported by the National Research Foundation, Sin-\ngapore under its International Research Centres in\nSingapore Funding Initiative, and the Natural Sci-\nence Foundation of China under Grant 61672288.\nAny opinions, ﬁndings and conclusions or recom-\nmendations expressed in this material are those of\nthe authors and do not reﬂect the views of National\nResearch Foundation, Singapore.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\n3351\nTimothy Baldwin, Marie-Catherine de Marneffe,\nBo Han, Young-Bum Kim, Alan Ritter, and Wei Xu.\n2015. Shared tasks of the 2015 workshop on noisy\nuser-generated text: Twitter lexical normalization\nand named entity recognition. In Proceedings of the\nWorkshop on Noisy User-generated Text, pages 126–\n135.\nHai Leong Chieu and Hwee Tou Ng. 2002. Named en-\ntity recognition: a maximum entropy approach using\nglobal information. In Proceedings of COLING.\nJason PC Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional lstm-cnns. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:357–370.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\nNatural language processing (almost) from scratch.\nJournal of Machine Learning Research, 12:2461–\n2505.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE conference\non computer vision and pattern recognition, pages\n248–255.\nLeon Derczynski, Diana Maynard, Giuseppe Rizzo,\nMarieke Van Erp, Genevieve Gorrell, Rapha ¨el\nTroncy, Johann Petrak, and Kalina Bontcheva. 2015.\nAnalysis of named entity recognition and linking\nfor tweets. Information Processing & Management,\n51(2):32–49.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJenny Rose Finkel, Trond Grenager, and Christopher\nManning. 2005. Incorporating non-local informa-\ntion into information extraction systems by gibbs\nsampling. In Proceedings of ACL.\nRadu Florian, Abe Ittycheriah, Hongyan Jing, and\nTong Zhang. 2003. Named entity recognition\nthrough classiﬁer combination. In Proceedings of\nNAACL.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of CVPR, pages 770–778.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-\ntional lstm-crf models for sequence tagging. arXiv\npreprint arXiv:1508.01991.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data. In Proceedings of ICML.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of NAACL-HLT.\nChenliang Li, Aixin Sun, Jianshu Weng, and Qi He.\n2014. Tweet segmentation and its application to\nnamed entity recognition. IEEE Transactions on\nknowledge and data engineering, 27(2):558–570.\nChenliang Li, Jianshu Weng, Qi He, Yuxia Yao, An-\nwitaman Datta, Aixin Sun, and Bu-Sung Lee. 2012.\nTwiner: named entity recognition in targeted twitter\nstream. In Proceedings of SIGIR.\nJing Li, Aixin Sun, Jianglei Han, and Chenliang Li.\n2018. A survey on deep learning for named entity\nrecognition. arXiv preprint arXiv:1812.09449.\nNut Limsopatham and Nigel Collier. 2016. Bidirec-\ntional LSTM for named entity recognition in twitter\nmessages. In Proceedings of the 2nd Workshop on\nNoisy User-generated Text (WNUT).\nBill Yuchen Lin, Frank F Xu, Zhiyi Luo, and Kenny\nZhu. 2017. Multi-channel bilstm-crf model for\nemerging named entity recognition in social media.\nIn Proceedings of the 3rd Workshop on Noisy User-\ngenerated Text.\nDekang Lin and Xiaoyun Wu. 2009. Phrase clustering\nfor discriminative learning. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP.\nDi Lu, Leonardo Neves, Vitor Carvalho, Ning Zhang,\nand Heng Ji. 2018. Visual attention model for name\ntagging in multimodal social media. In Proceedings\nof ACL, pages 1990–1999.\nGang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-\niqing Nie. 2015. Joint entity recognition and disam-\nbiguation. In Proceedings of EMNLP.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via bi-directional lstm-cnns-crf. In\nProceedings of ACL.\nDiana Maynard, Kalina Bontcheva, and Dominic Rout.\n2012. Challenges in developing opinion mining\ntools for social media. In Proceedings of the@ NLP\ncan u tag usergeneratedcontent.\nSeungwhan Moon, Leonardo Neves, and Vitor Car-\nvalho. 2018. Multimodal named entity recognition\nfor short social media posts. In Proceedings of\nNAACL.\nAlexandre Passos, Vineet Kumar, and Andrew McCal-\nlum. 2014. Lexicon infused phrase embeddings for\nnamed entity resolution. In Proceedings of CoNLL.\nLev Ratinov and Dan Roth. 2009. Design challenges\nand misconceptions in named entity recognition. In\nProceedings of CoNLL.\n3352\nAlan Ritter, Sam Clark, Mausam, and Oren Etzioni.\n2011. Named entity recognition in tweets: an ex-\nperimental study. In Proceedings of ACL.\nAlan Ritter, Oren Etzioni, and Sam Clark. 2012. Open\ndomain event extraction from twitter. In Proceed-\nings of SIGKDD.\nErik F Sang and Jorn Veenstra. 1999. Representing text\nchunks. In Proceedings of EACL.\nKentaro Torisawa et al. 2007. Exploiting wikipedia as\nexternal knowledge for named entity recognition. In\nProceedings of EMNLP-CoNLL.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. 2019. Multimodal transformer for\nunaligned multimodal language sequences. In Pro-\nceedings of ACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NIPS, pages 5998–\n6008.\nVikas Yadav and Steven Bethard. 2018. A survey on re-\ncent advances in named entity recognition from deep\nlearning models. In Proceedings of COLING.\nJie Yang, Shuailong Liang, and Yue Zhang. 2018. De-\nsign challenges and misconceptions in neural se-\nquence labeling. In Proceedings of COLING.\nQi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang.\n2018. Adaptive co-attention network for named en-\ntity recognition in tweets. In Proceedings of AAAI,\npages 5674–5681.\nGuoDong Zhou and Jian Su. 2002. Named entity\nrecognition using an hmm-based chunk tagger. In\nProceedings of ACL.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8400511741638184
    },
    {
      "name": "Transformer",
      "score": 0.7326661348342896
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6229549646377563
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5773585438728333
    },
    {
      "name": "Natural language processing",
      "score": 0.5643543004989624
    },
    {
      "name": "Named-entity recognition",
      "score": 0.42432230710983276
    },
    {
      "name": "Speech recognition",
      "score": 0.4086366891860962
    },
    {
      "name": "Task (project management)",
      "score": 0.07132318615913391
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    }
  ],
  "cited_by": 221
}