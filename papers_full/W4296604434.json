{
  "title": "M2TRec: Metadata-aware Multi-task Transformer for Large-scale and Cold-start free Session-based Recommendations",
  "url": "https://openalex.org/W4296604434",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5102921373",
      "name": "Walid Shalaby",
      "affiliations": [
        "Home Depot (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5086691172",
      "name": "Sejoon Oh",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5068120387",
      "name": "Amir Afsharinejad",
      "affiliations": [
        "Home Depot (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5056142478",
      "name": "Srijan Kumar",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5102967365",
      "name": "Xiquan Cui",
      "affiliations": [
        "Home Depot (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2982275843",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W3200664681",
    "https://openalex.org/W2951369132",
    "https://openalex.org/W4289488615",
    "https://openalex.org/W2626454364",
    "https://openalex.org/W2512965516",
    "https://openalex.org/W3206932362",
    "https://openalex.org/W2753328553",
    "https://openalex.org/W2964044287",
    "https://openalex.org/W2972941122",
    "https://openalex.org/W3035588407",
    "https://openalex.org/W3034427927",
    "https://openalex.org/W3197173998",
    "https://openalex.org/W3031331881",
    "https://openalex.org/W2963669159",
    "https://openalex.org/W2082927600",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3208349097",
    "https://openalex.org/W2474765392",
    "https://openalex.org/W3088983215",
    "https://openalex.org/W2745534091",
    "https://openalex.org/W2513020047",
    "https://openalex.org/W4300175872",
    "https://openalex.org/W2803744611",
    "https://openalex.org/W2953586472",
    "https://openalex.org/W2777210537",
    "https://openalex.org/W3034329572",
    "https://openalex.org/W2899457523",
    "https://openalex.org/W2964926209",
    "https://openalex.org/W3021402147",
    "https://openalex.org/W3112334685",
    "https://openalex.org/W3102619277",
    "https://openalex.org/W3199647641",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W3101707147",
    "https://openalex.org/W3013802045",
    "https://openalex.org/W3200585014",
    "https://openalex.org/W2086511124",
    "https://openalex.org/W3210938103",
    "https://openalex.org/W2911946608",
    "https://openalex.org/W4299286960",
    "https://openalex.org/W4287080796",
    "https://openalex.org/W3100538332",
    "https://openalex.org/W2792643794",
    "https://openalex.org/W3166827814",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3104492324",
    "https://openalex.org/W3152513422",
    "https://openalex.org/W3106433415"
  ],
  "abstract": "Session-based recommender systems (SBRSs) have shown superior performance\\nover conventional methods. However, they show limited scalability on\\nlarge-scale industrial datasets since most models learn one embedding per item.\\nThis leads to a large memory requirement (of storing one vector per item) and\\npoor performance on sparse sessions with cold-start or unpopular items. Using\\none public and one large industrial dataset, we experimentally show that\\nstate-of-the-art SBRSs have low performance on sparse sessions with sparse\\nitems. We propose M2TRec, a Metadata-aware Multi-task Transformer model for\\nsession-based recommendations. Our proposed method learns a transformation\\nfunction from item metadata to embeddings, and is thus, item-ID free (i.e.,\\ndoes not need to learn one embedding per item). It integrates item metadata to\\nlearn shared representations of diverse item attributes. During inference, new\\nor unpopular items will be assigned identical representations for the\\nattributes they share with items previously observed during training, and thus\\nwill have similar representations with those items, enabling recommendations of\\neven cold-start and sparse items. Additionally, M2TRec is trained in a\\nmulti-task setting to predict the next item in the session along with its\\nprimary category and subcategories. Our multi-task strategy makes the model\\nconverge faster and significantly improves the overall performance.\\nExperimental results show significant performance gains using our proposed\\napproach on sparse items on the two datasets.\\n",
  "full_text": "M2TRec: Metadata-aware Multi-task Transformer for\nLarge-scale and Cold-start free Session-based Recommendations\nWalid Shalaby\nwalid_shalaby@homedepot.com\nThe Home Depot\nAtlanta, Georgia, USA\nSejoon Oh\nsoh337@gatech.edu\nGeorgia Institute of Technology\nAtlanta, Georgia, USA\nAmir Afsharinejad\namir_afsharinejad@homedepot.com\nThe Home Depot\nAtlanta, Georgia, USA\nSrijan Kumar\nsrijan@gatech.edu\nGeorgia Institute of Technology\nAtlanta, Georgia, USA\nXiquan Cui\nxiquan_cui@homedepot.com\nThe Home Depot\nAtlanta, Georgia, USA\nABSTRACT\nSession-based recommender systems (SBRSs) have shown superior\nperformance over conventional methods. However, they show lim-\nited scalability on large-scale industrial datasets since most models\nlearn one embedding per item. This leads to a large memory re-\nquirement (of storing one vector per item) and poor performance on\nsparse sessions with cold-start or unpopular items. Using one pub-\nlic and one large industrial dataset, we experimentally show that\nstate-of-the-art SBRSs have low performance on sparse sessions\nwith sparse items. We propose M2TRec, a Metadata-aware Multi-\ntask Transformer model for session-based recommendations. Our\nproposed method learns a transformation function from item meta-\ndata to embeddings, and is thus, item-ID free (i.e., does not need to\nlearn one embedding per item). It integrates item metadata to learn\nshared representations of diverse item attributes. During inference,\nnew or unpopular items will be assigned identical representations\nfor the attributes they share with items previously observed dur-\ning training, and thus will have similar representations with those\nitems, enabling recommendations of even cold-start and sparse\nitems. Additionally, M2TRec is trained in a multi-task setting to\npredict the next item in the session along with its primary cate-\ngory and subcategories. Our multi-task strategy makes the model\nconverge faster and significantly improves the overall performance.\nExperimental results show significant performance gains using our\nproposed approach on sparse items on the two datasets.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíRecommender systems.\nACM Reference Format:\nWalid Shalaby, Sejoon Oh, Amir Afsharinejad, Srijan Kumar, and Xiquan\nCui. 2022. M2TRec: Metadata-aware Multi-task Transformer for Large-scale\nand Cold-start free Session-based Recommendations. In Sixteenth ACM\nConference on Recommender Systems (RecSys ‚Äô22), September 18‚Äì23, 2022,\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nRecSys ‚Äô22, September 18‚Äì23, 2022, Seattle, WA, USA\n¬© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9278-5/22/09. . . $15.00\nhttps://doi.org/10.1145/3523227.3551477\nSeattle, WA, USA. ACM, New York, NY, USA, 6 pages. https://doi.org/10.\n1145/3523227.3551477\n1 INTRODUCTION\nSession-based recommender systems (SBRSs) accurately model se-\nquential and evolving preferences of users from their session data\n(e.g., clicks and add-to-cart events). Session data can be associated\nwith item metadata, allowing SBRSs to capture item dependencies\nat the attribute level within the session. However, most of the exist-\ning SBRSs take the IDs of users and items as the main input source\nto learn session contexts and produce next item recommendations\n[13, 14, 40]. Recent hybrid models demonstrated improved perfor-\nmance when combining item embeddings and their attributes to be\nused as additional side information [5, 10, 27, 31, 32, 32, 38].\nHowever, there are two major issues that these recommenda-\ntion models face. The first issue is that they cannot scale with the\ngigantic sizes of industrial datasets. For instance, The Home De-\npot (THD) industrial dataset used in this paper has approximately\n40 million sessions and 0.6 million items. Since the dataset is cre-\nated by sampling several months of online sessions, the actual\nfull dataset (e.g., a year-long one) will be even much bigger. Most\nSBRSs [5, 8, 13, 14, 19, 40, 41, 44] that utilize a large item-ID em-\nbedding matrix can suffer from slow training or memory shortage\nproblems.\nThe second issue arises due to cold-start items and sparse ses-\nsions, i.e., sessions that contain new or unpopular items. SBRSs will\nhave limited or no ability to generate good representations for such\nitems since they have no-to-few interactions. Moreover, many exist-\ning models are incapable of scoring and recommending new items\nunseen during training [25, 35]. Even combining metadata informa-\ntion with item-IDs, i.e., item embeddings, to learn compound item\nrepresentation results in only a slight performance improvement\ncompared to using item-ID only [5, 10, 16, 27, 31, 33, 38]. This can\nbe attributed to the model overfitting item-ID as the main feature.\nTo tackle the above issues, we propose M2TRec, a Metadata-\naware Multi-task Transformer model (Figure 1). M2TRec is com-\npletely item-ID free (i.e., no item-ID embeddings) and uses only item\nattributes such as title, category, brand, color, and other metadata to\nlearn item representations. Since M2TRec does not require creating\nand learning a large item-ID embedding matrix, it can be easily\napplied to large industrial datasets. In addition, new items will still\nhave accurate representations using their metadata attributes, and\narXiv:2209.11824v1  [cs.IR]  23 Sep 2022\nRecSys ‚Äô22, September 18‚Äì23, 2022, Seattle, WA, USA Shalaby et al.\nFigure 1: M2TRec Architecture. (left) M2TRec first encodes metadata of each item in a session into embeddings and feeds the\nconcatenation of those metadata embeddings to a Transformer encoder. A current session encoding is obtained via average-\npooling of the item encodings from Transformer, and the session encoding is used for next item and next category predictions.\n(right) Detailed architecture illustration of a Transformer encoder used in M2TRec.\ntheir similarity with previously-observed items can be captured\neven if they have no or few interactions with users, making it\nsuitable even in highly dynamic deployment settings where the\nitem catalog changes frequently. Finally, recent research has shown\nthat multi-task learning (MTL) results in increased performance\nand generalizability of the model [6, 7]. Thus, given the focus on\nleveraging item metadata, we design M2TRec as a multi-task SBRS\ntrained not only to predict the next item but also its category and\nsubcategories to enhance the prediction accuracy of an individual\ntask.\nVia thorough experiments, we demonstrate the superior perfor-\nmance of the proposed model on real-world datasets. Our multi-task\ntraining allows faster convergence, higher accuracy with fewer it-\nerations, and robust performance with fewer training data (some\nexperimental results are omitted due to space limits). Our scalable\narchitecture serves both item and category recommendations in\none model with higher prediction performance than baselines in\nboth tasks.\n2 RELATED WORK\nNeural networks have served as the main architecture of exist-\ning SBRSs. Earlier works [ 8‚Äì10] utilize a recurrent neural net-\nwork (RNN) to model sequential dependencies of items within\na session. Recently, attention-based approaches [13, 14, 19, 23, 37]\nand graph neural network (GNN)-based methods [39‚Äì42, 44] have\nbeen proposed to enhance the longer and deeper dependencies for\nSBRSs. Furthermore, Transformer [ 34]-based SBRSs [ 2, 4, 5, 18]\nshow the state-of-the-art prediction performance due to its pow-\nerful and efficient self-attention mechanism. Multi-task learning\n(MTL) [11, 16, 20] has been also adopted for SBRSs to enhance the\nnext item prediction via generalization. However, the above models\nhave shortcomings that they (1) are susceptible to cold-start items\nor sessions, (2) cannot predict the categories of the next item in a\nsession, or (3) are not scalable to the real-world billion-scale rec-\nommendation setting since they have to store item-IDs and their\nembeddings, which are this paper is solving.\nMethodologies are developed to incorporate item metadata into\nSBRSs for modeling user/item dependencies [5, 10, 12, 16, 21, 27, 28,\n31, 33, 38, 43]. However, the vast majority of such methodologies\nhave at least one of the following two shortcomings. The first short-\ncoming is that cold-start items and/or users are removed during the\npre-processing of datasets used for evaluations of proposed mod-\nels [5, 12, 21, 28, 43]. Item metadata alone cannot give the model the\nability to recommend such cold-start items [17]. The model needs\nto have a separate mechanism for using items‚Äô content informa-\ntion and representation to recommend cold-start items. Tagliabue\net al. [30] propose a pipeline to learn accurate cold-start item repre-\nsentations with small changes to an existing model infrastructure.\nHowever, a separate neural model needs to be trained to obtain the\ncold-start embeddings, which limits the scalability of the solution.\nRaziperchikolaei et al. [22] and Zheng et al . [45] suggest hybrid\nand metadata-aware recommendation models to predict implicit\nfeedback for cold-start items of users, respectively. However, those\nmodels are not designed for the session-based recommendation set-\nting. The second shortcoming of such methodologies is that they do\nnot make use of item titles or descriptions as attribute features for\ncapturing product similarities [12, 16, 21, 27, 33, 38]. In such cases,\nitem-IDs are used as inputs to represent different products. Such\nrepresentation is unable to incorporate any relevant information\nabout cold-start items as opposed to using title encodings.\n3 METHODOLOGY\nNext Item Prediction: We denote a user sessionS= [ùêº1,ùêº2,ùêº3,...,ùêº ùëõ]\nas a sequence of items a user interacted within that session. Each\nitem ùêºùëò = {ùê¥ùëò,1,ùê¥ùëò,2,ùê¥ùëò,3,...,ùê¥ ùëò,ùëö}is described by a set of ùëöat-\ntributes which could be context-specific or item-specific. In this\nwork, we consider item-specific attributes only (e.g., title, descrip-\ntion, category). Each attribute ùê¥could be either textual, categorical,\nor numerical. In the setting of session-based recommendations, we\nare given a session S, and our objective is to maximize the predic-\ntion probability of the next item the user is most likely to interact\nwith given all previous items in S. Formally, the probability of the\ntarget item ùêºùëõ can be formulated as:\nùëù(ùêºùëõ|S[ùêº<ùëõ];ùúÉ) (1)\nwhere ùúÉ denotes the model parameters and S[ùêº<ùëõ]denotes the\nsequence of items prior to the target itemùêºùëõ. As in previous works [9,\nM2TRec: Metadata-aware Multi-task Transformer for Session-based Recommendations RecSys ‚Äô22, September 18‚Äì23, 2022, Seattle, WA, USA\nTable 1: Statistics of datasets used in the experiments.\nDataset Diginetica THD\n# of training and test sessions 191K, 16K 39M, 1.5M\n# of items 119K 575K\nMetadata / Attributes Product title, Category\nProduct title, Categories (L1, L2, L3, Leaf),\nManufacturer, Brand, Department Name,\nClass Name, Color\nPrediction tasks Item-ID, Category Item-ID, Categories (L1, L2, L3, Leaf)\n13, 14, 40], we generate dense next item sub-sequences from each\nsession Sfor training and testing. Therefore, a session Swith\nùëõ items will be broken down into ùëõ ‚àí1 sub-sequences such as\n{([ùêº1],ùêº2),..., ([ùêº1,ùêº2,...,ùêº ùëõ‚àí1],ùêºùëõ)}, where ([X], Y) means X as the\ninput sequence of items and Y as the target next item.\nItem Metadata Encoding: Item metadata can be numerical, cate-\ngorical, or unstructured such as title, description, and image. We pro-\npose a unified method for representing all item attributes. The objec-\ntive is to map every attribute ùê¥into a real-valued vector ùë£ùê¥ ‚ààRùëëùê¥.\nNumerical attributes ùëü are represented as a single-valued vector\nùë£ùëü ‚ààR. Categorical attributes ùê∂ ‚àà{ùëê1,ùëê2,...,ùëê ùë†}are encoded into\nvectors ùë£ùê∂ using an embedding layer dedicated to each attribute,\ni.e.,\nùë£ùê∂ = ùëêùëñùúÉ(ùê∂) ‚ààRùëëùê∂ (2)\nwhere ùëêùëñ is the one-hot encoded value of ùê∂, ùúÉ(ùê∂) ‚ààRùë†√óùëëùê∂ are\nthe weights of the category embedding matrix, ùë† is the number of\npossible values of ùê∂, and ùëëùê∂ is the dimensionality of ùê∂‚Äôs vector.\nTextual attributesùëá are first tokenized using a subword tokenizer\n[26] to obtain individual tokens [w1,w2,..., wùë°]and then encoded\ninto vectors ùë£ùëá. A simple and efficient encoding strategy is to create\na dedicated embedding layer for ùëá to map each token w into a\nvector and then aggregate the token vectors using mean or max\npooling, i.e.,\nùë£ùëá = Poolùë°\nùëñ=1(ùë§ùëñùúÉ(ùëá))‚àà Rùëëùëá (3)\nwhere ùë§ùëñ is the one-hot encoded value of token wùëñ, ùúÉ(ùëá) ‚ààRùëò√óùëëùëá\nare the weights of the token embedding matrix,ùëòis vocabulary size\nof ùëá, and ùëëùëá is the dimensionality of ùëá‚Äôs vector. Further enhance-\nments to textual attributes encoding can be achieved by sharing\nthe encoding parameters across all textual attributes that have\nsimilar vocabularies such as item title, description, category, color,\netc. Although the weight-sharing scheme is expected to reduce the\ntraining time, it may increase the overall model size. This is because\nthe vector size would be the same for all the attributes that share\nthe same encoder regardless of their vocabulary size. This will lead\nto high memory and storage requirements when deploying the\nmodel in production. Alternatively, in this work, we use a separate\nembedding layer for each textual attribute as in Eq. (3) and choose\nits vector size proportional to the attribute‚Äôs vocabulary size.\nSession Encoding: After encoding all metadata features for an\nitem ùêºùëò at position ùëò in the input session ùëÜ, we concatenate all the\nfeature vectors to create a compound vector representation ùë£ùêºùëò for\nùêºùëò, i.e.,\nùë£ùêºùëò = ùëêùëúùëõùëêùëéùë°(ùë£ùê¥1,ùë£ùê¥2,...,ùë£ ùê¥ùëö)‚àà Rùëëùêº (4)\nwhere ùëëùêº is the summation of the lengths of all feature vectors. Note\nthat item-IDs are not used to create the compound representation\nùë£ùêºùëò in Equation (4). We then use the compound representations of\nitems in ùëÜ as input to the session encoder in a pre-fusion fashion\nto learn a session encoding ùë£ùëÜ. First, Transformer encoder [ 34]\ngenerates contextual encodings for each session item ùë£ùêºùëò, followed\nby an average-pooling layer to generate the session encoding ùë£ùëÜ,\ni.e.,\nùë£ùëÜ = Poolùëõ‚àí1\nùëò=1 (Trans-Enc(ùë£ùêºùëò;ùúÉ(ùëíùëõùëêùëÜ)))‚àà RùëëùëÜ (5)\nwhere ùúÉ(ùëíùëõùëêùëÜ)is the model parameters of the Transformer encoder\ntrained with sessions ùëÜ.\nMulti-task Learning: M2TRec incorporates multi-task learning\n(MTL) to boost the performance of next item prediction. MTL has\nproven to be an effective mechanism to reduce the risk of over-\nfitting, learn more generalized shared representations for all the\ntasks, and improve the overall performance on each task by sharing\nthe knowledge acquired from other related tasks [24]. The target\nspace of next item prediction (i.e., all item-IDs) is much larger than\nthe space of other item attributes such as all categories or brands.\nTherefore, the task of next item category or brand prediction should\nbe easier to learn than next item prediction. Moreover, learning\nsuch auxiliary tasks would benefit the task of next item predic-\ntion since it biases the metadata encoding and the Transformer\nencoder layers to learn representations that are close not only to\nnext item, but also to other similar items belonging to the same\ncategory or brand, thus, narrowing down the space of possible next\nitem candidates to a much smaller set of items. To this end, we\ntrain M2TRec to predict next item attributes (e.g., item categories)\nas auxiliary tasks to the task of next item-ID prediction. For each\ntask, including next item-ID prediction, we create a prediction head\ncomposed of Fully Connected Layer (FCN) followed by Softmax\nfunction to generate the probability distribution over all candidates\nfor the corresponding task, i.e.,\nÀÜùë¶ùëò = ùë†ùëúùëìùë°ùëöùëéùë• (FCNùëò(ùë£ùëÜ;ùúÉ(ùëò)))‚àà Rùëëùëò (6)\nwhere ùúÉ(ùëò)is the parameters of the FCN for the ùëòth task, and ùëëùëò\nis the total number of possible outputs of the ùëòth task (e.g., total\nnumber of categories for category predictions).\nThe loss of the ùëòth task prediction head and overall prediction\nof M2TRec are calculated using cross-entropy loss as follows, re-\nspectively:\nLùëò = ‚àí\ndk‚àëÔ∏Å\nùëñ=1\nùë¶ùëòùëñ ¬∑log ÀÜùë¶ùëòùëñ, L=\nN‚àëÔ∏Å\nùëò=1\nLùëò (7)\nwhere ùë¶ùëò is a one-hot encoding including the ground-truth infor-\nmation for the ùëòth task.\n4 EXPERIMENTS\nDatasets: We conducted our experiments on two real-world datasets.\nWe exclude all sessions with only one item. Table 1 shows dataset\nstatistics along with the item metadata we used and the prediction\ntasks for each dataset.\nRecSys ‚Äô22, September 18‚Äì23, 2022, Seattle, WA, USA Shalaby et al.\nTable 2: The performance of M2TRec on next item prediction task on all sessions (All) and sessions with tail items (Sparse)\ncompared to other baseline methods. Tail items indicate items with less than 10 occurrences in a dataset. (Bold indicates the\nbest model, while the second-best model is underlined ).\nDataset Diginetica THD\nMethod HIT@20 Recall@20 MRR@20 HIT@20 Recall@20 MRR@20\nAll Sparse All Sparse All Sparse All Sparse All Sparse All Sparse\nSTAMP 25.45 16.05 41.61 28.89 7.17 4.44 31.91 14.65 38.49 18.90 14.88 6.46\nGRU4Rec 29.09 22.33 45.45 36.14 8.51 6.45 32.27 16.81 37.64 20.28 14.97 7.93\nNARM 30.49 21.78 47.60 36.90 8.13 5.55 28.98 11.31 35.45 15.59 11.63 3.97\nM2TRec 35.47 25.51 53.70 41.01 9.75 6.63 34.78 19.05 41.51 24.86 15.65 7.38\nImprovement 16.33% 14.2% 12.82% 11.1% 14.57% 2.79% 7.78% 13.3% 7.85% 22.6% 4.54% -6.94%\n‚Ä¢Diginetica 1 is an E-commerce dataset that was a part of\nCIKM Cup 2016 challenge. We use the transactional and\nproduct data and use pre-processing similar to [40].\n‚Ä¢THD is an E-commerce dataset obtained from The Home\nDepot, the largest home improvement retailer in the USA.\nThe dataset is composed of Add-to-Cart (ATC) events within\nmillions of online sessions. Similar to SIGIR 2021 data chal-\nlenge dataset[29], training data is created by sampling sev-\neral months of online purchase sessions. Test data is sampled\nfrom a disjoint and adjacent time period. The dataset has\nrich product metadata including 7 attributes: product title,\ncategories (L1, L2, L3, Leaf), brand, manufacturer, color, de-\npartment, and class name.\nBaselines: We select the following 3 state-of-the-art SBRSs to com-\npare them with M2TRec: (1) GRU4Rec [9]: A popular and first-\ngeneration SBRS that utilizes a Gated Recurrent Unit (GRU) [3] to\nmodel long-term dependencies within a session, (2) NARM [13]:\nAn attention-based SBRS that employs a hybrid encoder to reflect a\nuser‚Äôs global and local interests with an attention mechanism, and\n(3) STAMP [14]: An attention/memory-based SBRS that incorpo-\nrates a user‚Äôs short-term and long-term interests via a short-term\nattention and long-term memory modules, respectively.\nEvaluation Metrics: We use HIT@K, Recall@K, and MRR@K [36]\nto evaluate the performance of M2TRec. All the metrics range from\n0 to 1, and higher values are better. We chooseùêæ = 20 since it is a\nstandard value [15].\nImplementations: We used open-source implementations for all\nbaseline methods2. With M2Trec, we encode all the attributes as\ntextual. We use a dedicated embedding layer for each attribute\nfollowed by average pooling of individual tokens‚Äô vectors. The\nembedding dimension is set proportionally to the total number of\ndistinct tokens of the corresponding attribute vocabulary. For the\nTransformer encoder, we used 2 encoder layers with 8 attention\nheads in each layer and point-wise feed-forward networks consist-\ning of two fully-connected layers [2048, 128] with a ReLU activation\n[1] in between. We fine-tuned all the hyperparameters of M2TRec\non a validation dataset sampled randomly from THD data.\nNext item prediction task on all sessions: The performance of\nall the models on all sessions of the two datasets is shown in Table\n2. As we can notice, M2TRec outperforms all the baselines across\nall the evaluation metrics. On Diginetica, the relative performance\nimprovements of HIT@20, Recall@20, and MRR@20 are in the\n1https://competitions.codalab.org/competitions/11161\n2https://github.com/rn5l/session-rec\nrange of 13% ‚àº16%. On THD dataset, the relative performance\nimprovements are in the range of 5% ‚àº8%. As in [27], we compute\nthe relative performance improvement of a metric as the difference\nin the performance of M2TRec and the second runner over the\nperformance of the second runner on that metric reported in per-\ncentage. These improvements indicate the effectiveness of utilizing\nitem metadata and the multi-task learning regime which are unique\nto M2TRec, compared to other baselines which use only item-ID as\nthe main and only input for session-based recommendations.\nNext item prediction task on sparse sessions: Table 2 high-\nlights the performance of all the models on sparse sessions contain-\ning cold-start or tail items in the two datasets. Tail items indicate\nitems with less than 10 occurrences in a dataset. These sessions\nrepresent 34% and 12% of the total sessions in Diginetica and THD\ndatasets respectively. As we can notice, M2TRec relative improve-\nments on sparse sessions are much higher than all the other models,\nespecially on HIT@20 and Recall@20 for both datasets (e.g., 11% ‚àº\n23% boost on both datasets). These results demonstrate the effec-\ntiveness of our proposed item-ID free approach on sparse sessions\nand its robustness in mapping tail and cold-start items within these\nsessions into meaningful representations based on their metadata.\nPredicting next item‚Äôs category: One of the main objectives of\nthis research is to develop a scalable architecture that serves both\nitem and category recommendations in one model using an effi-\ncient MTL regime. We found significant performance gains when\njointly training our model to predict next item and its categories\nat different levels of the catalog taxonomy (see ablation study be-\nlow). We demonstrate the efficacy of training our SBRS to predict\nnext category over deriving it from session items by comparing the\ncategory prediction performance against two heuristics: (1) Per-\nsonalized top-N Frequent:This simple heuristic uses past session\nitems‚Äô categories and recommends the most frequent ones, and (2)\nTop-N Predicted:This simple strategy works by first predicting\ntop-N next items from a metadata-aware single task model called\nMeTRec (see ablation study below), and then uses their categories\nas recommendations such that the category of the top-ranked next\nitem will be ranked first and so on.\nFigure 2 shows the performance of category recommendation\nusing M2TRec against the two heuristics. Performance is measured\nin terms of HIT@20 and reported for L1, L2, L3, and leaf categories\nof THD dataset. As we can notice, the performance of M2TRec is\nsignificantly better than the two other strategies across all tasks.\nFor example, on leaf category prediction, the performance gains are\nin the range of 3%‚àº45%. These results demonstrate that M2TRec\ncan effectively perform next category prediction tasks.\nM2TRec: Metadata-aware Multi-task Transformer for Session-based Recommendations RecSys ‚Äô22, September 18‚Äì23, 2022, Seattle, WA, USA\nFigure 2: M2TRec Performance on Category Recommendation\nTable 3: The performance of M2TRec on all sessions (All) and sessions with tail items (Sparse) compared to variants of M2TRec.\nDataset Diginetica THD\nMethod HIT@20 Recall@20 MRR@20 HIT@20 Recall@20 MRR@20 Model\nSizeAll Sparse All Sparse All Sparse All Sparse All Sparse All Sparse\nTRecùëñùëë 30.24 16.93 46.96 29.45 8.55 4.35 32.90 11.58 39.40 15.22 15.31 4.90 110.3M\nMuTRecùëñùëë 32.69 21.10 50.13 35.18 9.50 5.71 33.73 13.93 40.35 17.92 15.43 5.90 111.1M\nTRecùë°ùëñùë°ùëôùëí 32.17 20.41 49.40 34.64 8.70 5.22 32.29 16.48 38.64 21.19 14.25 6.32 56.1M\nMeTRec 34.78 24.99 52.50 40.43 9.33 6.22 33.54 17.10 40.04 21.97 15.17 6.42 86.5M\nM2TRec 35.47 25.51 53.70 41.01 9.75 6.63 34.78 19.05 41.51 24.86 15.65 7.38 89.5M\nAblation Study of M2TRec: To investigate the contribution of\neach component of M2TRec, we developed the following variants:\n(1) TRecùë°ùëñùë°ùëôùëí : A variant of M2TRec which uses item title as the only\ninput feature without any additional meta-data. It leverages the\nsame architecture in Figure 1, but has one prediction head only to\npredict next item-ID, (2) MeTRec: Metadata-aware variant which\nutilizes all metadata as input features. This variant also has one\nprediction head to predict next item-ID, (3) TRecùëñùëë: A variant of\nM2TRec which uses item-IDs as the only input feature without any\nadditional meta-data attributes. It leverages the same architecture\nin Figure 1 but has only one prediction head to predict next item-\nID, and (4) MuTRecùëñùëë: Multi-task variant of TRecùëñùëë. The model is\ntrained on the same tasks as M2TRec, but uses only item-IDs as the\ninput features (i.e., it does not use other metadata features).\nThe performance of M2TRec and its variants on all and sparse\nsessions is shown in Table 3, where model size indicates the num-\nber of model parameters. M2TRec outperforms all other variants\nsignificantly, especially on sparse sessions where the performance\ngains on Diginetica dataset are in the range of 1% ‚àº9% in terms\nof HIT@20 and 1%‚àº12% in terms of Recall@20. On THD dataset,\nperformance gains are in the range of 2%‚àº7% and 3%‚àº10% in terms\nof HIT@20 and Recall@20 respectively. As we include all metadata\nin MeTRec, the performance on sparse sessions outpaces all other\nvariants. Moreover, the performance on all sessions improves signif-\nicantly and outpaces TRecùëñùëë and its multi-task version (MuTRecùëñùëë)\non Diginetica, while it is on par with MuTRecùëñùëë on THD dataset.\nThis demonstrates the usefulness of metadata-awareness and its\nsufficiency in providing competitive performance to classical item-\nID based SBRSs. As we can notice, M2Trec and MeTRec are about\n19%-22% less in size than the item-ID based variants. Besides, all\nthe metadata-aware variants are more scalable to the increase in\nitem catalog size compared to the item-ID based variants.\n5 CONCLUSION\nThis work provides a scalable and practical solution for leveraging\nmetadata to learn from cold-start items in the recommendation\nprocess. The key is using an item-ID free approach for recommen-\ndations. By using a metadata-based representation of items, the\nM2TRec model learns the representation for items with zero or few\ninteractions. Through experiments on two datasets, we show that\nM2TRec outperforms several state-of-the-art session-based recom-\nmendation models. Multi-task learning contributes to the model‚Äôs\npredictive performance. Importantly, M2TRec‚Äôs core ideas help in\ngenerating fast and accurate recommendations for cold start-items,\nsessions with tail items, and for the task of category prediction.\nREFERENCES\n[1] Abien Fred Agarap. 2018. Deep learning using rectified linear units (relu). arXiv\npreprint arXiv:1803.08375 (2018).\n[2] Xusong Chen, Dong Liu, Chenyi Lei, Rui Li, Zheng-Jun Zha, and Zhiwei Xiong.\n2019. Bert4sessrec: Content-based video relevance prediction with bidirectional\nencoder representations from transformer. In Proceedings of the 27th ACM Inter-\nnational Conference on Multimedia . 2597‚Äì2601.\n[3] Kyunghyun Cho, Bart van Merri√´nboer, Dzmitry Bahdanau, and Yoshua Bengio.\n2014. On the Properties of Neural Machine Translation: Encoder‚ÄìDecoder Ap-\nproaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and\nStructure in Statistical Translation . 103‚Äì111.\n[4] Gabriel de Souza Pereira Moreira, Sara Rabhi, Ronay Ak, and Benedikt Schifferer.\n2021. End-to-End Session-Based Recommendation on GPU. In Fifteenth ACM\nConference on Recommender Systems . 831‚Äì833.\n[5] Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and\nEven Oldridge. 2021. Transformers4Rec: Bridging the Gap between NLP and\nSequential/Session-Based Recommendation. In Fifteenth ACM Conference on\nRecommender Systems . 143‚Äì153.\n[6] Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-\nSeng Chua, and Depeng Jin. 2019. Neural multi-task recommendation from\nmulti-behavior data. In2019 IEEE 35th international conference on data engineering\n(ICDE). IEEE, 1554‚Äì1557.\n[7] Guy Hadash, Oren Sar Shalom, and Rita Osadchy. 2018. Rank and rate: multi-task\nlearning for recommender systems. In Proceedings of the 12th ACM Conference on\nRecommender Systems . 451‚Äì454.\n[8] Bal√°zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with\ntop-k gains for session-based recommendations. In Proceedings of the 27th ACM\ninternational conference on information and knowledge management . 843‚Äì852.\nRecSys ‚Äô22, September 18‚Äì23, 2022, Seattle, WA, USA Shalaby et al.\n[9] Bal√°zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).\n[10] Bal√°zs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos\nTikk. 2016. Parallel recurrent neural network architectures for feature-rich\nsession-based recommendations. In Proceedings of the 10th ACM conference on\nrecommender systems . 241‚Äì248.\n[11] Chao Huang, Jiahui Chen, Lianghao Xia, Yong Xu, Peng Dai, Yanqing Chen,\nLiefeng Bo, Jiashu Zhao, and Jimmy Xiangji Huang. 2021. Graph-enhanced\nmulti-task learning of multi-level transition dynamics for session-based recom-\nmendation. In AAAI Conference on Artificial Intelligence (AAAI) .\n[12] Dietmar Jannach, Malte Ludewig, and Lukas Lerche. 2017. Session-based item\nrecommendation in e-commerce: on short-term intents, reminders, trends and\ndiscounts. User Modeling and User-Adapted Interaction 27 (12 2017). https:\n//doi.org/10.1007/s11257-017-9194-1\n[13] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.\nNeural attentive session-based recommendation. In Proceedings of the 2017 ACM\non Conference on Information and Knowledge Management . 1419‚Äì1428.\n[14] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018. STAMP: short-\nterm attention/memory priority model for session-based recommendation. ,\n1831‚Äì1839 pages.\n[15] Malte Ludewig, Noemi Mauro, Sara Latifi, and Dietmar Jannach. 2019. Per-\nformance comparison of neural and non-neural approaches to session-based\nrecommendation. In Proceedings of the 13th ACM conference on recommender\nsystems. 462‚Äì466.\n[16] Wenjing Meng, Deqing Yang, and Yanghua Xiao. 2020. Incorporating user micro-\nbehaviors and item knowledge into multi-task learning for session-based rec-\nommendation. In Proceedings of the 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval . 1091‚Äì1100.\n[17] Gabriel Moreira, Dietmar Jannach, and Adilson Cunha. 2019. On the Importance\nof News Content Representation in Hybrid Neural Session-based Recommender\nSystems. arXiv preprint arXiv:1907.07629v3 (07 2019).\n[18] Gabriel de Souza P Moreira, Sara Rabhi, Ronay Ak, Md Yasin Kabir, and Even\nOldridge. 2021. Transformers with multi-modal features and post-fusion context\nfor e-commerce session-based recommendation. arXiv preprint arXiv:2107.05124\n(2021).\n[19] Zhiqiang Pan, Fei Cai, Yanxiang Ling, and Maarten de Rijke. 2020. An intent-\nguided collaborative machine for session-based recommendation. In Proceedings\nof the 43rd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 1833‚Äì1836.\n[20] Nan Qiu, BoYu Gao, Feiran Huang, Huawei Tu, and Weiqi Luo. 2021. Incorporating\nGlobal Context into Multi-task Learning for Session-Based Recommendation.\nIn International Conference on Knowledge Science, Engineering and Management .\nSpringer, 627‚Äì638.\n[21] Ruihong Qiu, Zi Huang, Jingjing Li, and Hongzhi Yin. 2020. Exploiting Cross-\nSession Information for Session-Based Recommendation with Graph Neural\nNetworks. ACM Trans. Inf. Syst. 38, 3, Article 22 (may 2020), 23 pages. https:\n//doi.org/10.1145/3382764\n[22] Ramin Raziperchikolaei, Guannan Liang, and Young-joo Chung. 2021. Shared\nNeural Item Representations for Completely Cold Start Problem. In Fifteenth\nACM Conference on Recommender Systems . 422‚Äì431.\n[23] Pengjie Ren, Zhumin Chen, Jing Li, Zhaochun Ren, Jun Ma, and Maarten De Rijke.\n2019. Repeatnet: A repeat aware neural recommendation machine for session-\nbased recommendation. In Proceedings of the AAAI Conference on Artificial Intel-\nligence, Vol. 33. 4806‚Äì4813.\n[24] Sebastian Ruder. 2017. An overview of multi-task learning in deep neural net-\nworks. arXiv preprint arXiv:1706.05098 (2017).\n[25] Martin Saveski and Amin Mantrach. 2014. Item cold-start recommendations:\nlearning local collective embeddings. In Proceedings of the 8th ACM Conference\non Recommender systems . 89‚Äì96.\n[26] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine\ntranslation of rare words with subword units. arXiv preprint arXiv:1508.07909\n(2015).\n[27] Jiayu Song, Jiajie Xu, Rui Zhou, Lu Chen, Jianxin Li, and Chengfei Liu. 2021.\nCBML: A Cluster-based Meta-learning Model for Session-based Recommenda-\ntion. In Proceedings of the 30th ACM International Conference on Information &\nKnowledge Management . 1713‚Äì1722.\n[28] Yang Song, Ali Mamdouh Elkahky, and Xiaodong He. 2016. Multi-rate deep\nlearning for temporal recommendation. In Proceedings of the 39th International\nACM SIGIR conference on Research and Development in Information Retrieval .\n909‚Äì912.\n[29] Jacopo Tagliabue, Ciro Greco, Jean-Francis Roy, Bingqing Yu, Patrick John Chia,\nFederico Bianchi, and Giovanni Cassani. 2021. Sigir 2021 e-commerce workshop\ndata challenge. arXiv preprint arXiv:2104.09423 (2021).\n[30] Jacopo Tagliabue, Bingqing Yu, and Federico Bianchi. 2020. The Embeddings\nThat Came in From the Cold: Improving Vectors for New and Rare Products with\nContent-Based Inference . Association for Computing Machinery, New York, NY,\nUSA, 577‚Äì578. https://doi.org/10.1145/3383313.3411477\n[31] Trinh Xuan Tuan and Tu Minh Phuong. 2017. 3D convolutional networks\nfor session-based recommendation with content features. In Proceedings of the\neleventh ACM conference on recommender systems . 138‚Äì146.\n[32] Bart≈Çomiej Twardowski. 2016. Modelling contextual information in session-aware\nrecommender systems with neural networks. In Proceedings of the 10th ACM\nConference on Recommender Systems . 273‚Äì276.\n[33] Flavian Vasile, Elena Smirnova, and Alexis Conneau. 2016. Meta-Prod2Vec:\nProduct Embeddings Using Side-Information for Recommendation. InProceedings\nof the 10th ACM Conference on Recommender Systems (Boston, Massachusetts,\nUSA) (RecSys ‚Äô16) . Association for Computing Machinery, New York, NY, USA,\n225‚Äì232. https://doi.org/10.1145/2959100.2959160\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998‚Äì6008.\n[35] Michail Vlachos, Celestine D√ºnner, Reinhard Heckel, Vassilios G Vassiliadis,\nThomas Parnell, and Kubilay Atasu. 2018. Addressing interpretability and cold-\nstart in matrix factorization for recommender systems. IEEE Transactions on\nKnowledge and Data Engineering 31, 7 (2018), 1253‚Äì1266.\n[36] Ellen M Voorhees et al. 1999. The trec-8 question answering track report.. In Text\nRetrieval Conference, Vol. 99. 77‚Äì82.\n[37] Meirui Wang, Pengjie Ren, Lei Mei, Zhumin Chen, Jun Ma, and Maarten de Rijke.\n2019. A collaborative session-based recommendation approach with parallel\nmemory modules. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval . 345‚Äì354.\n[38] Shoujin Wang, Liang Hu, and Longbing Cao. 2017. Perceiving the next choice\nwith comprehensive transaction embeddings for online recommendation. InJoint\nEuropean conference on machine learning and knowledge discovery in databases .\nSpringer, 285‚Äì302.\n[39] Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, and Minghui\nQiu. 2020. Global context enhanced graph neural networks for session-based\nrecommendation. In Proceedings of the 43rd International ACM SIGIR Conference\non Research and Development in Information Retrieval . 169‚Äì178.\n[40] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.\nSession-based recommendation with graph neural networks. In Proceedings of\nthe AAAI Conference on Artificial Intelligence , Vol. 33. 346‚Äì353.\n[41] Xin Xia, Hongzhi Yin, Junliang Yu, Yingxia Shao, and Lizhen Cui. 2021. Self-\nSupervised Graph Co-Training for Session-based Recommendation. In 30th ACM\nInternational Conference on Information and Knowledge Management (CIKM 2021) .\nACM.\n[42] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen\nZhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph Contextualized Self-\nAttention Network for Session-based Recommendation.. In IJCAI, Vol. 19. 3940‚Äì\n3946.\n[43] Jiaxuan You, Yichen Wang, Aditya Pal, Pong Eksombatchai, Chuck Rosenburg,\nand Jure Leskovec. 2019. Hierarchical temporal convolutional networks for\ndynamic recommender systems. In The world wide web conference . 2236‚Äì2246.\n[44] Feng Yu, Yanqiao Zhu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2020.\nTAGNN: Target attentive graph neural networks for session-based recommenda-\ntion. In Proceedings of the 43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval . 1921‚Äì1924.\n[45] Yujia Zheng, Siyi Liu, Zekun Li, and Shu Wu. 2021. Cold-start sequential recom-\nmendation via meta learner. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 35. 4706‚Äì4713.",
  "topic": "Metadata",
  "concepts": [
    {
      "name": "Metadata",
      "score": 0.8629653453826904
    },
    {
      "name": "Computer science",
      "score": 0.7954052686691284
    },
    {
      "name": "Session (web analytics)",
      "score": 0.7418420910835266
    },
    {
      "name": "Inference",
      "score": 0.6119154691696167
    },
    {
      "name": "Scalability",
      "score": 0.6021870374679565
    },
    {
      "name": "Task (project management)",
      "score": 0.5900388956069946
    },
    {
      "name": "Embedding",
      "score": 0.5788853168487549
    },
    {
      "name": "Transformer",
      "score": 0.5125250816345215
    },
    {
      "name": "Recommender system",
      "score": 0.4754452407360077
    },
    {
      "name": "Information retrieval",
      "score": 0.40153083205223083
    },
    {
      "name": "Machine learning",
      "score": 0.3769606351852417
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3721740245819092
    },
    {
      "name": "Database",
      "score": 0.22022810578346252
    },
    {
      "name": "World Wide Web",
      "score": 0.13329440355300903
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799939184",
      "name": "Home Depot (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 22
}