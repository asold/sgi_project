{
  "title": "Pre-trained protein language model sheds new light on the prediction of Arabidopsis protein–protein interactions",
  "url": "https://openalex.org/W4389455254",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2554280074",
      "name": "Kewei Zhou",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A5058590547",
      "name": "Chenping Lei",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2518687866",
      "name": "Jingyan Zheng",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2098668577",
      "name": "Yan Huang",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2147581520",
      "name": "Ziding Zhang",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2554280074",
      "name": "Kewei Zhou",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A5058590547",
      "name": "Chenping Lei",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2518687866",
      "name": "Jingyan Zheng",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2098668577",
      "name": "Yan Huang",
      "affiliations": [
        "China Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2147581520",
      "name": "Ziding Zhang",
      "affiliations": [
        "China Agricultural University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2043764521",
    "https://openalex.org/W2141978471",
    "https://openalex.org/W2724371674",
    "https://openalex.org/W2073547126",
    "https://openalex.org/W2167869241",
    "https://openalex.org/W2099278991",
    "https://openalex.org/W3182560787",
    "https://openalex.org/W2558534012",
    "https://openalex.org/W2534390616",
    "https://openalex.org/W3107527779",
    "https://openalex.org/W2557595285",
    "https://openalex.org/W2949841808",
    "https://openalex.org/W2136632285",
    "https://openalex.org/W2992752586",
    "https://openalex.org/W3080498684",
    "https://openalex.org/W3199468887",
    "https://openalex.org/W1998701725",
    "https://openalex.org/W2794956775",
    "https://openalex.org/W2957180077",
    "https://openalex.org/W2946297791",
    "https://openalex.org/W4291221220",
    "https://openalex.org/W2998220730",
    "https://openalex.org/W3090851383",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4220657176",
    "https://openalex.org/W4295027609",
    "https://openalex.org/W4310154745",
    "https://openalex.org/W4295929073",
    "https://openalex.org/W4317867801",
    "https://openalex.org/W2008840001",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W4283716699",
    "https://openalex.org/W2957436444",
    "https://openalex.org/W4205177317",
    "https://openalex.org/W2417561484",
    "https://openalex.org/W4324311365",
    "https://openalex.org/W3083022746",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W2338544304"
  ],
  "abstract": null,
  "full_text": "Zhou et al. Plant Methods          (2023) 19:141  \nhttps://doi.org/10.1186/s13007-023-01119-6\nRESEARCH Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nPlant Methods\nPre-trained protein language model sheds \nnew light on the prediction of Arabidopsis \nprotein–protein interactions\nKewei Zhou1, Chenping Lei1, Jingyan Zheng1, Yan Huang1 and Ziding Zhang1* \nAbstract \nBackground Protein–protein interactions (PPIs) are heavily involved in many biological processes. Consequently, \nthe identification of PPIs in the model plant Arabidopsis is of great significance to deeply understand plant growth \nand development, and then to promote the basic research of crop improvement. Although many experimental Arabi-\ndopsis PPIs have been determined currently, the known interactomic data of Arabidopsis is far from complete. In this \ncontext, developing effective machine learning models from existing PPI data to predict unknown Arabidopsis PPIs \nconveniently and rapidly is still urgently needed.\nResults We used a large-scale pre-trained protein language model (pLM) called ESM-1b to convert protein \nsequences into high-dimensional vectors and then used them as the input of multilayer perceptron (MLP). To avoid \nthe performance overestimation frequently occurring in PPI prediction, we employed stringent datasets to train \nand evaluate the predictive model. The results showed that the combination of ESM-1b and MLP (i.e., ESMAraPPI) \nachieved more accurate performance than the predictive models inferred from other pLMs or baseline sequence \nencoding schemes. In particular, the proposed ESMAraPPI yielded an AUPR value of 0.810 when tested on an inde-\npendent test set where both proteins in each protein pair are unseen in the training dataset, suggesting its strong \ngeneralization and extrapolating ability. Moreover, the proposed ESMAraPPI model performed better than several \nstate-of-the-art generic or plant-specific PPI predictors.\nConclusion Protein sequence embeddings from the pre-trained model ESM-1b contain rich protein semantic \ninformation. By combining with the MLP algorithm, ESM-1b revealed excellent performance in predicting Arabidopsis \nPPIs. We anticipate that the proposed predictive model (ESMAraPPI) can serve as a very competitive tool to accelerate \nthe identification of Arabidopsis interactome.\nKeywords Arabidopsis, Protein–protein interactions, Machine learning, Pre-trained language model, Natural \nlanguage processing\nBackground\nProtein–protein interactions (PPIs) are heavily involved \nin cellular biological processes, including signal trans -\nduction, transcriptional activation, and regulations of \nexpression and metabolism [1]. Thus, it is critical to iden-\ntify whether two proteins interact or not to help under -\nstand protein functions. Traditional experiments [e.g., \nisothermal titration calorimetry [2], pull-down assay [3], \nand surface plasmon resonance [4]] are low-throughput \n*Correspondence:\nZiding Zhang\nzidingzhang@cau.edu.cn\n1 State Key Laboratory of Animal Biotech Breeding, College of Biological \nSciences, China Agricultural University, Beijing 100193, China\nPage 2 of 10Zhou et al. Plant Methods          (2023) 19:141 \nand time-consuming. With the development of high-\nthroughput techniques, such as in vitro yeast two-hybrid \nscreening [5] and affinity purification coupled with mass \nspectrometry [6], the identification of PPI data has been \nsignificantly accelerated, and the bioinformatics applica -\ntions of PPI data have also been widely explored [7]. On \nthe one hand, the experimental PPI data are often com -\npiled as PPI interaction networks [8–10], and function -\nally unknown proteins in the networks can be annotated \nthrough network clustering and analysis [11–13]. On the \nother hand, the experimental PPI data can also be used \nto train PPI prediction models [14–16]. In this regard, \nmachine learning is an increasingly popular computa -\ntional method to learn data features deposited in known \nPPIs and build predictive models to predict unknown \ninteractions.\nSince protein interactions are mainly determined by \ntheir primary sequences, many efforts have focused on \ndeveloping sequence-based PPI predictors. To build a \nmachine learning model for PPI prediction, the key step \nis conducting feature engineering, which converts pro -\ntein sequences into fixed-dimensional vectors. The fre -\nquently used sequence encoding schemes include amino \nacid composition (AAC), dipeptide composition (DPC), \nconjoint triad (CT), and composition of k-spaced amino \nacid pairs (CKSAAP). These descriptive representations \nare often combined with traditional machine learning \nmethods, such as support vector machine (SVM) [17] \nand random forest (RF) [18], to develop effective PPI \npredictors. Regarding deep learning methods, primitive \ninformation without feature engineering can be used \nto extract more abstract representations. For instance, \none-hot encoding and position-specific scoring matrix \n(PSSM) representation have been integrated with the \nframework of convolutional neural network (CNN) to \nachieve higher performance [19, 20].\nBy analogy to natural language, a protein sequence \ncan be deemed a sentence, in which residue segments \nare regarded as words. Based on this hypothesis, natu -\nral language processing (NLP) methods have been used \nin protein representation. For instance, the typical word/\nsentence embedding techniques in NLP (e.g., word2vec \nand doc2vec) have been applied to protein sequence rep -\nresentations [21–23]. Although the word2vec/doc2vec \nmodels are either too shallow or trained with the corpus \ncontaining a limited number of existing proteins, they \nhave revealed very promising results in many protein bio-\ninformatics tasks. As one of the self-supervised language \nmodels, Transformer, released in 2017 by Google [24], \nsolved the problem of memory capacity and process -\ning speed. A typical Transformer is comprised of Atten -\ntion modules focusing on vital information from global \nto local, and it often showed significant performance \nimprovement when trained on large datasets [25]. Con -\nsidering the advantages of Transformer in NLP , Rives \net  al. used this technique to generate a protein lan -\nguage model (pLM) for the purpose of protein sequence \nembeddings [26]. ESM is a deep Transformer language \nmodel trained on UniRef50, which can learn multi-\nscale representations, including biochemical properties, \nremote homology, and alignment within a protein family. \nResearchers have applied similar representation in differ -\nent prediction tasks [27–31].\nLarge-scale identification of PPIs in the model plant \nArabidopsis is of significance to decipher plant gene reg -\nulatory relationships, deeply understand plants’ growth \nand development, and promote the basic research of \ncrop improvement and breeding. Although many known \nArabidopsis PPIs have been stored in public databases, \nthe Arabidopsis interactome remains incomplete. Thus, \ndeveloping effective machine learning methods trained \non existing PPI data to promptly predict unknown PPIs \nwill accelerate the determination of Arabidopsis interac -\ntome data, reduce the experimental cost and provide new \nhints for plant functional genomics. To our best knowl -\nedge, however, the pLM-based embeddings have not \nbeen employed for predicting Arabidopsis PPIs. Here, \nwe further explored the application of ESM representa -\ntion in predicting Arabidopsis PPIs. Through a series of \ncomputational experiments, we observed that the com -\nbination of ESM-1b representation (one representative \nESM model) with multilayer perceptron (MLP), termed \nESMAraPPI, yielded more powerful performance than \nthe predictive models inferred from other pLMs or con -\nventional sequence features. In the meantime, the pro -\nposed model also revealed better performance than \nseveral generic or plant-specific PPI predictors.\nResults and discussion\nThe computational framework and benchmarking datasets \nof ESMAraPPI\nThe flowchart of the proposed prediction method is illus-\ntrated in Fig.  1. To train and assess the performance of \ndifferent PPI prediction models, we collected high-qual -\nity experimental Arabidopsis PPIs as positive samples. \nAdditionally, we compiled negative training data by ran -\ndomly selecting Arabidopsis protein pairs, and the ratio \nof positive to negative samples was set as 1:10. To train \nand evaluate model performance, we followed Park and \nMarcotte’s advice [32] to construct one training dataset \n(i.e., C1) and two independent test sets (i.e., C2 and C3) \n(Fig. 1). Specifically, only one protein in each pair from C2 \nis allowed to be appeared in C1, whereas both proteins in \neach pair from C3 are unseen in C1. The representation \nof a protein was extracted from ESM-1b directly, which \nresulted in a feature vector of 1280 dimensionality. Since \nPage 3 of 10\nZhou et al. Plant Methods          (2023) 19:141 \n \nthe PPI prediction is a pair-input problem, Hadamard \nproduct was applied before inputting representations of \nprotein pairs to a 4-layer MLP for model training. More \ndetails about the dataset preparation, pLM feature vec -\ntor construction, and machine learning algorithm imple -\nmentation are available in the Methods section.\nESM‑1b coupled with MLP performed best in predicting \nArabidopsis PPIs\nNine different pLMs from ESM embed each protein \nsequence to a vector of 1280 dimensionality. We com -\nbined these pLMs with three machine learning algo -\nrithms (MLP , RF and SVM) to seek the best combination. \nThe 4-layer MLP computational framework, which \ncontains 1024, 512, 128, and 16 nodes, was optimally \nselected. Concerning RF and SVM, the correspond -\ning parameters were optimized through grid search. \nConsidering that positives and negative samples are \nhighly imbalanced in this work, we mainly quantified \nthe performance by plotting the precision–recall (PR) \ncurve and calculating the corresponding area under the \nPR curve (AUPR). As shown in Fig.  2, MLP-based mod -\nels yielded the highest AUPR values, followed by RF- \nand SVM-based models. In particular, the combination \nof MLP and ESM-1b (i.e., esm1b_t33_650M_UR50S) \nachieved the best performance (AUPR = 0.834 on C2 and \n0.810 on C3). To supplement the AUPR-based assess -\nment, we also plotted the receiver operating character -\nistic (ROC) curve and calculated the corresponding area \nunder the ROC curve (AUROC) for each combination. \nAgain, the MLP and ESM-1b combination resulted in the \nlargest AUROC value (Additional file 1: Fig. S1).\nWe further compared ESM-1b with three pLMs, Prot -\nTrans, UniRep, and TAPE. Note that these three pLMs \nwere trained using different training strategies from ESM-\n1b. Trained on data from UniRef and BFD covering up to \n393 billion amino acids, an auto-encoder model (ProtT5-\nXL-U50) from ProtTrans for the first time outperformed \nFig. 1 The schematic diagram of ESMAraPPI. Arabidopsis PPIs from the IntAct database with MIscore ≥ 0.45 were collected as positive samples. \nWe also compiled 10 times negative samples to construct an original dataset. Then, we divided the original dataset into three datasets (i.e., C1, \nC2 and C3). C1 was the training dataset, while C2 and C3 were two independent test datasets. The representations of protein pair were extracted \nfrom ESM-1b, and Hadamard product was applied before inputting to a 4-layer MLP . The final output was an interaction score between 0 and 1 (a \nprediction score ≥ 0.5 corresponded to a positive interaction)\nPage 4 of 10Zhou et al. Plant Methods          (2023) 19:141 \nexisting methods without the need of multiple sequence \nalignments (MSAs) or evolutionary information in sec -\nondary structure prediction [33]. UniRep was based \non multiplicative LSTM (mLSTM) and was trained on \nUniRef50 [34]. It was found that the amino-acid embed -\ndings learned by UniRep contained physiochemically \nmeaningful clusters. TAPE was a small Transformer \ntrained on UniRef50 [35], which embedded each pro -\ntein sequence to a vector of 768 and achieved compara -\nble performance with UniRep on protein fluorescence \nand stability prediction. Of the three machine learning \nalgorithms under investigation, the MLP algorithm again \nachieved the best performance in combination with these \nthree pLMs judged by AUPRC or AUROC values. Inter -\nestingly, ESM-1b also outperformed three other pLMs \nin the computational framework of MLP (Fig.  3). We \nfurther compared ESM-1b with two baseline sequence \nencoding schemes (i.e., AAC and DPC). AAC stands for \nthe compositions of each amino acid in the whole protein \nsequence, which transforms a protein into a vector of 20 \ndimensionality. DPC represents the compositions of two \ncontinuous amino acids in the whole protein sequence, \nwhich was used to convert a protein into a vector of 400 \ndimensionality. As shown in Additional file  1: Table  S1, \nthe combination of AAC and SVM seems to be optimal \n(AUPR = 0.519 on C2 and 0.481 on C3; AUROC = 0.852 \non C2 and 0.824 on C3), while the combination of DPC \nand RF achieves the best performance (AUPR = 0.646 on \nC2 and 0.564 on C3; AUROC = 0.884 on C2 and 0.845 on \nC3). Comparatively, the optimal performance of these \ntwo traditional encoding schemes is much inferior to that \nof ESM-1b.\nComparison to existing generic PPI prediction methods\nWe compared our method with four generic PPI predic -\ntion methods, including three sequence-based methods \n[i.e., D-SCRIPT [16], RAPPPID [36], and PIPR [37]] \nand one structure-based method [i.e., TAGPPI [38]]. \nD-SCRIPT first applied a pre-trained model to gener -\nate structurally informative feature representations of \nproteins, and then estimates an interaction probabil -\nity of protein pairs based on these features. RAPPPID \nis a deep learning-based PPI predictor implemented \nthrough a twin averaged weight-dropped LSTM net -\nwork employing multiple regularization methods in \nthe training step to learn generalized weights. When \nFig. 2 AUPR values of combinations between nine pLMs from ESM and three machine learning algorithms. Of the different ESM models, ESM-1v \nwas fine-tuned for predicting variant effects and contained five models with different random seeds. ESM-1b differs from ESM-1 mainly in higher \nlearning rate, dropout after word embedding, learned positional embeddings, final layer norm before the output, and tied input/output word \nembedding. A The results from the independent dataset C2 where only one protein in each pair appeared in the training dataset (i.e., C1), while B \ncorresponds to the results from the independent dataset C3 where no protein in each pair appeared in the training dataset (i.e., C1)\nPage 5 of 10\nZhou et al. Plant Methods          (2023) 19:141 \n \ntested on stringent PPI datasets containing proteins \nunseen in the training dataset, it reveals excellent per -\nformance. PIPR is a sequence-based PPI predictor \ncombining pre-trained amino acid embeddings with \na Siamese recurrent convolutional neural network \n(RCNN) architecture. TAGPPI is an end-to-end com -\nputational framework for PPI prediction, in which \nmulti-dimensional features by employing 1D convolu -\ntion operation on protein sequences and graph learning \nmethod on contact maps constructed from AlphaFold2 \nare considered.\nWe downloaded the source codes of PIPR, \nD-SCRIPT, RAPPPID, and TAGPPI and retrained the \ncorresponding predictive models using the C1 data -\nset. Moreover, we tested their performance on our \ntwo independent datasets (C2 and C3). As shown in \nFig.  4, the proposed ESMAraPPI considerably outper -\nformed the four existing PPI predictors in terms of \nFig. 3 PR and ROC curves of the predictive models from four pLMs in combination with MLP . A plots the PR curves on the independent test set C2, \nwhile B plots the PR curves on the independent test set C3. Parameters in the legends of A and B denote the corresponding AUPR values. C plots \nthe ROC curves on the independent test set C2, while D plots the ROC curves on the independent test set C3. Parameters in the legends of C and D \ndenote the corresponding AUROC values\nPage 6 of 10Zhou et al. Plant Methods          (2023) 19:141 \nAUPRC or AUROC values. In particular, the proposed \nESMAraPPI reveals robust performance on the C3 \ntest set. For the three existing sequence-based meth -\nods, the performance of PIPR is ranked as the best, \nfollowed by RAPPPID and D-SCRIPT. When tested \non the C3 test set, the performance ranking remains \nthe same, but the performance of PIPR and RAPPPID \ndropped rapidly. Considering the predicted protein \nstructural information used in PPI prediction, TAGPPI \nconsiderably surpassed these three pure sequence-\nbased models on both C2 and C3, although it also \ndropped sharply on C3.\nIn addition to the better performance of ESM-1b \nembedding with simple MLP , its computational effi -\nciency is also high. In either the model training or \nprediction steps, ESMAraPPI showed more rapid com -\nputational speed (Table 1 ).\nFig. 4 PR and ROC curves of ESMAraPPI and four existing generic PPI predictors in predicting Arabidopsis PPIs. A plots the PR curves \non the independent test set C2, while B plots the PR curves on the independent test set C3. Parameters in the legends of A and B denote \nthe corresponding AUPR values. C plots the ROC curves on the independent test set C2, while D plots the ROC curves on the independent test set \nC3. Parameters in the legends of C and D denote the corresponding AUROC values\nPage 7 of 10\nZhou et al. Plant Methods          (2023) 19:141 \n \nComparison to existing Arabidopsis PPI prediction \nmethods\nWe compared the proposed ESMAraPPI with two exist -\ning Arabidopsis PPI prediction methods [i.e., AraPPINet \n[39] and DeepAraPPI [40]]. AraPPINet was inferred \nfrom three-dimensional structures and functional evi -\ndence, encompassing 316,747 high-confidence interac -\ntions among 12,574 proteins. It exhibited high predictive \npower for discovering protein interactions at a 50% true \npositive rate. To allow for a fair comparison between \nour model and AraPPINet, we submitted our test data -\nsets (C2 and C3) directly to their web server, and the \ndefault threshold (0.5) reported by AraPPINet was also \nused to distinguish interactions and non-interactions. \nSince AraPPINet did not release the predictions at dif -\nferent threshold values, we were not able to compare \nESMAraPPI and AraPPINet through AUPRC or AUROC \nvalues. Thus, the routine measurements, such as Accu -\nracy, Matthews correlation coefficient (MCC), Recall [i.e., \ntrue positive rate (TPR)], Specificity [i.e., 1-false positive \nrate (FPR)] and Precision, were employed for perfor -\nmance comparison (Table  2). Judged by the MCC value, \nwhich is a more comprehensive measurement than the \nother parameters, ESMAraPPI outperforms AraPPINet \nin both C2 and C3 datasets (Table 2).\nWe finally compared ESMAraPPI with DeepAraPPI, \nwhich was recently developed in our team. As an inte -\ngrative Arabidopsis PPI prediction method, Deep -\nAraPPI comprises three individual predictors, (i) a \nword2vec encoding-based Siamese RCNN model, (ii) \na Domain2vec encoding-based MLP model, and (iii) \na GO2vec encoding-based MLP model [40]. The final \nDeepAraPPI model combined the prediction results \nof the three individual predictors through a Logistic \nregression model. We also tested DeepAraPPI on our \ndatasets. As shown in Table  3, ESMAraPPI outper -\nformed the individual predictors of DeepAraPPI (i.e., \nRCNN, Domain2vec, and Go2vec) in both test sets (C2 \nand C3). With respect to C3, ESMAraPPI surpassed the \nfinal DeepAraPPI model, which means our new method \nwas more competitive and will be more reliable in prac -\ntical applications.\nCase study\nTo explore the real application of ESMAraPPI, we pro -\nvided a case study related to the interaction prediction \nof two proteins (BIN2 and SOS2) involved in the salt \noverly sensitive (SOS) pathway. In 2020, Li et al. showed \nthat BIN2 functions as a negative regulator of primary \nroot growth under salt stress by phosphorylating and \ninhibiting SOS2 [41]. It should be emphasized that the \ninteraction between BIN2 and SOS2 was consistently \ndetermined by the yeast two-hybrid assay, the split-\nLUC assay and the BiFC assay in Li et al. ’s work, which \nhas not been included in any public database. Using the \nESMAraPPI model, BIN2 and SOS2 were predicted to \ninteract (prediction score = 0.592), indicating the pro -\nposed method has practical application in predicting \nArabidopsis PPIs.\nTable 1 Computational time required in different  methodsa\na All the training and prediction procedures were processed on a high-\nperformance computer with 20 cores CPU, 256G RAM, and Tesla V100 GPU\nb  Total predicting time means the computational time required for processing \nthe C3 test dataset\nESM‑1b + MLP TAGPPI RAPPPID PIPR D‑SCRIPT\nTraining \nepoch\n40 10 20 20 10\nTotal training \ntime\n56 s 9.29 h 1.12 h 700 s 7.22 h\nTotal predict-\ning  timeb\n0.1 s 583 s 18 s 5 s 82 s\nTable 2 Comparison of ESMAraPPI and AraPPINet on the C2 and C3 test sets\nMethods C2 C3\nAccuracy Specificity MCC Recall Precision Accuracy Specificity MCC Recall Precision\nESMAraPPI 0.957 0.994 0.708 0.589 0.901 0.954 0.994 0.688 0.557 0.902\nAraPPINet 0.939 0.999 0.551 0.337 0.966 0.937 0.999 0.534 0.318 0.966\nTable 3 AUPR and AUROC values of DeepAraPPI and ESMAraPPI \non the C2 and C3 test  setsa\na  Figure in bold font indicates the corresponding model achieved the maximal \nAUPR and AUROC value\nMethod AUPR AUROC\nC2 C3 C2 C3\nDeepAraPPI_RCNN 0.541 0.331 0.852 0.778\nDeepAraPPI_Domain2vec 0.706 0.639 0.884 0.845\nDeepAraPPI_Go2vec 0.771 0.709 0.942 0.917\nDeepAraPPI 0.871 0.785 0.978 0.944\nESMAraPPI 0.824 0.810 0.966 0.960\nPage 8 of 10Zhou et al. Plant Methods          (2023) 19:141 \nConclusion\nIn this work, we found that sequence representations \ndirectly generated by large-scale pre-trained pLMs \nwithout any further feature engineering can be success -\nfully used to develop machine learning-based Arabidop -\nsis PPI predictors. We have shown that the proposed \nESMAraPPI (i.e., ESM-1b + MLP) model yielded a highly \naccurate performance in predicting Arabidopsis PPIs. \nOn the one hand, it achieved dramatic performance \nimprovement in comparison to the models inferred from \nbaseline sequence encoding schemes. On the other hand, \nit also revealed better performance than several state-of-\nthe-art generic or plant-specific PPI predictors. The suc -\ncess of ESMAraPPI should be ascribed to the fact that the \nlarge-scale pre-trained pLMs can capture rich semantic \ninformation regarding protein sequence-structure-evolu-\ntion relationships. To facilitate the research community, \nwe have made all our codes and datasets freely available \nat https:// github. com/ keiwo/ ESMAr aPPI. We believe \nthat the application of pLMs in protein sequence repre -\nsentation is providing a very promising way to deal with \nfeature engineering in PPI prediction.\nMethods\nData collection and preprocessing\nExperimental Arabidopsis PPIs were first downloaded \nfrom IntAct (https:// www. ebi. ac. uk/ intact/ home), and \nonly PPIs with the type of direct interaction or physical \nassociation were further retained. Moreover, PPIs with \nMIscore < 0.45 were removed. Finally, we obtained 7729 \nPPIs, which are regarded as positive samples in this work. \nTo construct negative samples, we first removed pro -\nteins in positive samples from the complete Arabidop -\nsis protein list, and the remaining Arabidopsis proteins \nsharing ≥ 40% sequence identity with proteins in posi -\ntive samples were further filtered out. Then, we removed \nredundant proteins by applying a sequence identity cut-\noff of 40%, and 8382 proteins were retained. After that, \nwe obtained a protein list by mixing these 8382 proteins \nand proteins in positive samples, which were used to \nconstruct negative samples through random pairing. By \ncontrolling the ratio of positive and negative samples as \n1:10, 77,290 random protein pairs that were not experi -\nmentally identified as PPIs were selected as negative \nsamples. Finally, an original dataset containing 7729 posi-\ntive samples (i.e., PPIs) and 77,290 negative samples (i.e., \nnon-PPIs) was compiled in this work. To conduct model \ntraining and evaluation, we followed Park and Marcotte’s \nadvice to divide the original dataset into three datasets \n(i.e., C1, C2, and C3). C1 was the train dataset, while C2 \nand C3 were two independent test datasets. More details \nabout the sizes of the three datasets are listed in Table 4.\nProtein representation\nThe ESM models are available at https:// github. com/ \nfaceb ookre search/ esm/ tree/ v1.0.2. There are 13 mod -\nels in the ESM version we used. Of them, esm_msa1_\nt12_100M_UR50S and esm_msa1b_t12_100M_UR50S \nrequire extra MSAs as input, which were not further con-\nsidered. In the remaining 11 models, nine models encode \neach protein sequence into a vector of 1280 dimension -\nality, which were chosen for further investigation. We \ndownloaded these nine pre-trained ESM models and fol -\nlowed the ESM authors’ instructions to run them locally. \nAfter extracting the final layer’s hidden parameters, the \nmatrix was averaged on the first dimension to generate \n1280 features for each sequence. The ProtTrans model is \navailable at https:// github. com/ agema gician/ ProtT rans. \nWe downloaded prot_t5_xl_uniref50 (ProtT5), which \nconverted each protein sequence to a vector of 1024 \ndimensionality. The TAPE and UniRep models are avail -\nable at https:// github. com/ songl  ab- cal/ tape. Similarly, \nwe downloaded TAPE and UniRep that embed each pro -\ntein sequence to a vector of 1900 and 768 dimensionality, \nrespectively.\nMachine learning algorithms\nMultilayer perceptron (MLP)\nThrough the PyTorch machine learning framework, we \nimplemented a 4-layer MLP , which contains 1024, 512, \n128, and 16 nodes. To avoid the order bias from protein \npairs, the Hadamard product of two protein features, \nrather than their concatenation, was used as model input. \nThe sigmoid function was applied to the final output \nto yield a prediction score between 0 and 1 (a predic -\ntion score ≥ 0.5 corresponded to a positive interaction). \nThen, the binary cross entropy (BCE) loss function was \nimplemented.\nSupport vector machine (SVM)\nWe implemented SVM based on the sklearn package \nin Python, and the parameters were optimized by grid \nsearch. The kernel function was set as ’rbf’ , the regu -\nlarization parameter was set to 1, and the kernel coeffi -\ncient was set as ’scale’ . The other parameters were set as \ndefault. The model input was Hadamard product of two \nTable 4 Statistics of the C1, C2 and C3 datasets\nDataset #positive \nsamples\n#proteins \ninvolved \nin positive \nsamples\n#negative \nsamples\n#proteins \ninvolved in \nnegative samples\nC1 3519 1415 35,190 7068\nC2 3404 1781 34,040 10,586\nC3 806 551 8060 3534\nPage 9 of 10\nZhou et al. Plant Methods          (2023) 19:141 \n \nprotein features. A prediction score ≥ 0.5 was thought to \nbe interaction.\nRandom forest (RF)\nWe implemented RF based on the sklearn package in \nPython. The parameters were optimized by grid search. \nThe n_estimators was set as ’100’ , and max_depth was set \nas ’None’ . The model input was Hadamard product of two \nprotein features. A prediction score ≥ 0.5 was thought to \nbe interaction.\nPerformance evaluations\nAccuracy, Specificity, Precision, Recall, and MCC were \nused to evaluate the prediction performance. These \nparameters are defined as follows:\nwhere TP , TN, FP , and FN represent the numbers of true \npositives, true negatives, false positives, and false nega -\ntives, respectively. To provide a comprehensive perfor -\nmance assessment for each predictive model, the PR \ncurve was plotted, and the AUPRC value was also cal -\nculated to quantify the performance. In the meantime, \nthe ROC curve, which plots the TPR value against the \nFPR value at different thresholds, and the correspond -\ning AUROC value were also employed for performance \nassessment.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13007- 023- 01119-6.\nAdditional file 1.  \nAcknowledgements\nWe are grateful for the stimulating discussion with Dr. Stefan Wuchty at \nMiami University regarding the prediction and network analysis of plant \ninteractomes.\nAccuracy= TP + TN\nTP + FP + TN + FN\nSpeciﬁcity= 1 − FPR = TN\nTN + FP\nPrecision= TP\nTP + FP\nRecall= TPR = TP\nTP + FN\nMCC = TP× TN − FP× FN√(TP+ FP) × (TP+ FN) × (TN + FP) × (TN + FN)\nAuthor contributions\nKZ developed the prediction model and drafted the manuscript. CL partici-\npated in the model construction. JZ and YH provided suggestions on data col-\nlection, model construction, and performance assessment. ZZ supervised the \nstudy and significantly revised the manuscript. All authors read and approved \nthe final manuscript.\nFunding\nThis work was supported by the National Natural Science Foundation of China \n(31271414, 31471249, and 31970645).\nAvailability of data and materials\nThe codes and datasets supporting the conclusions of this article are available \nin the GitHub repository, https:// github. com/ keiwo/ ESMAr aPPI.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 29 August 2023   Accepted: 28 November 2023\nReferences\n 1. Orchard S, Ammari M, Aranda B, Breuza L, Briganti L, Broackes-\nCarter F, et al. The MIntAct project-IntAct as a common curation \nplatform for 11 molecular interaction databases. Nucleic Acids Res. \n2014;42(D1):D358–63.\n 2. Velazquez-Campoy A, Freire E. ITC in the post-genomic era… ? Priceless. \nBiophys Chem. 2005;115(23):115–24.\n 3. Louche A, Salcedo SP , Bigot S. Protein-protein interactions: pull-down \nassays. Methods Mol Biol. 2017;1615:247–55.\n 4. Jung SO, Ro HS, Kho BH, Shin YB, Kim MG, Chung BH. Surface plasmon \nresonance imaging-based protein arrays for high-throughput screening \nof protein-protein interaction inhibitors. Proteomics. 2005;5(17):4427–31.\n 5. Ito T, Chiba T, Ozawa R, Yoshida M, Hattori M, Sakaki Y. A comprehensive \ntwo-hybrid analysis to explore the yeast protein interactome. Proc Natl \nAcad Sci U S A. 2001;98(8):4569–74.\n 6. Kocher T, Superti-Furga G. Mass spectrometry-based functional prot-\neomics: from molecular machines to protein networks. Nat Methods. \n2007;4(10):807–15.\n 7. Nakajima N, Akutsu T, Nakato R. Databases for Protein-Protein Interac-\ntions. Methods Mol Biol. 2021;2361:229–48.\n 8. Li TB, Wernersson R, Hansen RB, Horn H, Mercer J, Slodkowicz G, et al. A \nscored human protein-protein interaction network to catalyze genomic \ninterpretation. Nat Methods. 2017;14(1):61–4.\n 9. Alanis-Lobato G, Andrade-Navarro MA, Schaefer MH. HIPPIE v2.0: \nenhancing meaningfulness and reliability of protein-protein interaction \nnetworks. Nucleic Acids Res. 2017;45(1):D408–14.\n 10. Szklarczyk D, Gable AL, Nastou KC, Lyon D, Kirsch R, Pyysalo S, et al. The \nSTRING database in 2021: customizable protein-protein networks, and \nfunctional characterization of user-uploaded gene/measurement sets. \nNucleic Acids Res. 2021;49(D1):D605–12.\n 11. Mirdita M, von den Driesch L, Galiez C, Martin MJ, Soding J, Steinegger M. \nUniclust databases of clustered and deeply annotated protein sequences \nand alignments. Nucleic Acids Res. 2017;45(D1):D170–6.\n 12. Huerta-Cepas J, Forslund K, Coelho LP , Szklarczyk D, Jensen LJ, von Mer-\ning C, et al. Fast genome-wide functional annotation through orthology \nsssignment by eggNOG-mapper. Mol Biol Evol. 2017;34(8):2115–22.\nPage 10 of 10Zhou et al. Plant Methods          (2023) 19:141 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 13. Galperin MY, Makarova KS, Wolf YI, Koonin EV. Expanded microbial \ngenome coverage and improved protein family annotation in the COG \ndatabase. Nucleic Acids Res. 2015;43(D1):D261–9.\n 14. Gainza P , Sverrisson F, Monti F, Rodola E, Boscaini D, Bronstein MM, et al. \nDeciphering interaction fingerprints from protein molecular surfaces \nusing geometric deep learning. Nat Methods. 2020;17(2):184–92.\n 15. Li YW, Golding GB, Ilie L. DELPHI: accurate deep ensemble model for \nprotein interaction sites prediction. Bioinformatics. 2021;37(7):896–904.\n 16. Sledzieski S, Singh R, Cowen L, Berger B. D-SCRIPT translates genome to \nphenome with sequence-based, structure-aware, genome-scale predic-\ntions of protein-protein interactions. Cell Syst. 2021;12(10):969–82.\n 17. Pugalenthi G, Kandaswamy KK, Suganthan PN, Archunan G, Sowd-\nhamini R. Identification of functionally diverse lipocalin proteins from \nsequence information using support vector machine. Amino Acids. \n2010;39(3):777–83.\n 18. Manavalan B, Shin TH, Kim MO, Lee G. AIPpred: sequence-based predic-\ntion of anti-inflammatory peptides using random forest. Front Pharmacol. \n2018;9(1):276.\n 19. Wang L, Wang HF, Liu SR, Yan X, Song KJ. Predicting protein-protein inter-\nactions from matrix-based protein sequence using convolution neural \nnetwork and feature-selective rotation forest. Sci Rep. 2019;9(1):9848.\n 20. Le NQK, Huynh TT, Yapp EKY, Yeh HY. Identification of clathrin proteins by \nincorporating hyperparameter optimization in deep learning and PSSM \nprofiles. Comput Meth Prog Bio. 2019;177:81–8.\n 21. Koca MB, Nourani E, Abbasoglu F, Karadeniz I, Sevilgen FE. Graph \nconvolutional network based virus-human protein-protein interaction \nprediction for novel viruses. Comput Biol Chem. 2022;101(1):10775.\n 22. Yang XD, Yang SP , Li QMG, Wuchty S, Zhang ZD. Prediction of human-\nvirus protein-protein interactions through a sequence embedding-based \nmachine learning method. Comput Struct Biotechnol J. 2020;18:153–61.\n 23. Cox S, Dong XL, Rai RH, Christopherson L, Zheng WF, Tropsha A, et al. A \nsemantic similarity based methodology for predicting protein-protein \ninteractions: Evaluation with P53-interacting kinases. J Biomed Inform. \n2020;111: 103579.\n 24. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al., \neditors. Attention is all you need. Proceedings of the 31st International \nConference on Neural Information Processing Systems; 2017 December; \nLong Beach, California, USA.\n 25. Lecun Y, Bottou L, Bengio Y, Haffner P . Gradient-based learning applied to \ndocument recognition. P Ieee. 1998;86(11):2278–324.\n 26. Rives A, Meier J, Sercu T, Goyal S, Lin ZM, Liu JS, et al. Biological structure \nand function emerge from scaling unsupervised learning to 250 million \nprotein sequences. Proc Natl Acad Sci U S A. 2021;118(15): e2016239118.\n 27. Nie LP , Quan LJ, Wu TF, He RJ, Lyu Q. TransPPMP: predicting pathogenic-\nity of frameshift and non-sense mutations by a Transformer based on \nprotein features. Bioinformatics. 2022;38(10):2705–11.\n 28. Wang L, Zhong H, Xue Z, Wang Y. Res-Dom: predicting protein domain \nboundary from sequence using deep residual network and Bi-LSTM. \nBioinform Adv. 2022;2(1):060.\n 29. Lin PC, Yan YM, Huang SY. DeepHomo2.0: improved protein-protein con-\ntact prediction of homodimers by transformer-enhanced deep learning. \nBrief Bioinform. 2022;24(1):499.\n 30. Shashkova TI, Umerenkov D, Salnikov M, Strashnov PV, Konstantinova AV, \nLebed I, et al. SEMA: antigen B-cell conformational epitope prediction \nusing deep transfer learning. Front Immunol. 2022;13:960985.\n 31. Kang Y, Elofsson A, Jiang Y, Huang W, Yu M, Li Z. AFTGAN: prediction of \nmulti-type PPI based on attention free transformer and graph attention \nnetwork. Bioinformatics. 2023;39(2):052.\n 32. Park Y, Marcotte EM. Flaws in evaluation schemes for pair-input computa-\ntional predictions. Nat Methods. 2012;9(12):1134–6.\n 33. Elnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, et al. \nProtTrans: toward understanding the language of life through self-super-\nvised learning. IEEE Trans Pattern Anal Mach Intell. 2022;44(10):7112–27.\n 34. Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM. Unified rational \nprotein engineering with sequence-based deep representation learning. \nNat Methods. 2019;16(12):1315–22.\n 35. Rao RS, Bhattacharya N, Thomas N, Duan Y, Chen X, Canny J, et al. Evaluat-\ning protein transfer learning with TAPE. Adv Neural Inf Process Syst. \n2019;32(1):9689–701.\n 36. Szymborski J, Emad A. RAPPPID: towards generalizable protein \ninteraction prediction with AWD-LSTM twin networks. Bioinformatics. \n2022;38(16):3958–67.\n 37. Chen MH, Ju CJT, Zhou GY, Chen XL, Zhang TR, Chang KW, et al. Multifac-\neted protein-protein interaction prediction based on Siamese residual \nRCNN. Bioinformatics. 2019;35(14):I305–14.\n 38. Song BS, Luo XY, Luo XL, Liu YS, Niu ZM, Zeng XX. Learning spatial struc-\ntures of proteins improves protein-protein interaction prediction. Brief \nBioinform. 2022;23(2):558.\n 39. Zhang FY, Liu SW, Li L, Zuo KJ, Zhao LX, Zhang LD. Genome-wide \ninference of protein-protein interaction networks identifies crosstalk in \nabscisic acid signaling. Plant Physiol. 2016;171(2):1511–22.\n 40. Zheng JY, Yang XD, Huang Y, Yang SP , Wuchty S, Zhang ZD. Deep \nlearning-assisted prediction of protein-protein interactions in Arabidopsis \nthaliana. Plant J. 2023;114(4):984–94.\n 41. Li JF, Zhou HP , Zhang Y, Li Z, Yang YQ, Guo Y. The GSK3-like Kinase BIN2 \nIs a Molecular Switch between the Salt Stress Response and Growth \nRecovery in Arabidopsis thaliana. Dev Cell. 2020;55(3):367–80.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Arabidopsis",
  "concepts": [
    {
      "name": "Arabidopsis",
      "score": 0.676064133644104
    },
    {
      "name": "Protein–protein interaction",
      "score": 0.4189292788505554
    },
    {
      "name": "Computer science",
      "score": 0.35893481969833374
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3338634669780731
    },
    {
      "name": "Computational biology",
      "score": 0.3275246024131775
    },
    {
      "name": "Chemistry",
      "score": 0.30525463819503784
    },
    {
      "name": "Biology",
      "score": 0.27844512462615967
    },
    {
      "name": "Biochemistry",
      "score": 0.17392688989639282
    },
    {
      "name": "Gene",
      "score": 0.13381555676460266
    },
    {
      "name": "Mutant",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I52158045",
      "name": "China Agricultural University",
      "country": "CN"
    }
  ],
  "cited_by": 12
}