{
    "title": "Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study",
    "url": "https://openalex.org/W4408165606",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A3184239393",
            "name": "Ryan K. McBain",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4213767403",
            "name": "Jonathan H Cantor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2198485068",
            "name": "Li'ang Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2203868507",
            "name": "Olesya Baker",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2002948643",
            "name": "Fang Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5020837048",
            "name": "Alyssa Halbisen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1311435632",
            "name": "Aaron Kofner",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2558772790",
            "name": "Joshua Breslau",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2404533129",
            "name": "Bradley Stein",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2181983277",
            "name": "Ateev Mehrotra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108890553",
            "name": "Hao Yu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4399977125",
        "https://openalex.org/W4377197101",
        "https://openalex.org/W1585586035",
        "https://openalex.org/W4393081673",
        "https://openalex.org/W3173909632",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4404636419",
        "https://openalex.org/W4389923012",
        "https://openalex.org/W4386887838",
        "https://openalex.org/W4390722663",
        "https://openalex.org/W2113224842",
        "https://openalex.org/W2571760747",
        "https://openalex.org/W2678784643",
        "https://openalex.org/W4251249965",
        "https://openalex.org/W1977690778",
        "https://openalex.org/W2119211753",
        "https://openalex.org/W3011113044",
        "https://openalex.org/W1999982467",
        "https://openalex.org/W2118282081",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4389917881",
        "https://openalex.org/W4393867901",
        "https://openalex.org/W4396691546",
        "https://openalex.org/W4398766485",
        "https://openalex.org/W4387929411",
        "https://openalex.org/W4308902180"
    ],
    "abstract": "Background With suicide rates in the United States at an all-time high, individuals experiencing suicidal ideation are increasingly turning to large language models (LLMs) for guidance and support. Objective The objective of this study was to assess the competency of 3 widely used LLMs to distinguish appropriate versus inappropriate responses when engaging individuals who exhibit suicidal ideation. Methods This observational, cross-sectional study evaluated responses to the revised Suicidal Ideation Response Inventory (SIRI-2) generated by ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. Data collection and analyses were conducted in July 2024. A common training module for mental health professionals, SIRI-2 provides 24 hypothetical scenarios in which a patient exhibits depressive symptoms and suicidal ideation, followed by two clinician responses. Clinician responses were scored from –3 (highly inappropriate) to +3 (highly appropriate). All 3 LLMs were provided with a standardized set of instructions to rate clinician responses. We compared LLM responses to those of expert suicidologists, conducting linear regression analyses and converting LLM responses to z scores to identify outliers (z score&gt;1.96 or &lt;–1.96; P&lt;0.05). Furthermore, we compared final SIRI-2 scores to those produced by health professionals in prior studies. Results All 3 LLMs rated responses as more appropriate than ratings provided by expert suicidologists. The item-level mean difference was 0.86 for ChatGPT (95% CI 0.61-1.12; P&lt;.001), 0.61 for Claude (95% CI 0.41-0.81; P&lt;.001), and 0.73 for Gemini (95% CI 0.35-1.11; P&lt;.001). In terms of z scores, 19% (9 of 48) of ChatGPT responses were outliers when compared to expert suicidologists. Similarly, 11% (5 of 48) of Claude responses were outliers compared to expert suicidologists. Additionally, 36% (17 of 48) of Gemini responses were outliers compared to expert suicidologists. ChatGPT produced a final SIRI-2 score of 45.7, roughly equivalent to master’s level counselors in prior studies. Claude produced an SIRI-2 score of 36.7, exceeding prior performance of mental health professionals after suicide intervention skills training. Gemini produced a final SIRI-2 score of 54.5, equivalent to untrained K-12 school staff. Conclusions Current versions of 3 major LLMs demonstrated an upward bias in their evaluations of appropriate responses to suicidal ideation; however, 2 of the 3 models performed equivalent to or exceeded the performance of mental health professionals.",
    "full_text": null
}