{
  "title": "Assessing the Accuracy and Reliability of Large Language Models in Psychiatry Using Standardized Multiple-Choice Questions: Cross-Sectional Study",
  "url": "https://openalex.org/W4410542647",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3037265872",
      "name": "Kaitlin Hanss",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146284142",
      "name": "Karthik V. Sarma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2026190185",
      "name": "Anne L. Glowinski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2766058590",
      "name": "Andrew Krystal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2251028533",
      "name": "Ramotse Saunders",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2581073702",
      "name": "Andrew Halls",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2209044862",
      "name": "Sasha Gorrell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128136206",
      "name": "Erin Reilly",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2991229128",
    "https://openalex.org/W4389885061",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4401358107",
    "https://openalex.org/W4399560926",
    "https://openalex.org/W4386117324",
    "https://openalex.org/W4288069275",
    "https://openalex.org/W4383741207",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W4386023761",
    "https://openalex.org/W4399701665",
    "https://openalex.org/W2084791780",
    "https://openalex.org/W4388681990",
    "https://openalex.org/W4248747466",
    "https://openalex.org/W2118347738",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W4388007786",
    "https://openalex.org/W4392504765",
    "https://openalex.org/W4368340908",
    "https://openalex.org/W4406892137",
    "https://openalex.org/W652322185"
  ],
  "abstract": "Background Large language models (LLMs), such as OpenAI’s GPT-3.5, GPT-4, and GPT-4o, have garnered early and significant enthusiasm for their potential applications within mental health, ranging from documentation support to chat-bot therapy. Understanding the accuracy and reliability of the psychiatric “knowledge” stored within the parameters of these models and developing measures of confidence in their responses (ie, the likelihood that an LLM response is accurate) are crucial for the safe and effective integration of these tools into mental health settings. Objective This study aimed to assess the accuracy, reliability, and predictors of accuracy of GPT-3.5 (175 billion parameters), GPT-4 (approximately 1.8 trillion parameters), and GPT-4o (an optimized version of GPT-4 with unknown parameters) with standardized psychiatry multiple-choice questions (MCQs). Methods A cross-sectional study was conducted where 3 commonly available, commercial LLMs (GPT-3.5, GPT-4, and GPT-4o) were tested for their ability to provide answers to single-answer MCQs (N=150) extracted from the Psychiatry Test Preparation and Review Manual. Each model generated answers to every MCQ 10 times. We evaluated the accuracy and reliability of the answers and sought predictors of answer accuracy. Our primary outcome was the proportion of questions answered correctly by each LLM (accuracy). Secondary measures were (1) response consistency to MCQs across 10 trials (reliability), (2) the correlation between MCQ answer accuracy and response consistency, and (3) the correlation between MCQ answer accuracy and model self-reported confidence. Results On the first attempt, GPT-3.5 answered 58.0% (87/150) of MCQs correctly, while GPT-4 and GPT-4o answered 84.0% (126/150) and 87.3% (131/150) correctly, respectively. GPT-4 and GPT-4o showed no difference in performance (P=.51), but they significantly outperformed GPT-3.5 (P&lt;.001). GPT-3.5 exhibited less response consistency on average compared to the other models (P&lt;.001). MCQ response consistency was positively correlated with MCQ accuracy across all models (r=0.340, 0.682, and 0.590 for GPT-3.5, GPT-4, and GPT-4o, respectively; all P&lt;.001), whereas model self-reported confidence showed no correlation with accuracy in the models, except for GPT-3.5, where self-reported confidence was weakly inversely correlated with accuracy (P&lt;.001). Conclusions To our knowledge, this is the first comprehensive evaluation of the general psychiatric knowledge encoded in commercially available LLMs and the first study to assess their reliability and identify predictors of response accuracy within medical domains. The findings suggest that GPT-4 and GPT-4o encode accurate and reliable general psychiatric knowledge and that methods, such as repeated prompting, may provide a measure of LLM response confidence. This work supports the potential of LLMs in mental health settings and motivates further research to assess their performance in more open-ended clinical contexts.",
  "full_text": null,
  "topic": "Reliability (semiconductor)",
  "concepts": [
    {
      "name": "Reliability (semiconductor)",
      "score": 0.7117393612861633
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6059423089027405
    },
    {
      "name": "Confidence interval",
      "score": 0.5441946983337402
    },
    {
      "name": "Psychology",
      "score": 0.4469680190086365
    },
    {
      "name": "Mental health",
      "score": 0.4397760033607483
    },
    {
      "name": "Medicine",
      "score": 0.3791806101799011
    },
    {
      "name": "Computer science",
      "score": 0.30768072605133057
    },
    {
      "name": "Psychiatry",
      "score": 0.2557941675186157
    },
    {
      "name": "Internal medicine",
      "score": 0.16890683770179749
    },
    {
      "name": "Artificial intelligence",
      "score": 0.15534457564353943
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I180670191",
      "name": "University of California, San Francisco",
      "country": "US"
    }
  ]
}