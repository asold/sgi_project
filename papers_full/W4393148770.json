{
    "title": "Dual-Window Multiscale Transformer for Hyperspectral Snapshot Compressive Imaging",
    "url": "https://openalex.org/W4393148770",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2107697430",
            "name": "Fulin Luo",
            "affiliations": [
                "Chongqing University"
            ]
        },
        {
            "id": "https://openalex.org/A1983188002",
            "name": "Xi Chen",
            "affiliations": [
                "Chongqing University"
            ]
        },
        {
            "id": "https://openalex.org/A2346892906",
            "name": "Xiuwen Gong",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2135663424",
            "name": "Wei-Wen Wu",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2111950797",
            "name": "Tan Guo",
            "affiliations": [
                "Chongqing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2107697430",
            "name": "Fulin Luo",
            "affiliations": [
                "Chongqing University"
            ]
        },
        {
            "id": "https://openalex.org/A1983188002",
            "name": "Xi Chen",
            "affiliations": [
                "Chongqing University"
            ]
        },
        {
            "id": "https://openalex.org/A2346892906",
            "name": "Xiuwen Gong",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2135663424",
            "name": "Wei-Wen Wu",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2111950797",
            "name": "Tan Guo",
            "affiliations": [
                "Chongqing University of Posts and Telecommunications"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3145185940",
        "https://openalex.org/W4312380264",
        "https://openalex.org/W3212235541",
        "https://openalex.org/W4281383159",
        "https://openalex.org/W3160284783",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W2787091153",
        "https://openalex.org/W2770113520",
        "https://openalex.org/W3170197681",
        "https://openalex.org/W3103465009",
        "https://openalex.org/W3123547918",
        "https://openalex.org/W2002498099",
        "https://openalex.org/W4377079779",
        "https://openalex.org/W4221150510",
        "https://openalex.org/W3015788359",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W3107026593",
        "https://openalex.org/W6753203523",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4319865968",
        "https://openalex.org/W3096654432",
        "https://openalex.org/W6780605350",
        "https://openalex.org/W2163753106",
        "https://openalex.org/W3216314363",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2166785998",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W6810003034",
        "https://openalex.org/W2084591647",
        "https://openalex.org/W4287062814",
        "https://openalex.org/W6753412334",
        "https://openalex.org/W6810167572",
        "https://openalex.org/W2271580427",
        "https://openalex.org/W3134510327",
        "https://openalex.org/W2990070112",
        "https://openalex.org/W2724359148",
        "https://openalex.org/W4364323060",
        "https://openalex.org/W3183925776",
        "https://openalex.org/W4327673664",
        "https://openalex.org/W3167456680",
        "https://openalex.org/W4226277663",
        "https://openalex.org/W4321232185",
        "https://openalex.org/W2963764784",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963125010",
        "https://openalex.org/W3165924482",
        "https://openalex.org/W2964309882",
        "https://openalex.org/W4287375617",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4312847199",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W4386075524",
        "https://openalex.org/W4324297125",
        "https://openalex.org/W4312960790",
        "https://openalex.org/W2884144629",
        "https://openalex.org/W4312628443",
        "https://openalex.org/W4313160444",
        "https://openalex.org/W3041490661",
        "https://openalex.org/W3171660447",
        "https://openalex.org/W3214707056"
    ],
    "abstract": "Coded aperture snapshot spectral imaging (CASSI) system is an effective manner for hyperspectral snapshot compressive imaging. The core issue of CASSI is to solve the inverse problem for the reconstruction of hyperspectral image (HSI). In recent years, Transformer-based methods achieve promising performance in HSI reconstruction. However, capturing both long-range dependencies and local information while ensuring reasonable computational costs remains a challenging problem. In this paper, we propose a Transformer-based HSI reconstruction method called dual-window multiscale Transformer (DWMT), which is a coarse-to-fine process, reconstructing the global properties of HSI with the long-range dependencies. In our method, we propose a novel U-Net architecture using a dual-branch encoder to refine pixel information and full-scale skip connections to fuse different features, enhancing the extraction of fine-grained features. Meanwhile, we design a novel self-attention mechanism called dual-window multiscale multi-head self-attention (DWM-MSA), which utilizes two different-sized windows to compute self-attention, which can capture the long-range dependencies in a local region at different scales to improve the reconstruction performance. We also propose a novel position embedding method for Transformer, named con-abs position embedding (CAPE), which effectively enhances positional information of the HSIs. Extensive experiments on both the simulated and the real data are conducted to demonstrate the superior performance, stability, and generalization ability of our DWMT. Code of this project is at https://github.com/chenx2000/DWMT.",
    "full_text": "Dual-Window Multiscale Transformer for Hyperspectral\nSnapshot Compressive Imaging\nFulin Luo1, Xi Chen1, Xiuwen Gong2, Weiwen Wu3, Tan Guo4*\n1College of Computer Science, Chongqing University\n2Faculty of Engineering, The University of Sydney\n3Department of Biomedical Engineering, Sun-Yat-sen University\n4School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications\nluoflyn@163.com, chenxi2000cd@gmail.com, gongxiuwen@gmail.com,\nwuweiw7@mail.sysu.edu.cn, guot@cqupt.edu.cn\nAbstract\nCoded aperture snapshot spectral imaging (CASSI) system\nis an effective manner for hyperspectral snapshot compres-\nsive imaging. The core issue of CASSI is to solve the inverse\nproblem for the reconstruction of hyperspectral image (HSI).\nIn recent years, Transformer-based methods achieve promis-\ning performance in HSI reconstruction. However, capturing\nboth long-range dependencies and local information while\nensuring reasonable computational costs remains a challeng-\ning problem. In this paper, we propose a Transformer-based\nHSI reconstruction method called dual-window multiscale\nTransformer (DWMT), which is a coarse-to-fine process,\nreconstructing the global properties of HSI with the long-\nrange dependencies. In our method, we propose a novel U-\nNet architecture using a dual-branch encoder to refine pixel\ninformation and full-scale skip connections to fuse differ-\nent features, enhancing the extraction of fine-grained fea-\ntures. Meanwhile, we design a novel self-attention mecha-\nnism called dual-window multiscale multi-head self-attention\n(DWM-MSA), which utilizes two different-sized windows\nto compute self-attention, which can capture the long-range\ndependencies in a local region at different scales to im-\nprove the reconstruction performance. We also propose a\nnovel position embedding method for Transformer, named\ncon-abs position embedding (CAPE), which effectively en-\nhances positional information of the HSIs. Extensive experi-\nments on both the simulated and the real data are conducted\nto demonstrate the superior performance, stability, and gen-\neralization ability of our DWMT. Code of this project is at\nhttps://github.com/chenx2000/DWMT.\nIntroduction\nHyperspectral images (HSI) consist of multiple contiguous\nspectral bands, providing richer spatial and spectral informa-\ntion compared with RGB images. Since HSI can reveal the\nspectral properties of object, it has been widely used in com-\nputer vision (CV) tasks such as image classification (Guo\net al. 2023), change detection (Luo et al. 2023), and medical\nimage processing (Meng et al. 2020). Therefore, it is highly\nvaluable to explore efficient and cost-effective hyperspec-\ntral imaging techniques. The traditional hyperspectral image\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nHDNet\nDWMT (ours)\nCST\nRGB Image\nMeasurement\n648.1 nm\n594.4 nm\n594.4 nm\n594.4 nm\n648.1 nm\n648.1 nm\nMeasurement\nRGB Image\n594.4 nm\n594.4 nm\n594.4 nm\n648.1 nm\n648.1 nm\n648.1 nm\nFigure 1: Real HSI reconstructed results of our DWMT com-\npared with three end-to-end trained methods on two (out of\n28) spectral channels of Scene 1 and Scene 3.\n(Schechner and Nayar 2002) typically is captured with hy-\nperspectral imager that is a very expensive equipment. To\nreduce the imaging costs, the snapshot compressive imaging\n(SCI) systems are developed to capture HSIs, using a 2D\nmeasurement to reconstruct a 3D HSI with a computational\nspectral imaging algorithm (Yuan, Brady, and Katsaggelos\n2021). This significantly improves imaging efficiency and\nenables capturing dynamic scenes. Among these systems,\ncoded aperture snapshot spectral imaging (CASSI) (Gehm\net al. 2007) has shown impressive performance. CASSI\nmodulates the HSI signals of different wavelengths through\nthe coded aperture and disperser, and then combines all the\nmodulated signals to achieve 2D measurement. The core\ntask of CASSI is to address the ill-posed inverse problem,\nrecovering a 3D HSI from the 2D measurement.\nThe traditional methods (Yuan 2016; Liu et al. 2018;\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3972\nZhang et al. 2019) for HSI reconstruction utilize hand-\ncrafted priors to regularize the reconstruction process, gen-\nerally resulting in poor generalization and limited recon-\nstruction quality. Recently, CNN-based methods (Miao et al.\n2019; Meng, Ma, and Yuan 2020; Hu et al. 2022) have\nbeen extensively employed to construct the 3D HSI from the\n2D measurement. However, CNN-based methods still have\nlimitations in capturing long-range dependencies to achieve\nglobal properties for HSI reconstruction.\nTransformer (Vaswani et al. 2017) has achieved tremen-\ndous success in the field of natural language processing\n(NLP), which has motivated researchers to explore the ap-\nplication of Transformer in computer vision (Dosovitskiy\net al. 2021). The vision Transformer (ViT) was first proposed\nin CV to achieve the long-range dependencies between to-\nkens, overcoming the limitations of CNNs. For HSI recon-\nstruction, Transformers also generate impressive results (Cai\net al. 2022a,b). However, the global Transformers (Dosovit-\nskiy et al. 2021) have high computational cost, while the\nlocal Transformers (Liu et al. 2021) have limitations in cap-\nturing long-range dependencies. Due to HSIs containing a\nlarge number of channels, a challenging problem is to si-\nmultaneously capture the long-range dependencies and the\nlocal information with reasonable computational costs.\nTo balance the long-range dependencies and the local rep-\nresentation, we propose a dual-window multiscale Trans-\nformer (DWMT) for HSI reconstruction. In this method,\nwe introduce a novel attention mechanism, namely dual-\nwindow multiscale mult-head self-attention (DWM-MSA).\nBased on the divide-and-conquer approach, our DWM-MSA\nis divided into four branches. Two branches perform self-\nattention within the window to achieve the local information,\nwhile the other two branches shuffle the tokens to perform\nthe long-range dependencies. We implement the long-range\ndependencies in two local regions with different window\nsizes, which can balance the global reconstruction properties\nand the computational cost with a local manner. By using di-\nverse window sizes, the varying scales of features and details\ncan be captured. In addition, we introduce a novel position\nembedding method, con-abs position embedding (CAPE),\nto enhance the positional information of image. As shown\nin Figure 2, our method produces the reconstructed images\non the real dataset that are clearer compared with the other\nmethods (Hu et al. 2022; Cai et al. 2022a), and our method\nrecovers more fine details in the images. The main contribu-\ntions of this paper include the following:\n• An end-to-end Transformer algorithm, DWMT, is pro-\nposed for HSI reconstruction, consisting of two stages,\ni.e., coarse feature extraction and fine pixel refinement.\n• We design a novel attention mechanism, DWM-MSA,\nwhich can simultaneously capture local information,\nlong-range dependencies and multi-scale information.\n• A novel position embedding method, CAPE, is devel-\noped to enhance the accuracy of positional information\nwithin the image efficiently.\n• We develop a fine pixel refinement branch to enhance the\nrepresentation of pixels, which improves the reconstruc-\ntion performance of our method.\nY\nCoded\nAperture Spectral\nSpatial\nSpatial Dispersive\nElement \nS′′SʹMS\nFigure 2: Schematic diagram of CASSI.\nRelated Work\nCoded Aperture Snapshot Spectral Imaging\nCoded aperture snapshot spectral imaging (CASSI) sys-\ntem utilizes a coded aperture (physical mask) and one or\nmore dispersive elements to modulate HSI signals at differ-\nent wavelengths, which can capture a 2D projection of the\n3D HSI (Gehm et al. 2007; Wagadarikar et al. 2008). The\nschematic diagram of CASSI is shown in Figure 2.\nSpecifically, we denote the 3D HSI as S ∈ RH×W×N ,\nwhere H, W, and N represent the height, width, and number\nof spectral channels, respectively. In the CASSI system, the\nfirst step is to modulate the initial signal S using the coded\naperture M ∈ RH×W (Cai et al. 2022a,b). S′ ∈ RH×W×N\ndenotes the modulated HSI, then the nth wavelength of S′\ncan be represented as:\nS′\nn = M ⊙ Sn (1)\nwhere n ∈ [1, ..., N] is the nth spectral channel, and ⊙ de-\nnotes the element-wise multiplication. The dispersive ele-\nment contains a linear dispersion α and a center wavelength\nλc (Gehm et al. 2007). After passing through the dispersive\nelement, S′ undergoes a shear along the y-axis, resulting\nin the sheared HSI S′′ ∈ RH×(W+α(N−1))×N . α denotes\nthe linear dispersion which can be understood as the shifting\nstep. Then, S′′ can be formulated as:\nS′′(x′, y′, n) =S′(x, y+ α(λn − λc), n) (2)\nwhere (x′, y′) denotes the coordinates on the detector plane,\nand λn represents the wavelength of nth spectral channel.\nTherefore, α(λn − λc) denotes the spatial displacement of\nthe nth channel. Consequently, the 2D measurement Y ∈\nRH×(W+α(N−1)) captured by CASSI can be expressed as:\nY =\nNX\nn=1\nS′′(:, :, N) +E (3)\nwhere E ∈ RH×(W+α(N−1)) is the measurement noise. In\nsummary, the core issue of HSI reconstruction is to recover\nthe HSI S with the 2D measurement Y.\nHSI Reconstruction\nHSI reconstruction is a crucial step in hyperspectral SCI.\nHowever, the reconstruction problem is ill-posed, necessi-\ntating an appropriate prior representation of HSI. Traditional\nHSI reconstruction methods (Wagadarikar et al. 2008; Yuan\n2016; Liu et al. 2018; Zhang et al. 2019) rely on predefined\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3973\nShift\nBack\nInput \nY\nB\nH×(W+\nα\n(N\n-\n1)\n)\nH×W×N\nM\nH×W×N\nCAFM\nOutput \nX\nʹʹ\nH×W×N\nLayer\nNorm\nDWM\nMSA\nLayer\nNorm\nConv\n1×1\nGELU\nConv\n1×1\nConcat\nH×W×N\nX\nX\nʹ\nR\nC\n+\n+\n3×3 DwConv\nStride=2\nC\nC\n1×1 Conv\nStride=1\nASPP\n2×2 DeConv\nStride=2\n3×3 DwConv\nStride=1\n3×3 Conv\nStride=1\nDWMAB\n4×4 Conv\nStride=2\nAvePooling\n3×3 Conv\n+\n+\nDWConv\n3×3\nDWConv\n3×3\nGELU\n+\nBilinear Upsample\n3×3 Conv\n(a)\n(b)\nFigure 3: (a) Overall architecture of our DWMT. (b) Dual-window multiscale attention block (DWMAB).\nhand-crafted priors. For example, GAP-TV (Yuan 2016)\nadopts total variation (TV) regularization as the prior for\neach spectral band. However, these approaches suffer from\npoor generalization and limited performance. Recently, deep\nlearning-based methods (Miao et al. 2019; Meng, Ma, and\nYuan 2020; Hu et al. 2022; Cai et al. 2022a,b) have shown\nthe capability to directly learn image priors by training\non large-scale datasets. For example, TSA-Net (Meng, Ma,\nand Yuan 2020) introduces a spatial-spectral self-attention,\nwhich sequentially processes each dimension in an order-\nindependent manner. However, CNN-based methods have\ncertain limitations in capturing long-range dependencies.\nWhile Transformer-based methods (Cai et al. 2022a,b) are\neffective to learn the long-range dependencies. However, it\nis very key to balance the long-range dependencies and the\nlocal representation for reasonable computational cost.\nTransformer\nVision Transformer Transformer (Vaswani et al. 2017) is\na network architecture based on the attention mechanism,\ncompletely abandoning recurrence and convolution. It was\ninitially introduced in the field of NLP for machine transla-\ntion tasks, and achieved outstanding performance. Inspired\nby the success of Transformer in NLP, researchers developed\nVision Transformer (ViT) (Dosovitskiy et al. 2021). ViT has\nshown promising performance in various CV tasks, such as\nsemantic segmentation (Dai et al. 2021a; Zhu et al. 2021),\nobject detection (Dai et al. 2021b; Gao et al. 2021), and im-\nage classification (Bhojanapalli et al. 2021; Lanchantin et al.\n2021), etc. However, the high computational demand of the\nglobal Transformer cannot be ignored, as its computational\ncomplexity is a multiple of the quadratic power of image\nsize.\nViT Variants To reduce the computational cost of global\nTransformers, some works (Liu et al. 2021; Wang et al.\n2022) have adopted local Transformers. Swin-Transformer\n(Liu et al. 2021) utilizes non-overlap window and performs\nself-attention in the window, exhibiting linear computational\ncomplexity with respect to the image size. To achieve a\nlarger receptive field while ensuring reasonable computa-\ntional cost, some works adopt the dilated window (Tu et al.\n2022; Wang et al. 2022). Furthermore, some works (Xia\net al. 2022; Ren et al. 2022; Zhu et al. 2023) utilize the\nsparse adaptive patterns to compute the attention. However,\nfor HSI, the multiple channels result in the computational\ncost of global Transformer impractically, while the local\nTransformer struggles with capturing the long-range depen-\ndencies of HSI. Therefore, we combine the advantages of\nthe global and local Transformers, effectively balancing the\nlong-range dependencies and the computational cost.\nDual-Window Multiscale Transformer\nThe overall structure of DWMT is illustrated in Figure 3. We\ndevelop an end-to-end framework based on Transformer for\nHSI reconstruction, consisting of two parts, i.e., coarse fea-\nture extraction and fine pixel refinement. Inspired by (Cao\net al. 2022; Cai et al. 2022a), we design two U-Net (Ron-\nneberger, Fischer, and Brox 2015) architectures separately\nfor the two parts, and full-scale skip connections (Huang\net al. 2020) are utilized. Among them, we employ an en-\nhanced U-Net architecture with a dual-branch encoder for\nfine pixel refinement, which includes the most crucial part\nin the proposed DWMT network, i.e., dual-window multi-\nscale attention block (DWMAB).\nOverall Architecture\nFirstly, the 2D measurement Y ∈ RH×(W+α(N−1)) is\nshifted back to a 3D image. Thus, the nth wavelength of\nthe shifted signal B ∈ RH×W×N can be represented as:\nBn(x, y) =Y(x, y− α((λn − λc)) (4)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3974\nMLP\nChannel\nAttention\n×\n×\nH×W×N\nN×N\nSpatial\nAttention\n×\n+\nH×W×N\nC\nH×W×2N\nH×W×N\nFirst\nInput\nMaxPool\nH×W×N\n1×1×N\n1×1×N\n3×3\nConv\n1×1×N\n+\n1×1×N\nSoftmax\nSoftmax\nSoftmax\nH×W×1\nH×W×1\nC\nH×W×2\nH×W×1\nH×W×1\nH×W×N\nAvgPool\nAvgPool\nMaxPool\n3×3\nConv\nMLP\nChannel\nAttention\n1×1\nConv\n1×1×N\n×\nSoftmax\n×\n+\nH×W×N\nH×W×N\nSecond\nInput\nFusion\nOutput\n(a)\n(b) (c)\nFigure 4: (a) Architecture of cross attention fusion module (CAFM). (b) Channel attention module. (c) Spatial attention module.\nLinear\nQuarter\nDivision\nInput \nX\nQ, K,V\nQ, K,V\nWindow\nMSA\nQ\nw1\n, K\nw1\n,V\nw1\nQ\nw1\nʹ\n, K\nw1\nʹ\n,V\nw1\nʹ\nQ\nw2\n, K\nw2\n,V\nw2\nQ\nw2\nʹ\n, K\nw2\nʹ\n,V\nw2\nʹ\nA\nw1\nA\nw1\nʹ\nA\nw2\nA\nw2\nʹ\nC\nC\nConcat\nWindow size 1\nWindow size 2\nOutput \nY\nShuffle\nShuffle\nUnshuffle\nUnshuffle\nWindow\nMSA\nWindow\nMSA\nWindow\nMSA\nH W N\n� �\nH W C\n� �\nH W N\n� �\nLinear\n2\n1\n2\n1\n4\nHW C\nS\nS\n� �\n2\n1\n2\n1\n4\nHW C\nS\nS\n� �\n2\n1\n2\n1\n4\nHW C\nS\nS\n� �\n2\n1\n2\n1\n4\nHW C\nS\nS\n� �\n2\n2\n2\n2\n4\nHW C\nS\nS\n� �\n2\n2\n2\n2\n4\nHW C\nS\nS\n� �\n2\n2\n2\n2\n4\nHW C\nS\nS\n� �\n2\n2\n2\n2\n4\nHW C\nS\nS\n� �\nFigure 5: Schematic diagram of dual-window multiscale multi-head self-attention (DWM-MSA).\nwhere n ∈ [1, ..., N] is the nth channel. Then, we copy the\ncoded aperture M for N (the number of channels) times\nand concatenate them with B. The obtained result is subse-\nquently fed into a 1×1 convolution (Conv) layer for further\nprocessing, and its output X ∈ RH×W×N is passed to the\ncoarse feature extraction and fine pixel refinement. Next, we\nadd the result of fine pixel refinement R ∈ RH×W×N to X,\nresulting in the reconstructed HSI X′′ ∈ RH×W×N .\nIf we denote the ground truth HSI as X∗ ∈ RH×W×N ,\nthen the overall network loss function can be represented as:\nL = ∥X′′ − X∗∥2 (5)\nCoarse Feature Extraction The coarse feature extrac-\ntion part is a U-Net architecture comprising of a four-stage\nencoder-decoder. In the encoder, we use a 3× 3 depth-wise\nconvolution (DwConv) layer for downsampling. The bottle-\nneck layer utilizes an atrous spatial pyramid pooling (ASPP)\nmodule (Chen et al. 2017, 2018). In the decoder, we use a\n2×2 deconvolution (DeConv) layer for upsampling.\nFine Pixel Refinement The fine pixel refinement part con-\nsists of a three-stage encoder-decoder architecture, where\nthe encoder is a dual-branch structure. In the first branch\nof the encoder, each stage employs a 4×4 Conv layer for\ndownsampling. The second branch takes the features from a\nsmaller scale and processes them through DWMABs. These\nprocessed features are fused with the results obtained from\nthe first branch of the encoder for feature enhancement. Ac-\ncording to (Woo et al. 2018; Zhou et al. 2023), we design\na cross attention fusion module (CAFM) to effectively in-\ntegrate the features from the two encoder branches. The\nschematic diagram of CAFM is shown in Figure 4. In the\ndecoder, we use a 2×2 DeConv layer for upsampling.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3975\nDual-Window Multiscale Attention Block\nTo combine the advantages of global Transformer and local\nTransformer, we propose a novel Transformer module called\ndual-window multiscale attention block (DWMAB). We ap-\nply conditional position embedding (CPE) (Chu et al. 2023)\nto the input Xin ∈ RH×W×N via a 3×3 DwConv layer:\nX = CPE(Xin) +Xin = DwConv(Xin) +Xin (6)\nDual-Window Multiscale Multi-Head Self-Attention\nInspired by ShuffleNet (Zhang et al. 2018) and Half-Shuffle\nTransformer (Cai et al. 2022c), we propose a novel self-\nattention mechanism called dual-window multiscale multi-\nhead self-attention (DWM-MSA) based on the divide-and-\nconquer approach, which effectively captures the long-range\ndependencies in a local region. Additionally, it enables to\ncapture the features and details at different scales with two\ndifferent-sized windows. The schematic diagram of DWM-\nMSA is depicted in Figure 5.\nMultiscale Window Partition First, we linearly project\nX to obtain the query, key, and value:\nQ = XWq, K = XWk, V = XWv (7)\nwhere Wq, Wk, Wv represent the projection\nweights for the query, key and value, respectively.\nThen, we divide the query, key, and value into\nfour equal parts along the channel dimension: Q =\n[Qw1, Q′\nw1, Qw2, Q′\nw2], K = [Kw1, K′\nw1, Kw2, K′\nw2]\nand V = [Vw1, V′\nw1, Vw2, V′\nw2].\nNext, we apply W-MSA (Liu et al. 2021) using two dif-\nferent window sizes to\nh\nQw1, Kw1, Vw1, Q\n′\nw1, K\n′\nw1, V\n′\nw1\ni\nand\nh\nQw2, Kw2, Vw2, Q\n′\nw2, K\n′\nw2, V\n′\nw2\ni\n, respectively.\nMulti-Head Self-Attention Calculation We will calcu-\nlate a single window multi-head self-attention (MSA). As\ndepicted in the top first branch of Figure 5, we employ\nseveral non-overlapping windows of size S1×S1 to seg-\nment Qw1, Kw1, Vw1, which essentially reshapes them\ninto Qw1, Kw1, Vw1 ∈ R\nHW\nS2\n1\n×S2\n1 ×C\n4\n. We divide them\ninto several heads along the channel dimension: Qw1 =\u0002\nQ1\nw1, ...,Qh\nw1\n\u0003\n, Kw1 =\n\u0002\nK1\nw1, K2\nw1, ...,Kh\nw1\n\u0003\nand Vw1 =\u0002\nV1\nw1, V2\nw1, ...,Vh\nw1\n\u0003\n. In addition, we incorporate posi-\ntional information again by utilizing the absolute position\nembedding (APE) (Vaswani et al. 2017), adding a learnable\nparameter Pn\nw1 ∈ RS2\n1 ×S2\n1 for implementation. Let d = C\n4hdenote the dimension of each head and h denote the num-\nber of heads, then the self-attention of the nth head can be\nrepresented as:\nAn\nw1 = softmax\n\u0012Qn\nw1Kn\nw1\nT\n√\nd\n+ Pn\nw1\n\u0013\nVn\nw1 (8)\nThe output of the first branch can be represented as:\nAw1 = Concat\n\u0000\nA1\nw1, ··· , Ah\nw1\n\u0001\n(9)\nNext, we will calculate the window MSA for the sec-\nond branch. First, we partition Q′\nw1, K′\nw1, V′\nw1 into non-\noverlapping windows of size S1×S1. Then, we apply the\nshuffle operation by reshaping them into R\nS2\n1 ×HW\nS2\n1\n×C\n4\n. The\nsubsequent operations are the same as the first branch. After\ncomputing the attention An\nw1\n′\n, we implement an unshuffle\noperation, reshaped it into R\nHW\nS2\n1\n×S2\n1 ×C\n. Then the output of\nthe second branch can be represented as:\nA\n′\nw1 = Concat\n\u0010\nA1\nw1\n′\n, ··· , Ah\nw1\n′ \u0011\n(10)\nSubsequently, we perform the same steps to calculate the\nself-attention for the other two branches with a window size\nof S2×S2, resulting in Aw2 and A\n′\nw2. Then the outputs\nof the four branches are concatenated together and a lin-\near layer is used to obtain the final output of DWM-MSA\nX′ ∈ RH×W×N :\nX′ = Linear(Concat[Aw1, A′\nw1, Aw2, A′\nw2]) (11)\nFinally, X′ is passed through a feed-forward network. The\nself-attention computation for each window focuses on the\nfeatures at different scales, and the fusion of these features\nyields the ultimate self-attention result.\nCon-Abs Position Embedding Transformers typically\nadopt a single type of position embedding method, such as\nabsolute position embedding (APE) (Vaswani et al. 2017),\nrelative position embedding (RPE) (Shaw, Uszkoreit, and\nVaswani 2018), or conditional position embedding (CPE)\n(Chu et al. 2023). However, in DWMAB, we introduce a\nnovel position embedding method called con-abs position\nembedding (CAPE) that combines two types of position em-\nbedding. Specifically, CAPE embeds the input with the con-\nditional position information at the beginning of the block,\nand utilizes the absolute position embedding after the multi-\nplication operation between Q and K.\nExperiments\nDataset Description\nSimilar to (Meng, Ma, and Yuan 2020; Hu et al. 2022; Cai\net al. 2022a,b), we conduct the simulation and real experi-\nments with 28 spectral channels from 450 nm to 650 nm.\nSimulation Data The simulation experiments are con-\nducted on two datasets, i.e., CA VE (Park et al. 2007) and\nKAIST (Choi et al. 2017). CA VE consists of 32 HSIs with\na spatial size of 512×512, while KAIST comprises 30 HSIs\nwith a spatial size of 2704×3376. According to the previ-\nous works (Meng, Ma, and Yuan 2020; Hu et al. 2022; Cai\net al. 2022a,b), we use CA VE as the training set and select\nten scenes from KAIST for testing.\nReal Data We use the real HSIs collected by the CASSI\nsystem (Meng, Ma, and Yuan 2020) to validate the real ap-\nplication performance of the proposed network.\nImplementation Details\nWhen conducting experiments on simulation data and real\ndata, we cropped patches from the 3D HSI with a spatial\nsize of 256×256 and 660×660, respectively. Data is modu-\nlated by the mask and then sheared to simulate the CASSI\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3976\nScene GAP-TV DeSCI TSA-Net HDNet MST CST DWMT\n1 26.82, 0.754 27.13, 0.748 32.03, 0.892 35.14, 0.935 35.40, 0.941 35.96, 0.949 36.46, 0.957\n2 22.89, 0.610 23.04, 0.620 31.00, 0.858 35.67, 0.940 35.87, 0.944 36.84, 0.955 37.75, 0.963\n3 26.31, 0.802 26.62, 0.818 32.25, 0.915 36.03, 0.943 36.51, 0.953 38.16, 0.962 38.47, 0.965\n4 30.65, 0.852 34.96, 0.897 39.19, 0.953 42.30, 0.969 42.27, 0.973 42.44, 0.975 44.23, 0.984\n5 23.64, 0.703 23.94, 0.706 29.39, 0.884 32.69, 0.946 32.77, 0.947 33.25, 0.955 33.99, 0.963\n6 21.85, 0.663 22.38, 0.683 31.44, 0.908 34.46, 0.952 34.80, 0.955 35.72, 0.963 36.17, 0.970\n7 23.76, 0.688 24.45, 0.743 30.32, 0.878 33.67, 0.926 33.66, 0.925 34.86, 0.944 35.22, 0.949\n8 21.98, 0.655 22.03, 0.673 29.35, 0.888 32.48, 0.941 32.67, 0.948 34.34, 0.961 34.56, 0.968\n9 22.63, 0.682 24.56, 0.732 30.01, 0.890 34.89, 0.942 35.39, 0.949 36.51, 0.957 37.41, 0.965\n10 23.10, 0.584 23.59, 0.587 29.59, 0.874 32.38, 0.937 32.50, 0.941 33.09, 0.945 34.00, 0.959\nAverage 24.36, 0.669 25.27, 0.721 31.46, 0.894 34.97, 0.943 35.18, 0.948 36.12, 0.957 36.82, 0.964\nParams - - 44.25M 2.37M 2.03M 3.00M 14.48M\nFLOPs (G) - - 110.03 154.76 28.15 40.10 46.71\nTable 1: PSNR (dB) (left in each cell) and SSIM (right in each cell) values for different methods on ten scenes in the KAIST\ndataset. The bold font is the best performance.\n450\n500\n550\n600\n650\nWavelength (nm)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nDensity\nGround Truth\nTSA\n-\nNet, corr: 0.9826\nHDNet, corr: 0.9836\nMST, corr: 0.9837\nCST, corr: 0.9796\nDWMT, corr: 0.9989\nTSA\n-\nNet\nHDNet\nMST\nCST\nDWMT\nGround Truth\n476.5 nm\n536.5 nm\n575.3 nm\n625.1 nm\nRGB Image\nSpectral Density Curves\nMeasurement\nFigure 6: Reconstructed results with four (out of 28) spectral channels of Scene 5 on the simulation data.\nsystem. The dispersion shifting step α is set to 2. Thus,\nthe simulation and real data have 2D measurement sizes of\n256×310 and 660×714, respectively. Data augmentation in-\nvolves random rotation and flipping. In DWM-MSA, we set\nthe sizes of the two windows as 8×8 and 16×16, respec-\ntively. The DWMT is implemented in PyTorch and trained\nand tested on a single RTX A6000 GPU. The model em-\nploys the Adam (Kingma and Ba 2015) optimizer (β1 = 0.9\nand β2 = 0.999) with 500 epochs and an initial learning\nrate of 4×10−4. The learning rate is adjusted by Cosine An-\nnealing scheme. The batch size is set to 5, and the train-\ning objective is to minimize the Root Mean Square Error\n(RMSE) between the reconstructed image and the corre-\nsponding ground truth.\nSimulation HSI Reconstruction\nWe compare DWMT with other methods in terms of PSNR,\nSSIM, Params, and FLOPs, including two hand-crafted\nprior-based methods (GAP-TV (Yuan 2016) and DeSCI (Liu\net al. 2018)), two CNN-based methods (TSA-Net (Meng,\nMa, and Yuan 2020), HDNet (Hu et al. 2022)), and two\nTransformer-based methods (MST (Cai et al. 2022b), CST\n(Cai et al. 2022a)). The test results on ten simulation scenes\nin KAIST are shown in Table 1. It can be observed that\nour DWMT significantly outperforms the comparative meth-\nods in terms of reconstruction quality across all ten scenes,\ndemonstrating the effectiveness of our method. Particularly,\ncompared with the recent Transformer-based methods (MST\n(Cai et al. 2022b), CST (Cai et al. 2022a)), our method\nachieves an average PSNR improvement of 1.64 dB and 0.70\ndB, as well as SSIM improvement of 0.016 and 0.007.\nAs shown in Figure 6, we present the reconstruction re-\nsults of our DWMT compared with four other end-to-end\nmethods on four (out of 28) spectral channels of a simulated\nHSI in Scene 5. The bottom left portion displays the enlarged\nlocal details. It can be observed that the previous methods\nfail to clearly depict the letters on the cup or some letters\nsuffer from distortions, and some reconstructed images even\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3977\nMethod 1 2 3 4 5\nenhancement - ✓ ✓ ✓ ✓\nCAFM - - - ✓ ✓\nSAH-MSA - - - ✓ -\nDWM-MSA - - ✓ - ✓\nPSNR 32.23 32.34 36.55 36.46 36.82\nSSIM 0.908 0.908 0.963 0.963 0.964\nTable 2: Ablation study on the simulated HSI datasets.\nWindow Size 8×8 16×16 dual-window\nPSNR 36.66 36.40 36.82\nSSIM 0.963 0.962 0.964\nParams 16.79M 12.18M 14.48M\nFLOPs (G) 47.94 45.48 46.71\nTable 3: Comparison of different window sizes.\nMethod APE CPE CAPE\nPSNR 36.51 36.31 36.82\nSSIM 0.963 0.961 0.964\nParams 14.47M 4.63M 14.48M\nFLOPs (G) 46.54 46.71 46.71\nTable 4: Comparison of position embedding methods. APE\nis absolute position embedding, CPE is conditional position\nembedding, and CAPE is con-abs position embedding.\nappear speckles. In contrast, our DWMT produces clearer\nimages to identify the letter shape on the cup that is closer to\nthe ground truth, and it is more effective to recover the fine-\ngrained structural content and detailed information. We have\nalso plotted the spectral density curves corresponding to the\nselected regions outlined in the yellow box on the RGB im-\nage. The correlation between our curves and the ground truth\nvalues is higher than the other methods, demonstrating the\neffectiveness of our DWMT in spectral consistency.\nReal HSI Reconstruction\nFollowing the previous works (Meng, Ma, and Yuan 2020;\nCai et al. 2022a,b), we retrain our model on all scenes of the\nCA VE and KAIST datasets, and then conduct the restora-\ntion experiments on the real HSIs captured by the CASSI\nsystem. During training, we add 11-bit shot noise into the\nmeasurements to simulate the real imaging conditions. As\nshown in Figure 1, our DWMT outperforms other methods\nin noise suppression and detail reconstruction, demonstrat-\ning its generalizability and robustness.\nAblation Study\nWe conduct ablation experiments on the simulated HSI\ndatasets (Park et al. 2007; Choi et al. 2017).\nBreak-Down Ablation As shown in Table 2, the absence\nof any module leads to a degradation in the reconstruc-\ntion performance, highlighting the design rationality of our\nDWMT. Among them, the enhancement refers to the sec-\nond branch of the encoder in fine pixel refinement stage and\nCAFM refers to replacing it with channel-wise concatena-\ntion and a 1×1 Conv layer. Particularly, when the DWM-\nMSA module is removed, the PSNR and SSIM decrease by\n4.07 dB and 0.054, respectively, demonstrating the crucial\nrole of our DWM-MSA in the HSI reconstruction.\nSelf-Attention Mechanism Analysis We replace our\nDWM-MSA with SAH-MSA (Cai et al. 2022a), another ad-\nvanced attention mechanism used for HSI reconstruction.\nThe results, as shown in Table 2, indicate that our method\nachieves a PSNR improvement of 0.36 dB compared with\nSAH-MSA. Notably, SAH-MSA has been proven to outper-\nform some classical attention mechanisms such as G-MSA\n(Dosovitskiy et al. 2021) and Swin-MSA (Liu et al. 2021)\nin HSI reconstruction (Cai et al. 2022a). This also demon-\nstrates the effectiveness of our DWM-MSA. Moreover, in\nthe CST (Cai et al. 2022a) evaluation, the PSNR is 36.12\ndB when SAH-MSA is employed in our framework, it in-\ncreases to 36.46 dB, further validating the effectiveness of\nour proposed overall framework.\nWindow Size Analysis We utilize a single-scale window\nfor self-attention calculation to compare with our method\n(double-scale window). As shown in Table 3, when using\nboth 8×8 and 16× 16 windows simultaneously, the recon-\nstruction performance is superior to using only an 8×8 or\n16×16 window alone. This highlights that our method can\nlearn more valuable information from the multi-scale fea-\ntures and achieves superior reconstruction performance.\nPosition Embedding Analysis We compare our proposed\nCAPE with two other methods, APE (Vaswani et al. 2017)\nand CPE (Chu et al. 2023). As shown in Table 4, for CAPE\n(our method), the PSNR is improved by 0.31 dB and 0.51\ndB compared with APE and CPE, respectively. This verifies\nthat our CAPE incorporates the advantages of both them,\neffectively enhancing positional information to improve the\nreconstruction performance.\nConclusion\nIn this paper, we address the limitations of existing Trans-\nformers for HSI reconstruction in capturing long-range de-\npendencies and local representation with reasonable com-\nputational cost. To overcome these issues, we propose an\neffective Transformer-based algorithm (DWMT) for HSI re-\nconstruction. In our DWMT, we propose a novel U-Net ar-\nchitecture with a dual-branch encoder to enhance the extrac-\ntion of fine-grained image features. The most crucial part of\nDWMT is the self-attention calculation, i.e., DWM-MSA,\nallowing to capture the long-range dependencies in a local\nregion at different scales. Additionally, we introduce a novel\nposition embedding method in Transformer, CAPE, which\ncombines the advantages of absolute position embedding\nand conditional position embedding, enhancing the posi-\ntional information of image. Through comprehensive quan-\ntitative and qualitative evaluations against the state-of-the-\nart algorithms, we demonstrate the effectiveness and gener-\nalizability of our proposed method. Furthermore, the abla-\ntion experiments indicate that each module in our method\nsignificantly improves the reconstruction performance.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3978\nAcknowledgments\nThis work was supported by the National Natural Sci-\nence Foundation of China under Grant 62071340, 62371076\nand 62201109, and the Natural Science Foundation of\nChongqing under Grant CSTB2022NSCQ-MSX0452.\nReferences\nBhojanapalli, S.; Chakrabarti, A.; Glasner, D.; Li, D.; Un-\nterthiner, T.; and Veit, A. 2021. Understanding robustness\nof transformers for image classification. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 10231–10241.\nCai, Y .; Lin, J.; Hu, X.; Wang, H.; Yuan, X.; Zhang, Y .;\nTimofte, R.; and Van Gool, L. 2022a. Coarse-to-fine sparse\ntransformer for hyperspectral image reconstruction. In Eu-\nropean Conference on Computer Vision (ECCV), 686–704.\nCai, Y .; Lin, J.; Hu, X.; Wang, H.; Yuan, X.; Zhang, Y .; Tim-\nofte, R.; and Van Gool, L. 2022b. Mask-guided spectral-\nwise transformer for efficient hyperspectral image recon-\nstruction. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 17502–\n17511.\nCai, Y .; Lin, J.; Wang, H.; Yuan, X.; Ding, H.; Zhang, Y .;\nTimofte, R.; and Gool, L. V . 2022c. Degradation-aware\nunfolding half-shuffle transformer for spectral compressive\nimaging. In Advances in Neural Information Processing\nSystems (NeurIPS), 37749–37761.\nCao, H.; Wang, Y .; Chen, J.; Jiang, D.; Zhang, X.; Tian, Q.;\nand Wang, M. 2022. Swin-unet: Unet-like pure transformer\nfor medical image segmentation. In European Conference\non Computer Vision (ECCV), 205–218.\nChen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and\nYuille, A. L. 2017. Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully\nconnected crfs. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (TPAMI), 40(4): 834–848.\nChen, L.-C.; Zhu, Y .; Papandreou, G.; Schroff, F.; and\nAdam, H. 2018. Encoder-decoder with atrous separable con-\nvolution for semantic image segmentation. In Proceedings\nof the European Conference on Computer Vision (ECCV),\n801–818.\nChoi, I.; Kim, M.; Gutierrez, D.; Jeon, D.; and Nam, G.\n2017. High-quality hyperspectral reconstruction using a\nspectral prior. ACM Transactions on Graphics (SIGGRAPH\nAsia), 36(6): 218:1–218:13.\nChu, X.; Tian, Z.; Zhang, B.; Wang, X.; and Shen, C.\n2023. Conditinal positional encodings for vision Transform-\ners: Hierarchical vision transformer using shifted windows.\nIn International Conference on Learning Representations\n(ICLR), 1–19.\nDai, X.; Chen, Y .; Xiao, B.; Chen, D.; Liu, M.; Yuan, L.;\nand Zhang, L. 2021a. Dynamic head: Unifying object detec-\ntion heads with attentions. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 7373–7382.\nDai, Z.; Cai, B.; Lin, Y .; and Chen, J. 2021b. Up-detr: Unsu-\npervised pre-training for object detection with transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 1601–1610.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2021. An image is worth 16x16\nwords: Transformers for image recognition at scale. InInter-\nnational Conference on Learning Representations (ICLR),\n1–22.\nGao, P.; Zheng, M.; Wang, X.; Dai, J.; and Li, H. 2021. Fast\nconvergence of detr with spatially modulated co-attention.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 3621–3630.\nGehm, M. E.; John, R.; Brady, D. J.; Willett, R. M.; and\nSchulz, T. J. 2007. Single-shot compressive spectral imaging\nwith a dual-disperser architecture. Optics Express, 15(21):\n14013–14027.\nGuo, T.; Wang, R.; Luo, F.; Gong, X.; Zhang, L.; and Gao,\nX. 2023. Dual-View Spectral and Global Spatial Feature Fu-\nsion Network for Hyperspectral Image Classification. IEEE\nTransactions on Geoscience and Remote Sensing (TGRS),\n61: 5512913.\nHu, X.; Cai, Y .; Lin, J.; Wang, H.; Yuan, X.; Zhang, Y .; Tim-\nofte, R.; and Van Gool, L. 2022. Hdnet: High-resolution\ndual-domain learning for spectral compressive imaging. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 17542–17551.\nHuang, H.; Lin, L.; Tong, R.; Hu, H.; Zhang, Q.; Iwamoto,\nY .; Han, X.; Chen, Y .-W.; and Wu, J. 2020. Unet 3+: A\nfull-scale connected unet for medical image segmentation.\nIn ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 1055–\n1059.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for\nstochastic optimization. In International Conference on\nLearning Representations (ICLR), 1–15.\nLanchantin, J.; Wang, T.; Ordonez, V .; and Qi, Y . 2021. Gen-\neral multi-label image classification with transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 16478–16488.\nLiu, Y .; Yuan, X.; Suo, J.; Brady, D. J.; and Dai, Q.\n2018. Rank minimization for snapshot compressive imag-\ning. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI), 41(12): 2990–3006.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 10012–10022.\nLuo, F.; Zhou, T.; Liu, J.; Guo, T.; Gong, X.; and Ren, J.\n2023. Multiscale diff-changed feature fusion network for\nhyperspectral image change detection. IEEE Transactions\non Geoscience and Remote Sensing (TGRS), 61: 5502713.\nMeng, Z.; Ma, J.; and Yuan, X. 2020. End-to-end low\ncost compressive spectral imaging with spatial-spectral self-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3979\nattention. In European conference on computer vision\n(ECCV), 187–204.\nMeng, Z.; Qiao, M.; Ma, J.; Yu, Z.; Xu, K.; and Yuan, X.\n2020. Snapshot multispectral endomicroscopy. Optics Let-\nters, 45(14): 3897–3900.\nMiao, X.; Yuan, X.; Pu, Y .; and Athitsos, V . 2019. l-net:\nReconstruct hyperspectral images from a snapshot measure-\nment. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), 4059–4069.\nPark, J.-I.; Lee, M.-H.; Grossberg, M. D.; and Nayar, S. K.\n2007. Multispectral imaging using multiplexed illumination.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 1–8.\nRen, S.; Zhou, D.; He, S.; Feng, J.; and Wang, X. 2022.\nShunted self-attention via multi-scale token aggregation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 10853–10862.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-\nvolutional networks for biomedical image segmentation. In\nInternational Conference on Medical image computing and\ncomputer-assisted intervention (MICCAI), 234–241.\nSchechner, Y . Y .; and Nayar, S. K. 2002. Generalized mo-\nsaicing: Wide field of view multispectral imaging. IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n(TPAMI), 24(10): 1334–1348.\nShaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-attention\nwith relative position representations. In The North Ameri-\ncan Chapter of the Association for Computational Linguis-\ntics (NAACL), 464–468.\nTu, Z.; Talebi, H.; Zhang, H.; Yang, F.; Milanfar, P.; Bovik,\nA.; and Li, Y . 2022. Maxvit: Multi-axis vision transformer.\nIn European Conference on Computer Vision (ECCV), 459–\n479.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS), 5998–6008.\nWagadarikar, A.; John, R.; Willett, R.; and Brady, D. 2008.\nSingle disperser design for coded aperture snapshot spectral\nimaging. Applied Optics, 47(10): B44–B51.\nWang, W.; Yao, L.; Chen, L.; Lin, B.; Cai, D.; He, X.; and\nLiu, W. 2022. CrossFormer: A versatile vision transformer\nhinging on cross-scale attention. In International Confer-\nence on Learning Representations (ICLR), 1–15.\nWoo, S.; Park, J.; Lee, J.-Y .; and Kweon, I. S. 2018. Cbam:\nConvolutional block attention module. InProceedings of the\nEuropean conference on computer vision (ECCV), 3–19.\nXia, Z.; Pan, X.; Song, S.; Li, L. E.; and Huang, G. 2022.\nVision transformer with deformable attention. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 4794–4803.\nYuan, X. 2016. Generalized alternating projection based to-\ntal variation minimization for compressive sensing. In 2016\nIEEE International Conference on Image Processing (ICIP),\n2539–2543.\nYuan, X.; Brady, D. J.; and Katsaggelos, A. K. 2021. Snap-\nshot compressive imaging: Theory, algorithms, and applica-\ntions. IEEE Signal Processing Magazine, 38(2): 65–88.\nZhang, S.; Wang, L.; Fu, Y .; Zhong, X.; and Huang, H. 2019.\nComputational hyperspectral imaging based on dimension-\ndiscriminative low-rank tensor recovery. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 10183–10192.\nZhang, X.; Zhou, X.; Lin, M.; and Sun, J. 2018. Shufflenet:\nAn extremely efficient convolutional neural network for mo-\nbile devices. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 6848–\n6856.\nZhou, H.; Luo, F.; Zhuang, H.; Weng, Z.; Gong, X.; and Lin,\nZ. 2023. Attention Multi-hop Graph and Multi-scale Convo-\nlutional Fusion Network for Hyperspectral Image Classifica-\ntion. IEEE Transactions on Geoscience and Remote Sensing\n(TGRS), 61: 5508614.\nZhu, F.; Zhu, Y .; Zhang, L.; Wu, C.; Fu, Y .; and Li, M. 2021.\nA unified efficient pyramid transformer for semantic seg-\nmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2667–2677.\nZhu, L.; Wang, X.; Ke, Z.; Zhang, W.; and Lau, R. W. 2023.\nBiFormer: Vision Transformer with Bi-Level Routing At-\ntention. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 10323–\n10333.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3980"
}