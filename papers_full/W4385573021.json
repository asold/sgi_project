{
    "title": "Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer",
    "url": "https://openalex.org/W4385573021",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2947056188",
            "name": "Zhengbao Jiang",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2734668338",
            "name": "Luyu Gao",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2973263376",
            "name": "Zhiruo Wang",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2108645404",
            "name": "Jun Araki",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2136200480",
            "name": "Haibo Ding",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2148123616",
            "name": "Jamie Callan",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A277131583",
            "name": "Graham Neubig",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6863631769",
        "https://openalex.org/W4205456754",
        "https://openalex.org/W1981208470",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W2256784804",
        "https://openalex.org/W4225644300",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W4281251078",
        "https://openalex.org/W2798658104",
        "https://openalex.org/W4225933709",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4324016655",
        "https://openalex.org/W3035324702",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W3128581554",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W2951534261",
        "https://openalex.org/W3185250692",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W3176390686",
        "https://openalex.org/W3099977667",
        "https://openalex.org/W3156836409",
        "https://openalex.org/W3093742767",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3118668786",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3217305727",
        "https://openalex.org/W4385573075",
        "https://openalex.org/W3199493219",
        "https://openalex.org/W4246183800",
        "https://openalex.org/W4225909425",
        "https://openalex.org/W4206121183",
        "https://openalex.org/W4281777585",
        "https://openalex.org/W4303684020",
        "https://openalex.org/W4206555128",
        "https://openalex.org/W3172119680",
        "https://openalex.org/W3184402450",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W3168875417",
        "https://openalex.org/W3021397474"
    ],
    "abstract": "Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents. This is usually done through two separate models, a retriever that encodes the query and finds nearest neighbors, and a reader based on Transformers. These two components are usually modeled separately, which necessitates a cumbersome implementation and is awkward to optimize in an end-to-end fashion. In this paper, we revisit this design and eschew the separate architecture and training in favor of a single Transformer that performs retrieval as attention (RAA), and end-to-end training solely based on supervision from the end QA task. We demonstrate for the first time that an end-to-end trained single Transformer can achieve both competitive retrieval and QA performance on in-domain datasets, matching or even slightly outperforming state-of-the-art dense retrievers and readers. Moreover, end-to-end adaptation of our model significantly boosts its performance on out-of-domain datasets in both supervised and unsupervised settings, making our model a simple and adaptable end-to-end solution for knowledge-intensive tasks.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2336–2349\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nRetrieval as Attention: End-to-end Learning of Retrieval\nand Reading within a Single Transformer\nZhengbao Jiang♡∗, Luyu Gao ♡∗, Jun Araki ♢, Haibo Ding ♢†,\nZhiruo Wang♡, Jamie Callan ♡, Graham Neubig ♡\n♡Language Technologies Institute, Carnegie Mellon University\n♢Bosch Research North America\n{zhengbaj,luyug,zhiruow,callan,gneubig}@cs.cmu.edu\n{jun.araki,haibo.ding}@us.bosch.com\nAbstract\nSystems for knowledge-intensive tasks such\nas open-domain question answering (QA) usu-\nally consist of two stages: efficient retrieval\nof relevant documents from a large corpus\nand detailed reading of the selected documents\nto generate answers. Retrievers and readers\nare usually modeled separately, which neces-\nsitates a cumbersome implementation and is\nhard to train and adapt in an end-to-end fash-\nion. In this paper, we revisit this design and\neschew the separate architecture and training\nin favor of a single Transformer that performs\nRetrieval as Attention (ReAtt), and end-to-end\ntraining solely based on supervision from the\nend QA task. We demonstrate for the first\ntime that a single model trained end-to-end\ncan achieve both competitive retrieval and QA\nperformance, matching or slightly outperform-\ning state-of-the-art separately trained retriev-\ners and readers. Moreover, end-to-end adap-\ntation significantly boosts its performance on\nout-of-domain datasets in both supervised and\nunsupervised settings, making our model a\nsimple and adaptable solution for knowledge-\nintensive tasks. Code and models are available\nat https://github.com/jzbjyb/ReAtt.\n1 Introduction\nKnowledge-intensive tasks such as question an-\nswering (QA), fact checking, and dialogue gener-\nation require models to gather relevant informa-\ntion from potentially enormous knowledge corpora\n(e.g., Wikipedia) and generate answers based on\ngathered evidence. A widely used solution is to first\nretrieve a small number of relevant documents from\nthe corpus with a bi-encoder architecture which en-\ncodes queries and documents independently for\nefficiency purposes, then read the retrieved docu-\nments in a more careful and expansive way with a\ncross-encoder architecture which encodes queries\n∗The first two authors contributed equally.\n†Haibo Ding is now at Amazon.\nand documents jointly (Lee et al., 2019; Guu et al.,\n2020; Lewis et al., 2020; Izacard et al., 2022). The\ndistinction between retrieval and reading leads to\nthe widely adopted paradigm of treating retrievers\nand readers separately. Retrievers and readers are\nusually two separate models with heterogeneous\narchitectures and different training recipes, which\nis cumbersome to train. Even though two models\ncan be combined in an ad-hoc way for downstream\ntasks, it hinders effective end-to-end learning and\nadaptation to new domains.\nThere have been several attempts to connect\nup reader and retriever training (Lee et al., 2019;\nGuu et al., 2020; Lewis et al., 2020; Sachan et al.,\n2021; Lee et al., 2021a; Izacard et al., 2022). How-\never, retrievers in these works are not learned in a\nfully end-to-end way. They require either initial-\nization from existing supervisedly trained dense\nretrievers (Lewis et al., 2020), or expensive un-\nsupervised retrieval pretraining as warm-up (Lee\net al., 2019; Guu et al., 2020; Sachan et al., 2021;\nLee et al., 2021a; Izacard et al., 2022). The re-\nliance on retrieval-specific warm-up and the ad-hoc\ncombination of retrievers and readers makes them\nless of a unified solution and potentially hinders\ntheir domain adaptation ability. With the ultimate\ngoal of facilitating downstream tasks, retriever and\nreader should instead be fused more organically\nand learned in a fully end-to-end way.\nIn this paper, we focus on one of the most im-\nportant knowledge-intensive tasks, open-domain\nQA. We ask the following question: is it possi-\nble to perform both retrieval and reading within\na single Transformer model, and train the model\nin a fully end-to-end fashion to achieve compet-\nitive performance from both perspectives? Such\na single-model end-to-end solution eliminates the\nneed for retrieval-specific annotation and warm-up\nand simplifies retrieval-augmented training, mak-\ning adaptation to new domains easier. Based on the\nanalogy between self-attention which relates dif-\n2336\nferent tokens in a single sequence (Vaswani et al.,\n2017) and the goal of retrieval which is to relate\nqueries with relevant documents, we hypothesize\nthat self-attention could be a natural fit for retrieval,\nand it allows an organic fusion of retriever and\nreader within a single Transformer.\nSpecifically, we start from a encode-decoder T5\n(Raffel et al., 2020) and use it as both retriever\nand reader. We use the first B encoder layers as\nbi-encoder to encode queries and documents inde-\npendently, and the attention score at layer B+ 1\n(denoted as retrieval attention) to compute rele-\nvance scores, as shown in Fig. 1. We found that\ndirectly using self-attention for retrieval underper-\nforms strong retrievers, which we conjecture is\nbecause self-attention pretrained on local context\nis not sufficient to identify relevant information in\nthe large representation space of the whole cor-\npus. To solve this, we propose to compute re-\ntrieval attention between a query and a large num-\nber of documents and adjust the retrieval attention\nacross documents. For each query, we compute\nretrieval attention over both close documents that\npotentially contain positive and hard negative docu-\nments, and documents of other queries in the same\nbatch as random negatives. The retrieval attention\nis adjusted by minimizing its discrepancy from the\ncross-attention between the decoder and encoder\n(denoted as target attention), which is indicative\nof the usefulness of each document in generating\nanswers (Izacard and Grave, 2021a). The resulting\nRetrieval as Attention model (ReAtt) is a single\nT5 trained based on only QA annotations and si-\nmultaneously learns to promote useful documents\nthrough cross-document adjustment.\nWe train ReAtt on Natural Questions dataset\n(NQ) (Kwiatkowski et al., 2019) in a fully end-to-\nend manner. It achieves both competitive retrieval\nand QA performance, matching or slightly out-\nperforming state-of-the-art retriever ColBERT-NQ\n(Khattab et al., 2020) trained with explicit retrieval\nannotations and strong QA model FiD (Izacard and\nGrave, 2021b,a), demonstrating for the first time\nend-to-end training can produce competitive re-\ntriever and reader within a single model. To further\ntest ReAtt’s generalization and end-to-end adapta-\ntion ability, we conduct zero-shot, supervised, and\nunsupervised adaptation experiments on 7 datasets\nfrom the BEIR benchmark (Thakur et al., 2021).\nIn all settings, end-to-end adaptation improves the\nretrieval performance usually by a large margin,\n1312doc11 qry1 ans\ncross-encoderbi-encoder\nself-/cross-attentionretrieval (self-)attentiontarget (cross-)attention\n…\ncontextual representation (CR)CR for retrieval attentionCR for target attention\ndecoder\nFigure 1: Illustration of Retrieval as Attention (ReAtt)\nwith the first B=2 encoder layers as bi-encoder (i.e.,\nretriever) and the rest L-B=2 layers as cross-encoder.\nDuring training, the retrieval attention between a query\nq1 and documents d11,12,13 is adjusted by minimizing\nits discrepancy from the target attention. For simplicity,\nwe use a single arrow to represent attention of a single\nhead between multiple tokens.\nachieving comparable or superior performance to\nstrong retrieval adaptation and pretraining methods.\n2 Retrieval as Attention (ReAtt)\nWith the goal of developing a single Transformer\nthat can perform both retrieval and reading, and\nthe analogy between retrieval and self-attention,\nwe first introduce architecture changes to allow\nretrieval as attention (§ 2.2), then examine how\nwell attention as-is can be directly used to perform\nretrieval (§ 2.3).\n2.1 Formal Definition\nWe first briefly define the task of retrieval and\nquestion answering. As mentioned in the intro-\nduction, queries and documents need to be repre-\nsented independently for efficient retrieval which\nimplies a bi-encoder architecture that has no inter-\naction between queries and documents. Without\nloss of generality, we use Ed = biencoder(d) to\ndenote one or multiple representations generated\nby a bi-encoder based on a document from a cor-\npus d ∈D, and likewise Eq = biencoder(q) to\ndenote query representations. 1 The top-k docu-\nments most relevant to a query are retrieved by\nDret\nq = argtopkd∈Dr(Eq,Ed), where function r\n1Queries and documents can use different bi-encoders but\nwe use one notation for simplicity.\n2337\ncomputes relevance based on query and document\nrepresentations which can be as simple as a dot\nproduct if queries and documents are encoded into\na single vector, and Dret\nq stands for the returned\ndocuments. We consider encoder-decoder-based\ngenerative question answering in this paper, which\njointly represents queries and retrieved documents\nwith the encoder Eq,d = crossencoder(q,d), and\ngenerates the answer a autoregressively with the\ndecoder Pgen(a|q,d) = Pgen(a|Eq,d). To han-\ndle multiple retrieved documents, we follow the\nfusion-in-decoder model (FiD) (Izacard and Grave,\n2021b) which encodes each query-document pair\nindependently and fuse these representations in de-\ncoder through cross-attention Pgen(a|q,Dret\nq ) =\nPgen(a|Eq,d1 ,...,E q,d|Dretq |). Negative log like-\nlihood (NLL) is used in optimization LQA =\n−log Pgen(a|q,Dret\nq ).\n2.2 Leveraging Attention for Retrieval\nNext, we introduce our method that directly uses\nself-attention between queries and documents as\nretrieval scores.\nPutting the Retriever into Transformers As\nillustrated in Fig. 1, we choose T5 (Raffel et al.,\n2020) as our base model, use the first B layers\nof the encoder as the bi-encoder “retriever” by\ndisabling self-attention between queries and docu-\nments, and the remaining L−Blayers as the cross-\nencoder “reader”. We use the self-attention paid\nfrom query tokens to document tokens at theB+1-\nth layer as the retrieval score, which is denoted as\nretrieval attention(green arrows in Fig. 1). It is\ncomputed based on the independent query and doc-\nument contextual representations from the last (B-\nth) layer of the bi-encoder (green blocks in Fig. 1).\nFormally for an H-head Transformer, document\nand query representations are:\nEd = {KB+1,h\nd ∈R|d|×e}H\nh=1,\nEq = {QB+1,h\nq ∈R|q|×e}H\nh=1,\nwhere Kand Qare key and query vectors of the to-\nken sequence used in self-attention, |d|and |q|are\ndocument and query length, and eis the dimension-\nality of each head. The retrieval attention matrix\nfrom query tokens to document before softmax for\none head is computed by:\nAB+1,h\nq,d = QB+1,h\nq ×KB+1,h\nd\nT\n∈R|q|×|d|.\nDirectly using attention for retrieval can not only\nleverage its ability to identify relatedness, it is also\na natural and simple way to achieve both retrieval\nand reading in a single Transformer with minimal\narchitectural changes, which facilitates our final\ngoal of end-to-end learning.\nFrom Token Attention to Document Relevance\nGiven the token-level attention scores AB+1,h\nq,d , the\nrelevance betweenq and d is computed by avg-max\naggregation: choosing the most relevant document\ntoken for each query token (i.e., max) then averag-\ning across query tokens:\nrh(q,d) =avg0\n(\nmax1(AB+1,h\nq,d )\n)\n, (1)\nwhere 1 and 0 refer to the dimension over which the\noperation is applied. This is similar to the MaxSim\nand sum operators used in ColBERT (Khattab and\nZaharia, 2020), with the intuition that a relevant\ndocument should match as many query tokens as\npossible with the best-matching token. The final\nrelevance is a weighted sum over all heads:\nr(q,d) =\nH∑\nh=1\nPhead\nh ·rh(q,d),\nwhere Ph is a learnable weight that sums to one.\nAs explained in the next section, we empirically\nfind only a few attention heads with non-random re-\ntrieval performance, and among them one particular\nhead is significantly better than the others. Given\nthis observation, we introduce a low temperature\nτ to promote this sparsity Phead\nh = exp(wh/τ)∑\nh′exp(w′\nh/τ) ,\nwhich always ends with a single headwith the\ngreat majority of the weight, which is denoted as\nretrieval headh∗. As a result, the learned head\nweights are practically a head selector, a fact that\ncan also be exploited to make test-time retrieval\nmore efficient.\nEnd-to-end Retrieval with Attention To per-\nform retrieval over a corpus, we first generate key\nvectors KB+1,h∗\nd of retrieval head for all document\ntokens offline and index them with FAISS library\n(Johnson et al., 2021). For each query token, we\nissue its vector (QB+1,h∗\nq ) to the index to retrieve\ntop-K′document tokens, which yields a filtered set\nof documents, each of which has at least one token\nretrieved by a query token. We then fetch all tokens\nof filtered documents, compute relevance scores\nfollowing Eq. 1, and return top-Kdocuments with\nthe highest scores rh∗(q,d). This is similar to the\ntwo-stage retrieval in ColBERT (Khattab and Za-\nharia, 2020), and we reuse their successful practice\n2338\nrandom docs needretrieval attnBM25/ReAttindex\nq1q2q3q4\nd11d21d31d41\nd12d22d32d42\nclose docs needretrieval+targetattn d13d23d33d43\nFigure 2: Illustration of approximate attention over the\ncorpus with |Q|=4 queries in a batch and K=3 close\ndocuments per query. We use q1 as an example to illus-\ntrate the required computation, where close documents\nrequire both retrieval and target attention while random\ndocuments only require retrieval attention.\nin index compression and search approximation to\nmake test-time retrieval efficient, which we refer to\nSanthanam et al. (2021) for details.\n2.3 How Good is Attention As-is?\nTo examine this question, we use T5-large and test\nqueries from the Natural Question dataset (NQ),\nretrieve 100 documents with BM25, compute rel-\nevance scores rh(q,d) with half layers (B = 12)\nas bi-encoder, and measure its correlation with\nthe gold binary annotation. We found that among\nH = 24 heads, 4 heads have non-trivial correla-\ntions of 0.137, 0.097, 0.082, and 0.059. We fur-\nther perform end-to-end retrieval over Wikipedia\nusing the best head, achieving top-10 retrieval ac-\ncuracy of 43.5%, inferior to 55.5% of BM25. This\ndemonstrates that there are indeed heads that can\nrelate queries with relevant documents, but they\nare not competitive. We hypothesize that because\nself-attention is usually trained by comparing and\nrelating tokens in a local context (512/1024 tokens)\nit cannot effectively identify relevant tokens in the\nenormous representation space of a corpus with\nmillions of documents. This discrepancy motivates\nus to compute retrieval attention between queries\nand potentially all documents (i.e., attention over\nthe corpus), and adjust attention across documents\nto promote useful ones.\n3 Learning Retrieval as Attention\nWe first approximate attention over the corpus at\ntraining time by sub-sampling a manageable num-\nber of documents for each query containing both\npotentially relevant and random documents (§ 3.1).\nNext, we introduce our end-to-end training objec-\ntive that optimizes a standard QA loss while also\nadding supervision to promote attention over docu-\nments that are useful for the end task (§ 3.2).\n3.1 Approximate Attention over the Corpus\nEncoding the entire corpus and computing atten-\ntion between the query and all documents is very\nexpensive. To make it practical, we propose to sub-\nsample a small set of documents for each query to\napproximate the whole corpus. Inspired by nega-\ntive sampling methods used in dense retriever train-\ning (Karpukhin et al., 2020; Xiong et al., 2021;\nKhattab and Zaharia, 2020), we sub-sample both\n(1) documents close to queries that can be either rel-\nevant or hard negatives, and (2) random documents\nthat are most likely to be easy negatives. This al-\nlows the model to distinguish between relevant and\nhard negative documents, while simultaneously pre-\nventing it from losing its ability to distinguish easy\nnegatives, which form the majority of the corpus.\nIterative Close Document Sub-sampling To\nsample documents close to a query Dclose\nq , we start\nfrom widely used lexical retriever BM25 (Robert-\nson and Zaragoza, 2009) to retrieve K = 100doc-\numents, as shown by the orange blocks in Fig. 2.\nWe set K to a relatively large number to better\napproximate the local region, inspired by Izacard\nand Grave (2021b)’s findings that QA performance\nincreases as more documents are used.\nThis fixed set of close documents can become\noutdated and no longer close to the query anymore\nas the retrieval attention gets better. To provide dy-\nnamic close sub-samples, we re-index the corpus\nand retrieve a new set of K documents using the\ncurrent retrieval attention after each iteration. It is\nsimilar in spirit to the hard negative mining meth-\nods used in Karpukhin et al. (2020); Khattab et al.\n(2020), with a major difference that we do not man-\nually or heuristically annotate documents but in-\nstead learn from the end loss with cross-document\nadjustment, which will be explained in § 3.2.\nIn-batch Random Document Sub-sampling\nWe use close documents of other queries in the\nsame batch as the random documents of the current\nquery Drandom\nq = ∪q′∈Q∧q′̸=qDclose\nq′ where Qcon-\ntains all queries in a batch, as shown by the green\nblocks in Fig. 2, which has the advantage of reusing\ndocument representations across queries. This\nis similar to the in-batch negatives used in DPR\n(Karpukhin et al., 2020) with a major difference\nthat we reuse a token representations (KB+1,h\nd ,1 ≤\nh ≤H) across queries instead of a single-vector\ndocument representation.\n2339\n3.2 Cross-document Adjustment with\nDecoder-to-Encoder Attention Distillation\nGiven the sub-sampled |Q|×Kdocuments Dq =\nDclose\nq ∪Drandom\nq for each query q, we compute the\nretrieval attention-based relevance scores r(q,d)\nand adjust them across multiple documentsd ∈Dq\nonly relying on end task supervision. Since re-\ntrieval is simply a means to achieve the down-\nstream task, documents useful for downstream\ntasks should be promoted by retrieval. Inspired by\nreader-to-retriever distillation (Izacard and Grave,\n2021a; Yang and Seo, 2020), we measure docu-\nment usefulness based on cross-attention between\ndecoder and encoder, and minimize retrieval atten-\ntion’s discrepancy from it through distillation. In\ncontrast to Izacard and Grave (2021a) that learns\ntwo models iteratively and alternatively, we opti-\nmize QA and distillation loss in a single model\nsimultaneously.\nMinimizing KL-divergence Between Retrieval\nand Target Attention Specifically, we denote\ncross-attention before softmax of the first posi-\ntion/token of the last decoder layer as target at-\ntention Ca,q,Dq ∈RH×|Dq|×(|d|+|q|) where a is\nthe answer, |Dq|is the number of sub-sampled\ndocuments to be fused by the decoder (§ 2.1),\nand |d|is document length.2 To aggregate token-\nlevel target attention into document-level distribu-\ntion Ptgt(a,q,Dq) ∈R|Dq|, we first perform soft-\nmax over all tokens in all query-document pairs\n(|Dq|×(|d|+|q|)), sum over tokens of each query-\ndocument pair (|d|+ |q|), then average across mul-\ntiple heads (H):\nPtgt(a,q,Dq) =avg0\n(\nsum2\n(\nsoftmax1,2(Ca,q,Dq )\n))\n.\nGiven relevance scores obtained from retrieval at-\ntention, the final cross-document adjustment loss is\nthe KL-divergence between relevance distribution\nPret and target distribution Ptgt:\nPret(q,Dq) =softmax\n(\nr(q,d1),...,r (q,d|Dq|)\n)\n.\nLcross-doc = KL\n(\nPtgt(a,q,Dq)\n⏐⏐⏐\n⏐⏐⏐Pret(q,Dq)\n)\n,\n(2)\nwhere the overline indicates stop gradient back\npropagation to target distributions. Our final loss\ncombines QA loss and cross-document adjustment\nloss with αas combination weight.\nL= LQA + α·Lcross-doc. (3)\n2We also attempted other variations of target attention and\nfound performances are similar, consistent with observations\nin Izacard and Grave (2021a).\nZero Target Attention for Random Documents\nFor a batch with |Q|queries, we need to com-\npute retrieval attention and target attention between\n|Q|×|Q|× Kquery-document pairs. This is both\ncomputation- and memory-intensive when batch\nsize is large, especially for target attention because\nit requires L−Blayers of joint encoding of query-\ndocument pairs in the cross-encoder. To alleviate\nthis, we make a simple and effective assumption\nthat in-batch random documents are not relevant to\nthe current query thus having zero target attention:\nPtgt(a,q,Drandom\nq ) ∈R|Drandom\nq | ←0. As a result,\nwe only need to run cross-encoder and decoder\nfor K close documents of each query, as shown\nin Fig. 2. In Appendix A we will introduce our\nefficient implementation to make it possible to run\na large batch size over a limited number of GPUs.\n3.3 Domain Adaptation Methods\nOne of the major benefits of a single end-to-end\ntrainable model is that given a new corpus from\na new domain, possibly without retrieval annota-\ntions, we can easily adapt it by end-to-end training.\nThis section describes how we adapt ReAtt under\ndifferent setups.\nWe consider adapting ReAtt with (1) QA super-\nvision, (2) information retrieval (IR) supervision,\nor (3) unsupervised adaptation where we only have\naccess to the document corpus. Although our goal\nis to learn retrieval through downstream tasks in-\nstead of retrieval supervision, being able to con-\nsume retrieval annotations is helpful when retrieval\nsupervision is indeed available. To do so, we con-\nvert retrieval task with annotations in the form of\nquery-document-relevance triples ⟨q,d,l⟩into a\ngenerative task: given a query, the target is to gen-\nerate its relevant document and the corresponding\nrelevance with the following format “relevance: l.\nd”. If a query has multiple relevant documents,\nwe follow Izacard and Grave (2021b) to randomly\nsample one of them. For unsupervised adaptation,\nwith simplicity as our primary goal, we randomly\nchoose one sentence from a document and mask\none entity, which is considered as the “query”, and\nhave our model generate the masked entity as the\n“answer”, similar to salient span masking (SSM)\nused in Guu et al. (2020).\n4 In-domain Experiments\nIn this section, we examine if supervisedly training\nReAtt end-to-end with only QA supervision yields\nboth competitive retrieval and QA performance.\n2340\nDatasets, Baselines, and Metrics We train our\nmodel using the Natural Questions dataset (NQ).\nWe compare retrieval performance with lexical\nmodels BM25 (Robertson and Zaragoza, 2009),\npassage-level dense retrievers DPR, ANCE, coCon-\ndenser, FiD-KD, YONO (with and without retrieval\npretraining) (Karpukhin et al., 2020; Oguz et al.,\n2021; Xiong et al., 2021; Gao and Callan, 2022;\nIzacard and Grave, 2021a; Lee et al., 2021a), and\ntoken/phrase-level dense retrievers DensePhrase,\nColBERT, ColBERT-NQ (Lee et al., 2021b; Khat-\ntab and Zaharia, 2020; Khattab et al., 2020). 3\nAmong them ColBERT-NQ, FiD-KD and YONO\nare the most fair-to-compare baselines because\nof either similar token-level retrieval granularity\n(ColBERT-NQ) or similar end-to-end training set-\ntings (FiD-KD and YONO). We report top-k re-\ntrieval accuracy (R@k), the fraction of queries\nwith at least one retrieved document containing an-\nswers. We compare QA performance with ORQA,\nREALM, RAG, FiD, EMDR2, YONO, UnitedQA,\nand R2-D2 (Lee et al., 2019; Guu et al., 2020;\nLewis et al., 2020; Izacard and Grave, 2021b,a;\nSachan et al., 2021; Lee et al., 2021a; Cheng et al.,\n2021; Fajcik et al., 2021) using exact match (EM),\namong which FiD, EMDR 2, and YONO are the\nmost fair-to-compare baselines because they have\nsimilar model sizes and training settings.\n5 Implementation Details of ReAtt\nReAtt is based on T5-large with B = 12encoder\nlayers as bi-encoder and temperaturesτ = 0.001 to\nselect the best retrieval head. We retrieveK = 100\nclose documents for each query, and use a batch\nsize of |Q|= 64 queries to obtain in-batch ran-\ndom documents. We use α= 8to combine cross-\ndocument adjustment loss with QA loss. We use\nAdamW with a learning rate of 5e-5, 10% steps of\nwarmup, and linear decay. We first warmup cross-\nattention’s ability to distinguish documents by only\nusing the QA loss for 3K steps, then train with\nthe combined losses (Eq. 3) for 4 iterations, where\nthe first iteration uses close documents returned\nby BM25, and the following 3 iterations use close\ndocuments returned by the previous ReAtt model\n(denoted as ReAtt BM25). Each iteration has 8K\nupdate steps and takes ∼1.5 days on a single node\nwith 8 ×A100 GPUs with 80GB memory. Since\nDPR (Karpukhin et al., 2020) achieves stronger\nperformance than BM25, training with close doc-\n3ColBERT is trained on MS MARCO, ColBERT-NQ is on NQ.\nModels R@1 R@5 R@20 R@100 #Params.\nsupervised retrievers\nBM25 23.9 45.9 63.8 78.9 -\nDPR 45.9 68.1 80.0 85.9 220M\nDPRnew 52.5 72.2 81.3 87.3 220M\nDPR-PAQ - 74.2 84.0 89.2 220M\nANCE - - 81.9 87.5 220M\ncoCondenser - 75.8 84.3 89.0 220M\nDensePhrase 51.1 69.9 78.7 - 330M\nColBERT - - 79.1 - 110M\nColBERT-NQ 54.3 75.7 85.6 90.0 110M\nsemi/unsupervised retrievers\nFiD-KD 49.4 73.8 84.3 89.3 220M\nYONOw/o PT - - 72.3 82.2 165M\nYONOw/ PT - 75.3 85.2 90.2 165M\nReAtt DPR 54.6 77.2 86.1 90.7 165M\nReAtt BM25 55.8 77.4 86.0 90.4 165M\nTable 1: Retrieval performance on NQ. PT is retrieval\npretraining. Fair-to-compare baselines are highlighted\nwith background color. Best performance is in bold.\numents returned by DPR can potentially reduce\ntraining time. We experimented with training on\nclose documents from DPR for a single iteration\nwith 16K steps (denoted as ReAtt DPR). Since both\napproaches achieve similar performance (Tab. 1\nand Tab. 2) and ReAtt DPR is cheaper to train, we\nuse it in other experimental settings.\nAt test-time, we save key vectors of all tokens\nin the corpus and use exact index from FAISS (i.e.,\nfaiss.IndexFlatIP) to perform inner-product\nsearch. We retrieve K′= 2048document tokens\nfor each query token and return top-100 documents\nwith the highest aggregated scores (Eq. 1) to gen-\nerate answers. We found compressing index with\nclustering and quantization proposed by Santhanam\net al. (2021) can greatly reduce search latency and\nindex size with a minor retrieval accuracy loss.\n5.1 Overall Results\nWe compare ReAtt with various retrievers and\nreaders in Tab. 1 and Tab. 2. ReAtt achieves\nboth slightly better retrieval performance than the\nstrongest retriever baseline ColBERT-NQ (Khat-\ntab et al., 2020) and comparable QA performance\nthan the strong reader baseline FiD-KD (Izacard\nand Grave, 2021a) on NQ, demonstrating for the\nfirst time that fully end-to-end training using QA\nsupervision can produce both competitive retrieval\nand QA performance. Compared to another single-\nmodel architecture YONO (Lee et al., 2021a),\nReAtt offers better performance without cumber-\nsome pretraining to warm-up retrieval.\n2341\nModels EM #Params.\nORQA (Lee et al., 2019) 33.3 330M\nREALM (Guu et al., 2020) 40.4 330M\nRAG (Lewis et al., 2020) 44.5 220M\nFiD (Izacard and Grave, 2021b) 51.4 990M\nFiD-KD (Izacard and Grave, 2021a) 54.4 990M\nEMDR2 (Sachan et al., 2021) 52.5 440M\nYONOw/o PT (Lee et al., 2021a) 42.4 440M\nYONOw/ PT (Lee et al., 2021a) 53.2 440M\nUnitedQA (Cheng et al., 2021) 54.7 1.870B\nR2-D2 (Fajcik et al., 2021) 55.9 1.290B\nReAtt DPR 54.0 770M\nReAtt BM25 54.7 770M\nTable 2: QA performance on NQ. PT is retrieval pre-\ntraining. Fair-to-compare baselines are highlighted.\nBest performance is in bold.\n5.2 Ablations\nWe perform ablation experiments to understand\nthe contribution of each component. Due to re-\nsource limitations, all ablations are trained with\n2K steps per iteration. We use ReAtt trained with\nB=12 bi-encoder layers, |Q|=16 batch size, and\nα=8 cross-document loss weight as the baseline,\nremove one component or modify one hyperparam-\neter at a time to investigate its effect. As shown\nin Tab. 3, we found: 1. Only using QA loss with-\nout cross-document adjustment (#2) improves re-\ntrieval performance over the original T5 (#3), but\ncross-document adjustment is necessary to achieve\nfurther improvement (#1). 2. Iteratively retrieving\nclose documents with the current model is helpful\n(#5 vs #1). 3. In-batch random documents are ben-\neficial (#4 vs #1), and a larger batch size leads to\nlarger improvements (#8-11). 4. A larger weight on\ncross-document adjustment loss improves retrieval\nperformance but hurts QA performance, with 4∼8\nachieving a good trade-off (#12-15). 5. A small\nnumber of bi-encoder layers (#6) significantly hurts\nretrieval while a large number of layers (#7) signif-\nicantly hurts QA, suggesting choosing equal num-\nbers of layers in bi-encoder and cross-encoder.\n6 Out-of-domain Generalization and\nAdaptation\nIn this section, we examine both zero-shot re-\ntrieval performance on out-of-domain datasets and\nReAtt’s end-to-end adaptability in supervised (QA,\nIR) and unsupervised settings.\n6.1 Datasets, Baselines, and Metrics\nWe choose 7 datasets from BEIR (Thakur et al.,\n2021), a benchmark covering diverse domains\n# Methods R@1 R@5 R@20 R@100 EM\nReAtt baseline withB=12, |Q|=16, α=8\n1 41.9 68.8 82.5 88.9 46.3\nremove one component\n2 - cross-doc loss 21.7 49.0 71.5 83.5 46.0\n3 - QA (=T5) 13.2 33.7 53.6 67.7 3.0\n4 - in-batch 38.1 66.0 80.3 87.6 46.7\n5 - iterative 41.2 68.3 82.0 88.4 45.0\ndifferent #layers in bi-encoderB\n6 B=6 19.1 42.1 62.4 78.1 40.3\n7 B=18 38.2 63.8 79.3 87.4 35.2\ndifferent batch sizesQ\n8 |Q|=4 39.4 66.1 80.7 88.1 45.0\n9 |Q|=8 40.7 67.1 82.1 88.6 45.7\n10 |Q|=32 43.6 69.4 82.8 89.1 46.4\n11 |Q|=64 45.5 71.0 83.3 89.4 47.3\ndifferent cross-doc loss weightsα\n12 α=1 37.4 65.4 80.9 88.0 47.3\n13 α=2 39.7 66.9 81.7 88.4 47.4\n14 α=4 40.9 68.0 82.1 88.8 46.9\n15 α=16 42.0 68.8 82.5 88.8 45.5\nTable 3: Ablations by removing one component or\nchanging one hyperparameter from the ReAtt baseline.\nand tasks. On each dataset we compare ReAtt\nwith different types of retrievers including BM25,\nDPR, and ColBERT. We consider 2 QA datasets\n(BioASQ and FiQA (Tsatsaronis et al., 2015;\nMaia et al., 2018)) and one IR dataset (MS\nMARCO (Nguyen et al., 2016)) to evaluate su-\npervised adaptation capability, and 4 other datasets\n(CQADupStack, TREC-COVID, SCIDOCS, Sci-\nFact (Hoogeveen et al., 2015; V oorhees et al., 2020;\nCohan et al., 2020; Wadden et al., 2020)) to eval-\nuate unsupervised adaptation capability. Detailed\nstatistics are listed in Tab. 8. We report nDCG@10\nto measure retrieval performance and EM to mea-\nsure QA performance. We group all baselines into\nthree categories and denote them with different\ncolors in the following tables:\n• Supervised adaptation models are trained with\ndownstream task supervision, including RAG\ntrained on BioASQ, Contriever fine-tuned on\nFiQA, and docT5query, ANCE, ColBERT, and\nContriever fine-tuned on MS MARCO (Nogueira\nand Lin, 2019; Xiong et al., 2021; Khattab and\nZaharia, 2020; Izacard et al., 2021).\n• Unsupervised adaptation models are trained on\ndomain corpus in an unsupervised way such as\ncontrastive learning or pseudo query generation,\nincluding SimCSE and TSDAE+GPL (Gao et al.,\n2021c; Wang et al., 2021a,b).\n• Pretraining models are trained on corpora with-\n2342\nTasks QA Retrieval\nDatasets BioASQ FiQA MS MARCO\nzero-shot performance\nBM25 68.1 23.6 22.8\nDPR 14.1 11.2 17.7\nColBERT-NQ 65.5 23.8 32.8\nReAtt 71.1 30.1 32.3\nadditional training\nContriever - 32.9 docT5query 33.8\nSimCSE 58.1 31.4 ANCE 38.8\nTSDAE+GPL 61.6 34.4 ColBERT 40.1\nContrieverw/ FT - 38.1 Contriever 40.7\nReAtt +5.876.9 +8.538.6 ReAtt +7.639.9\nTable 4: nDCG@10 of zero-shot and supervised adapta-\ntion experiments on two QA and one IR datasets. We\nuse colors to denote categories: pretraining, unsuper-\nvised adaptation, and supervised adaptation. Baselines\ncomparable to ReAtt are highlighted with blue back-\nground color. We also show the improvement of ReAtt\nover zero-shot performance in subscript.\n# Ablations nDCG@1 @5 EM\n1 RAG 14.6 13.0 1.3\n2 + reader 14.6 13.0 - 27.5 26.2\n3 + qry enc (e2e) 0.0 0.0 -13.0 25.7 -1.9\n4 + doc/qry enc ∗ 29.4 27.1 14.1 5.0 3.7\n5 + reader (pipe) 29.4 27.1 - 27.8 22.8\n6 + qry enc 23.3 23.2 -4.0 26.2 -1.6\n7 T5 49.2 47.7 0.0\n8 + e2e 75.2 73.5 25.7 44.4 44.4\n9 ReAtt 72.8 70.1 17.2\n10 + e2e 77.4 75.4 5.3 47.2 30.0\nTable 5: RAG and ReAtt on BioASQ. Each indent in-\ndicates fine-tuning one more component than its parent\nwith performance difference colored with green/red. ∗\ndenotes fine-tuning conducted sequentially instead of\njointly with the current component.\nout direct exposure to the target domain, such\nas Contriever (Izacard et al., 2021) trained with\ncontrastive learning on Wikipedia and CCNet.\nWe highlight baselines in the same category as\nReAtt in the following tables since comparison be-\ntween them is relatively fair. Details of adaptation\nof ReAtt can be found in Appendix B.\n6.2 Experimental Results\nResults of supervised and unsupervised adaptation\nare listed in Tab. 4, Tab. 5, and Tab. 6 respectively.\nZero-shot Generalization Ability As shown in\nTab. 4 and Tab. 6, the zero-shot performance of\nReAtt is significantly better than other zero-shot\nbaselines on two QA datasets and one fact checking\ndataset (+3.0/+6.5/+4.5 on BioASQ/FiQA/SciFact\nthan the second best), and overall comparable\non the rest of datasets (-0.5/-0.6/-3.0/-1.0 on MS\nMethods CQA. TRECC. SCIDOCS SciFact\nzero-shot performance\nBM25 29.9 65.6 15.8 66.5\nDPR 15.3 33.2 7.7 31.8\nANCE 29.6 65.4 12.2 50.7\nColBERT-NQ 33.9 48.9 15.6 65.3\nReAtt 33.3 62.6 14.8 71.0\nadditional training\nContriever 34.5 59.6 16.5 67.7\nSimCSE 29.0 68.3 - 55.0\nTSDAE+GPL 35.1 74.6 - 68.9\nReAtt +3.336.6 +13.476.0 +1.015.8 +0.271.2\nTable 6: nDCG@10 of zero-shot and unsupervised adap-\ntation on four datasets. Format is similar to Tab. 4\nMARCO/CQA./TRECC./SCIDOCS than the best\nwhich is usually BM25), demonstrating that our\nend-to-end training with QA loss on NQ produces\na robust retriever. We conjecture that the superior\nperformance on QA datasets can be attributed to\nour end-to-end training using QA loss which learns\nretrieval that better aligns with the end task than\ntraining with retrieval annotations.\nRetrieval Adaptation with QA Supervision As\nshown in the left-hand side of Tab. 4, end-to-end\nadaptation with QA supervision significantly im-\nproves ReAtt’s retrieval performance by 5.8/8.5 on\nBioASQ/FiQA, achieving similar performance as\nContriever fine-tuned on FiQA, and better perfor-\nmance than other unsupervised methods, confirm-\ning the end-to-end adaptability of our methods.\nEnd-to-end QA Adaptation We perform end-to-\nend adaptation on BioASQ and compare with RAG\nas a baseline, which combines DPR as retriever and\nBART as reader, and DPR has a query and docu-\nment encoder. Since updating document encoder\nrequires corpus re-indexing, it is fixed during fine-\ntuning. We found end-to-end fine-tuning fails on\nRAG. To understand why, we conduct a rigorous\nexperiment that breaks down each component of\nRAG to find the failure point in Tab. 5.\nStarting from the initial model trained on NQ\n(#1), we first fine-tune the reader while fixing the\nquery encoder (#2), and as expected QA perfor-\nmance improves. However fine-tuning both query\nencoder and reader (end-to-end #3) makes the re-\ntriever collapse with zero relevant documents re-\nturned, indicating end-to-end fine-tuning does not\nwork for RAG on new domains. In order to im-\nprove both retrieval and QA, we need to fine-tune\nRAG in a pipeline manner: first fine-tune the re-\n2343\ntriever (both query and doc encoder) similarly to\nDPR using retrieval annotations (#4), then fine-\ntune the reader (#5). With the DPR-like fine-tuned\nretriever, end-to-end fine-tuning of query encoder\nand reader still fails (#6), although the retriever\ndoes not completely collapse.\nEnd-to-end fine-tuning of ReAtt improves re-\ntrieval and QA simultaneously. Fine-tuning start-\ning from ReAtt trained on NQ is better than starting\nfrom T5, indicating the capability learned in NQ\ncould be transferred to BioASQ. Comparing RAG\nand ReAtt, we identify several keys that enable end-\nto-end adaptation. (1) ReAtt relying on token-level\nattention has a strong initial performance, (2) cross-\ndocument adjustment over both close and random\ndocuments in ReAtt provides a better gradient es-\ntimation than only using retrieved documents in\nRAG, (3) distillation-based loss in ReAtt might be\nmore effective than multiplying the retrieval proba-\nbility into the final generation probability.\nLeveraging Retrieval Annotations As shown\non the right-hand side of Tab. 4, ReAtt is able to\nconsume retrieval supervision in a generative for-\nmat and achieve competitive performance as other\nsupervised dense retrievers.\nUnsupervised Adaptation with SSM As shown\nin Tab. 6, adaptation by simply masking salient\nentities from sentences as input and generating\nmasked entities using ReAtt improves the retrieval\nperformance on 4 datasets, some by a large mar-\ngin, achieving comparable or superior performance\nthan strong retrieval adaptation methods such as\nTSDAE+GPL that relies on query generation. This\nindicates that our end-to-end trainable model also\nworks well in unsupervised settings without involv-\ning too many engineering heuristics.\n7 Related Work\nRetrieval-augmented question answering utilizes\nevidence retrieved from an external knowledge\nsource to facilitate question answering. There have\nbeen several attempts to learn retrievers and readers\njointly. ORQA, REALM, RAG, EMDR2, YONO,\nand Atlas (Lee et al., 2019; Guu et al., 2020; Sachan\net al., 2021; Lee et al., 2021a; Izacard et al., 2022)\nfirst warm-up retrievers using unsupervised pre-\ntraining methods such as inverse cloze task (ICT),\nsalient span masking (SSM), and large-scale con-\ntrastive learning, or initialize from supervised re-\ntrievers, then fine-tune both retriever and reader on\ndownstream tasks. They either use fixed index (Lee\net al., 2019; Lewis et al., 2020) or asynchronously\nupdate the index during training (Guu et al., 2020;\nSachan et al., 2021; Lee et al., 2021a; Izacard et al.,\n2022). Recently, retrieval-augmented models are\nscaled up to very large corpora such as the web\n(Piktus et al., 2021; Borgeaud et al., 2021), making\nthem capable of handling information out of the\nscope of Wikipedia. Atlas (Izacard et al., 2022)\nscales up retrieval-augmented models with T5-11B\nas the reader and Contriever (Izacard et al., 2021)\nas the retriever and achieves strong few-shot per-\nformance on multiple benchmarks. Detailed com-\nparisons of these models can be found in Tab. 7.\nMore related works on dense retrieval, unsuper-\nvised retrieval learning, and retrieval augmentation\nfor language modeling can be found in Appendix C.\n8 Conclusion\nWe propose retrieval as attention (ReAtt), a single\nTransformer model that can be learned in an end-\nto-end fashion only using end task loss. We demon-\nstrated on NQ dataset that ReAtt can achieve both\ncompetitive retrieval and QA performance. We\nfurther show that ReAtt is easy to adapt to other\ndomains in both supervised and unsupervised set-\ntings, achieving both boosted retrieval and end task\nperformance. Future directions include better end-\nto-end training objectives, efficient training and in-\nference, and transferring our solution to large-scale\npretraining.\nLimitations\nReAtt is based on token-level representations, and\nbelongs to the same category as token-level dense\nretrievers such as ColBERT (Khattab and Zaharia,\n2020). Comparing to passage-level dense retrievers\nsuch as DPR (Karpukhin et al., 2020), token-level\nretrievers usually offer better performance (shown\nin Tab. 1, Tab. 4, and Tab. 6) but require more\nspace to store the index and longer query time.\nOur methods have the same limitation. We found\nColBERT’s practice in index compression and ap-\nproximate search (Khattab and Zaharia, 2020; San-\nthanam et al., 2021, 2022) also works for our model,\nmaking this issue less of a concern.\nAcknowledgments\nThis work was supported by a gift from Bosch Re-\nsearch. We would like to thank Chunting Zhou, Uri\nAlon, Omar Khattab, and Patrick Lewis for their\ninsightful feedback and help with experiments.\n2344\nReferences\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2021. Improving language models by retrieving from\ntrillions of tokens. CoRR, abs/2112.04426.\nHao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He,\nWeizhu Chen, and Jianfeng Gao. 2021. Unitedqa: A\nhybrid approach for open domain question answer-\ning. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n3080–3090. Association for Computational Linguis-\ntics.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel S. Weld. 2020. SPECTER:\ndocument-level representation learning using citation-\ninformed transformers. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2020, Online, July 5-10, 2020,\npages 2270–2282. Association for Computational\nLinguistics.\nMartin Fajcik, Martin Docekal, Karel Ondrej, and Pavel\nSmrz. 2021. R2-D2: A modular baseline for open-\ndomain question answering. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2021,\nVirtual Event / Punta Cana, Dominican Republic, 16-\n20 November, 2021, pages 854–870. Association for\nComputational Linguistics.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-\n11 November, 2021, pages 981–993. Association for\nComputational Linguistics.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 2843–2853. Associ-\nation for Computational Linguistics.\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021a.\nCOIL: revisit exact lexical match in information re-\ntrieval with contextualized inverted list. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL-\nHLT 2021, Online, June 6-11, 2021, pages 3030–\n3042. Association for Computational Linguistics.\nLuyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan.\n2021b. Scaling deep contrastive learning batch size\nunder memory limited setup. In Proceedings of the\n6th Workshop on Representation Learning for NLP\n(RepL4NLP-2021), pages 316–321, Online. Associa-\ntion for Computational Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021c.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021, pages 6894–\n6910. Association for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. CoRR,\nabs/2002.08909.\nDoris Hoogeveen, Karin M. Verspoor, and Timothy\nBaldwin. 2015. Cqadupstack: A benchmark data\nset for community question-answering research. In\nProceedings of the 20th Australasian Document Com-\nputing Symposium, ADCS 2015, Parramatta, NSW,\nAustralia, December 8-9, 2015, pages 3:1–3:8. ACM.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Towards unsupervised\ndense information retrieval with contrastive learning.\nCoRR, abs/2112.09118.\nGautier Izacard and Edouard Grave. 2021a. Distilling\nknowledge from reader to retriever for question an-\nswering. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\nEACL 2021, Online, April 19 - 23, 2021, pages 874–\n880. Association for Computational Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Few-shot learning with retrieval aug-\nmented language models. CoRR, abs/2208.03299.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.\nBillion-scale similarity search with gpus. IEEE\nTrans. Big Data, 7(3):535–547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 6769–6781. Associa-\ntion for Computational Linguistics.\n2345\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2020. Relevance-guided supervision for openqa with\ncolbert. CoRR, abs/2007.00814.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Ef-\nficient and effective passage search via contextual-\nized late interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval,\nSIGIR 2020, Virtual Event, China, July 25-30, 2020,\npages 39–48. ACM.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Trans. Assoc. Comput. Linguistics, 7:452–\n466.\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\njape, Christopher D. Manning, and Kyoung-Gu Woo.\n2021a. You only need one model for open-domain\nquestion answering. CoRR, abs/2112.07381.\nJinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b.\nPhrase retrieval learns passage retrieval, too. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2021,\nVirtual Event / Punta Cana, Dominican Republic, 7-\n11 November, 2021, pages 3661–3672. Association\nfor Computational Linguistics.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open do-\nmain question answering. In Proceedings of the 57th\nConference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, pages 6086–6096.\nAssociation for Computational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nJi Ma, Ivan Korotkov, Yinfei Yang, Keith B. Hall, and\nRyan T. McDonald. 2021. Zero-shot neural passage\nretrieval via domain-targeted synthetic question gen-\neration. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, EACL 2021, Online,\nApril 19 - 23, 2021, pages 1075–1088. Association\nfor Computational Linguistics.\nMacedo Maia, Siegfried Handschuh, André Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. Www’18 open challenge:\nFinancial opinion mining and question answering. In\nCompanion of the The Web Conference 2018 on The\nWeb Conference 2018, WWW 2018, Lyon , France,\nApril 23-27, 2018, pages 1941–1942. ACM.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In Proceedings of\nthe Workshop on Cognitive Computation: Integrat-\ning neural and symbolic approaches 2016 co-located\nwith the 30th Annual Conference on Neural Infor-\nmation Processing Systems (NIPS 2016), Barcelona,\nSpain, December 9, 2016, volume 1773 of CEUR\nWorkshop Proceedings. CEUR-WS.org.\nRodrigo Nogueira and Jimmy Lin. 2019. From\ndoc2query to doctttttquery.\nBarlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick\nS. H. Lewis, Vladimir Karpukhin, Aleksandra Pik-\ntus, Xilun Chen, Sebastian Riedel, Wen-tau Yih,\nSonal Gupta, and Yashar Mehdad. 2021. Domain-\nmatched pre-training tasks for dense retrieval. CoRR,\nabs/2107.13602.\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\nPatrick Lewis, Barlas Oguz, Edouard Grave, Wen-\ntau Yih, and Sebastian Riedel. 2021. The web is\nyour oyster - knowledge-intensive NLP against a very\nlarge web corpus. CoRR, abs/2112.09924.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and\nHaifeng Wang. 2021. Rocketqa: An optimized train-\ning approach to dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, NAACL-HLT 2021, On-\nline, June 6-11, 2021, pages 5835–5847. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nStephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333–389.\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances\nin Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing\n2346\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 25968–25981.\nKeshav Santhanam, Omar Khattab, Christopher Potts,\nand Matei Zaharia. 2022. PLAID: an efficient engine\nfor late interaction retrieval. CoRR, abs/2205.09707.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2021. Col-\nbertv2: Effective and efficient retrieval via\nlightweight late interaction. CoRR, abs/2112.01488.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Proceedings of\nthe Neural Information Processing Systems Track on\nDatasets and Benchmarks 1, NeurIPS Datasets and\nBenchmarks 2021, December 2021, virtual.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R. Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, Yannis Almirantis, John Pavlopoulos, Nico-\nlas Baskiotis, Patrick Gallinari, Thierry Artières,\nAxel-Cyrille Ngonga Ngomo, Norman Heino, Éric\nGaussier, Liliana Barrio-Alvers, Michael Schroeder,\nIon Androutsopoulos, and Georgios Paliouras. 2015.\nAn overview of the BIOASQ large-scale biomedical\nsemantic indexing and question answering competi-\ntion. BMC Bioinform., 16:138:1–138:28.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nEllen M. V oorhees, Tasmeer Alam, Steven Bedrick,\nDina Demner-Fushman, William R. Hersh, Kyle Lo,\nKirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2020.\nTREC-COVID: constructing a pandemic information\nretrieval test collection. SIGIR Forum, 54(1):1:1–\n1:12.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 7534–7550. Association for Computa-\ntional Linguistics.\nKexin Wang, Nils Reimers, and Iryna Gurevych. 2021a.\nTSDAE: using transformer-based sequential denois-\ning auto-encoderfor unsupervised sentence embed-\nding learning. In Findings of the Association for\nComputational Linguistics: EMNLP 2021, Virtual\nEvent / Punta Cana, Dominican Republic, 16-20\nNovember, 2021, pages 671–688. Association for\nComputational Linguistics.\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2021b. GPL: generative pseudo labeling\nfor unsupervised domain adaptation of dense retrieval.\nCoRR, abs/2112.07577.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,\nand Christian Szegedy. 2022. Memorizing transform-\ners. In The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022. OpenReview.net.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nSohee Yang and Minjoon Seo. 2020. Is retriever merely\nan approximator of reader? CoRR, abs/2010.10999.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation.\nCoRR, abs/2205.12674.\n2347\nModel Architecture Retriever training Granu.Retriever Reader Single Init. Warm-up End-to-end loss\nORQA (Lee et al., 2019) BERT BERT ✗ BERT ICT Prob. marginalization Passage\nREALM (Guu et al., 2020) BERT BERT ✗ BERT ICT, SSM Prob. marginalization Passage\nRAG (Lewis et al., 2020) BERT BART ✗ DPR - Prob. marginalization Passage\nEMDR2 (Sachan et al., 2021) BERT T5 ✗ BERT ICT, SSM Expectation maximization Passage\nYONO (Lee et al., 2021a) T5 T5 ✓ T5 SSM Attention distillation Passage\nAtlas (Izacard et al., 2022) BERT T5 ✗ Contriever MLM Perplexity distillation Passage\nReAtt T5 T5 ✓ T5 - Attention distillation Token\nTable 7: Detailed comparison between end-to-end retriever-reader models. ICT is inverse cloze task, SSM is salient\nspan masking, and MLM is masked language modeling. Granu. is retrieval granularity.\nDataset Domain Task Train Test\n#Queries #Annotations #Queries |Corpus|\nIn-domain\nNQ Wiki Question answering 79K 133K 3,610 21.015M\nOut-of-domain supervised adaptation\nBioASQ Biomed Question answering 3K 32K 500 1.000M\nFiQA Finance Question answering 6K 14K 648 58K\nMS MARCO Misc Information retrieval 503K 533K 6,980 8.842M\nOut-of-domain unsupervised adaptation\nCQADupStack StackExchange Duplicate question retrieval - - 13,145 457K\nTREC-COVID Biomed Information retrieval - - 50 171K\nSCIDOCS Science Citation prediction - - 1,000 26K\nSciFact Science Fact checking 1K 1K 300 5K\nTable 8: Statistics of 8 datasets categorized by experimental settings, including the number of training/test queries,\nretrieval annotations (query-document pairs), and documents in the corpus.\nA Efficient Implementation\nUnder typical optimization setups where the loss is\npoint-wise with respect to each training data, like\ntraining classifiers or readers, scaling batch size\ncan be easily achieved with gradient accumulation.\nHowever, due to the use of in-batch negatives, our\nsystems, like others (Karpukhin et al., 2020; Qu\net al., 2021), require having all examples in a batch\nto reside in GPUs simultaneously when trained di-\nrectly. Larger batches therefore need proportionally\nmore GPU memory.\nIn order to accommodate large batches with our\nlimited memory hardware, we adopt the gradient\ncache approach (Gao et al., 2021b) decouple in-\nstances in the same batch. In particular, we run\nan extra forward pass over the large batch in infer-\nence mode and record (1) representations for all\nquery and document tokens (QB+1,h\nq and KB+1,h\nd )\nand (2) decoder-encoder target attention values\n(Ca,q,Dq ). Note that we do notstore model internal\nactivation nor perform gradient computation with\nrespect to model parameters in this step. With (1)\nwe can compute the retrieval attention, and with (2)\nwe can compute cross-document adjustment loss\n(Eq. 2). We then compute and cache gradient vec-\ntors of all query and document vectors with respect\nto Eq. 2. We finally optimize the model with a\nsufficiently small batch size to fit in GPU memory\nand use cached gradient in the backward pass of\nthe Eq. 2.\nB Details of Adaptation Experiments\nFor supervised adaptation, we train on BioASQ,\nFiQA, and MS MARCO separately using all train-\ning queries. For CQADupStack, we merge the\ndocument corpora of 12 sub-domains into a sin-\ngle corpus to sample masked sentences for salient\nspan masking training. For each of the 4 unsuper-\nvised domain adaptation datasets (CQADupStack,\nTREC-COVID, SCIDOCS, SciFact), we sample\n20∼100K sentences and mask one entity, which is\napproximately proportional to the size of the corpus\nwith a larger sampling rate for small corpora. We\nreuse the same hyperparameters as NQ (§ 5), ex-\ncept that we train each model for a single iteration\nusing close documents from BM25 with 4K update\nsteps and a batch size of 16. Since MS MARCO\nhas a large number of annotations, we train for 12K\nupdate steps.\n2348\nC Related Work\nDense Retrieval Models Dense retrieval mod-\nels can be categorized into two groups, passage-\nlevel retrievers (Karpukhin et al., 2020; Oguz et al.,\n2021; Xiong et al., 2021; Gao and Callan, 2022)\nand token/phrase-level retrievers (Khattab and Za-\nharia, 2020; Khattab et al., 2020; Gao et al., 2021a;\nLee et al., 2021b). Passage-level retrievers en-\ncode queries and documents into a single vec-\ntor, while token/phrase-level retrievers directly use\ntoken/phrase representations, resulting in multi-\nvector representations. Passage-level retrievers\nare usually more efficient but less expressive than\ntoken-level retrievers.\nUnsupervised Retrieval Learning Unsuper-\nvised retrieval learning methods can be categorized\ninto two types: pretraining-based (Lee et al., 2019;\nGao et al., 2021c; Wang et al., 2021a; Gao and\nCallan, 2021; Izacard et al., 2021) and question\ngeneration-based (Ma et al., 2021; Wang et al.,\n2021b). SimCSE (Gao et al., 2021c) obtains rep-\nresentations of the same input by passing through\nthe model twice with different dropout masks and\nminimizes their distance. Contriever (Izacard et al.,\n2021) is trained by large-scale contrastive learn-\ning with random cropping of text spans sampled\nfrom Wikipedia and CCNet. GPL (Wang et al.,\n2021b) leverages query generators to obtain pseudo\nqueries, and collect positive and negative docu-\nments by pseudo labeling using a cross-encoder.\nRetrieval Augmentation for Language Modeling\nRetrieval from external datastore to improve lan-\nguage modeling perplexity has been explored by\nmany works, where additional tokens are retrieved\nduring generation based on contextual represen-\ntations (Khandelwal et al., 2020; Borgeaud et al.,\n2021; Wu et al., 2022; Zhong et al., 2022). They\ndiffer in whether retrieval is fixed or learnable, re-\ntrieval frequency, and contextual representations\nused to perform nearest neighbors search.\n2349"
}