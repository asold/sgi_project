{
  "title": "MMTN: Multi-Modal Memory Transformer Network for Image-Report Consistent Medical Report Generation",
  "url": "https://openalex.org/W4382240685",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1980103316",
      "name": "Yiming Cao",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2155397016",
      "name": "Li-Zhen Cui",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2082114532",
      "name": "Lei Zhang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2133664224",
      "name": "Fuqiang Yu",
      "affiliations": [
        "Shangdong Agriculture and Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2086285432",
      "name": "Zhen Li",
      "affiliations": [
        "Qilu Hospital of Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2102627060",
      "name": "Xu Yonghui",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A1980103316",
      "name": "Yiming Cao",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2155397016",
      "name": "Li-Zhen Cui",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2082114532",
      "name": "Lei Zhang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2133664224",
      "name": "Fuqiang Yu",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2086285432",
      "name": "Zhen Li",
      "affiliations": [
        "Qilu Hospital of Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2102627060",
      "name": "Xu Yonghui",
      "affiliations": [
        "Shandong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4225926446",
    "https://openalex.org/W3173688449",
    "https://openalex.org/W3096799362",
    "https://openalex.org/W6771429369",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W956551720",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W6784955093",
    "https://openalex.org/W2913279579",
    "https://openalex.org/W2903721568",
    "https://openalex.org/W2964195337",
    "https://openalex.org/W6755002340",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3168117093",
    "https://openalex.org/W3166142651",
    "https://openalex.org/W3212101459",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W8316075",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2560313346",
    "https://openalex.org/W2334763311",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W6639657675",
    "https://openalex.org/W2890888035",
    "https://openalex.org/W3204326462",
    "https://openalex.org/W6698228248",
    "https://openalex.org/W2963282262",
    "https://openalex.org/W2985703188",
    "https://openalex.org/W3167939936",
    "https://openalex.org/W2997704374",
    "https://openalex.org/W3204703315",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W3177048142",
    "https://openalex.org/W4287120153",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2803411968",
    "https://openalex.org/W2892220259",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W3098325931",
    "https://openalex.org/W2979861699",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W4285531589",
    "https://openalex.org/W4304206659"
  ],
  "abstract": "Automatic medical report generation is an essential task in applying artificial intelligence to the medical domain, which can lighten the workloads of doctors and promote clinical automation. The state-of-the-art approaches employ Transformer-based encoder-decoder architectures to generate reports for medical images. However, they do not fully explore the relationships between multi-modal medical data, and generate inaccurate and inconsistent reports. To address these issues, this paper proposes a Multi-modal Memory Transformer Network (MMTN) to cope with multi-modal medical data for generating image-report consistent medical reports. On the one hand, MMTN reduces the occurrence of image-report inconsistencies by designing a unique encoder to associate and memorize the relationship between medical images and medical terminologies. On the other hand, MMTN utilizes the cross-modal complementarity of the medical vision and language for the word prediction, which further enhances the accuracy of generating medical reports. Extensive experiments on three real datasets show that MMTN achieves significant effectiveness over state-of-the-art approaches on both automatic metrics and human evaluation.",
  "full_text": "MMTN: Multi-Modal Memory Transformer Network for Image-Report\nConsistent Medical Report Generation\nYiming Cao1,2, Lizhen Cui1,2*, Lei Zhang1,2, Fuqiang Yu1,2, Zhen Li3, Yonghui Xu2∗\n1School of Software, Shandong University, Jinan, China\n2Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University, Jinan, China\n3Department of Gastroenterology, Qilu Hospital of Shandong University, Jinan, China\n{caoyiming, leizh, yfq}@mail.sdu.edu.cn, {clz,qilulizhen}@sdu.edu.cn, xu.yonghui@hotmail.com\nAbstract\nAutomatic medical report generation is an essential task\nin applying artificial intelligence to the medical domain,\nwhich can lighten the workloads of doctors and promote\nclinical automation. The state-of-the-art approaches employ\nTransformer-based encoder-decoder architectures to gener-\nate reports for medical images. However, they do not fully\nexplore the relationships between multi-modal medical data,\nand generate inaccurate and inconsistent reports. To address\nthese issues, this paper proposes a Multi-modal Memory\nTransformer Network (MMTN) to cope with multi-modal\nmedical data for generating image-report consistent medical\nreports. On the one hand, MMTN reduces the occurrence of\nimage-report inconsistencies by designing a unique encoder\nto associate and memorize the relationship between medi-\ncal images and medical terminologies. On the other hand,\nMMTN utilizes the cross-modal complementarity of the med-\nical vision and language for the word prediction, which fur-\nther enhances the accuracy of generating medical reports. Ex-\ntensive experiments on three real datasets show that MMTN\nachieves significant effectiveness over state-of-the-art ap-\nproaches on both automatic metrics and human evaluation.\nIntroduction\nMedical image reports utilize free text to describe and ex-\nplain the medical observations in images, which are mainly\nwritten by doctors based on their medical knowledge and\nexperience. To alleviate the heavy workload of doctors, au-\ntomatic report generation has become a critical task.\nThe state-of-the-art works in medical report generation\ntask adopt the encoder-decoder architecture (Zhang et al.\n2020; Liu et al. 2021a) to automatically generate reports for\nmedical images. Although these works can generate textual\nnarratives for medical images, they are still limited in fully\nexploiting the information from medical multi-modal data,\nsuch as the consistent mapping bewteen medical images and\nreports and the utilization of important medical terminology\nknowledge, which is demonstrated in Figure 1. Therefore,\nthere are some issues that need to be further explored:\n1) The relationships between multi-modal medical data are\nnot fully explored. Some works (Chen et al. 2020, 2021)\n*Corresponding Authors\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFindings：\nA Is polyp was seen in the transverse colon, \nabout 2*2 mm in size, with smooth surface \nmucosa, the same color as the surrounding \nmucosa, and no echinoderm -like changes at \nthe base. The remaining transverse colon \nmucosa was smooth, with a clear submucosal \nvascular texture, with regular peristalsis.\nFigure 1: An example of gastroenterology report, where\naligned image and report are marked in different colors and\nmedical terminology knowledge are underlined in red.\nonly leverage two types of data (i.e.,images and text) to gen-\nerate reports, ignoring essential medical knowledge. Some\nworks introduce medical knowledge (e.g., disease tags (Li\net al. 2019) and regions (Liu et al. 2021a)) to guide the re-\nport generation, without exploring the correlations between\nknowledge and images or texts. These works do not fully\nexploit medical data’s multi-modal nature and relationships.\n2) The generated reports show a deficiency in both precision\nand consistency. Most approaches (Yuan et al. 2019; You\net al. 2021) directly align image visual features and report\nlinguistic features to generate reports. The limitation of an-\nnotated correspondence between images and text results in\ninaccuracies and inconsistencies in the sentences generated\nby these methods. In addition, some essential medical termi-\nnologies in medical reports cannot be effectively generated.\nTo tackle the above limitations, in this paper, we propose\na M\nulti-modal Memory Transformer Network (MMTN) to\ngenerate semantically coherent and consistent medical im-\nage reports. To take full advantage of the multi-modal na-\nture of medical data, MMTN is capable of incorporating\nand processing multi-modal medical data, i.e., medical im-\nage, terminology knowledge, and text report simultaneously,\nand exploring the interactions between different modalities\nto improve the quality of medical report generation. To make\nthe report cover important medical terminologies, we de-\nsigned the MMTN encoder to capture and memorize the re-\nlationship between medical images and medical terminolo-\ngies, which can assist in guiding the transformation from im-\nage visual features to report text features with the medium of\nmedical terminologies. Specifically, the grid module and ter-\nminology BERT module extract features from medical im-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n277\nages and terminologies, respectively. The memory augment\nmodule is devised to learn the relationship between two fea-\ntures using learnable memory matrices. Furthermore, to ex-\nploit the cross-modal complementarity of multi-modal med-\nical features, we apply the multi-modal fusion layer on top\nof the MMTN decoder to adaptively learn the contribution of\nmulti-modal visual features and linguistic features for word\ngeneration. Experimental results on three real-world medi-\ncal image report datasets illustrate the effectiveness of our\nMMTN. The contributions are summarized as follows:\n• We propose a Multi-modal Memory Transformer Net-\nwork to process multi-modal medical data including\nmedical image, terminology knowledge, and report text,\nand design a unique encoder to associate and memorize\nvisual features of medical images and representations of\nterminologies, which assists in bridging the distance be-\ntween vision and language.\n• We build a multi-modal fusion layer, attached to the top\nof the MMTN decoder, to weigh the contribution of vi-\nsual and linguistic features by exploiting the cross-modal\ncomplementarity of multi-modal medical features, and to\ngenerate an image-report consistent report.\n• We experimentally evaluate MMTN using three real-\nworld datasets. The results demonstrate that MMTN out-\nperforms state-of-the-art methods on both automatic met-\nrics and human evaluation, indicating that our MMTN\ncan generate accurate medical reports.\nRelated Work\nThe existing works mainly explore the image captioning and\nreport generation for medical domain.\nImage Captioning\nThe task of image captioning has been studied by two main\napproaches: traditional methods and deep learning based\nmethods. For traditional methods, the retrieval- (Gupta,\nVerma, and Jawahar 2012) and template-based (Mitchell\net al. 2012) models are the most commonly adopted for\ncaption generation. With the development of deep learn-\ning (He et al. 2016; Huang et al. 2017), the encoder-decoder\nstructures (Shin et al. 2016) are widely used. The visual\ncaptioning models employ attention mechanisms (Rennie\net al. 2017; You et al. 2016) to improve performance. In\naddition, extra information is adopted to assist text gener-\nation for Natural Language Processing (NLP) and image\ncaption tasks, such as pre-trained embeddings (Zhang et al.\n2019), pre-built knowledge graphs (Li et al. 2019), and pre-\ntrained models (Devlin et al. 2019). The Transformer-based\nmodel (Cornia et al. 2020; Zhang et al. 2021) also greatly\nimproves the performance of the task.\nHowever, these methods cannot be directly transferred to\nmedical report generation tasks. Medical reports do not con-\nsist of only a sentence of short text but a long paragraph\nconsisting of normal and abnormal descriptions. The image\ncaption methods do not cope effectively with the properties.\nMedical Report Generation\nSimilar to image captioning, most existing works of report\ngeneration adopt the encoder-decoder paradigm to gener-\nate reports. Works (Yuan et al. 2019; You et al. 2021) fuse\nthe image features with the medical tags or concepts pre-\ndicted by Convolutional Neural Network (CNN) to generate\nreports. Some approaches adopted extra information (such\nas context (Jing, Xie, and Xing 2018) and topic representa-\ntions (Li et al. 2018)) to assist report generation. Other meth-\nods append auxiliary modules to CNN-RNN architecture,\nsuch as the recurrent generation model (Xue et al. 2018),\nand clinical features (Zhou et al. 2021). The graph neural\nnetworks (Liang et al. 2018) are derived to the predefined\nabnormal graphs (Li et al. 2019) and pre-constructed graph\nembedding modules (Zhang et al. 2020) for report gener-\nation. Subsequently, Transformer-based approaches (Chen\net al. 2020; Liu et al. 2021a; Cao et al. 2022) are pro-\nposed to solve the problem that RNN-based models cannot\neffectively handle dependencies between distant-location.\nWorks (Chen et al. 2020, 2021) use memory vectors to mem-\norize the interaction between images and reports. Besides,\nthe contrastive model, CA (Liu et al. 2021b), captures and\ndescribes abnormal regions, and unsupervised KGAE (Liu\net al. 2021c) relaxes the dependency on paired data.\nHowever, these works did not fully explore relationships\nbetween multi-modal medical data. Our work differs from\nthese in that we not only associate and memorize the rela-\ntionship between images and terminologies, but also use the\nproperties of multi-modal data to generate reports.\nMulti-Modal Memory Transformer Network\nThe multi-modal memory Transformer network consists of\nthree core components, namely the MMTN encoder, the\nMMTN decoder, and the multi-modal fusion layer.\nThe overall architecture of our MMTN is depicted in Fig-\nure 2. The MMTN encoder is in charge of processing input\nimages and medical terminologies into the enriched features,\naiming to associate and memorize the relationship between\ngrid features and terminological features. The MMTN de-\ncoder receives the output of the encoder and the word em-\nbeddings of reports to generate semantic states. The multi-\nmodal fusion layer conducts joint representations of multi-\nmodal features by self-directed learning the contribution of\nenriched features and semantic states to generate semanti-\ncally consistent medical reports.\nMMTN Encoder\nFor the generated report to encompass important medical\nterminologies, the MMTN encoder is devised to associate\nand memorize the relationship between visual features of\nmedical images and medical terminology representations,\nwhich assists in bridging the gap between images and re-\nports. The MMTN encoder consists of a grid module, a ter-\nminology BERT, and a memory augment module.\nGrid Module Given any medical imageI, the grid module\nis designed to extract grid features fg of I. The grid features\nfg are extracted by a pre-trained CNN model (Huang et al.\n2017). Specifically, the image I is first divided into several\n278\nInput Image \nnormal, ileal ulcer,\nileocecal polyp, colonic\npolyp, colon cancer, \ncolonic space occupying,\ncolonic mucosal lesions,\nrectal adenoma,rectal\npolyp, rectal\ncancer\n, proctitis, rectal\nspace occupying\nMedical T\nerminology \n(Findings)\nModality 1: Image\nModality 2: Knowledge \nImage Grid \nCNN\nGrid Module \nBERT \nFeed Forward \nTerminology BERT\nMasked  \nMulti-Head\nAttention\nAdd & Norm \nModality 3: Text \nMedical Reports \nWord\nEmbedding\nSemantic \nStates\nLinear \nMulti-Head\nAttention\nAdd & Norm \nAdd & Norm \nFeed Forward \nGrid \nFeatures \nTerminological  \nFeatures \nMulti-Head \nAttentionMasked  \nMulti-Head\nAttention\nMulti-Head\nAttention\nAdd & Norm \nAdd & Norm \nFeed Forward \nK  \nMemory  \nMatrix \nConcatConcat\nMemory Augment\nModule \nV Memory\nMatrix \nMMTN Encoder\nX N\nMMTN Decoder\nX N\nOne polyp was found in\nthe rectum, with smooth\nsurface mucosa and the\nsame color as the\nsurrounding mucosa\nWords Prediction \nMasked  \nMulti-Head\nAttention\nMulti-Head \nAttention\nMultimodal\nFusion Layer\nEnriched \nFeatures \nFigure 2: Overview of our proposed MMTN architecture. The input images and medical terminology knowledge are first fed\ninto the MMTN encoder, consisting of the grid module, terminology BERT, and a stack of memory augment modules, to obtain\nthe enriched features. A stack of MMTN decoders is in charge of generating the semantic states. The multi-modal fusion layer\nmeasures the contribution of two features to generate a medical report.\nequal-sized regions, and then each grid feature gi of the re-\ngion is extracted separately from the last convolutional layer\nof CNN. Subsequently, the final grid featuresfg are obtained\nby concatenating each extracted grid feature. The grid mod-\nule can be expressed as:\nfg = FGM (I) =Concat[g1, g2 . . . ,gR] (1)\nwhere FGM (.) denotes the grid module, Concat indicates\nthe concatenation operation, R is the number of regions.\nTerminology BERT The terminology BERT is adopted to\nrepresent the contextual information of medical terminolo-\ngies related to medical reports, which helps to improve the\ncontextual relevance of reports.\nWe first build two corpora of commonly used medical\nterminologies for gastrointestinal and thoracic diseases. For\ngastrointestinal diseases, we invite gastroenterologists to\nprovide medical terminologies that often appear in reports,\nsuch as “smooth mucosa”, “polypoid protrusion”, and “sur-\nface erosion”. In addition, the medical terminologies for tho-\nracic diseases are automatically extracted from the “Find-\nings” part of medical reports with the frequencies no less\nthan three times in the corpus, such as “no pneumothorax”,\n“biapical plural thickening”, and “hyperexpanded lung”.\nFurthermore, we employ a BERT-based module to ex-\ntract terminological features. The terminology BERT mod-\nule consists of a pre-trained BERT model (Devlin et al.\n2019; Zhang et al. 2021) and a feed-forward network to ex-\ntract terminological features from the defined terminology\ncorpus. The process can be formalized as:\nfB = BERT (C) (2)\nft = Attmask\n\u0000\nFFN\n\u0000\nfB\u0001\u0001\n(3)\nwhere fB is the output of the pre-trained BERT model,C de-\nnotes the word sequence of the terminology corpus,Attmask\nis the masked multi-head attention, FFN represents the\nfully connected feed-forward network, and ft indicates the\nterminological features.\nMemory Augment Module The memory augment mod-\nule is proposed to associate and memorize the hidden cor-\nrelation between medical images and terminologies. For a\nmedical image, there are corresponding medical terminolo-\ngies in the report to describe it. To exploit the characteris-\ntics, we adopt the memory augment module to represent the\ncorrelation between visual context and medical terminology\nfeatures, which is beneficial to guide the report generation.\nThe input of the memory augment module is the joint fea-\ntures Qm generated by grid features fg and terminological\nfeatures ft under an attention mechanism. A set of keys and\nvalues for self-attention are employed to memorize semantic\ncontext information between medical images and terminolo-\ngies. The keys and values are implemented as two learnable\nmatrices, namely MemK and MemV , which can be up-\ndated by SGD. The feature interactions in the memory aug-\nment module are computed by scaled dot-product attention.\nSubsequently, the output of multi-head attention is applied\n279\nto the feed-forward layer. Finally, the enriched features fe\nare obtained by the residual connection and normalization\noperation layer. Formally, the process can be defined as:\nQm = Attention(Qj, Kj, Vj) (4)\nQj = WQjAttmask(ft) (5)\nKj = WKj fg, Vj = WV jfg (6)\nAttention(Q, K, V) =Softmax (QKT\n√\nd\n)V (7)\nfa = Attention(WQmQm, Km, Vm) (8)\nKm = Concat[WKmQm, MemK] (9)\nVm = Concat[WV mQm, MemV ] (10)\nfe = AddNorm(FFN (AddNorm(fa))) (11)\nwhere Qm denotes the input of memory augment module,\nQx, Kx and Vx (x ∈ {j, m}) represent the query, key and\nvalue matrix, WQx , WKx and WVx (x ∈ {j, m}) are learn-\nable weight matrices, d indicates a scaling factor, fa is the\noutput of the multi-head attention layer in this module, and\nAddNorm is composition of a residual connection and of a\nnormalization layer.\nMMTN Decoder\nThe MMTN decoder is adopted to generate the semantic\nstates based on previously generated words and the enriched\nfeatures. The text sequence features fw of medical reports\nare extracted by word embedding layer, and then regarded\nas the input of the first layer of the MMTN decoder. The\nsecond layer is a multi-head attention operation with K and\nV matrices from the enriched featuresfe of MMTN encoder.\nThe MMTN decoder can be formalized as:\nfs = AddNorm(Attmask(fw)) (12)\nfm = AddNorm(Attention(WQhfs, WKhfe, WV hfe))\n(13)\nfh = AddNorm(FFN (fm)) (14)\nwhere fs and fm denote the intermediate outputs of the de-\ncoder, and fh is the semantic states.\nMulti-Modal Fusion Layer\nTwo modal features are obtained by modules mentioned\nabove, namely the enriched features fe and the semantic\nstates fh. To obtain a semantically coherent medical report,\nwe designed a multi-modal fusion layer, attached to the up-\nper layer of the MMTN decoder. The module combines the\nfeature information of two modalities to calculate the con-\ntribution of visual features and linguistic features to each\ngenerated sequence. The multi-modal fusion layer can be\ndefined as follows:\nQo = WQoAttmask(WQafe, WKafh, WV afh) (15)\nKo = WKofe, Vo = WV ofe (16)\nOutput = Attention(Qo, Ko, Vo)WA (17)\nwhere Qo, Ko and Vo are the query, key and value matrix of\nthe multi-head attention, Output denotes the result of multi-\nhead attention for the generated reports,WQx, WKx, WV x\n(x ∈ {o, a}), and WA are learnable weight matrices.\nTraining\nFor each training sample (I, r), where I is a group of im-\nages and r is the corresponding medical report composed of\nground truth sequences, the loss L of report generation is\nminimized by the cross-entropy loss:\nL(θ) =−\nMX\ni=1\nlog(pθ(si|s1:i−1)) (18)\nwhere θ is the parameters of our MMTN model, s1:M repre-\nsents ground truth sequences of the report r.\nExperiment\nIn this section, we first describe the experimental settings.\nThen, we demonstrate the experimental results, including\nperformance comparisons, case studies, and ablation studies\nto evaluate the performance of MMTN against state-of-the-\nart baseline methods.\nExperimental Settings\nDataset We conduct experiments on three datasets.\n1) Gastrointestinal Endoscope image dataset (GE) is a\nprivate dataset contains white light images and their Chi-\nnese reports from the Department of Gastroenterology. The\ndataset consists of 3,168 patients. Each patient has multiple\ngastrointestinal endoscope images from different perspec-\ntives with their corresponding medical reports. We obtain\n15,345 images and 3,069 reports collected from the dataset\nby selecting patients with 5 images. We collect 126 medical\nterminologies from gastroenterologists, including 89 abnor-\nmal findings and 37 normal findings.\n2) IU-CX (Demner-Fushman et al. 2016) is a public chest\nX-ray dataset. We select 2,896 radiology reports with frontal\nand lateral view images from the original dataset. We ex-\ntract 97 medical terminologies from the <Abstract> field,\nincluding 80 abnormal and 17 normal findings.\n3) MIMIC-CXR (Johnson 2019) is the largest public chest\nX-ray dataset including 473,057 images and 206,563 re-\nports. We adopt the same criterion with IU-CX to select sam-\nples, which results in 142,772 images and 71,386 reports.\nThe medical terminologies are the same as IU-CX.\nParameter Settings The method is implemented in Py-\ntorch 1.7.1 based on Python 3.8.5 and trained on a server\nwith an Intel Core i9-10900K CPU, and an Nvidia RTX\n3090 GPU. We randomly split both datasets into 7:1:2 train-\ning:validation:testing data to train and evaluate our method.\nA pre-trained DenseNet-121 is adopted to extract grid fea-\ntures, with 7 × 7 grid size. The Chinese word segmentation\nmodule of Jieba (Jieba 2019) is employed for processing the\nreports of GE. The number of heads is set to 8, the layer\nnumber N of Transformer is 3, and the number of memory\nvectors is 40 rows. If not specifically specified, the hidden\ndimension of MMTN is 512. The dropout probability is 0.1.\nThe ADAM optimizer with a batch size of 32 and a learning\nrate of 1e-5 is employed to minimize the loss function.\n280\nDataset Architecture Methods BLEU-1 BLEU-2 BLEU-3 BLEU-4 CIDEr ROUGE-L\nGE\nCNN-RNN\n-based\nSaT 0.643 0.552 0.506 0.414 0.557 0.613\nAAtt 0.649 0.549 0.491 0.419 0.579 0.617\nCoAtt 0.774 0.654 0.618 0.575 0.674 0.748\nRGKG 0.752 0.676 0.609 0.554 0.684 0.726\nTransformer\n-based\nTransformer 0.689 0.572 0.584 0.521 0.604 0.691\nR2GEN 0.779 0.677 0.619 0.574 0.679 0.736\nPPKED 0.791 0.684 0.624 0.579 0.691 0.749\nCMN 0.782 0.679 0.621 0.572 0.686 0.742\nMMTN(ours) 0.799 0.692 0.634 0.589 0.703 0.748\nIU-CX\nCNN-RNN\n-based\nSaT 0.216 0.124 0.087 0.066 0.294 0.307\nAAtt 0.220 0.127 0.089 0.068 0.295 0.308\nCoAtt 0.455 0.288 0.205 0.154 0.277 0.369\nHRGRA 0.438 0.298 0.208 0.151 0.343 0.322\nKER 0.455 0.288 0.205 0.154 0.277 0.369\nRGKG 0.441 0.291 0.203 0.147 0.304 0.367\nTransformer\n-based\nTransformer 0.396 0.254 0.179 0.135 - 0.342\nR2GEN 0.470 0.304 0.219 0.165 - 0.371\nPPKED 0.483 0.315 0.224 0.168 0.351 0.376\nCMN 0.475 0.309 0.222 0.170 - 0.375\nAlignTransformer 0.484 0.313 0.225 0.173 - 0.379\nMMTN(ours) 0.486 0.321 0.232 0.175 0.361 0.375\nDataset Architecture Methods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L\nMIMIC-CXR\nCNN-RNN\n-based\nSaT 0.299 0.184 0.121 0.084 0.124 0.263\nAAtt 0.299 0.185 0.124 0.088 0.118 0.266\nTransformer\n-based\nTransformer 0.314 0.192 0.127 0.090 0.125 0.265\nR2GEN 0.353 0.218 0.145 0.103 0.142 0.277\nPPKED 0.360 0.224 0.149 0.106 0.149 0.284\nCMN 0.353 0.218 0.148 0.106 0.142 0.278\nAlignTransformer 0.378 0.235 0.156 0.112 0.158 0.283\nMMTN(ours) 0.379 0.238 0.159 0.116 0.161 0.283\nTable 1: Comparison of baselines and MMTN on automatic metrics on the three datasets.\nBaselines We compare our MMTN to the following state-\nof-the-art approaches. The CNN-RNN-based methods in-\nclude SaT (Vinyals et al. 2015), AAtt (Lu et al. 2017),\nCoAtt (Jing, Xie, and Xing 2018), and RGKG (Zhang\net al. 2020). The Transformer-based methods are Trans-\nformer (Chen et al. 2020), R2GEN (Chen et al. 2020), PP-\nKED (Liu et al. 2021a), CMN (Chen et al. 2021), and Align-\nTransformer (You et al. 2021). For the IU-CX dataset, we\nalso compare with HRGRA (Li et al. 2018) and KER (Li\net al. 2019) that utilize template retrieval method for tho-\nracic diseases, and the templates are not defined in GE and\nMIMIC-CXR dataset.\nEvaluation Metrics We employ both automatic metrics\nand human evaluation to evaluate the performance for the\nmedical report generation. 1) Automatic Metrics: BLEU\n(unigram to 4-gram) (Papineni et al. 2002), ROUGE-\nL (Lin 2004), METEOR (Banerjee and Lavie 2005), and\nCIDEr (Vedantam, Zitnick, and Parikh 2015). 2) Human\nEvaluation: For the samples in GE, we randomly select 50\nsamples and invite gastroenterologists and graduate students\nwho collaborate with us as experts to evaluate the reports\ngenerated by baseline methods and our MMTN. Each sam-\nple is given the ground-truth report, and experts are asked\nto select the most consistent report among those generated\nby the different methods. Evaluation metrics include the re-\nport completeness, the correctness of generated abnormality\nfindings, and contextual coherence. We collect results from\n10 experts and calculate the ratio of the number of times that\neach model is selected to the number of total evaluations as\nthe human evaluation score of each model.\nResults on Report Generation\nAutomatic Evaluation We compare MMTN with base-\nline methods on three datasets for the report generation task,\nwith all performances on automatic metrics shown in Ta-\nble 1. It is highlighted that the best and second best\nre-\nsults. Our MMTN is superior to all baseline models on\nBLEU-n and CIDEr (or METEOR) scores on three datasets,\ndemonstrating the effectiveness and accuracy of MMTN in\ngenerating medical reports. MMTN is second only to PP-\nKED and AlignTransformer on ROUGE-L. PPKED incor-\nporates additional semantic information and abnormality\ngraph (i.e., abnormal regions and observation graph) into\nthe generation model, which guides it to learn the most\ncommon subsequence of ground truth reports. AlignTrans-\nformer introduces additional disease label predictions to\nguide the generation of abnormality descriptions and there-\nfore achieves the best performance on IU-CX. Our MMTN\nalso achieves a competitive performance on ROUGE-L com-\npared to the above two methods. The results on automatic\nmetrics demonstrate that our MMTN is capable of generat-\n281\nMethod SaT AAtt CoAtt RGKG Transformer R2GEN PPKED CMN MMTN\nHuman Evaluation Score 0.018 0.020 0.062 0.122 0.054 0.132 0.192 0.152 0.248\nTable 2: The results of our MMTN and baselines on human evaluation scores.\nMetrics AlignTrans MMTN t p\nBLEU-1 0.378 0.379 -4.950 0.008**\nBLEU-2 0.235 0.238 -6.124 0.004**\nBLEU-3 0.156 0.159 -3.674 0.021*\nBLEU-4 0.112 0.116 -4.899 0.008**\nMETEOR 0.158 0.161 -3.598 0.024*\nROUGE-L 0.283 0.283 0.000 1.000\nTable 3: Results of t-test analysis (*:p <0.05, **: p <0.01)\ning accurate and coherent reports.\nIn addition, we obtain some observations by comparing\nmethods with different architectures. First, models guided\nby medical knowledge (i.e., HRGRA, KER, RGKG, PP-\nKED, and our MMTN) obtain higher or equivalent auto-\nmatic metrics scores. This observation validates that knowl-\nedge is essential to guide the transformation from visual fea-\ntures to linguistic features in the medical domain. Second,\ncompared with the vanilla CNN-RNN structure (i.e., SaT),\nthe vanilla Transformer (i.e., Transformer) works slightly\nbetter. Consistent with the performance, most Transformer-\nbased models outperform CNN-RNN-based models on auto-\nmatic evaluation metrics, indicating that self-attention plays\na positive role in the transformation of multi-modal features.\nThird, compared to models using the co-attention mech-\nanism, approaches equipped with memory modules (i.e.,\nR2GEN, CMN, and our MMTN) exhibit better performance.\nOne possible explanation is that using memory modules en-\nables visual and linguistic features to be transformed in a\nsingle identical space. Our MMTN outperforms R2GEN and\nCMN in most metrics, illustrating that associating visual fea-\ntures with medical terminologies representations facilitates\nreport generation. Last, the CoAtt, HRGRA, and PPKED\nadopt extra semantic information (e.g., medical tags, report\ntemplates, and abnormal graphs). The three methods also\nachieve good outcomes on specific metrics, which shows\nthat additional information is helpful for performance im-\nprovement. However, our MMTN still achieves state-of-the-\nart performance without using such information.\nHuman Evaluation To evaluate the clinical readability of\nthe generated report, we invite three digestive gastroenterol-\nogists and seven graduate students to evaluate the reports\ngenerated by MMTN and baseline methods. Given random\n50 samples of GE1, we ask each expert to select one report\nthat is most consistent with the ground truth descriptions for\neach sample. The human evaluation score for each method\nis the proportion of times the method is selected by experts\nout of the total number of evaluations. For example, MMTN\n1The human evaluation did not evaluate the IU-CX and\nMIMIC-CXR datasets because we did not have access to results\nprovided by professional radiologists.\nis selected 124 times by experts as the report closest to the\nground truth, so its human evaluation score is 124 / 500 =\n0.248. The human evaluation results are presented in Ta-\nble 2. The results show that the MMTN is better than base-\nline methods in clinical practice, demonstrating MMTN’s\ncapability of generating accurate and reliable reports.\nSignificant Tests To verify whether there are significant\ndifferences between our MMTN and state-of-the-art mod-\nels, we conduct a t-test on automatic metrics. Due to the\npage limitation, only results on MIMIC-CXR with mini-\nmal improvement compared to the strongest baseline (i.e.,\nAlignTransformer) are presented. As shown in Table 3, the\nsamples show significant differences on BLEU-1-4 and ME-\nTEOR, indicating that the improvement of MMTN is sig-\nnificant compared to baseline methods, and the comparison\nresults rule out the possibility that the advantage of our al-\ngorithm is the result of sampling difference.\nQualitative Analysis To further investigate the effective-\nness of our MMTN, we conduct qualitative analysis on three\ndatasets with their ground-truth and generated reports. We\nrandomly select a sample from each dataset to perform a\ncase study, and visualization results are shown in Figure 3.\nThe first row is the sample from GE (note that gastroenterol-\nogists translate the ground-truth and generated reports of GE\nfrom Chinese to English), and the middle and last row rep-\nresent the sample from IU-CX and MIMIC-CXR, respec-\ntively. It can be observed that MMTN is capable of generat-\ning reports consistent with the ground truth. In GE sample,\nthe generated report accurately reports the locations (i.e.,as-\ncending colon) and types (i.e.,polyp) of lesions. Similarly, in\nIU-CX and MIMIC-CXR samples, MMTN also accurately\ndescribes most types of lesions, such as opacities, cavitary\nlesion, and hyperinflation. In addition, MMTN also gener-\nates the descriptions for normal regions, such as “smooth\nmucosa”, “No pleural effusion”, and “No focal consolida-\ntion”. Normal descriptions generation facilitates the coher-\nence and completeness of the report. It is worth noting that\nthe reports generated by MMTN cover almost all of common\nmedical terminologies.\nTo further investigate how the MMTN associates visual\ninformation of images and representations of medical termi-\nnologies, we visualize image-text attention mappings from\nthe multi-head attention of the decoder. Figure 3 shows inter-\nmediate image-text correspondences for several medical ter-\nminologies between visual features and word embeddings. It\nis observed that MMTN correctly aligns regions in images\nwith indicated terminologies. Taking the first case in Fig-\nure 3 as an example, our MMTN can correctly identify dis-\neases, i.e., “hemispheric polyp”, and can also indicate med-\nical terminologies about the position and trait, such as “as-\ncending colon”, “smooth mucosa” and “vascular texture”.\nThis observation suggests that our model not only generates\n282\nMMTN Generated Report: \nThere is the hyperinflation of the lungs. The heart is cardiomegaly. The mediastinal contour  and \nhemidiaphragm are within normal limits. There is no pneumothorax or pleural effusion. There are no \nfocal areas of consolidation or suspicious pulmonary opacity.\n“cardiomegaly ” “hemidiaphragm”“hyperinflation ” “mediastinal contours”\nGround-Truth Report: \nThe lungs appear hyperinflated . \nNo focal consolidation, effusion or \npne um othora x.  N o si gns o f \ncongestion or edema. Heart size is \nmildly prominent .  Mediastinal \ncontour  is normal.  No acute \nosseous abnormality. No free air \nbelow the right hemidiaphragm.\nMMTN Generated Report: \nThere is bilateral interstitial opacity. There was a cavernous lesion  in the lung apex . The \ncardiomediastinal silhouette  were normal. There is no focal consolidation. There is no pleural \neffusion. There is no evidence of pneumothorax.\n“opacity ”\n “lung apex”\n “cavitary lesion”“cardiomediastinal silhouette”\nGround-Truth Report: \nT h e r e  a r e  d i f f u s e  b i l a t e r a l \ninterstitial and alveolar opacities . \nThere are irregular opacities in the \nleft lung apex , that represent a \ncavitary lesion  in the left lung \napex. There are streaky opacities \nin the right upper lobe, XXXX \nscarring. The cardiomediastinal \nsilhouette  is normal in size and \nc o n t o u r .  T h e r e  i s  n o \npneumothorax or large pleural \neffusion.\nMMTN Generated Report: \nA hemispherical polyp with smooth mucosa was seen in the ascending colon. The submucosal vessels \nwere clearly textured with regular peristalsis.\n“ascending colon” “hemispheric polyp” “smooth mucosa” “vascular texture”\nGround-Truth Report: \nA hemispherical polyp  of about \n0.5 cm in diameter with smooth \nsurface and clear border was seen \ni n  t h e  p r o x i m a l  p a r t  o f  t h e \nascending colon . The mucosa  \nw a s  s m o o t h,  w i t h  c l e a r \nsubmucosal vascular texture  and \nregular peristalsis.\nFigure 3: Visualizations of image-text attention mappings on GE (the first row), IU-CX (the middle row), and MIMIC-CXR\n(the last row). The left part is the image and its ground-truth report, and the right part is the MMTN generated reports and the\nmappings of image region and medical terminologies. Colors from blue to red represent the weights from low to high.\n\\MAM \\MT \\MFL Full\nDesigns on GE\n0.6\n0.8Score\n\\MAM \\MT \\MFL Full\nDesigns on IU-CX\n0.2\n0.4\n\\MAM \\MT \\MFL Full\nDesigns on MIMIC-CXR\n0.1\n0.2\n0.3\nCIDEr\nROUGE-L\nBLEU-1\nBLEU-2\nBLEU-3\nBLEU-4\nMETEOR\nFigure 4: Ablation study for different designs.\ncoherent medical reports but also enhances the alignment\nbetween the images and the generated texts.\nAblation Studies\nEffect of components. We conduct ablation studies on the\nthree datasets to investigate the effectiveness of each module\nof MMTN. Specifically, \\MAM excludes the memory aug-\nment module from MMTN, \\MT does not consider medical\nterminologies and only utilizes the grid feature as the output\nof the MMTN encoder, and\\MFL drops the multi-modal fu-\nsion layer. As shown in Figure 4, MMTN\\MT has the worst\nperformance, revealing that introducing medical terminolo-\ngies can effectively improve report generation accuracy. On\nthe other hand, the performance of MMTN\\MAM is poor,\nwhich demonstrates that aligning and memorizing the rela-\ntionship between images and terminologies is indeed helpful\nto bridging the distance between visual and linguistic fea-\ntures. The performance of MMTN\\MFL is similar to that\nof MMTN\\MAM, indicating the multi-modal fusion layer\nplays a certain role in improving performance. These results\nsuggest that the modules mentioned above are efficient for\nthe report generation task.\nConclusion\nIn this paper, we propose a multi-modal memory Trans-\nformer network to address multi-modal medical data, in-\ncluding image, text report, and terminology knowledge to\nimprove the quality of medical report generation. To cover\nimportant medical terminologies in the generated reports,\nthe MMTN encoder is designed to align and memorize\nthe relationship between visual and terminological features.\nFurther, we employ the multi-modal fusion layer to calculate\nthe contribution of vision and language features to the report.\nExtensive experiments on three real world datasets demon-\nstrate that our proposed MMTN achieves superior perfor-\nmance than mainstream approaches.\n283\nAcknowledgments\nThis work is partially supported by National Key\nR&D Program of China No.2021YFF0900800; NSFC\nNo.62202279; Shandong Provincial Key Research and\nDevelopment Program (Major Scientific and Techno-\nlogical Innovation Project) (No. 2021CXGC010506 and\nNO.2021CXGC010108); the State Scholarship Fund by the\nChina Scholarship Council (CSC).\nReferences\nBanerjee, S.; and Lavie, A. 2005. METEOR: An Auto-\nmatic Metric for MT Evaluation with Improved Correlation\nwith Human Judgments. In Proceedings of the Workshop\non Intrinsic and Extrinsic Evaluation Measures for Ma-\nchine Translation and/or Summarization@ACL 2005, Ann\nArbor, Michigan, USA, June 29, 2005, 65–72. Association\nfor Computational Linguistics.\nCao, Y .; Cui, L.; Yu, F.; Zhang, L.; Li, Z.; Liu, N.; and\nXu, Y . 2022. KdTNet: Medical Image Report Generation\nvia Knowledge-Driven Transformer. In Database Systems\nfor Advanced Applications - 27th International Conference,\nDASFAA 2022, volume 13247 ofLecture Notes in Computer\nScience, 117–132. Springer.\nChen, Z.; Shen, Y .; Song, Y .; and Wan, X. 2021. Cross-\nmodal Memory Networks for Radiology Report Generation.\nIn Proceedings of the 59th Annual Meeting of the Associ-\nation for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing,\nACL/IJCNLP 2021, 5904–5914. Association for Computa-\ntional Linguistics.\nChen, Z.; Song, Y .; Chang, T.; and Wan, X. 2020. Generat-\ning Radiology Reports via Memory-driven Transformer. In\nProceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2020, 1439–1449.\nAssociation for Computational Linguistics.\nCornia, M.; Stefanini, M.; Baraldi, L.; and Cucchiara, R.\n2020. Meshed-Memory Transformer for Image Captioning.\nIn 2020 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, 10575–10584. Computer Vi-\nsion Foundation / IEEE.\nDemner-Fushman, D.; Kohli, M. D.; Rosenman, M. B.;\nShooshan, S. E.; Rodriguez, L.; Antani, S.; Thoma, G. R.;\nand McDonald, C. J. 2016. Preparing a collection of radiol-\nogy examinations for distribution and retrieval. Journal of\nthe American Medical Informatics Association, 23(2): 304–\n310.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2019, 4171–4186. Association for Com-\nputational Linguistics.\nGupta, A.; Verma, Y .; and Jawahar, C. V . 2012. Choosing\nLinguistics over Vision to Describe Images. In Proceedings\nof the Twenty-Sixth AAAI Conference on Artificial Intelli-\ngence. AAAI Press.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016 ,\n770–778. IEEE Computer Society.\nHuang, G.; Liu, Z.; van der Maaten, L.; and Weinberger,\nK. Q. 2017. Densely Connected Convolutional Networks.\nIn 2017 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, 2261–2269. IEEE Computer So-\nciety.\nJieba. 2019. “Jieba” Chinese text segmentation: built to be\nthe best Python Chinese word segmentation module. https:\n//github.com/fxsjy/jieba. Accessed: 2023-03-22.\nJing, B.; Xie, P.; and Xing, E. P. 2018. On the Automatic\nGeneration of Medical Imaging Reports. In Proceedings of\nthe 56th Annual Meeting of the Association for Computa-\ntional Linguistics, ACL 2018, 2577–2586. Association for\nComputational Linguistics.\nJohnson, A. E. W. 2019. MIMIC-CXR: A large publicly\navailable database of labeled chest radiographs. CoRR,\nabs/1901.07042.\nLi, C. Y .; Liang, X.; Hu, Z.; and Xing, E. P. 2019.\nKnowledge-Driven Encode, Retrieve, Paraphrase for Med-\nical Image Report Generation. In The Thirty-Third AAAI\nConference on Artificial Intelligence, AAAI 2019, 6666–\n6673. AAAI Press.\nLi, Y .; Liang, X.; Hu, Z.; and Xing, E. P. 2018. Hybrid\nRetrieval-Generation Reinforced Agent for Medical Image\nReport Generation. In Advances in Neural Information Pro-\ncessing Systems 31: Annual Conference on Neural Informa-\ntion Processing Systems 2018, NeurIPS 2018, 1537–1547.\nLiang, X.; Hu, Z.; Zhang, H.; Lin, L.; and Xing, E. P. 2018.\nSymbolic Graph Reasoning Meets Convolutions. In Ad-\nvances in Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing Systems\n2018, NeurIPS 2018, 1858–1868.\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLiu, F.; Wu, X.; Ge, S.; Fan, W.; and Zou, Y . 2021a. Explor-\ning and Distilling Posterior and Prior Knowledge for Radi-\nology Report Generation. In IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2021, 13753–13762.\nComputer Vision Foundation / IEEE.\nLiu, F.; Yin, C.; Wu, X.; Ge, S.; Zhang, P.; and Sun, X.\n2021b. Contrastive Attention for Automatic Chest X-ray\nReport Generation. In Findings of the Association for Com-\nputational Linguistics: ACL/IJCNLP 2021, volume ACL/I-\nJCNLP 2021 of Findings of ACL, 269–280. Association for\nComputational Linguistics.\nLiu, F.; You, C.; Wu, X.; Ge, S.; Wang, S.; and Sun, X.\n2021c. Auto-Encoding Knowledge Graph for Unsupervised\nMedical Report Generation. In Advances in Neural Infor-\nmation Processing Systems 34: Annual Conference on Neu-\nral Information Processing Systems 2021, NeurIPS 2021 ,\n16266–16279.\nLu, J.; Xiong, C.; Parikh, D.; and Socher, R. 2017. Know-\ning When to Look: Adaptive Attention via a Visual Sentinel\n284\nfor Image Captioning. In 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2017, 3242–\n3250. IEEE Computer Society.\nMitchell, M.; Dodge, J.; Goyal, A.; Yamaguchi, K.; Stratos,\nK.; Han, X.; Mensch, A. C.; Berg, A. C.; Berg, T. L.; and III,\nH. D. 2012. Midge: Generating Image Descriptions From\nComputer Vision Detections. In EACL 2012, 13th Confer-\nence of the European Chapter of the Association for Com-\nputational Linguistics, 747–756. The Association for Com-\nputer Linguistics.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W. 2002. Bleu:\na Method for Automatic Evaluation of Machine Translation.\nIn Proceedings of the 40th Annual Meeting of the Associa-\ntion for Computational Linguistics, 311–318. ACL.\nRennie, S. J.; Marcheret, E.; Mroueh, Y .; Ross, J.; and Goel,\nV . 2017. Self-Critical Sequence Training for Image Cap-\ntioning. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2017, 1179–1195. IEEE Com-\nputer Society.\nShin, H.; Roberts, K.; Lu, L.; Demner-Fushman, D.; Yao, J.;\nand Summers, R. M. 2016. Learning to Read Chest X-Rays:\nRecurrent Neural Cascade Model for Automated Image An-\nnotation. In 2016 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2016, 2497–2506. IEEE Com-\nputer Society.\nVedantam, R.; Zitnick, C. L.; and Parikh, D. 2015. CIDEr:\nConsensus-based image description evaluation. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2015, 4566–4575. IEEE Computer Society.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.\nShow and tell: A neural image caption generator. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2015, 3156–3164. IEEE Computer Society.\nXue, Y .; Xu, T.; Long, L. R.; Xue, Z.; Antani, S. K.; Thoma,\nG. R.; and Huang, X. 2018. Multimodal Recurrent Model\nwith Attention for Automated Radiology Report Generation.\nIn Medical Image Computing and Computer Assisted Inter-\nvention - MICCAI 2018, volume 11070 of Lecture Notes in\nComputer Science, 457–466. Springer.\nYou, D.; Liu, F.; Ge, S.; Xie, X.; Zhang, J.; and Wu, X.\n2021. AlignTransformer: Hierarchical Alignment of Visual\nRegions and Disease Tags for Medical Report Generation.\nIn Medical Image Computing and Computer Assisted Inter-\nvention - MICCAI 2021, volume 12903 of Lecture Notes in\nComputer Science, 72–82.\nYou, Q.; Jin, H.; Wang, Z.; Fang, C.; and Luo, J. 2016. Image\nCaptioning with Semantic Attention. In 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV , USA, June 27-30, 2016, 4651–4659.\nIEEE Computer Society.\nYuan, J.; Liao, H.; Luo, R.; and Luo, J. 2019. Automatic\nRadiology Report Generation Based on Multi-view Image\nFusion and Medical Concept Enrichment. In Medical Image\nComputing and Computer Assisted Intervention - MICCAI\n2019, volume 11769 of Lecture Notes in Computer Science,\n721–729.\nZhang, H.; Bai, J.; Song, Y .; Xu, K.; Yu, C.; Song, Y .; Ng,\nW.; and Yu, D. 2019. Multiplex Word Embeddings for\nSelectional Preference Acquisition. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing, EMNLP-IJCNLP\n2019, Hong Kong, China, November 3-7, 2019, 5246–5255.\nAssociation for Computational Linguistics.\nZhang, X.; Sun, X.; Luo, Y .; Ji, J.; Zhou, Y .; Wu, Y .; Huang,\nF.; and Ji, R. 2021. RSTNet: Captioning With Adaptive At-\ntention on Visual and Non-Visual Words. In IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2021, 15465–15474. Computer Vision Foundation / IEEE.\nZhang, Y .; Wang, X.; Xu, Z.; Yu, Q.; Yuille, A. L.; and Xu,\nD. 2020. When Radiology Report Generation Meets Knowl-\nedge Graph. In The Thirty-Fourth AAAI Conference on Ar-\ntificial Intelligence, AAAI 2020, 12910–12917.\nZhou, Y .; Huang, L.; Zhou, T.; Fu, H.; and Shao, L. 2021.\nVisual-Textual Attentive Semantic Consistency for Medical\nReport Generation. In 2021 IEEE/CVF International Con-\nference on Computer Vision, ICCV 2021, 3965–3974. IEEE.\n285",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7478744387626648
    },
    {
      "name": "Encoder",
      "score": 0.6349406242370605
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5750036239624023
    },
    {
      "name": "Transformer",
      "score": 0.5709540247917175
    },
    {
      "name": "Modal",
      "score": 0.5427037477493286
    },
    {
      "name": "Complementarity (molecular biology)",
      "score": 0.47327208518981934
    },
    {
      "name": "Memorization",
      "score": 0.42797958850860596
    },
    {
      "name": "Machine learning",
      "score": 0.3579593300819397
    },
    {
      "name": "Voltage",
      "score": 0.1336698830127716
    },
    {
      "name": "Engineering",
      "score": 0.09992033243179321
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ]
}