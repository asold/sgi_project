{
  "title": "Generation-Augmented Retrieval for Open-Domain Question Answering",
  "url": "https://openalex.org/W3176182290",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2796257948",
      "name": "Yuning Mao",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2118516085",
      "name": "Pengcheng He",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2101917160",
      "name": "Xiaodong Liu",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "PEN American Center"
      ]
    },
    {
      "id": "https://openalex.org/A2166559730",
      "name": "Yelong Shen",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2104437897",
      "name": "Jian-Feng Gao",
      "affiliations": [
        "PEN American Center",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2103606203",
      "name": "Jiawei Han",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2108390110",
      "name": "Wei‐Zhu Chen",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3100292568",
    "https://openalex.org/W2963609889",
    "https://openalex.org/W3039017601",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3035169992",
    "https://openalex.org/W4299585995",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3115037692",
    "https://openalex.org/W2962854379",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2740321901",
    "https://openalex.org/W3157758108",
    "https://openalex.org/W2142769902",
    "https://openalex.org/W2989312920",
    "https://openalex.org/W2164547069",
    "https://openalex.org/W2964823609",
    "https://openalex.org/W2970168256",
    "https://openalex.org/W2117473841",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2913443447",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3175111331",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3022840849",
    "https://openalex.org/W3094090968",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2593864460",
    "https://openalex.org/W2148972377",
    "https://openalex.org/W3118423943",
    "https://openalex.org/W3130740619",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2995638926",
    "https://openalex.org/W3021282678",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3043590089",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2982817792"
  ],
  "abstract": "Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 4089–4100\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4089\nGeneration-Augmented Retrieval for Open-Domain Question Answering\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\n1University of Illinois, Urbana-Champaign 2Microsoft Azure AI 3Microsoft Research\n1{yuningm2, hanj}@illinois.edu\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen}@microsoft.com\nAbstract\nWe propose Generation-Augmented Retrieval\n(GAR) for answering open-domain questions,\nwhich augments a query through text genera-\ntion of heuristically discovered relevant con-\ntexts without external resources as supervi-\nsion. We demonstrate that the generated con-\ntexts substantially enrich the semantics of the\nqueries and G AR with sparse representations\n(BM25) achieves comparable or better per-\nformance than state-of-the-art dense retrieval\nmethods such as DPR (Karpukhin et al., 2020).\nWe show that generating diverse contexts for a\nquery is beneﬁcial as fusing their results con-\nsistently yields better retrieval accuracy. More-\nover, as sparse and dense representations are\noften complementary, GAR can be easily com-\nbined with DPR to achieve even better per-\nformance. G AR achieves state-of-the-art per-\nformance on Natural Questions and TriviaQA\ndatasets under the extractive QA setup when\nequipped with an extractive reader, and con-\nsistently outperforms other retrieval methods\nwhen the same generative reader is used.1\n1 Introduction\nOpen-domain question answering (OpenQA) aims\nto answer factoid questions without a pre-speciﬁed\ndomain and has numerous real-world applications.\nIn OpenQA, a large collection of documents (e.g.,\nWikipedia) are often used to seek information per-\ntaining to the questions. One of the most com-\nmon approaches uses a retriever-reader architecture\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\nset of documents using the question as the query\nand then reads the retrieved documents to extract\n(or generate) an answer. The retriever is crucial as it\nis infeasible to examine every piece of information\nin the entire document collection ( e.g., millions\nof Wikipedia passages) and the retrieval accuracy\nbounds the performance of the (extractive) reader.\n∗Work was done during internship at Microsoft Azure AI.\n1Our code is available at https://github.com/\nmorningmoni/GAR.\nEarly OpenQA systems (Chen et al., 2017)\nuse classic retrieval methods such as TF-IDF and\nBM25 with sparse representations. Sparse methods\nare lightweight and efﬁcient, but unable to per-\nform semantic matching and fail to retrieve rele-\nvant passages without lexical overlap. More re-\ncently, methods based on dense representations\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\nembed queries and passages into a latent vector\nspace, in which text similarity beyond lexical over-\nlap can be measured. Dense retrieval methods can\nretrieve semantically relevant but lexically differ-\nent passages and often achieve better performance\nthan sparse methods. However, the dense mod-\nels are more computationally expensive and suffer\nfrom information loss as they condense the entire\ntext sequence into a ﬁxed-size vector that does not\nguarantee exact matching (Luan et al., 2020).\nThere have been some recent studies on query re-\nformulation with text generation for other retrieval\ntasks, which, for example, rewrite the queries to\ncontext-independent (Yu et al., 2020; Lin et al.,\n2020; Vakulenko et al., 2020) or well-formed (Liu\net al., 2019) ones. However, these methods re-\nquire either task-speciﬁc data (e.g., conversational\ncontexts, ill-formed queries) or external resources\nsuch as paraphrase data (Zaiem and Sadat, 2019;\nWang et al., 2020) that cannot or do not trans-\nfer well to OpenQA. Also, some rely on time-\nconsuming training process like reinforcement\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\n2019; Wang et al., 2020) that is not efﬁcient enough\nfor OpenQA (more discussions in Sec. 2).\nIn this paper, we propose Generation-\nAugmented Retrieval ( GAR), which augments\na query through text generation of a pre-trained\nlanguage model (PLM). Different from prior\nstudies that reformulate queries, GAR does not\nrequire external resources or downstream feedback\nvia RL as supervision, because it does not rewrite\nthe query but expands it with heuristically discov-\n4090\nered relevant contexts, which are fetched from\nPLMs and provide richer background information\n(Table 2). For example, by prompting a PLM\nto generate the title of a relevant passage given\na query and appending the generated title to the\nquery, it becomes easier to retrieve that relevant\npassage. Intuitively, the generated contexts\nexplicitly express the search intent not presented\nin the original query. As a result, GAR with\nsparse representations achieves comparable or\neven better performance than state-of-the-art\napproaches (Karpukhin et al., 2020; Guu et al.,\n2020) with dense representations of the original\nqueries, while being more lightweight and efﬁcient\nin terms of both training and inference (including\nthe cost of the generation model) (Sec. 6.4).\nSpeciﬁcally, we expand the query (question) by\nadding relevant contexts as follows. We conduct\nseq2seq learning with the question as the input\nand various freely accessible in-domain contexts as\nthe output such as the answer, the sentence where\nthe answer belongs to , and the title of a passage\nthat contains the answer. We then append the gen-\nerated contexts to the question as the generation-\naugmented query for retrieval. We demonstrate\nthat using multiple contexts from diverse gener-\nation targets is beneﬁcial as fusing the retrieval\nresults of different generation-augmented queries\nconsistently yields better retrieval accuracy.\nWe conduct extensive experiments on the Nat-\nural Questions (NQ) (Kwiatkowski et al., 2019)\nand TriviaQA (Trivia) (Joshi et al., 2017) datasets.\nThe results reveal four major advantages of GAR:\n(1) GAR, combined with BM25, achieves signif-\nicant gains over the same BM25 model that uses\nthe original queries or existing unsupervised query\nexpansion (QE) methods. (2) GAR with sparse rep-\nresentations (BM25) achieves comparable or even\nbetter performance than the current state-of-the-art\nretrieval methods, such as DPR (Karpukhin et al.,\n2020), that use dense representations. (3) Since\nGAR uses sparse representations to measure lexical\noverlap2, it is complementary to dense representa-\ntions: by fusing the retrieval results of GAR and\nDPR, we obtain consistently better performance\nthan either method used individually. (4) GAR\noutperforms DPR in the end-to-end QA perfor-\nmance (EM) when the same extractive reader is\nused: EM=41.8 (43.8 when combining with DPR)\n2Strictly speaking, GAR with sparse representations han-\ndles semantics before retrieval by enriching the queries, while\nmaintaining the advantage of exact matching.\non NQ and 62.7 on Trivia, creating new state-of-\nthe-art results for extractive OpenQA. GAR also\noutperforms other retrieval methods under the gen-\nerative setup when the same generative reader is\nused: EM=38.1 (45.3 when combining with DPR)\non NQ and 62.2 on Trivia.\nContributions. (1) We propose Generation-\nAugmented Retrieval ( GAR), which augments\nqueries with heuristically discovered relevant con-\ntexts through text generation without external su-\npervision or time-consuming downstream feedback.\n(2) We show that using generation-augmented\nqueries achieves signiﬁcantly better retrieval and\nQA results than using the original queries or ex-\nisting unsupervised QE methods. (3) We show\nthat GAR, combined with a simple BM25 model,\nachieves new state-of-the-art performance on two\nbenchmark datasets in extractive OpenQA and com-\npetitive results in the generative setting.\n2 Related Work\nConventional Query Expansion . GAR shares\nsome merits with query expansion (QE) meth-\nods based on pseudo relevance feedback (Rocchio,\n1971; Abdul-Jaleel et al., 2004; Lv and Zhai, 2010)\nin that they both expand the queries with relevant\ncontexts (terms) without the use of external super-\nvision. GAR is superior as it expands the queries\nwith knowledge stored in the PLMs rather than\nthe retrieved passages and its expanded terms are\nlearned through text generation.\nRecent Query Reformulation. There are recent\nor concurrent studies (Nogueira and Cho, 2017;\nZaiem and Sadat, 2019; Yu et al., 2020; Vaku-\nlenko et al., 2020; Lin et al., 2020) that reformu-\nlate queries with generation models for other re-\ntrieval tasks. However, these studies are not eas-\nily applicable or efﬁcient enough for OpenQA be-\ncause: (1) They require external resources such as\nparaphrase data (Zaiem and Sadat, 2019), search\nsessions (Yu et al., 2020), or conversational con-\ntexts (Lin et al., 2020; Vakulenko et al., 2020)\nto form the reformulated queries, which are not\navailable or showed inferior domain-transfer per-\nformance in OpenQA (Zaiem and Sadat, 2019);\n(2) They involve time-consuming training process\nsuch as RL. For example, Nogueira and Cho (2017)\nreported a training time of 8 to 10 days as it uses\nretrieval performance in the reward function and\nconducts retrieval at each iteration. In contrast,\nGAR uses freely accessible in-domain contexts like\n4091\npassage titles as the generation targets and standard\nseq2seq learning, which, despite its simplicity, is\nnot only more efﬁcient but effective for OpenQA.\nRetrieval for OpenQA. Existing sparse retrieval\nmethods for OpenQA (Chen et al., 2017) solely rely\non the information of the questions. GAR extends\nto contexts relevant to the questions by extracting\ninformation inside PLMs and helps sparse meth-\nods achieve comparable or better performance than\ndense methods (Guu et al., 2020; Karpukhin et al.,\n2020), while enjoying the simplicity and efﬁciency\nof sparse representations. GAR can also be used\nwith dense representations to seek for even better\nperformance, which we leave as future work.\nGenerative QA. Generative QA generates answers\nthrough seq2seq learning instead of extracting an-\nswer spans. Recent studies on generative OpenQA\n(Lewis et al., 2020a; Min et al., 2020; Izacard and\nGrave, 2020) are orthogonal to GAR in that they\nfocus on improving the reading stage and directly\nreuse DPR (Karpukhin et al., 2020) as the retriever.\nUnlike generative QA, the goal of GAR is not to\ngenerate perfect answers to the questions but perti-\nnent contexts that are helpful for retrieval. Another\nline in generative QA learns to generate answers\nwithout relevant passages as the evidence but solely\nthe question itself using PLMs (Roberts et al., 2020;\nBrown et al., 2020). GAR further conﬁrms that one\ncan extract factual knowledge from PLMs, which\nis not limited to the answers as in prior studies but\nalso other relevant contexts.\n3 Generation-Augmented Retrieval\n3.1 Task Formulation\nOpenQA aims to answer factoid questions with-\nout pre-speciﬁed domains. We assume that a large\ncollection of documents C (i.e., Wikipedia) are\ngiven as the resource to answer the questions and\na retriever-reader architecture is used to tackle the\ntask, where the retriever retrieves a small subset\nof the documents D ⊂C and the reader reads the\ndocuments D to extract (or generate) an answer.\nOur goal is to improve the effectiveness and efﬁ-\nciency of the retriever and consequently improve\nthe performance of the reader.\n3.2 Generation of Query Contexts\nIn GAR, queries are augmented with various heuris-\ntically discovered relevant contexts in order to re-\ntrieve more relevant passages in terms of both quan-\ntity and quality. For the task of OpenQA where the\nquery is a question, we take the following three\nfreely accessible contexts as the generation targets.\nWe show in Sec. 6.2 that having multiple gener-\nation targets is helpful in that fusing their results\nconsistently brings better retrieval accuracy.\nContext 1: The default target (answer). The de-\nfault target is the label in the task of interest, which\nis the answer in OpenQA. The answer to the ques-\ntion is apparently useful for the retrieval of relevant\npassages that contain the answer itself. As shown\nin previous work (Roberts et al., 2020; Brown et al.,\n2020), PLMs are able to answer certain questions\nsolely by taking the questions as input (i.e., closed-\nbook QA). Instead of using the generated answers\ndirectly as in closed-book QA, GAR treats them\nas contexts of the question for retrieval. The ad-\nvantage is that even if the generated answers are\npartially correct (or even incorrect), they may still\nbeneﬁt retrieval as long as they are relevant to the\npassages that contain the correct answers (e.g., co-\noccur with the correct answers).\nContext 2: Sentence containing the default tar-\nget. The sentence in a passage that contains the\nanswer is used as another generation target. Sim-\nilar to using answers as the generation target, the\ngenerated sentences are still beneﬁcial for retriev-\ning relevant passages even if they do not contain\nthe answers, as their semantics is highly related to\nthe questions/answers (examples in Sec. 6.1). One\ncan take the relevant sentences in the ground-truth\npassages (if any) or those in the positive passages\nof a retriever as the reference, depending on the\ntrade-off between reference quality and diversity.\nContext 3: Title of passage containing the de-\nfault target. One can also use the titles of rele-\nvant passages as the generation target if available.\nSpeciﬁcally, we retrieve Wikipedia passages using\nBM25 with the question as the query, and take the\npage titles of positive passages that contain the an-\nswers as the generation target. We observe that\nthe page titles of positive passages are often entity\nnames of interest, and sometimes (but not always)\nthe answers to the questions. Intuitively, if GAR\nlearns which Wikipedia pages the question is re-\nlated to, the queries augmented by the generated\ntitles would naturally have a better chance of re-\ntrieving those relevant passages.\nWhile it is likely that some of the generated\nquery contexts involve unfaithful or nonfactual in-\nformation due to hallucination in text generation\n(Mao et al., 2020) and introduce noise during re-\n4092\ntrieval, they are beneﬁcial rather than harmful over-\nall, as our experiments show that GAR improve\nboth retrieval and QA performance over BM25 sig-\nniﬁcantly. Also, since we generate 3 different (com-\nplementary) query contexts and fuse their retrieval\nresults, the distraction of hallucinated content is\nfurther alleviated.\n3.3 Retrieval with Generation-Augmented\nQueries\nAfter generating the contexts of a query, we append\nthem to the query to form a generation-augmented\nquery.3 We observe that conducting retrieval with\nthe generated contexts ( e.g., answers) alone as\nqueries instead of concatenation is ineffective be-\ncause (1) some of the generated answers are rather\nirrelevant, and (2) a query consisting of the correct\nanswer alone (without the question) may retrieve\nfalse positive passages with unrelated contexts that\nhappen to contain the answer. Such low-quality\npassages may lead to potential issues in the follow-\ning passage reading stage.\nIf there are multiple query contexts, we conduct\nretrieval using queries with different generated con-\ntexts separately and then fuse their results. The per-\nformance of one-time retrieval with all the contexts\nappended is slightly but not signiﬁcantly worse.\nFor simplicity, we fuse the retrieval results in a\nstraightforward way: an equal number of passages\nare taken from the top-retrieved passages of each\nsource. One may also use weighted or more so-\nphisticated fusion strategies such as reciprocal rank\nfusion (Cormack et al., 2009), the results of which\nare slightly better according to our experiments.4\nNext, one can use any off-the-shelf retriever for\npassage retrieval. Here, we use a simple BM25\nmodel to demonstrate that GAR with sparse repre-\nsentations can already achieve comparable or better\nperformance than state-of-the-art dense methods\nwhile being more lightweight and efﬁcient (includ-\ning the cost of the generation model), closing the\ngap between sparse and dense retrieval methods.\n4 OpenQA with G AR\nTo further verify the effectiveness of GAR, we\nequip it with both extractive and generative read-\ners for end-to-end QA evaluation. We follow the\n3One may create a title ﬁeld during document indexing\nand conduct multi-ﬁeld retrieval but here we append the titles\nto the questions as other query contexts for generalizability.\n4We use the fusion tools at https://github.com/\njoaopalotti/trectools.\nreader design of the major baselines for a fair com-\nparison, while virtually any existing QA reader can\nbe used with GAR.\n4.1 Extractive Reader\nFor the extractive setup, we largely follow the de-\nsign of the extractive reader in DPR (Karpukhin\net al., 2020). Let D = [d1, d2, ..., dk] denote the list\nof retrieved passages with passage relevance scores\nD. Let Si = [s1, s2, ..., sN ] denote the top N text\nspans in passagedi ranked by span relevance scores\nSi. Brieﬂy, the DPR reader uses BERT-base (De-\nvlin et al., 2019) for representation learning, where\nit estimates the passage relevance score Dk for\neach retrieved passage dk based on the [CLS] to-\nkens of all retrieved passages D, and assigns span\nrelevance scores Si for each candidate span based\non the representations of its start and end tokens.\nFinally, the span with the highest span relevance\nscore from the passage with the highest passage rel-\nevance score is chosen as the answer. We refer the\nreaders to Karpukhin et al. (2020) for more details.\nPassage-level Span Voting. Many extractive QA\nmethods (Chen et al., 2017; Min et al., 2019b; Guu\net al., 2020; Karpukhin et al., 2020) measure the\nprobability of span extraction in different retrieved\npassages independently, despite that their collec-\ntive signals may provide more evidence in deter-\nmining the correct answer. We propose a simple\nyet effective passage-level span voting mechanism,\nwhich aggregates the predictions of the spans in\nthe same surface form from different retrieved pas-\nsages. Intuitively, if a text span is considered as the\nanswer multiple times in different passages, it is\nmore likely to be the correct answer. Speciﬁcally,\nGAR calculates a normalized score p(Si[j]) for the\nj-th span in passage di during inference as follows:\np(Si[j]) =softmax(D)[i] ×softmax(Si)[j]. GAR\nthen aggregates the scores of the spans with the\nsame surface string among all the retrieved pas-\nsages as the collective passage-level score.5\n4.2 Generative Reader\nFor the generative setup, we use a seq2seq frame-\nwork where the input is the concatenation of the\nquestion and top-retrieved passages and the target\noutput is the desired answer. Such generative read-\ners are adopted in recent methods such as SpanSe-\n5We ﬁnd that the number of spans used for normalization\nin each passage does not have signiﬁcant impact on the ﬁnal\nperformance (we takeN = 5) and using the raw or normalized\nstrings for aggregation also perform similarly.\n4093\nqGen (Min et al., 2020) and Longformer (Belt-\nagy et al., 2020). Speciﬁcally, we use BART-large\n(Lewis et al., 2019) as the generative reader, which\nconcatenates the question and top-retrieved pas-\nsages up to its length limit (1,024 tokens, 7.8 pas-\nsages on average). Generative GAR is directly com-\nparable with SpanSeqGen (Min et al., 2020) that\nuses the retrieval results of DPR but not comparable\nwith Fusion-in-Decoder (FID) (Izacard and Grave,\n2020) since it encodes 100 passages rather than\n1,024 tokens and involves more model parameters.\n5 Experiment Setup\n5.1 Datasets\nWe conduct experiments on the open-domain ver-\nsion of two popular QA benchmarks: Natural Ques-\ntions (NQ) (Kwiatkowski et al., 2019) and Trivi-\naQA (Trivia) (Joshi et al., 2017). The statistics of\nthe datasets are listed in Table 1.\nDataset Train / Val / Test Q-len A-len #-A\nNQ 79,168 / 8,757 / 3,610 12.5 5.2 1.2\nTrivia 78,785 / 8,837 / 11,313 20.2 5.5 13.7\nTable 1: Dataset statistics that show the number of sam-\nples per data split, the average question (answer) length,\nand the number of answers for each question.\n5.2 Evaluation Metrics\nFollowing prior studies (Karpukhin et al., 2020),\nwe use top-k retrieval accuracy to evaluate the per-\nformance of the retriever and the Exact Match (EM)\nscore to measure the performance of the reader.\nTop-k retrieval accuracy is deﬁned as the pro-\nportion of questions for which the top-k retrieved\npassages contain at least one answer span, which\nis an upper bound of how many questions are “an-\nswerable” by an extractive reader.\nExact Match (EM) is the proportion of the pre-\ndicted answer spans being exactly the same as (one\nof) the ground-truth answer(s), after string normal-\nization such as article and punctuation removal.\n5.3 Compared Methods\nFor passage retrieval, we mainly compare with\nBM25 and DPR, which represent the most used\nstate-of-the-art methods of sparse and dense re-\ntrieval for OpenQA, respectively. For query ex-\npansion, we re-emphasize that GAR is the ﬁrst QE\napproach designed for OpenQA and most of the\nrecent approaches are not applicable or efﬁcient\nenough for OpenQA since they have task-speciﬁc\nobjectives, require external supervision that was\nshown to transfer poorly to OpenQA, or take many\ndays to train (Sec. 2). We thus compare with a clas-\nsic unsupervised QE method RM3 (Abdul-Jaleel\net al., 2004) that does not need external resources\nfor a fair comparison. For passage reading, we\ncompare with both extractive (Min et al., 2019a;\nAsai et al., 2019; Lee et al., 2019; Min et al., 2019b;\nGuu et al., 2020; Karpukhin et al., 2020) and gen-\nerative (Brown et al., 2020; Roberts et al., 2020;\nMin et al., 2020; Lewis et al., 2020a; Izacard and\nGrave, 2020) methods when equipping GAR with\nthe corresponding reader.\n5.4 Implementation Details\nRetriever. We use Anserini (Yang et al., 2017) for\ntext retrieval of BM25 and GAR with its default\nparameters. We conduct grid search for the QE\nbaseline RM3 (Abdul-Jaleel et al., 2004).\nGenerator. We use BART-large (Lewis et al.,\n2019) to generate query contexts in GAR. When\nthere are multiple desired targets (such as multi-\nple answers or titles), we concatenate them with\n[SEP] tokens as the reference and remove the [SEP]\ntokens in the generation-augmented queries. For\nTrivia, in particular, we use the value ﬁeld as the\ngeneration target of answer and observe better per-\nformance. We take the checkpoint with the best\nROUGE-1 F1 score on the validation set, while\nobserving that the retrieval accuracy of GAR is rel-\natively stable to the checkpoint selection since we\ndo not directly use the generated contexts but treat\nthem as augmentation of queries for retrieval.\nReader. Extractive GAR uses the reader of DPR\nwith largely the same hyperparameters, which is\ninitialized with BERT-base (Devlin et al., 2019)\nand takes 100 (500) retrieved passages during train-\ning (inference). Generative GAR concatenates the\nquestion and top-10 retrieved passages, and takes\nat most 1,024 tokens as input. Greedy decoding is\nadopted for all generation models, which appears to\nperform similarly to (more expensive) beam search.\n6 Experiment Results\nWe evaluate the effectiveness of GAR in three\nstages: generation of query contexts (Sec. 6.1),\nretrieval of relevant passages (Sec. 6.2), and pas-\nsage reading for OpenQA (Sec. 6.3). Ablation\nstudies are mostly shown on the NQ dataset to un-\nderstand the drawbacks of GAR since it achieves\n4094\nQuestion: when did bat out of hell get released?\nAnswer: September 1977 {September 1977}\nSentence: Bat Out of Hell is the second studio album and the major - label debut by American rock singer Meat\nLoaf ... released inSeptember 1977on Cleveland International / Epic Records.\n{The album was released in September 1977 on Cleveland International / Epic Records.}\nTitle: Bat Out of Hell {Bat Out of Hell}\nQuestion: who sings does he love me with reba?\nAnswer: Brooks & Dunn {Linda Davis}\nSentence: Linda Kaye Davis( born November 26, 1962 ) is an American country music singer.\n{“ Does He Love You ” is a song written by Sandy Knox and Billy Stritch, and recorded as a duet by American\ncountry music artists Reba McEntire and Linda Davis.}\nTitle: Does He Love Me[SEP]Does He Love Me (Reba McEntire song)[SEP] I Do (Reba McEntire album)\n{Linda Davis [SEP] Greatest Hits V olume Two (Reba McEntire album) [SEP] Does He Love You}\nQuestion: what is the name of wonder womans mother?\nAnswer: Mother Magda {Queen Hippolyta}\nSentence: In the Amazonian myths, she is the daughter of the Amazon queen Sifrat and the male dwarf Shuri,\nand is the mother of Wonder Woman.{Wonder Woman’s origin story relates that she was sculpted from clay\nby her mother Queen Hippolyta and given life by Aphrodite.}\nTitle: Wonder Woman[SEP]Diana Prince[SEP]Wonder Woman (2011 TV pilot)\n{Wonder Woman [SEP] Orana (comics) [SEP] Wonder Woman (TV series)}\nTable 2: Examples of generated query contexts. The issue of generating wrong answers is alleviated by generat-\ning other contexts highly related to the question/answer. Ground-truth references are shown in the {braces}.\nbetter performance on Trivia.\n6.1 Query Context Generation\nAutomatic Evaluation. To evaluate the quality\nof the generated query contexts, we ﬁrst measure\ntheir lexical overlap with the ground-truth query\ncontexts. As suggested by the nontrivial ROUGE\nscores in Table 3, GAR does learn to generate\nmeaningful query contexts that could help the re-\ntrieval stage. We next measure the lexical overlap\nbetween the query and the ground-truth passage.\nThe ROUGE-1/2/L F1 scores between the original\nquery and ground-truth passage are 6.00/2.36/5.01,\nand those for the generation-augmented query are\n7.05/2.84/5.62 (answer), 13.21/6.99/10.27 (sen-\ntence), 7.13/2.85/5.76 (title) on NQ, respectively.\nSuch results further demonstrate that the generated\nquery contexts signiﬁcantly increase the word over-\nlap between the queries and the positive passages,\nand thus are likely to improve retrieval results.6\nContext ROUGE-1 ROUGE-2 ROUGE-L\nAnswer 33.51 20.54 33.30\nSentence 37.14 24.71 33.91\nTitle 43.20 32.11 39.67\nTable 3: ROUGE F1 scores of the generated query\ncontexts on the validation set of the NQ dataset.\n6We use F1 instead of recall to avoid the unfair favor of\n(longer) generation-augmented query.\nCase Studies . In Table 2, we show several ex-\namples of the generated query contexts and their\nground-truth references. In the ﬁrst example, the\ncorrect album release date appears in both the gen-\nerated answer and the generated sentence, and the\ngenerated title is the same as the Wikipedia page\ntitle of the album. In the last two examples, the\ngenerated answers are wrong but fortunately, the\ngenerated sentences contain the correct answer and\n(or) other relevant information and the generated\ntitles are highly related to the question as well,\nwhich shows that different query contexts are com-\nplementary to each other and the noise during query\ncontext generation is thus reduced.\n6.2 Generation-Augmented Retrieval\nComparison w. the state-of-the-art . We next\nevaluate the effectiveness of GAR for retrieval.\nIn Table 4, we show the top-k retrieval accuracy\nof BM25, BM25 with query expansion (+RM3)\n(Abdul-Jaleel et al., 2004), DPR (Karpukhin et al.,\n2020), GAR, and GAR +DPR.\nOn the NQ dataset, while BM25 clearly under-\nperforms DPR regardless of the number of retrieved\npassages, the gap between GAR and DPR is signiﬁ-\ncantly smaller and negligible whenk ≥100. When\nk ≥500, GAR is slightly better than DPR despite\nthat it simply uses BM25 for retrieval. In con-\ntrast, the classic QE method RM3, while showing\n4095\nMethod NQ Trivia\nTop-5 Top-20 Top-100 Top-500 Top-1000Top-5 Top-20 Top-100 Top-500 Top-1000\nBM25 (ours) 43.6 62.9 78.1 85.5 87.8 67.7 77.3 83.9 87.9 88.9\nBM25 +RM3 44.6 64.2 79.6 86.8 88.9 67.0 77.1 83.8 87.7 88.9\nDPR 68.3 80.1 86.1 90.3 91.2 72.7 80.2 84.8 - -\nGAR 60.9 74.4 85.3 90.3 91.7 73.1 80.4 85.7 88.9 89.7\nGAR+DPR 70.7 81.6 88.9 92.0 93.2 76.0 82.1 86.6 - -\nTable 4: Top-k retrieval accuracy on the test sets . All baselines are evaluated by ourselves and better than\nreported in Karpukhin et al. (2020). G AR helps BM25 to achieve comparable or better performance than DPR.\nmarginal improvement over the vanilla BM25, does\nnot achieve comparable performance with GAR or\nDPR. By fusing the results of GAR and DPR in\nthe same way as described in Sec. 3.3, we further\nobtain consistently higher performance than both\nmethods, with top-100 accuracy 88.9% and top-\n1000 accuracy 93.2%.\nOn the Trivia dataset, the results are even more\nencouraging – GAR achieves consistently better\nretrieval accuracy than DPR when k ≥ 5. On\nthe other hand, the difference between BM25 and\nBM25 +RM3 is negligible, which suggests that\nnaively considering top-ranked passages as relevant\n(i.e., pseudo relevance feedback) for QE does not\nalways work for OpenQA. Results on more cutoffs\nof k can be found in App. A.\nEffectiveness of diverse query contexts . In\nFig. 1, we show the performance of GAR when\ndifferent query contexts are used to augment the\nqueries. Although the individual performance\nwhen using each query context is somewhat similar,\nfusing their retrieved passages consistently leads\nto better performance, conﬁrming that different\ngeneration-augmented queries are complementary\nto each other (recall examples in Table 2).\nPerformance breakdown by question type . In\nTable 5, we show the top-100 accuracy of the com-\npared retrieval methods per question type on the\nNQ test set. Again, GAR outperforms BM25 on\nall types of questions signiﬁcantly and GAR +DPR\nachieves the best performance across the board,\nwhich further veriﬁes the effectiveness of GAR.\n6.3 Passage Reading with G AR\nComparison w. the state-of-the-art . We show\nthe comparison of end-to-end QA performance of\nextractive and generative methods in Table 6. Ex-\ntractive GAR achieves state-of-the-art performance\namong extractive methods on both NQ and Trivia\ndatasets, despite that it is more lightweight and\ncomputationally efﬁcient. Generative GAR outper-\n1 5 10 20 50 100 200 300 500 1000\nk: # of retrieved passages\n30\n40\n50\n60\n70\n80\n90Top-k Accuracy (%)\nAnswer+Sentence+Title\nAnswer+Sentence\nAnswer+Title\nAnswer\nTitle\nSentence\nFigure 1: Top-k retrieval accuracy on the test\nset of NQ when fusing retrieval results of different\ngeneration-augmented queries.\nType Percentage BM25 DPR GAR GAR+DPR\nWho 37.5% 82.1 88.0 87.5 90.8\nWhen 19.0% 73.1 86.9 83.8 88.6\nWhat 15.0% 76.5 82.6 81.5 86.0\nWhere 10.9% 77.4 89.1 87.0 90.8\nOther 9.1% 79.3 78.1 81.8 84.2\nHow 5.0% 78.2 83.8 83.2 85.5\nWhich 3.3% 89.0 90.7 94.1 94.9\nWhy 0.3% 90.0 90.0 90.0 90.0\nTable 5: Top-100 retrieval accuracy breakdown of\nquestion type on NQ . Best and second best methods\nin each category are bold and underlined, respectively.\nforms most of the generative methods on Trivia but\ndoes not perform as well on NQ, which is some-\nwhat expected and consistent with the performance\nat the retrieval stage, as the generative reader only\ntakes a few passages as input and GAR does not\noutperform dense retrieval methods on NQ when k\nis very small. However, combining GAR with DPR\nachieves signiﬁcantly better performance than both\nmethods or baselines that use DPR as input such as\nSpanSeqGen (Min et al., 2020) and RAG (Lewis\net al., 2020a). Also, GAR outperforms BM25 sig-\nniﬁcantly under both extractive and generative se-\n4096\nMethod NQ Trivia\nExtractive\nHard EM (Min et al., 2019a) 28.1 50.9 -\nPath Retriever (Asai et al., 2019) 32.6 - -\nORQA (Lee et al., 2019) 33.3 45.0 -\nGraph Retriever (Min et al., 2019b) 34.5 56.0 -\nREALM (Guu et al., 2020) 40.4 - -\nDPR (Karpukhin et al., 2020) 41.5 57.9 -\nBM25 (ours) 37.7 60.1 -\nGAR 41.8 62.7 74.8\nGAR+DPR 43.8 - -\nGenerative\nGPT-3 (Brown et al., 2020) 29.9 - 71.2\nT5 (Roberts et al., 2020) 36.6 60.5 -\nSpanSeqGen (Min et al., 2020) 42.2 - -\nRAG (Lewis et al., 2020a) 44.5 56.1 68.0\nFID (Izacard and Grave, 2020)51.4 67.6 80.1\nBM25 (ours) 35.3 58.6 -\nGAR 38.1 62.2 -\nGAR+DPR 45.3 - -\nTable 6: End-to-end comparison with the state-of-\nthe-art methods in EM . For Trivia, the left column\ndenotes the open-domain test set and the right is the\nhidden Wikipedia test set on the public leaderboard.\ntups, which again shows the effectiveness of the\ngenerated query contexts, even if they are heuristi-\ncally discovered without any external supervision.\nThe best performing generative method FID\n(Izacard and Grave, 2020) is not directly compara-\nble as it takes more (100) passages as input. As an\nindirect comparison, GAR performs better than FID\nwhen FID encodes 10 passages (cf. Fig. 2 in Izac-\nard and Grave (2020)). Moreover, since FID relies\non the retrieval results of DPR as well, we believe\nthat it is a low-hanging fruit to replace its input with\nGAR or GAR +DPR and further boost the perfor-\nmance.7 We also observe that, perhaps surprisingly,\nextractive BM25 performs reasonably well, espe-\ncially on the Trivia dataset, outperforming many\nrecent state-of-the-art methods.8 Generative BM25\nalso performs competitively in our experiments.\nModel Generalizability. Recent studies (Lewis\net al., 2020b) show that there are signiﬁcant ques-\ntion and answer overlaps between the training and\ntest sets of popular OpenQA datasets. Speciﬁcally,\n60% to 70% test-time answers also appear in the\ntraining set and roughly 30% test-set questions\nhave a near-duplicate paraphrase in the training\nset. Such observations suggest that many questions\nmight have been answered by simple question or\n7This claim is later veriﬁed by the best systems in the\nNeurIPS 2020 EfﬁcientQA competition (Min et al., 2021).\n8We ﬁnd that taking 500 passages during reader inference\ninstead of 100 as in Karpukhin et al. (2020) improves the\nperformance of BM25 but not DPR.\nanswer memorization. To further examine model\ngeneralizability, we study the per-category perfor-\nmance of different methods using the annotations\nin Lewis et al. (2020b).\nMethod Total Question\nOverlap\nAnswer\nOverlap\nOnly\nNo\nOverlap\nDPR 41.3 69.4 34.6 19.3\nGAR+DPR (E)43.8 66.7 38.1 23.9\nBART 26.5 67.6 10.2 0.8\nRAG 44.5 70.7 34.9 24.8\nGAR+DPR (G)45.3 67.9 38.1 27.0\nTable 7: EM scores with question-answer overlap\ncategory breakdown on NQ. (E) and (G) denote ex-\ntractive and generative readers, respectively. Results of\nbaseline methods are taken from Lewis et al. (2020b).\nThe observations on Trivia are similar and omitted.\nAs listed in Table 7, for theNo Overlap category,\nGAR +DPR (E) outperforms DPR on the extractive\nsetup and GAR +DPR (G) outperforms RAG on the\ngenerative setup, which indicates that better end-\nto-end model generalizability can be achieved by\nadding GAR for retrieval. GAR +DPR also achieves\nthe best EM under the Answer Overlap Only cat-\negory. In addition, we observe that a closed-book\nBART model that only takes the question as input\nperforms much worse than additionally taking top-\nretrieved passages, i.e., GAR +DPR (G), especially\non the questions that require generalizability. No-\ntably, all methods perform signiﬁcantly better on\nthe Question Overlap category, which suggests that\nthe high TotalEM is mostly contributed by question\nmemorization. That said, GAR +DPR appears to\nbe less dependent on question memorization given\nits lower EM for this category.9\n6.4 Efﬁciency of G AR\nGAR is efﬁcient and scalable since it uses sparse\nrepresentations for retrieval and does not in-\nvolve time-consuming training process such as\nRL (Nogueira and Cho, 2017; Liu et al., 2019).\nThe only overhead of GAR is on the generation of\nquery contexts and the retrieval with generation-\naugmented (thus longer) queries, whose computa-\ntional complexity is signiﬁcantly lower than other\nmethods with comparable retrieval accuracy.\nWe use Nvidia V100 GPUs and Intel Xeon Plat-\ninum 8168 CPUs in our experiments. As listed in\n9The same ablation study is also conducted on the retrieval\nstage and similar results are observed. More detailed discus-\nsions can be found in App. A.\n4097\nTraining Indexing Retrieval\nDPR 24h w. 8 GPUs 17.3h w. 8 GPUs 30 min w. 1 GPU\nGAR 3∼6h w. 1 GPU 0.5h w. 35 CPUs 5 min w. 35 CPUs\nTable 8: Comparison of computational cost between\nDPR and G AR at different stages. The training time\nof GAR is for one generation target but different gener-\nators can be trained in parallel.\nTable 8, the training time of GAR is 3 to 6 hours\non 1 GPU depending on the generation target. As\na comparison, REALM (Guu et al., 2020) uses\n64 TPUs to train for 200k steps during pre-training\nalone and DPR (Karpukhin et al., 2020) takes about\n24 hours to train with 8 GPUs. To build the indices\nof Wikipedia passages, GAR only takes around 30\nmin with 35 CPUs, while DPR takes 8.8 hours\non 8 GPUs to generate dense representations and\nanother 8.5 hours to build the FAISS index (John-\nson et al., 2017). For retrieval, GAR takes about\n1 min to generate one query context with 1 GPU,\n1 min to retrieve 1,000 passages for the NQ test\nset with answer/title-augmented queries and 2 min\nwith sentence-augmented queries using 35 CPUs.\nIn contrast, DPR takes about 30 min on 1 GPU.\n7 Conclusion\nIn this work, we propose Generation-Augmented\nRetrieval and demonstrate that the relevant contexts\ngenerated by PLMs without external supervision\ncan signiﬁcantly enrich query semantics and im-\nprove retrieval accuracy. Remarkably, GAR with\nsparse representations performs similarly or better\nthan state-of-the-art methods based on the dense\nrepresentations of the original queries. GAR can\nalso be easily combined with dense representa-\ntions to produce even better results. Furthermore,\nGAR achieves state-of-the-art end-to-end perfor-\nmance on extractive OpenQA and competitive per-\nformance under the generative setup.\n8 Future Extensions\nPotential improvements . There is still much\nspace to explore and improve for GAR in future\nwork. For query context generation, one can ex-\nplore multi-task learning to further reduce computa-\ntional cost and examine whether different contexts\ncan mutually enhance each other when generated\nby the same generator. One may also sample multi-\nple contexts instead of greedy decoding to enrich a\nquery. For retrieval, one can adopt more advanced\nfusion techniques based on both the ranking and\nscore of the passages. As the generator and re-\ntriever are largely independent now, it is also inter-\nesting to study how to jointly or iteratively optimize\ngeneration and retrieval such that the generator is\naware of the retriever and generates query contexts\nmore beneﬁcial for the retrieval stage. Last but not\nleast, it is very likely that better results can be ob-\ntained by more extensive hyper-parameter tuning.\nApplicability to other tasks . Beyond OpenQA,\nGAR also has great potentials for other tasks that\ninvolve text matching such as conversation utter-\nance selection (Lowe et al., 2015; Dinan et al.,\n2020) or information retrieval (Nguyen et al., 2016;\nCraswell et al., 2020). The default generation tar-\nget is always available for supervised tasks. For\nexample, for conversation utterance selection one\ncan use the reference utterance as the default target\nand then match the concatenation of the conversa-\ntion history and the generated utterance with the\nprovided utterance candidates. For article search,\nthe default target could be (part of) the ground-truth\narticle itself. Other generation targets are more task-\nspeciﬁc and can be designed as long as they can\nbe fetched from the latent knowledge inside PLMs\nand are helpful for further text retrieval (matching).\nNote that by augmenting (expanding) the queries\nwith heuristically discovered relevant contexts ex-\ntracted from PLMs instead of reformulating them,\nGAR bypasses the need for external supervision to\nform the original-reformulated query pairs.\nAcknowledgments\nWe thank Vladimir Karpukhin, Sewon Min, Gau-\ntier Izacard, Wenda Qiu, Revanth Reddy, and Hao\nCheng for helpful discussions. We thank the anony-\nmous reviewers for valuable comments.\nReferences\nNasreen Abdul-Jaleel, James Allan, W Bruce Croft,\nFernando Diaz, Leah Larkey, Xiaoyan Li, Mark D\nSmucker, and Courtney Wade. 2004. Umass at trec\n2004: Novelty and hard. Computer Science Depart-\nment Faculty Publication Series, page 189.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2019. Learn-\ning to retrieve reasoning paths over wikipedia\ngraph for question answering. arXiv preprint\narXiv:1911.10470.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\n4098\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1870–\n1879, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nGordon V Cormack, Charles LA Clarke, and Stefan\nBuettcher. 2009. Reciprocal rank fusion outper-\nforms condorcet and individual rank learning meth-\nods. In Proceedings of the 32nd international ACM\nSIGIR conference on Research and development in\ninformation retrieval, pages 758–759.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\nCampos, and Ellen M V oorhees. 2020. Overview\nof the trec 2019 deep learning track. arXiv preprint\narXiv:2003.07820.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\nAlexander Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\nLowe, et al. 2020. The second conversational in-\ntelligence challenge (convai2). In The NeurIPS’18\nCompetition, pages 187–208. Springer.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\nGautier Izacard and Edouard Grave. 2020. Lever-\naging passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\nBillion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601–1611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\nWu, Sergey Edunov, Danqi Chen, and Wen-\ntau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics, 7:452–466.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6086–6096, Florence,\nItaly. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\nRockt¨aschel, et al. 2020a. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. arXiv\npreprint arXiv:2005.11401.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2020b. Question and answer test-train overlap in\nopen-domain question answering datasets. arXiv\npreprint arXiv:2008.02637.\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\nJimmy Lin. 2020. Query reformulation using query\nhistory for passage retrieval in conversational search.\narXiv preprint arXiv:2005.02230.\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\nPhilip S Yu. 2019. Generative question reﬁnement\nwith deep reinforcement learning in retrieval-based\nqa system. In Proceedings of the 28th ACM Inter-\nnational Conference on Information and Knowledge\nManagement, pages 1643–1652.\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle\nPineau. 2015. The ubuntu dialogue corpus: A large\ndataset for research in unstructured multi-turn dia-\nlogue systems. arXiv preprint arXiv:1506.08909.\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\nMichael Collins. 2020. Sparse, dense, and at-\ntentional representations for text retrieval. arXiv\npreprint arXiv:2005.00181.\n4099\nYuanhua Lv and ChengXiang Zhai. 2010. Positional\nrelevance model for pseudo-relevance feedback. In\nProceedings of the 33rd international ACM SIGIR\nconference on Research and development in infor-\nmation retrieval, pages 579–586.\nYuning Mao, Xiang Ren, Heng Ji, and Jiawei Han.\n2020. Constrained abstractive summarization: Pre-\nserving factual consistency with constrained genera-\ntion. arXiv preprint arXiv:2010.12723.\nSewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi\nChen, Eunsol Choi, Michael Collins, Kelvin Guu,\nHannaneh Hajishirzi, Kenton Lee, Jennimaria Palo-\nmaki, et al. 2021. Neurips 2020 efﬁcientqa compe-\ntition: Systems, analyses and lessons learned. arXiv\npreprint arXiv:2101.00133.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\nproach for weakly supervised question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2851–\n2864, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\ntrieval and reading for open domain question answer-\ning. arXiv preprint arXiv:1911.03868.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. Ambigqa: Answering\nambiguous open-domain questions. arXiv preprint\narXiv:2004.10645.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. Ms marco: A human-generated machine read-\ning comprehension dataset.\nRodrigo Nogueira and Kyunghyun Cho. 2017. Task-\noriented query reformulation with reinforcement\nlearning. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 574–583, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910.\nJoseph Rocchio. 1971. Relevance feedback in in-\nformation retrieval. The Smart retrieval system-\nexperiments in automatic document processing ,\npages 313–323.\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\nand Raviteja Anantha. 2020. Question rewriting for\nconversational question answering. arXiv preprint\narXiv:2004.14652.\nXiao Wang, Craig Macdonald, and Iadh Ounis. 2020.\nDeep reinforced query reformulation for informa-\ntion retrieval. arXiv preprint arXiv:2007.07987.\nPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\nEnabling the use of lucene for information retrieval\nresearch. In Proceedings of the 40th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 1253–1256.\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,\nPaul Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020.\nFew-shot generative conversational query rewriting.\narXiv preprint arXiv:2006.05009.\nSalah Zaiem and Fatiha Sadat. 2019. Sequence to se-\nquence learning for query expansion. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelli-\ngence, Student Abstract Track , volume 33, pages\n10075–10076.\n4100\nA More Analysis of Retrieval\nPerformance\nWe show the detailed results of top-k retrieval accu-\nracy of the compared methods in Figs. 2 and 3.\nGAR performs comparably or better than DPR\nwhen k ≥100 on NQ and k ≥5 on Trivia.\n1 5 10 20 50 100 200 300 500 1000\nk: # of retrieved passages\n20\n30\n40\n50\n60\n70\n80\n90Top-k Accuracy (%)\nGAR +DPR\nDPR\nGAR\nBM25 +RM3\nBM25\nFigure 2: Top-k retrieval accuracy of sparse and\ndense methods on the test set of NQ. GAR improves\nBM25 and achieves comparable or better performance\nthan DPR when k ≥100.\n1 5 10 20 50 100\nk: # of retrieved passages\n50\n55\n60\n65\n70\n75\n80\n85Top-k Accuracy (%)\nGAR +DPR\nDPR\nGAR\nBM25 +RM3\nBM25\nFigure 3: Top-k retrieval accuracy on the Trivia test\nset. GAR achieves better results than DPR whenk ≥5.\nWe show in Table 9 the retrieval accuracy break-\ndown using the question-answer overlap categories.\nThe most signiﬁcant gap between BM25 and other\nmethods is on the Question Overlap category,\nwhich coincides with the fact that BM25 is un-\nable to conduct question paraphrasing (semantic\nmatching). GAR helps BM25 to bridge the gap by\nproviding the query contexts and even outperform\nDPR in this category. Moreover, GAR consistently\nimproves over BM25 on other categories and GAR\n+DPR outperforms DPR as well.\nMethod Total Question\nOverlap\nAnswer\nOverlap\nOnly\nNo\nOverlap\nBM25 78.8 81.2 85.1 70.6\nDPR 86.1 93.2 89.5 76.8\nGAR 85.3 94.1 87.9 73.7\nGAR+DPR 88.9 96.3 91.7 79.8\nTable 9: Top-100 retrieval accuracy by question-\nanswer overlap categories on the NQ test set.",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.6395890116691589
    },
    {
      "name": "Question answering",
      "score": 0.6324785947799683
    },
    {
      "name": "Computer science",
      "score": 0.6297590136528015
    },
    {
      "name": "Natural language processing",
      "score": 0.581676721572876
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5037669539451599
    },
    {
      "name": "Open domain",
      "score": 0.4831511378288269
    },
    {
      "name": "Computational linguistics",
      "score": 0.47299084067344666
    },
    {
      "name": "Information retrieval",
      "score": 0.4710787832736969
    },
    {
      "name": "Linguistics",
      "score": 0.4500744640827179
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42557424306869507
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.42548051476478577
    },
    {
      "name": "Library science",
      "score": 0.38047611713409424
    },
    {
      "name": "Philosophy",
      "score": 0.17667439579963684
    },
    {
      "name": "Mathematics",
      "score": 0.1408177614212036
    },
    {
      "name": "Biology",
      "score": 0.05646142363548279
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}