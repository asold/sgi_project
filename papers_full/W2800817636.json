{
  "title": "Exploring the Functional and Geometric Bias of Spatial Relations Using Neural Language Models",
  "url": "https://openalex.org/W2800817636",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A5014291440",
      "name": "Simon Dobnik",
      "affiliations": [
        "University of Gothenburg"
      ]
    },
    {
      "id": "https://openalex.org/A5065365925",
      "name": "Mehdi Ghanimifard",
      "affiliations": [
        "University of Gothenburg"
      ]
    },
    {
      "id": "https://openalex.org/A5079991004",
      "name": "John D. Kelleher",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251360611",
    "https://openalex.org/W2251848082",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2800199466",
    "https://openalex.org/W114341944",
    "https://openalex.org/W4237036636",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1593539964",
    "https://openalex.org/W1561639296",
    "https://openalex.org/W2147191817",
    "https://openalex.org/W2119775030",
    "https://openalex.org/W2342225866",
    "https://openalex.org/W2149448122",
    "https://openalex.org/W2759709360",
    "https://openalex.org/W2060351906",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2749367827",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3009125560",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2051826382",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2032846947",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2519350824"
  ],
  "abstract": "The challenge for computational models of spatial descriptions for situated dialogue systems is the integration of information from different modalities. The semantics of spatial descriptions are grounded in at least two sources of information: (i) a geometric representation of space and (ii) the functional interaction of related objects that. We train several neural language models on descriptions of scenes from a dataset of image captions and examine whether the functional or geometric bias of spatial descriptions reported in the literature is reflected in the estimated perplexity of these models. The results of these experiments have implications for the creation of models of spatial lexical semantics for human-robot dialogue systems. Furthermore, they also provide an insight into the kinds of the semantic knowledge captured by neural language models trained on spatial descriptions, which has implications for image captioning systems.",
  "full_text": "Proceedings of the First International Workshop on Spatial Language Understanding (SpLU-2018), pages 1–11\nNew Orleans, Louisiana, June 6, 2018.c⃝2018 Association for Computational Linguistics\nExploring the Functional and Geometric Bias of Spatial Relations Using\nNeural Language Models\nSimon Dobnik∗ Mehdi Ghanimifard∗ John D. Kelleher†\n∗CLASP and FLOV , University of Gothenburg, Sweden\n†Dublin Institute of Technology, Ireland\n∗{simon.dobnik,mehdi.ghanimifard}@gu.se †john.d.kelleher@dit.ie\nAbstract\nThe challenge for computational models of\nspatial descriptions for situated dialogue sys-\ntems is the integration of information from\ndifferent modalities. The semantics of spa-\ntial descriptions are grounded in at least two\nsources of information: (i) a geometric repre-\nsentation of space and (ii) the functional inter-\naction of related objects that. We train sev-\neral neural language models on descriptions of\nscenes from a dataset of image captions and\nexamine whether the functional or geometric\nbias of spatial descriptions reported in the liter-\nature is reﬂected in the estimated perplexity of\nthese models. The results of these experiments\nhave implications for the creation of models of\nspatial lexical semantics for human-robot di-\nalogue systems. Furthermore, they also pro-\nvide an insight into the kinds of the semantic\nknowledge captured by neural language mod-\nels trained on spatial descriptions, which has\nimplications for image captioning systems.\n1 Introduction\nSpatial language understanding is fundamental re-\nquirement for human-robot interaction through di-\nalogue. A natural task for a human to request a\nrobot to fulﬁl is to retrieve or replace an object for\nthem. Consequently, a particularly frequent form\nof spatial description within human-robot interac-\ntion is a locative expression. A locative expression\nis a noun phrase that describes the location of one\nobject (the target object) relative to another object\n(the landmark). The relative location of the target\nobject is speciﬁed through a prepositional phrase:\nBring me the big red book  \nTarget\non the table  \nLandmark  \nPrepositional\nPhrase  \nLocative Expression\n.\nIn order to understand these forms of spatial de-\nscriptions a robot must be equipped with compu-\ntational models of the spatial semantics of prepo-\nsitions that enable them to ground the semantics\nof the locative expression relative to the context of\nthe situated dialogue.\nA natural approach to developing these compu-\ntational models is to deﬁne them in terms of scene\ngeometry. And, indeed, there is a tradition of re-\nsearch that follows this path, see for example (Lo-\ngan and Sadler, 1996; Kelleher and Costello, 2005,\n2009). However, there is also a body of experi-\nmental and computational research that has high-\nlighted that the semantics of spatial descriptions\nare dependent on several sources of information\nbeyond scene geometry, including functional se-\nmantics (which encompasses a range of factors\nsuch as world knowledge about the typical inter-\nactions between objects, and object affordances)\n(Coventry and Garrod, 2004). We can illustrate\nthis distinction between geometric and function-\nally deﬁned semantics using a number of exam-\nples. To illustrate a geometric semantics: assum-\ning a spatial meaning, anything can be described\nas to left of anything else so long the spatial con-\nﬁguration of the two objects is geometrically cor-\nrect. However, as (Coventry et al., 2001) has\nshown the spatial description the umbrella is over\nthe man is sensitive to the protective affordances\nof the umbrella to stop rain, and is appropriate in\ncontexts where, the umbrella is not in a geometri-\ncally prototypical position above the man, so long\nas the umbrella is protecting the man from the rain.\nA further complication with regard to modelling\nthe semantics of spatial descriptions is that experi-\nmental results indicate that the contribution of ge-\nometrical and functional factors is not the same for\nevery spatial relation (Garrod et al., 1999; Coven-\ntry et al., 2001). This experimental work shows\nthat there is an interplay between function and ge-\n1\nometry in the deﬁnition of spatial semantics and\ntherefore the spatial meaning of given spatial rela-\ntion is neither fully functional nor fully geometric.\nRather, spatial terms can be ordered on a spectrum\nbased on the sensitivity of their semantics to geo-\nmetric or functional factors.\nGiven the distinction between geometric and\nfunctional factors in shaping spatial semantics,\na useful analysis that would inform the design\nand creation of computational models of spa-\ntial semantics is to identify the particular seman-\ntic bias (geometric/functional) that each spatial\nterm evinces. However, such an analysis is dif-\nﬁcult. Native speakers do not have strong in-\ntuitions about the bias of prepositions and such\nbias had to be established experimentally (Coven-\ntry et al., 2001; Garrod et al., 1999) or through\nlinguistic analysis (Herskovits, 1986, p.55). 1 Re-\nviewing the literature on this experimental and an-\nalytic work reveals that prepositions such as in,\non, at, over, under have been identiﬁed as being\nfunctionally biased, whereas above, below, left of\nand right of are geometrically biased. Other spa-\ntial relations may be somewhere in between. In\nthis paper we will use these relations as ground-\ntruth pointers against which our methods will be\nevaluated. If the method is successful, then we\nare able to make predictions about those relations\nthat have not been veriﬁed for their bias experi-\nmentally. Knowing the bias of a spatial relation\nis useful both theoretically and practically. The-\noretically, it informs us about the complexity of\ngrounded semantics of spatial relations. In par-\nticular, it engages with the “what” and “where”\ndebate where it has been argued that spatial rela-\ntions are not only spatial (i.e. geometric) (Landau\nand Jackendoff, 1993; Coventry and Garrod, 2004;\nLandau, 2016). Practically, the procedure to esti-\nmate the bias is useful for natural language genera-\ntion systems, for example in situated robotic appli-\ncations that cannot be trained end-to-end. Given\nthat a particular pair of objects can be described\ngeometrically with several spatial relations, the\nknowledge of functional bias may be used as a ﬁl-\nter, prioritising those relations that are more likely\nfor a particular pair of objects, thereby incorporat-\n1The discussion of Herskovits focuses on interaction of\nobjects conceptualised as geometric shapes, for example on:\ncontiguity with line or surface. The fact that the inter-\nacting objects can be conceptualised as different geometric\nshapes points and therefore related by a particular preposi-\ntions points to their functional nature as discussed here.\ning functional knowledge. This approach to gen-\neration of spatial descriptions is therefore similar\nto the approach that introduces a cognitive load\nbased hierarchy of spatial relations (Kelleher and\nKruijff, 2006) or a classiﬁcation-based approach\nthat combines geometric (related to the bounding\nbox), textual (word2vec embeddings) and visual\nfeatures (ﬁnal layer of a convolutional network)\n(Ramisa et al., 2015). The functional geometric\nbias of spatial relations could also be used to in-\nform semantic parsing, for example in preposi-\ntional phrase attachment resolution (Christie et al.,\n2016; Delecraz et al., 2017).\nPrevious work has investigated metrics of the\nsemantic bias of spatial prepositions, see (Dobnik\nand Kelleher, 2013, 2014). (Dobnik and Kelle-\nher, 2013) uses (i) normalised entropy of target-\nlandmark pairs to estimate variation of targets and\nlandmarks per relation and (ii) log likelihood ra-\ntio to predict the strength of association of target-\nlandmark pairs with a spatial relation and presents\nranked lists of relations by the degree of argu-\nment variation or strength of the association re-\nspectively. The approach hypothesises that func-\ntionally biased relations are more selective in the\nkind of targets and landmarks they co-occur with.\nThe reasoning behind this is that geometrically it\nis possible to relate a wider range of objects than in\nthe case where additional functional constrains be-\ntween objects are also applied. (Dobnik and Kelle-\nher, 2014) generalises over landmarks and targets\nin WordNet hierarchy and estimates the generality\nof the types of landmark. Again, the work hypoth-\nesises that functional relations are more restricted\nin their choice of target and landmark objects and\ntherefore are generally more speciﬁc in terms of\nthe WordNet hierarchy. Both papers present re-\nsults compatible with the hypotheses where the\nfunctional or geometric nature of prepositions is\npredicted in line with the experimental studies\n(Garrod et al., 1999; Coventry et al., 2001).\nSensitive to the fact that relations such asin and\non not only have spatial usage but also usages that\nmay be considered metaphoric (Steen et al., 2010),\nboth (Dobnik and Kelleher, 2013) and (Dobnik\nand Kelleher, 2014) were based on an analysis of\na corpus of image captions. The idea being that\ndescriptions of images are more likely to contain\nspatial descriptions grounded in the image. For\nsimilar reasons, we also employ a corpus of image\ndescriptions (larger than in the previous work).\n2\nThis paper adopts a similar research hypothe-\nsis to (Dobnik and Kelleher, 2014, 2013), namely\nthat: it is possible to distinguish between function-\nally biased and geometrically biased spatial rela-\ntions by examining the diversity of the contexts in\nwhich they occur. Deﬁning the concept of context\nin terms of the target and landmark object pairs\nthat a relation occurs within, the rationale of this\nhypothesis is that: geometrically biased relations\nare more likely to be observed in a more diverse set\nof contexts, compared to functionally biased rela-\ntions, because the use of a geometrically biased\nrelation only presupposes the appropriate geomet-\nric conﬁguration whereas the use of a functionally\nbiased relation is also constrained by object affor-\ndances or typical interactions.\nHowever, the work presented in this paper pro-\nvides a more general analytical technique based\non a neural language model (Bengio et al., 2003;\nMikolov et al., 2010) which is applied to a larger\ndataset of spatial descriptions. We use neural lan-\nguage models as the basic tool for our analysis\nbecause they are already commonly used to learn\nthe syntax and semantics of words in an unsu-\npervised way. The contribution of this paper in\nrelation to (i) the previous analyses of geomet-\nric and functional aspects of spatial relations is\nthat it examines whether similar predictions can\nbe made using these more general tools of repre-\nsenting meaning of words and phrases; the contri-\nbution to (ii) deep learning of language and vision\nis that it examines to what extent highly speciﬁc\nworld-knowledge can be extracted from a neural\nlanguage model. The paper proceeds as follows:\nin Section 2 we describe the datasets and their pro-\ncessing, in Section 3 we describe the basics behind\nlanguage models and the notion of perplexity, in\nSection 4 and 5 we present and discuss our results.\nWe conclude in Section 6.\nThe code that was used to produce the\ndatasets and results discussed in this paper can\nbe found at: https://github.com/GU-CLASP/\nfunctional-geometric-lm.\n2 Datasets\nThe Amsterdam Metaphor Corpus (Steen et al.,\n2010) which is based on a subsection of a BNC\nreveals that the spatial sense of prepositions are\nvery rare in genres such as news, ﬁction and aca-\ndemic texts. For example, below only has two\ninstances that are not labelled as a metaphor and\nmore than 60% of fragments with in, on, and\nover are not used in their spatial sense. For\nthis reason Dobnik and Kelleher (2013) use two\nimage description corpora (IAPR TC-12 (Grub-\ninger et al., 2006) and Flickr8k (Rashtchian et al.,\n2010)) where spatial uses of prepositions are com-\nmon. They apply a dependency parser and a set\nof post-processing rules to extract spatial relations,\ntarget and landmark object triplets. The size of this\nextracted dataset is 96,749 instances and is rela-\ntively small for training a neural language model.\n(Kordjamshidi et al., 2017) released CLEF 2017\nmultimodal spatial role labelling dataset (mSpRL)\nwhich is a human annotated subset of the IAPR\nTC-12 Benchmark corpus for spatial relations, tar-\ngets and landmarks (Kordjamshidi et al., 2011)\ncontaining 613 text ﬁles and 1,213 sentences.\nWhile this dataset could not be used to train a lan-\nguage model directly, a spatial role labelling clas-\nsiﬁer could be trained on it to identify spatial rela-\ntions and arguments which would then be used to\nproduce a bootstrapped dataset for training a neu-\nral language model.\nRecently, Visual Genome (Krishna et al., 2017)\nhas been released which is a crowd-source an-\nnotated corpus of 108K images which also in-\ncludes annotations of relationships between (pre-\nviously annotated) bounding boxes. Relation-\nships are predicates that relate objects which in-\nclude spatial relations (2404639, “cup on table”),\nverbs (2367163, “girl holding on to bear”) as\nwell as combinations of verbs and spatial relations\n(2317920, “woman standing on snow”) and oth-\ners. We use this dataset in the work reported here.\nIts advantage is that it contains a large number\nof annotated relationships but the disadvantage is\nthat these are collected in a crowd-sourced setting\nand are therefore sometimes noisy but we assume\nthese are still of better quality than those from a\nbootstrapped machine annotated dataset.\nTo extract spatial relations from the annotated\nrelationships, we created a dictionary of their syn-\ntactic forms based on the lists of English spatial\nrelations in Landau (1996) and Herskovits (1986).\nFor the training data we preserve all items an-\nnotated as relationships as single tokens (“jump-\ning over”) and we simplify some of the composite\nspatial relations based on our dictionary, e.g. “left\nof” and “to the left of” become “left” to increase\nthe frequency of instances. This choice could have\naffected our results if done without careful consid-\n3\neration. While compound variants of spatial re-\nlations have slightly different meanings, we only\ncollapsed those relations for which we assumed\nthis would not affect their geometric or functional\nbias. Furthermore, Dobnik and Kelleher (2013)\nshow that compound relations cluster with their\nnon-compound variants using normalised entropy\nof target-landmark pairs as a metric. Finally, some\nvariation was due to the shorthand notation used\nby the annotators, e.g. “to left of”. The reason be-\nhind keeping all relation(ships) in the training set\nis to train the language model on as many targets\nand landmarks as possible and to learn paradig-\nmatic relations between them. We normalise all\nwords to lowercase and remove the duplicate de-\nscriptions per image (created by different anno-\ntators). We also check for and remove instances\nwhere a spatial relation is used as an object, e.g.\n“chair on left”. We remove instances where one of\nthe words has fewer than 100 occurrences in the\nwhole dataset which reduces the dataset size by\n10%. We add start and end tokens to the triplets\n(⟨s⟩target relation landmark ⟨/s⟩) as required for\ntraining and testing a language model. The dataset\nis shufﬂed and split into 10 folds that are later used\nin cross-validation. In the evaluation, we take 20\nsamples per spatial relation from the held out data\nof those relations that are members of the dictio-\nnary created previously. This way the average per-\nplexity is always calculated on the same number\nof samples per each relation.2\n3 Language model and perplexity\n3.1 Language model\nProbabilistic language models capture the sequen-\ntial properties of language or paradigmatic rela-\ntions between sequences of words. Using the\nchain rules of probabilities they estimate the like-\nlihood of a sequence of words:\nP(w1:T ) =\nT\n∑\nt=1\nP(wt+1|w1:t ) (1)\nNeural language models estimate probabilities by\noptimising parameters of a function represented in\na neural architecture (Bengio et al., 2003):\nˆP(wt+1|w1:t = vk1:t ) =f (vt−1;Θ) =ˆyt (2)\n2The reason we use 20 sample is that this is also the size\nof the 10% test folds in the down-sampled dataset described\nlater. In selecting 20 items for the test-set we also ensure that\nit contains the vocabulary in the down-sampled training folds.\nwhere Θ represents parameters of the model, f be-\ning the composition of functions within the neu-\nral network architecture, and vk1:t the words up to\ntime t in the sequence. The output of the function\nis ˆyt ∈Rn, a vector of probabilities, with each di-\nmension representing the probability of a word in\nthe vocabulary. The loss of a recurrent language\nmodel is the average surprisal for each batch of\ndata (Graves et al., 2013; Mikolov et al., 2010):\nloss(S) =−∑\ns∈S\n|s|\n∑\nt=0\nlog(ˆyt (vkt+1 ))\n|S|×|s| (3)\nNote that our architecture is deliberately simple as\nwe apply it in an experimental setting with con-\nstrained descriptions3. We use a Keras implemen-\ntation (Chollet et al., 2015), and ﬁt the model pa-\nrameters with Adam (Kingma and Ba, 2014) with\na batch size of 32 and iterations of 20 epochs. On\neach iteration the language model is optimised on\nthe loss which is related to perplexity as described\nin the following section.\n3.2 Perplexity\nInstead of calculating the averages of likelihoods\nfrom Equation 1, which might get very low on\nlong sequences of text, we use perplexity which\nis an exponential measure for average negative log\nlikelihoods of the model. This solves the repre-\nsentation problem with ﬂoating points and large\nsamples of data.\nPerplexity(S,P) =2ES[−log2(P(w1:T ))] (4)\nwhere w1:T is an instance in a sample collection\nS. Perplexity is often used for evaluating language\nmodels on test sets. Since language models are\noptimised for low perplexities 4, the perplexity of\na trained model can be used as a measure of ﬁt of\nthe model with the samples.\n4 Varying targets and landmarks\n4.1 Hypotheses\nAs a language model encodes semantic relations\nbetween words in a sequence we therefore expect\nthat the distinction between functional and geo-\nmetric spatial relations will also be captured by\n3For more details on the architecture see Section A.1 in\nthe supplementary material, in particular Figure 6 and Equa-\ntion 5.\n4Equation 4 is related to Equation 3 as perplexity is 2Loss\ngiven a neural model as the likelihood model.\n4\nit. As functionally biased spatial relations are used\nin different situational contexts than geometrically\nbiased spatial relations, we expect that a language\nmodel will capture this bias in different distribu-\ntions of target and landmark objects in the forms\nof the perplexity of phrases. Our weak hypothesis\nis that the perplexity of phrases on the test set re-\nﬂects the functional-geometric bias of a spatial re-\nlation (Hypothesis 1). We take the assumption that\nfunctionally-biased relations are more selective in\nterms of their target and landmark choice (Sec-\ntion 1) and consequently sequences such as <s>\ntarget relation landmark </s> with func-\ntional relations have a higher predictability in the\ndataset resulting in a lower perplexity in the lan-\nguage model (Hypothesis 2). Related to this hy-\npothesis, there is a stronger hypothesis that target\nand landmark are predictable with a given func-\ntional spatial relation (Hypothesis 3).\n4.2 Method\nWe train two language models as described in\nSection 3.1. For training and evaluation 10-fold\ncross-validation is used and average results are re-\nported. We ensure that the evaluation sets con-\ntain no vocabulary not seen during the training.\nThe language model 1 (LM1) is trained on unre-\nstricted frequencies of instances. In training the\nlanguage model 2 (LM2) we down-sample rela-\ntions so that they are represented with equal fre-\nquencies. The dataset to train LM2 contains 200\ninstances of each possible relations while the eval-\nuation set contains 20 instances for each spatial\nrelation. Note that using this method some tar-\ngeted spatial relations might disappear from the\nevaluation set as their frequency in the held-out\ndata is too low. In addition to the requirement that\nthe evaluation set contains no out-of-vocabulary\nitems, the target and landmarks are included with-\nout restriction on their frequency, as they occur\nwith these spatial relations.\n4.3 Results\nFigure 1 shows the estimated average perplexi-\nties of a subset of spatial relations, those that sat-\nisfy the sampling frequency requirement described\nin Section 4.2. Functionally and geometrically\nbiased spatial relations as identiﬁed experimen-\ntally in the literature (Section 1) are represented\nwith orange and blue bars respectively. There is\na tendency that functionally biased relations lead\nto lower mean perplexity of phrases (Hypothesis\n(a) test-set\n(b) training set\nFigure 1: Mean perplexities of spatial descriptions of\nLM1 (orange: functionally biased, blue: geometrically\nbiased relations).\n2 is conﬁrmed) and also that there is a tendency\nthat spatial relations of a particular bias cluster to-\ngether (Hypothesis 1 is also conﬁrmed). We re-\nport results both on the training set and the test set\nwhich show the same tendencies. This means that\nour model generalises well on the test set and that\nthe latter is representative.\nHowever, in the language model the perplexities\nare biased by the frequency of individual words:\nmore frequent words are more likely and therefore\nthey are associated with lower LM perplexity. The\nresults show high Spearman’s rank correlation co-\nefﬁcient ρ = 0.90 between frequencies of spatial\nrelation in the dataset and the perplexity of the\nmodel on the test set: on (329,529) > in (108,880)\n> under (11,631) > above (8,952) > over (5,714)\n> at (4,890) > below (2,290) > across (1,230) >\nleft (996) > right (891). For the purposes of our\ninvestigation in predictability of target-landmark\npairs (Hypothesis 3) we should avoid the bias in\nthe training set. In order to exclude the bias of\nfrequencies of relations, we evaluate LM2 where\nspatial relations are presented with equal frequen-\n5\n(a) test-set\n(b) training set\nFigure 2: Mean perplexities of LM2 by spatial rela-\ntion (orange: functionally biased, blue: geometrically\nbiased).\ncies in training. Figure 2 shows the ranking of spa-\ntial relations by the perplexities when the language\nmodel was trained with balanced frequencies. The\ntwo kinds of spatial relations are less clearly sep-\narable as the colours overlap (Hypothesis 3 is not\nconﬁrmed). In comparison to Figure 1 there is an\nobservable trend that all instances lead to lower\nperplexities in the training set which is the effect of\ndown-sampling on vocabulary size. Figure 2 also\nshows that phrases with geometrically biased spa-\ntial relations have a higher change towards lower\nperplexities.\nNoting that the frequency of using functionally-\nbiased spatial relations are higher in English, this\nbias and our strong hypothesis for predictability\nof target-landmark pairs can be expressed with\nsimple joint probabilities which we are estimating\nwith the language model:\nP(target,relation,landmark ) =\nP(relation)P(target,landmark |relation)\nIt is possible that targets and landmarks that occur\nwith these relations are very speciﬁc to these rela-\ntions but infrequent with other relations. When we\nremove their frequency support provided by the\nfrequency of relations these targets and landmarks\nbecome infrequent in the dataset and therefore less\nprobable which on overall results in higher per-\nplexities of phrases with functionally-biased rela-\ntions. Speciﬁcity of targets and landmarks can be\na source of these results.\nTo provide (some) evidence for this assump-\ntion, Figure 3 shows the average ratios of unique\ntypes over total types of targets and landmarks in\nthe balanced dataset over 10-folds on which LM2\nwas trained. There is a very clear division be-\ntween functionally and geometrically biased spa-\ntial relations in terms of the uniqueness of tar-\ngets, functionally-biased relations are occurring\nwith more unique ones which contributes to higher\nperplexity of LM2. There is less clear distinction\nbetween the two kinds of spatial relations in terms\nof uniqueness of landmarks. Some functional re-\nlations such as on occur with fewer unique land-\nmarks than targets (from .11 to .06), some ge-\nometric relations such as right occur with more\nunique landmarks than targets (from .07 to .11).\nThe asymmetry between targets and landmarks is\nexpected since the choice of landmarks in the im-\nage description task is restricted by the choice\nof the targets (as well as other contextual factors\nsuch as visual salience). They have to be “good\nlandmarks” to relate the targets to. A functional\nrelation-landmark pair is more related to the target\nthrough the landmark’s affordances whereas a ge-\nometric relation-landmark pair is more related to\nthe target through geometry. This might explain\nfor example, why on has fewer, butright has more\nunique landmarks than targets. On the other hand\nthere are also relations where the ratio of unique\ntargets and landmarks is very similar, for example\nat (.14 and .14). Overall, it appears that if unique-\nness of objects is contributing to the perplexity of\nthe language model of phrases which functionally-\nbiased relations (which in this balanced dataset is\nthe case) then this is more contributed by targets\nrather than the landmarks.\nTo further explore the idea of asymmetry be-\ntween targets and landmarks we re-arranged the\ntargets and landmarks in the descriptions from\nthe balanced dataset that LM2 was trained to\n<s> landmark relation target </s> and\ntrained LM2′. The average perplexities over 10-\nfolds of cross-validation are shown in Figure 4.\n6\n(a) targets\n (b) landmarks\nFigure 3: Ratio between unique types and all types per spatial relation in the balanced dataset for LM2.\nComparing Figure 4 with Figure 2 we ﬁrst observe\nthat the perplexity of LM2 ′on the descriptions is\noverall several magnitudes lower than the perplex-\nity of LM2 (max 0.06, max 140). Secondly, we\nobserve that the perplexities of phrases containing\ndifferent relations are very similar and that there is\nno separation of phrases by perplexity depending\non the relation bias. The results are in line with\nour argument above. Knowing the landmark, it is\nmuch easier for the language model to predict the\nrelation (of either kinds) and the target.\nFigure 4: Mean perplexities of LM2 ′ by spatial rela-\ntion (orange: functionally biased, blue: geometrically\nbiased)\nIn conclusion, the explanation why descriptions\nwith functionally-biased relations have a higher\nperplexity than descriptions with geometrically-\nbiased descriptions appears to be twofold: (i)\nfunctionally-biased relations are more selective\nof their targets as expressed by the uniqueness\ncounts, and (ii) functional relations are also more\nselective of their landmarks but this fact works\nagainst the performance of the language model.\nAs it is trained on the sequence left to right, it\nhas to learn to predict relations only on the basis\nof targets which in the case of functionally-biased\nrelations are represented by more unique tokens\nthan geometrically-biased relations. The more in-\nformative words, the landmarks, that would enable\nthe language model to predict a functional rela-\ntion, comes last, after the relation has already been\nseen. The possible reason why geometrically-\nbiased relations lead to lower perplexities of a lan-\nguage model on descriptions is because they have\nfewer unique targets. Hence, our Hypothesis 1\nwhich linked selectivity of functionally-biased re-\nlations to low perplexity of phrases can be refuted.\nIn spatial relations the order of the semantic in-\nterpretation of tokens (that we want to capture in\nthese experiments) is different from the linear syn-\ntactic order of order which can be captured by the\nlanguage model. When this order is changed as in\nLM2′our predictions come closer to the hypothe-\nsis (Figure 4).5\nBy removing the frequency bias on spatial re-\nlations in LM2 we ﬁx the distribution of spatial\nrelations and examine the effect of distribution of\ntargets and landmarks on perplexities of phrases\n(spatial relation as ﬁxed context). In the follow-\ning section, we ﬁx the distributions of targets and\nlandmarks of each spatial relation and examine the\nperplexity of phrases when another spatial relation\nis projected in this context (targets-landmarks as\nﬁxed context).\n5Modulo that landmarks are, as discussed above, well-\npredictive of relations of both kinds.\n7\n5 Varying spatial relations\n5.1 Hypotheses\nGiven a particular spatial relation, the distribu-\ntion of targets and landmarks that occur with it\ncreates a particular signature of targets and land-\nmarks, the target-landmark context of a spatial re-\nlation. In this experiment, we investigate the effect\non perplexity of phrases when another spatial rela-\ntion is projected in such a target-landmark context.\nGiven different selectivity of functionally- and\ngeometrically-biased spatial relations, namely the\nfunctionally-based spatial relations are more se-\nlective of their targets and landmarks and therefore\ncreate more speciﬁc contexts, we should observe\ndifferences in perplexities of phrases when other\nspatial relations are projected in these contexts.\nIn particular, we hypothesise that geometrically-\nbiased spatial relations are more easily swap-\npable than functionally-biased spatial relations as\nmeasured by the perplexity of a language model\ntrained on the original, non-swapped phrases (Hy-\npothesis 4).\n5.2 Method\nWe use LM2 from Section 4 (trained on the bal-\nanced frequencies of spatial relations) with no ad-\nditional training from the previous experiment.\nWe group descriptions in the evaluation set by spa-\ntial relation. For each phrase containing a par-\nticular spatial relation, we replace it with every\nother spatial relation and estimate the perplexity of\nthe resulting phrase using a language model. Fi-\nnally, we calculate the mean of perplexities over\nall phrases. We use 10-fold cross-validation and\nreport the ﬁnal means across the 10 folds.\n5.3 Results\nFigure 5 shows a %-increase in mean complex-\nities from those in Figure 2 when LM2 is ap-\nplied on phrases with swapped relations in the\ncontexts of the original relations. Hence, the\ncolumn “at” shows the %-increase in perplexi-\nties of phrases that originally contained at in the\nvalidation dataset but this was replaced by all\nother spatial relations. Comparing with Figure 2\nthe estimated perplexities are higher across all\ncases which is predictable. There is a weak ten-\ndency that replacing functionally-biased relations\nwith other relations leads to higher perplexities of\nspatial descriptions than replacing geometrically-\nbiased relations, but the distinction is not clear cut\nFigure 5: %-increase in perplexities of LM2 shown per\ncontext of the original preposition when swapped with\nanother one.\n(Hypothesis 4 partially conﬁrmed). The lack of a\nclear distinction between two classes of descrip-\ntions conﬁrms our previous observations about\nlandmarks and targets: the LM has learned par-\nticular contexts for both kinds of descriptions.\n6 Discussion and conclusion\nWe explored the degree that the functional and ge-\nometric character of spatial relations can be iden-\ntiﬁed by a neural language model by focusing on\nspatial descriptions of controlled length and con-\ntaining normalised relations. Our ﬁrst question\nwas about the implications of using a neural lan-\nguage model for this task. The previous research\n(Dobnik and Kelleher, 2013) used normalised en-\ntropy of target-landmarks per relation and log like-\nlihood ratio between target-landmarks and rela-\ntions to test this. These are focused measures that\nestimate the variation and the strength of associa-\ntion of words in a corpus. On the other hand, a lan-\nguage model provides a more general probabilistic\nrepresentation of the entire description. As such it\ncaptures any kind of associations between words\nin a sequence. The other important observation is\nthat it captures sequential relations in the direction\nleft to right and as we have seen the sequential na-\nture of the language model does not correspond\nprecisely with the order in which semantic argu-\nments of spatial relations are interpreted. How-\never, nonetheless we can say that language models\nare able to capture a distinction between functional\nand geometric spatial relations (plus other seman-\ntic distinctions) to a similar degree of success as\npreviously reported measures. Our initial hypothe-\nsis about the greater selectivity of spatial relations\n8\nfor its arguments is correct but it is exempliﬁed in a\ngreater perplexity of a language model in the con-\ntext of balanced spatial relations. We argued that\nthis has to do with the fact that the targets are more\nunique to these relations (which is consequence of\na greater speciﬁcity for arguments of functionally\nbiased relations) and is also related to the way a se-\nquential language model works. In the unbalanced\ndataset, the perplexity of the language model is re-\nversed (it is lower with functionally biased rela-\ntions) because the speciﬁcity of targets to relations\nis boosted with greater frequency of functionally-\nbiased relations. The fact that functionally-biased\nrelations are more frequent is probably related to\nthe fact that such descriptions are more informa-\ntive than purely geometric ones if available for a\nparticular pair of objects.\nWe can only report tendencies based on the per-\nplexities of our language models as our conclu-\nsions. This is because the functional-geometric\nbias is graded, because the predictions are highly\ndependent on the quality and the size of the\ndataset, and because other semantic relations\nmight also be expressed by this measure. We\nchose a large contemporary dataset of image de-\nscriptions because we hope that it contains a high\nproportion of prepositions used as spatial rela-\ntions. However, there is no guarantee that all\nprepositions in this dataset are used this way.\nWe observe that there is considerable variation\nof obtained values across the 10-folds of cross-\nvalidation and we report the mean values over all\nfolds. As an illustration, in the supplementary ma-\nterial (Section A.2) we give an example of graphs\nfrom two intermediary folds.\nUsing a language model in this task we have\nalso learned new insights about the way language\nmodels encode spatial relations in image descrip-\ntions. It has been pointed out (cf. (Kelleher and\nDobnik, 2017) among others) that convolutional\nneural networks with an attention model are de-\nsigned to detect objects whereas spatial relations\nbetween objects are likely to be predicted by the\nlanguage model. In this work we show that lan-\nguage models are not only predicting the rela-\ntion (which is expected) but are able to distin-\nguish between different classes of relations thus\nencoding ﬁner semantic distinctions. This tells us\nthat language models are able to encode a surpris-\ning amount of information about world knowledge\nwith a usual caveat that it is difﬁcult to separate\nseveral strands of this knowledge.\nThe work can be extended in several ways. One\nway is to study dataset effects on the predicted re-\nsults. Datasets with descriptions of robotic actions\nand instructions may be particularly promising as\nthey focus on spatial uses. Different normalisa-\ntions of spatial relations have a signiﬁcant effect\non the results. In this work composite spatial rela-\ntions such on the left side of are normalised to sim-\nple spatial relations such as left. However, these\ncould be treated as separate relations as difference\nbetween may exist. A more systematic examina-\ntion of clusters of spatial relations would eventu-\nally tell us what other spatial relations not yet iden-\ntiﬁed as functionally or geometrically biased have\nsimilar properties to those that have identiﬁed as\nsuch experimentally.\nAcknowledgements\nThe research of Dobnik and Ghanimifard was\nsupported by a grant from the Swedish Research\nCouncil (VR project 2014-39) for the establish-\nment of the Centre for Linguistic Theory and\nStudies in Probability (CLASP) at Department of\nPhilosophy, Linguistics and Theory of Science\n(FLoV), University of Gothenburg.\nThe research of Kelleher was supported by the\nADAPT Research Centre. The ADAPT Cen-\ntre for Digital Content Technology is funded un-\nder the SFI Research Centres Programme (Grant\n13/RC/2106) and is co-funded under the European\nRegional Development Funds.\nReferences\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nFranc ¸ois Chollet et al. 2015. Keras. https://\ngithub.com/keras-team/keras.\nGordon Christie, Ankit Laddha, Aishwarya Agrawal,\nStanislaw Antol, Yash Goyal, Kevin Kochersberger,\nand Dhruv Batra. 2016. Resolving language and\nvision ambiguities together: Joint segmentation\n& prepositional attachment resolution in captioned\nscenes. arXiv, 1604.02125 [cs.CV].\nKenny R Coventry and Simon C Garrod. 2004.Saying,\nseeing, and acting: the psychological semantics of\nspatial prepositions. Psychology Press, Hove, East\nSussex.\n9\nKenny R. Coventry, Merc `e Prat-Sala, and Lynn\nRichards. 2001. The interplay between geometry\nand function in the apprehension of Over, Under,\nAbove and Below. Journal of Memory and Lan-\nguage, 44(3):376–398.\nSebastien Delecraz, Alexis Nasr, Frederic Bechet, and\nBenoit Favre. 2017. Correcting prepositional phrase\nattachments using multimodal corpora. In Proceed-\nings of the 15th International Conference on Parsing\nTechnologies, pages 72–77, Pisa, Italy. Association\nfor Computational Linguistics.\nSimon Dobnik and John D. Kelleher. 2013. Towards\nan automatic identiﬁcation of functional and geo-\nmetric spatial prepositions. In Proceedings of PRE-\nCogSsci 2013: Production of referring expressions\n– bridging the gap between cognitive and computa-\ntional approaches to reference , pages 1–6, Berlin,\nGermany.\nSimon Dobnik and John D. Kelleher. 2014. Explo-\nration of functional semantics of prepositions from\ncorpora of descriptions of visual scenes. InProceed-\nings of the Third V&L Net Workshop on Vision and\nLanguage, pages 33–37, Dublin, Ireland. Dublin\nCity University and the Association for Computa-\ntional Linguistics.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural information\nprocessing systems, pages 1019–1027.\nSimon Garrod, Gillian Ferrier, and Siobhan Campbell.\n1999. In and on: investigating the functional geom-\netry of spatial prepositions. Cognition, 72(2):167–\n189.\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep recur-\nrent neural networks. In Acoustics, speech and sig-\nnal processing (icassp), 2013 ieee international con-\nference on, pages 6645–6649. IEEE.\nMichael Grubinger, Paul D. Clough, Henning M ¨uller,\nand Thomas Deselaers. 2006. The IAPR bench-\nmark: A new evaluation resource for visual informa-\ntion systems. In Proceedings of OntoImage 2006:\nWorkshop on language resources for content-based\nmage retrieval during LREC 2006, Genoa, Italy. Eu-\nropean Language Resources Association.\nAnnette Herskovits. 1986. Language and spatial cog-\nnition: an interdisciplinary study of the preposi-\ntions in English. Cambridge University Press, Cam-\nbridge.\nJohn D. Kelleher and Fintan J. Costello. 2005. Cog-\nnitive representations of project prepositions. In\nIn Proceedings of the Second ACL-Sigsem Work-\nshop on The Linguistic Dimensions of Prepositions\nand their Used In Computational Linguistic For-\nmalisems and Applications.\nJohn D. Kelleher and Fintan J. Costello. 2009. Apply-\ning computational models of spatial prepositions to\nvisually situated dialog. Computational Linguistics,\n35(2):271–306.\nJohn D. Kelleher and Simon Dobnik. 2017. What is not\nwhere: the challenge of integrating spatial represen-\ntations into deep learning architectures. In Proceed-\nings of the Conference on Logic and Machine Learn-\ning in Natural Language (LaML 2017), Gothenburg,\n12 –13 June, volume 1 of CLASP Papers in Compu-\ntational Linguistics, pages 41–52, Gothenburg, Swe-\nden.\nJohn D. Kelleher and Geert-Jan M. Kruijff. 2006. In-\ncremental generation of spatial referring expressions\nin situated dialog. In Proceedings of the 44th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL-44, pages 1041–1048. Association\nfor Computational Linguistics.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nParisa Kordjamshidi, Taher Rahgooy, Marie-Francine\nMoens, James Pustejovsky, Umar Manzoor, and\nKirk Roberts. 2017. CLEF 2017: Multimodal spa-\ntial role labeling (mSpRL) task overview. InExperi-\nmental IR Meets Multilinguality, Multimodality, and\nInteraction, pages 367–376, Cham. Springer Inter-\nnational Publishing.\nParisa Kordjamshidi, Martijn Van Otterlo, and Marie-\nFrancine Moens. 2011. Spatial role labeling: To-\nwards extraction of spatial relations from natural\nlanguage. ACM Transactions on Speech and Lan-\nguage Processing, 8(3):4:1–4:36.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma,\nMichael Bernstein, and Li Fei-Fei. 2017. Visual\ngenome: Connecting language and vision using\ncrowdsourced dense image annotations. Interna-\ntional Journal of Computer Vision, 123(1):32–73.\nBarbara Landau. 1996. Multiple geometric representa-\ntions of objects in languages and language learners.\nLanguage and space, pages 317–363.\nBarbara Landau. 2016. Update on “What” and “where”\nin spatial language: A new division of labor for spa-\ntial terms. Cognitive Science, 41(S2):321–350.\nBarbara Landau and Ray Jackendoff. 1993. “what”\nand “where” in spatial language and spatial cogni-\ntion. Behavioral and Brain Sciences , 16(2):217–\n238, 255–265.\nG.D. Logan and D.D. Sadler. 1996. A computational\nanalysis of the apprehension of spatial relations. In\nM. Bloom, P.and Peterson, L. Nadell, and M. Gar-\nrett, editors, Language and Space , pages 493–529.\nMIT Press.\n10\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nArnau Ramisa, Josiah Wang, Ying Lu, Emmanuel\nDellandrea, Francesc Moreno-Noguer, and Robert\nGaizauskas. 2015. Combining geometric, textual\nand visual features for predicting prepositions in im-\nage descriptions. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 214–220, Lisbon, Portugal. Asso-\nciation for Computational Linguistics.\nCyrus Rashtchian, Peter Young, Micah Hodosh, and\nJulia Hockenmaier. 2010. Collecting image annota-\ntions using Amazon’s Mechanical Turk. InProceed-\nings of the NAACL HLT 2010 Workshop on creating\nspeech and language data with Amazon’s Mechani-\ncal Turk, Los Angeles, CA. North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nGerard J Steen, Aletta G Dorst, J Berenike Herrmann,\nAnna Kaal, Tina Krennmayr, and Trijntje Pasma.\n2010. A method for linguistic metaphor identiﬁca-\ntion: From MIP to MIPVU , volume 14. John Ben-\njamins Publishing.\nA Supplementary material\nA.1 Language Model Architecture\nFigure 6: The recurrent language model diagram with\nLSTM recurrent unit.\nThe neural language model architecture with\nthe Long-Short Terms Memory (LSTM) function\nand its parameters, similar to tied weights in (Gal\nand Ghahramani, 2016):\n•We ∈Rn×d for word embeddings,\n•WLST M ∈R2d×4d for parameters of the Long-\nShort Term Memory function,\n•WFinal ∈Rd×n of the ﬁnal dense layer with\nsoftmax.\nwhere n is the vocabulary size for V =\n{v1,v2,..., vn}and d is both the embeddings size\nand the memory size in LSTM. For mini-batches\nfrom training data, these parameters are being up-\ndated using a stochastic gradient descent to min-\nimise the loss.\nxt = δvkt ·We (5)\n\n\ni\nf\no\ng\n\n=\n\n\nσ\nσ\nσ\ntanh\n\n\n(( xt\nht−1\n)\n·WLST M\n)\n(6)\nct = f ◦ct−1 +i ◦g (7)\nht = o ◦tanh(ct ) (8)\nˆyt = softmax(ht ·Wf inal +b) (9)\nwhere δvkt represents the one-hot encoding of the\nt-th word in the sequence. The xt is the word em-\nbedding for this word, and two vectors ct and ht\nrepresent the states of the recurrent unit. Figure 6\nillustrates the same equation.\nA.2 Evaluation\n(a)\n(b)\nFigure 7: Mean perplexities of LM2 by spatial relation\nfor (a) folds 1 and (b) 2 (orange: functionally biased,\nblue: geometrically biased).\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7753391265869141
    },
    {
      "name": "Perplexity",
      "score": 0.7598720788955688
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6145548820495605
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5998368263244629
    },
    {
      "name": "Closed captioning",
      "score": 0.5970486402511597
    },
    {
      "name": "Natural language processing",
      "score": 0.5910535454750061
    },
    {
      "name": "Spatial relation",
      "score": 0.5863845348358154
    },
    {
      "name": "Situated",
      "score": 0.46435025334358215
    },
    {
      "name": "Representation (politics)",
      "score": 0.4397534728050232
    },
    {
      "name": "Language model",
      "score": 0.42783671617507935
    },
    {
      "name": "Image (mathematics)",
      "score": 0.22690725326538086
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I881427289",
      "name": "University of Gothenburg",
      "country": "SE"
    }
  ],
  "cited_by": 13
}