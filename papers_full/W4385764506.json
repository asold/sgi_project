{
  "title": "SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation",
  "url": "https://openalex.org/W4385764506",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100429593",
      "name": "Xuewei Li",
      "affiliations": [
        "Tencent (China)",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5043731569",
      "name": "Tom Wu",
      "affiliations": [
        "Zhejiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101500719",
      "name": "Zhongang Qi",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5089410490",
      "name": "Gaoang Wang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5102004349",
      "name": "Ying Shan",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100407758",
      "name": "Xi Li",
      "affiliations": [
        null,
        "Zhejiang University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2019673249",
    "https://openalex.org/W3175201472",
    "https://openalex.org/W4281476522",
    "https://openalex.org/W3187216907",
    "https://openalex.org/W4214713996",
    "https://openalex.org/W3119419912",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2741141082",
    "https://openalex.org/W2884414611",
    "https://openalex.org/W4221142197",
    "https://openalex.org/W4319300281",
    "https://openalex.org/W4312415581",
    "https://openalex.org/W4302306593",
    "https://openalex.org/W3206981895",
    "https://openalex.org/W4295814684",
    "https://openalex.org/W3121058064",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W4308536459",
    "https://openalex.org/W2586114507",
    "https://openalex.org/W3211783547",
    "https://openalex.org/W4312960790",
    "https://openalex.org/W4285410160",
    "https://openalex.org/W3174725336",
    "https://openalex.org/W4225726016",
    "https://openalex.org/W4288640779",
    "https://openalex.org/W3183913595",
    "https://openalex.org/W4205163772",
    "https://openalex.org/W4399310720",
    "https://openalex.org/W2895250390",
    "https://openalex.org/W2900581974",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2972887266",
    "https://openalex.org/W3170279373",
    "https://openalex.org/W3208869643",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3034515714",
    "https://openalex.org/W3131500599"
  ],
  "abstract": "As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original 360 degree data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates the pixel density of original 360 degree data, respectively. Experimental results on Stanford2D3D Panoramic datasets show that SGAT4PASS significantly improves performance and robustness, with approximately a 2% increase in mIoU, and when small 3D disturbances occur in the data, the stability of our performance is improved by an order of magnitude. Our code and supplementary material are available at https://github.com/TencentARC/SGAT4PASS.",
  "full_text": "SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic\nSegmentation\nXuewei Li1,2, Tao Wu1, Zhongang Qi2, Gaoang Wang3, Ying Shan2 and Xi Li1,4∗\n1College of Computer Science and Technology, Zhejiang University\n2ARC Lab, Tencent PCG\n3Zhejiang University-University of Illinois at Urbana-Champaign Institute, Zhejiang University\n4Zhejiang – Singapore Innovation and AI Joint Research Lab, Hangzhou\n{xueweili, taowucs}@zju.edu.cn, zhongangqi@tencent.com, gaoangwang@intl.zju.edu.cn,\nyingsshan@tencent.com, xilizju@zju.edu.cn\nAbstract\nAs an important and challenging problem in com-\nputer vision, PAnoramic Semantic Segmentation\n(PASS) gives complete scene perception based on\nan ultra-wide angle of view. Usually, prevalent\nPASS methods with 2D panoramic image input\nfocus on solving image distortions but lack con-\nsideration of the 3D properties of original 360◦\ndata. Therefore, their performance will drop a\nlot when inputting panoramic images with the\n3D disturbance. To be more robust to 3D dis-\nturbance, we propose our Spherical Geometry-\nAware Transformer for PAnoramic Semantic Seg-\nmentation (SGAT4PASS), considering 3D spher-\nical geometry knowledge. Specifically, a spher-\nical geometry-aware framework is proposed for\nPASS. It includes three modules, i.e., spherical\ngeometry-aware image projection, spherical de-\nformable patch embedding, and a panorama-aware\nloss, which takes input images with 3D distur-\nbance into account, adds a spherical geometry-\naware constraint on the existing deformable patch\nembedding, and indicates the pixel density of orig-\ninal 360◦ data, respectively. Experimental results\non Stanford2D3D Panoramic datasets show that\nSGAT4PASS significantly improves performance\nand robustness, with approximately a 2% increase\nin mIoU, and when small 3D disturbances occur\nin the data, the stability of our performance is\nimproved by an order of magnitude. Our code\nand supplementary material are available at https:\n//github.com/TencentARC/SGAT4PASS.\n1 Introduction\nThere has been a growing trend of practical applications\nbased on 360◦ cameras in recent years, including holistic\nsensing in autonomous vehicles [de La Garanderie et al.,\n2018; Ma et al., 2021; Gao et al., 2022], immersive view-\ning in augmented reality and virtual reality devices [Xu et\n∗Corresponding author.\n(a) Original image\n (b) 5◦ pitch rotation image\n(c) 5◦ roll rotation image\n0 1 2 3 4 5\nroll & pitch angle\n0\n2\n4\n6Var. of mIoU\nBaselineMax :5.147\nOursMax :0.066\nBaseline\nOurs (d) Angle & Variance of mIoU\n0 1 2 3 4 5\npitch angle\n48\n52\n56\n60\n64Mean mIoU\nBaseline\nOurs\n(e) Pitch angle & Mean mIoU\n0 1 2 3 4 5\nroll angle\n48\n52\n56\n60\n64Mean mIoU\nBaseline\nOurs (f) Roll angle & Mean mIoU\nFigure 1: The results with 3D disturbance input. (a) is the original\nimage, and (b) / (c) is the images rotated 5◦ in pitch / roll axis.\nOur baseline is Trans4PASS+. Compared with the minor change in\nimages, the huge variance / performance change in SGA validation\nis shown in (d) / (e) and (f). “Mean” and “Variance” are defined in\ndetail in Section 4.1.\nal., 2018; Xu et al., 2021; Ai et al., 2022], etc. Panoramic\nimages with an ultra-wide angle of view deliver complete\nscene perception in many real-world scenarios, thus draw-\ning increasing attention in the research community in com-\nputer vision. Panoramic semantic segmentation (PASS) is es-\nsential for omnidirectional scene understanding, as it gives\npixel-wise analysis for panoramic images and offers a dense\nprediction technical route acquiring 360◦ perception of sur-\nrounding scenes [Yang et al., 2021a].\nMost existing PASS approaches use equirectangular pro-\njection (ERP) [Sun et al., 2021; Yang et al., 2021b] to con-\nvert original 360◦ data to 2D panoramic images. However,\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1125\nthese methods often suffer from two main problems: large\nimage distortions and lack of Spherical Geometry-Aware\n(SGA) robustness that resists 3D disturbance. These prob-\nlems lead neural networks to only learn suboptimal solutions\nfor panoramic segmentation [Yang et al., 2019; Wanget al.,\n2021a]. Although some recent works [Zhang et al., 2022a;\nZhang et al., 2022b] take serious distortions into account in\ntheir models and become the current state-of-the-art (SOTA),\nthey still do not pay enough attention to the SGA properties\nof the original 360◦ data, resulting in performance degra-\ndation even with small projection disturbance. As shown\nin Figure 1b and Figure 1c, applying 5◦ rotation on the\npitch or roll axis of original 360◦ data carries only mi-\nnor changes in 2D panoramic images. However, as shown\nin Figure 1e, Figure 1f, and Figure 1d, the performances of\nTrans4PASS+ [Zhang et al., 2022b] (the blue lines) drop a\nlot (about 4%), and the variance increases by almost 2 or-\nders of magnitude, because the axis rotations lead to different\nspherical geometry relations between pixels in the projected\npanoramic images, which the existing methods fail to adapt.\nBesides disturbance, the ERP also introduces boundaries to\npanoramic images that the original 360◦ data do not have.\nSome adjacent pixels are disconnected and some objects are\nseparated, which is a severe issue, especially for semantic\nsegmentation. Furthermore, there also exists a difference in\npixel sampling density between the original 360◦ data and its\ncorresponding projection image, e.g., pixels are over sampled\nin the antarctic and arctic areas of 2D panoramic images. All\nthese issues make panoramic semantic segmentation a chal-\nlenging task, and the above characteristics should be well\nstudied to design a robust model that adapts to disturbance,\ndisconnection, uneven density, and other SGA properties.\nImproving robustness and taking SGA properties into ac-\ncount, we propose a novel model, i.e., Spherical Geometry-\nAware Transformer for PAnoramic Semantic Segmentation\n(SGAT4PASS), equipped with the SGA framework and SGA\nvalidation. The proposed SGA framework includes SGA im-\nage projection in the training process, Spherical Deformable\nPatch Embedding (SDPE), and a panorama-aware loss. SGA\nimage projection provides images with 3D disturbance to im-\nprove the 3D robustness of the model. SDPE improves the\npatch embedding and makes it consider not only the image\ndistortions with deformable operation but also spherical ge-\nometry with SGA intra- and inter-offset constraints. The\npanorama-aware loss deals with the difference in pixel den-\nsity between the original 360◦ data and its corresponding 2D\npanoramic images. Moreover, we propose a new validation\nmethod, i.e., SGA validation, to evaluate the 3D robustness\nof various models comprehensively, which considers differ-\nent 3D disturbances for input images, and measures the aver-\nage performance and the variance for comparisons. Extensive\nexperimental results on popular Stanford2D3D panoramic\ndatasets [Armeni et al., 2017] demonstrate that our proposed\napproach achieves about 2% and 6% improvements on tradi-\ntional metrics and SGA metrics, respectively.\nThe contributions of this paper are summarized as follows:\n• We propose SGAT4PASS, a robustness model for the\nPASS task, which utilizes SGA image projection to deal\nwith the 3D disturbance issue caused by ERP.\n• We introduce SDPE to combine spherical geometry with\ndeformable operation to better deal with panoramic im-\nage distortion. And we also propose panorama-aware\nloss to ease the oversampling problem.\n• We evaluate SGAT4PASS on the popular benchmark\nand perform extensive experiments with both traditional\nmetrics and proposed SGA metrics, which demonstrate\nthe effectiveness of each part of the framework.\n2 Related Work\nThe two most related fields are panoramic semantic segmen-\ntation and dynamic and deformable vision transformers.\n2.1 Panoramic Semantic Segmentation\nSemantic segmentation of panoramic images has many ap-\nplications in real-world scenarios, such as autonomous driv-\ning [Ye et al., 2021], panoramic lenses safety and monitoring\napplications [Poulin-Girard and Thibault, 2012 ], etc. With\nthe development of deep learning, many neural networks have\nbeen developed for panoramic semantic segmentation. Deng\net al. [Deng et al., 2017] first proposed a semantic segmen-\ntation framework for wide-angle (fish-eye) images and trans-\nformed an existing pinhole urban scene segmentation dataset\ninto synthetic datasets. Yang et al. [Yang et al., 2019] de-\nsigned a semantic segmentation framework for panoramic an-\nnular images using a panoramic annular camera with an entire\nField of View (FoV) for panoramic surrounding perception\nbased on a single camera. Furthermore, Yang et al. [Yang et\nal., 2020] proposed DS-PASS to improve it with a more effi-\ncient segmentation model with attention connections. PASS\nsolutions can be divided into two main fields: distortion-\naware strategies and 2D-geometry-aware ones.\nFor distortion-aware strategies, Tateno et al. [Tateno et\nal., 2018] proposed using specially designed distortion-aware\nconvolutions in a fixed manner to address image distortions.\nFurthermore, ACDNet [Zhuang et al., 2022] combined con-\nvolution kernels with different dilation rates adaptively and\nused fusion-driven methods to take advantage of several pro-\njections. Jiang et al. [Jiang et al., 2019] designed a spher-\nical convolution operation. Lee et al. [Lee et al., 2018 ]\nused spherical polyhedrons to represent panoramic views\nto minimize the difference in spatial resolution of the sur-\nface of the sphere and proposed new convolution and group-\ning methods for the representation of spherical polyhedrons.\nHu et al. [Hu et al., 2022] designed and proposed a distor-\ntion convolutional module based on the image principle to\nsolve the distortion problem caused by the distortion of the\npanoramic image. Zhang et al. [Zhang et al., 2022a] [Zhang\net al., 2022b] designed their Trans4PASS and Trans4PASS+\nthat perceived spherical distortion and solved the distortion\nproblem of spherical images better through their Deformable\nPatch Embedding (DPE) and Deformable Multi-Layer Per-\nception (DMLP) modules. Also, Trans4PASS+ is the cur-\nrent SOTA panoramic semantic segmentation model and is\nour baseline. For 2D geometry-aware strategies, horizontal\nfeatures are mainly used based on the ERP inherent property.\nSun et al. [Sun et al., 2021] proposed HoHoNet and Pintore\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1126\nFigure 2: Overall review of SGAT4PASS. We borrow the network from Trans4PASS+, and add three main modules: Spherical geometry-\naware (SGA) image projection, SDPE, and panorama-aware loss. (Lower left) SGA image projection rotates the input panoramic images to\nmimic 3D disturbance. (Lower middle) SDPE adds several SGA constraints on deformable patch embedding and let it consider both image\ndistortions and spherical geometry. (Lower right) Panorama-aware loss (PA loss) takes into account the pixel density of a sphere.\net al. [Pintore et al., 2021] proposed SliceNet to use the ex-\ntracted feature maps in a 1D horizontal representation.\nFor our SGAT4PASS based on the distortion-aware SOTA\nmodel, Trans4PASS+, we add SGA information from the\noriginal 360◦ data instead of the 2D geometry prior to\npanoramic images to improve not only its performance but\nalso its robustness when meeting 3D disturbance.\n2.2 Dynamic and Deformable Vision Transformers\nRegarding the field of vision transformers, some works have\ndeveloped architectures with dynamic properties. Chen et\nal. [Chen et al., 2021] and Xia et al. [Xia et al., 2022] used\ndeformable designs in later stages of the encoder. Yue et\nal. [Yue et al., 2021] used a progressive sampling strategy to\nlocate discriminatory regions. Deformable DETR [Zhu et al.,\n2020] used deformable attention to deal with feature maps.\nSome other works used adaptive optimization of the num-\nber of informative tokens to improve efficiency [Wang et al.,\n2021b] [Rao et al., 2021] [Yin et al., 2022] [Xu et al., 2022].\nZhang et al. [Zhang et al., 2022a] [Zhang et al., 2022b] de-\nsigned their Trans4PASS and Trans4PASS+ based on DPE\nand Deformable Multi-Layer Perception (DMLP) modules,\nand we use Trans4PASS+ as our baseline.\n3 Method\nWe present Spherical Geometry-Aware Transformer for\nPAnoramic Semantic Segmentation (SGAT4PASS) in this\nsection. First, we introduce the background of panoramic\nsemantic segmentation in Section 3.1. Second, we describe\nour main idea to apply different SGA properties in panoramic\nsemantic segmentation in Section 3.2. To improve the 3D\nrobustness of SGAT4PASS, we propose SGA Image Projec-\ntion, Spherical Deformable Patch Embedding (SDPE), and\npanorama-aware loss. Specifically, SGA Image Projection\nadds rotated samples in training; SDPE adds SGA constraints\non the deformable patch embedding; and the panorama-aware\nloss fuses sphere pixel density to training process.\n3.1 Background\nWe first describe a general formulation of PASS and then in-\ntroduce the spherical geometry property that we focus mainly\non. Panoramic images are based on original360◦ data formu-\nlated in the spherical coordinate system (based on longitude\nand latitude). To convert it to a rectangular image in a Carte-\nsian coordinate system, ERP is a widely used projection in\nthis field: x = (θ − θ0)cosϕ1, y= (ϕ − ϕ1), where θ0 = 0 is\nthe central latitude and ϕ1 = 0 is the central longitude. The\nERP-processed rectangular images are used as the input sam-\nple in datasets and fed to the neural network, and the rectan-\ngular semantic segmentation results are obtained to compare\nwith the ground truth and calculate the metrics. Although\ntraditional methods can treat PASS as the conventional 2D\nsemantic segmentation task and deal with panoramic images\neasily, the spherical geometry property is partly ignored.\n3.2 Spherical Geometry-Aware (SGA) Framework\nWe propose the SGA framework for PASS with SGA image\nprojection, SDPE, and panorama-aware loss. To deal with the\ninevitable 3D disturbance during the acquisition of the input\nimage, our SGA image projection aims to encode the origi-\nnal 360◦ data spherical geometry by generating input images\nwith different rotations. We design SDPE to model spatial\ndependencies on a sphere, making patch embedding consider\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1127\nMethod Avg mIoU\nF1 mIoU\nStdConv [Tateno et al.\n, 2018] - 32.6\nCubeMap [Tateno et al., 2018] - 33.8\nDistConv [Tateno et al., 2018] - 34.6\nSWSCNN [Esteves et al., 2020] 43.4 -\nTangent (ResNet-101) [Eder et al., 2020] 45.6 -\nFreDSNet [Berenguel-Baeta et al., 2022] - 46.1\nPanoFormer [Shen et al., 2022] 48.9 -\nHoHoNet (ResNet-101) [Sun et al., 2021] 52.0 53.9\nTrans4PASS (Small)[Zhang et al., 2022a] 52.1 53.3\nCBFC [Zheng et al., 2023] 52.2 -\nTrans4PASS+ (Small)[Zhang et al., 2022b] 53.7 53.6\nOurs (Small) 55.3 56.4\nTable 1: Comparison with the SOTA methods on Stanford2D3D\nPanoramic datasets. We follow recent works to compare the per-\nformance of both official fold 1 and the average performance of all\nthree official folds. respectively. “Avg mIoU” / “F1 mIoU” means\nthe mIoU performance of three official folds on average / official\nfold 1. A considerable improvement is gained.\nboth spherical geometry and image distortions. Furthermore,\na panorama-aware loss is proposed to model the pixel density\nof a sphere, making the loss weight distribution more simi-\nlar to the original 360◦ data. With these three modules, the\nspherical geometry is well employed in the PASS task.\nSpherical Geometry-Aware (SGA) Image Projection\nThe original 360◦ data follow a spherical distribution and are\nspherically symmetric. After rotating any angle along the yaw\n/ pitch / roll axis, the transformed data are still equivalent to\nthe original data. Traditional strategies assume that the im-\nages are taken with the yaw / pitch / roll angle equal to zero\ndegrees, which is too ideal in real-world scenarios and ignores\nthe camera disturbance and random noise. When the rotation\nangle is disturbed, traditional strategies usually have a large\ndegradation in the PASS task. SGA image projection fuses\nthis property between the inevitable equirectangular projec-\ntion and regular image augmentation to make models robust\nto 3D disturbance.\nWe use T to represent the forward process of ERP trans-\nformation, which is the process of converting spherical co-\nordinates to plane coordinates, and use T−1 to represent the\ninverse process of ERP that transforms the plane back onto\nthe sphere. Given an ERP-processed input panoramic image,\nwe first transform the image I originally in plane coordinates\nto spherical coordinates through the inverse ERP process. Af-\nter that, we use the rotation matrix in the three-dimensional\n(3D) space to perform a 3D rotation in the spherical coordi-\nnate system. For a general rotation in a 3D space, the angles\nof yaw, pitch, and roll are αuse, βuse, and γuse, respectively.\nThe corresponding rotation matrix is R(αuse, βuse, γuse). We\nmultiply R by the data in the spherical coordinate system to\nobtain the rotated data in the spherical coordinate system.\nFinally, we use the ERP forward process to convert the ro-\ntated spherical coordinate system image into a panoramic im-\nage, thus obtaining a certain rotated image of the real in-\nput of the network. The corresponding point in input im-\nage of a pixel in rotated image may not have integer coor-\ndinates, and we select the nearest pixel as its corresponding\npixel to be generic to the ground truth transformation. Based\non these operations, we build our SGA image projection,\nO3D(I, αuse, βuse, γuse) = T(R(αuse, βuse, γuse) · T−1(I)).\n(See Section C “Details for SGA Image Projection” in the\nsupplementary material for details.) At the beginning of the\ntraining process, we set the maximum rotation angle of the\nyaw / pitch / roll axis at (αtrain, βtrain, γtrain).\nSDPE: Spherical Deformable Patch Embedding\nWe first introduce DPE, and then fuse spherical geometry into\nDPE by SGA constraints to earn SDPE.\nFaced with image distortions in panoramic images, DPE,\nconsidering different distortions in different regions of an in-\nput panoramic image, is a popular solution [Zhang et al.,\n2022a] [ Zhang et al., 2022b]. In detail, given a 2D input\npanoramic image, the standard patch embedding handles it\ninto flattened patchesH×W, and the resolution of each patch\nis (s, s). A learnable projection layer transforms each patch\ninto out-dimensional embeddings. For each patch, the offsets\n∆DPE\n(i,j) of the ith row jth column pixel are defined as:\n∆DPE\n(i,j) =\n\u0014\nmin(max(-kD · H, g(f)(i,j)), kD · H)\nmin(max(-kD · W, g(f)(i,j)), kD · W)\n\u0015\n, (1)\nwhere g(·) is the offset prediction function. Hyperparameter\nkD puts an upper bound on the learnable offsets ∆DPE\n(i,j) . For\nimplementation, the deformable convolution operation [Dai\net al., 2017] is popularly employed to realize DPE.\nWhen fusing spherical geometry into DPE, human photo-\ngraphic and ERP priors are taken into consideration, in which\nthe plane formed by pitch and roll axes is always parallel to\nthe ground plane and the projection cylinder is perpendicu-\nlar to the ground plane. As a result, we add SGA constraints\nmainly on the yaw axis. In detail, we give intra-offset and\ninter-offset constraints on ∆DPE\n(i,j) . For convenience, we use\n∆m,n\n(i,j) to represent the ith row jth column pixel of the learn-\nable offset for the mth row nth column patch.\nIntra-offset constraint. Based on the phenomenon that the\noriginal 360◦ data are symmetric on any longitude and the\nprojection cylinder in ERP is symmetric in any line perpen-\ndicular to the base of the cylinder, the offset of any pixel in 2D\ninput panoramic image I should be symmetric on its perpen-\ndicular. To be generic to the learnable offsets ∆m,n\n(i,j) dealing\nwith the image distortions, we use a constraint Lintra:\nLintra =\nX\nm,n\nX\ni,j\nLintra\n2 (∆m,n\n(i,j), ∆S\nm,n\n(i,j)), (2)\nwhere ∆S\nm,n\n(i,j) is the single patch offset that is formed sym-\nmetrically along the yaw axis with ∆m,n\n(i,j) as the template.\nLintra\n2 (·, ·) represents the element-wise L2 loss.\nInter-offset constraint. Based on the phenomenon that the\nprojection cylinder in ERP can be slit and expanded from any\nline perpendicular to the base of the cylinder, the offset of\nany pixel in 2D input panoramic image I corresponding to\nthe same latitude of the original 360◦ data should be simi-\nlar. To be generic to the learnable ∆DPE\n(i,j) dealing with the\nimage distortions, we use a constraint, Linter, to model this\nproperty. For a certain pixel, we use the average offset in the\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1128\n(β,γ,α) (◦) BL mIoU / P\nAcc (β,γ,α) (◦) BL mIoU / P\nAcc (β,γ,α) (◦) BL mIoU / P\nAcc (β,γ,α) (◦) BL mIoU / P\nAcc\nOur mIoU / P\nAcc Our mIoU / P\nAcc Our mIoU / P\nAcc Our mIoU / P\nAcc\n(0,0,0) 53.617 / 81.483 (0,5,0) 49.292 / 78.346 (5,0,0) 49.468 / 78.500 (5,5,0) 47.234 / 77.129\n56.374 / 83.135 56.073 / 82.892 56.074 / 82.905 55.784 / 82.794\n(0,0,90) 53.918 / 81.590 (0,5,90) 49.861 / 78.656 (5,0,90) 49.400 / 78.373 (5,5,90) 47.589 / 77.361\n56.441 / 83.130 55.954 / 82.847 56.128 / 82.895 55.636 / 82.657\n(0,0,180) 53.587 / 81.476 (0,5,180) 49.344 / 78.532 (5,0,180) 49.536 / 78.585 (5,5,180) 47.458 / 77.307\n56.246 / 83.054 55.951 / 82.906 55.714 / 82.796 55.501 / 82.750\n(0,0,270) 53.669 / 81.459 (0,5,270) 49.462 / 78.445 (5,0,270) 49.363 / 78.485 (5,5,270) 47.726 / 77.451\n56.223 / 83.051 55.924 / 82.779 55.983 / 82.904 55.732 / 82.701\nTable 2: Detail performance comparison with Tran4PASS+ on Stanford2D3D Panoramic datasets official fold 1 with SGA metrics. All 18\nsituations are shown, and the analysis is in table 3. “BL” means the baseline, i.e., Tran4PASS+. “PAcc” meas the pixel accuracy metric.\nStatistics Baseline Ours\nmIoU P Acc\nmIoU PAcc\nMean 50.033 78.949 55.984\n(+5.951) 82.887 (+3.938)\nVariance 5.147 2.413 0.066 (-5.081) 0.020 (-2.393)\nRange 6.684 4.461 0.940 (-5.744) 0.478 (-3.983)\nTable 3: Overall performance comparison with Tran4PASS+ on\nStanford2D3D Panoramic datasets in table 2 setting. “PAcc” means\nthe pixel accuracy metric. SGAT4PASS earns considerable mean\nperformance and significant robustness improvement.\nwhole horizontal line as its constraint:\nLinter =\nX\nm,n\nX\ni,j\nLinter\n2 (∆m,n\n(i,j), ∆m,AVG\n(i,j) ), (3)\nwhere ∆m,AVG\n(i,j) is the average of each component in\n{∆m,n\n(i,j), n∈ W}, and Linter\n2 (·, ·) represents the L2 loss for\neach component length of the two vectors. Then the total\nSDPE loss is: LSDPE = Linter + Lintra.\nPanorama-Aware Loss\nBecause the panoramic images are rectangular in shape, the\nregion of the antarctic and arctic areas in the original 360◦\ndata is over sampled than the one near the equator. How-\never, due to human photographic priors, the semantics of\nthe antarctic (ground, floor, etc.) and arctic areas (sky, ceil-\ning, etc.) are relatively simple, as seen in the sample im-\nages of Figure 1 and Figure 2. When using traditional seg-\nmentation loss for supervised training, we treat each pixel\nequally, which leads to models paying relatively less atten-\ntion to semantic rich regions near the equator. To deal with\nthis phenomenon, we design our panorama-aware loss. For\nan ERP-processed panoramic image, the number of pixels in\neach horizontal line is the same, but the corresponding reso-\nlution density on the original sphere of each horizontal line is\nvery different. For this reason, we design a loss to reweight\nthe loss proportion of different horizontal lines depending on\nits height. For a pixel (m, n)|m∈ [1, HI], n∈ [1, WI] (WI\nand HI are the width and height of the input image), we give\nSGAIP SDPE P A\nmIoU Pixel accuracy\n53.617 81.483\n✓ 54.637 82.303\n✓ 54.554 81.508\n✓ 54.833\n81.733\n✓ ✓ ✓ 56.374 83.135\nTable 4: Effect of each SGAT4PASS module. We validate them\non Stanford2D3D Panoramic datasets official fold 1 with traditional\nmetrics. “SGAIP” / “SDPE” / “PA” means our SGA image projec-\ntion / spherical deformable patch embedding / panorama-aware loss.\nUsing anyone, an average improvement of 1.058% mIoU / 0.365%\npixel accuracy is gained when using three gains 2.757% / 1.652%.\na weight w(m,n)\npan when calculating its per pixel loss:\nw(m,n)\npan = cos(|2m − HI|\nHI\n· π\n2 ). (4)\nWe use Wpan to represent the set that includes all w(m,n)\npan .\nWhen faced with a panoramic semantic segmentation prob-\nlem, we first estimate the usage scenario to determine β and\nγ used in SGA image projection when α is often set as 360◦\nin common condition. We set our total loss as:\nLall = (1 + λw · Wpan) ⊙ LSEG + λs · LSDPE , (5)\nwhere LSEG is the common per pixel loss for semantic seg-\nmentation, ⊙ is the element-wise matrix multiplication, λw\nand λs are hyperparameters.\n4 Experiments\nIn this section, we evaluate our SGAT4PASS against the pop-\nular benchmark, Stanford2D3D, for both traditional metrics\nand our SGA validation.\n4.1 Datasets and Protocols\nWe validate SGAT4PASS on Stanford2D3D Panoramic\ndatasets [Armeni et al., 2017]. It has 1,413 panoramas, and\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1129\n(a) Original picture\n (b) Label\n (c) Baseline results\n (d) Our results\n(e) Rotated original picture\n (f) Rotated label\n (g) Baseline rotated results\n (h) Our rotated results\nFigure 3: Visualization comparison of SGAT4PASS and Trans4PASS+. The rotation of the pitch / roll / yaw axis is 5◦ / 5◦ / 180◦.\nSGAT4PASS gains the better results of semantic class “door” and “sofa” (highlighted by red dotted line boxes).\nStatistics Baseline Ours\nmIoU Pix el\naccuracy mIoU Pixel accuracy\n(β, γ,\nα) = (1◦, 1◦, 360◦)\nMean 53.473 81.251 56.212 (+2.739) 83.021 (+1.770)\nVariance 0.056 0.029 0.011 (-0.045) 0.003 (-0.026)\nRange 0.856 0.591 0.394 (-0.462) 0.192 (-0.399)\n(β, γ,\nα) = (0◦, 0◦, 360◦)\nMean 53.698 81.502 56.321 (+2.623) 83.093 (+1.591)\nVariance 0.017 0.003 0.008 (-0.009) 0.002 (-0.001)\nRange 0.331 0.131 0.218 (-0.113) 0.084 (-0.047)\nTable 5: Overall performance comparison on Stanford2D3D\nPanoramic datasets in different SGA metrics in two more favor-\nable settings for Tran4PASS+. SGAT4PASS also earns considerable\nmean performance and significant robustness improvement.\n13 semantic classes are labeled, and has 3 official folds, fold\n1 / 2 / 3. We follow the report style of previous work [Zhang\net al., 2022a] [Zhang et al., 2022b].\nOur experiments are conducted with a server with four\nA100 GPUs. We use Trans4PASS+ [Zhang et al., 2022b]\nas our baseline and set an initial learning rate of 8e-5, which\nis scheduled by the poly strategy with 0.9 power over 150\nepochs. The optimizer is AdamW [Kingma and Ba, 2015 ]\nwith epsilon 1e-8, weight decay 1e-4, and batch size is 4 on\neach GPU. Other settings and hyperparameters are set the\nsame as Trans4PASS+ [Zhang et al., 2022b]. For each in-\nput panoramic image I in an iteration, there is a 50% chance\nof using it directly and the other 50% chance of using it after\nSGA image projection, O3D(I, αuse, βuse, γuse), where αuse\n/ βuse / γuse uniformly sampled from 0 to αtrain / βtrain /\nγtrain. We set (βtrain, γtrain, αtrain) = (10◦, 10◦, 360◦). λw\nand λs are set as 0.3 and 0.3, respectively.\nSGA validation. Most PASS datasets use a unified ERP\nway to process original 360◦ data, PASS models have the po-\ntential to overfit the ERP way, cannot handle 3D disturbance\nwell and have little 3D robustness. To validate the robustness\nof the PASS models, we propose a novel SGA validation.nα,\nnβ, and nγ are the number of different angles for the yaw /\npitch / roll axis, respectively, andnα · nβ · nγ different-angle\npanoramic images for a certain original 360◦ data is earned.\nPanoramic semantic segmentation models are validated in all\nnα · nβ · nγ settings, and their statistics are reported as SGA\nmetrics. In our SGA validation, “Mean” means the average of\nall nα · nβ · nγ traditional results (e.g., mIoU, per pixel accu-\nracy, etc.). “Variance” means the variance of all nα · nβ · nγ\nresults. “Range” means the gap between the maximum and\nminimum results of allnα ·nβ ·nγ results. Compared to tradi-\ntional validation, SGA validation avoids models gain perfor-\nmance by fitting the ERP way of datasets and reflects objec-\ntive 3D robustness. In detail, we assume that the 3D rotation\ndisturbance is at most 5◦ / 5◦ / 360◦ of pitch (β ) / roll (γ ) /\nyaw (α) angle. We set nα = 4 (0◦, 90◦, 180◦, 270◦), nβ = 2\n(0◦, 5◦), and nγ = 2 (0◦, 5◦). We use the mean of them as\nthe final performance and observe the performance difference\namong them to indicate the 3D robustness of models.\n4.2 Performance Comparison\nIn this part, we first compare several recent SOTA methods\nwith traditional metrics, and then compare the latest SOTA\nTrans4PASS+ in detail with SGA metrics.\nTraditional metrics. Comparison results on Stanford2D3D\nPanoramic datasets with SOTA methods in traditional met-\nrics are shown in Table 1. Following recent work, we re-\nport the performance of both official fold 1 and the aver-\nage performance of all three official folds. From the results,\nSGAT4PASS outperforms current SOTA models by 2.8% /\n1.6% mIoU, respectively, which means that our SGAT4PASS\nhas a considerable performance margin compared to current\nmodels with traditional metrics.\nSGA metrics. Comparison results on Stanford2D3D\nPanoramic datasets with our SGA validation metrics are\nshown in Table 3, and Table 2 is the detailed performance\nof each situation. For mean mIoU / pixel accuracy, an im-\nprovement of nearly 6% / 4% is achieved, respectively. Fur-\nthermore, our variance is about 1\n100 and our fluctuation range\nis about 1\n10 . These results show that our SGAT4PASS have\nmuch better robustness than Trans4PASS+.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1130\nNetwork Test Method mIoU beam board bookcase ceiling\nchair clutter column door floor sofa table wall window\nTrans4Pass+ Traditional 53.62 0.39 74.4 65.32 84.21\n62.86 36.44 15.96 32.79 93.09 44.10 63.67 75.02 46.90\nOurs 56.37 0.73 74.05 65.91 84.20\n64.53 41.24 19.62 52.67 93.08 56.92 58.86 76.43 44.62\nTrans4Pass+ SGA 50.03 0.26 73.78 62.21 83.82\n61.87 32.11 10.93 20.26 92.96 38.33 61.78 74.35 37.73\nOurs 55.98 0.78 73.94 65.56 84.08\n64.39 40.96 18.31 51.64 92.98 56.53 58.14 76.06 44.42\nTable 6: Per-class mIoU results on Stanford2D3D Panoramic datasets according to the fold 1 data setting with traditional mIoU and per-pixel\naccuracy metrics. No mark for the results that the gap between Trans4Pass+ and Ours less than 5% (performance at the same level). Our\nresults will be red when Ours outperforms more than 5%. If Ours outperforms more than 10%, our results will be bold and red. There is no\nsemantic class that Trans4Pass+ outperforms Ours 5% or more.\n4.3 Ablation Study\nEffect of three modules in training process.The effec-\ntiveness of SGA image projection, SDPE, and panorama-\naware loss are studied on Stanford2D3D Panoramic datasets\nofficial fold 1 with traditional metrics as shown in Table 4. (a)\nSGA image projection: Using it alone improves the baseline\nmIoU / per pixel accuracy by 1.020% / 0.820%. (b) SDPE:\nUsing SDPE alone outperforms the baseline by 0.937% and\n0.025% in mIoU and per pixel accuracy. (b) Panorama-aware\nloss: Using it alone improves the baseline by 1.216% and\n0.250% in mIoU and per pixel accuracy.\nEffect of SGA validation. We demonstrate the effect of\nSGA validation, which means a stronger generalizability to\nresist 3D rotational perturbation. We carried out experi-\nments with two smaller disturbance settings on the pitch and\nroll axes ((β, γ, α) = (1 ◦, 1◦, 360◦) / (0◦, 0◦, 360◦)), which\nare more favorable settings for Trans4PASS+ [Zhang et al.,\n2022b], because it is designed for the standard panoramic\nview image ((β use, γuse, αuse) = (0 ◦, 0◦, 0◦)). The overall\nstatistical results are shown in Table 5. For the (β, γ, α) =\n(1◦, 1◦, 360◦) setting, an improvement of approximately 2.7\n% / 1.7 % is obtained for the mean mIoU / pixel accuracy.\nOur variance is approximately 1\n5 / 1\n10 and our fluctuation\nrange is approximately 1\n2 / 1\n3 in mIoU / pixel precision. In\n(β, γ, α) = (0 ◦, 0◦, 360◦) setting, mean mIoU / pixel ac-\ncuracy gains approximately 2.6% / 1.6% improvement, vari-\nances / fluctuation is approximately 1\n2 / 2\n3 for SGAT4PASS.\nSGAT4PASS has better robustness even with little 3D per-\nturbations. The detailed performance of these two settings\nand the performance of several random rotation settings are\nshown in Section A “Detailed Performance of SGA Valida-\ntion” in the supplementary material.\n4.4 Discussion and Visualizations\nPerformance of all semantic classes and visualizations.\nWe show the detailed performance of all 13 semantic classes\non the Stanford2D3D Panoramic datasets with both tradi-\ntional and SGA metrics in Table 6, respectively. We focus\nmainly on the classes with significant performance gaps and\nmark the gap larger than 5% / 10% as red numbers / bold red\nnumbers, respectively. There is no semantic class for which\nthe baseline is significantly better. From the results, we can\nlearn that the “sofa” and “door” classes improve more. An\nimage with “door” and “sofa” is visualized in Figure 3. Ro-\ntation of the pitch / roll / yaw axis is 5◦ / 5◦ / 180◦. The\n0.1 0.2 0.3 0.4 0.5\nw\n50\n52\n54\n56mIoU (%)\nw\n(a) λw-mIoU\n0.1 0.2 0.3 0.4 0.5\ns\n50\n52\n54\n56mIoU (%)\ns\n (b) λs-mIoU\nFigure 4: Influence of λs and λw in SGAT4PASS. The results are\ncarried out on Stanford2D3D Panoramic datasets official fold 1.\nbaseline prediction gap between the original and rotated in-\nput is large, which means less robustness. It predicts the door\nnear the right boundary in Figure 3c overall right, but it is\ntotally wrong with rotation in Figure 3g when SGAT4PASS\npredicts both correct. The baseline predictions for the sofa\nchange a lot with rotation when SGAT4PASS is stable. More\nvisualizations are shown in Section B “More Visualizations”\nin the supplementary material.\nDifferent hyperparameters. λw and λs are hyperparame-\nters in our SGAT4PASS. λs / λw determines the proportion\nof our constraint of spherical geometry in SDPE / panorama-\naware loss. We apply them on the baseline, respectively. Tra-\nditional mIoU results are shown in Figure 4a and Figure 4b,\nand we choose 0.3 / 0.3 as the final λw / λs.\n5 Conclusion\nWe have studied an underexplored but important field in\npanoramic semantic segmentation, i.e., the robustness of\ndealing with 3D disturbance panoramic input images. We\nhave shown that using our SGA framework is key to im-\nproving the semantic segmentation quality of 3D disturbance\ninputs. It applies spherical geometry prior to panoramic se-\nmantic segmentation and gains considerable improvement. In\ndetail, the SGA framework includes SGA image projection,\nSDPE, and panorama-aware loss. We also validated the ef-\nfectiveness of our SGAT4PASS on popular datasets with the\ntraditional metrics and the proposed SGA metrics, and stud-\nied its properties both empirically and theoretically.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1131\nAcknowledgements\nThis work is supported in part by National Key Re-\nsearch and Development Program of China under Grant\n2020AAA0107400, National Natural Science Foundation of\nChina under Grant U20A20222, National Science Foundation\nfor Distinguished Young Scholars under Grant 62225605,\nResearch Fund of ARC Lab, Tencent PCG, The Ng Teng\nFong Charitable Foundation in the form of ZJU-SUTD IDEA\nGrant, 188170-11102 as well as CCF-Zhipu AI Large Model\nFund (CCF-Zhipu202302).\nContribution Statement\nXuewei Li and Tao Wu contributed equally to this work.\nReferences\n[Ai et al., 2022] Hao Ai, Zidong Cao, Jinjing Zhu, Haotian\nBai, Yucheng Chen, and Ling Wang. Deep learning for\nomnidirectional vision: A survey and new perspectives.\narXiv preprint arXiv:2205.10468, 2022.\n[Armeni et al., 2017] Iro Armeni, Sasha Sax, Amir R Zamir,\nand Silvio Savarese. Joint 2d-3d-semantic data for indoor\nscene understanding. arXiv preprint arXiv:1702.01105,\n2017.\n[Berenguel-Baeta et al., 2022] Bruno Berenguel-Baeta, Je-\nsus Bermudez-Cameo, and Jose J Guerrero. Fred-\nsnet: Joint monocular depth and semantic segmenta-\ntion with fast fourier convolutions. arXiv preprint\narXiv:2210.01595, 2022.\n[Chen et al., 2021] Zhiyang Chen, Yousong Zhu, Chaoyang\nZhao, Guosheng Hu, Wei Zeng, Jinqiao Wang, and Ming\nTang. Dpt: Deformable patch-based transformer for visual\nrecognition. In Proc. ACM MM, 2021.\n[Dai et al., 2017] Jifeng Dai, Haozhi Qi, Yuwen Xiong,\nYi Li, Guodong Zhang, Han Hu, and Yichen Wei. De-\nformable convolutional networks. In Proc. ICCV, pages\n764–773, 2017.\n[de La Garanderie et al., 2018] Greire Payen de La Garan-\nderie, Amir Atapour Abarghouei, and Toby P Breckon.\nEliminating the blind spot: Adapting 3d object detection\nand monocular depth estimation to 360 panoramic im-\nagery. In Proc. ECCV, pages 789–807, 2018.\n[Deng et al., 2017] Liuyuan Deng, Ming Yang, Yeqiang\nQian, Chunxiang Wang, and Bing Wang. Cnn based se-\nmantic segmentation for urban traffic scenes using fisheye\ncamera. In Proc. IEEE Intell. Vehicles Symp., pages 231–\n236. IEEE, 2017.\n[Eder et al., 2020] Marc Eder, Mykhailo Shvets, John Lim,\nand Jan-Michael Frahm. Tangent images for mitigating\nspherical distortion. In Proc. CVPR, 2020.\n[Esteves et al., 2020] Carlos Esteves, Ameesh Makadia, and\nKostas Daniilidis. Spin-weighted spherical cnns. Proc.\nNeurIPS, 33:8614–8625, 2020.\n[Gao et al., 2022] Shaohua Gao, Kailun Yang, Hao Shi, Kai-\nwei Wang, and Jian Bai. Review on panoramic imaging\nand its applications in scene understanding. arXiv preprint\narXiv:2205.05570, 2022.\n[Hu et al., 2022] Xing Hu, Yi An, Cheng Shao, and Hu-\nosheng Hu. Distortion convolution module for semantic\nsegmentation of panoramic images based on the image-\nforming principle. IEEE Trans. Instrum. Meas., 71:1–12,\n2022.\n[Jiang et al., 2019] Chiyu Jiang, Jingwei Huang, Karthik\nKashinath, Philip Marcus, Matthias Niessner, et al.\nSpherical cnns on unstructured grids. arXiv preprint\narXiv:1901.02039, 2019.\n[Kingma and Ba, 2015] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In Proc.\nICLR, 2015.\n[Lee et al., 2018] Yeonkun Lee, Jaeseok Jeong, Jongseob\nYun, Wonjune Cho, and Kuk-Jin Yoon. Spherephd: Ap-\nplying cnns on a spherical polyhedron representation of\n360 degree images. arXiv preprint arXiv:1811.08196,\n2018.\n[Ma et al., 2021] Chaoxiang Ma, Jiaming Zhang, Kailun\nYang, Alina Roitberg, and Rainer Stiefelhagen. Densep-\nass: Dense panoramic semantic segmentation via unsuper-\nvised domain adaptation with attention-augmented context\nexchange. In Proc. ITSC, pages 2766–2772. IEEE, 2021.\n[Pintore et al., 2021] Giovanni Pintore, Marco Agus, Eva\nAlmansa, Jens Schneider, and Enrico Gobbetti. Slicenet:\ndeep dense depth estimation from a single indoor\npanorama using a slice-based representation. In Proc.\nCVPR, pages 11536–11545, 2021.\n[Poulin-Girard and Thibault, 2012] Anne-Sophie Poulin-\nGirard and Simon Thibault. Optical testing of panoramic\nlenses. Opt. Eng., 51(5):053603, 2012.\n[Rao et al., 2021] Yongming Rao, Wenliang Zhao, Benlin\nLiu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit:\nEfficient vision transformers with dynamic token sparsifi-\ncation. Proc. NeurIPS, 34:13937–13949, 2021.\n[Shen et al., 2022] Zhijie Shen, Chunyu Lin, Kang Liao,\nLang Nie, Zishuo Zheng, and Yao Zhao. Panoformer:\nPanorama transformer for indoor 360 ◦ depth estimation.\nIn Proc. ECCV, pages 195–211. Springer, 2022.\n[Sun et al., 2021] Cheng Sun, Min Sun, and Hwann-Tzong\nChen. Hohonet: 360 indoor holistic understanding with\nlatent horizontal features. In Proc. CVPR, pages 2573–\n2582, 2021.\n[Tateno et al., 2018] Keisuke Tateno, Nassir Navab, and\nFederico Tombari. Distortion-aware convolutional filters\nfor dense prediction in panoramic images. In Proc. ECCV,\npages 707–722, 2018.\n[Wang et al., 2021a] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\nIn Proc. ICCV, pages 568–578, 2021.\n[Wang et al., 2021b] Yulin Wang, Rui Huang, Shiji Song,\nZeyi Huang, and Gao Huang. Not all images are worth\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1132\n16x16 words: Dynamic transformers for efficient image\nrecognition. Proc. NeurIPS, 34:11960–11973, 2021.\n[Xia et al., 2022] Zhuofan Xia, Xuran Pan, Shiji Song, Li Er-\nran Li, and Gao Huang. Vision transformer with de-\nformable attention. In Proc. CVPR, pages 4794–4803,\n2022.\n[Xu et al., 2018] Mai Xu, Yuhang Song, Jianyi Wang,\nMingLang Qiao, Liangyu Huo, and Zulin Wang. Predict-\ning head movement in panoramic video: A deep reinforce-\nment learning approach. IEEE Trans. Pattern Anal. Mach.\nIntell., 41(11):2693–2708, 2018.\n[Xu et al., 2021] Yanyu Xu, Ziheng Zhang, and Shenghua\nGao. Spherical dnns and their applications in 360 images\nand videos. IEEE Trans. Pattern Anal. Mach. Intell., 2021.\n[Xu et al., 2022] Yifan Xu, Zhijie Zhang, Mengdan Zhang,\nKekai Sheng, Ke Li, Weiming Dong, Liqing Zhang,\nChangsheng Xu, and Xing Sun. Evo-vit: Slow-fast token\nevolution for dynamic vision transformer. In Proc. AAAI,\nvolume 36, pages 2964–2972, 2022.\n[Yang et al., 2019] Kailun Yang, Xinxin Hu, Luis M\nBergasa, Eduardo Romera, and Kaiwei Wang. Pass:\nPanoramic annular semantic segmentation. IEEE Trans.\nIntell. Trans. Syst., 21(10):4171–4185, 2019.\n[Yang et al., 2020] Kailun Yang, Xinxin Hu, Hao Chen,\nKaite Xiang, Kaiwei Wang, and Rainer Stiefelhagen. Ds-\npass: Detail-sensitive panoramic annular semantic seg-\nmentation through swaftnet for surrounding sensing. In\nProc. IEEE Intell. Vehicles Symp., pages 457–464. IEEE,\n2020.\n[Yang et al., 2021a] Kailun Yang, Xinxin Hu, and Rainer\nStiefelhagen. Is context-aware cnn ready for the surround-\nings? panoramic semantic segmentation in the wild. IEEE\nTrans. Image Process., 30:1866–1881, 2021.\n[Yang et al., 2021b] Kailun Yang, Jiaming Zhang, Simon\nReiß, Xinxin Hu, and Rainer Stiefelhagen. Capturing\nomni-range context for omnidirectional segmentation. In\nProc. CVPR, pages 1376–1386, 2021.\n[Ye et al., 2021] Zhiyuan Ye, Hai-Bo Wang, Jun Xiong, and\nKaige Wang. Ghost panorama using a convex mirror.Opt.\nLett., 46(21):5389–5392, 2021.\n[Yin et al., 2022] Hongxu Yin, Arash Vahdat, Jose M Al-\nvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-\nvit: Adaptive tokens for efficient vision transformer. In\nProc. CVPR, pages 10809–10818, 2022.\n[Yue et al., 2021] Xiaoyu Yue, Shuyang Sun, Zhanghui\nKuang, Meng Wei, Philip HS Torr, Wayne Zhang, and\nDahua Lin. Vision transformer with progressive sampling.\nIn Proc. ICCV, pages 387–396, 2021.\n[Zhang et al., 2022a] Jiaming Zhang, Kailun Yang, Chaoxi-\nang Ma, Simon Reiß, Kunyu Peng, and Rainer Stiefelha-\ngen. Bending reality: Distortion-aware transformers for\nadapting to panoramic semantic segmentation. In Proc.\nCVPR, pages 16917–16927, 2022.\n[Zhang et al., 2022b] Jiaming Zhang, Kailun Yang, Hao Shi,\nSimon Reiß, Kunyu Peng, Chaoxiang Ma, Haodong Fu,\nKaiwei Wang, and Rainer Stiefelhagen. Behind every do-\nmain there is a shift: Adapting distortion-aware vision\ntransformers for panoramic semantic segmentation. arXiv\npreprint arXiv:2207.11860, 2022.\n[Zheng et al., 2023] Zishuo Zheng, Chunyu Lin, Lang Nie,\nKang Liao, Zhijie Shen, and Yao Zhao. Complementary\nbi-directional feature compression for indoor 360deg se-\nmantic segmentation with self-distillation. InProc. WACV,\npages 4501–4510, 2023.\n[Zhu et al., 2020] Xizhou Zhu, Weijie Su, Lewei Lu, Bin\nLi, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection.\narXiv preprint arXiv:2010.04159, 2020.\n[Zhuang et al., 2022] Chuanqing Zhuang, Zhengda Lu,\nYiqun Wang, Jun Xiao, and Ying Wang. Acdnet:\nAdaptively combined dilated convolution for monocular\npanorama depth estimation. In Proc. AAAI, volume 36,\npages 3653–3661, 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1133",
  "topic": "Computer vision",
  "concepts": [
    {
      "name": "Computer vision",
      "score": 0.6832748055458069
    },
    {
      "name": "Computer science",
      "score": 0.6636444330215454
    },
    {
      "name": "Panorama",
      "score": 0.6530430912971497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6209839582443237
    },
    {
      "name": "Embedding",
      "score": 0.5776770710945129
    },
    {
      "name": "Segmentation",
      "score": 0.5651248693466187
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5543790459632874
    },
    {
      "name": "Pixel",
      "score": 0.5005409717559814
    },
    {
      "name": "Geometry",
      "score": 0.3923361599445343
    },
    {
      "name": "Mathematics",
      "score": 0.23100429773330688
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I168879160",
      "name": "Zhejiang University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ]
}