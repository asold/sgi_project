{
    "title": "Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer",
    "url": "https://openalex.org/W3087738387",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2559549025",
            "name": "Lu, Siyuan",
            "affiliations": [
                "Nanjing University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2354108523",
            "name": "Wang Mei-qi",
            "affiliations": [
                "Nanjing University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2121778622",
            "name": "Liang Shuang",
            "affiliations": [
                "Nanjing University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2061151323",
            "name": "Lin Jun",
            "affiliations": [
                "Nanjing University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2315005619",
            "name": "Wang Zhong-feng",
            "affiliations": [
                "Nanjing University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2947946877",
        "https://openalex.org/W3017024317",
        "https://openalex.org/W2966892770",
        "https://openalex.org/W2608093348",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3203309275",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2910396952",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868"
    ],
    "abstract": "Designing hardware accelerators for deep neural networks (DNNs) has been much desired. Nonetheless, most of these existing accelerators are built for either convolutional neural networks (CNNs) or recurrent neural networks (RNNs). Recently, the Transformer model is replacing the RNN in the natural language processing (NLP) area. However, because of intensive matrix computations and complicated data flow being involved, the hardware design for the Transformer model has never been reported. In this paper, we propose the first hardware accelerator for two key components, i.e., the multi-head attention (MHA) ResBlock and the position-wise feed-forward network (FFN) ResBlock, which are the two most complex layers in the Transformer. Firstly, an efficient method is introduced to partition the huge matrices in the Transformer, allowing the two ResBlocks to share most of the hardware resources. Secondly, the computation flow is well designed to ensure the high hardware utilization of the systolic array, which is the biggest module in our design. Thirdly, complicated nonlinear functions are highly optimized to further reduce the hardware complexity and also the latency of the entire system. Our design is coded using hardware description language (HDL) and evaluated on a Xilinx FPGA. Compared with the implementation on GPU with the same setting, the proposed design demonstrates a speed-up of 14.6x in the MHA ResBlock, and 3.4x in the FFN ResBlock, respectively. Therefore, this work lays a good foundation for building efficient hardware accelerators for multiple Transformer networks.",
    "full_text": "Hardware Accelerator for Multi-Head Attention and\nPosition-Wise Feed-Forward in the Transformer\nSiyuan Lu, Meiqi Wang, Shuang Liang, Jun Lin, and Zhongfeng Wang\nSchool of Electronic Science and Engineering, Nanjing University, Nanjing, China\nEmail: {sylu, mqwang, sliang }@smail.nju.edu.cn, {jlin, zfwang}@nju.edu.cn\nAbstractâ€”Designing hardware accelerators for deep neural\nnetworks (DNNs) has been much desired. Nonetheless, most\nof these existing accelerators are built for either convolutional\nneural networks (CNNs) or recurrent neural networks (RNNs).\nRecently, the Transformer model is replacing the RNN in the\nnatural language processing (NLP) area. However, because of\nintensive matrix computations and complicated data ï¬‚ow being\ninvolved, the hardware design for the Transformer model has\nnever been reported. In this paper, we propose the ï¬rst hardware\naccelerator for two key components, i.e., the multi-head attention\n(MHA) ResBlock and the position-wise feed-forward network\n(FFN) ResBlock, which are the two most complex layers in\nthe Transformer. Firstly, an efï¬cient method is introduced to\npartition the huge matrices in the Transformer, allowing the two\nResBlocks to share most of the hardware resources. Secondly, the\ncomputation ï¬‚ow is well designed to ensure the high hardware\nutilization of the systolic array, which is the biggest module in\nour design. Thirdly, complicated nonlinear functions are highly\noptimized to further reduce the hardware complexity and also the\nlatency of the entire system. Our design is coded using hardware\ndescription language (HDL) and evaluated on a Xilinx FPGA.\nCompared with the implementation on GPU with the same\nsetting, the proposed design demonstrates a speed-up of 14.6 Ã— in\nthe MHA ResBlock, and 3.4 Ã— in the FFN ResBlock, respectively.\nTherefore, this work lays a good foundation for building efï¬cient\nhardware accelerators for multiple Transformer networks.\nIndex Terms â€”Transformer, Natural Language Processing\n(NLP), Hardware Accelerator, FPGA, Neural Network\nI. I NTRODUCTION\nRecurrent neural networks (RNNs), long-short memory\n(LSTM) [7], and gated recurrent (GRU) [3], used to be the best\nsolutions in the natural language processing (NLP) area. This\nsituation was changed when the Transformer model [11] was\ninvented in 2017, which outperforms previous RNN models in\nmultiple tasks. By avoiding the recurrent calculations and tak-\ning full advantage of the attention mechanism, the Transformer\nand Transformer-based pre-trained language models (such as\nBERT [4], ALBERT [8], T5 [9], ERINE [10], and structBERT\n[14]) have achieved state-of-the-art accuracy in various NLP\ntasks.\nIn spite of making great progress in relative ï¬elds, the high\ncomputation complexity and huge memory requirements of\nthese powerful Transformer networks are making them hard\nto be operated in mobile devices or embedded systems. More\nand more researchers are paying attention to this problem,\nThis work was supported by the National Natural Science Foundation of\nChina under Grant 61604068, the Fundamental Research Funds for the Central\nUniversities under Grant 021014380065, the Key Research Plan of Jiangsu\nProvince of China under Grant BE2019003-4. (Corresponding authors: Jun\nLin; Zhongfeng Wang.)\nand one way to solve it is through model compression [5].\nSeveral techniques have been used to compress these networks,\nincluding data quantization [2], pruning, knowledge distillation\nand Architecture-Invariant Compression (AIC) [8].\nRecently, building FPGA or ASIC hardware accelerators for\ndeep neural networks (DNNs) has achieved great success in\nboth academic and industrial societies, which makes us believe\nthat designing efï¬cient hardware architectures for these Trans-\nformer networks must be an important topic as well. By imple-\nmenting them on hardware platforms, the inference systems of\nmany NLP applications, such as machine translation, question\nanswering, and sentiment analysis, are able to achieve higher\nspeed or lower power consumption or both. However, intense\nmatrix computations, complicated data ï¬‚ow, and complex non-\nlinear functions are making it hard to design efï¬cient hardware\narchitecture for the Transformer. To the best of our knowledge,\nwe are the ï¬rst to propose a speciï¬c hardware accelerator for\nthe Transformer. In the open literature, the A3 [6] is the only\nhardware architecture for accelerating the attention mechanism\nin various neural networks, which is not speciï¬cally designed\nfor the Transformer.\nAs mentioned in [11] and [8], most of the trainable pa-\nrameters and the computations are in the multi-head attention\n(MHA) ResBlock and the position-wise feed-forward network\n(FFN) ResBlock, which is discussed by Section II in detail.\nIn this work, we design a reconï¬gurable hardware architecture\nbased on systolic array (SA) for the MHA ResBlock and the\nFFN ResBlock, which are the two most complex layers in the\nTransformer.\nMain contributions of this work can be summarized as\nfollows:\nâ€¢ We provide an efï¬cient method to partition the huge\nmatrices in the Transformer, which allows the MHA\nResBlock and the FFN ResBlock to share most of the\nhardware resources.\nâ€¢ We propose the ï¬rst hardware architecture design which\ncan complete the calculations for both these two Res-\nBlocks. To ensure the high hardware utilization of the\nSA, which is the biggest module in our design, the\ncomputation ï¬‚ow is well designed.\nâ€¢ Two most complicated nonlinear functions, including the\nscaled masked-softmax and the layer normalization, are\nhighly optimized to become more hardware-friendly. As\nthe â€œbottle-neckâ€ in the proposed architecture, the latency\nof layer normalization is reduced as much as possible.\narXiv:2009.08605v1  [eess.SP]  18 Sep 2020\nEncoder Stack\nâ€¦â€¦ NÃ—\nIdentical\nEncoder \nLayers\nInput Sequence\nPositional Encoding\nInput Embedding\nMHA ResBlock\nFFN ResBlock\nMHA ResBlock\nFFN ResBlock\nâ€¦â€¦ NÃ—\nIdentical\nDecoder \nLayers\nLast Output Sequence\nPositional Encoding\nOutput Embedding\nMHA ResBlock\nFFN ResBlock\nDecoder Stack\nMHA ResBlock\nLinear & Softmax\nMHA ResBlock\nFFN ResBlock\nMHA ResBlock\nOutput Probabilities\nFig. 1. The model architecture of the Transformer.\nAfter quantizing the Transformer base model in [11] (distin-\nguished from the Transformer big model) with 8-bit integers\n(INT8), we also evaluate our design on the Xilinx xcvu13p-\nfhga2104-3-e FPGA, when the max sequence length (denoted\nas s) is equal to 64 and the batch size is equal to 1.\nThe hardware experimental results demonstrate a speed-up of\n14.6Ã—in the MHA ResBlock, and a speed-up of 3.4 Ã—in the\nFFN ResBlock, compared to a GPU implementation on an\nNVIDIA V100.\nThe rest of this paper is organized as follows. Section II\ngives a brief review of the Transformer networks, and explains\nthe importance of accelerating the MHA ResBlock and the\nFFN ResBlock. Section III presents the method of matrix\npartitioning. Section IV describes the proposed hardware ar-\nchitecture. Experimental results are given in Section V . Section\nVI concludes this paper.\nII. B ACKGROUND AND MOTIVATION\nA. The Model Architecture of the Transformer\nThe model architecture of the Transformer is described in\nFig. 1, containing an encoder stack and a decoder stack. Notice\nthat most of the trainable parameters and the computations are\nin these two stacks, and other components beside the stacks\nsuch as the embedding layers and the softmax output layer\nhave not been taken into account by this work. As is shown\nin Fig. 1, all the encoder layers and the decoder layers are\ncomposed of two kinds of ResBlocks, the MHA ResBlock\nand the FFN ResBlock.\nFig. 2 shows the structure of the MHA ResBlock. An MHA\nResBlock has hâ€œAttention Headsâ€, and the input of each Head\nis the same as the input of the ResBlock, including three\ntensors: V (values), K (keys), and Q (queries). The Scaled\nOutput\nMulti-Head Attention\nHead-hHead-2Head-1\nScaled Dot-Product \nAttention\nâ€¦â€¦ \nV\nScaled Dot-Product \nAttention\nLinear Linear LinearLinear Linear Linear\nScaled Dot-Product \nAttention\nLinear Linear Linear\nK Q\nConcatenation\nğ•ğŸ \nğŠğŸ \nğğŸ \nğ•ğŸ \nğŠğŸ \nğğŸ \nğ•ğ¡ \nğŠğ¡ \nğğ¡ \nAttention(ğğŸ, ğŠğŸ, ğ•ğŸ) \nLinear\nLayer Normalization\nAttention(ğğŸ, ğŠğŸ, ğ•ğŸ) \nAttention(ğğ¡, ğŠğ¡, ğ•ğ¡) \nFig. 2. The structure of the MHA ResBlock.\nDot-Product Attention function in the MHA can be expressed\nas follows:\nAttention(Qi,Ki,Vi) =softmax(Mask(QiKT\niâˆšdk\n))Vi. (1)\nThe Mask operation is used to mask out all values in the\ninput of the softmax corresponding to illegal connections, and\nthe parameter dk, which is equal to 64 in both the Transformer\nbase model and the Transformer big model. The parameter h\nis equal to 8 in the base model, or equal to 16 in the big\nmodel.\nThe FFN ResBlock contains a fully connected feed-forward\nnetwork, consisting of two linear sublayers and a ReLU\nactivation between them:\nFFN (x) =ReLU(xW1 + b1)W2 + b2,\nFFN ResBlock(x) =LayerNorm(x+ FFN (x)). (2)\nB. Transformer-Based Pre-Trained Models\nAn important pre-trained model is Bidirectional Encoder\nRepresentations from Transformers (BERT). Analyses in [5]\nalso point out that, the MHA and the FFN ResBlocks still\noccupy most of the storage space and have the highest numbers\nof FLOPs.\nThe General Language Understanding Evaluation (GLUE)\nbenchmark [12] is a collection of diverse natural language\nunderstanding tasks. Recently, many Transformer-based pre-\ntrained models have obtained top placements on the GLUE\nscore list. Most of these models, such as T5 [9], ERINE\n[10], and structBERT [14], have very similar structure to the\nBERT. These facts all prove the necessity of designing efï¬cient\nhardware accelerators for the MHA and the FFN ResBlocks,\nwhich are two commonly used structures in these models.\nIII. P ARTITIONING MATRICES IN THE FFN AND THE MHA\nConsidering the characteristics of the Transformer architec-\nture, we believe that the proposed hardware accelerator should\nbe able to accelerate not only the MHA ResBlock, but also\nthe FFN ResBlock. To make sure that the MHA ResBlock and\nthe FFN ResBlock can reuse the hardware resources, we ï¬rst\nHead-1\nâ€¦â€¦ \nQ\n64\nğ–ğğŸ \ns\nğğ¦ğ¨ğğğ¥ \n Q\nğğŸ \nğğ¢ğšğ¬ğŠğŸ \nğ–ğŠğŸ (ğŸ”ğŸ’Ã— ğ’…ğ’ğ’ğ’…ğ’†ğ’) \ns\nğŠğŸ \nğ•ğŸ \n64\ns\n64\ns\n64\ns\nğğ¢ğšğ¬ğ•ğŸ \ntransposition\nScaled \nMasked-Softmax\nMask-\nMatrix\nğğŸğ‘²ğŸ\nğ‘» \ns\ns\nğ–ğ•ğŸ (ğŸ”ğŸ’Ã— ğ’…ğ’ğ’ğ’…ğ’†ğ’) \nğğ¦ğ¨ğğğ¥ \nğğ¦ğ¨ğğğ¥ \nK=V\n64\ns\nAttention(ğğŸ, ğŠğŸ, ğ•ğŸ) \nHead-h\nHead-2\nK(V)\nQ\nK(V)\nAttention(ğğŸ, ğŠğŸ, ğ•ğŸ) \nAttention(ğğ¡, ğŠğ¡, ğ•ğ¡) \ns\nğğ¦ğ¨ğğğ¥ \nğğ¦ğ¨ğğğ¥ \n â€¦â€¦ \n64\nMatrix P\nğ–ğ† \nğğ¢ğšğ¬ğ† \nğğ¦ğ¨ğğğ¥ \nMatrix Gs\nLayerNorm\nğğ¦ğ¨ğğğ¥ Outputs\nğğ¢ğšğ¬ğğŸ \n(a) The MHA ResBlock.\ns\nğğ¦ğ¨ğğğ¥ \n X\nğ–ğŸ \nğğ¦ğ¨ğğğ¥ \nğ›ğŸ \nReLU\nğ–ğŸ \nğğ¦ğ¨ğğğ¥ \nğğ’‡ğ’‡ \nğ›ğŸ \ns\nğğ¦ğ¨ğğğ¥ \nReLU(X     +    )\nğ–ğŸ s\nğğ’‡ğ’‡ \nLayerNorm\nFFN(X)\nğğ¦ğ¨ğğğ¥ Outputs\nğğ’‡ğ’‡ \nğ›ğŸ (b) The FFN ResBlock.\nFig. 3. Matrix Operations in the MHA and the FFN ResBlocks. Note that all the multiply operations marked in this ï¬gure are dealing with cross products.\nanalyze these two ResBlocks from the perspective of matrix\noperations, and then give a method to partition the matrices\nso that all the general matrix-matrix multiplications (GEMMs)\ncan be done with one and the same systolic array (SA), the\nsize of which is limited to sÃ—64.\nAssuming that the input of the FFN is called X, the shape of\nthe tensor X is the same as Q (one of the input tensors of the\nMHA), which is [ batch size,seq len q,dmodel]. Addition-\nally, Fig. 1 shows that the tensor K is always equal to the tensor\nV , the shape of which is [ batch size,seq len v,dmodel].\nIn normal circumstances, seq len q is equal to seq len v,\nso the shape of all these four tensors can be expressed\nas [ batch size,s,d model]. Supposing that the batch size is\nequal to 1, the computations of these two ResBlocks can be\nconsidered sets of matrix operations, which are represented in\nFig. 3. Obviously, an sÃ—64 SA can support all the matrix mul-\ntiplications in the Linear sublayers of all the Heads. However,\nhow to complete other multiplications between larger matrices,\nincluding P Ã—WG, XÃ—W1, and ReLU(XW1 + b1) Ã—W2,\nis another important issue to be considered.\nTABLE I\nVARIATIONS ON THE TRANSFORMER AND THE BERT ARCHITECTURES .\ndmodel dff h\nTransformer-base 512 2048 8\nTransformer-big 1024 4096 16\nBERTBASE 768 3072 12\nBERTLARGE 1024 4096 16\nTable I shows that in these Transformer networks, we all\nhave dmodel = 64h, and dff = 4dmodel = 256h. On the basis\nof this pattern, the three large weight matrices WG, W1, and\nW2 can be partitioned as shown in Fig. 4. Thus, most of the\nGEMMs can be done with an sÃ—64 SA.\nThe only one left is the operation of Qi Ã—KT\ni in each Head\nğğ¦ğ¨ğğğ¥ \nğğ¦ğ¨ğğğ¥ \nğğ’‡ğ’‡ \nğğ’‡ğ’‡ \nğğ¦ğ¨ğğğ¥ \nğğ¦ğ¨ğğğ¥ \n=64h\nğğ¦ğ¨ğğğ¥ =4\nğ–ğ†ğŸ \nğ–ğ†ğŸ \nâ€¦â€¦ \nğ–ğ†ğ¡ \n64 64\nâ€¦â€¦ \nğ–ğŸğŸ \nğ–ğŸğŸ \nğ–ğŸğŸ’ğ¡ \n64 64\nâ€¦â€¦ \nğ–ğŸğŸ \nğ–ğŸğŸ \nğ–ğŸğ¡ \nğ–ğŸ \nğ–ğ† \nğ–ğŸ \nFig. 4. Partition WG, W1, and W2.\nof the MHA. The ratio of the number of multiplications in\nthis operation to the entire MHA ResBlock can be roughly\ncalculated as follows:\ns2642h\ns2642h+ 3(64s(dmodel)2)h+ s(dmodel)3 + (64s3)h\n= s\ns+ 256h2 + 64.\n(3)\nSince 256h2 is no smaller than 16,384 and s is usually\nno bigger than 128, this ratio should be very small, which\nillustrates that the management of this single operation will\nnot inï¬‚uence the overall hardware utilization much. If s is\nsmaller than 64, it can be done with the sÃ—64 SA through\nzero padding to the Ki. Otherwise by partitioning the Qi, the\nsÃ—64 SA can still support this operation with little impact\non the utilization of the SA.\nIV. H ARDWARE ARCHITECTURE DESIGN FOR THE\nPROPOSED ACCELERATOR\nUsing the proposed method of partitioning these weight\nmatrices, the complete hardware accelerator is designed. The\ntop-level architecture is illustrated in Fig. 5.\nThe sÃ—64 SA is made up of a 2D array of processing\nelements (PE), with s rows and 64 columns. It is designed to\noutput the product matrix column by column, so each column\nhas s elements. Connected to the SA output, s adders are\nrequired to add the bias to the product matrix, and another\nAlgorithm 1: The Overall Computation Flow\n1 if Calculating MHA ResBlock then\n2 for i= 1;iâ‰¤h; i+ +do\n3 Temp1=QWQi + BiasQi;\n4 Temp2=KWKi + BiasKi;\n5 Softmax Input=Temp1 Ã—Temp2T ;\n6 Temp1=Softmax output,\nTemp2=VWV i+ BiasV i;\n7 Pi=Temp1 Ã—Temp2;\n8 end\n9 for i= 1;iâ‰¤h; i+ +do\n10 Gi=PÃ—WGi + BiasGi + Qi;\n11 end\n12 Output=LayerNorm(G);\n13 end\n14 if Calculating FFN ResBlock then\n15 for i= 1;iâ‰¤4h; i+ +do\n16 Pi=ReLU(XW1i + b1i );\n17 end\n18 for i= 1;iâ‰¤h; i+ +do\n19 Gi=PW2i + b2i + Xi;\n20 end\n21 Output=LayerNorm(G);\n22 end\n23 return Output\ns adders are required to add the residual before calculating\nthe layer normalization function. Overall, the SA Module\nhas the highest computational complexity, containing at least\n64s multipliers and 64s adders. To increase the hardware\nutilization, we make the calculations of the Softmax Module\nrunning parallel to V Ã—Wvi + BiasV 1 (line 6 in Algorithm\n1). Owing to carefully designing the computation ï¬‚ow of the\nentire system, the SA Module will hardly stop running until\nthe LayerNorm Module starts. As long as the Softmax module\ncan give the output no later than the SA module ï¬nishing\ncalculating â€œVWV i+BiasV iâ€, the latency of the entire system\nwill be determined by the SA module and the LayerNorm\nmodule. The architectures of these two nonlinear modules are\nintroduced in detail as follows.\nA. Scaled Masked-Softmax\nThe Softmax module in the proposed architecture is used\nto calculate the scaled masked-softmax function. For the con-\nvenience of discussion, we named the input matrix Qi Ã—KT\ni\n(refer to line 5 in Algorithm 1) as D, the shape of which is\nsÃ—s. The output matrix is deï¬ned as Y, and the mask matrix is\ndeï¬ned as M. Therefore, the scaled masked-softmax function\ncan be expressed as:\nY(i,j) ={\nexp(D(i,j)\n8 )/âˆ‘s\nj=1,M(i,j)=0(exp(D(i,j)\n8 )) M(i,j) = 0,\n0 M(i,j) = 1.\n(4)\nSA Module\ns Ã—  64\nSoftmax Module\nWeight \nMemory\nTemp2\ns Ã—  64\nQ or X\ns Ã—  64h\nTemp1\ns Ã—  max(s,64)\nK=V\ns Ã—  64h\nInput\nInput\nP or ReLU(X    )\ns Ã—  256h\nğ–ğŸ \nBias \nMemory\nReLU\nOutput\nData Memory\nâ€¦\nâ€¦â€¦ \ns \nAdders\nLayerNorm Module s \nAdders\nFig. 5. The top-level architecture of our design.\nEXP \nUnit\nMask Matrix\n0\nD(1,j)\nIf D(1,j)>D(1,max):\nD(1,max)=D(1,j)\nSUM LN \nUnit\nStage One\nStage Four\nStage Two\nStage Three\nâ€¦â€¦ M(1,j) M(s,j)\nY(1,j)\nâ€¦â€¦ \nâˆ-\nD(1,max)\n>>3bits\nEXP \nUnit\n0\nD(s,j)\nIf D(s,j)>D(1,max):\nD(s,max)=D(s,j)\nSUM LN \nUnit\nY(s,j)\nâˆ-\nD(s,max)\n>>3bits\nFig. 6. The architecture of Softmax module. The â€œ >>â€ denotes right shift\noperation.\nAlthough the computational complexity of this Softmax\nModule is lower than the SA module, the exponentiation\nand division calculations are still quite expensive. In [13], by\nmaking good use of the log sum-exp trick [15] and designing\nalgorithmic reduction strategies for exponential function and\nlogarithmic function, a high-speed and low-complexity hard-\nware architecture for softmax function was proposed. These\ntricks and strategies are also used in this work to build an\nefï¬cient architecture for scaled masked-softmax. The division\ncalculation and numerical underï¬‚ow can be avoided by using\nthe log-sum-exp trick ( âˆ€iâˆˆ1,2,...,d k,Ï‡max â‰¥Ï‡i):\nSoftmax(Ï‡i) = exp(Ï‡i âˆ’Ï‡max)\nâˆ‘dk\nj=1 exp(Ï‡j âˆ’Ï‡max)\n= exp(Ï‡i âˆ’Ï‡max âˆ’ln(\ndkâˆ‘\nj=1\nexp(Ï‡j âˆ’Ï‡max)))\n(5)\nAccording to Equation (5), the computation of this module\ncan be broken into four different phases, which is described\nin Fig. 6. The transformations of exponential function and\nlogarithmic function allow us to build the Softmax module\nwithout using any regular multipliers and lookup tables. The\ndetailed architectures of the EXP Unit and the LN Unit are\nthe same as [13].\nB. Layer Normalization\nCalculate Matrix G\ntime\n64h cycles 64h cycles 64h cycles\n E(G,1) var(G,1)\n=var(G,1)^(-0.5)\nOutput(1,1~64h)\n64h cycles 64h cycles 64h cycles\n E(G,s) var(G,s)\n=var(G,s)^(-0.5)\nOutput(s,1~64h)\nG(1,1)~G(1,64h)\nG(s,1)~G(s,64h)\nğ«ğŸ \nğ«ğ¬ \nOutput(i,j)=\n(G(i,j)-E(G,i))*\nğ«ğ¢ \nğ›„ğ£ + ğœ·ğ’‹ \nThe straight forward way:\n64h cycles 64h cycles E(G,i)\nvar(G,i) Output(i,1~64h)\nG(i,1)~G(i,64h)\nğ«ğ¢ \nOptimized by step one:\n64h cycles E(G,i)\nvar(G,i)=E(G,i)^2-E(G.*G,i)\nOutput(i,1~64h)G(i,1)~G(i,64h)\nğ«ğ¢ \nOptimized by step one and step two:\n E(G.*G,i)\nâ€¦â€¦\nâ€¦â€¦\nLayerNorm module \nis not running LayerNorm module starts running\nFig. 7. The method to minimize the latency of the LayerNorm module.\nAs discussed in Section II, both of these two ResBlock have\nto calculate the layer normalization function before the output\nstarts. This means that the LayerNorm module is always on\nthe critical path of the system latency. In this subsection, we\npropose a method to minimize its latency.\nUnlike the batch normalization, the layer normalization does\nnot impose any restriction on the size of a mini-batch. So it is\nable to be used in the pure online regime with the batch size\nequal to 1. [1]. The layer normalization function used in these\ntwo ResBlocks is:\nOutput(i,j) =G(i,j) âˆ’E(G,i)âˆš\nvar(G,i) +Îµ\nÎ³j + Î²j, (6)\nwhere the constant Îµis equal to 10âˆ’8, which is used to avoid\nthe denominator from being zero. The variable E(G,i) is the\nmean value of all the elements in the i-th row of matrix G\n(sÃ—dmodel):\nE(G,i) = 1\ndmodel\ndmodelâˆ‘\nk=1\nG(i,k). (7)\nThe variance of these elements is deï¬ned as:\nvar(G,i) = 1\ndmodel\ndmodelâˆ‘\nk=1\n[(G(i,k) âˆ’E(G,i))2]. (8)\nInput\nSUM\nG (sÃ—64h)\nSUM\nSUM\nSUM\nâ€¦â€¦ \nâ€¦â€¦ \nâ€¦â€¦ \nâ€¦â€¦ \nG(1,j)\nG(s,j)\nG(1,j)\nG(s,j)\nâ€¦â€¦ \nG(1,t)\nG(s,t)\nâ€¦â€¦ \nE(G,1)\nE(G,s)\n-\nâ€¦â€¦ -\nx^(-0.5)\nx^(-0.5)\nğ«ğŸ \nğ«ğ¬ \nğœ·ğ’• \nğ›„ğ­ \nÎ²(64h)Î³(64h)\n-\n-\nğ›„ğ­ \nğœ·ğ’• \nOutput(1,t)\nOutput(s,t)\nâ€¦â€¦ \nâ€¦â€¦ \nâ€¦ \nâ€¦â€¦ \nğ›† \nğ›† \nFig. 8. The architecture of LayerNorm module.\nAccording to these above equations, the straightforward way\nto calculate the layer normalization is described in Fig. 7. To\ncalculate E(G) and var(G), at least 128hcycles are added to\nthe whole system latency.\nAs is shown in Fig. 7, there are two steps in our method of\nminimizing the delay of this module, and the key is to make\nthe LayerNorm module start running in advance. The ï¬rst\nstep is using saccumulators to calculate âˆ‘dmodel\nk=1 G(i,k), and\nkeeping them connected directly to the input of this module.\nThe second step is choosing another way to calculate the\nvariance:\nvar(G,i) =E(G,i)2 âˆ’ 1\ndmodel\ndmodelâˆ‘\nk=1\nG(i,k)2. (9)\nAt last, very few cycles are required between the system\nï¬nishing calculating all the elements of matrix G and starting\nthe output, which also means the latency of the entire system\nis further reduced. The architecture of the LayerNorm module\nis described in Fig. 8. The â€œxË†(-0.5)â€ unit is implemented with\na lookup table in our experiment.\nV. E XPERIMENTAL RESULTS\nA. Quantization of Transformer Base Model\nBefore evaluating our complete design with FGPA, we\nquantize a Transformer base model for a machine translation\ntask 1. This model has been trained and tested with IWSLT\n2016 German-English parallel corpus, and the test BLEU score\nis 23.88 on â€œtst2014â€. Learning from [2], replacing FP32 with\nINT8 in the Transformer can greatly reduce the computational\ncomplexity with limited accuracy loss.\nSince linear approximation is used in the exponential func-\ntion and the logarithmic function of the Softmax module, the\n1https://github.com/Kyubyong/transformer\nprocess of the quantization is divided into two steps. First, all\nthe trainable variable matrices and activation matrices in Fig. 3\nare all quantized with INT8, but the internal calculations in the\nScaled Masked-Softmax operation are still implemented with\nFP32. After that the BLEU score drops to 23.48, proving that\nquantizing with INT8 in this network is acceptable. Second,\nthe Softmax module is quantized based on the ï¬xed-point\nmodel built in the ï¬rst step. The previously mentioned log-\nsum-exp trick and the transformations of exponential function\nand logarithmic function are used. The ï¬nal BLEU score of\nthe quantized Transformer base model is 23.57, which is even\na little higher than 23.48. These results also show that using\nthe simpliï¬ed architecture for softmax designed in [13] has\nlittle impact on the accuracy of this translation task.\nB. Hardware Implementation Results\nBy setting the batch size to 1 and the max sequence\nlength to 64, the proposed architecture is evaluated on Xilinx\nxcvu13p-fhga2104-3-e FPGA by using the Vivado 2018.2. The\nsimulation results show that it takes 21,344 cycles and 42,099\ncycles to ï¬nish the calculation of MHA ResBlock and FFN\nResBlock, respectively. The Vivado implementation results\nshow that our design can run up to 200MHz, and the total on-\nchip power is 16.7W (13.3W dynamic power and 3.4W device\nstatic power). The utilization report is presented in TABLE II.\nTABLE II\nUTILIZATION REPORT FOR THE PROPOSED HARDWARE ACCELERATOR\nAND ITS PRIMARY MODULES\nLUT CLB Registers BRAM DSP\nAvailable 1728000 3456000 2688 12288\nTop 471563 217859 498 129\n64Ã—64 SA 420867 173110 0 0\nSoftmax 21190 32623 0 0\nLayerNorm 10551 5325 27.5 129\nWeight Memory 3379 80 456 0\nUsing the same hyper parameters (batch size equal to 1\nand max sequence length equal to 64), we also measure\nthe latency of these two layers in a GPU implementation\nof the Transformer base model 2 on an NVIDIA V100. The\ncomparison results are shown in TABLE III, proving that our\ndesign is able to accelerate the inference for the Transformer\non FPGA platform.\nTABLE III\nCOMPARISONS BETWEEN FPGA AND GPU L ATENCY RESULTS\nFPGA Latency GPU Latency Speed-Up\nMHA ResBlock 106.7us 1557.8us 14.6Ã—\nFFN ResBlock 210.5us 713.4us 3.4Ã—\nVI. C ONCLUSION AND FUTURE WORK\nIn this work, we present the ï¬rst hardware accelerator for\nthe MHA ResBlock and the FFN ResBlock in the Transformer.\nThe FPGA implementation shows promising results in terms\n2https://github.com/jadore801120/attention-is-all-you-need-pytorch\nof both speed and power, which demonstrates that this design\ncan contribute to operating the Transformer network in mobile\ndevice or embedded systems. In the future, we will build\na FPGA or ASIC accelerator for the complete Transformer\ninference.\nREFERENCES\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffery E. Hinton. Layer\nnormalization. arXiv preprint arXiv:1607.06450 , 2016.\n[2] Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada, Vivek Menon,\nSun Choi, Kushal Datta, and Vikram Saletore. Efï¬cient 8-bit quantiza-\ntion of transformer neural machine language translation model. arXiv\npreprint arXiv:1906.00532, 2019.\n[3] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Ben-\ngio. Empirical evaluation of gated recurrent neural networks on sequence\nmodeling. In NIPS 2014 Workshop on Deep Learning, December 2014 ,\n2014.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language un-\nderstanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) ,\npages 4171â€“4186, 2019.\n[5] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang,\nDeming Chen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov.\nCompressing large-scale transformer-based models: A case study on\nbert. arXiv preprint arXiv:2002.11985 , 2020.\n[6] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H Oh, Yeonhong\nPark, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W\nLee, et al. a3: Accelerating attention mechanisms in neural networks\nwith approximation. In 2020 IEEE International Symposium on High\nPerformance Computer Architecture (HPCA) , pages 328â€“341. IEEE,\n2020.\n[7] Sepp Hochreiter and J Â¨urgen Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735â€“1780, 1997.\n[8] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,\nPiyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised\nlearning of language representations. In International Conference on\nLearning Representations, 2019.\n[9] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Explor-\ning the limits of transfer learning with a uniï¬ed text-to-text transformer.\narXiv preprint arXiv:1910.10683 , 2019.\n[10] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han\nZhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie:\nEnhanced representation through knowledge integration. arXiv preprint\narXiv:1904.09223, 2019.\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems , pages 6000â€“6010, 2017.\n[12] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. Glue: A multi-task benchmark and analysis\nplatform for natural language understanding. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP , pages 353â€“355, 2018.\n[13] Meiqi Wang, Siyuan Lu, Danyang Zhu, Jun Lin, and Zhongfeng Wang.\nA high-speed and low-complexity architecture for softmax function in\ndeep learning. In 2018 IEEE Asia Paciï¬c Conference on Circuits and\nSystems (APCCAS), pages 223â€“226. IEEE, 2018.\n[14] Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and\nLuo Si. Structbert: Incorporating language structures into pre-training for\ndeep language understanding. arXiv preprint arXiv:1908.04577 , 2019.\n[15] Bo Yuan. Efï¬cient hardware architecture of softmax layer in deep neural\nnetwork. 2016 29th IEEE International System-on-Chip Conference\n(SOCC), pages 323â€“326, 2016."
}