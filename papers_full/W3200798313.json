{
  "title": "Pose Transformers (POTR): Human Motion Prediction with Non-Autoregressive Transformers",
  "url": "https://openalex.org/W3200798313",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4287067991",
      "name": "Martínez-González, Angel",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A2753750709",
      "name": "Villamizar Michael",
      "affiliations": [
        "Idiap Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4287067993",
      "name": "Odobez, Jean-Marc",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2101032778",
    "https://openalex.org/W6779709467",
    "https://openalex.org/W1990429558",
    "https://openalex.org/W3040942941",
    "https://openalex.org/W2963548793",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2788865504",
    "https://openalex.org/W2964203186",
    "https://openalex.org/W6704520437",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W1735317348",
    "https://openalex.org/W2021150171",
    "https://openalex.org/W2896588340",
    "https://openalex.org/W6746208923",
    "https://openalex.org/W2993797559",
    "https://openalex.org/W3002101995",
    "https://openalex.org/W2071882725",
    "https://openalex.org/W3045246204",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6780917870",
    "https://openalex.org/W3109717189",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3109174848",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W2964089333",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3036644940",
    "https://openalex.org/W2983925976"
  ],
  "abstract": "We propose to leverage Transformer architectures for non-autoregressive human motion prediction. Our approach decodes elements in parallel from a query sequence, instead of conditioning on previous predictions such as instate-of-the-art RNN-based approaches. In such a way our approach is less computational intensive and potentially avoids error accumulation to long term elements in the sequence. In that context, our contributions are fourfold: (i) we frame human motion prediction as a sequence-to-sequence problem and propose a non-autoregressive Transformer to infer the sequences of poses in parallel; (ii) we propose to decode sequences of 3D poses from a query sequence generated in advance with elements from the input sequence;(iii) we propose to perform skeleton-based activity classification from the encoder memory, in the hope that identifying the activity can improve predictions;(iv) we show that despite its simplicity, our approach achieves competitive results in two public datasets, although surprisingly more for short term predictions rather than for long term ones.",
  "full_text": "Pose Transformers (POTR): Human Motion Prediction with\nNon-Autoregressive Transformers\nAngel Mart´ınez-Gonz´alez*†, Michael Villamizar†and Jean-Marc Odobez*†\n* ´Ecole Polytechnique F´ed´eral de Lausanne, Switzerland\n†Idiap Research Institute, Martigny, Switzerland\nangel.martinez@idiap.ch, michael.villamizar@idiap.ch, odobez@idiap.ch\nAbstract\nWe propose to leverage Transformer architectures for\nnon-autoregressive human motion prediction. Our ap-\nproach decodes elements in parallel from a query sequence,\ninstead of conditioning on previous predictions such as in\nstate-of-the-art RNN-based approaches. In such a way our\napproach is less computational intensive and potentially\navoids error accumulation to long term elements in the se-\nquence. In that context, our contributions are fourfold:\n(i) we frame human motion prediction as a sequence-to-\nsequence problem and propose a non-autoregressive Trans-\nformer to infer the sequences of poses in parallel; (ii) we\npropose to decode sequences of 3D poses from a query se-\nquence generated in advance with elements from the input\nsequence; (iii) we propose to perform skeleton-based ac-\ntivity classiﬁcation from the encoder memory, in the hope\nthat identifying the activity can improve predictions; (iv)\nwe show that despite its simplicity, our approach achieves\ncompetitive results in two public datasets, although surpris-\ningly more for short term predictions rather than for long\nterm ones.\n1. Introduction\nAn important ability of an artiﬁcial system aiming at\nhuman behaviour understanding resides in its capacity to\napprehend the human motion, including the possibility to\nanticipate motion and activities (e.g. reaching towards ob-\njects). As such, human motion prediction ﬁnds applications\nin visual surveillance or human-robot interaction (HRI) and\nhas been a hot topic researched for decades.\nWith the recent popularity of deep learning, Recurrent\nNeural Networks (RNN) have replaced conventional meth-\nods that relied on Markovian dynamics [12] and smooth\nbody motion [20] and instead learn these sequence proper-\nties from data. However, motion prediction remains a chal-\nlenging task due to the non-linear nature of the articulated\nbody structure. Although the different motions of the body\nEncoder Decoder\nTransformer ...\nQuery sequence\nProb\nActivity\nFigure 1: Proposed approach for non-autoregressive mo-\ntion prediction approach with Transformers. The input se-\nquence is encoded with the Transformer encoder. The de-\ncoder works in a non-autoregressive fashion generating the\npredictions of all poses in parallel. Finally, the encoder em-\nbeddings are used for skeleton-based activity classiﬁcation.\nlandmarks are highly correlated, their relations and tempo-\nral evolution are hard to model in learning systems.\nRecently, a family of RNN-based approaches have pro-\nposed to frame the task of human motion prediction as a\nsequence-to-sequence problem. These methods usually rely\non stacks of LSTM or GRU modules and solve the task\nwith autoregressive decoding: generating predictions one\nat a time conditioned on previous predictions [17, 2]. This\npractice has two major shortcomings. First, autoregressive\nmodels are prone to accumulate prediction errors over time:\npredicted elements are conditioned to previous predictions,\ncontaining a degree of error, thus potentially increasing the\nerror for new predictions. Second, autoregressive mod-\nelling is not parallelizable which may cause deep models to\nbe more computationally intensive since predicted elements\nare generated sequentially one at time.\nSince their breakthrough in machine translation [21],\nTransformer neural network have been adopted in other re-\nsearch areas for different sequence-to-sequence tasks such\nas automatic speech recognition [11] and object detec-\ntion [3]. These methods leverage the long range memory of\narXiv:2109.07531v1  [cs.CV]  15 Sep 2021\nthe attention modules to identify speciﬁc entries in the input\nsequence which are relevant for prediction, a shortcoming\nof RNN models. During training, Transformers allows par-\nallelization with look ahead masking. Yet, at testing time,\nthey use an autoregressive setting which makes it difﬁcult to\nleverage the parallelization capabilities. Hence, autoregres-\nsive Transformers exhibit large inference processing times\nhampering their use in applications that require real-time\nperformance such as in HRI.\nIn this paper, we thus investigate the use of non-\nautoregressive human motion prediction aiming to reduce\ncomputational cost of autoregressive inference with the\nTransformer neural network and potentially avoid error\npropagation. Our work is inline with recent methods [3, 7]\nthat perform non-autoregressive (parallel) decoding with\nTransformers. Contrary to state-of-the-art methods that rely\nonly in a Transformer encoder for human motion predic-\ntion [1, 22], our approach uses as well a Transformer de-\ncoder architecture with self- and encoder-decoder attention.\nInspired by recent research in non-autoregressive machine\ntranslation [7], we generate the inputs to the decoder in ad-\nvance with elements from the input sequence. We show that\nthis strategy, though simple, is effective and helps reducing\nthe error in short and long term horizons.\nIn addition, we explore the inclusion of activity informa-\ntion by predicting as well activity from the input sequences.\nModelling motion and activity prediction jointly has not of-\nten been investigated by previous works, though these top-\nics are highly related. Indeed, a better ability at identifying\nan activity may improve the selection of the dynamics to\nbe applied to the sequence. Hence, we propose a skeleton-\nbased activity classiﬁcation by classifying activities using\nthe encoder self-attention predictions. We train our models\njointly for activity classiﬁcation and motion prediction and\nstudy the potential of this multi-task framework. Code and\nmodels will be made public 1.\nThe rest of the paper is organized as follows. Section 2\npresents relevant state-of-the-art methods to our work. Sec-\ntion 3 introduces our approach for non-autoregressive mo-\ntion prediction with Transformers. Experimental protocol\nand results are presented in Section 4 and Section 5 con-\ncludes our work.\n2. Related work\nDeep autoregressive methods. Early deep learning ap-\nproaches used stacks of RNN units to model human mo-\ntion. For example, the work in [6] introduces an encoder-\nrecurrent-decoder (ERD) network with a stack of LSTM\nunits. The approach prevents of error accumulation and\ncatastrophic drift by including a schedule to add Gaussian\nnoise to the inputs and increase the model robustness. How-\n1https://github.com/idiap/potr\never, this scheduling is hard to tune in practice. The work\npresented in [17] uses a encoder-decoder RNN architecture\nwith a single GRU unit. The architecture includes a resid-\nual connection between decoder inputs and outputs as a\nway of modeling velocities in the predicted sequence. This\nconnectivity reduces discontinuity between input sequences\nand predictions and adds robustness at long time horizons.\nAlong this line, the approach in [2] introduce a decoder that\nexplicitly models the spatial dependencies between the dif-\nferent body parts with small specialized networks, each pre-\ndicting a speciﬁc body part (e.g. elbow). Final predictions\nare decoded following the hierarchy of the body skeleton\nwhich reduces the drift effect. Recently, a family of meth-\nods prevent the drift issue by including adversarial losses\nand enhance prediction quality with geodesic body mea-\nsurements [8] or by framing motion prediction as an pose\ninpainting process with GANs [9]. However, training with\nadversarial loses is difﬁcult and hard to stabilize.\nAttention-based approaches have recently gained inter-\nest for modeling human motion. For example, the work\npresented in [22] exploits a self-attention module to attend\nthe input sequence with a sliding window of small subse-\nquences from the input. Ideally, attention should be larger\nin elements of the input sequence that repeat with time. Pre-\ndiction works in an autoregressive fashion using a Graph\nConvolutional Network (GCN) to decode attention embed-\ndings to 3D skeletons. Along the same line [1] introduces\nan spatio-temporal self-attention module to explicitly model\nthe spatial components of the sequence. Input sequences are\nprocessed by combining two separate self-attention mod-\nules: a spatial module to model body part relationships and\na temporal module to model temporal relationships. Pre-\ndictions are generated by aggregating attention embeddings\nwith feedforward networks in an autoregressive fashion.\nOur work differs from these works. First, our archi-\ntecture is a encoder-decoder Transformer, with self- and\nencoder-decoder attention. This allows us to exploit the\nTransformer decoder to identify elements in the input se-\nquence relevant for prediction. Secondly, our architecture\nworks in non-autoregressive fashion to prevent the overhead\nof autoregressive decoding.\nNon-autoregressive modelling. Most neural network-\nbased models for sequence-to-sequence modelling use au-\ntoregressive decoding: generating entries in the sequence\none at a time conditioned on previous predicted elements.\nThis approach is not parallelizable causing deep learning\nmodels to be more computationally intensive, as in the case\nof machine translation with Transformers [21, 18]. Al-\nthough in principle Transformers are paralellizable, autore-\ngressive decoding makes impossible to leverage this prop-\nerty during inference. Therefore, recent efforts have sought\nto parallelize decoding with transformers in machine trans-\nlation using fertilities [7] and in visual object detection by\ndecoding sets [3].\nNon-autoregressive modeling has also been explored in\nthe human motion prediction literature. Clearly, the most\nchallenging aspect is to represent the temporal dependen-\ncies for decoding predictions. Most of the solutions in the\nliterature provide additional information to the decoder that\naccount for the temporal correlations in the target sequence.\nDifferent methods have been proposed relying in decoder\narchitectures that exploit temporal convolutions [14], feed-\ning the decoder with learnable embeddings [13], or rely-\ning in a representation of the sequence in the frequency do-\nmain [23]. The work presented in [23] represents the tem-\nporal dependencies using the Discrete Cosine Transform\n(DCT) of the sequence. During inference a GCN predicts\nthe DCT coefﬁcients of the target sequence. However, to\naccount for smoothness, during training, the GCN is trained\nto predict both input and target sequence DCT coefﬁcients.\nThe approach in [14] performs a similar approach, mod-\nelling separately short term and long term dependencies\nwith temporal convolutions. Their decoder is composed of\na short term and long term temporal encoders that move in\na sliding window. Short and long term information are then\nprocessed by a spatial decoder to produce pose frames.\nOur approach contrast from these methods in different\nways. First, we do not incorporate any prior information\nof the temporality of the sequences and let the Transformer\nlearn these from sequences of skeletons. Additionally, our\ndecoding process relies in a simple strategy to generate\nquery sequences from the inputs rather than relying in learn-\nable query embeddings.\n3. Method\nThe goal of our study is to explore solutions for hu-\nman motion prediction leveraging the parallelism proper-\nties of Transformers during inference. In the following\nsections we introduce our Pose Transformer (POTR), a\nnon-autoregressive Transformer for motion prediction and\nskeleton-based activity recognition.\n3.1. Problem Formulation\nGiven a sequence X = {x1:T}of 3D poses, we seek\nto predict the most likely immediate following sequence\nY = {y1:M}, where xt,yt ∈RN are N-dimensional pose\nvectors (skeletons). This problem is strongly related with\nconditional sequence modelling where the goal is to model\nthe probabilities P(Y|X; θ) with model parameters θ. In\nour work, θare the parameters of a Transformer.\nGiven its temporal nature, motion prediction has been\nwidely addressed as an autoregressive approach in an\nencoder-decoder conﬁguration: the encoder takes the con-\nditioning motion sequence x1:T and computes a represen-\ntation z1:T (memory). The decoder then generates pose\nvectors yt one by one taking z1:T and its previous gen-\nerated vectors yτ<t. While this autoregressive approach\nexplicitly models the temporal dependencies of the pre-\ndicted sequence y1:M, it requires to execute the decoder M\ntimes. This becomes computationally expensive for very\nlarge Transformers, which in principle have the property of\nparallelization (exploited during training). Moreover, au-\ntoregressive modelling is prone to propagate errors to future\npredictions: predicting pose vector yt relies in predictions\nyτ<t which in practice contain a degree of error. We ad-\ndress these limitations by modelling the problem in a non-\nautoregressive fashion as we describe in the following.\n3.2. Pose Transformers\nThe overall architecture of our POTR approach is shown\nin Figure 2. Similarly to the original Transformer [21], our\nencoder and decoder modules are composed of feed forward\nnetworks and multi-head attention modules. While the en-\ncoder architecture stays unchanged, the decoder works in a\nnon-autoregressive fashion to avoid error accumulation and\nreduce computational cost.\nOur POTR comprises three main components: a pose en-\ncoding neural network φ that computes pose embeddings\nfor each 3D pose vector in the input sequence, a non-\nautoregressive Transformer, and a pose decoding neural net-\nwork ψthat computes a sequence of 3D pose vectors. While\nthe Transformer learns the temporal dependencies, the net-\nworks φand ψshall identify spatial dependencies between\nthe different body parts for encoding and decoding pose\nvector sequences.\nMore speciﬁcally, our architecture works as follows.\nFirst, the pose encoding network φcomputes an embedding\nof dimension Dfor each pose vector in the input sequence\nx1:T. The Transformer encoder takes the sequence of pose\nembeddings (agreggated with positional embeddings) and\ncomputes the representation z1:T with a stack of L multi-\nhead self-attention layers. The Transformer decoder takes\nthe encoder outputs z1:T as well as a query sequence q1:M\nand computes an output embedding with a stack ofLmulti-\nhead self- and encoder-decoder attention layers. Finally,\npose predictions are generated in parallel by the network\nψfrom the decoder outputs and a residual connection with\nq1:M. We detail each component in the following.\nTransformer Encoder. It is composed of L layers, each\nwith a standard architecture consisting of multi-head self-\nattention modules and a feed forward networks. The en-\ncoder receives as input the sequence of pose embeddings of\ndimension Dadded with positional encodings and produces\na sequence of embeddings z1:T of the same dimensionality.\nTransformer Decoder. Our Transformer decoder follows\nthe standard architecture: it compriseLlayers of multi-head\nself- and encoder-decoder attention modules and feed for-\nward networks. In our work, every layer in the decoder gen-\nFigure 2: Overview of our approach for non-autoregressive human motion prediction. Our model is composed of networksφ\nand ψ, and a non-autoregressive Transformer built on feed forward networks and multi-head attention layers as in [21]. First,\na network φcomputes embeddings for each pose in the input sequence. Then, the Transformer processes the sequence and\ndecodes attention embeddings in parallel. Finally, the predicted sequence is generated with network ψin a residual fashion.\nActivity classiﬁcation is performed by adding a learnable class token x0 to the input sequence.\nerates predictions. The decoder receives a query sequence\nq1:M and encoder outputs z1:T and produces M output em-\nbeddings in a single pass. These are then decoded by the\nnetwork ψinto 3D body skeletons.\nThe decoding process starts by generating the input to\nthe decoder q1:M. As remarked in [7] given that non-\nautoregressive decoding exhibits complete conditional in-\ndependence between predicted elements yt, the decoder in-\nputs should account as much as possible for the time corre-\nlations between them. Additionally, q1:M should be easily\ninferred. Inspired by non-autoregressive machine transla-\ntion [7], we use a simple approach ﬁllingq1:M using copied\nentries from the encoder inputs. More precisely, each entry\nqt is a copy of a selected query pose from the encoder in-\nputs x1:T. We select the last element of the sequence xT as\nthe query pose and ﬁll the query sequence with this entry.\nGiven the residual learning setting, predicting motion can\nbe seen as predicting the necessary pose offsets from last\nconditioning pose xT to each element yt.\n3.3. Pose Encoding and Decoding\nInput and output sequences are processed from and to 3D\npose vectors with networks φand ψ respectively. The net-\nwork φis shared by the Transformer encoder and decoder.\nIt computes a representation of dimension D for each of\nthe 3D skeletons in the input and query sequences. The de-\ncoding network ψtransforms the M decoder predictions of\ndimension Dto 3D skeletons residuals independently at ev-\nery decoder layer.\nThe aim of the φ and ψ networks is to model the spa-\ntial relationships between the different elements of the body\nstructure. To do this, we investigated two approaches. In the\nﬁrst one we consider a simple approach settingφand ψwith\nsingle linear layers. In the second approach we follow [22]\nand use Graph Convolutional Networks (GCN) that densely\nlearn the spatial connectivity in the body.\nTo make our manuscript self contained, we brieﬂy in-\ntroduce how GCNs work in our human motion prediction\napproach. Given a feature representation of the human\nbody with Knodes, a GCN learns the relationships between\nnodes with the strength of the graph edges represented by\nthe adjacency matrix A ∈RK×K. Examples of represen-\ntations are body skeletons or embeddings. A GCN layer\nl takes as input a matrix of node features Hl−1 ∈RK×F\nwith F features per node, and a set of learnable weights\nWl ∈RF×O. Then, the layer computes output features\nHl = σ(AlHl−1Wl), (1)\nwhere σ is an activation function. A network is com-\nposed by stacking layers which aggregates features of the\nvicinity of the nodes.\nOur GCN architecture is shown in Figure 3. It is in-\nspired in the architecture presented in [22], where matrices\nAl and weights Wl are learnt. It is composed of a stack\nof Sresidual modules of graph convolution layers followed\nby batch normalization, tanh activations and dropout layers.\nxSGraph convolution layer\nBatch norm\nTanh\nDropout\nGC GC GC GC\nFigure 3: Our Graph Convolutional Network architecture.\nIt comprises graph convolution layers followed by tanh\nactivations, batch normalization, and dropout layers. As\nin [22], our architecture has Sresidual connections.\nWe set the internal feature dimension to F = 512features\nper node until the output layer that generates pose embed-\ndings. Though we can normally squeeze as many inner lay-\ners, we set S = 1.\n3.4. Activity Recognition\nActivity can normally be understood as a sequence of\nmotion of the different body parts in interaction with the\nscene context (objects or people). In our method, the Trans-\nformer encoder encodes the body motion with a series of\nself-attention layers. We explore the use of encoder outputs\nz1:T for activity classiﬁcation (as a second task) and train a\nclassiﬁer to determine the action corresponding to the mo-\ntion sequence presented as input to the Transformer.\nWe explore two approaches. The ﬁrst approach consist\non using the entire Transformer encoder outputs z1:T as in-\nput to the classiﬁer. However, these normally contain many\nzeroed entries suppressed by the probability maps normal-\nization in the multi-head attention layers. Naively using\nthese for activity classiﬁcation might lead our classiﬁer to\nstruggle in discarding these many zero elements. Therefore,\nsimilar to [4], we include a specializedclass token in the in-\nput sequence to store information about the activity of the\nsequence. The class token x0 is a learnable embedding that\nis padded to input sequence to form x0:T. In the output of\nencoder embeddings z0:T, z0 works as the activity represen-\ntation of the encoded motion sequence. To perform activity\nclassiﬁcation we feed z0 to a single linear layer to predict\nclass probabilities for Cactivity classes (see Figure 2).\n3.5. Training\nWe train our model in a multi-task fashion to jointly\npredict motion and activity. Let ˆyl\n1:M be the predicted\nsequence of N-dimensional pose vectors at layer l of the\nTransformer decoder. We compute the layerwise loss\nLl = 1\nM ·N\nM∑\nt=1\n||ˆyl\nt −y∗\nt||1, (2)\nwhere y∗\nt is the ground truth skeleton at target sequence\nentry t. The overall motion prediction loss Lmotion is com-\nputed by averaging the losses over all decoder layers Ll.\nFinally, we train our POTR with the loss\nLPOTR = Lmotion + λLactivity, (3)\nwhere Lactivity is the multi-class cross entropy loss and\nλ= 1.\n4. Experiments\nThis section presents the experiments we conducted to\nevaluate our approach.\n4.1. Data\nHuman 3.6M [10]. We used the Human 3.6M dataset in\nour experiments for human motion prediction. The dataset\ndepicts seven actors performing 15 activities, e.g. walk-\ning, eating, sitting, etc. We followed standard protocols for\ntraining and testing [17, 2, 23]. Subject 5 is used for testing\nwhile the others for training. Input sequences are 2 seconds\nlong and testing is performed over the ﬁrst 400 ms of the\npredicted sequence. Evaluation is done in a total of 120\nsequences (8 seeds) across all activities by computing the\nEuler angle error between predictions and ground truth.\nNTU Action Dataset [19]. The NTU-RGB+D dataset is\none of the biggest benchmark datasets for human activity\nrecognition. It is composed of 58K Kinect 2 videos of 40\ndifferent actors performing 60 different actions. We fol-\nlowed the cross subject evaluation protocol provided by the\nauthors using 40K sequences for training and 16.5K for test-\ning. Given the small length of the sequences, we set input\nand output sequences length to 1.3 seconds (40 frames) and\n660 ms (20 frames) respectively.\n4.2. Implementation details\nData Preprocessing. We apply standard normalization to\nthe input and ground truth skeletons by substracting the\nmean and dividing by the standard deviation computed over\nthe whole training set. For the H3.6M dataset we remove\nglobal translation of the skeletons and represent the skele-\ntons with rotation matrices. Skeletons in the NTU dataset\nare represented in 3D coordinates and are centred by sub-\ntracting the spine joint.\nTraining. We use Pytorch as our deep learning frame-\nwork in all our experiments. Our POTR is trained with\nAdamW [15] setting the learning rate to 10−04 and weight\ndecay to 10−05. POTR models for the H3.6M dataset are\ntrained during 100K steps with warmup schedule during\n10K steps. For the NTU dataset we train POTR models\nduring 300K steps with warmup schedule during 30K.\nModels. We set the dimension of the embeddings in our\nmilliseconds 80 160 320 400 560 1000\nPOTR-AR 0.23 0.57 0.99 1.14 1.37 1.81\nPOTR 0.23 0.55 0.94 1.08 1.32 1.79\nPOTR-GCN (enc) 0.22 0.56 0.94 1.01 1.30 1.77\nPOTR-GCN (dec) 0.24 0.57 0.96 1.10 1.33 1.77\nPOTR-GCN (full) 0.23 0.57 0.96 1.10 1.33 1.80\nTable 1: H3.6M prediction performance in terms of the Eu-\nler angle error. Top: autoregressive (POTR-AR) and non-\nautoregressive POTR models using linear layers for net-\nworks φ and ψ. Bottom: non-autoregressive models with\nGCNs for network φ(enc), network ψ(dec) and both (full).\nPOTR models to D = 128. The multi-head attention mod-\nules are set with pre-normalization and four attention heads\nand four layers in encoder and decoder.\n4.3. Evaluation metrics\nEuler Angle Error. We followed standard practices to\nmeasure the error of pose predictions in the H3.6M dataset\nby computing the euclidean norm between predictions and\nground truth Euler angle representations.\nMean Average Precision (mAP). We use mAP@10cm to\nmeasure the performance of predictions in the NTU dataset.\nA successful detection is considered when the predicted 3D\nbody landmark falls within a distance less than 10 cm from\nthe ground truth.\nMean Per Joint Position Error (MPJPE). We use the\nMPJPE to evaluate error in the NTU dataset. MPJPE mea-\nsures the average error in Euclidean distance between the\npredicted 3D body landmarks and the ground truth.\n4.4. Results\n4.4.1 Evaluation on H3.6M Dataset\nIn this section, we validate our proposed approach for mo-\ntion prediction in the H3.6M dataset.\nNon-Autoregressive Prediction. Table 1 compares the per-\nformance in terms of the Euler angle error of our POTR\nwith its autoregressive version (POTR-AR). Lower values\nare better. The autoregressive version do not use the query\npose and predicts pose vectors one at a time from its own\npredictions. Our non-autoregressive approach shows lower\nerror than its counter part in most of the time intervals.\nPose Encoding and Decoding. We experimented with the\nnetworks φand ψ using either linear layers or GCNs. Ta-\nble 1 reports the results (bottom part). We indicate when\nmodels are trained with GCN in the encoder (enc), decoder\n(dec) or in both (full). We observe that the use of GCN\nreduces the errors when it is applied exclusively to the en-\ncoder. Using a shallow GCN ( S = 1) ψ might be a weak\nattempt to decode pose vectors. However, we observed that\nthe small size of the H3.6M dataset might not be enough to\nlearn deeper GCN architectures.\nComparison with the State-Of-The-Art.Tables 2 and 3\ncompares our best performing model with the state-of-the-\nart in terms of angle error for all the activities in the dataset.\nOur POTR often obtains the ﬁrst and second lower errors in\nin the short term, and the lowest average error in the 80ms\nrange. The use of the last input sequence entry as the query\npose most probably helps to signiﬁcantly reduce the error in\nthe immediate horizons. However, this strategy introduces\nlarger errors for longer horizons where the difference be-\ntween further pose vectors in the sequence and the query\npose is larger (see Figure 4 for some examples). In such\na case, it appears that autoregressive approaches perform\nbetter as a result of updating the conditioning decoding dis-\ntribution to elements closer in time.\nAttention Weights Visualization.In Figure 5(a) we visu-\nalize the encoder-decoder attention maps for one predici-\ntion instance of four activities in the dataset. Figure 5(b)\nfurther shows the attention between elements of the input\nand predicted sequences for the walking action. Due to the\ncontinuity within such activity, we can notice a high de-\npendency (attention) between the last elements of the input\nand the ﬁrts elements of the predicted sequences, while the\nprediction of further elements also pay attention to other in-\nput elements of the input matching the same phase of the\nwalking cycle. A similar behavior is observed for the di-\nrection example. For the eating and discussion activities\ninvolving less body motion, we can notice that while the\napproach slightly attends to the last elements of the input,\nit also strongly attends to other speciﬁc segments. Further\nanalysis would be needed to analyse the behavior of these\nweight matrices.\nComputational Requirements. We measured the compu-\ntational requirements of models POTR and POTR-AR by\nthe number of sequences per second (SPS) of their forward\npass in a single Nvidia card GTX 1050. We tested mod-\nels with 4 layers in encoder and decoder, and 4 heads in\ntheir attention layers. We input sequences of 50 elements\nand predict sequences of 25 elements. POTR runs at 149.2\nSPS while POTR-AR runs at 8.9 SPS. Therefore, the non-\nautoregressive approach is less computationally intensive.\n4.4.2 Evaluation on NTU Dataset\nThis section presents our results on jointly predicting mo-\ntion and activity on the NTU dataset.\nMotion Prediction Performance. Table 4 compares our\nPOTR with the different decoding settings using the mAP.\nNotice that removing the activity loss ( λ = 0) slightly\ndrops the performance for the longer horizons. The non-\nautoregressive setting shows higher mAP than the autore-\ngressive setting, specially in long term. However, setting\nWalking Eating Smoking Discussion\nmilliseconds 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400\nZero Velocity [17] 0.39 0.68 0.99 1.15 0.27 0.48 0.73 0.86 0.26 0.48 0.97 0.95 0.31 0.67 0.94 1.04\nSeq2seq. [17] 0.28 0.49 0.72 0.81 0.23 0.39 0.62 0.76 0.33 0.61 1.05 1.15 0.31 0.68 1.01 1.09\nAGED [8] 0.22 0.36 0.55 0.67 0.17 0.28 0.51 0.64 0.27 0.43 0.82 0.84 0.27 0.56 0.76 0.83\nRNN-SPL [2] 0.26 0.40 0.67 0.78 0.21 0.34 0.55 0.69 0.26 0.48 0.96 0.94 0.30 0.66 0.95 1.05\nDCT-GCN (ST) [22]0.18 0.31 0.49 0.56 0.16 0.29 0.50 0.62 0.22 0.41 0.86 0.80 0.20 0.51 0.77 0.85\nST-Transformer [1] 0.21 0.36 0.58 0.63 0.17 0.30 0.49 0.60 0.22 0.43 0.88 0.82 0.19 0.52 0.79 0.88\nPOTR-GCN (enc) 0.16 0.40 0.62 0.73 0.11 0.29 0.53 0.68 0.14 0.39 0.84 0.82 0.17 0.56 0.85 0.96\nTable 2: H3.6M performance comparison with the state-of-the-art in terms of the Euler angle error for the common walking,\neating, smoking and discussion across different horizons.\nDirections Greeting Phoning Posing Purchases Sitting\nmilliseconds 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400\nSeq2seq [17] 0.26 0.47 0.72 0.840.75 1.17 1.74 1.830.230.430.690.820.36 0.71 1.22 1.480.51 0.97 1.07 1.160.41 1.05 1.49 1.63\nAGED [8] 0.230.390.63 0.690.56 0.81 1.30 1.460.19 0.34 0.50 0.680.31 0.58 1.12 1.340.46 0.781.01 1.070.41 0.76 1.05 1.19\nDCT-GCN (ST) [22]0.26 0.45 0.710.790.360.60 0.95 1.130.53 1.02 1.35 1.480.190.44 1.01 1.240.430.651.05 1.130.290.45 0.80 0.97\nST-Transformer [1]0.250.380.75 0.860.350.611.101.320.53 1.04 1.41 1.540.61 0.68 1.051.280.430.77 1.30 1.370.290.460.841.01\nPOTR-GCN (enc)0.200.45 0.79 0.910.290.69 1.17 1.300.50 1.10 1.50 1.650.180.521.18 1.470.33 0.631.041.090.250.47 0.92 1.09\nSitting down Taking photos Waiting Walking Dog Walking Together Average\nmilliseconds 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400\nSeq2seq. [17] 0.39 0.81 1.40 1.620.24 0.51 0.90 1.050.28 0.53 1.02 1.140.56 0.91 1.26 1.400.31 0.58 0.87 0.910.36 0.67 1.02 1.15\nAGED [8] 0.33 0.620.981.100.23 0.48 0.81 0.950.240.501.021.130.50 0.81 1.151.270.23 0.41 0.560.620.31 0.540.850.97\nDCT-GCN (ST) [22]0.300.61 0.90 1.000.140.34 0.58 0.700.230.50 0.911.140.46 0.791.121.290.15 0.34 0.52 0.570.270.52 0.83 0.95\nST-Transformer [1]0.32 0.66 0.981.100.150.380.640.750.220.510.981.220.430.781.151.300.170.370.58 0.620.30 0.55 0.90 1.02\nPOTR-GCN (enc)0.250.63 1.00 1.120.120.41 0.71 0.860.170.56 1.14 1.370.350.791.21 1.330.150.44 0.63 0.700.220.56 0.94 1.01\nTable 3: Euler angle error results for the reminder of the 11 actions in the H3.6M dataset with our main non-autoregressive\ntransformer.\nmilliseconds 80 160 320 400 500 660 avg accuracy\nPOTR-AR 0.96 0.92 0.85 0.83 0.80 0.76 0.76 0.32\nPOTR 0.96 0.93 0.89 0.87 0.86 0.840.84 0.38\nPOTR (λ= 0) 0.96 0.93 0.89 0.870.85 0.83 0.83 -\nPOTR (memory)0.96 0.92 0.88 0.87 0.85 0.83 0.83 0.30\nPOTR-GCN (enc)0.96 0.92 0.88 0.87 0.85 0.83 0.83 0.27\nPOTR-GCN (dec)0.96 0.92 0.88 0.86 0.85 0.83 0.83 0.34\nPOTR-GCN (full)0.95 0.90 0.85 0.84 0.82 0.79 0.79 0.30\nTable 4: NTU motion prediction performance in terms of\nthe mAP@10cm for different time horizons. Higher values\nare better. Model marked with memory replace the class\ntoken with the encoded memory for activity classiﬁcation.\nthe networks φand ψwith GCNs does not bring many ben-\neﬁts compared to using linear layers.\nFigure 6 compares their per body part mAP and MPJPE\nusing linear layers for φ and ψ. POTR-AR shows larger\nMPJPE and lower mAP than POTR specially for the body\nextremities (arms and legs).\nActivity Recognition. Table 4 compares the classiﬁcation\naccuracy for the different POTR conﬁgurations. Using a\nspecialized activity token shows better performance than us-\ning the encoder memory z1:T. Given that the memory em-\nbeddings contain many non-informative zeroed values the\nclassiﬁer could get stuck in an attempt to ignore them.\nTable 5 compares the classiﬁcation accuracy with state-\nof-the-art methods from sequences of 3D skeletons or color\nimages. We can see that our approach only performs inline\nwith the state-of-the-art method with the lowest accuracy,\nbut can note that methods using only skeletal information\nperform worse. Among this category, the method presented\nby [19] achieves the largest accuracy. It relies on a stack of\nLSTM modules with specialized part-based cells that pro-\ncesses groups of body parts (arms, torso and legs). Such\nan explicit scheme could potentially improve our approach\nwhich is a simpler modeling of the overall body motion,\nespecially given the size of the training set. The best perfor-\nmance overall is obtained by [16] which combines color im-\nages and skeleton modalities. In such case, including image\ncontext provides extra information that cannot be extracted\nfrom skeletal data, e.g. objects of interaction.\n5. Conclusions\nIn this paper we addressed the problem of non-\nautoregressive human motion prediction with Transformers.\nWe proposed to decode predictions in parallel from a query\nsequence generated in advance with elements from the in-\nput sequence. Additionally, we analyzed different settings\nto encode and decode pose embeddings. We leveraged the\nencoder memory embeddings to perform activity classiﬁ-\ncation with an activity token. Our non-autogressive method\nWalking\n Eating\nDiscussion\n Directions\nFigure 4: Qualitative results for the H36M dataset. We show results for four actions and show ground truth and predicted\nelements coloured in gray and red respectively.\n(a)\nEating DiscussionWalking Directions\ninputs\npredictions\n0               10              20               30              40\n  0\n  5 \n10 \n15 \n20\n0               10              20               30              40\n  0\n  5 \n10 \n15 \n20\n0               10              20               30             40\n  0\n  5 \n10 \n15 \n20\n0               10               20               30               40\n  0\n  5 \n10 \n15 \n20\ninputs\npredictions\ninputs\npredictions\ninputs\npredictions\n(b)\nt1 t3 t5 t7 t9 t11 t13 t15 t17 t19 t21 t23 t25 t27 t29 t31 t33 t35 t37 t39 t41 t49t43 t45 t47\nt1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t25t20 t24t21 t22 t23t4\nFigure 5: H3.6M datasest encoder-decoder attention weight visualization. (a) Raw encoder-decoder attention maps. Input\nand predicted entries are represented by columns and rows respectively. (b) Attention weights between input (gray) and\npredicted (blue) skeleton sequences of the walking action. Only weights larger than the median are visualized. The thickness\nof the lines are proportional to the attention weights. For visualization purposes we show only half of the input sequence;\nFigure 6: NTU per body part motion prediction perfor-\nmance in terms of (a) mAP@10cm. Higher is better and,\n(b) MPJPE. Lower is better.\noutperforms its autoregressive version in long term horizons\nand is less computationally intensive. Finally, despite the\nsimplicity of our approach we have obtained competitive\nresults on motion prediction in two public datasets.\nOur work opens the door for more research. One of\nthe main drawbacks in our method is the increased error at\nlong term horizons as a consequence of non-autoregressive\ndecoding and relying in a single pose vector as query se-\nquence. A more suitable strategy to explore would be to\nrely in a set of M query poses by sampling from the input\nMethod Skeletons RGB Accuracy\nSkeletal quads [5] √ - 38.62 %\n2 Layer P-LSTM [19] √ - 62.93 %\nMulti-task [16] √ √ 85.5 %\nMulti-task [16] - √ 84.6 %\nOurs POTR √ - 38.0 %\nOurs POTR (memory) √ - 30.0 %\nTable 5: Activity classiﬁcation performance comparison\nwith the state-of-the-art in the NTU dataset. We specify\nif methods work with skeleton sequences or color images.\nor selected using the encoder self-attention embeddings by\nposition modelling such as in [7].\nAcknowledgments: This work was supported by the Euro-\npean Union under the EU Horizon 2020 Research and Inno-\nvation Action MuMMER (MultiModal Mall Entertainment\nRobot), project ID 688146, as well as the Mexican National\nCouncil for Science and Technology (CONACYT) under\nthe PhD scholarships program.\nReferences\n[1] Emre Aksan, Peng Cao, Manuel Kaufmann, and Otmar\nHilliges. Spatio-temporal transformer for 3d human motion\nprediction. arXiv:2004.08692v2, 2021. 2, 7\n[2] Emre Aksan, Manuel Kaufmann, and Otmar Hilliges. Struc-\ntured prediction helps 3d human motion modelling. In The\nIEEE International Conference on Computer Vision (ICCV),\nOct 2019. First two authors contributed equally. 1, 2, 5, 7\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision (ECCV), 2020. 1, 2, 3\n[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929, 2020. 5\n[5] Georgios Evangelidis, Gurkirt Singh, and Radu Horaud.\nSkeletal quads: Human action recognition using joint\nquadruples. In 2014 22nd International Conference on Pat-\ntern Recognition, pages 4513–4518, 2014. 8\n[6] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji-\ntendra Malik. Recurrent network models for human dy-\nnamics. In IEEE International Conference on Computer Vi-\nsion (ICCV), ICCV ’15, page 4346–4354, USA, 2015. IEEE\nComputer Society. 2\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K.\nLi, and Richard Socher. Non-autoregressive neural machine\ntranslation. In International Conference on Learning Repre-\nsentations, 2018. 2, 4, 8\n[8] Liang-Yan Gui, Yu-Xiong Wang, Xiaodan Liang, and Jose\nM. F. Moura. Adversarial geometry-aware human motion\nprediction. In Proceedings of the European Conference on\nComputer Vision (ECCV), September 2018. 2, 7\n[9] Alejandro Hernandez, J ¨urgen Gall, and Francesc Moreno.\nHuman motion prediction via spatio-temporal inpainting. In\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 7133–7142, 2019. 2\n[10] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6m: Large scale datasets and pre-\ndictive methods for 3d human sensing in natural en viron-\nments. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 36(7):1325–1339, jul 2014. 5\n[11] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Trans-\nformers are rnns: Fast autoregressive transformers with lin-\near attention. In Proceedings of the International Conference\non Machine Learning (ICML), 2020. 1\n[12] Andreas M. Lehrmann, Peter V . Gehler, and Sebastian\nNowozin. Efﬁcient nonlinear markov models for human mo-\ntion. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1314–1321, 2014. 1\n[13] Bin Li, Jian Tian, Zhongfei Zhang, Hailin Feng, and Xi Li.\nMultitask non-autoregressive model for human motion pre-\ndiction. IEEE Transactions on Image Processing, 30:2562–\n2574, 2021. 3\n[14] Chen Li, Zhen Zhang, Wee Sun Lee, and Gim Hee Lee. Con-\nvolutional sequence to sequence model for human dynamics.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), June 2018. 3\n[15] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2019. 5\n[16] Diogo C. Luvizon, David Picard, and Hedi Tabia. 2d/3d pose\nestimation and action recognition using multitask deep learn-\ning. In IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 5137–5146, 2018. 7, 8\n[17] Julieta Martinez, Michael J. Black, and Javier Romero. On\nhuman motion prediction using recurrent neural networks. In\nCVPR, 2017. 1, 2, 5, 7\n[18] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. 2019. 2\n[19] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+d: A large scale dataset for 3d human activity anal-\nysis. In IEEE Conference on Computer Vision and Pattern\nRecognition, June 2016. 5, 7, 8\n[20] L. Sigal, M. Isard, H. Haussecker, and M. J. Black. Loose-\nlimbed people: Estimating 3D human pose and motion using\nnon-parametric belief propagation. International Journal of\nComputer Vision, 98(1):15–48, May 2011. 1\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors,Advances in Neural Infor-\nmation Processing Systems, volume 30. Curran Associates,\nInc., 2017. 1, 2, 3, 4\n[22] Mao Wei, Liu Miaomiao, and Salzemann Mathieu. History\nrepeats itself: Human motion prediction via motion atten-\ntion. In European Conference on Computer Vision (ECCV),\n2020. 2, 4, 5, 7\n[23] Mao Wei, Liu Miaomiao, Salzemann Mathieu, and Li Hong-\ndong. Learning trajectory dependencies for human motion\nprediction. In ICCV, 2019. 3, 5",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.7589507102966309
    },
    {
      "name": "Computer science",
      "score": 0.7414625287055969
    },
    {
      "name": "Transformer",
      "score": 0.7206599712371826
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6059097051620483
    },
    {
      "name": "Encoder",
      "score": 0.5623347163200378
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5189485549926758
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5168364644050598
    },
    {
      "name": "Machine learning",
      "score": 0.35912859439849854
    },
    {
      "name": "Algorithm",
      "score": 0.34802213311195374
    },
    {
      "name": "Mathematics",
      "score": 0.11935290694236755
    },
    {
      "name": "Engineering",
      "score": 0.10083553194999695
    },
    {
      "name": "Econometrics",
      "score": 0.07103997468948364
    },
    {
      "name": "Voltage",
      "score": 0.06613162159919739
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I7495430",
      "name": "Idiap Research Institute",
      "country": "CH"
    }
  ],
  "cited_by": 12
}