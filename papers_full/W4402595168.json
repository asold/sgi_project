{
  "title": "So you want your private LLM at home? A survey and benchmark of methods for efficient GPTs",
  "url": "https://openalex.org/W4402595168",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4287484651",
      "name": "Tuggener, Lukas",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Sager, Pascal",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Taoudi-Benchekroun, Yassine",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A4224611717",
      "name": "Grewe, Benjamin F.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226637062",
      "name": "Stadelmann, Thilo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6851775633",
    "https://openalex.org/W4390052133",
    "https://openalex.org/W6926044172",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6795386847",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W6854866820",
    "https://openalex.org/W3170647102",
    "https://openalex.org/W3095166428",
    "https://openalex.org/W6842258392",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W6846164622",
    "https://openalex.org/W6853048723",
    "https://openalex.org/W6802298259",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6851412683",
    "https://openalex.org/W6848221137",
    "https://openalex.org/W6677103964",
    "https://openalex.org/W2156150815",
    "https://openalex.org/W6677580257",
    "https://openalex.org/W6854094408",
    "https://openalex.org/W6848451824",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W6703652217",
    "https://openalex.org/W6717768942",
    "https://openalex.org/W2058373514",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2742229469",
    "https://openalex.org/W3156333129",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W6850936240",
    "https://openalex.org/W6839193947",
    "https://openalex.org/W6749838110",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W6782879696",
    "https://openalex.org/W3194676777",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W6803096969",
    "https://openalex.org/W6851145179",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W6849594959",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W6846453221",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W6852874933",
    "https://openalex.org/W4285192178"
  ],
  "abstract": "At least since the introduction of ChatGPT, the abilities of generative large language models (LLMs), sometimes called GPTs, are at the center of the attention of AI researchers, entrepreneurs, and others. However, for many applications, it is not possible to call an existing LLM service via an API due to data protection concerns or when no task-appropriate LLM exists. On the other hand, deploying or training a private LLM is often prohibitively computationally expensive. In this paper, we give an overview of the most important recent methodologies that help reduce the computational footprint of LLMs. We further present extensive benchmarks for seven methods from two of the most important areas of recent progress: model quantization and low-rank adapters, showcasing how it is possible to leverage state-of-the-art LLMs with limited resources. Our benchmarks include resource consumption metrics (e.g. GPU memory usage), a state-of-the-art quantitative performance evaluation as well as a qualitative performance study conducted by eight individual human raters. Our evaluations show that quantization has a profound effect on GPU memory requirements. However, we also show that these quantization methods, contrary to how they are advertised, cause a noticeable loss in text quality. We further show that low-rank adapters allow effective model fine-tuning with moderate compute resources. For methods that require less than 16 GB of GPU memory, we provide easy-to-use Jupyter notebooks that allow anyone to deploy and fine-tune state-of-theart LLMs on the Google Colab free tier within minutes without any prior experience or infrastructure.",
  "full_text": null,
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.8662428259849548
    },
    {
      "name": "Computer science",
      "score": 0.4837510585784912
    },
    {
      "name": "Telecommunications",
      "score": 0.3359203338623047
    },
    {
      "name": "Geography",
      "score": 0.07347598671913147
    },
    {
      "name": "Cartography",
      "score": 0.0696631669998169
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    }
  ]
}