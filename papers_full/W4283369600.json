{
  "title": "Industrial cylinder liner defect detection using a transformer with a block division and mask mechanism",
  "url": "https://openalex.org/W4283369600",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100318591",
      "name": "Qian Liu",
      "affiliations": [
        "Nanjing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5083815471",
      "name": "Xiaohua Huang",
      "affiliations": [
        "Nanjing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5082109737",
      "name": "Xiuyan Shao",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A5019435679",
      "name": "Fei Hao",
      "affiliations": [
        "Nanjing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2902323275",
    "https://openalex.org/W2156300716",
    "https://openalex.org/W1993705000",
    "https://openalex.org/W2971058209",
    "https://openalex.org/W3121831259",
    "https://openalex.org/W3111544438",
    "https://openalex.org/W2923486253",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W2469312016",
    "https://openalex.org/W2884367402",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2982512126",
    "https://openalex.org/W2965127303",
    "https://openalex.org/W2982284503",
    "https://openalex.org/W2795853800",
    "https://openalex.org/W2768955070",
    "https://openalex.org/W2406523001",
    "https://openalex.org/W2912069721",
    "https://openalex.org/W3003895596",
    "https://openalex.org/W2910591031",
    "https://openalex.org/W2978871429",
    "https://openalex.org/W2806070179",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2989676862",
    "https://openalex.org/W3104156061",
    "https://openalex.org/W3106250896"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports\nIndustrial cylinder liner defect \ndetection using a transformer \nwith a block division and mask \nmechanism\nQian Liu1, Xiaohua Huang1,4*, Xiuyan Shao2 & Fei Hao3\nIn the field of artificial intelligence, a large number of promising tools, such as condition-based \nmaintenance, are available for large internal combustion engines. The cylinder liner, which is a key \nengine component, is subject to defects due to the manufacturing process. In addition, the cylinder \nliner straightforwardly affects the usage and safety of the internal combustion engine. Currently, the \ndetection of cylinder liner quality mainly depends on manual human detection. However, this type \nof detection is destructive, time-consuming, and expensive. In this paper, a new cylinder liner defect \ndatabase is proposed. The goal of this research is to develop a nondestructive yet reliable method \nfor quantifying the surface condition of the cylinder liner. For this purpose, we propose a transformer \nmethod with a block division and mask mechanism on our newly collected cylinder liner defect \ndatabase to automatically detect defects. Specifically, we first use a local defect dataset to train the \ntransformer network. With a hierarchical-level architecture and attention mechanism, multi-level \nand discriminative feature are obtained. Then, we combine the transformer network with the block \ndivision method to detect defects in 64 local regions, and merge their results for the high-resolution \nimage. The block division method can be used to resolve the difficulty of the in detecting the small \ndefect. Finally, we design a mask to suppress the influence of noise. All methods allow us to achieve \nhigher accuracy results than state-of-the-art algorithms. Additionally, we show the baseline results on \nthe new database.\nSurface defects leads directly to quality problems of the product, additionally affect the chemical and physical \nproperties of the product surface. The cylinder liner is a key component of the internal combustion engine. \nTherefore, the appearance of cylinder liner surface defects, such as cracks and sand holes, will result in quality \nand safety problems in the engine. As a result, manufacturers have proposed strict industrial requirements for \nguaranteeing the quality of cylinder liners during the production process. Currently, the detection of cylinder \nliner surface defect quality mainly relies on manual visual testing. However, manual visual testing cannot meet \nthe production requirements in terms of work efficiency. In addition, this type of detection suffers from human \nfactors, such as emotion and subjective experience. Moreover, some product defects are small in size and diverse \nin shape, and it is difficult for human eyes to observe these defects. Additionally, detection may be harmful to the \ntesters’ health, Therefore, manual detection cannot meet the requirements of current mass industrial production.\nThe nondestructive testing (NDT) method is another way for manufacturers to inspect defects. From an \nindustrial viewpoint, the purpose of NDT is to determine whether a material or part will satisfactorily perform \nits intended function. This method is mainly focuses on the many aspects of the quality and serviceability of \nmaterials and structures and incorporates all the technologies for process monitoring and the detection and \nmeasurement of significant properties. For example, Dong et al. used X-Ray to analyze the abnormalities in the \narea around the  weld1. Hato et al. used a high-speed scanning laser observation system to nondestructively test \neach layer of GdBa2Cu3O7-x (GdBCO) ion-beam-assisted-deposition and pulsed-laser-deposition (IBAD-PLD) \ncoated  conductor2. In the last two decades, the rapid development of the machine vision-based detection algo -\nrithm has promoted the development of surface defect detection technology. It has also promoted the computer \nvision platform as one method of NDT. Compared with manual detection and NDT with a scanning laser, a \nOPEN\n1School of Computer Engineering, Nanjing Institute of Technology, Nanjing, China. 2School of Economic and \nManagement, Southeast University, Nanjing, JiangSu, China. 3School of Mechanical Engineering, Nanjing Institute \nof Technology, Nanjing, JiangSu, China. 4Advanced Industrial Technology Research Institute, Nanjing Institute of \nEngineering, Nanjing, JiangSu, China. *email: xiaohuahwang@gmail.com\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nmachine vision-based platform not only improves the efficiency of detection, but is also economic and flexible \nfor manufacturers. For example, Liu et al. proposed an improved Particle Swarm Optimization Support Vec-\ntor Machine (PSO-SVM) based on imaging technology to detect the defects of a  vortex3. Although traditional \nmachine vision algorithms exhibit efficiency in defect detection, professional knowledge is required. Therefore, \nit is desirable to directly obtain high-level features from the data for defect detection.\nWith the development of Graphics Processing Unit (GPU), deep learning technology has been broadly applied \nto various real-world  applications4,5. By utilizing the benefits of GPUs, most deep learning networks have also \nbeen applied to on-fly detection across all aspects of  industry6,7. These networks strongly promote the develop-\nment of industrial inspection and address the disadvantages of classical detection technology. For example, \nShifted Windows (SWIN)  Transformer8, in which hierarchical feature maps are built by merging image patches in \ndeeper layers, has linear computation complexity to input the image size and is used to resolve the deficiencies of \nConvolution Neural Network (CNN) on feature extraction. A shift operation is used to improve the receptive field \nof CNN. An improved Single Shot MultiBox Detector (SSD) algorithm with a region of  interest9 was proposed \nto detect the defects of filling line  containers9. This work indicates that the background noise can be suppressed \nin the region of interest. Although the performance of deep learning networks exceeds human performance in \nsome specific domains, there still exists difficulty in detecting defects, especially in very high-resolution images. \nThis is caused by the following two reasons. First, in practice, the shape and size of surface defects of industrial \nproducts are different; using an image algorithm for feature extraction requires many resources for algorithm \ndesign, which shows that its universality for the target object is poor. Second, in high resolution images, com -\npared with regular objects, small objects have less information and the training of small objects is difficult to \nmark. This leads to poor performance when directly employing the previous object detection method for small \nobject detection. Moreover, the detect methods designed for small objects are often too complex or specific. For \nexample, for small targets such as bottle in PASCAL VOC  dataset10, the features extracted from deep network \ncontain little information about the small  target11,12. Therefore, detecting small and various defect requires a \nwell-designed feature learning network.\nIn this paper, motivated by two recent  studies9,8, we establish a cylinder liner defect database and propose a \nTransformer network with Block division approach and Mask mechanism (TBM) to detect cylinder liner surface \ndefect. More specifically, the defect patches are first collected as a training set. Then, with these training data, \nthe Swin transformer, which is an encoder-decoder that is used as the backbone for feature extraction of the \nmask-RCNN network, learns the attention region. For the testing procedure, a mask mechanism that is based \non morphology and used to extract the region of interest is proposed, and the Swin transformer is applied to \ndetect the defects in regions of interest.\nThe key contributions of this paper are described as follows:\n• To the best of our knowledge, we provide the first publicly available cylinder liner defect database.\n• We propose a new defect detection system based on transformer with a block division approach and a mask \nmechanism to address the small defect detection problem for cylinder liners.\n• We compare several state-of-the-art algorithms for object detection on the cylinder liner database and pro-\nvide the baseline results for further research. Additionally, the proposed method is demonstrated to obtain \npromising and considerable performance in cylinder liner defect detection.\nThe remainder of this paper is organized as follows. In Section “ Related work” , literature closely related to our \nproposed method is presented. In Section “ Cylinder liner defect database ” , our cylinder liner defect database \nis described. In Section “ System architecture” , the proposed method for defect detection is presented. In Sec-\ntion “Experimental results and discussion ” , the experimental results on the cylinder liner defect database are \nshown and discussed. In Section “Conclusion” , this paper is concluded.\nRelated work\nObject detection algorithm. The feature extraction ability of deep learning is better than that of artifi-\ncially designed feature extraction operator, so using CNNs for defect detection has become a research hotspot \nin the field of contemporary object detection. Currently, object detection algorithms based on CNNs can be \ndivided into two categories: one-stage and two-stage object detection algorithms.\nOne‑stage object detection algorithms. One-stage object detection algorithms are utilized to directly predict \nobject bounding boxes for an image in a one-stage fashion. In other words, there are no intermediate tasks that \nmust be performed to output the product. The most common examples of one-stage object detectors are  SSD13 \nand Y ou Only Look Once v3 (Y oloV3)14. The  SSD13 is a single-stage object detection method that discretizes the \noutput space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map \nlocation. SSD’s architecture builds on the venerable VGG-16 architecture but discards the fully connected lay-\ners. Instead, a set of auxiliary convolutional layers are added, thus enabling the extraction of features at multiple \nscales and progressively decreasing the size of the input to each subsequent layer. Additionally, the successful \nSSD method utilizes the proposed multibox loss function, combining condition loss and location loss. This algo-\nrithm successfully integrates regression and classification tasks into the overall CNN structure and obtains the \ntarget category information and location information directly through a convolutional neural network. Under \nthe action of the anchor mechanism, the region recommendation algorithm is cancelled. These two improve-\nments greatly improve the detection efficiency of the network. In  Y oloV314 model, DarkNet-53 backbone is \nadopted. The backbone network improves the feature extraction ability of the model. At the same time, under \nthe ideas of dense convolutional network (DenseNet) and feature pyramid network (FPN), the model can detect \n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nsmall targets more accurately, the feature pyramid is formed, and the fusion between features is realized, which \nexpands the semantic information of the low feature level. Moreover, the Y oloV3 algorithm improves the loss \nfunction. In the category loss part, logic regression is used to replace the softmax function. Although the one-\nstage detection algorithm can quickly detect objects, they suffer from the class imbalance problem, more post-\nprocessing and low accuracy rates.\nTwo‑stage object detection algorithms. Two-stage object detection algorithms consider the detection task into \ntwo stages. In the first stage, the candidate region proposals are determined, and in the second stage, these \nproposals, which are generated from the regional proposal network (RPN) layer by using CNN, are classified. \nWith the RPN network, which is a full convolution model, the extraction efficiency of candidate boxes is greatly \nimproved. In the process of using the RPN network, an anchor mechanism and a nonmaximum suppression \nalgorithm are used. The representative algorithms are R-CNN15, Fast R-CNN16, and Faster R-CNN17. R-CNN15 \nuses a selective search algorithm to generate candidate regions and then uses an image processing algorithm to \nscale the candidate regions to a fixed size. The processed regions are input into the designed CNN network for \nfeature extraction, and the region classification is completed under the effect of the SVM classifier. Meanwhile, \nthe fine-tuning of the border is completed, and the target information is finally obtained. Although the accuracy \nof this algorithm is high, considerable computing time is required to generate candidate regions. Meanwhile, \nwhen the image processing algorithm is used to fix the size of the region, there is also the problem of image dis-\ntortion, which leads to the confusion of information. In addition, a large number of candidate regions show the \nproblem of computational redundancy in CNN. To solve the problem of information loss caused by solidifying \nthe size of candidate regions, Fast R-CNN 16 is used to improve R-CNN. Different from R-CNN, Fast R-CNN \ninputs the whole image into CNN for calculation, and under the effect of ROI pooling, the output of CNN is \nfixed to a certain size of eigenvector. In this model, classification and regression are implemented in different \nnetworks, so although the defects are high, the detection speed is low. This model does not solve the problem that \nconsiderable computation time is required to generate candidate regions. Fast R-CNN also increases the com-\nputation of the model to a certain extent. To solve this problem, Faster R-CNN17 was proposed. Under the effect \nof ROI pooling and the corresponding hardware conditions, this model can accept any size of the input image. \nHere, the backbone network is designed to extract the features of the input image to obtain the corresponding \nfeature map, which is shared by the RPN and the fully connection layer of the surface, reducing the amount of \ncalculation to a certain extent. To solve the time problem of generating candidate regions, an RPN network is \ndesigned to generate the candidate regions. Compared with a one-stage network, the performance of Faster \nR-CNN is better than that of a one-stage network, but it is slower than a one-stage network.\nDefect detection algorithm. Given its development, deep learning has been widely applied in various \ndefect detection tasks. Deep learning can be divided into two categories.\nOne‑stage defect detection algorithms. Based on the advantage of the one-stage object detection algorithm, it is \nmore efficient and elegant in design. For example, Chen et al. proposed using a generative adversarial network \n(GAN)18 and Y oloV3  algorithm14 to detect defects in wafer die  pie6. The pseudo defective images generated by \nGAN from the real defective images were used as the training image set. Then defects were measured based on \nthe bounding boxes predicted by Y oloV3. Although the detection speed is fast when using Y oloV3, the major dis-\nadvantage is that it is difficult to guarantee that GAN can output more natural and realistic images, as GAN has \nmore hyperparameters and is influenced by the background complexity and the size of the input images. Yin et \nal. proposed a real-time automated defect detection system for sewer lines by using a deep-learning  algorithm19. \nThey used Y oloV3, which was trained with a dataset of 4,056 samples with six types of defects and one type of \nconstruction feature. Although it achieved a mean average precision of 85.37%, the system was influenced by \nthe noise of the background. An improved SSD algorithm was proposed to detect the surface defects of filling \nline  containers9. More specifically, the VGG-16 network was replaced by MobileNet, which strongly simplifies \nthe detection model and increases the recognition rate to 95%. Before feeding an image into SSD, Hough circle \ndetection was used in the preprocessing phase to locate the edge of the cover and mitigate the impact of useless \nbackground on the recognition accuracy. Compared with  Y oloV319, VGG-16 segmented the region of interest, \nwhich suppressed the background noise and improved the detection rate of very small defects. This indicates that \naccording to specific defects, the segmentation can improve the performance.\nTwo‑stage defect detection algorithm. Based on the advantage of the one-stage object detection algorithm, the \ntwo-stage detectors have superior accuracy. To date, some works have been presented to detect defects in various \n fields20–23. For example, Perez et al. first studied defect detection using convolution neural network (CNN)20. As \nmold, deterioration, and stains frequently occur on the surface of buildings, a pretrained CNN classifier with \nVGG-1624 as the backbone was proposed, and finally CNN was combined with class activation maps for target \nlocation. In their work, this model considers an image to belong to only one category. This means that multiple \ntypes of defects are not considered for the VGG architecture. Therefore, this architecture may not be suitable \nfor more than binary classes. Duong and Kim implemented a deep neural network for bearing fault  diagnosis22. \nThey segmented the continuous signal into lengths of 500 and 1000 data points for bearing fault diagnosis. \nKumar et al. proposed an ensemble of binary CNNs for automated defect detection based on CCTV inspection \nof  sewers21. However, their  method21,22 required many labeled fault samples to train the fault detection model.\nConsidering the small number of defective components, Gibert et al. proposed deep multitask learning by \ncombining multiple detectors to learn a robust anomaly detector, which resolves the problem caused by the \nnumber of different possible failure  modes25. The multiple detectors contain four different tasks that include the \n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\ndetection of the missing, damaged and good fasteners, the binary classifications of good and bad fasteners, and \nthe classification of good fasteners. Tabernik et al. presented a segmentation-based deep learning architecture \nfor the detection and segmentation of surface  anomalies7. More importantly, their proposed method enables the \nmodel to be trained using a small number of samples. The architecture was formulated as a two-stage design. In \nthe first stage, a segmentation network was implemented to perform pixelwise localization of the surface defect. \nThe benefit of this approach is to increase the effective number of training samples and prevent overfitting by \neffectively considering each pixel as an individual training sample, However, only 25-30 defective training images \nare able to be learned. In the second stage, an additional network for binary classification was built on top of the \nsegmentation network. However, acquiring pixel-level labels is both time- and labor-intensive. Di et al. proposed \na semisupervised surface defect detection method based on GAN for hot-rolled strip steel  workpieces26. In \ntheir method, they used a convolution encode-decode module for unsupervised feature learning and trained an \nautoencoding module with one classification layer as a GAN discriminator. Both labeled and unlabeled samples \nhave been used to train classifiers with different learning strategies but have been unable to predict defect regions.\nCylinder liner defect database\nTo the best of our knowledge, there is currently no database of cylinder liner defects. Therefore, we collect a \nnew cylinder liner defect database from industry, namely, the Cylinder Liner Defect (CLD) database. In general, \nthere are several types of defects on cylinder liners, such as sand holes, bumps, cracks, oil stains, scratches, and \nwear. During the manufacturing process of cylinder liners, various defects, such as sand holes, crack, and wear, \nmay occur due to the uneven stress distribution, temperature, and impurities. In this paper, we focus on three \nfrequently occurring defects, including sand holes, scratches, and wear, which are described as follows:\n• Sand hole: This is mainly caused by the casting production form of the cylinder liner. The gas and nonmetallic \ninclusions cannot be discharged from the liquid metal before solidification, resulting in sand hole defects on \nthe cylinder  liner27. As sand holes easily deteriorate the performance of cylinder liners, even causing cylinder \ncollapse and water leakage, they are the primary defect of cylinder liners. An example is shown in Figure 1a. \nAs seen from Figure 1a, the sand hole is very small.\n• Scratch: During casting of the cylinder liner, once the actual deformation caused by the combined action of \nvarious stresses exceeds its plastic limit, it will result in cracks on the cylinder  liner28. These cracks affect the \nservice life and replacement cycle of the cylinder liner, which undoubtedly leads to potential safety hazards in \nthe service stage of the cylinder liner. A scratch defect is shown in Figure 1b. The defect appears as a snowflake \ncrack.\n• Wear: This often occurs in the process of production and transportation. In the production process, the \ngenerated waste materials will damage the cylinder liner, and in the transporting process, friction and colli-\nsion will produce massive wear defects. For an internal combustion engine, wear in cylinder liner damages \nthe tightness of the cylinder liner and also degrades the power of the engine. The wear defect is shown in \nFigure 1c. There is a line-type wear along the edge of the cylinder liner.\nFollowing the existing utilized methods 29,30 in industrial data collection, we use three charge-coupled device \n(CCD) industrial cameras for image acquisition and three light emitting diodes (LEDs) as the light source, the \npositions of which are illustrated in Fig. 2. More specifically, considering the defects existed different positions \nof the cylinder liner, we use three area-array CCD cameras (Camera 1, Camera 2, and Camera 3, as indicated in \nFig. 2) to collect the defect images from the upper top surface (Scene 1), skirt (Scene 2), and inner wall (Scene \nFigure 1.  Three frequently occurring cylinder liner defects. The images from left to right represent sand hole, \nscratches, and wear, respectively.\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\n3) of the cylinder liner. The advantage of this process is to quickly and intuitively obtaining two-dimensional \ncylinder liner images. We collected a total of 7500 images, some of which are shown in Fig. 3. The defect annota-\ntion requires professional knowledge, and it also lacks of no specific standard for human annotation. Therefore, \nwe asked two experts to manually annotate the surface defects. With two experts’ annotations, only 585 images \nwith a size of 2448 × 2048 contain defects, including sand holes, scratches, and wear. Three local enlarged defects \nare illustrated in Fig. 4.\nSystem architecture\nOur proposed framework is shown in Fig.  5. It consists of two stages: a transformer network stage and a block \ndivision and mask mechanism stage. The transformer network focuses on training a deep network based on \na local defect dataset, in which each image contains one defect, while the block division method separates an \nimage into 64 blocks and then uses the network to detect each block, subsequently mapping the detected results \ninto an original image, and the mask mechanism suppresses the noise in the background. In this section, we \nwill detail each stage.\nFigure 2.  The structure of data collection for cylinder liner defects. Best viewed in color.\nFigure 3.  Three cylinder liner examples taken by an area-array CCD camera.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nTransformer network. The Swin  transformer8 is capable of serving a general-purpose backbone for com-\nputer vision. It uses a hierarchical architecture to address problems such as the scale of visual entities and the \nhigh resolution of pixels in images. To enhance the performance of defect detection of cylinder liners, we pro-\npose using a Swin transformer network followed by a detection model. To improve the accuracy of the Swin \ntransformer, we propose an image processing method to enhance the image quality. Here we use a Gaussian \nfilter, which is a linear smooth filter, to primarily remove the Gaussian noise.\nSwin transformer. The Swin transformer is one variation of an encoder-decoder architecture. Figure 6 shows its \narchitecture, where H and W are the height and width of an image, respectively.  Following8, we briefly describe \nthe Swin architecture by borrowing their some formulations.\nFigure 4.  The local enlarged region of the cylinder liner, where the defect is located inside the red rectangle.\nFigure 5.  Our proposed framework for detecting very small defects on high-resolution cylinder liner images. \nBest viewed in color.\nFigure 6.  Swin transformer  architecture8 used in our paper.\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nAs shown in Fig.  6, the Swin transformer mainly uses three modules to build its architecture. It consists of \nthe patch partition module, linear embedding module, and Swin transformer block. The patch partition module \nsplits an input RGB image into nonoverlapping patches, where the feature of each patch is a concatenation of \nthe raw pixel RGB value, while the linear embedding module predicts each patch to any arbitrary dimension C \non this raw-valued feature. A Swin transformer block provides efficiency by limiting self-attention computation \nto nonoverlapping local windows. For the Swin transformer, the key component is the Swin transformer block.\nSwin transformer block. This block is mainly composed of a window based self-attention layer (W-MSA) mod-\nule and a shift window based multihead self-attention layer (SW-MSA) module. The Swin transformer block is \nshown in Fig. 7. In Fig. 7, W-MSA is used to calculate window based attention, while SW-MSA attention after \nsliding the window. Layer normalization (LN) is used to normalize the data of the input network, and MLP is \ncomposed of two fully connected layers, which successively pass through the fully connected layer, GELU activa-\ntion function, dropout layer, full connected layer, and dropout layer. Using W-MSA and SW-MSA, consecutive \nSwin transformer blocks are computed as,\nwhere ˆzk denotes the output features of the kth block, where k = 1,... ,4 in our paper.\nPipeline. The patch partition module splits an input RGB image into nonoverlapping patches. Then, the linear \nembedding module and Swin transformer block are composed. The pipeline of the linear embedding and Swin \ntransformer block is referred to as ”Stage 1“ . To produce a hierarchical representation, three additional stages are \napplied. Importantly, the Swin transformer uses patch merging layers to reduce the number of patches, when the \nnetwork becomes deeper. More specifically, in ”Stage 2“ , the features of each group of 2 × 2 neighboring patches \nare concatenated by using the first patch merging layer, and then a linear layer is applied on the 4C−dimen-\nsional concatenated features. This results in the number of patches decreasing by a multiple of 4 and the output \ndimension as 2C. Swin transformer blocks are applied afterward for feature transformation, with the resolution \nmaintained at H\n8 × W\n8  . This procedure is repeated twice, as ”Stage 3“ and ”Stage 4“ , with output resolutions of \nH\n16 × W\n16 and H\n32 × W\n32 , respectively.\nDiscussion. The characteristics of the Swin transformer are as follows: (1) The Swin transformer formulates a \nhierarchical feature map by merging the patterns across deep levels, and has also linear computational complex-\nity, because only self attention calculations are performed for each local window. Among them, windows are not \noverlapped, and the number of patches in each window is fixed. (2) The shifted window spans the upper layer \nand improves the performance of the model. Query patches in the same window share the same key set, which \nimproves the efficiency of accessing memory.\nDetection model. The detection model is primarily based on the basic architecture of Mask  RCNN31. The pipe-\nline is realized by using feature pyramid networks (FPN), followed by region proposal network (RPN), fully \nconcatenated layers, and a binary mask prediction branch, as depicted in Figure  5. Nonmaximum suppression \n(NMS) is performed as a postprocessing step to obtain the final set for detection. In the following section, we \n(1)zk+1 = MLP(LN(W-MSA(LN( ˆzk )) +ˆzk )) + W-MSA (LN( ˆzk )) +ˆzk ,\n(2)ˆzk+1 = MLP (LN(SW-MSA (LN(zk+1)) + zk+1)) + SW-MSA (LN(zk+1)) + zk+1,\nFigure 7.  Block of swin  transformer8 used in our paper, where LN is layer normalization layer.\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nbriefly introduce how the detection model is realized. More technical details can be found in the Mask RCNN \n study31.\nIn our approach, we implement ResNet50 as the backbone for FPN to extract defect image features. FPN \naims at building high-level semantic feature maps at all scales. It takes a defect images and exports a five-scale \nfeature pyramid, by using a top-down architecture. Then, according to the anchors, RPN with 3 × 3 convolution \nand two 1 × 1 convolutions is used to propose candidate object bounding boxes (that is, region proposals) in the \ndifferent scales. Subsequently, features of region proposal are extracted by the RoI align layer, which removes \nthe harsh quantization of RoIPool, properly aligning the extracted features with the input. Then the features are \nfed into the fully connected layers and softmax layers, which can finally estimate softmax probability of defects \n(i.e.,  ’ classification‘ in Fig. 5) and also refine the bounding box positions (i.e., ’Box regression‘ in Fig.  5)for the \ndefect targets. At the same time, the features are fed into the mask prediction branch, which consists of four \nconvolution layers and one deconvolution layer, to predict the defect target mask.\nBlock division and mask mechanism. In the real-world implementations, we expect the Swin trans-\nformer to better detect small defects in the high-resolution images. However, according to our empirical \nresearch, it is very difficult to locate the defect, once the high-resolution image is used as the network input. This \nis because the image scale will make the defect be missed. Therefore, in order to quickly and accurately detect \nthe defect, we propose block division (BD) method for the detection procedure. More specifically, we divide the \nhigh-resolution image ( 2448 × 2048 ) into 64 blocks, and then use the trained Swin transformer to detect each \nblock. Finally, we map these detected results into the high-resolution image. Our proposed approach can be used \nto improve the detection precision. Our experiments solidly validate this proposal.\nAlthough the block division method can be used to resolve the small defects existing in high-resolution \nimages, for cylinder liners, there are some special characteristics, such as the useless region. For example, in the \nfirst example of Fig. 3, the background is outside the outlier cycle and inside the inner cycle. These useless regions \nwill cause the noise in the detection performance. Therefore, we propose a mask mechanism to address these \nissues. The mask mechanism is implemented to design the specific binary mask, where ‘1’ means foreground and \n‘0’ means background. With the specific mask, the region of interest will be found. The advantage is to suppress \nthe noise due to background. In this implementation, we multiply the detected results by the mask so that the \nfalsely detected results in the background will be removed. Considering the acquisition of cylinder liner images \nacquired from different cameras, we design three kinds of masks as follows:\n• For the rough region made from the upper camera (as shown in the first image of Fig. 3), we set the appropri-\nate threshold for image binarization. Then, we use morphological operations and closed operations to remove \nthe pixels with small binarization and ensure that the binarization region contains the region of interest.\n• For the upper end face image (as shown in the second image of Fig.  3), we measure the coordinates of the \nupper end face image. According to the detected area in the upper face, we determine the center of the circle \nand radius of the maximum circle and the minimum circle. Last, we take a ring binary image as the region \nof interest of the upper end face image.\n• There is another type of picture (as shown in the third image of Fig. 3). We manually design a specific mask \nto extract the region of interest.\nThe masks for the three types of images are shown in Fig. 8.\nExperimental results and discussion\nExperiment setup. We conduct the experiments on our collected CLD database. For training the backbone \nnetwork, we create the local surface defect dataset derived from CLD database. For the defect patch image, we \nextract it by using annotation information of the original cylinder liner surface defect. All defect patch images \nare normalized to a size of 256 × 256 . Each defect patch image only contains one defect. Last, we manually \nextract 1061 defect patch images from the training set as the ‘training defect subset’ , and 118 images from the test \nset as the ‘test defect subset’ . In the network training, number of epochs is set as 300, batch size as 6, learning rate \nas 0.0001, and decay value as 0.05. All codes are run on NVIDIA Titan RTX (24GB) and implemented based on \nPython 3.7 and PyTorch 1.8. The experiment settings are listed in Table 1.\nTo evaluate block division and mask mechanism, we select 54 original high-resolution images as the test data. \nThese data are used to verify the final performance of the proposed method.\nIn the prediction stage, we adopt the mean average precision (mAP), which is the mean precision over all \nclasses as follows,\nwhere n is the number of classes. AP i is the average precision of the ith class. For the ith class, we compute the \nprecision and recall for this class at different class confidence thresholds (from 0.0 to 1.0).\nwhere pi = TP\nTP +FP  , ri = TP\nTP +FN  , TP is the true positive, FP is the false positive, and FN is the false negative.\n(3)MAP = 1\nn\nn∑\ni=1\nAPi\n(4)AP i =\n∫ 1\n0\npiridri,\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nAblation study. Backbone selection. To validate the rationale of our proposed network, we compare our \nbasic network with  Y oloV314, Y oloV5 (https://pjreddie.com/),  RetinaNet32, Faster R-CNN17, Mask  RCNN31, and \nCascade  RCNN33. The algorithm comparison is conducted on the local surface defect dataset. The performance \ncomparison is shown in Fig. 9. Swin transformer outperforms the other state-of-the-art methods, especially us-\ning a very small model for Swin Transformer. A mAP of 0.603 is achieved using Swin transformer with a very \nsmall model; this result represents an increases in the mAP of 0.134 and 0.115 compared with the Mask RCNN \nwith the Resnet50 and Resnet101 backbones, respectively. Considering the comparison, we choose the Swin \ntransformer as our network.\nNumber of stages. To evaluate the influence of the stages, we study the results under the different numbers of \nstages. As shown in Fig.  6, the Swin transformer architecture has four stages, and a mAP of 0.706 is achieved. \nWhen we reduce the number of stages to one, which means that only ‘Stage 1’ is included, the mAP is 0.296. \nWith more stacked stages, the mAP is 0.388, 0.57, and 0.6 for the two-stage, three-stage, and five-stage models, \nrespectively. This indicates that the performance is improved with an increasing number of stages. However, \nwhen we use 5 stages, the performance is degraded. This may be explained by the fact that more than 4 stages \nwill suppress the discriminative feature as the output resolution will be very small.\nFigure 8.  Our proposed mask mechanism for cylinder liner.\nTable 1.  Experimental setting used in the training process.\nHyperparameter Value\nTraining epochs 300\nBatch size 6\nLearning rate 0.0001\nWeight decay 0.0.05\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nEvaluation of Gaussian filter. In our method, we use Gaussian filter as data augmentation. To see the influence \nof the Gaussian filter, we compare the size of the Gaussian filter, including 3 × 3 , 5 × 5 , and 7 × 7 . The compari-\nson is shown in Table 2. It is seen that the Gaussian filter with 5 × 5 slightly outperforms that with 3 × 3 filter. \nMoreover, we further investigate the mAP of the three categories. The results are shown in Table 3. It is seen that \nusing a Gaussian filter with 5 × 5 improves the precision of detection on the wear class, by increasing the mAP \nof 0.065, compared with that with 3 × 3 filter. Unfortunately, the Gaussian filter with 5 × 5 fails to considerably \nimprove the performance of the sand hole class, but it is still competitive with the Gaussian filter with 3 × 3 on \nscratch class. Considering the abovementioned analysis, we choose 5 × 5 for the Gaussian filter.\nEvaluation of block division. This experiment aims to evaluate the performance of the block division method. \nHere, we compare them with the “Original method” . In the original method, we use the original high-resolu-\ntion image with its corresponding defect labels for training and testing. The comparison results are reported \nin Table 4. The “Original method” obtained the lowest mAP of 0.129 among all methods. Additionally, for all \nclasses, it loses the capability to detect very small defects. This can be explained by that the feature pyramid \nmethod causing the small defect to disappear with the deeper layer. In contrast, the block division method \nfocuses on the several very small regions, such that FPN will not have a negative influence on the detection. To \nsome extent, the block division method increases all the classes. For example, for the sand hole class, the block \nFigure 9.  Performance comparison of various networks on the local defect database in terms of mAP . D53, \nR50, and R101 represent DarkNet53, ResNet50, and ResNet 101 backbones, respectively. SWIN-T-P4W7 and \nSWIN-S-P4W7 indicate very small Swin transformer models, respectively.\nTable 2.  Performance of the Swin transformer under various Gaussian filter parameters. The best result is in \nbold.\nMethod Size mAP\nWithout Gaussian filter − 0.671\nWith Gaussian filter 3 × 3 0.700\nWith Gaussian filter 5 × 5 0.706\nWith Gaussian filter 7 × 7 0.649\nTable 3.  Performance of Swin Transformer with Gaussian filters on three classes, where we compare the \nresults under the filter sizes of 3 × 3 and 5 × 5. The best result is in bold.\nGaussian filter Wear Scratch Sand hole mAP\n3 × 3 0.613 0.755 0.733 0.700\n5 × 5 0.678 0.749 0.692 0.706\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\ndivision method increases the detection precision by 0.273. The comparison demonstrates that the block divi-\nsion method can better address the abovementioned problem.\nEvaluation of mask mechanism. We evaluate the TBM without/with the mask mechanism to determine its \ncontribution to detection. The quantitative analysis is shown in Table 4. It is seen that with the mask mechanism, \nthe block division method is increased from 0.277 to 0.537 by increasing the mAP to 0.26. The mAPs of wear, \nscratches and sand hole are increased by 0.284, 0.281, and 0.215, respectively. These results demonstrate that \nadding the mask mechanism promisingly improves the performance of the block division method. Moreover, we \nqualitatively analyze the mask mechanism in Fig. 10. In Fig. 10b, we can see that for the first example (in the first \ncolumn), there are too many falsely detected results in the background. In fact, this background does not contain \ninformation about the cylinder liner. The mask mechanism removes the detected errors and increases the perfor-\nmance of the block division method. The influence is the same as in the second example (in the second column). \nTherefore, the mask mechanism can suppress the influence of the background in our proposed method.\nPerformance comparison. \nTo evaluate the performance of our proposed method and the existing object detection for cylinder liner surface \ndefect detection, we use five deep learning networks for comparison. The parameters of our proposed method are \nshown in Table 5. The compared five deep learning networks include Y ou Only Look Once (Y olo)V314, Y oloV5 \n(https://pjreddie.com/),  RetinaNet32, Faster Region-CNN (RCNN)17, Mask  RCNN31, and Cascade  RCNN33. We \nused ResNet50 as backbone for Mask RCNN and Faster RCNN, and ResNet101 as backbone for Mask RCNN, \nCascade RCNN, Faster RCNN, and RetinaNet. During training, we directly utilize the default parameters for \nthese baseline networks. The results are presented in Table 6.\nAccording to Table 6, among all baseline algorithms (w.o. TBM), Y oloV5 achieves the best mAP of 0.16, in \nwhich accuracies of 0.309, 0.089, and 0.084 are obtained for the wear, scratch, and sand hole classes, respectively. \nFor the wear class, the best result is obtained by Faster RCNN with ResNet 50. For the scratch class, the best \naccuracy of 0.121 is obtained using Cascade RCNN. For sand hole class, the best accuracy of 0.084 is obtained \nusing Y oloV5. Scratch and sand hole classes are difficult to detect. This may be explained by the fact that scratch \nand sand hole defects are very small. In contrast, a mAP of 0.537 and accuracies of 0.628, 0.455, and 0.529 for the \nwear, scratch, and sand hole classes, respectively, are obtained using our proposed TBM. Compared with Y oloV5, \nour proposed method increases the mAP by 0.357. Additionally, for sand hole class, the increased accuracy is \n0.445, and for scratch, the accuracy is increased by 0.366.\nBased on block division and the mask mechanism, we compared the transformer architecture with other deep \nlearning frameworks. The results are shown in the last 10 rows. As seen from the results, among all compared \nalgorithms, Faster RCNN (R101) achieves the second-best mAP of 0.429. Instead, our proposed TBM with \ntransformer architecture obtains a mAP of 0.537, and accuracies of 0.628, 0.455, and 0.529 for the wear, scratch, \nand sand hole classes, respectively. Compared with TBM with Faster RCNN (R101), the performance is increased \nby 0.128. It is seen that the TBM significantly improved the performance of defect detection.\nIn addition to performance, we compare our proposed method with other approaches in computing efficiency. \nAs indicated in Table 6, without TBM, among Mask RCNN, Cascade RCNN, Faster RCNN, RetinaNet, Y oloV3, \nand Y oloV5, Y oloV5 has the highest efficiency of 92.1 images/second, and Mask RCNN has the slowest detection \nspeed. Even with TBM, Y oloV5 achieves the fastest detect speed, in which the fps is 93.6 images/second. For \nmost cases, the TBM increased the computing efficiency. For example, for the Mask RCNN, the detection speed \nis increased by approximately 4 images per second. In addition, even when using the transformer architecture, \nthe computing efficiency is 21.4 images per second, which is still competitive with other methods. Therefore, the \nexperimental results demonstrate that the TBM has better computing efficiency in defect detection.\nDiscussion. As shown in Fig. 9, we use a local defect database, in which each image contains only one defect, \nto validate the Swin transformer, a mAP of 0.603 is achieved. The degradation in the detection accuracy is caused \nby two reasons: (1) each block may contain more than one various defect and (2) defects are diverse, when we \nuse the TBM in the original high-dimension image.\nTable 4.  Ablation study of the TBM, where S means that we use the local defect dataset to train the network, \nBD represents the block division method, and the best result is in bold.\nMethod\nModule Class\nmAPS BD Mask Wear Scratch Sand hole\nOriginal method 0.286 0.061 0.041 0.129\nTBM ✓ 0.002 0.02 0.001 0.008\nTBM ✓ ✓ 0.344 0.174 0.314 0.277\nTBM ✓ ✓ ✓ 0.628 0.455 0.529 0.537\n12\nVol:.(1234567890)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nConclusion\nIn this paper, a transformer with a block division and mask mechanism (TBM) is proposed to considerably per-\nform both defect detection and classification tasks on our new cylinder liner defect database for a cylinder liner \ndefect against complex industrial scenarios. The proposed TBM can not only quickly detect very small cylinder \nliner defects but also suppress the noise caused by the background. The visual and quantitative experimental \nresults have shown that our detection algorithm boosts the performance of the network under high-resolution \nimages and provides a generic framework for other networks. The experimental analysis also established that \nthe block division and mask mechanism can help transformers with greater accuracy. Experimental results have \nshown that the proposed method achieves an average of 0.537 mAP for three defects. In addition, the proposed \nFigure 10.  Visual analysis of two detected results (a) before and (b) after adding the mask mechanism. Best \nviewed in zoom and color.\nTable 5.  Parameter setup of our proposed method.\nComponent Parameter\nGaussian filter 5 × 5\nNumber of blocks 64\nNumber of stages 4\n13\nVol.:(0123456789)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\nmethod outperformed other state-of-the-art algorithms, including Y oloV3, Y oloV5, and Mask RCNN, showing \n0.451, 0.377, and 0.396 improvements in accuracy, respectively.\nIn the future, we will study an approach to resolve the few-sample problem, as our database contains less than \n1000 images. Additionally, the surface of an industrial cylinder liner is a three-dimensional curved surface. In \nour database, the cylinder liner defect image is captured by an area array camera, which causes the inconsist-\nency in the background acquisition conditions in different areas, and then affects the detection results. We will \ndesign an appropriate image acquisition mechanism and select a linear array camera for image acquisition to \nimprove the image quality.\nData availability\nThe datasets generated during and/or analysed during the current study are available from the corresponding \nauthor on reasonable request.\nReceived: 6 March 2022; Accepted: 15 June 2022\nReferences\n 1. Dong, X., Taylor, C. & Cootes, T. Automatic Inspection of Aerospace Welds Using X‑Ray Images. in: Proceedings of the International \nConference on Pattern Recognition, 2002–2007 (2018).\n 2. Hato, T. et al. Non-destructive testing of each layer in GdBCO IBAD-PLD coated conductor by using a high-speed scanning laser \nobservation system. IEEE Trans. Appl. Superconduct. 21(3), 3381–3384 (2011).\n 3. Liu, B., Hou, D., Huang, P ., Liu, B. & Zhang, G. An improved PSO-SVM model for online recognition defects in eddy current \ntesting. Nondestr. Test. Eval. 28(4), 367–385 (2013).\n 4. Guo, G. & Zhang, N. A survey on deep learning based face recognition. Comput. Vis. Image Undertand. 189, 102805 (2019).\n 5. Masud, M., Sikder, N., Nahid, A., Bairagi, A. & AlZain, M. A machine learning approach to diagnosing lung and colon cancer \nusing a deep learning-based classification framework. Sensors 21(3), 748 (2021).\n 6. Chen, S., Kang, C. & Perng, C. Detecting and measuring defects in wafer die using GAN and YOLOv3. Appl. Sci. 10, 8725 (2020).\n 7. Tabernik, D., Ela, S. & Skvar, J. Segmentation-based deep-learning approach for surface-defect detection. J. Intell. Manuf. 31(3), \n759–776 (2020).\n 8. Liu, Z. et al. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. in: Proceedings of International Conference \non Computer Vision, 10012–10022 (2021).\n 9. Li, Y ., Huang, H., Xie, Q., Y ao, L. & Chen, Q. Research on a surface defect detection algorithm based on mobile net-SSD. Appl. Sci. \n8(1678), 1–17 (2018).\n 10. Everingham, M., Van Gool, L. & Williams, C. The pascal visual object classes (VOC) challenge. Int. J. Comput. Vis. 88(2), 303–338 \n(2010).\n 11. Mathe, S., Pirinen, A. & Sminchisescu, C. Reinforcement learning for visual object detection. in: Proceedings of CVPR, 2894–2902 \n(2016).\n 12. Zhao, Z., Zheng, P ., Xu, S. & Wu, X. Object detection with deep learning: A review. IEEE Trans. Neural Netw. Learn. Syst. 30(11), \n3212–3232 (2019).\nTable 6.  Performance comparison and detection efficiency between our proposed method and the state-\nof-the-art algorithms in object detection. The best and second best results are in bold and underlined, \nrespectively. R50 and R101 represent ResNet50 and ResNet101, respectively.\nMethod Backbone\nClass\nFPS (image/s) mAPWear Scratch Sand hole\nMask  RCNN31 ResNet50 0.166 0.032 0.018 25.7 0.072\nMask  RCNN31 ResNet101 0.288 0.057 0.078 16.7 0.141\nCascade  RCNN33 ResNet101 0.254 0.121 0.068 16.5 0.148\nFaster  RCNN17 ResNet50 0.315 0.072 0.047 24.8 0.145\nFaster  RCNN17 ResNet101 0.302 0.081 0.078 19.4 0.154\nRetinaNet32 ResNet50 0.163 0.052 0.054 26.1 0.09\nRetinaNet32 ResNet101 0.167 0.110 0.03 20.1 0.102\nY oloV314 D53 0.241 0.009 0.007 84.3 0.086\nY oloV5 D53 0.309 0.089 0.084 92.1 0.16\nTBM Mask RCNN (R50) 0.567 0.309 0.31 26.2 0.395\nTBM Mask RCNN (R101) 0.571 0.320 0.326 20.6 0.406\nTBM Cascade RCNN (R101) 0.511 0.352 0.373 17.1 0.412\nTBM Faster RCNN (R50) 0.429 0.372 0.4 27.6 0.4\nTBM Faster RCNN (R101) 0.473 0.376 0.437 21.4 0.429\nTBM RetinaNet (R50) 0.448 0.304 0.446 31.1 0.399\nTBM RetinaNet (R101) 0.502 0.332 0.374 23.6 0.402\nTBM Y oloV3 0.517 0.351 0.378 85.5 0.415\nTBM Y oloV5 0.51 0.343 0.358 93.6 0.404\nTBM Transformer 0.628 0.455 0.529 21.4 0.537\n14\nVol:.(1234567890)Scientific Reports |        (2022) 12:10689  | https://doi.org/10.1038/s41598-022-14971-8\nwww.nature.com/scientificreports/\n 13. Liu, W ., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. & Berg, A. SSD: Single Shot Multibox Detector. in: Proceedings of \nECCV , 21–37 (2016).\n 14. Redmon, J. & Farhadi, A. YOLOv3: An Incremental Improvement. http:// arxiv. org/ abs/ 1804. 02767 (2018).\n 15. Girshick, R., Donahue, J., Darrell, T. & Malik, J. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. \nin: Proceedings of CVPR (2014).\n 16. Girshick, R. Fast R‑CNN. in: Proceedings of International Conference on Comuter Vision, 1440–1448 (2015).\n 17. Ren, S., He, K., Girshick, R. & Sun, J. Faster RCNN: Towards real-time object detection with region proposal networks. IEEE Trans. \nPattern Anal. Mach. Intell. 39(6), 1137–149 (2017).\n 18. Goodfellow, I. et al. Generative Adversarial Networks. in: Proceedings of Advances in Neural Information Processing Systems \n(2014).\n 19. Yin, X. et al. A deep learning-based framework for an automated defect detection system for sewer pipes. Autom. Constr.  109, \n102967 (2019).\n 20. Perez, H., Tah, J. & Mosavi, A. Deep learning for detecting building defects using convolutional neural networks. Sensors 19(16), \n3556 (2019).\n 21. Kumar, S. et al. Deep learning-based automated detection of sewer defects in CCTV videos. J. Comput. Civil Eng. 34(1), 1–13 \n(2020).\n 22. Duong, B. & Kim, J. Non-mutually exclusive deep neural network classifier for combined modes of bearing fault diagnosis. Sensors \n18(4), 1–15 (2018).\n 23. Cha, Y ., Choi, W ., Suh, G., Sadegh, M. & Oral, B. Autonomous structural visual inspection using region-based deep learning for \ndetecting multiple damage types. Comput. Aided Civil Infrastruct. Eng. 33(9), 731–747 (2018).\n 24. Simonyan, K. & Zisserman, A. Very Deep Convolutional Networks for Large‑Scale Image Recognition. in: Proceedings of ICLR \n(2015).\n 25. Gibert, X., Patel, V . & Chellappa, R. Deep multi-task learning for railway track inspection. IEEE Trans. Intell. Transp. Syst. 18(1), \n1–12 (2017).\n 26. Di, H., Ke, X., Peng, Z. & Zhou, D. Surface defect classification of steels with a new semi-supervised learning method. Opt. Lasers \nEng. 117, 40–48 (2019).\n 27. Vadivelu, T., Reddy, C. & Prasanthi, G. Design and fabrication of die back door for manufacturing of cylinder liners. Adv. Appl. \nMech. Eng. 1, 1089–1102 (2020).\n 28. Ghasemi, R., Johansson, J., Stahl, J. & Jarfors, A. Load effect on scratch micro-mechanisms of solution strengthened compacted \ngraphite irons. Tribol. Int. 133, 182–192 (2019).\n 29. Wu, Y ., Guo, D., Liu, H. & Huang, Y . An end-to-end learning method for industrial defect detection. Assem. Autom. 40(1), 31–39 \n(2019).\n 30. Lawrence, K. & Ramamoorthy, B. Surface Texture Evaluation of Cylinder Liners Using Machine Vision 143–148 (Recent Researches \nin Communications, Electronics, Signal Processing and Automatic Control, 2012).\n 31. He, K., Gkioxari, G., Dollar, P . & Girshick, R. Mask R-CNN. IEEE Trans. Pattern Anal. Mach. Intell. 42(2), 386–397 (2017).\n 32. Lin, T., Goyal, P ., Girshick, R., He, K. & Dollar, P . Focal loss for dense object detection. IEEE Trans. Pattern Anal. Mach. Intell.  \n42(2), 218–237 (2020).\n 33. Cai, Z. & Vasconcelos, N. Cascade R-CNN: High quality object detection and instance segmentation. IEEE Trans. Pattern Anal. \nMach. Intell. 43(5), 1483–1489 (2021).\nAcknowledgements\nThis research was in part supported by National Natural Science Foundation of China (Grant Nos. 62076122, \n72001040), the Jiangsu Specially-Appointed Professor Program, the Talent Startup project of NJIT (No. \nYKJ201982), the Opening Project of Jiangsu Province Engineering Research Center of IntelliSense Technology \nand System (No. ITS202102), and the Opening Project of Advanced Industrial Technology Research Institute, \nNanjing Institute of Technology (No. XJY202102).\nAuthor contributions\nX.H. and Q.L. conceived the methodology, X.H. acquired the funding, Q.L. conducted the experiments, Q.L. \nwrote the main manuscript text and prepared all figures, X.H. and X.S. reviewed and edited manuscript text and \nanalyzed the results, F .H. was in charge of database description.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to X.H.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5784305334091187
    },
    {
      "name": "Transformer",
      "score": 0.5673582553863525
    },
    {
      "name": "Cylinder block",
      "score": 0.4763236939907074
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4412391483783722
    },
    {
      "name": "Cylinder",
      "score": 0.41156625747680664
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32454410195350647
    },
    {
      "name": "Voltage",
      "score": 0.28817301988601685
    },
    {
      "name": "Engineering",
      "score": 0.26523858308792114
    },
    {
      "name": "Mechanical engineering",
      "score": 0.24802175164222717
    },
    {
      "name": "Electrical engineering",
      "score": 0.1365078091621399
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799736854",
      "name": "Nanjing Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I76569877",
      "name": "Southeast University",
      "country": "CN"
    }
  ]
}