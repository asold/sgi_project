{
  "title": "Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective",
  "url": "https://openalex.org/W4389519576",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3031002218",
      "name": "Md Tahmid Rahman Laskar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284876901",
      "name": "Xue-Yong Fu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095817051",
      "name": "Cheng Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2891696475",
      "name": "Shashi Bhushan Tn",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2996264288",
    "https://openalex.org/W4385565211",
    "https://openalex.org/W4386081793",
    "https://openalex.org/W3113822960",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3199088047",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3175218683",
    "https://openalex.org/W4220732108",
    "https://openalex.org/W2125336414",
    "https://openalex.org/W4284713203",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385574201",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3212150810",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W4385890089",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3171639395",
    "https://openalex.org/W4385570128",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4385570371",
    "https://openalex.org/W4385570893",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W3169942382",
    "https://openalex.org/W4287887899",
    "https://openalex.org/W3100560913",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4353089798",
    "https://openalex.org/W4287888693",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4319793302"
  ],
  "abstract": "This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT-3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA-2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 343–352\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBuilding Real-World Meeting Summarization Systems using\nLarge Language Models: A Practical Perspective\nMd Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN\nDialpad Canada Inc.\n{tahmid.rahman,xue-yong,cchen,sbhushan}@dialpad.com\nAbstract\nThis paper studies how to effectively build\nmeeting summarization systems for real-world\nusage using large language models (LLMs).\nFor this purpose, we conduct an extensive eval-\nuation and comparison of various closed-source\nand open-source LLMs, namely, GPT-4, GPT-\n3.5, PaLM-2, and LLaMA-2. Our findings re-\nveal that most closed-source LLMs are gener-\nally better in terms of performance. However,\nmuch smaller open-source models like LLaMA-\n2 (7B and 13B) could still achieve performance\ncomparable to the large closed-source models\neven in zero-shot scenarios. Considering the\nprivacy concerns of closed-source models for\nonly being accessible via API, alongside the\nhigh cost associated with using fine-tuned ver-\nsions of the closed-source models, the open-\nsource models that can achieve competitive per-\nformance are more advantageous for industrial\nuse. Balancing performance with associated\ncosts and privacy concerns, the LLaMA-2-7B\nmodel looks more promising for industrial us-\nage. In sum, this paper offers practical insights\non using LLMs for real-world business meeting\nsummarization, shedding light on the trade-offs\nbetween performance and cost.\n1 Introduction\nMeetings are a widely used method for collabora-\ntion, with 55 million meetings occurring each week\nin the USA (Zhong et al., 2021; Hu et al., 2023).\nWith the rise of remote work, meetings have be-\ncome even more crucial. While the widespread use\nof video conferencing software has made it easier\nto record meetings, the huge number of meetings\nmakes it challenging to keep up with the infor-\nmation exchanged during them, emphasizing the\nneed for automated methods of accessing key in-\nformation. To address this issue, a summarization\nsystem that generates text summaries from meet-\ning transcripts can be highly beneficial. However,\nthe state-of-the-art neural summarization models\n(Lewis et al., 2020; Zhang et al., 2020; Raffel et al.,\n2020) require large domain-specific summarization\ndatasets for model training, which is difficult to\nobtain in real-world industrial scenarios due to the\nlack of domain-specific annotated data.\nRecently, the impressive capability of LLMs to\nsolve a wide range of tasks even in zero-shot sce-\nnarios (Laskar et al., 2023a; Qin et al., 2023; Bang\net al., 2023) has drawn a lot of interest among\npractitioners to apply LLMs to solve real-world\nproblems. However, despite the effectiveness of\nLLMs across several tasks, there is a scarcity of\ncomparative analysis between LLMs in long con-\nversational data, especially in tasks such as meeting\nsummarization. Thus, an extensive evaluation of\nLLMs in long meeting transcripts is an important\nstep for the development of a real-world meeting\nsummarization system that leverages LLM tech-\nnologies. In this regard, this paper aims to provide\na comprehensive analysis of various LLMs, which\nincludes closed-source models like GPT-3.5 (i.e.,\nChatGPT1), GPT-4 (OpenAI, 2023), and PaLM-2\n(Google, 2023), and two models (7B and 13B) of\nan open-source LLM: LLaMA-2 (Touvron et al.,\n2023). Our investigation focuses not only on the\nsummarization quality of these models but also\nconsiders the cost-effectiveness and computational\ndemands, providing a practical perspective to use\nsuch models in the real world.\nOur experimental results show that while most\nclosed-source models generally achieve better per-\nformance in meeting summarization datasets, the\nopen-source LLaMA-2 models still achieve compa-\nrable performance while being significantly smaller.\nFor this reason, an extensive model like GPT-4,\ndespite its marginally superior performance, is\ndeemed not as cost-effective due to its substantially\nhigher computational requirements, alongside pos-\ning privacy risks since such closed-source models\nare only accessible via API. Furthermore, using\n1https://openai.com/blog/chatgpt\n343\nfine-tuned versions of these closed-source models\nalso significantly increases the API usage cost 2.\nThus, the trade-off between performance, cost, and\ncomputational demands makes open-source models\nlike LLaMA-2 more favorable for industrial deploy-\nment. The insights from our research shed light\non the practicalities of using LLMs for summariz-\ning meeting transcripts. Thus, providing valuable\nguidance for similar industrial applications. Con-\nsidering various factors, we employ LLaMA-2-7B\nin a real-world setting to generate summaries from\nAutomatic Speech Recognition (ASR)-generated\ntranscripts (Fu et al., 2022; Khasanova et al., 2022;\nLaskar et al., 2022a,b, 2023b; Manderscheid and\nLee, 2023) of organizational meetings. Below, we\nsummarize our major contributions in this paper:\n(i) We conduct an extensive evaluation of closed-\nsource LLMs as well as open-source LLMs in sev-\neral benchmark meeting summarization datasets.\n(ii) We also present two approaches to address\nthe long input sequence length issue in LLMs.\n(iii) Finally, we demonstrate a practical perspec-\ntive on the trade-offs that come with selecting a\nmodel for real-world usage based on its perfor-\nmance, cost, and computational requirements.\n2 Related Work\nIn recent years, neural models based on the trans-\nformer architecture (Vaswani et al., 2017) have\nled to state-of-the-art performance across various\nsummarization datasets (Lewis et al., 2020; Zhang\net al., 2020; Raffel et al., 2020). However, these\ntransformer-based models require domain-specific\nfine-tuning (Devlin et al., 2019) to achieve the opti-\nmum performance. Thus, in real-world scenarios\nwhere in-domain labeled datasets are difficult to ob-\ntain, directly applying pre-trained transformers for\nzero-shot summarization may not yield great perfor-\nmance. In this regard, the impressive zero-shot per-\nformance of LLMs across various summarization\ndatasets (Laskar et al., 2023a) has drawn interest\namong practitioners to build real-world summariza-\ntion systems via leveraging LLMs.\nIn the real world, one interesting application of\nsummarization is generating concise notes of long\norganizational meetings. Though several datasets\n(Janin et al., 2003; Carletta et al., 2005; Gliwa et al.,\n2019; Clifton et al., 2020; Chen et al., 2021; Khal-\nman et al., 2021; Zhu et al., 2021; Cho et al., 2021;\n2https://openai.com/blog/\ngpt-3-5-turbo-fine-tuning-and-api-updates\nChen et al., 2022; Nedoluzhko et al., 2022; Hu et al.,\n2023) have been studied for the meeting summa-\nrization task in recent years, most of these datasets\nare not related to the business/organizational con-\ntext. This makes these datasets less relevant for the\nevaluation of summarization models that require\nthe generation of summaries from long organiza-\ntional meetings. In this regard, some notable ex-\nceptions are the AMI (Carletta et al., 2005) dataset,\nthe ICSI (Janin et al., 2003) dataset, and the QM-\nSUM (Zhong et al., 2021) dataset, as they consist\nof organizational meeting transcripts, contrary to\nmost other datasets.\nSince the development of a summarization sys-\ntem in a real-world industrial setting requires an\nextensive evaluation of the summarization model\nto ensure customer satisfaction, in this paper, we\nevaluate various LLMs in benchmark meeting sum-\nmarization datasets. More specifically, we use the\nfollowing datasets: AMI, ICSI, and QMSUM. All\nthese datasets are constructed from organizational\nmeetings. We hope that our extensive evaluation\nof LLMs in these meeting summarization datasets\nwill help researchers and industry professionals to\nharness the power of LLMs to build real-world\nmeeting summarization systems.\n3 Our Methodology\nThe objective of this research is to identify the most\neffective model to summarize organizational meet-\nings that could be used in real-world applications\nin scenarios when in-domain labeled datasets are\nnot available. Therefore, in this paper, we study\nLLMs due to their impressive zero-shot capabili-\nties. However, one major issue with existing LLMs\nis their limitation to handle long contexts (Liu et al.,\n2023). Since organizational meetings are usually\nquite longer (Zhong et al., 2021), the input limit\nrestrictions of LLMs make it difficult to consider\nthe whole transcript sequence. In this regard, we\nstudy how to handle the long input sequence issue\nin LLMs based on the following two approaches\n(an overview of our proposed approaches is also\nshown in Figure 1):\n(i) Summarization via Truncation:In this ap-\nproach, we handle the input length restrictions in\nLLMs by only giving the first n words of the meet-\ning transcripts as input to the LLM for summariza-\ntion. Our prompt for the LLM is given below:\nPrompt: “Summarize the following conversa-\ntion: [TRUNCATED TRANSCRIPT]”\n344\nFigure 1: An overview of our proposed approaches. On the left (i), we demonstrate the Summarization via Truncationapproach\nwhere only the first n words of the whole transcript are given as input to generate the summary. On the right (ii), we demonstrate\nthe Summarization via Chapterizationapproach where the summaries of every n words of the transcript are first generated,\ndenoted as chapter summaries, and then the final summary is generated either by (a) re-writing, (b) re-summarizing, or (c)\nconcatenating the chapter summaries.\n(ii) Summarization via Chapterization:In our\nprevious approach, the context of the whole tran-\nscript could not be given as input to the LLMs since\nonly the truncated input sequence, consisting of the\nfirst n words is provided. While this approach is\nquite efficient in terms of cost, the performance of\nthe summarization may not be optimal due to trun-\ncating the input sequence length. To address this\nissue, we propose summarization via chapteriza-\ntion. In this approach, we sequentially summarize\nevery n words. Here, we denote every n words as\nchapters and the summary generated in each chap-\nter as chapter summary. Afterward, we generate the\nfinal summary from these chapter summaries, ei-\nther by concatenating the chapter summaries or by\nre-summarizing/re-writing the chapter summaries.\nTo generate the summary of each chapter, we use\nthe same prompt as we used in our previous ap-\nproach of summarization via truncation. To gener-\nate the final summary from the chapter summaries,\nin addition to directly concatenating the chapter\nsummaries, we investigate the following prompts:\nPrompt (Re-write):“Rewrite the following text\nby maintaining coherency: [Chapter Summaries]”\nPrompt (Re-summarize):“Summarize the fol-\nlowing text: [Chapter Summaries]”\n4 Experiments\nIn this section, we first present our models along\nwith their implementation details. Next, we demon-\nstrate the datasets we used for evaluation. Finally,\nwe demonstrate our experimental findings.\n4.1 Models\nWe use four different LLMs (three closed-source\nand one open-source) to benchmark their perfor-\nmance in meeting transcripts. For the open-source\nmodel, we run our experiments in ag2-standard-96\nmachine in Google Cloud Platform3 (GCP), having\n384GB of RAM memory and 8 NVIDIA L4 GPUs\n(24GB). For the closed-source models, we use their\nrespective APIs. Below, we describe these models.\nGPT-3.5: It is an autoregressive LLM that lever-\nages the reinforcement learning from human feed-\nback (RLHF) mechanism. It is the first back-\nbone model behind ChatGPT and obtains impres-\nsive zero-shot performance across various tasks\n(Laskar et al., 2023a). We use the gpt-3.5-turbo-\n0613 model from OpenAI4 that has the maximum\ncontext length of 4K tokens.\nGPT-4: It is the latest LLM released by Ope-\nnAI which is also the first multimodal model in the\nGPT series (OpenAI, 2023). It is considered more\nreliable having better instruction-following capa-\nbilities than GPT-3.5. We use the version ofgpt-4\nthat can consider the context length of 8k tokens.\nPaLM-2: PaLM-2 is an LLM (Google, 2023)\ndeveloped by Google. It leverages the mixture of\nobjectives technique (Google, 2023) and signifi-\ncantly outperforms the original PaLM (Chowdhery\net al., 2022) model. We use the text-bison@001\nmodel in Google’s VertexAI5 for PaLM-2 that has\nan input context window of 8K tokens.\nLLaMA-2: LLaMA-2 (Touvron et al., 2023)\nis an open-source LLM developed by Meta. One\nmajor advantage of LLaMA-2 over the previously\nmentioned LLMs is that it is open-sourced and\navailable for both research and commercial pur-\nposes. In this paper, we use the respective Chat\nversions of LLaMA-2 for both 7B and 13B models\nfrom HuggingFace6 (Wolf et al., 2020).\n3https://cloud.google.com/\n4https://platform.openai.com/docs/models/\n5https://cloud.google.com/vertex-ai/docs/\ngenerative-ai/model-reference/text\n6https://huggingface.co/meta-llama\n345\nDataset No. of Meetings Avg. Transcript Len. Avg. Summary Len.\nQMSUMFiltered 37 10546 108\nAMI 20 7194 292\nICSI 6 13998 454\nTable 1: Evaluation Dataset Statistics.\n4.2 Datasets\nAs our goal is to build the summarization system\nfor real-world ASR-generated transcripts in the or-\nganizational meeting domain, we use datasets that\nare more relevant to our use-case. Note that for\nall datasets, we use their respective test sets and\nevaluate different LLMs using the same zero-shot\nprompts as our goal is to build a real-world meeting\nsummarization system in scenarios when training\ndatasets are not available. Thus, more generalized\nperformance across various datasets is prioritized\nover obtaining state-of-the-art results on a particu-\nlar dataset. Below, we describe these datasets (also\nsee Table 1).\nQMSUM dataset:It is a meeting summariza-\ntion dataset that consists of 232 meetings in mul-\ntiple domains (Zhong et al., 2021). However, this\ndataset is particularly constructed to conduct query-\nbased summarization (Laskar et al., 2022c) of meet-\nings, while our objective is to build a real-world\nsummarization system that is required to generate\nthe overall meeting summaries. Thus, we exclude\ninstances from the QMSUM dataset that require\nthe generation of meeting summaries depending on\nspecific queries. This results in a filtered version of\nthe QMSUM dataset (contains only 37 samples in\nthe test set) that is more relevant to our target task.\nAMI dataset:The AMI (Carletta et al., 2005)\ndataset contains 140 scenario-based product design\nrelated meetings. The length of each meeting is\nusually between 30-40 minutes.\nICSI dataset: The ICSI (Janin et al., 2003)\ndataset consists of 75 meetings. The length of\neach meeting in this dataset is approximately 1\nhour. This dataset consists of research-related dis-\ncussions among students at the International Com-\nputer Science Institute (ICSI) in Berkeley.\n4.3 Results & Discussions\nFor performance evaluation, we use ROUGE-1, 2,\nL (R-1, R-2, R-L) (Lin, 2004), and BERTScore (B-\nS) (Zhang et al., 2019) as our evaluation metrics.\nFor B-S, we use the DeBERTa-xlarge-mnli (He\net al., 2020) model. Below, we present our findings.\n4.3.1 Performance on Benchmark Datasets\nWhile most of the LLMs we use in this paper for\nevaluation support at least 4K tokens, we find that\nlonger sequence length makes the inference slower,\nwhich is not practical as our goal is to build a sum-\nmarization system for production usage. Also, in\nterms of open-source models like LLaMA-2, it\nincreases the computational requirements (e.g., re-\nquires high-end GPUs). Considering these restric-\ntions, we use n = 2500 words as the maximum\ninput sequence length for all models to ensure a fair\nevaluation. We show the results for all LLMs in\ndifferent datasets in Table 2 (For simplicity, we also\ndemonstrate the performance based on the average\nacross all datasets according to various summariza-\ntion types and different models in Figure 2). Below,\nwe summarize our observations:\n(i) In the QMSUM (Filtered) dataset, we sur-\nprisingly find that the summarization via chapter-\nization approaches fail to outperform the summa-\nrization via truncationapproach in many scenarios.\nThis is surprising since the model does not have\naccess to the whole context when the summariza-\ntion via truncationapproach is used, indicating that\nthe gold reference summaries in this dataset could\npossibly be more biased towards the beginning of\nthe transcript.\n(ii) In the AMI and ICSI datasets, among the\nclosed-source models, the performance of the\nPaLM-2 model noticeably lags behind that of\nGPT-3.5, and GPT-4. More interestingly, PaLM-2\neven lags behind much smaller LLaMA-2-7B and\nLLaMA-2-13B models. Note that we did not ob-\nserve such a poor performance by PaLM-2 in the\nQMSUM dataset.\n(iii) In the AMI dataset, we find that GPT-4 is\nthe best in terms of ROUGE-1 and BERTScore,\nwhile GPT-3.5 is found to be the best in terms of\nROUGE-2&L. For both models, the best results\nin these metrics are achieved based on the chap-\nterization via re-writingapproach. However, we\nfind that the performance is generally much poorer\nfor these models when the chapterization via re-\nsummarization and the summarization via trunca-\ntion approaches are used (we also find quite similar\ntrends for the LLaMA-2 models in this dataset).\n(iv) In the ICSI dataset, we find that various ap-\nproaches using GPT-3.5 led to the best performance\nacross different metrics. In terms of ROUGE-1&L,\nwe find that chapterization via re-writingis the\nbest, while in terms of ROUGE-2 and BERTScore,\n346\nDataset\nModel Type QMSUM (Filtered) AMI ICSI\nR-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S\nGPT-3.5 chapter (concat) 23.01 6.62 14.12 57.41 39.74 9.93 19.71 58.94 36.01 7.56 15.21 57.32\nGPT-3.5 chapter (resummarize) 30.71 6.20 18.61 61.02 29.36 5.95 16.08 57.83 23.43 3.03 11.54 53.46\nGPT-3.5 chapter (rewrite) 27.31 6.70 15.12 58.21 39.68 9.94 19.72 59.71 37.50 7.55 16.29 57.25\nGPT-3.5 truncation 32.01 6.62 19.02 60.81 29.63 6.52 16.43 57.85 20.42 2.71 10.95 52.87\nGPT-4 chapter (concat) 27.60 6.71 16.45 59.39 39.36 9.18 17.73 59.61 34.50 6.28 14.97 57.05\nGPT-4 chapter (resummarize) 32.11 6.11 18.41 61.52 30.02 6.56 15.96 57.88 21.84 3.89 11.86 55.64\nGPT-4 chapter (rewrite) 30.05 7.06 17.07 60.13 39.76 9.65 19.25 59.76 36.39 7.52 16.17 57.28\nGPT-4 truncation 33.41 7.30 17.82 60.91 32.56 6.75 16.93 58.01 20.42 4.23 12.02 53.64\nPaLM-2 chapter (concat) 20.61 4.12 11.92 48.92 16.11 1.01 11.35 47.08 15.12 1.24 11.27 43.59\nPaLM-2 chapter (resummarize) 16.62 3.50 10.32 46.01 7.26 0.64 5.59 37.97 5.31 0.37 3.75 37.25\nPaLM-2 chapter (rewrite) 22.01 4.20 13.21 51.23 8.56 0.84 5.86 41.47 8.58 0.39 5.83 36.57\nPaLM-2 truncation 13.92 2.51 9.13 45.62 18.36 2.82 10.82 45.93 8.43 0.95 6.07 42.93\nLLaMA-2-13b chapter (concat) 15.38 4.54 10.19 51.93 34.85 8.95 18.23 55.88 32.31 6.75 14.27 53.97\nLLaMA-2-13b chapter (resummarize) 29.01 5.71 17.64 55.49 28.73 6.28 16.61 54.71 26.84 4.42 13.31 54.32\nLLaMA-2-13b chapter (rewrite) 26.73 6.33 16.83 54.37 37.38 8.37 19.36 57.55 33.53 6.05 15.06 54.42\nLLaMA-2-13b truncation 28.64 6.39 18.29 55.32 33.38 7.24 18.64 55.38 24.62 3.35 13.39 51.58\nLLaMA-2-7b chapter (concat) 15.72 4.37 10.03 51.93 32.34 8.08 16.33 53.91 32.42 7.21 13.82 55.06\nLLaMA-2-7b chapter (resummarize) 29.65 6.37 17.41 57.66 30.92 5.95 16.63 56.63 24.72 4.45 12.17 46.13\nLLaMA-2-7b chapter (rewrite) 27.99 6.25 17.21 56.17 37.62 8.41 18.43 56.35 26.59 4.11 12.39 48.52\nLLaMA-2-7b truncation 25.48 5.69 15.01 53.58 30.22 6.59 16.52 55.35 17.72 2.14 9.24 48.84\nTable 2: Performance of LLMs on the QMSUM (Filtered), AMI, and ICSI Datasets.\nchapterization via concatenationled to the best re-\nsult. Similar to the AMI dataset, we again observe\npoor performance for GPT and LLaMA models\nusing the chapterization via re-summarizationand\nsummarization via truncationapproaches in the\nICSI dataset, with truncation-based approach lead-\ning to the poorest performance. This may indicate\nthat in AMI and ICSI datasets that require longer\nsummaries (approximately, 300-450 words long\nsummaries on average), either concatenation or re-\nwriting of the chapter summaries is more useful.\n(v) In the QMSUM dataset, the truncation ap-\nproach is found to be the best-performing approach,\nas GPT-4 using this technique achieves the best\nresult in terms of ROUGE-1&2, while GPT-3.5\nachieves the best based on ROUGE-L. However,\nin terms of BERTScore, we find that GPT-4 based\non chapterization via resummarizationapproach\nto be the best. These findings may indicate that\nin datasets where the average gold reference sum-\nmary length is smaller (in QMSUM, the average\nsummary length is just 108 words), chapterization\nvia resummarizationor summarization via trunca-\ntion approaches are more useful.\n(vi) The average across all datasets in terms of\ndifferent LLMs (see Figure 2a) demonstrate that\nGPT-4 is generally the best, followed by GPT-3.5,\nwith PaLM-2 being the worst performer. We also\nobserve that LLaMA-2 models achieve competitive\nperformance, with the 7B model performing almost\nsimilar to the 13B model. This opens up the pos-\nsibility of fine-tuning smaller LLMs on in-domain\ndatasets to achieve further performance gain.\nContext Length\n2500 5000\nDataset R-1 R-2 R-L B-S R-1 R-2 R-L B-S\nQMSUM 33.41 7.30 17.82 60.91 33.43 7.69 18.30 61.02\nAMI 32.56 6.75 16.93 58.01 30.59 5.68 14.94 56.97\nICSI 20.42 4.23 10.92 53.64 24.32 4.41 12.04 54.53\nTable 3: Results based on Context Length for GPT-4.\n(vii) Based on the average across all datasets in\nterms of different summarization types (see Figure\n2b), the chapterization via rewritingapproach is\nfound to be the best. We also do not find any signif-\nicant difference in performance based on various\nsummarization types. Since the truncation-based\napproach achieves performance comparable to var-\nious chapterization-based approaches even without\nconsidering the whole context, further investigation\nof the gold summaries in these datasets is needed.\n4.3.2 Case Study\nTo further investigate the performance of LLMs, we\nconduct some case studies using theSummarization\nvia Truncationapproach, as demonstrated below.\n(i) Case Study on Sequence Length:In this case\nstudy, we investigate how the value ofn for maxi-\nmum input sequence length impacts the overall per-\nformance. In our study, we use the GPT-4 model\nand increase the value of n from 2500 to 5000. We\npresent the results in different datasets in Table 3 to\nfind that increasing the maximum input sequence\nlength from 2500 to 5000 does not necessarily lead\nto an increase in performance. More specifically,\nin datasets that have an average transcript length\n347\n(a) Based on different model types.\n(b) Based on different summarization types.\nFigure 2: Average score across all datasets based on (a) model typesand (b) summarization types.\nof more than 10,000 words, we observe that the\nperformance is increased with the increase in con-\ntext length (e.g., QMSUMFiltered and ICSI). While\nthe performance drops in datasets having smaller\ncontext length (e.g., AMI). Liu et al. (2023) also\nfind that increasing the sequence length of LLMs\nto consider long contexts does not necessarily im-\nprove the performance. Thus, future work should\nconduct more qualitative evaluations to identify the\nreason behind this trend.\n(ii) Case Study on Prompt Variations:Here, we\nconduct experiments with GPT-3.5 in the QMSUM\ndataset using the following prompts:\nLong: Generate a long and descriptive summary\nof the following conversation.\nMedium: Generate a summary of the following\nconversation.\nShort: Generate a very short and concise sum-\nmary of the following conversation.\nWe present our results for this case study in Ta-\nble 4 to find that different prompts yield different\nresults. For instance, prompting to generate long\nsummaries led to an average summary length of\n402.70, which also led to the poorest performance\nin terms of both ROUGE and BERTScore. Mean-\nwhile, shorter summaries yield better performance.\nSince the average length of the summary in QM-\nSUM is 108 words, the prompt to generate the sum-\nmary without explicitly specifying the size (long or\nType R-1 R-2 R-L B-S Length\nTruncation (Long) 23.61 5.68 13.64 55.99 402.70\nTruncation (Medium) 32.01 6.62 19.02 60.81 136.35\nTruncation (Short) 31.81 6.19 18.76 60.34 74.40\nTable 4: Results based on Prompt Variations in the\nQMSUM (filtered) Dataset for GPT-3.5. Here, ‘Length’\ndenotes “average length of the generated summary”.\nshort) leads to an average summary length of 136\nwords, which also achieves the best performance.\nThese findings demonstrate that based on user re-\nquirements, we may build summarization systems\nin the real-world that can generate summaries of\nvarious lengths.\n5 Using LLMs in Real-World Systems\nTo deploy LLMs in the real world, we study the\nfollowing aspects: cost and inference speed.\nCost: Based on our experiments, we find that\nexcept for the PaLM-2 model, the closed-source\nLLMs usually outperform the open-source LLaMA-\n2 models. While GPT-4 generally performs the\nbest, it is also more costly. As of the writing of this\npaper, the pricing7 in OpenAI for the GPT series\nmodels are as follows: for GPT-4, the 8K context\nversion costs 0.03$ per 1K input tokens and 0.06$\nper 1K output tokens, while for the 4K context ver-\nsion of GPT-3.5 that we use costs 0.0015$ per 1K\n7https://openai.com/pricing\n348\ninput tokens and 0.002$ per 1K output tokens. On\naverage, it makes GPT-4 25 times more costly than\nGPT-3.5. Meanwhile, for PaLM-2, the pricing8 in\nGoogle Cloud is 0.0010$ per 1000 characters (for\nboth input and output). Approximately, 1 token\nis considered as 4 characters. Thus, the cost for\nPaLM-2 is 0.0040$ per 1K tokens, making it about\n2.5 times more costly than GPT-3.5. Regarding\nLLaMA-2, we were able to run it in a machine\nwith 1 NVIDIA L4 GPU, while the LLaMA-2-13B\nmodel was possible to run using 2 L4 GPUs. How-\never, using multiple GPUs significantly increases\nthe production cost. Thus, in terms of real-world\nusage, LLaMA-2-7B is more useful than LLaMA-\n2-13B as we observe almost similar performance\nusing these models across various datasets.\nInference Speed: We also measure the infer-\nence speed of different LLMs. For this purpose,\nwe collected 100 meeting transcripts from Dialpad9\nthat consist of real-world conversations. All the col-\nlected transcripts have at least 2500 words. Based\non our analysis using the Summarization via Trun-\ncation approach, we find that GPT-3.5 is the fastest,\nas it takes 2.5 seconds on average per transcript for\ninference, followed by PaLM-2 which takes about\n3.2 seconds on average. While GPT-4 achieves\nthe best performance in terms of ROUGE and\nBERTScore metrics, it is also the slowest among\nthe commercial closed-source LLMs since it takes\nabout 11 seconds on average per transcript. We also\nbenchmark the inference speed of LLaMA-2-7B in\nGCP on a machine having 1 NVIDIA L4 GPU and\nfind that it takes 15 seconds on average.\nDeployment: While using the APIs of closed-\nsource models usually leads to faster inference\nspeed, there are some drawbacks of using closed-\nsource LLMs. For instance, businesses need to\nsend their customer data using the 3rd-party API,\nwhich may lead to potential privacy risks. Mean-\nwhile, we also observe that these LLM APIs some-\ntimes become too slow when the demand is high,\nleading to API request failure. By considering\nsuch cases of API failures, the average inference\nspeed of GPT-4 is increased to 40 seconds per tran-\nscript (up from 11 seconds). Thus, based on cost\nand inference speed trade-off, alongside the pri-\nvacy concerns and the possibility of fine-tuning on\nin-domain datasets without additional deployment\ncosts, LLaMA-2-7B looks more promising for pro-\n8https://cloud.google.com/vertex-ai/pricing\n9https://www.dialpad.com/ca/\nduction usage. Meanwhile, we can also leverage\nvarious model optimization (Zhu et al., 2023) tech-\nniques like quantization, pruning, distillation, etc.\nto further reduce the memory requirement as well\nas improve the inference speed. Therefore, we se-\nlect the LLaMA-2-7B model for real-world usage\nand after further fine-tuning on our in-domain data,\nwe successfully deploy it in our production envi-\nronment in a machine having 1 NVIDIA L4 GPU.\n6 Conclusion\nIn this paper, our extensive study involving vari-\nous LLMs led to several key insights on building a\nreal-world meeting summarization system. While\nmost closed-source LLMs usually outperform their\nopen-source counterparts, striking a balance be-\ntween cost and performance (while also addressing\npotential privacy risks) is crucial for practical, real-\nworld deployment. This became evident in the case\nof GPT-4, which, while showing superior perfor-\nmance in most datasets, was considerably less cost-\neffective. By considering the performance and cost\ntrade-off, as well as the privacy concern, we deploy\na summarization model based on LLaMA-2-7B to\ngenerate live summaries of ASR-generated meeting\ntranscripts. This research thereby provides crucial\ninsights and a clear perspective on deploying LLMs\nin real-world industrial settings. In the future, we\nwill investigate the performance of LLMs by apply-\ning various optimization techniques.\nLimitations\nOne of the limitations of this work is that the mod-\nels were only evaluated on academic datasets even\nthough our focus was to build this system for real-\nworld usage for a particular business organization\nto summarize meeting conversations. Thus, fu-\nture work should focus on constructing a dataset\nfor the target domain consisting of real-world con-\nversations. Moreover, it has been found recently\nthat existing evaluation metrics like ROUGE have\nseveral limitations while evaluating the perfor-\nmance of LLMs in summarization datasets (Laskar\net al., 2023a; Goyal et al., 2022). Thus, future\nwork should also benchmark the performance of\nLLMs in meeting summarization based on exten-\nsive human evaluation. While this paper eval-\nuates 3 closed-source LLMs and 1 open-source\nLLM, there are many other recently proposed open-\nsource LLMs (Zhang et al., 2023; Zhao et al., 2023;\n349\nChang et al., 2023), such as Cerebras10, Pythia11,\nDolly12, Vicuna13, MPT14, RedPajama15, Falcon16,\nXGen17, Mistral18, which are not evaluated in this\nwork. Nonetheless, LLaMA-2 is found to be one\nof the best-performing open-source models (Zhao\net al., 2023) and so we select this model in our\nwork. Though the performance of different LLMs\nmay not be state-of-the-art in various datasets, the\nintended development of the summarization system\nusing LLMs was to ensure more generalized perfor-\nmance in our targeted domain across various types\nof meetings, instead of obtaining state-of-the-art\nperformance in a particular dataset.\nEthics Statement\nLicense: We maintained the licensing require-\nments accordingly while using different tools from\nthe providers (e.g., OpenAI, Google, Meta, Hug-\ngingFace).\nPrivacy: To protect user privacy, sensitive data\nsuch as personally identifiable information (e.g.,\ncredit card number, phone number, person names)\nwere removed while benchmarking the inference\nspeed of different LLMs on the collected 100 real-\nworld transcripts.\nIntended Use: Note that our model is intended\nto provide business organizations with a quick\noverview of the meetings. While poor summariza-\ntion quality may lead to a bad user experience, it\nshould not lead to any ethical concern since the\nsummary is required to be generated based on only\nthe given transcript. Meanwhile, the LLM that\nwould be used in production for summarization\nwill only do inference but will not be re-trained\non live meeting transcripts. Only the users of a\nparticular meeting will have access to the summary\nand so information from any other meetings will\nnot be revealed to the users.\n10https://huggingface.co/cerebras/\nCerebras-GPT-13B\n11https://github.com/EleutherAI/pythia\n12https://huggingface.co/databricks/\ndolly-v2-7b\n13https://huggingface.co/lmsys/vicuna-7b-v1.5\n14https://huggingface.co/mosaicml/\nmpt-7b-instruct\n15https://huggingface.co/togethercomputer/\nRedPajama-INCITE-7B-Instruct\n16https://huggingface.co/tiiuae/falcon-7b\n17https://github.com/salesforce/xgen\n18https://huggingface.co/mistralai/\nMistral-7B-Instruct-v0.1\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nJean Carletta, Simone Ashby, Sebastien Bourban, Mike\nFlynn, Mael Guillemot, Thomas Hain, Jaroslav\nKadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa\nKronenthal, et al. 2005. The ami meeting corpus:\nA pre-announcement. In International workshop on\nmachine learning for multimodal interaction, pages\n28–39. Springer.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nKaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2023. A sur-\nvey on evaluation of large language models. arXiv\npreprint arXiv:2307.03109.\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin\nGimpel. 2022. Summscreen: A dataset for abstrac-\ntive screenplay summarization. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8602–8615.\nYulong Chen, Yang Liu, Liang Chen, and Yue Zhang.\n2021. DialogSum: A real-life scenario dialogue sum-\nmarization dataset. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 5062–5074, Online. Association for Computa-\ntional Linguistics.\nSangwoo Cho, Franck Dernoncourt, Tim Ganter,\nTrung Bui, Nedim Lipka, Walter Chang, Hailin Jin,\nJonathan Brandt, Hassan Foroosh, and Fei Liu. 2021.\nStreamHover: Livestream transcript summarization\nand annotation. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 6457–6474, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAnn Clifton, Sravana Reddy, Yongze Yu, Aasish Pappu,\nRezvaneh Rezapour, Hamed Bonab, Maria Eskevich,\nGareth Jones, Jussi Karlgren, Ben Carterette, et al.\n2020. 100,000 podcasts: A spoken english document\ncorpus. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 5903–\n5917.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\n350\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nXue-yong Fu, Cheng Chen, Md Tahmid Rahman\nLaskar, Shayna Gardiner, Pooja Hiranandani, and\nShashi Bhushan Tn. 2022. Entity-level sentiment\nanalysis in contact center telephone conversations.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing: Industry\nTrack, pages 484–491, Abu Dhabi, UAE. Association\nfor Computational Linguistics.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. SAMSum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. In Proceedings of the 2nd Workshop on\nNew Frontiers in Summarization, pages 70–79, Hong\nKong, China. Association for Computational Linguis-\ntics.\nGoogle. 2023. Palm 2 technical report. Goole AI.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\ngpt-3. arXiv preprint arXiv:2209.12356.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nYebowen Hu, Timothy Ganter, Hanieh Deilamsalehy,\nFranck Dernoncourt, Hassan Foroosh, and Fei Liu.\n2023. MeetingBank: A benchmark dataset for meet-\ning summarization. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 16409–\n16423, Toronto, Canada. Association for Computa-\ntional Linguistics.\nAdam Janin, Don Baron, Jane Edwards, Dan Ellis,\nDavid Gelbart, Nelson Morgan, Barbara Peskin,\nThilo Pfau, Elizabeth Shriberg, Andreas Stolcke, et al.\n2003. The icsi meeting corpus. In 2003 IEEE In-\nternational Conference on Acoustics, Speech, and\nSignal Processing, 2003. Proceedings.(ICASSP’03).,\nvolume 1, pages I–I. IEEE.\nMisha Khalman, Yao Zhao, and Mohammad Saleh.\n2021. Forumsum: A multi-speaker conversation sum-\nmarization dataset. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages\n4592–4599.\nElena Khasanova, Pooja Hiranandani, Shayna Gardiner,\nCheng Chen, Simon Corston-Oliver, and Xue-Yong\nFu. 2022. Developing a production system for Pur-\npose of Call detection in business phone conversa-\ntions. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies: Industry Track, pages 259–267, Hybrid: Seattle,\nWashington + Online. Association for Computational\nLinguistics.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\nand Jimmy Huang. 2023a. A systematic study and\ncomprehensive evaluation of ChatGPT on benchmark\ndatasets. In Findings of the Association for Com-\nputational Linguistics: ACL 2023, pages 431–469,\nToronto, Canada. Association for Computational Lin-\nguistics.\nMd Tahmid Rahman Laskar, Cheng Chen, Xue-yong Fu,\nMahsa Azizi, Shashi Bhushan, and Simon Corston-\noliver. 2023b. AI coach assist: An automated ap-\nproach for call recommendation in contact centers\nfor agent coaching. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 599–\n607, Toronto, Canada. Association for Computational\nLinguistics.\nMd Tahmid Rahman Laskar, Cheng Chen, Jonathan\nJohnston, Xue-Yong Fu, Shashi Bhushan TN, and Si-\nmon Corston-Oliver. 2022a. An auto encoder-based\ndimensionality reduction technique for efficient en-\ntity linking in business phone conversations. In Pro-\nceedings of the 45th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, pages 3363–3367.\nMd Tahmid Rahman Laskar, Cheng Chen, Aliak-\nsandr Martsinovich, Jonathan Johnston, Xue-Yong\nFu, Shashi Bhushan Tn, and Simon Corston-Oliver.\n2022b. BLINK with Elasticsearch for efficient entity\nlinking in business conversations. In Proceedings of\nthe 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies: Industry Track, pages\n344–352, Hybrid: Seattle, Washington + Online. As-\nsociation for Computational Linguistics.\nMd Tahmid Rahman Laskar, Enamul Hoque, and\nJimmy Xiangji Huang. 2022c. Domain adaptation\nwith pre-trained transformers for query-focused ab-\nstractive text summarization. Computational Linguis-\ntics, 48(2):279–320.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How lan-\nguage models use long contexts. arXiv preprint\narXiv:2307.03172.\n351\nEtienne Manderscheid and Matthias Lee. 2023. Predict-\ning customer satisfaction with soft labels for ordinal\nclassification. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 5: Industry Track), pages 652–659,\nToronto, Canada. Association for Computational Lin-\nguistics.\nAnna Nedoluzhko, Muskaan Singh, Marie Hledíková,\nTirthankar Ghosal, and Ond ˇrej Bojar. 2022. Elitr\nminuting corpus: A novel dataset for automatic\nminuting from multi-party meetings in english and\nczech. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 3174–\n3182.\nOpenAI. 2023. Gpt-4 technical report.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,\nXiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-\nwei Zhang, Fei Wu, et al. 2023. Instruction tuning\nfor large language models: A survey. arXiv preprint\narXiv:2308.10792.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan, Asli Celikyil-\nmaz, Yang Liu, Xipeng Qiu, et al. 2021. Qmsum: A\nnew benchmark for query-based multi-domain meet-\ning summarization. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 5905–5921.\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.\n2021. MediaSum: A large-scale media interview\ndataset for dialogue summarization. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5927–5934,\nOnline. Association for Computational Linguistics.\nXunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weip-\ning Wang. 2023. A survey on model compres-\nsion for large language models. arXiv preprint\narXiv:2308.07633.\n352",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9170411825180054
    },
    {
      "name": "Computer science",
      "score": 0.713474690914154
    },
    {
      "name": "Open source",
      "score": 0.6394003629684448
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.44573426246643066
    },
    {
      "name": "Artificial intelligence",
      "score": 0.174041748046875
    },
    {
      "name": "Programming language",
      "score": 0.12679025530815125
    },
    {
      "name": "Software",
      "score": 0.08280327916145325
    }
  ]
}