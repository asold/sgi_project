{
    "title": "LocalViT: Analyzing Locality in Vision Transformers",
    "url": "https://openalex.org/W3156811085",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2062210512",
            "name": "Li, Yawei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1913798565",
            "name": "Zhang Kai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225581206",
            "name": "Cao, Jiezhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744068600",
            "name": "Timofte, Radu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745051819",
            "name": "Magno Michele",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742795624",
            "name": "Benini Luca",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742174613",
            "name": "Van Gool, Luc",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W2612445135",
        "https://openalex.org/W2796438033",
        "https://openalex.org/W2953106684",
        "https://openalex.org/W2946948417",
        "https://openalex.org/W3139445856",
        "https://openalex.org/W3136525061",
        "https://openalex.org/W2886953980",
        "https://openalex.org/W2964259004",
        "https://openalex.org/W3019527251",
        "https://openalex.org/W3122154272",
        "https://openalex.org/W3000514857",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W3109319753",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W3126337037",
        "https://openalex.org/W2508457857",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W2951638509",
        "https://openalex.org/W3193659426",
        "https://openalex.org/W3016265891",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2805163084",
        "https://openalex.org/W3091156754",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3115390238",
        "https://openalex.org/W2511730936",
        "https://openalex.org/W2952632681",
        "https://openalex.org/W2963840672",
        "https://openalex.org/W3029385331",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3119686997",
        "https://openalex.org/W3184564979",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3128626728",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2949650786",
        "https://openalex.org/W3111551570",
        "https://openalex.org/W2902199720"
    ],
    "abstract": "The aim of this paper is to study the influence of locality mechanisms in vision transformers. Transformers originated from machine translation and are particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking is a locality mechanism for information exchange within a local region. In this paper, locality mechanism is systematically investigated by carefully designed controlled experiments. We add locality to vision transformers into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to vision transformers with different architecture designs, which shows the generalization of the locality concept. For ImageNet2012 classification, the locality-enhanced transformers outperform the baselines Swin-T, DeiT-T, and PVT-T by 1.0%, 2.6% and 3.1% with a negligible increase in the number of parameters and computational effort. Code is available at https://github.com/ofsoundof/LocalViT.",
    "full_text": "LocalViT: Analyzing Locality in Vision Transformers\nYawei Li1, Kai Zhang 1, Jiezhang Cao 1, Radu Timofte 1,2, Michele Magno 3, Luca Benini 4,5, Luc Van Gool 1,6\nAbstract— The aim of this paper is to study the influence\nof locality mechanisms in vision transformers. Transformers\noriginated from machine translation and are particularly good\nat modelling long-range dependencies within a long sequence.\nAlthough the global interaction between the token embeddings\ncould be well modelled by the self-attention mechanism of\ntransformers, what is lacking is a locality mechanism for infor-\nmation exchange within a local region. In this paper, locality\nmechanism is systematically investigated by carefully designed\ncontrolled experiments. We add locality to vision transformers\ninto the feed-forward network. This seemingly simple solution is\ninspired by the comparison between feed-forward networks and\ninverted residual blocks. The importance of locality mechanisms\nis validated in two ways: 1) A wide range of design choices\n(activation function, layer placement, expansion ratio) are\navailable for incorporating locality mechanisms and proper\nchoices can lead to a performance gain over the baseline, and\n2) The same locality mechanism is successfully applied to vision\ntransformers with different architecture designs, which shows\nthe generalization of the locality concept. For ImageNet2012\nclassification, the locality-enhanced transformers outperform\nthe baselines Swin-T [1], DeiT-T [2] and PVT-T [3] by 1.0%,\n2.6% and 3.1% with a negligible increase in the number\nof parameters and computational effort. Code is available at\nhttps://github.com/ofsoundof/LocalViT.\nI. I NTRODUCTION\nRecent advances in machine learning (ML) research, such\nas computer vision and natural language processing, have\nbeen driven by backbone models that can be adapted to\ndifferent problems [4], [5]. However, the foundation models\nin vision and language tend to be heavy and computational\nexpensive, thus hindering their applicability in edge devices\nsuch as drones and robotics. Thus, how to improve the\nefficiency of neural models such as convolutional neural\nnetworks (CNNs) and transformers becomes important.\nCNNs are based on locality in that convolutional filters\nonly perceive a local region of the input image, i.e. the\nreceptive field. By stacking multiple layers, the effective\nreceptive fields of a deep neural network can be enlarged\nprogressively. This design enables the network to learn a\nhierarchy of deep features, which is essential for the success\nof CNNs. Meanwhile, the local, repetitive connections save\nmany parameters compared with fully connected layers. Yet,\n1Computer Vision Lab, D-ITET, ETH Z ¨urich, Switzerland. E-mail:\nyawei.li@vision.ee.ethz.ch.\n2Center for Artificial Intelligence and Data Science (CAIDAS), the\nUniversity of W ¨urzburg, Germany.\n3Center for Project-Based Learning, D-ITET, ETH Z ¨urich, Switzerland.\n4Integrated Systems Laboratory, D-ITET, ETH Z ¨urich, Switzerland.\n5Department of Electrical, Electronic and Information Engineering, Uni-\nversity of Bologna, Italy.\n6Processing Speech and Images (PSI), KU Leuven, Belgium.\none problem is that a larger receptive field can only be\nachieved by combining layers, despite alternative attempts\nat enlarging the receptive field [6].\nFig. 1: Comparison between LocalViT and the baseline\ntransformers. The transformers enhanced by the proposed\nlocality mechanism outperform their baselines\nA parallel research strand incorporates global connectivity\ninto the network via self-attention [4], [7], [8], [9], [10].\nThis family of networks, i.e. transformer networks, originates\nfrom machine translation and is very good at modelling\nlong-range dependencies in sequences. There also is a rising\ninterest in applying transformers to vision [11], [5], [2].\nVision transformers have already achieved performances\nquite competitive with their CNN counterparts.\nTo process 2D images with transformers, the input image\nis first converted to a sequence of tokens which correspond\nto patches in the image. Then the attention module attends\nto all tokens and a weighted sum is computed as the tokens\nfor the next layer. In this way, the effective receptive field\nis expanded to the whole image via a single self-attention\nlayer. Yet, the problem of visual transformers is that global\nconnectivity contradicts the convolutional idea.\nConsidering the merits of CNNs vs. transformers, a natural\nquestion is whether we can efficiently combine the locality of\nCNNs and the global connectivity of vision transformers to\nimprove performance while not increasing model complexity.\nThis aim of this paper is aligned with other works that try\nto answer this interesting question, i.e. taking the advantage\nof both convolution and transformers [12], [13], [14], [15],\n[16], [17]. Differently, we provide a systematic analysis\narXiv:2104.05707v2  [cs.CV]  12 Feb 2025\nof various design choices of the locality mechanism by\nrigorous and controlled experiments. Beyond that, we try\nto generalize the conclusions from the ablation study to a\nbunch of vision transformers. Thus, the aim of this paper\nis to thoroughly investigate a single component (locality\nmechanism) in vision transformers.\nTo conduct the investigation, we start with a mechanism\nthat injects locality into the feed-forward network of trans-\nformers, which is inspired by examining the feed-forward\nnetwork and inverted residuals [18], [19]. The feed-forward\nnetwork of transformers consists of two fully connected lay-\ners and the hidden dimension between them is expanded to\nextract richer features. Similarly, in inverted residual blocks,\nthe hidden channel between the two 1 × 1 convolutions is\nalso expanded. The major difference between them is the ef-\nficient depth-wise convolution in the inverted residual block.\nSuch convolution can provide precisely the mechanism for\nlocal information aggregation which is missing in the feed-\nforward network of vision transformers. To cope with the\nconvolution, the image tokens of the sequence from the self-\nattention module must be rearranged to a 2D feature map,\nwhich is processed by the feed-forward network. The class\ntoken is split out and bypasses the feed-forward network.\nThe derived new feature map is converted back to image\ntokens and concatenated with the bypassed class token. The\nconcatenated sequence is processed by the next transformer\nlayer.\nThrough the empirical study, we derive two sets of conclu-\nsions. Firstly, four important properties of the investigated\nlocality mechanism are revealed. i. Locality mechanism\nalone can already improve the performance of the baseline\ntransformer. ii. A better activation function can result in a\nsignificant performance gain. iii. The locality mechanism is\nmore important for lower layers. iv. Expanding the hidden\ndimension of the feed-forward network leads to a larger\nmodel capacity and a higher classification accuracy. Sec-\nondly, as shown in Fig. 1, the locality mechanism is suc-\ncessfully applied to 5 vision transformers, which underlines\nits generality. The contributions of this paper are three-fold:\n1) We study a locality mechanism that enhances vision\ntransformers. The modified transformer architecture\ncombines a self-attention mechanism for global rela-\ntionship modelling and a locality mechanism for local\ninformation aggregation.\n2) We analyze the basic properties of the introduced\nlocality mechanism. The influence of each component\n(depth-wise convolution, non-linear activation func-\ntion, layer placement, and hidden dimension expansion\nratio) is singled out.\n3) We apply these ideas to vision transformers incl.\nDeiT [2], Swin transformers [1], T2T-ViT [20],\nPVT [3], and TNT [21]. Experiments show that the\nsimple technique proposed in this paper generalizes\nwell to various transformer architectures.\nII. R ELATED WORK\nA. Transformers\nTransformers were first introduced in [4] for machine\ntranslation. The proposed attention mechanism aggregates\ninformation from the whole input sequence. Thus, transform-\ners are especially good at modelling long-range dependencies\nbetween elements of a sequence. Since then, there have been\nseveral attempts to adapt transformers towards vision and\nrobotics [11], [22], [23], [24], [25], [26], [27]. Most recently,\ntransformers are proposed to solve robotic problems by\nlearning the mapping from language and vision observations\nto robot actions [23].\nB. Locality vs. global connectivity\nBoth local information and global connectivity help to\nreason about the relationships between image contents. The\nconvolution operation applies a sliding window to the input\nand local information is inherently aggregated to compute\nnew representations. Thus, locality is an intrinsic property\nof CNNs [28]. Although CNNs can extract information from\na larger receptive field by stacking layers and forming deep\nnetworks, they still lack global connectivity [29], [30], [31].\nTo overcome this problem, some researchers add global\nconnectivity to CNNs with non-local blocks [32], [33].\nBy contrast, transformers are especially good at modelling\nlong-range dependencies within a a sequence owing to their\nattention mechanism [4]. But, in return, a locality mechanism\nremains to be added for visual perception. Some works\nalready contributed towards this goal [20], [1], [34], [35],\n[36], [37]. Those works mainly focus on improving the\ntokenization and self-attention parts. There are also some\nworks that introduce hybrid architectures of CNNs and\ntransformers [15], [16]. In the meanwhile, we also noticed\nsome other works introducing convolutions to different parts\nof transformers [17], [12]. The difference between our work\nand the other works is that we systematically investigate\nlocality mechanism and single out its importance to trans-\nformer architectures. This is inspired by the comparison\nbetween vision transformers and the inverted residual blocks\nin MobileNets.\nC. Inverted residuals\nCompared with normal convolution, the computations of\ndepth-wise convolution are only conducted channel-wise.\nThat is, to obtain a channel of the output feature map,\nthe convolution is only conducted on one input feature\nmap. Thus, depth-wise convolution is efficient both in terms\nof parameters and computation. Thus, Howard et al. first\nproposed the MobileNet architecture based on depth-wise\nseparable convolutions [38]. This lightweight and computa-\ntionally efficient network is quite friendly for mobile devices.\nSince then, depth-wise convolution has been widely used to\ndesign efficient models. Inverted residual blocks are based\non depth-wise convolution and were first introduced in Mo-\nbileNetV2 [18]. The inverted residual blocks are composed\nof a sequence of 1 × 1 - depth-wise - 1 × 1 convolutions.\nThe hidden dimension between the two 1 × 1 convolutions\nis expanded. The utilization of depth-wise convolution avoids\nthe drastic increase of model complexity brought by normal\nconvolution. Due to the efficiency of this module, it is\nwidely used to form the search space of neural architecture\nsearch (NAS) [19], [39], [40]. The expansion of the hidden\ndimension of inverted residuals is quite similar to the feed-\nforward network of vision transformers. This motivates us to\nthink about the connection between them (See Sec. III-B).\nIII. M ETHODOLOGY\nTransformers are usually composed of encoders and de-\ncoders with similar building blocks. For the image classifi-\ncation task considered here, only the encoders are included\nin the network. Thus, we mainly describe the operations in\nthe encoder layers. The encoders have two components, i.e.\nthe self-attention mechanism that relates a token to all of the\ntokens and a feed-forward network that is applied to every\ntoken. We specifically explain how to introduce locality into\nthe feed-forward network.\nA. Input interpretation\n(a) The input is regarded as a sequence of tokens.\n(b) An equivalent perspective is to still rearrange the tokens as a 2D lattice.\nFig. 2: Visualization of the feed-forward network in trans-\nformers from different perspectives. In this figure, n = 2,\nγ = 2, d = 5.\nSequence perspective. Inherited from language mod-\nelling, transformers regard the input as a sequence that\ncontains elements of embedded vectors. Consider an input\nimage X ∈ RC×H×W , where C and H × W denote the\nchannel and spatial dimension of the input image, resp. The\ninput image is first converted to tokens { ˆXi ∈ Rd|i =\n1, . . . , N}, where d = C × p2 is the embedding dimension\nand N = HW\np2 . The tokens can be aggregated into a matrix\nˆX ∈ RN×d.\nSelf-attention. In the self-attention mechanism, the rela-\ntionship between the tokens is modelled by the similarity\nbetween the projected query-key pairs, yielding the attention\nscore. The new tokens are computed as the weighted sum of\nthe project values. That is,\nZ = Softmax(QKT /\n√\nd)V, (1)\nwhere the Softmax function is applied to the rows of the\nsimilarity matrix and d provides a normalization. The query,\nkey, and value are a projection of the tokens,i.e. Q = ˆXWQ,\nK = ˆXWK, V = ˆXWV . The projection matrices WQ and\nWK have the same size while WV could have a different\nsize. In practice, the three projection matrices usually have\nthe same size, i.e. WQ, WK, WV ∈ Rd×d.\nFeed-forward network. After the self-attention layer, a\nfeed-forward network is appended. The feed-forward net-\nwork consists of two fully-connected layers and transforms\nthe features along the embedding dimension. The hidden di-\nmension between the two fully-connected layers is expanded\nto learn a richer feature representation. That is,\nY = f(ZW1)W2, (2)\nwhere W1 ∈ Rd×γd, W2 ∈ Rγd×d, and f(·) denotes a non-\nlinear activation function. For the sake of simplicity, the bias\nterm is omitted. The dimension expansion ratio γ is usually\nset to 4. As shown in Fig. 2a, the input to the feed-forward\nnetwork is regarded as a sequence of embedding vectors.\nLattice perspective. Since the feed-forward network is\napplied position-wise to a sequence of tokens Z ∈ RN×d, an\nexactly equivalent representation is to rearrange the sequence\nof tokens into a 2D lattice as shown in Fig. 2b. Then the\nreshaped feature representation is\nZr = Seq2Img(Z), Zr ∈ Rh×w×d, (3)\nwhere h = H/p and w = W/p. The operation Seq2Img\nconverts a sequence to a 2D feature map. Each token is\nplaced to a pixel location of the feature map. The benefit\nof this perspective is that the proximity between tokens is\nrecovered, which provides the chance to introduce locality\ninto the network. The fully-connected layers could be re-\nplaced by 1 × 1 convolutions, i.e.\nYr = f(Zr ⊛ Wr\n1) ⊛ Wr\n2, (4)\nY = Img2Seq(Yr), (5)\nwhere Wr\n1 ∈ Rd×γd×1×1 and Wr\n2 ∈ Rγd×d×1×1 are\nreshaped from W1 and W2 and represent the convolutional\nkernels. The operation Img2Seq converts the image feature\nmap back to a token sequence which is used by the next\nself-attention layer.\nB. Locality\nSince only 1×1 convolution is applied to the feature map,\nthere is a lack of information interaction between adjacent\npixels. Besides, the self-attention part of the transformer only\ncaptures global dependencies between all of the tokens. Thus,\nthe transformer block does not have a mechanism to model\nthe local dependencies between nearby pixels. It would be\ninteresting if locality could be brought to transformers in an\nefficient way.\n1 × 1 Conv\n1 × 1 Conv\n1 × 1 Conv\n3 × 3 DW Conv\n1 × 1 Conv\nSeq2Img\n1 × 1 Conv\n3 × 3 DW Conv\n1 × 1 Conv\nImg2Seq\n(a) (b) (c)\nFig. 3: Comparison between the (a) convolutional version\nof the feed-forward network in vision transformers, the (b)\ninverted residual blocks, and (c) the utilized network that\nbrings locality mechanism into transformers. “DW” denotes\ndepth-wise convolution. To cope with the convolution oper-\nation, the conversion between sequence and image feature\nmap is added by “Seq2Img” and “Img2Seq” in (c). Note\nthat after each convolution, there are activation functions.\nIn Table II, we systematically investigate influence of the\nactivation function after the depthwise convolution in (c).\nThe expansion of the hidden dimension between fully-\nconnected layers and the lattice perspective of the feed-\nforward network remind us of the inverted residual block\nproposed in MobileNets [18], [19]. As shown in Fig. 3, both\nof the feed-forward network and the inverted residual expand\nand squeeze the hidden dimension by 1×1 convolution. The\nonly difference is that there is a depth-wise convolution in\nthe inverted residual block. Depth-wise convolution applies\na k × k (k > 1) convolution kernel per channel. The\nfeatures inside the k × k kernel is aggregated to compute\na new feature. Thus, depth-wise convolution is an efficient\nway of introducing locality into the network. Considering\nthat, we reintroduce depth-wise convolution into the feed-\nforward network of transformers. And the computation could\nbe represented as\nYr = f\n\u0000\nf(Zr ⊛ Wr\n1) ⊛ Wd\n\u0001\n⊛ Wr\n2, (6)\nwhere Wd ∈ Rγd×1×k×k is the kernel of the depth-wise\nconvolution. The finally used network is shown in Fig. 3c.\nThe input, i.e. a sequence of tokens is first reshaped to a\nfeature map rearranged on a 2D lattice. Then two 1 × 1\nconvolutions along with a depth-wise convolution are applied\nto the feature map. After that, the feature map is reshaped to\na sequence of tokens which are used as by the self-attention\nof the network transformer layer.\nNote that the non-linear activation functions are not vi-\nsualized in Fig. 3. Yet, they play a quite important role in\nenhancing the network capacity, especially for efficient net-\nworks. In particular, we try ReLU6, h-swish [19], squeeze-\nand-excitation (SE) module [41], efficient channel attention\n(ECA) module [42], and their combinations. A thorough\nanalysis of the activation function is discussed in the ex-\nperiments section.\nC. Class token\nTo apply vision transformers to image classification, a\ntrainable class token is added and inserted into the token\nembedding, i.e.\nˆX ← Concat(Xcls, ˆX), (7)\nwhere ← denotes the assignment operation, Xcls ∈ R1×d\nis the class token. The new matrix has the dimension of\n(N + 1)× d and N + 1 = HW\np2 + 1 tokens. In the self-\nattention module, the class token exchanges information\nwith all other image tokens and gathers information for the\nfinal classification. In the feed-forward network, the same\ntransformation is applied to the class and image tokens.\nWhen depth-wise convolution is introduced into the feed-\nforward network, the sequence of tokens needs to be re-\narranged into an image feature map. Yet, the additional\ndimension brought by the class token makes the exact\nrearrangement impossible. To circumvent this problem, we\nsplit the N + 1 tokens in Eqn. (1) into a class token and\nimage tokens again, i.e.\n(Zcls, Z) ← Split(Z). (8)\nThen the new image token is passed through the feed-forward\nnetwork according to Eqns. (3), (6), and (5), leading to\nY. The class token is not passed through the feed-forward\nnetwork. Instead, it is directly concatenated with Y, i.e.\nY ← Concat(Zcls, Y). (9)\nThe split and concatenation of the class token is done for\nevery layer. Although the class token Zcls is not passed\nthrough the feed-forward network, the performance of the\noverall network is not adversely affected. This is because\nthe information exchange and aggregation is done only in\nthe self-attention part. A feed-forward network like Eqn. (2)\nonly enforces a transformation within each token.\nIV. E XPERIMENTAL RESULTS\nThis section gives the experimental results for image\nclassification. We first study how the locality brought by\ndepth-wise convolution can improve the performance of\ntransformers. Then we investigate the influence of several\ndesign choices including the non-linear activation function,\nthe placement of the locality, and the hidden dimension\nexpansion ratio γ. All those experiments are based on DeiT-\nT [2]. Then, we study the generalization to other vision\ntransformers including T2T-ViT [20], PVT [3], TNT [21],\nSwin transformer [1] for image classification. The transform-\ners that are equipped with locality are denoted as LocalViT\nfollowed by the suffix that denotes the basic architecture.\nA. Implementation details\nWe introduce the locality mechanism into five vision\ntransformers including DeiT [2], Swin transformers [1], T2T-\nViT [20], PVT [3], TNT [21]. Since those transformers\nhave different architectures, slightly different considerations\nshould be made. First of all, a Tokens-to-Token (T2T)\nmodule is designed in T2T-ViT [20] and is inserted into\nthe head of the network. Basically, the T2T module is\nalso a transformer block with feed-forward networks. Thus,\nthe same modification is also applied to the T2T module.\nSecondly, TNT introduced an inner transformer block for the\nimage tokens along with the outer transformer block. Yet, we\nobserved huge increase of GPU memory. Thus, the locality\nmechanism is only applied to the outer transformer block\nof TNT [21]. Thirdly, for PVT [3], the class token is only\nintroduced in the final stage of the pyramid. Thus, the split\nand concatenation of the class token for the feed-forward\nnetwork is only applied in the final stage. Fourthly, there is\nno class token in Swin transformers [1]. The classification\nis done based on an averaged pooled feature map. Thus, the\nspecial treatment of the class token is not needed in our\nmodified Swin transformers.\nFor fast experiment, we shrink TNT and Swin transformers\nand get smaller versions of them. TNT-T is derived by\nreducing the embedding dimension from 384 to 192. Swin-\nM is derived by reducing the number of transformer blocks\nin the third stage from 6 to 2.\nExperimental setup. The ImageNet2012 dataset [43] is\nused in this paper. The dataset contains 1.28M training im-\nages and 50K validation images from one thousand classes.\nWe follow the same training protocol as DeiT [2]. The input\nimage is randomly cropped with size 224 × 224. Cross-\nentropy is used as the loss function. Label smoothing is used.\nThe weight decay factor is set to 0.05. The AdamW optimizer\nis used with a momentum of 0.9. The training continues\nfor 300 epochs. The batch size is set to 1024. The initial\nlearning rate is set to 1 × 10−3 and decreases to 1 × 10−5\nfollowing a cosine learning rate scheduler. During validation,\na center crop of the validation images is conducted. We use\n8 NVIDIA TITAN RTX GPUs to run the experiments.\nB. Influence of the locality\nTABLE I: Investigation of the locality brought by depth-wise\nconvolution. *ReLU6 is used as the activation function after\ndepth-wise convolution. †Results derived by modifying the\nDeiT architecture and training the network with the same\ntraining protocol\nNetwork γ Depthwise\nConv\nParams Top-1\n(M) Acc. (%)\nDeiT-T [2] 4 No 5.7 72.2\nLocalViT-T 4 No 5.7 72.5 (0.3↑)\nLocalViT-T* 4 Yes 5.8 73.7 (1.5↑)\nDeiT-T [2] 6 No 7.5 73.1†\nLocalViT-T 6 No 7.5 74.3 (1.2↑)\nLocalViT-T* 6 Yes 7.7 76.1 (3.0↑)\nWe first study how the local information could help to\nimprove the performance of vision transformers in Table I.\nTABLE II: Investigation of the non-linear activation function.\nThe combination of HS, ECA [42], and SE [41] is studied.\n“HS” means h-swish activation. “SE-**” means the reduction\nratio in the squeeze-and-excitation module. γ is set to 4.\nActivation Params Top-1\n(M) Acc. (%)\nDeit-T [2] 5.7 72.2\nReLU6 5.8 73.7 (1.5↑)\nHS 5.8 74.4 (2.2↑)\nHS + ECA 5.8 74.5 (2.3↑)\nHS + SE-192 5.9 74.8 (2.6↑)\nHS + SE-96 6.0 74.8 (2.6↑)\nHS + SE-48 6.1 75.0 (2.8↑)\nHS + SE-4 9.4 75.8 (3.6↑)\nDifferent hidden dimension expansion ratios γ are investi-\ngated. First of all, due to the change of the operations in\nthe feed-forward network (Sec. IV-A), the Top-1 accuracy\nof LocalViT-T is slightly increased even without the depth-\nwise convolution. The performance gain is 0.3% for γ = 4\nand is increased to 1.2% for γ = 6. Note that compared\nwith DeiT-T, no additional parameters and computation are\nintroduced for the improvement. When locality is incorpo-\nrated into the feed-forward network, there is a significant\nimprovement of the model accuracy, i.e. 1.5% for γ = 4\nand 3.0% for γ = 6. Compared with the baseline, there\nonly is a marginal increase in the number of parameters and\na negligible increase in the amount of computation. Thus,\nthe performance of vision transformers can be significantly\nimproved by the incorporation of a locality mechanism and\nthe adaptation of the operation in the feed-forward network.\nC. Activation functions\nThe non-linear activation function after depth-wise convo-\nlution used in the above experiments is simply ReLU6. The\nbenefit of using other non-linear activation functions is also\nstudied. In Table II, the ablation study based on LocalViT-\nT is done. First of all, by replacing the activation function\nfrom ReLU6 to h-swish, the gain of Top-1 accuracy over\nthe baseline is increased from 1.5% to 2.2%. This shows\nthe benefit of h-swish activation functions can be easily ex-\ntended from CNNs to vision transformers. Next, the h-swish\nactivation function is combined with other channel attention\nmodules including ECA [42] and SE [41]. The ECA and SE\nmodules are placed directly after the h-swish function. By\nadding ECA, the performance is further improved by 0.1%.\nConsidering that only 60 parameters are introduced, this\nimprovement is still considerable under a harsh parameter\nbudget.\nAnother significant improvement is brought by a squeeze-\nand-excitation module. When the reduction ratio in the\nsqueeze-and-excitation module is reduced from 192 to 4, the\ngain of Top-1 accuracy is gradually increased from 2.6% to\n3.6%. The number of parameters is also increased accord-\ningly. Note that, for all of the networks, the computational\ncomplexity is almost the same. This implies that if there is no\nstrict limitation on the number of parameters, advanced non-\nlinear activation functions could be used. In the following\nTABLE III: Influence of the placement of locality. “All”\nmeans all of the transformer layers are enhanced by depth-\nwise convolution. “Low”, “Mid”, and “High” mean the\nlower, middle, and higher transformer layers are equipped\nwith depth-wise convolution, respectively. The study is based\non LocalViT-T.\nLayer Params FLOPs Top-1\nPlacement (M) (G) Acc. (%)\nHigh: 9∼12 5.78 1.26 69.1\nMid: 5∼8 5.78 1.26 72.1\nLow: 1∼4 5.78 1.26 73.1\nLow: 1∼8 5.84 1.27 74.0\nAll: 1∼12 5.91 1.28 74.8\nTABLE IV: Investigating the expansion ratio of hidden layers\nin the feed-forward network.\nγ SE Params FLOPs Top-1\n(M) (G) Acc. (%)\n1 No 3.1 0.7 65.9\nYes 3.1 0.7 66.2\n2 No 4.0 0.9 70.1\nYes 4.0 0.9 70.6\n3 No 4.9 1.1 72.9\nYes 5.0 1.1 73.1\n4 No 5.8 1.3 74.4\nYes 5.9 1.3 74.8\nexperiments, we use the combination of h-swish and SE\nas the non-linear activation function after depth-wise con-\nvolution. Additionally, the reduction ratio of the squeeze-\nand-excitation module is chosen such that only 4 channels\nare kept after the squeeze operation. This choice of design\nachieves a good balance between the number of parameters\nand the model accuracy. Thus, local information is also\nimportant in vision transformers. A wide range of efficient\nmodules could be introduced into the feed-forward network\nof vision transformers to expand the network capacity.\nD. Placement of locality, expansion ratio, and discussion\nThe transformer layers where the locality is introduced\ncan also influence the performance of the network. Thus, an\nablation study based on LocalViT-T is conducted to study\ntheir effect. The results is reported in Table III. There are\nin total 12 transformer layers in the network. We divide\nthe 12 layers into 3 groups corresponding to “Low”, “Mid”,\nand “High” stages. For the former 3 rows of Table III, we\nindependently insert locality into the three stages. As the\nlocality is moved gradually from lower stages to the higher\nstages, the accuracy of the network is decreased. This shows\nthat local information is especially important for the lower\nlayers. This is also consistent with our intuition. When the\ndepth-wise convolution is applied to the lower layers, the\nlocal information aggregated there could also be propagated\nto the higher layers, which is important to improve the overall\nperformance of the network.\nWhen the locality is introduced only in the higher stage,\nthe Top-1 accuracy is even lower than DeiT-T. To investigate\nwhether locality in the higher layers always has an adverse\neffect, we progressively allow more lower layers to have\nTABLE V: Image classification results for different CNNs\nand vision transformers. The locality functionality is enabled\nfor five different vision transformers\nNetwork Params\n(M)\nFLOPs\n(G)\nTop-1\nAcc. (%)\nTop-5\nAcc. (%)\nCNNs\nResNet-18 [31] 11.7 1.8 69.8 89.1\nResNet-50 [31] 25.6 4.1 76.1 92.9\nDenseNet-169 [44] 14.2 3.4 75.6 92.8\nRegNet-4GF [45] 20.7 4.0 80.0 –\nMobileNetV1 [38] 4.2 0.6 70.6 –\nMobileNetV2 [18] 6.9 0.6 74.7 –\nEfficientNet-B0 [40] 5.3 0.4 77.1 93.3\nTransformers\nDeiT-T [2] 5.7 1.3 72.2 91.1\nLocalViT-T 5.9 1.3 74.8 (2.6↑) 92.6\nDeiT-T\n⚗ [2] 5.9 1.3 74.5 –\nDeiT-S [2] 22.1 4.6 79.8 95.1\nLocalViT-S 22.4 4.6 80.8 (1.0↑) 95.4\nDeiT-S\n⚗ [2] 22.4 4.6 81.2 –\nT2T-ViT-7 [20] 4.3 1.2 71.7 –\nLocalViT-T2T 4.3 1.2 72.5 (0.8↑) –\nTNT-T [21] 6.1 1.4 73.6 91.9\nLocalViT-TNT 6.3 1.4 75.9 (2.3↑) 93.0\nPVT-T [3] 13.2 4.7 75.1 92.3\nLocalViT-PVT 13.5 4.8 78.2 (3.1↑) 94.2\nSwin-M [1] 21.2 3.0 79.2 94.5\nLocalViT-Swin-M 21.7 3.0 80.4 (1.2↑) 95.0\nSwin-T [1] 28.3 4.5 80.9 95.3\nLocalViT-Swin 29.1 4.5 81.9 (1.0↑) 95.7\ndepth-wise convolution until locality is enabled for all layers.\nThis corresponds to the last three rows of Table III. Starting\nfrom the lower layers, the performance of the network could\nbe gradually improved as locality is enabled for more layers.\nThus, introducing the locality to the lower layers is more\nadvantageous compared with higher layers.\nThe effect of the expansion ratio of the hidden dimension\nof the feed-forward network is also investigated. The results\nare shown in Table IV. Expanding the hidden dimension of\nthe feed-forward network can have a significant effect on the\nperformance of the transformers. As the expansion ratio is\nincreased from 1 to 4, the Top-1 accuracy is increased from\nless than 70% to nearly 75%. The model complexity is also\nalmost doubled. Thus, the network performance and model\ncomplexity can be balanced by the hidden dimension expan-\nsion ratio γ. Squeeze-and-excitation can be more beneficial\nfor smaller γ.\nE. Generalization and comparison with state-of-the-art\nFinally, we try to incorporate locality into vision trans-\nformers including DeiT [2], Swin transformers [1], T2T-\nViT [20], TNT [21], PVT [3] and compare their performance\nwith CNNs. We draw three major conclusions from Table V.\nFirstly, the effectiveness of locality can be generalized to a\nwide range of vision transformers based on the following\nobservations. 1) Compared with DeiT, LocalViT can yield\na higher classification accuracy for both the tiny and small\nversion of the network. The increase of Top-1 accuracy is\n2.6% and 1.0%, resp. LocalViT-T even outperforms DeiT-T\n⚗\nwhich is enhanced by knowledge distillation from RegNetY-\n160 [45]. The small version LocalViT-S is slightly worse\n(a) DeiT-S [2] vs. LocalViT-S.\n (b) Swin-T [1] vs. LocalViT-Swin.\n (c) PVT-T [3] vs. LocalViT-PVT.\n (d) TNT-T [21] vs. LocalViT-TNT.\nFig. 4: Comparison of Top-1 and Top-5 accuracy between the baseline transformers and the locality enhanced LocalViT\n(a) Input.\n (b) Pooling.\n (c) DeiT\n (d) LocalViT\nFig. 5: Comparison of feature maps. (a) Random sampled\ninput images from ImageNet (b) Max pooled images with\nkernel size 16 × 16. (c) & (d) Feature map of the last\ntransformer layer from DeiT and LocalViT\nthan DeiT-S\n⚗ by 0.4%. 2) LocalViT-T2T outperforms T2T-\nViT-7 by 0.8%. Note that T2T-ViT already tries to model the\nlocal structure information in the tokens-to-token module. 3)\nIn TNT, an additional transformer block is used to extract\nlocal features for the image tokens. Thus, the locality is also\nconsidered in TNT. The modified network, i.e. LocalViT-\nTNT could still improve the classification accuracy by a\nlarge margin of 2.3%. 4) The biggest improvement comes\nfrom PVT. Introducing the locality module leads to a gain\nof 3.1% over PVT-T. 5) Swin transformer already adopts\nshifted windows that constrain attention in a local region.\nYet, adding locality processing module into the network\ncould still improve the performance of Swin transformers.\n6) The comparison of the training log between the baseline\ntransformers and LocalViT is shown in Fig. 4. As shown\nin Fig. 4, during the training phase, LocalViT outperforms\nthe baseline transformers consistently in terms of both Top-\n1 and Top-5 accuracy. The gap between LocalViT and the\nbaseline transformers is more obvious in the early training\nphase. For example, the gap of the Top-1 accuracy between\nLocalViT-T and DeiT-T could be as large as 10% during\nthe early training phase (at about Epoch 25). Thus, this\nconfirms that the locality mechanism introduced by LocalViT\ncan enlarge the capacity of vision transformers and lead to\nbetter performances.\nSecondly, some versions of the enhanced vision trans-\nformer LocalViT are already quite comparable or even out-\nperform CNNs. This conclusion can be drawn by making the\npairwise comparison, i.e. LocalViT-Tvs. MobileNetV2 (1.4),\nLocalViT-S vs. ResNet-50, LocalViT-T2T vs. MobileNetV1,\nLocalViT-PVT vs. DenseNet-169 etc.\nThirdly, by comparing the feature maps of transformers\nwith and without locality mechanism in Fig. 5c and Fig. 5d,\nit is clear that LocalViT does a better job at localizing the\nobjects in the presented input images.\nDiscussion of limitation. As shown in Table V, intro-\nducing locality mechanism increases the complexity of the\nnetwork. As a result, the inference of the network could be\nslowed down. We report the throughput of different methods\non one NVIDIA TITAN Xp GPU in Table VI. When com-\nparing with the DeiT-T and TNT-T, there is only a marginal\ndecrease (less than 10%) of throughput for LocalViT. The\nthroughput of LocalViT-T and DeiT-T is almost comparable.\nFor PVT-T and Swin-T, the locality enhanced transformer\nLocalViT is faced with a larger throughput decrease. But\nthe the decrease is still within 24%. Yet, considering the\nnon-trivial improvement of the locality enhanced transformer\nnetwork, we think this is acceptable. The value of this\npaper is to show the importance of locality mechanism\nin vision transformers rather than achieving state-of-the-art\nperformances. And we believe that the conclusion derived\nunder rigorous and controlled experiments could help the\ncommunity to understand the locality mechanism in vision\ntransformers. Combination with other design choices towards\nmore efficient networks could be done in follow-up works.\nTABLE VI: Throughput comparison between the baseline\nnetworks and those enhanced by the locality mechanism. The\ninference is conducted on a single NVIDIA TITAN Xp GPU\nNetwork Throughput\nimages/s Network Throughput\nimages/s\nDeiT-T 346.3 PVT-T 318.4\nLocalViT-T 336.9 (2.7% ↓) LocalViT-PVT 248.3 (22.0% ↓)\nTNT-T 222.0 Swin-T 206.3\nLocalViT-TNT 208.3 (6.2% ↓) LocalViT-Swin 158.2 (23.3% ↓)\nV. C ONCLUSION\nIn this paper, we investigated the influence of locality\nmechanism in the feed-forward of vision transformers. We\nintroduced the locality mechanism into vision transformers\nby incorporating 2D depth-wise convolutions followed by a\nnon-linear activation function into the feed-forward network\nof vision transformers. To cope with the locality mechanism,\nthe sequence of tokens embedding is rearranged into a lattice\nas a 2D feature map, which is used as the input to the en-\nhanced feed-forward network. To enable the rearrangement,\nthe class token is split before the feed-forward network and\nconcatenated with other image embeddings after the feed-\nforward network. A series of studies were made to investigate\nvarious factors (activation function, layer placement, and\nexpansion ratio) that might influence of performance of the\nlocality mechanism. The proposed locality mechanism is\nsuccessfully applied to five different vision transformers,\nwhich validates its generality.\nREFERENCES\n[1] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using\nshifted windows,” arXiv preprint arXiv:2103.14030 , 2021.\n[2] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efficient image transformers & distillation\nthrough attention,” arXiv preprint arXiv:2012.12877 , 2020.\n[3] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu,\nP. Luo, and L. Shao, “Pyramid vision transformer: A versatile\nbackbone for dense prediction without convolutions,” arXiv preprint\narXiv:2102.12122, 2021.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”arXiv\npreprint arXiv:1706.03762, 2017.\n[5] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[6] F. Yu and V . Koltun, “Multi-scale context aggregation by dilated\nconvolutions,” arXiv preprint arXiv:1511.07122 , 2015.\n[7] H. Wang, Z. Wu, Z. Liu, H. Cai, L. Zhu, C. Gan, and S. Han,\n“Hat: Hardware-aware transformers for efficient natural language\nprocessing,” arXiv preprint arXiv:2005.14187 , 2020.\n[8] S. Mehta, M. Ghazvininejad, S. Iyer, L. Zettlemoyer, and H. Hajishirzi,\n“Delight: Very deep and light-weight transformer,” arXiv preprint\narXiv:2008.00623, 2020.\n[9] Z. Wu, Z. Liu, J. Lin, Y . Lin, and S. Han, “Lite transformer with\nlong-short range attention,” arXiv preprint arXiv:2004.11886 , 2020.\n[10] Y . Li, Y . Fan, X. Xiang, D. Demandolx, R. Ranjan, R. Timofte, and\nL. Van Gool, “Efficient and explicit modelling of image hierarchies\nfor image restoration,” in Proc. CVPR, 2023, pp. 18 278–18 289.\n[11] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nProc. ECCV. Springer, 2020, pp. 213–229.\n[12] J. Guo, K. Han, H. Wu, C. Xu, Y . Tang, C. Xu, and Y . Wang,\n“Cmt: Convolutional neural networks meet vision transformers,” arXiv\npreprint arXiv:2107.06263, 2021.\n[13] Y . Liu, G. Sun, Y . Qiu, L. Zhang, A. Chhatkuli, and L. Van Gool,\n“Transformer in convolutional neural networks,” arXiv preprint\narXiv:2106.03180, 2021.\n[14] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu, “Incorpo-\nrating convolution designs into visual transformers,” arXiv preprint\narXiv:2103.11816, 2021.\n[15] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani,\n“Bottleneck transformers for visual recognition,” arXiv preprint\narXiv:2101.11605, 2021.\n[16] C. Li, T. Tang, G. Wang, J. Peng, B. Wang, X. Liang, and\nX. Chang, “Bossnas: Exploring hybrid cnn-transformers with block-\nwisely self-supervised neural architecture search,” arXiv preprint\narXiv:2103.12424, 2021.\n[17] S. d’Ascoli, H. Touvron, M. Leavitt, A. Morcos, G. Biroli, and L. Sa-\ngun, “Convit: Improving vision transformers with soft convolutional\ninductive biases,” arXiv preprint arXiv:2103.10697 , 2021.\n[18] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n“MobileNetV2: Inverted residuals and linear bottlenecks,” in Proc.\nCVPR, 2018, pp. 4510–4520.\n[19] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan,\nW. Wang, Y . Zhu, R. Pang, V . Vasudevan et al. , “Searching for\nmobilenetv3,” in Proc. ICCV, 2019, pp. 1314–1324.\n[20] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng,\nand S. Yan, “Tokens-to-token vit: Training vision transformers from\nscratch on imagenet,” arXiv preprint arXiv:2101.11986 , 2021.\n[21] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, “Transformer\nin transformer,” arXiv preprint arXiv:2103.00112 , 2021.\n[22] M. Saleh, Y . Wang, N. Navab, B. Busam, and F. Tombari, “Clou-\ndattention: Efficient multi-scale attention scheme for 3d point cloud\nlearning,” in Proc. IROS. IEEE, 2022, pp. 1986–1992.\n[23] A. Brohan, N. Brown, J. Carbajal, Y . Chebotar, J. Dabis, C. Finn,\nK. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al. , “Rt-1:\nRobotics transformer for real-world control at scale,” arXiv preprint\narXiv:2212.06817, 2022.\n[24] C. Shi, Y . Zheng, and A. M. Fey, “Recognition and prediction of\nsurgical gestures and trajectories using transformer models in robot-\nassisted surgery,” in Proc. IROS. IEEE, 2022, pp. 8017–8024.\n[25] S. Lee, E. Yi, J. Lee, J. Yoo, H. Lee, and S. H. Kim, “Fully\nconvolutional transformer with local-global attention,” in Proc. IROS.\nIEEE, 2022, pp. 552–559.\n[26] E. V . Mascaro, S. Ma, H. Ahn, and D. Lee, “Robust human motion\nforecasting using transformer-based model,” in Proc. IROS. IEEE,\n2022, pp. 10 674–10 680.\n[27] A. Bucker, L. Figueredo, S. Haddadinl, A. Kapoor, S. Ma, and R. Bon-\natti, “Reshaping robot trajectories using natural language commands:\nA study of multi-modal data alignment using transformers,” in Proc.\nIROS. IEEE, 2022, pp. 978–984.\n[28] Y . LeCun, Y . Bengio et al. , “Convolutional networks for images,\nspeech, and time series,” The handbook of brain theory and neural\nnetworks, vol. 3361, no. 10, p. 1995, 1995.\n[29] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\nwith deep convolutional neural networks,” in Proc. NeurIPS, 2012, pp.\n1097–1105.\n[30] K. Simonyan and A. Zisserman, “Very deep convolutional networks\nfor large-scale image recognition,” arXiv preprint arXiv:1409.1556 ,\n2014.\n[31] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. CVPR, 2016, pp. 770–778.\n[32] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural\nnetworks,” in Proc. CVPR, 2018, pp. 7794–7803.\n[33] D. Liu, B. Wen, Y . Fan, C. C. Loy, and T. S. Huang, “Non-local recur-\nrent network for image restoration,” arXiv preprint arXiv:1806.02919,\n2018.\n[34] J. Li, Y . Yan, S. Liao, X. Yang, and L. Shao, “Local-to-global self-\nattention in vision transformers,” arXiv preprint arXiv:2107.04735 ,\n2021.\n[35] W. Wang, L. Yao, L. Chen, D. Cai, X. He, and W. Liu, “Crossformer:\nA versatile vision transformer based on cross-scale attention,” arXiv\ne-prints, pp. arXiv–2108, 2021.\n[36] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and\nC. Shen, “Twins: Revisiting the design of spatial attention in vision\ntransformers,” Advances in Neural Information Processing Systems ,\nvol. 34, 2021.\n[37] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao,\n“Multi-scale vision longformer: A new vision transformer for high-\nresolution image encoding,” in Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , 2021, pp. 2998–3008.\n[38] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “MobileNets: Efficient\nconvolutional neural networks for mobile vision applications,” arXiv\npreprint arXiv:1704.04861, 2017.\n[39] M. Tan, B. Chen, R. Pang, V . Vasudevan, M. Sandler, A. Howard,\nand Q. V . Le, “Mnasnet: Platform-aware neural architecture search\nfor mobile,” in Proc. CVPR, 2019, pp. 2820–2828.\n[40] M. Tan and Q. V . Le, “Efficientnet: Rethinking model scaling for\nconvolutional neural networks,” arXiv preprint arXiv:1905.11946 ,\n2019.\n[41] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nProc. CVPR, 2018, pp. 7132–7141.\n[42] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “Eca-net: Efficient\nchannel attention for deep convolutional neural networks,” in Proc.\nCVPR, 2020.\n[43] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Im-\nageNet: A large-scale hierarchical image database,” in Proc. CVPR ,\n2009, pp. 248–255.\n[44] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proc. CVPR, 2017.\n[45] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Doll ´ar,\n“Designing network design spaces,” in Proc. CVPR, 2020, pp. 10 428–\n10 436."
}