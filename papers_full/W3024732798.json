{
    "title": "Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition",
    "url": "https://openalex.org/W3024732798",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3192015700",
            "name": "Tian, Zhengkun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746493293",
            "name": "Yi, Jiangyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1978954940",
            "name": "TAO Jianhua",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105167896",
            "name": "Bai Ye",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1924112616",
            "name": "Zhang Shuai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2493978785",
            "name": "Wen, Zhengqi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2951655021",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2936123380",
        "https://openalex.org/W2976556660",
        "https://openalex.org/W2911291251",
        "https://openalex.org/W2972034672",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2939111082",
        "https://openalex.org/W2132083787",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W854541894",
        "https://openalex.org/W3014413043",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2989134874",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W2526425061",
        "https://openalex.org/W3015671919",
        "https://openalex.org/W2514741789",
        "https://openalex.org/W2127141656"
    ],
    "abstract": "Non-autoregressive transformer models have achieved extremely fast inference speed and comparable performance with autoregressive sequence-to-sequence models in neural machine translation. Most of the non-autoregressive transformers decode the target sequence from a predefined-length mask sequence. If the predefined length is too long, it will cause a lot of redundant calculations. If the predefined length is shorter than the length of the target sequence, it will hurt the performance of the model. To address this problem and improve the inference speed, we propose a spike-triggered non-autoregressive transformer model for end-to-end speech recognition, which introduces a CTC module to predict the length of the target sequence and accelerate the convergence. All the experiments are conducted on a public Chinese mandarin dataset AISHELL-1. The results show that the proposed model can accurately predict the length of the target sequence and achieve a competitive performance with the advanced transformers. What's more, the model even achieves a real-time factor of 0.0056, which exceeds all mainstream speech recognition models.",
    "full_text": "Spike-Triggered Non-Autoregressive Transformer for\nEnd-to-End Speech Recognition\nZhengkun Tian1,2, Jiangyan Yi1, Jianhua Tao1,2,3, Ye Bai1,2, Shuai Zhang1,2, Zhengqi Wen1\n1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3 CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China\n{zhengkun.tian, jiangyan.yi, jhtao, ye.bai, shuai.zhang, zqwen}@nlpr.ia.ac.cn\nAbstract\nNon-autoregressive transformer models have achieved ex-\ntremely fast inference speed and comparable performance with\nautoregressive sequence-to-sequence models in neural machine\ntranslation. Most of the non-autoregressive transformers decode\nthe target sequence from a predeﬁned-length mask sequence. If\nthe predeﬁned length is too long, it will cause a lot of redun-\ndant calculations. If the predeﬁned length is shorter than the\nlength of the target sequence, it will hurt the performance of\nthe model. To address this problem and improve the inference\nspeed, we propose a spike-triggered non-autoregressive trans-\nformer model for end-to-end speech recognition, which intro-\nduces a CTC module to predict the length of the target sequence\nand accelerate the convergence. All the experiments are con-\nducted on a public Chinese mandarin dataset AISHELL-1. The\nresults show that the proposed model can accurately predict the\nlength of the target sequence and achieve a competitive perfor-\nmance with the advanced transformers. What’s more, the model\neven achieves a real-time factor of 0.0056, which exceeds all\nmainstream speech recognition models.\nIndex Terms: CTC module, spike triggered non-autoregressive\ntransformer, end-to-end speech recognition\n1. Introduction\nAlthough autoregressive models have achieved great success in\nvarious NLP tasks and speech recognition[1, 2, 3, 4, 5, 6], the\nautoregressive characteristics result in a large latency during the\ninference process [7]. Most of the attention-based sequence-\nto-sequence models generate the target sequence in an autore-\ngressive fashion. These models predict the next token condi-\ntioned on the previously generated tokens and the source state\nsequence. By contrast, the non-autoregressive model gets rid of\ntemporal dependency and able to perform parallel computing,\ngreatly improving the speed of inference.\nNon-autoregressive transformers (NAT) have achieved\ncomparable results with autoregressive models in neural ma-\nchine translation and speech recognition [7, 8, 9, 10, 11, 12, 13].\nDifferent from the autoregressive sequence-to-sequence model,\nthe NAT takes a ﬁxed-length mask sequence as input to predict\ntarget sequence. The setting of this predeﬁned length is very im-\nportant. If the length is shorter than the actual length, this will\ncause many errors of deletion. On the contrary, a longer length\nwill cause the model to generate duplicate tokens and consume\nadditional calculations. To our best knowledge, there are three\nways to estimate the length of the target sequence. Firstly, some\nworks introduce a neural network module behind the encoder to\npredict the target length [7, 8, 9]. These method cannot guar-\nantee the accuracy of the predicted lengths. During inference,\nit is necessary to sample different lengths to select the optimal\nsequence. Secondly, [10, 11] set an empirical(or maximum)\nlength based on the length of the source sequence. To guarantee\nthe performance of the model, the length is often much longer\nthan the actual length of the target sequence. It will result in ex-\ntra calculation cost and affect the inference speed. Thirdly, [12]\nutilizes the CTC loss function instead of the cross entropy to op-\ntimize the model, which makes the model generate tokens with-\nout calculating the length of the target sequence. However, the\ncharacteristics of CTC will cause the model to generate some\nduplicate tokens and a large number of blanks during inference,\nand it does not accelerate the inference speed.\nFor speech recognition, the number of valid characters or\nwords contained in a piece of speech is affected by various fac-\ntors such as the speaker’s speech rate, silence, and noise. It is\nunreasonable to set a ﬁxed length only according to the duration\nof the audio. To estimate the length of the target sequence accu-\nrately and accelerate the inference speech, we propose a spike-\ntriggered non-autoregressive transformer (ST-NAT) for end-to-\nend speech recognition, which introduces a CTC module to pre-\ndict the length of the target sequence and accelerate the con-\nvergence. The CTC loss plays three important roles in our\nproposed model. Firstly, ST-NAT utilizes the CTC module to\npredict the length of target sequences. The CTC module can\ngenerate spike-like label posterior probabilities. The number\nof spikes accurately reﬂects the length of the target sequence\n[9, 14]. During inference, the ST-NAT can count the number\nof spikes to avoid redundant calculations. Secondly, the ST-\nNAT adopts the encode states corresponding to the positions of\nspikes as the input of the decoder. We assume that the triggered\nencode state sequence can obtain more prior information than\nthe mask sequence, which may able to improve the performance\nof the model. Thirdly, the ST-NAT adapts the CTC loss as an\nauxiliary loss to speed up training and convergence [5]. Ad-\nditionally, a non-autoregressive transformer cannot model the\ninter-dependencies between the outputs. Therefore, we improve\nthe model performance by integrating the output probabilities\npredicted by the ST-NAT and a neural language model. All ex-\nperiments are conducted on a public Chinese mandarin dataset\nAISHELL-1. The results show that the ST-NAT can predict\nthe length of the target sequence accurately and achieve com-\nparable performance with the most advanced end-to-end mod-\nels. The probability of missing words or characters is less than\n2%. What’s more, the model even achieves a real-time factor\n(RTF) of 0.0056, which exceeds all mainstream speech recog-\nnition models.\nThe remainder of this paper is organized as follows. Section\n2 describes our proposed triggered non-autoregressive trans-\nformers. Section 3 presents our experimental setup and results.\nThe conclusions and future work will be given in Section 4.\narXiv:2005.07903v1  [eess.AS]  16 May 2020\n2. Spike-Triggered Non-Autoregressive\nTransformer\n2.1. Model Architecture\nThe spike-trigger non-autoregressive transformer consists of an\nencoder, a decoder, and a CTC module, as depicted in Fig.1.\nBoth encoder and decoder are composed of multi-head atten-\ntion layers and feed-forward layers [4], which is similar to the\nspeech transformer [6].\nAs shown in Fig.1, we put a 2D convolution front end at the\nbottom of the encoder to process the input speech feature se-\nquences simply, including dimension transformation (from 40\nto 320), time-axis down-sampling, and adding sine-cosine posi-\ntional information.\nMulti-head attention (MHA) layer allows the model to fo-\ncus on the information from different positions. Each head hi\nis a complete self-attention component. Q, K and V represent\nqueries, keys and values respectively. dk is the dimension of\nkeys. WQ ∈Rdm×dq , WK ∈Rdm×dk , WV ∈Rdm×dv and\nWO ∈Rdm×dm are projection parameter matrices.\nSelfAttn(Q,K,V ) =softmax(QKT\n√dk\n)V (1)\nMultiHead(Q,K,V ) =Concat(h1,h2,...hnh )WO\nwhere hi =SelfAttn(QWQ\ni ,KW K\ni ,VW V\ni )\n(2)\nFeed-forward network (FFN) contains two linear layers and\na gated liner unit (GLU) [15] activation function [16, 17].\nFFN (x) =GLU(xW1 + b1)W2 + b2 (3)\nwhere parameters W1 ∈Rdm×2dff , W2 ∈Rdff ×dm , b1 ∈\nRdm and b2 ∈Rdm are learnable.\nThe sine and consine positional embedding proposed by [4]\nare applied for all the experiments in this paper. Besides, the\nmodel also apply residual connection and layer normalization.\nThe ST-NAT introduces a CTC module to predict the length\nof the target sequence and accelerate the convergence. The\nCTC module only consists of a linear project layer. Most\nnon-autoregressive transformer model adopts a ﬁxed-length se-\nquence ﬁlled with ’⟨MASK⟩’ as the input of the decoder. These\nsequences don’t contain any useful information. In fact, the\nCTC spike is usually located in the range of one speciﬁc word.\nTherefore, the ST-NAT utilizes the encoded states correspond-\ning to the CTC spike as the input of the decoder. We assume that\nthe triggered encode state sequence contains some prior infor-\nmation on the target words, which makes the decoding process\nmore purposeful than guessing from the empty sequence.\n2.2. Training\nIt is very important to predict the length of the target sequence\naccurately. When the predicted length T′ is shorter than the\ntarget length T, there is no doubt that the generated sequence\nwill miss many words or characters, which means that it causes\nmany deletion errors. Instead, the predicted length T′is longer\nthan the target length of T, it will cost extra calculation and\neven generate many duplicate tokens. The ST-NAT can pre-\ndict the length of the target sequence accurately, through counts\nthe number of spikes produced by the CTC module. When the\nprobability that the CTC module generates a non-blank token is\ngreater than the trigger threshold β, the corresponding trigger\nposition is recorded. This process can be described as follows.\nPOS(i) =\n{\ntriggerd, 1 −pb ≥β\nignored, 1 −pb <β (4)\n2D Conv Layer\nMulti-Head\nSelf-Attention\nFeed Forward \nCTC Project Layer\nMulti-Head\nSelf-Attention\nFeed Forward\nMulti-Head\nAttention\nProject Layer\nCTC Loss CE Loss\n+Positional\nEncoding\nAdd & Norm\nAdd & Norm\nAdd & Norm\nAdd & Norm\nAdd & Norm\n+\nSpike-like Trigger \nProbabilities\nPositional\nEncoding\nJoint Training\nα 1-α\nEncoded States\nEncoder\nDecoder\n6x\n6x\nExtract\nBy\nSpikes2x\nInput Feature Sequences\nTrigger Threshold β\nFigure 1: The spike-triggered non-autoregressive transformer\nhas three components, an encoder, a decoder, and a CTC mod-\nule. The encoder processes the input feature sequences into\nencoded states sequence. The CTC module computes spike-\nlike posteriors from encoded states. And then the decoder ex-\ntract encoded states sequence from the corresponding position\nof spikes as the input. The whole system is trained jointly.\nWhere POS(i) means the i-th position of the encoder output\nstates. pb is the blank probability predicted by the CTC module.\nThen the probability of non-blank can be expressed as 1 −pb.\nThe ST-NAT also inserts an end-of-sentence token ’⟨EOS⟩’ into\nthe target sequence to guarantee the model still able to generate\na correct sequence, when the predicted length T′is larger than\nthe target length T.\nFurthermore, it has been widely proved that the CTC loss\nfunction [18] is effective to assist the model to accelerate the\ntraining and convergence [5]. It is difﬁcult to train a non-\nautoregressive model from scratch. Therefore, we use the CTC\nloss as an auxiliary loss function to optimize the model.\nL=\n{\nαLCTC + (1−α)LCE , T ′≥T\nLCTC , T ′<T (5)\nwhere LCE is the cross entropy loss [19] andLCTC is the CTC\nloss. αis the weight of CTC in joint loss function. T′is the\npredicted target length. T is the real target length. If T′ is\nsmaller than T, the ST-NAT only utilizes the CTC loss optimize\nthe encoder. Thanks to the CTC module, the ST-NAT can be\ntrained from scratch and without any pre-training or other tricks.\n2.3. Inference\nDuring inference, we just select the token which has the highest\nprobability at each position. Generating the token ’ ⟨EOS⟩’ or\nthe last word in the sequence means the end of the decoding\nprocess.\nNon-autoregressive model cannot model the temporal de-\npendencies between the output labels. This largely prevents\nthe improvement of model performance. We also intro-\nduce a transformer-based language model into decoding pro-\ncess. Neural language model makes up the weakness of non-\nautoregressive model. The joint decoding process can be de-\nscribed as\nˆy= arg max\ny\n(logP(y|x) +λlogPLM (y|x)) (6)\nwhere ˆy is the predict sequence. And PLM (y|x) is the proba-\nbility of language model. λis the weight of the language model\nprobabilities.\n3. Experiments and Results\n3.1. Dataset\nIn this work, all experiments are conducted on a public Man-\ndarin speech corpus AISHELL-1 1. The training set contains\nabout 150 hours of speech (120,098 utterances) recorded by 340\nspeakers. The development set contains about 20 hours (14,326\nutterances) recorded by 40 speakers. And about 10 hours (7,176\nutterances / 36109 seconds) of speech is used as test set. The\nspeakers of different sets are not overlapped.\n3.2. Experiment Setup\nFor all experiments, we use 40-dimensional FBANK features\ncomputed on a 25ms window with a 10ms shift. We chose 4233\ncharacters (including a padding symbol ’⟨PAD⟩’ , an unknown\nsymbol ’⟨UNK⟩’ and an end-of-sentence symbol ’ ⟨EOS⟩’) as\nmodel units.\nOur proposed model and baseline models are built on Open-\nTransformer2. The ST-NAT model consists of 6 encoder blocks\nand 6 decoder blocks. There are 4 heads in multi-head atten-\ntion. The 2D convolution front end utilizes two-layer time-axis\nCNN with ReLU activation, stride size 2, channels 320, and ker-\nnel size 3. The output size of the multi-head attention and the\nfeed-forward layers are 320. We adopt an Adam optimizer with\nwarmup steps 12000 and the learning rate scheduler reported in\n[4]. After 80 epochs, we average the parameters saved in the\nlast 20 epochs. We also use the time mask and frequency mask\nmethod proposed in [20] for the baseline transformer, SAN-\nCTC, and all non-autoregressive models. During inference, we\nuse a beam search with a width of 5 for the baseline Transformer\nmodel, SAN-CTC model and the ST-NAT with language model.\nWe use the character error rate (CER) to evaluate the perfor-\nmance of different models. For evaluating the inference speed\nof different models, we decode utterances one by one to com-\npute real-time factor (RTF) on the test set. The RTF is the time\ntaken to decode one second of speech. All experiments are con-\nducted on a GeForce GTX TITAN X 12G GPU.\n3.3. Results\n3.3.1. Explore the effects of different weights and trigger\nthresholds.\nWe train the ST-NAT model with different CTC weights αand\ntrigger thresholds βfrom scratch. As shown in Table.1, the trig-\nger NAT model with CTC weight 0.7 and trigger threshold 0.3\ncan achieve a CER of 7.66% on test. At the same threshold, the\ntrigger NAT with weight 0.6 can achieve the best performance\n1http://www.openslr.org/13/\n2https://github.com/ZhengkunTian/OpenTransformer\nTable 1: Comparison of the model with different CTC weights\nαand trigger thresholds β. We evaluate the CER(%) on devel-\nopment and test set, respectively.\nCTC Trigger Thresholdβ\nWeight α 0.1 0.3 0.5 0.7\n0.1 7.67/8.66 7.56/8.50 7.66/8/45 7.56/8.50\n0.3 7.37/8.14 7.25/8.12 7.26/8.19 7.21/8.01\n0.5 7.06/7.97 7.10/7.88 7.38/8.14 7.30/8.12\n0.6 7.06/7.88 6.88/7.67 7.05/7.77 7.01/7.70\n0.7 7.26/8.05 6.91/7.66 7.03/7.87 7.39/8.02\nTable 2: Comparison of the effects of different trigger threshold\non the inference speed. We record the time that the ST-NAT\nspends on decoding test set and calculate the real-time factor.\nThreshold β 0.1 0.3 0.5 0.7\nPerformance 7.88 7.67 7.77 7.70\nSeconds 212.04 202.59 200.62 198.44\nRTF 0.0059 0.0056 0.0055 0.0054\non development set. The CTC weights and trigger thresholds\naffect the performance of the model in different aspects. The\nCTC weights α are used to balance the performance of CTC\ntrigger module and decoder. However, the trigger threshold β\nare used to determine how many encoder states are triggered.\nBoth weights and thresholds play important roles in the perfor-\nmance of the models.\n3.3.2. Explore the effects of different trigger thresholds on the\ninference speed.\nWe evaluate our ST-NAT with different trigger thresholds on the\ninference speed. All the ST-NAT models are trained with a CTC\nweight of 0.6. It is obvious from the Table.2 that the larger the\nthreshold, the faster the model decode an utterance. When the\ntrigger threshold is 0.7, the model achieves an RTF of 0.0054. It\nalso means the model only has a latency of nearly 20 millisec-\nonds. However, a large threshold does not mean that the model\ncan achieve the best performance. A large trigger threshold\nmight cause the predicted length generated by the CTC trigger\nto be shorter than the target length, which in turn will hurt the\nperformance of the model. Fortunately, different trigger thresh-\nolds have only little effect on the speed of inference, which can\neven be ignored.\n3.3.3. Analysis on trigger mechanism.\nWe analyze the trigger non-autoregressive transformer from the\nfollowing two perspectives.\nOn the one hand, we explore the relationship between the\npredicted length by the model and the target length, as show in\nFig.2. The histogram record the difference between the target\nlength and the predicted length. When the value is less than or\nequal to zero, it means that the predicted length is less than or\nequal to the target length. This will not cause irreversible ef-\nfects. The decoder is still able to predict a token at the end of\nsentence. We can ﬁnd that the vast majority of predicted length\nhave no any errors. What’s more, the probability of missing\nwords or characters is even less than 2%. Even for the most of\nweights (0.3, 0.5 and 0.7), the maximum predict error does not\nexceed 4. Therefore, we conclude that the CTC model can pre-\n0.1 0.3 0.5 0.7\n-7 2\n-6 4\n-5 20\n-4 77 7 2 1\n-3 227 33 16 30\n-2 792 194 175 255\n-1 2357 1459 1583 2184\n0 3640 5390 5262 4569\n1 55 88 132 129\n2 2 5 4 7\n3 2 1\n7176 7176 7176 7176\n57 93 138 137\n7119 7083 7038 7039\n0.992057 0.98704 0.980769 0.980909\n2 4\n0\n1000\n2000\n3000\n4000\n5000\n6000\n-7\n-6\n2\n4\n20\n77\n227\n792\n2357\n3640\n55\n2\n7\n33\n194\n1459\n5390\n88\n5\n2\n16\n175\n1583\n5262\n132\n4\n2\n1\n30\n255\n2184\n4569\n129\n7\n1\n0\n1000\n2000\n3000\n4000\n5000\n6000\n-7 -6 -5 -4 -3 -2 -1 0 1 2 3\n# Sentence\n# Real Target Length - Predicted Length\n0.1 0.3 0.5 0.7\nFigure 2: The analysis of the predicted length. The histogram\nshows the difference between the target length and the predicted\nlength.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1 1 12 13 14 15 16 17 18 1\nProbability\n# Frames\nTrigger\n SIL\n 一\n 线 城\n 市\n签\n 约\n 十\n 七\n 万\n 套\n(a) The realtionship bettween trigger and word boundaries\n0 10 20 30 40 50 60 70 80\n0\n5\n10\n(b) Attention mechanism visualization\nFigure 3: We visually analyzed the test set sentences\n’BAC009S0764W0149’. (a)The line chart shows the rela-\ntionship between trigger position and character pronunciation\nboundaries. The dotted lines indicate the pronunciation bound-\nary information and the spikes present the spike-like posterior\nprobability of the CTC module. (b) is from the 4th source atten-\ntion mechanism of the decoder.\ndict the length of the target sequence approximately accurately.\nHowever, if the value is larger than zero, the model will miss\nsome words permanently. We can ﬁx this problem by adding a\npadding bias to the predicted length.\nOn the other hand, Fig.3(a) shows the relationship between\nthe trigger position and the word pronunciation boundary. There\nis no triggered spike in the range of silence. Within the scope\nof the last pronounced word, there are two triggered spikes. Be-\ncause we also take an end-of-sentence token into consideration\nduring training. It’s obvious that each spike is within the bound-\nary of the word. Therefore, our assumption, that the triggered\nencode state sequence contains more prior information on the\ntarget sequence, is reasonable. It’s obvious from Fig.3(b) that\nthe ST-NAT model can make the target sequence better aligned\nto the encoded states sequence. What’s more, the center of the\nalignment position almost coincides with the trigger position,\nwhich again veriﬁes our assumption.\n3.3.4. Compare with other models.\nWe also compare our proposed ST-NAT model with various\nmain-stream models, e.g. traditional model, CTC-based model,\ntransducer model, and attention-based sequence-to-sequence\nmodel. Under the same training condition and the same model\nparameters, we train a Speech-Transformer[6], NAT-MASKED\nTable 3: Compare with other models in performance and real-\ntime factor.\nModel DEV TEST RTF\nTDNN-Chain (Kaldi) [21] - 7.45 -\nLAS[22] - 10.56 -\nSpeech-Transformer * 6.57 7.37 0.0504\nSA-Transducer †[16] 8.30 9.30 0.1536\nSAN-CTC * [23] 7.83 8.74 0.0168\nSync-Transformer †[24] 7.91 8.91 0.1183\nNAT-MASKED * [11] 7.16 8.03 0.0058\nST-NAT(ours) 6.88 7.67 0.0056\nST-NAT+LM(ours) 6.39 7.02 0.0292\n* These models are re-implemented by ourselves accord-\ning to the papers.\n†We supplement the RTF of our previous two models.\n[11], and our proposed ST-NAT model, where the speech trans-\nformer applies a beam search with beam width 5 to decoding\nutterances.\nFrom Table.3, we can ﬁnd the ST-NAT models can achieve\ncomparable performance with the advanced speech-transformer\nmodel [6] and TDNN-Chain model [21], which is better than\nLAS. From another perspective, the ST-NAT has the fastest in-\nference speed among them, which is only about 1/10 of speech-\ntransformer. The ST-NAT with a transformer language model\ncan achieve the best CER of 7.02% on the test set and an RTF\nof 0.0292.\nCompared with the streaming end-to-end model, e.g. SAN-\nCTC [23], Sync-Transformer [24], and SA-Transducer [16], the\nST-NAT can not only achieve the best performance, but also\nthe fastest inference speed. We suppose that the ST-NAT can\ndecode an utterance with all context and without temporal de-\npendencies.\nBy contrast, we also re-implement a NAT-MASKED model\nin a BERT-like way [11], which adopts a ﬁxed-length (set as 60)\nmask sequence as the input. The NAT-MASKED has the same\nparameters as our ST-NAT except for the CTC module. We ﬁnd\nthe ST-NAT can achieve better performance. We guess that it\nis difﬁcult for the model to learn to predict the target words(or\ncharacters) and the target length jointly. Both of them have a\nvery close inference speed.\n4. Conclusions and Future Works\nTo estimate the length of the target sequence accurately\nand accelerate the inference speech, we proposed a spike-\ntriggered non-autoregressive transformer (ST-NAT) for end-to-\nend speech recognition, which introduce a CTC module to pre-\ndict the target length and accelerate the convergence. The ST-\nNAT adopts the encode states corresponding to the positions of\nspikes as the input of the decoder. In the inference process,\nST-NAT can count the number of spikes to avoid redundant\ncalculations. We conduct all experiments on a public Chinese\nmandarin dataset AISEHLL-1. The results show that the CTC\nmodule can accurately predict the length of the target sequence.\nThe ST-NAT model has achieved achieve comparable perfor-\nmance with the advanced speech transformer model. However,\nthe ST-NAT has a real-time factor of 0.0056, which exceeds all\nmainstream models. What’s more, the ST-NAT with a language\nmodel can still have a very high inference speed. In the future,\nwe will try to utilize the CTC module for joint decoding to im-\nprove the performance of the model during inference.\n5. References\n[1] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[2] C. J. K, B. Dzmitry, S. Dmitriy, C. Kyunghyun, and B. Yoshua,\n“Attention-based models for speech recognition,” in Advances in\nneural information processing systems, 2015, pp. 577–585.\n[3] C. William, J. Navdeep, L. Quoc, and V . Oriol, “Listen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,” in 2016 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2016,\npp. 4960–4964.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems , 2017, pp.\n5998–6008.\n[5] K. Suyoun, H. Takaaki, and W. Shinji, “Joint ctc-attention based\nend-to-end speech recognition using multi-task learning,” in2017\nIEEE international conference on acoustics, speech and signal\nprocessing (ICASSP). IEEE, 2017, pp. 4835–4839.\n[6] D. Linhao, X. Shuang, and X. Bo, “Speech-transformer: a no-\nrecurrence sequence-to-sequence model for speech recognition,”\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018, pp. 5884–5888.\n[7] J. Lee, E. Mansimov, and K. Cho, “Deterministic non-\nautoregressive neural sequence modeling by iterative reﬁnement,”\narXiv preprint arXiv:1802.06901, 2018.\n[8] J. Gu, J. Bradbury, C. Xiong, V . O. Li, and R. Socher,\n“Non-autoregressive neural machine translation,” arXiv preprint\narXiv:1711.02281, 2017.\n[9] X. Ma, C. Zhou, X. Li, G. Neubig, and E. Hovy, “Flowseq: Non-\nautoregressive conditional sequence generation with generative\nﬂow,”arXiv preprint arXiv:1909.02480, 2019.\n[10] Y . Wang, F. Tian, D. He, T. Qin, C. Zhai, and T.-Y . Liu, “Non-\nautoregressive machine translation with auxiliary regularization,”\nin Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 33, 2019, pp. 5377–5384.\n[11] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, “Non-\nautoregressive transformer automatic speech recognition,” arXiv\npreprint arXiv:1911.04908, 2019.\n[12] J. Libovick `y and J. Helcl, “End-to-end non-autoregressive neural\nmachine translation with connectionist temporal classiﬁcation,”\narXiv preprint arXiv:1811.04719, 2018.\n[13] N. Moritz, T. Hori, and J. Le Roux, “Triggered attention for\nend-to-end speech recognition,” in ICASSP 2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2019, pp. 5666–5670.\n[14] ——, “Streaming end-to-end speech recognition with joint ctc-\nattention based models,” in Proc. IEEE Workshop on Automatic\nSpeech Recognition and Understanding (ASRU), 2019.\n[15] Y . N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language\nmodeling with gated convolutional networks,” in Proceedings of\nthe 34th International Conference on Machine Learning-Volume\n70. JMLR. org, 2017, pp. 933–941.\n[16] Z. Tian, J. Yi, J. Tao, Y . Bai, and Z. Wen, “Self-Attention Trans-\nducers for End-to-End Speech Recognition,” in Proc. Interspeech\n2019, 2019, pp. 4395–4399.\n[17] Z. Fan, J. Li, S. Zhou, and B. Xu, “Speaker-aware speech-\ntransformer,” in 2019 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU). IEEE, 2019, pp. 222–229.\n[18] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber, “Con-\nnectionist temporal classiﬁcation: labelling unsegmented se-\nquence data with recurrent neural networks,” in Proceedings of\nthe 23rd international conference on Machine learning, 2006, pp.\n369–376.\n[19] P.-T. De Boer, D. P. Kroese, S. Mannor, and R. Y . Rubinstein,\n“A tutorial on the cross-entropy method,” Annals of operations\nresearch, vol. 134, no. 1, pp. 19–67, 2005.\n[20] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, “Specaugment: A simple data augmen-\ntation method for automatic speech recognition,” arXiv preprint\narXiv:1904.08779, 2019.\n[21] D. Povey, V . Peddinti, D. Galvez, P. Ghahremani, V . Manohar,\nX. Na, Y . Wang, and S. Khudanpur, “Purely sequence-trained neu-\nral networks for asr based on lattice-free mmi.” in Interspeech,\n2016, pp. 2751–2755.\n[22] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and L. Xie,\n“Component fusion: Learning replaceable language model com-\nponent for end-to-end speech recognition system,” in ICASSP\n2019 - 2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2019, pp. 5361–5635.\n[23] J. Salazar, K. Kirchhoff, and Z. Huang, “Self-attention networks\nfor connectionist temporal classiﬁcation in speech recognition,” in\nICASSP 2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2019, pp. 7115–\n7119.\n[24] Z. Tian, J. Yi, Y . Bai, J. Tao, S. Zhang, and Z. Wen, “Synchronous\ntransformers for end-to-end speech recognition,” inICASSP 2020\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2020, pp. 5666–5670."
}