{
  "title": "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection",
  "url": "https://openalex.org/W2986617323",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5064222284",
      "name": "Siddhant Garg",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A5056497976",
      "name": "Thuy Vu",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5056376686",
      "name": "Alessandro Moschitti",
      "affiliations": [
        "Amazon (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2767857566",
    "https://openalex.org/W6736172710",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2469060249",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2339995566",
    "https://openalex.org/W2587528408",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W1966443646",
    "https://openalex.org/W2337120330",
    "https://openalex.org/W2760753016",
    "https://openalex.org/W2806019191",
    "https://openalex.org/W2865914962",
    "https://openalex.org/W6751880537",
    "https://openalex.org/W6730073950",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2120735855",
    "https://openalex.org/W6948213869",
    "https://openalex.org/W2947641054",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2963324947",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2950193743",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2988869004",
    "https://openalex.org/W2803820154",
    "https://openalex.org/W2049434052",
    "https://openalex.org/W2556553881"
  ],
  "abstract": "We propose TandA, an effective technique for fine-tuning pre-trained Transformer models for natural language tasks. Specifically, we first transfer a pre-trained model into a model for a general task by fine-tuning it with a large and high-quality dataset. We then perform a second fine-tuning step to adapt the transferred model to the target domain. We demonstrate the benefits of our approach for answer sentence selection, which is a well-known inference task in Question Answering. We built a large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. Our approach establishes the state of the art on two well-known benchmarks, WikiQA and TREC-QA, achieving the impressive MAP scores of 92% and 94.3%, respectively, which largely outperform the the highest scores of 83.4% and 87.5% of previous work. We empirically show that TandA generates more stable and robust models reducing the effort required for selecting optimal hyper-parameters. Additionally, we show that the transfer step of TandA makes the adaptation step more robust to noise. This enables a more effective use of noisy datasets for fine-tuning. Finally, we also confirm the positive impact of TandA in an industrial setting, using domain specific datasets subject to different types of noise.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nTANDA: Transfer and Adapt Pre-Trained\nTransformer Models for Answer Sentence Selection\nSiddhant Garg∗\nUniversity of Wisconsin-Madison\nMadison, WI, USA\nsidgarg@cs.wisc.edu\nThuy Vu\nAmazon Alexa\nManhattan Beach, CA, USA\nthuyvu@amazon.com\nAlessandro Moschitti\nAmazon Alexa\nManhattan Beach, CA, USA\namosch@amazon.com\nAbstract\nWe propose TAND A, an effective technique for ﬁne-tuning\npre-trained Transformer models for natural language tasks.\nSpeciﬁcally, we ﬁrsttransfer a pre-trained model into a model\nfor a general task by ﬁne-tuning it with a large and high-\nquality dataset. We then perform a second ﬁne-tuning step to\nadapt the transferred model to the target domain. We demon-\nstrate the beneﬁts of our approach for answer sentence selec-\ntion, which is a well-known inference task in Question An-\nswering. We built a large scale dataset to enable the trans-\nfer step, exploiting the Natural Questions dataset. Our ap-\nproach establishes the state of the art on two well-known\nbenchmarks, WikiQA and TREC-QA, achieving the impres-\nsive MAP scores of 92% and 94.3%, respectively, which\nlargely outperform the the highest scores of 83.4% and 87.5%\nof previous work. We empirically show that T\nAND A gener-\nates more stable and robust models reducing the effort re-\nquired for selecting optimal hyper-parameters. Additionally,\nwe show that the transfer step of T\nAND A makes the adapta-\ntion step more robust to noise. This enables a more effective\nuse of noisy datasets for ﬁne-tuning. Finally, we also conﬁrm\nthe positive impact of T\nAND A in an industrial setting, using\ndomain speciﬁc datasets subject to different types of noise.\n1 Introduction\nIn recent years, virtual assistants have become a central asset\nfor technological companies. This has increased the interest\nof AI researchers in studying and developing conversational\nagents, some popular examples being Google Home, Siri\nand Alexa. This has renewed the research interest in Ques-\ntion Answering (QA) and, in particular, in two main tasks:\n(i) answer sentence selection (AS2), which, given a question\nand a set of answer sentence candidates, consists in selecting\nsentences (e.g., retrieved by a search engine) correctly an-\nswering the question; and (ii) machine reading (MR) (Chen\net al. 2017) or reading comprehension, which, given a ques-\ntion and a reference text, consists in ﬁnding a text span an-\nswering it. Even though the latter is gaining more and more\npopularity, AS2 is more relevant to a production scenario\n∗Work done while the author was an intern at Amazon Alexa.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nsince, a combination of a search engine and an AS2 model\nalready implements an initial QA system.\nThe AS2 task was originally deﬁned in the TREC com-\npetition (Wang, Smith, and Mitamura 2007). With the ad-\nvent of neural models, it has had signiﬁcant contributions\nfrom techniques such as (He and Lin 2016; Yang et al. 2018;\nWang and Jiang 2016).\nRecently, approaches for training neural language models,\ne.g., ELMO (Peters et al. 2018), GPT (Radford et al. 2018),\nBERT (Devlin et al. 2018), RoBERTa (Liu et al. 2019), XL-\nNet (Dai et al. 2019) have led to major advancements in\nseveral NLP subﬁelds. These methods capture dependencies\nbetween words and their compounds by pre-training neural\nnetworks on large amounts of data. Interestingly, the result-\ning models can be easily applied to solve different NLP ap-\nplications by just ﬁne-tuning them on the training data of\nthe target tasks. For example, the Transformer (V aswani et\nal. 2017) can be pre-trained on a large amount of data obtain-\ning a powerful language model, which can then be special-\nized for solving speciﬁc NLP tasks by just adding new layers\nand training them on the target data. Although the approach\nis simple, the procedure for ﬁne-tuning a Transformer-based\nmodel is not completely understood, and can result in high\naccuracy variance, with a possible on-off behavior, e.g.,\nmodels may always predict a single label. As a consequence,\nresearchers need to invest a considerable effort in selecting\nsuitable parameters, with no theory or a well-assessed best\npractice helping them. Such problem also affects QA, and,\nin particular, AS2 since no large and and accurate dataset\nhas been developed for it.\nIn this paper, we study the use of Transformer-based mod-\nels for AS2 and provide effective solutions to tackle the data\nscarceness problem for AS2 and the instability of the ﬁne-\ntuning step. In detail, the contributions of our papers are:\n• We improve stability of Transformer models by adding an\nintermediate ﬁne-tuning step, which aims at specializing\nthem to the target task (AS2), i.e., this step transfers a pre-\ntrained language model to a model for the target task.\n• We show that the transferred model can be effectively\nadapted to the target domain with a subsequent ﬁne-\ntuning step, even when using target data of small size.\n• Our Transfer and Adapt (T\nAND A) approach makes ﬁne-\n7780\ntuning: (i) easier and more stable, without the need of\ncherry picking parameters; and (ii) robust to noise, i.e.,\nnoisy data from the target domain can be utilized to train\nan accurate model.\n• We built ASNQ, a dataset for AS2, by transforming\nthe recently released Natural Questions (NQ) corpus\n(Kwiatkowski et al. 2019) from MR to AS2 task. This was\nessential as our transfer step requires a large and accurate\ndataset. ASNQ is an important contribution of our work\nto the research community.\n1\n• Finally, the generality of our approach and empirical in-\nvestigation suggest that our TAND A ﬁndings also apply\nto other NLP tasks, especially, textual inference, although\nempirical analysis is essential to conﬁrm these claims.\nWe evaluated T\nAND A on two well-known academic bench-\nmarks, i.e., TREC-QA (Wang, Smith, and Mitamura 2007)\nand WikiQA (Yang, Yih, and Meek 2015), as well as three\ndifferent industrial datasets, where questions are derived\nfrom Alexa trafﬁc and candidate sentences are selected from\nweb data. The results show that:\n• T\nAND A improves the stability of Transformer models.\nIn the adapt step, the model accuracy throughout differ-\nent epochs show a smooth and convex behavior, which is\nideal for estimating optimal parameters.\n• We improved the state of the art in AS2 by just applying\nBERT and RoBERTa to AS2 and further improved it by\nalmost 10 absolute percent points in MAP with T\nAND A.\n• TAND A achieves much higher accuracy than traditional\nﬁne-tuning, especially in case of noise data. For example,\nthe drop in performance is up to one order of magnitude\nlower with T\nAND A, i.e., 2.5%, when we inject 20% of\nnoise in the WikiQA and TREC-QA datasets.\n• Our experiments with real-world datasets built from\nAlexa trafﬁc data conﬁrm all our above ﬁndings. Speciﬁ-\ncally, we observe the same robustness to noise, which, in\nthis case, is generated by real sources.\nThe rest of the paper is structured as follows: we describe\nrelated work in Section 2, details of our T\nAND A approach\nin Section 3, present the new AS2 dataset in Section 4. The\nexperimental results on academic benchmarks, including ex-\nperiments with noise data, are reported in Section 5, while\nthe results on real-world data are illustrated in Section 6. Fi-\nnally, we derive conclusions in Section 7.\n2 Related Work\nRecent AS2 models are based on Deep Neural Networks\n(DNNs), which learn distributed representations of the in-\nput data and are trained to apply a series of non-linear trans-\nformations to the input question and answer, represented as\ncompositions of word or character embeddings. DNN archi-\ntectures learn answer sentence-relevant patterns using intra-\npair similarities as well as cross-pair, question-to-question\nand answer-to-answer similarities, when modeling the input\n1The ASNQ dataset and trained models can be accessed at\nhttps://github.com/alexa/wqa tanda\ntexts. For example, the CNN network by Severyn and Mos-\nchitti has two separate embedding layers for the question\nand answer, which are followed by the respective convo-\nlution layers. The output of the latter is concatenated and\nthen passed through the ﬁnal fully-connected joint layer.\nThey also added embeddings encoding relational links be-\ntween matching words (Severyn and Moschitti 2016): a sort\nof hardcoded attention, which highly increases accuracy.\nMore recent papers (Shen, Yang, and Deng 2017; Tran\net al. 2018; Tay, Tuan, and Hui 2018) also propose a tech-\nnique of inter-weighted alignment networks for this task.\nWhile others (Wang and Jiang 2016; Bian et al. 2017;\nY oon et al. 2019) use a compare aggregate architecture,\nwhich also exploits an attention mechanism over the ques-\ntion and answer sentence embeddings. Tayyar Madabushi,\nLee, and Barnden (2018) propose a method that integrates\nﬁne-grained Question Classiﬁcation with a Deep Learning\nmodel designed for AS2.\nPrevious work (Wang, Smith, and Mitamura 2007) was\ncarried out on relatively small datasets compared to other\nNLP tasks, such as machine translation, e.g., the WMT15\nEnglish-Czech dataset (Luong and Manning 2016) contains\n15.8 million sentence pairs. This further motivates our work\nof creating a new large scale AS2 dataset, ASNQ, which is\ntwo orders of magnitudes larger than datasets such as TREC-\nQA (Wang, Smith, and Mitamura 2007).\nASNQ is based on the NQ dataset, which is a corpus de-\nsigned to (i) study MR tasks, and (ii) solve several problems\nof previous corpora such as SQuAD (Rajpurkar et al. 2016).\nMR models, e.g., (Seo et al. 2016), are different from those\nused for AS2 and beyond the purpose of our paper. It is still\nworthwhile to mention that Min, Seo, and Hajishirzi (2017)\nexplored transfer learning for the BiDAF model for MR.\nV ery recent works (Devlin et al. 2018; Yang et al. 2019)\nuse pre-trained Transformer models for MR. In this context,\nT\nAND A and ASNQ may provide alternative research direc-\ntions, e.g., a more stable model that can then be ﬁned tuned\nto the MR task.\nWang et al. (2019) report some marginal improvement for\nthe task of text classiﬁcation by ﬁxing weights of Trans-\nformer models derived by BERT, when training the classi-\nﬁcation layer. Sun et al. (2019) carried out additional pre-\ntraining of BERT-derived models on the target dataset. They\nalso used different learning rates for different model layers.\n3T AND A: Transfer and Adapt\nWe propose to train Transformer models for the AS2 by ap-\nplying a two-step ﬁne-tuning, called Transfer\nAND Adapt\n(TAND A). The ﬁrst step transfers the language model of the\nTransformer to the AS2 task; and the second ﬁne-tuning step\nadapts the obtained model to the speciﬁc target domain, i.e.,\nspeciﬁc types of questions and answers. We ﬁrst provide a\nbackground on AS2 and Transformer models, and, then ex-\nplain our approach in more detail.\n3.1 AS2 task and model deﬁnition\nAS2 can be deﬁned as follows: given a questionq and a set\nof answer sentence candidatesS = {s1, .., sn}, select a sen-\ntence sk that correctly answersq. We can model the task as a\n7781\nFigure 1: Transfer and Adapt for Answer Sentence Selection, applied to BERT\nfunction r : Q ×P(S) → S, deﬁned asr(q, S)= sk, where\nk = argmaxi p(q, si) and p(q, si) is the probability of cor-\nrectness of si. We estimatep(q, si) using neural networks,\nin particular, Transformer models, as explained below.\nFigure 2: Transformer architecture with on top a linear clas-\nsiﬁer for ﬁne-tuning on AS2. Here [CLS] Tok\n1\n1,...,Tok1\nN\n[SEP] Tok2\n1,...,Tok2\nM [EOS] is the input to the model.\n3.2 Transformer models for AS2\nTransformer-based models are neural networks designed to\ncapture dependencies between words, i.e., their interdepen-\ndent contexts. Fig. 2 shows the standard architecture for a\ntext pair classiﬁcation task. The input consists of two pieces\nof text, Tok\n1\n1,...,Tok1\nN and Tok2\n1,...,Tok2\nM delimited by three\ntags, [CLS], [SEP] and [EOS] (beginning, separator and end\nof sentence). The input is encoded as embeddings based on\ntokens, segments and their positions. These are fed as input\nto several blocks (up to 24) containing layers for multi-head\nattention, normalization and feed forward processing. The\nresult of this transformation is an embedding,x, represent-\ning the text pair, which models the dependencies between\nwords and segments of the two sentences. For a downstream\ntask, x is fed (after applying a non linearity function) to a\nfully connected layer having weights:W\nT and BT . The out-\nput layer can be used to implement the task function. For\nexample, a softmax can be used to model the probability\nof a text pair classiﬁcation, as described by the equation:\nˆy = softmax(W\nT ×tanh(x)+ BT ).\nTheoretically, this model can be trained using the log\ncross-entropy loss: L = −∑\nl∈{0,1} yl × log(ˆyl) on pairs\nof text. In practice, a large amount of supervised data will\nbe required for this step of training. The important contribu-\ntion by Devlin et al., (2018) was to pre-train the language\nmodel, i.e., the sentence pair representation, on using sur-\nrogate tasks such as masked language modeling and next\nsentence prediction.\nThe left block in Fig. 1 illustrates the pre-training step of\na Transformer model, highlighting that some words in the\npair are masked. This way the model learns to generalize the\ninput while providing the same output. The default transfer\napproach, deﬁned in previous work, ﬁne-tunes the Trans-\nformer model to the target task and domain (in one shot).\nFor AS2, the training data comprises of question and sen-\ntence pairs with positive or negative labels according to the\ntest: the sentence correctly answers the question or not. This\nﬁne-tuning is rather critical as the initial task learned during\nthe pre-training stage is very different from AS2.\nWhen only small target data is available, the transfer pro-\ncess from the language model to AS2 task is unstable. We\nconjecture that a large number of examples are required to\nﬁne-tune the large number of Transformer parameters on the\nnew task. An evidence of this is the on-off effect, that is,\nthe ﬁne-tuned model always predicts a single label for all\nexamples. More in general, the model accuracy is unstable\nshowing a large variance over different ﬁne-tuning attempts.\nWe explain this behavior considering the quality and\nquantity of the training data required for the transfer step\n7782\n(from the pre-trained model to AS2). More precisely, a small\nnumber of data examples prevents an effective convergence\nto the task, while noisy data leads to incorrect convergence.\nThus, we propose to divide the ﬁne-tuning process in two\nsteps: transfer to the task and then adapt to the target domain\n(T\nAND A). This is advantageous over a single ﬁne-tuning\nstep, since the latter would require either (i) the availability\nof a large dataset for the target domain, which is undesirable\ndue to higher difﬁculty and cost of collection of domain spe-\nciﬁc data over general data; or (ii) merging the general and\ndomain speciﬁc data in a single training step, which is not\noptimal since the the model needs to be specialized only to\nthe target data. In principle when using a combination of\ngeneral and domain speciﬁc data, instance weighting can be\nused by giving more importance to the target data. However,\nﬁnding the right weights is complex as neural models do not\nconverge to a global optimum: thereby leading to very dif-\nferent outcomes for different weights.\n3.3 T AND A\nThe two steps of our approach are depicted in the center and\nright blocks of Fig. 1. We apply a standard ﬁne-tuning step\nusing a large scale general purpose dataset for AS2. This\nstep is supposed to transfer the Transformer language model\nto the AS2 task. The resulting model will not perform opti-\nmally on the data of the target domain due to the speciﬁcity\nof the latter. We thus apply a second ﬁne-tuning step to adapt\nthe classiﬁer to the target AS2 domain. For example, in the\ntransfer step, we may have general questions such as,What\nis the average heart rate of a healthy personwhile, in the\nadapt step, the target domain, e.g., sport news, may contain\nspeciﬁc questions such as:When did the Philadelphia eagles\nplay the fog bowl?\nUsing different training steps on the target data to improve\nperformance is a rather intuitive approach. In this paper, we\nhighlight the role of the transfer step, which (i) greatly re-\nduces the amount of data required in the adaptation step;\nand (ii) stabilizes the model, making it robust to noise. We\nempirically demonstrate both claims in our experiments.\n4 Answer-Sentence Natural Questions\nWe needed an accurate, general and large AS2 corpus\nto validate the beneﬁts of T\nAND A. Since existing AS2\ndatasets are small in size, we built a new AS2 dataset\ncalled Answer Sentence Natural Questions (ASNQ) derived\nfrom the recently released Google Natural Questions (NQ)\ndataset (Kwiatkowski et al. 2019).\nNQ is a large scale dataset intended for the MR task,\nwhere each question is associated with a Wikipedia page.\nFor each question, a long paragraph (long\nanswer) that\ncontains the answer is extracted from the reference page.\nEach long\nanswer may contain phrases annotated as\nshort answer.A long answer can contain multiple\nsentences, thus NQ is not directly applicable for AS2.\nFor each question in ASNQ, the positive candidate an-\nswers are those sentences that occur in the long answer para-\ngraphs in NQ and containannotated short answers. The re-\nmaining sentences from the document are labeled as nega-\nFigure 3: An example of data instance conversion from NQ\nto ASNQ.\nLabel S ∈ LA SA ∈ S # Train #D e v\n1 No No 19,446,120 870,404\n2 No Yes 428,122 25,814\n3 Yes No 442,140 29,558\n4 Yes Yes 61,186 4,286\nTable 1: Label description for ASNQ. Here S, LA, SA refer\nto answer sentence, long answer passage and short answer\nphrase respectively.\ntive for the target question. The negative examples can be of\nthe following types:\n1. Sentences from the document that are in the\nlong\nanswer but do not contain the annotated\nshort answers. It is possible that these sentences might\ncontain theshort\nanswer.\n2. Sentences from the document that are not in the\nlong answer but contain theshort answer string,\nthat is, such occurrence is purely accidental.\n3. Sentences from the document that are neither in the\nlong answer nor contain theshort answer.\nThe generation of negative examples is particularly im-\npactful to the robustness of the model in identifying the best\nanswer out of the similar but incorrect ones. ASNQ has four\nlabels that describe possible confusing levels of a sentence\ncandidate. We apply the same processing both to training\nand development sets of NQ. An example is shown in Fig. 3,\nwhile the ASNQ statistics are reported in Table 1.\nASNQ contains 57,242 distinct questions in the training\nset and 2,672 distinct questions in the dev. set, which is an\norder of magnitude larger than most public AS2 datasets. For\nthe transfer step in T\nAND A, we use ASNQ sentence pairs\nwith labels 1, 2 and 3 as negatives and Label 4 as positives.\n5 Experiments on Standard Benchmarks\nWe provide empirical evidence on the beneﬁts of using\nT\nAND A on two commonly used benchmarks for AS2:\n7783\nMode Train Dev Test\nQ A Q A Q A\nraw 2118 20360 296 2733 633 6165\nno all- 873 8672 126 1130 243 2351\nClean 857 8651 121 1126 237 2341\nTable 2: WikiQA dataset statistics\nWikiQA and TREC-QA, which enable a direct comparison\nwith previous work.\n5.1 Academic Datasets\nWikiQA (Yang, Yih, and Meek 2015) has some questions\nwith no correct answer sentence (all-) or with only correct\nanswer sentences (all+). Table 2 reports the corpus statistics\nof the versions:raw (as distributed), withoutall- questions,\nand without bothall- and all+ questions (Clean). We follow\nthe most used setting: training with theno all- mode and\nthen answer candidate sentences per question in testing with\nthe Clean mode.\nTREC-QA is another popular benchmark (Wang, Smith,\nand Mitamura 2007) for AS2. We removed questions with-\nout answers, or with only correct or only incorrect answer\nsentence candidates, from the dev. and test sets. This resulted\nin 1, 229, 65and 68questions, and53, 417, 1, 117and 1, 442\nquestion-answer pairs for training, dev. and test sets, respec-\ntively. This setting refers to theClean setting (Shen, Yang,\nand Deng 2017), which is a TREC-QA standard.\nQNLI, namely, Question Natural Language Inference is a\ndataset (Wang et al. 2018) derived from the Stanford Ques-\ntion Answering Dataset (SQuAD) (Rajpurkar et al. 2016)\nby converting question-paragraph pairs into sentence pairs,\nresulting in dataset with 105k question-answer pairs for\ntraining and 5.4k pairs in the dev. data. We carry out ex-\nperiments of using QNLI for the transfer step in T\nAND A\nand compare it with previous methods (Y oon et al. 2019),\nwhich are based on QNLI for sequential ﬁne-tuning of non-\nTransformer models for AS2.\n5.2 Training and testing details\nMetrics We measure system accuracy with Mean Average\nPrecision (MAP) and Mean Reciprocal Recall (MRR) evalu-\nated on the test set, using the entire set of candidates for each\nquestions (this varies according to the different datasets).\nModels We use the pre-trained BERT-Base (12 layer),\nBERT-Large (24 layer), RoBERTa-Base (12 layer) and\nRoBERTa-Large-MNLI (24 layer) models, which were re-\nleased as checkpoints for use in downstream tasks.\nTraining We adopt Adam optimizer (Kingma and Ba\n2014) with a learning rate of 2e-5 for the transfer step on the\nASNQ dataset and a learning rate of 1e-6 for the adapt step\non the target dataset. We apply early stopping on the dev. set\nof the target corpus for both steps based on the highest MAP\nscore. We set the max number of epochs equal to 3 and 9 for\nadapt and transfer steps, respectively. We set the maximum\nsequence length for BERT/RoBERTa to 128 tokens.\nModel MAP MRR\nComp-Agg + LM + LC 0.764 0.784\nComp-Agg + LM + LC+ TL(QNLI) 0.834 0.848\nBERT-B FT WikiQA 0.813 0.828\nBERT-B FT ASNQ 0.884 0.898\nBERT-B TAND A (ASNQ→WikiQA ) 0.893 0.903\nBERT-L FT WikiQA 0.836 0.853\nBERT-L FT ASNQ 0.892 0.904\nBERT-L TAND A (ASNQ→ WikiQA) 0.904 0.912\nRoBERTa-B FT ASNQ 0.882 0.894\nRoBERTa-B TAND A (ASNQ→ WikiQA) 0.889 0.901\nRoBERTa-L FT ASNQ 0.910 0.919\nRoBERTa-L TAND A (ASNQ→ WikiQA ) 0.920 0.933\nTable 3: Performance of different models on WikiQA\ndataset. Here Comp-Agg + LM + LC refers to a Compare-\nAggregate model with Language Modeling and Latent Clus-\ntering as proposed by Y oon et al. (2019). TL(QNLI) refers\nto Transfer Learning from the QNLI corpus. L and B stand\nfor Large and Base, respectively.\nParameter Tuning We selected learning rates for Adam\noptimizer in the transfer and adapt steps as follows: (i) We\ntested a reasonable set of values for the transfer and adapt\nsteps, identifying two promising values,1e −6 and 2e −5,\nfor the former, and ﬁve values{1e −6, 2e −6, 5e −6, 1e −\n5, 2e−5}for the latter. These are within the range of typical\nlearning rates for the Adam optimizer. (ii) We tested the ten\ncombinations for the T\nAND A approach and we selected the\nvalue pair, (2e − 5,1e − 6), for the transfer and adapt step\nrespectively, which optimizes the MAP on the dev. set of\nthe target dataset. As T\nAND A makes ﬁne-tuning stable, we\nended up selecting the same parameter values for all tested\ndatasets. It should be noted that the optimality of a larger\nlearning rate for the ﬁrst step and a smaller learning rate for\nthe second step supports our claim of considering the second\nstep of T\nAND A as domain adaptation process. For the exper-\niments with one ﬁne-tuning step (baseline), we again chose\neither 1e −6 or 2e −5, i.e, following the common practice\nof BERT ﬁne-tuning.\n5.3 Main Results\nWikiQA Table 3 reports the MAP and MRR of different\npre-trained Transformer models for two methods: standard\nﬁne-tuning (FT) and T\nAND A. The latter takes two argu-\nments that we indicate astransfer dataset → adapt dataset.\nThe table shows that:\n• BERT-Large and BERT-Base with standard ﬁne-tuning on\nWikiQA match the current state of the art by Y oon et\nal. (2019).\n2 The latter uses the compare aggregate model\nwith latent clustering, ELMO embeddings, and transfer\nlearning from the QNLI corpus.\n• T\nAND A provides a large improvement over the state of the\nart, which has been regularly contributed to by hundreds\nof researchers.\n2https://aclweb.org/aclwiki/Question Answering (State of\nthe art)\n7784\nModel MAP MRR\nComp-Agg + LM + LC 0.868 0.928\nComp-Agg + LM + LC + TL(QNLI) 0.875 0.940\nBERT-B FT TREC-QA 0.857 0.937\nBERT-B FT ASNQ 0.823 0.872\nBERT-B TAND A (ASNQ→ TREC-QA) 0.912 0.951\nBERT-L FT TREC-QA 0.904 0.946\nBERT-L FT ASNQ 0.824 0.872\nBERT-L TAND A (ASNQ→ TREC-QA ) 0.912 0.967\nRoBERTa-B FT ASNQ 0.849 0.907\nRoBERTa-B TAND A (ASNQ→TREC-QA ) 0.914 0.952\nRoBERTa-L FT ASNQ 0.880 0.928\nRoBERTa-L TAND A (ASNQ→ TREC-QA) 0.943 0.974\nTable 4: Performance of different models on TREC-QA\ndataset. Here Comp-Agg + LM + LC refers to a Compare-\nAggregate model with Language Modeling and Latent Clus-\ntering as proposed in (Y oon et al. 2019). TL(QNLI) refers to\nTransfer Learning from the QNLI corpus. L and B stand for\nLarge and Base, respectively.\n• RoBERTa-Large T\nAND A using ASNQ→WikiQA estab-\nlish an impressive new state of the art for AS2 on WikiQA\nof 0.920 and 0.933 in MAP and MRR, respectively.\n• Finally, we note that the standard ﬁne-tuning on ASNQ\nalready outperforms the previous state of the art. This is\nmainly due to the fact that as ASNQ and WikiQA are both\nbased on answers from Wikipedia.\nThe next section conﬁrms the results above on TREC-QA.\nTREC-QA Table 4 reports the results of our experiments\nwith TREC-QA. We note that:\n• RoBERTa-Large T\nAND A with ASNQ → TREC-QA\nagain establishes an impressive performance of 0.943 in\nMAP and 0.974 in MRR, outperforming the previous state\nof the art by Y oon et al. (2019).\n• Both BERT-Base and Large ﬁne purely tuned on the\nTREC-QA corpus can surpass the previous state of the\nart, probably because the size of TREC-QA training cor-\npus is larger than that of WikiQA.\n• T\nAND A improves all the models: BERT-Base, RoBERTa-\nBase, BERT-Large and RoBERTa-Large, outperforming\nthe previous state of the art with all of them.\n• Finally, the model obtained with FT on just ASNQ pro-\nduces the expected results: it performs much lower than\nany T\nAND A model and also lower than FT on just\nTREC-QA since the target domain of TREC questions is\nsigniﬁcantly different from that of ASNQ.\nWe also tried FT on the merged ASNQ and TREC-QA\ndataset to show that the sequential FT, i.e., T\nAND A, im-\nproves over this. BERT-Base model ﬁne-tuned on ASNQ∪\nTREC-QA achieves a MAP and MRR of 0.898 and 0.929,\nrespectively. These are signiﬁcantly lower than 0.912 MAP\nand 0.951 MRR, obtained with T\nAND A. We also stress the\nother important beneﬁts of TAND A: (i) it enables modular-\nity, thereby avoiding to retrain on the large ASNQ data (FT\non any target dataset can start from the model transferred\nFigure 4: MAP and MRR on the WikiQA-Test-data varying\nwith number of ﬁne-tuning epochs on the WikiQA-Train-\ndata for simple FT and T\nAND A.\nwith ASNQ); and (ii) it has a higher training efﬁciency (tar-\nget datasets are much smaller than ASNQ).\n5.4 Properties of T AND A\nStability of TAND A Systematic and effective ﬁne-tuning\nof Transformer models is still an open problem. There is no\ntheory or even a well-assessed best practice suggesting the\noptimal number of epochs to be used for ﬁne-tuning. We\nclaim that T\nAND A can robustly transfer language models to\nthe target task and this produces more stable models. For sta-\nbility, we mean a low variance of the model accuracy (i) be-\ntween two consecutive training epochs, and (ii) between two\npairs of models that haveclose accuracy on the development\nset. For example, BERT FT has a high variance in accuracy\nwith the number of epochs, leading to some extreme cases of\nan on-off behavior, i.e, the classiﬁer may only predict nega-\ntive labels for target task (due to unbalanced datasets).\nTo test our hypothesis, we compared T\nAND A with stan-\ndard FT by varying the number of training epochs for the\nadaptation step (here we do not use early stopping for\nT\nAND A). Figure 4 shows the plots of MAP and MRR scores\nwith BERT-Base on WikiQA test set with different number\nof training epochs on the train set. As expected FT has a\nhigh variance while T\nAND A shows a small variance. A di-\nrect consequence of this better behavior is the higher proba-\nbility to select an optimal epoch parameter on the dev. set.\nRobustness to Noise in WikiQA and TREC-QA Better\nmodel stability also means robustness to noise. We empir-\nically studied this conjecture by artiﬁcially injecting noise\nin the training sets of WikiQA and TREC-QA, by randomly\nsampling questions-answer pairs from the training set and\nswitching their label values. We chose random samples of\n10% and 20% of the training data, generating 867 and 1734\nnoisy labels, on WikiQA, respectively, and 5341 and 10683\nnoisy labels, on TREC-QA, respectively. We used the same\nClean data setting to directly compare with the original per-\nformance. Table 5 shows the MAP and MRR of BERT-\n7785\nBERT-base WikiQA TREC-QA\nMAP % Drop MRR % Drop MAP % Drop MRR % Drop\nNo noise Fine-tuning 0.813 - 0.828 - 0.857 - 0.937 -\n10% noise Fine-tuning 0.775 4.67% 0.793 4.22% 0.826 3.62% 0.902 3.73%\n20% noise Fine-tuning 0.629 22.63% 0.645 22.10% 0.738 13.88% 0.843 10.03%\nNo noise TAND A (ASNQ→*) 0.893 - 0.903 - 0.912 - 0.951 -\n10% noise TAND A (ASNQ→ *) 0.876 1.90% 0.889 1.55% 0.896 1.75% 0.941 1.05%\n20% noise TAND A (ASNQ→ *) 0.870 2.57% 0.886 1.88% 0.891 2.30% 0.937 1.47%\nTable 5: Model accuracy when noise is injected into WikiQA and TREC-QA datasets.∗ indicates the target dataset for the\nsecond step of ﬁne-tuning (adapt step).\nBase using FT and TAND A, also indicating the drop per-\ncentage (% ) in accuracy due to the injection of noise. We\nnote that standard FT is highly affected by noisy data, e.g.,\non WikiQA the accuracy decreases by 22.63%, when 20%\nof noise is injected. V ery interestingly, the models using\nT\nAND A are affected an order of magnitude less, i.e., just\n2.57%. A similar trend can be observed aslo for the results\non TREC-QA.\nModel WikiQA TREC-QA\nMAP MRR MAP MRR\nNeg: 1 Pos: 4 0.870 0.880 0.808 0.847\nNeg: 2 Pos: 4 0.751 0.763 0.662 0.751\nNeg: 3 Pos: 4 0.881 0.895 0.821 0.869\nNeg: 2,3 Pos: 4 0.883 0.898 0.823 0.871\nNeg: 1,2,3 Pos: 4 0.884 0.898 0.823 0.872\nTable 6: Impact of different labels of ASNQ on ﬁne-tuning\nBERT for answer sentence selection. Neg and Pos refers\nto question-answer (QA) pairs of that particular label being\nchosen for ﬁne-tuning.\n5.5 Insights on ASNQ\nAblation studies Fig. 3 shows that we generated different\ntypes of negative examples for the AS2 task, namely, labels\n1,2 and 3. We carried out experiments by ﬁne-tuning BERT-\nBase on ASNQ with speciﬁc label categories assigned to the\nnegative class. Table 6 shows the results: Label 3 is the most\neffective negative type of the three, i.e., the models only us-\ning Label 3 as negative class are just subject to a marginal\ndrop in performance with respect to the model using all la-\nbels. Labels 2 and 3 provide the same accuracy than the three\nlabels as the negative class.\nBERT-Base WikiQA TREC-QA\nMAP MRR MAP MRR\nFT QNLI 0.760 0.766 0.820 0.890\nFT ASNQ 0.884 0.898 0.823 0.872\nTAND A (QNLI→) 0.832 0.852 0.863 0.906\nTAND A (ASNQ→) 0.893 0.903 0.912 0.951\nTable 7: Comparison of TAND A with ASNQ and QNLI\nASNQ vs. QNLI Another way to empirically evaluate the\nimpact of ASNQ is to compare it with other similar datasets,\ne.g., QNLI, observing the performance of the latter when\nused for a simple FT or in T\nAND A. Table 7 shows that both\nFT and TAND A using ASNQ provide signiﬁcantly better\nperformance than QNLI on the WikiQA dataset.\nOn TREC-QA dataset the results show that (i) FT on\nQNLI performs better than ASNQ but (ii) when TAND A\nuses ASNQ as transfer step, the models can better adapt to\nTREC-QA data than when using QNLI for the same transfer\ntype. On one hand, this conﬁrms the claim about the high\nquality of ASNQ. It is a more general and accurate AS2\ndataset and is better suited for transferring the Transformer\nlanguage model. On the other hand, it provides some evi-\ndence that the transfer step is very important and is not just\na way for initializing weights of the adaptation step.\nDataset Questions QA Pairs Pos. Neg.\nSample 1 435 21,530 19,598 1,932\nSample 2 441 44,593 40,136 4,457\nSample 3 452 45,300 42,131 3,169\nTable 8: Statistics of samples 1, 2 and 3 (accurate test sets)\n6 Experiments on data from Alexa\nTo show that our results generalize well, we tested our mod-\nels using four different AS2 datasets created with questions\nsampled from the customers’ interaction with Alexa Virtual\nAssistant.\n6.1 Datasets\nWe built three test sets based on three samples of ques-\ntions labelled with information intent that can be answered\nusing unstructured text. Questions from Sample 1 are ex-\ntracted from NQ questions while questions for samples 2\nand 3 are generated from Alexa users’ questions. For each\nquestions, we selected 100 sentence candidates from the top\ndocuments retrieved by a search engine: (i) for samples 1\nand 2, we used an elastic search system, ingested with sev-\neral web domains, ranging from Wikipedia toreference.com,\ncoolantarctica.com, www.cia.gov/library, etc. (ii) For Sam-\nple 3, we retrieved the candidate answers using a commer-\ncial search engine, to have a higher retrieval quality.\n7786\nMODEL Sample 1 Sample 2 Sample 3\nPrec@1 MAP MRR Prec@1 MAP MRR Prec@1 MAP MRR\nBERT\nBase\nNAD 49.80 0.506 0.638 52.69 0.432 0.629 41.86 0.352 0.543\nASNQ 55.06 0.557 0.677 44.31 0.395 0.567 44.19 0.369 0.561\nTANDA (ASNQ→NAD) 58.70 0.585 0.703 58.68 0.474 0.683 49.42 0.391 0.613\nLarge\nNAD 53.85 0.537 0.671 53.29 0.469 0.629 43.61 0.395 0.558\nASNQ 57.49 0.552 0.686 50.89 0.440 0.630 45.93 0.399 0.585\nTANDA (ASNQ→NAD) 61.54 0.607 0.725 63.47 0.514 0.727 51.16 0.439 0.616\nRoBERTa\nBase\nNAD 59.11 0.563 0.699 56.29 0.511 0.670 48.26 0.430 0.612\nASNQ 58.70 0.587 0.707 54.50 0.473 0.656 45.35 0.437 0.608\nTANDA (ASNQ→NAD) 65.59 0.623 0.757 62.87 0.537 0.714 56.98 0.473 0.679\nLarge\nNAD 70.81 0.654 0.796 63.47 0.581 0.734 52.91 0.490 0.651\nASNQ 64.37 0.627 0.750 59.88 0.526 0.705 54.65 0.478 0.674\nTANDA (ASNQ→NAD) 71.26 0.680 0.805 74.85 0.625 0.821 58.14 0.514 0.699\nTable 9: Comparison between FT and TAND A on real-world datasets derived from Alexa Virtual Assistant trafﬁc\nThe statistics of the three sample test sets are reported in\nTable 8. Their aim is to capture variations in terms of ques-\ntion sources and retrieval systems, thus providing more gen-\neral results. Additionally, since questions without annotated\nanswers do not affect system accuracy, the results we pro-\nvide refer to aClean setting, i.e., questions with all positive\nor negative answers are removed (noall+ and noall-).\nData Split Questions QA Pairs Neg. Pos.\nTrain 25,226 134,765 125,779 8,986\nDev 2,802 14,974 14,014 960\nTable 10: Statistics of the Noisy Alexa dataset (NAD)\nWe also built a noisy dataset (NAD) with a similar ap-\nproach to Sample 2, with the restriction of retrieving only\n10 candidates per question. This enables the cheaper anno-\ntation of a larger number of questions, which is important to\nbuild an effective training set. Indeed, the number of ques-\ntions in NAD is one order of magnitude larger than those\nused for the previous samples (test questions). Additionally,\nNAD required an increased velocity of annotations result-\ning in a higher error rate, which we quantify around 20-25%\n(as estimated on a small sample). The statistics of NAD are\npresented in Table 10.\n6.2 Results\nIn these experiments, we used, as usual, ASNQ for the trans-\nfer step, and NAD as our target dataset for the adapt step.\nTable 9 reports the comparative results using simple FT on\nNAD (denoted simply by NAD) and tested on samples 1, 2\nand 3. We note that:\n• applying ASNQ for the transfer step always provides im-\nprovement over FT.\n• BERT Large T\nAND A improves over BERT Base TAND A\nfor all the three different dataset samples.\n• Using TAND A with RoBERTa produces an even higher\nimprovement than with BERT.\n• All these experiments using NAD for training and accu-\nrate datasets for testing, show that TAND A is robust to\nreal-world noise of NAD as it always provides signiﬁ-\ncantly large gains over simple FT.\n7 Conclusions\nIn this paper, we have presented a novel approach for ﬁne-\ntuning pre-trained Transformer models and tested it on a\ngeneral natural language inference task, namely, answer sen-\ntence selection (AS2). Our approach, T\nAND A, performs two\nﬁne-tuning steps sequentially: (i) on a general, large and\nhigh-quality dataset, which transfers a pre-trained model\nto the target task; and (ii) on the target dataset to perform\ndomain adaptation. The results on two well-known AS2\ndatasets, WikiQA and TREC-QA, show an impressive im-\nprovement over the state of the art. Additionally, our exper-\niments in an industrial setting derive the same results and\nconclusions we found with the academic benchmarks.\nOur research deepens the general understanding of trans-\nfer learning for Transformer models. The ﬁrst step of\nT\nAND A produces an intermediate model with three main\nfeatures: (i) it can be more effectively used for ﬁne-tuning\non the target NLP application, being more stable and easier\nto adapt to other tasks; (ii) it is robust to noise, which might\naffect the target domain data; and (iii) it enables modular-\nity and efﬁciency, i.e., once a Transformer model is adapted\nto the target general task, e.g., AS2, only the adapt step\nis needed for each targeted domain. This is an important\nadvantage in terms of scalability as the data of (possibly\nmany) different target domains can be typically smaller than\nthe dataset for the transfer step(ASNQ), thereby causing the\nmain computation to be factorized on the initial transfer step.\nWe conjecture that one caveat of using simple ﬁne-tuning\non a combination of ASNQ and target data may produce\nan accuracy improvement similar to T\nAND A. However,\nsuch a combination can be tricky to optimize as the target\ndata requires greater weighting than the more general data\nof ASNQ during ﬁne-tuning the model. Our experiments\nwith TREC-QA show that a simple union of the dataset\nwith ASNQ is sub-optimal than sequential ﬁne-tuning over\nASNQ followed by TREC-QA. In any case, the important\nmodular aspect of T\nAND A will not hold in such a scenario.\nInteresting future work can be devoted to address the\nquestion about the applicability and generalization of the\n7787\nTAND A approach to other NLP tasks. In the speciﬁc con-\ntext of AS2, it would be interesting to test if ASNQ can pro-\nduce the same beneﬁts for related but clearly different tasks,\ne.g., paraphrasing or textual entailment, where the relation\nbetween the members of text pairs are often different from\nthose occurring between questions and answers.\nReferences\nBian, W.; Li, S.; Yang, Z.; Chen, G.; and Lin, Z. 2017. A compare-\naggregate model with dynamic-clip attention for answer selection.\nIn CIKM 2017, CIKM ’17, 1987–1990. New Y ork, NY , USA:\nACM.\nChen, D.; Fisch, A.; Weston, J.; and Bordes, A. 2017.\nReading wikipedia to answer open-domain questions. CoRR\nabs/1704.00051.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J. G.; Le, Q. V .; and\nSalakhutdinov, R. 2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context.CoRR abs/1901.02860.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018. BERT:\npre-training of deep bidirectional transformers for language under-\nstanding. CoRR abs/1810.04805.\nHe, H., and Lin, J. 2016. Pairwise word interaction modeling\nwith deep neural networks for semantic similarity measurement.\nIn NAACL 2016: Human Language Technologies, 937–948. San\nDiego, California: Association for Computational Linguistics.\nKingma, D. P ., and Ba, J. 2014. Adam: A method for stochastic\noptimization. CoRR abs/1412.6980.\nKwiatkowski, T.; Palomaki, J.; Redﬁeld, O.; Collins, M.; Parikh,\nA.; Alberti, C.; Epstein, D.; Polosukhin, I.; Kelcey, M.; Devlin,\nJ.; Lee, K.; Toutanova, K. N.; Jones, L.; Chang, M.-W.; Dai, A.;\nUszkoreit, J.; Le, Q.; and Petrov, S. 2019. Natural questions: a\nbenchmark for question answering research.TACL.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019.\nRoberta: A robustly optimized BERT pretraining approach.CoRR\nabs/1907.11692.\nLuong, M.-T., and Manning, C. D. 2016. Achieving open vocabu-\nlary neural machine translation with hybrid word-character models.\nIn Association for Computational Linguistics (ACL).\nMin, S.; Seo, M.; and Hajishirzi, H. 2017. Question answering\nthrough transfer learning from large ﬁne-grained supervision data.\nIn ACL 2017, 510–517. V ancouver, Canada: Association for Com-\nputational Linguistics.\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.;\nLee, K.; and Zettlemoyer, L. 2018. Deep contextualized word\nrepresentations. CoRR abs/1802.05365.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2018. Language models are unsupervised multitask\nlearners.\nRajpurkar, P .; Zhang, J.; Lopyrev, K.; and Liang, P . 2016. Squad:\n100, 000+ questions for machine comprehension of text. CoRR\nabs/1606.05250.\nSeo, M. J.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2016.\nBidirectional attention ﬂow for machine comprehension. CoRR\nabs/1611.01603.\nSeveryn, A., and Moschitti, A. 2015. Learning to rank short text\npairs with convolutional deep neural networks. InProceedings of\nthe 38th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, SIGIR ’15, 373–382. New\nY ork, NY , USA: ACM.\nSeveryn, A., and Moschitti, A. 2016. Modeling relational informa-\ntion in question-answer pairs with convolutional neural networks.\nCoRR abs/1604.01178.\nShen, G.; Yang, Y .; and Deng, Z.-H. 2017. Inter-weighted align-\nment network for sentence pair modeling. InEMNLP 2017, 1179–\n1189. Copenhagen, Denmark: Association for Computational Lin-\nguistics.\nSun, C.; Qiu, X.; Xu, Y .; and Huang, X. 2019. How to ﬁne-tune\nBERT for text classiﬁcation?CoRR abs/1905.05583.\nTay, Y .; Tuan, L. A.; and Hui, S. C. 2018. Multi-cast attention\nnetworks for retrieval-based question answering and response pre-\ndiction. CoRR abs/1806.00778.\nTayyar Madabushi, H.; Lee, M.; and Barnden, J. 2018. Integrat-\ning question classiﬁcation and deep learning for improved answer\nselection. In Proceedings of the 27th International Conference on\nComputational Linguistics, 3283–3294. Santa Fe, New Mexico,\nUSA: Association for Computational Linguistics.\nTran, Q. H.; Lai, T.; Haffari, G.; Zukerman, I.; Bui, T.; and Bui,\nH. 2018. The context-dependent additive recurrent neural net. In\nNAACL 2018: Human Language Technologies, V olume 1 (Long Pa-\npers), 1274–1283. New Orleans, Louisiana: Association for Com-\nputational Linguistics.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is\nall you need. InAdvances in Neural Information Processing Sys-\ntems 30. Curran Associates, Inc. 5998–6008.\nWang, S., and Jiang, J. 2016. A compare-aggregate model for\nmatching text sequences.CoRR abs/1611.01747.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman,\nS. R. 2018. GLUE: A multi-task benchmark and analysis platform\nfor natural language understanding.CoRR abs/1804.07461.\nWang, R.; Su, H.; Wang, C.; Ji, K.; and Ding, J. 2019. To\ntune or not to tune? how about the best of both worlds?ArXiv\nabs/1907.05338.\nWang, M.; Smith, N. A.; and Mitamura, T. 2007. What is the Jeop-\nardy model? a quasi-synchronous grammar for QA. In(EMNLP-\nCoNLL) 2007, 22–32. Prague, Czech Republic: Association for\nComputational Linguistics.\nYang, L.; Ai, Q.; Guo, J.; and Croft, W. B. 2018. anmm: Rank-\ning short answer texts with attention-based neural matching model.\nCoRR abs/1801.01641.\nYang, W.; Xie, Y .; Lin, A.; Li, X.; Tan, L.; Xiong, K.; Li, M.; and\nLin, J. 2019. End-to-end open-domain question answering with\nBERTserini. In NAACL 2019, 72–77. Minneapolis, Minnesota:\nAssociation for Computational Linguistics.\nYang, Y .; Yih, W.-t.; and Meek, C. 2015. WikiQA: A challenge\ndataset for open-domain question answering. In EMNLP 2015,\n2013–2018. Lisbon, Portugal: Association for Computational Lin-\nguistics.\nY oon, S.; Dernoncourt, F.; Kim, D. S.; Bui, T.; and Jung, K. 2019.\nA compare-aggregate model with latent clustering for answer se-\nlection. CoRR abs/1905.12897.\n7788",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8200370669364929
    },
    {
      "name": "Transformer",
      "score": 0.733258068561554
    },
    {
      "name": "Sentence",
      "score": 0.7009176015853882
    },
    {
      "name": "Inference",
      "score": 0.6245733499526978
    },
    {
      "name": "Task (project management)",
      "score": 0.5068872570991516
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5018167495727539
    },
    {
      "name": "Domain adaptation",
      "score": 0.4796050786972046
    },
    {
      "name": "Transfer of learning",
      "score": 0.4594470262527466
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.42880091071128845
    },
    {
      "name": "Machine learning",
      "score": 0.42481717467308044
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    }
  ]
}