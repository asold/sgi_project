{
  "title": "Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy",
  "url": "https://openalex.org/W4385570521",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2795141342",
      "name": "Zekai Chen",
      "affiliations": [
        "Bristol-Myers Squibb (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4281957673",
      "name": "Mariann Micsinai Balan",
      "affiliations": [
        "Bristol-Myers Squibb (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2077831040",
      "name": "Kevin Brown",
      "affiliations": [
        "Bristol-Myers Squibb (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4319300504",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2934399013",
    "https://openalex.org/W4283318146",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4287554008",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4281644150",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3167198381",
    "https://openalex.org/W2753919178",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2340636398",
    "https://openalex.org/W2993873509",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W4307001389",
    "https://openalex.org/W3149054887",
    "https://openalex.org/W2019607817",
    "https://openalex.org/W4385573954",
    "https://openalex.org/W1991418021",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2066671159",
    "https://openalex.org/W2988630324",
    "https://openalex.org/W4286985375",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3210650932",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients’ clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 5: Industry Track, pages 332–340\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nBoosting Transformers and Language Models for Clinical Prediction in\nImmunotherapy\nZekai Chen and Mariann Micsinai Balanand Kevin Brown\nBristol-Myers Squibb, NJ, USA\n{zekai.chen}@bms.com\nAbstract\nClinical prediction is an essential task in the\nhealthcare industry. However, the recent suc-\ncess of transformers, on which large language\nmodels are built, has not been extended to\nthis domain. In this research, we explore the\nuse of transformers and language models in\nprognostic prediction for immunotherapy using\nreal-world patients’ clinical data and molecu-\nlar proﬁles. This paper investigates the poten-\ntial of transformers to improve clinical predic-\ntion compared to conventional machine learn-\ning approaches and addresses the challenge of\nfew-shot learning in predicting rare disease ar-\neas. The study benchmarks the efﬁcacy of base-\nlines and language models on prognostic pre-\ndiction across multiple cancer types and inves-\ntigates the impact of different pretrained lan-\nguage models under few-shot regimes. The\nresults demonstrate signiﬁcant improvements\nin accuracy and highlight the potential of NLP\nin clinical research to improve early detection\nand intervention for different diseases.\n1 Introduction\nPredicting and measuring treatment response is\namong the most fundamental tasks in clinical\nmedicine. Particularly, in cancer immunother-\napy (Pardoll, 2012), antibodies against pro-\ngrammed death-1/programmed death ligand 1 (PD-\n1/PD-L1) have led to US FDA approval of several\nPD-1/PD-L1 treatment strategies for patients with\nmetastatic cancer. However, not all patients derive\nclinical beneﬁts (Topalian et al., 2016), empha-\nsizing the need to identify who will respond to im-\nmunotherapy (Chowell et al., 2021). Thus, accurate\ntreatment response and disease progress forecast\nbased on the patient’s clinical features and molec-\nular proﬁle will effectively improve the treatment\nefﬁciency and spur the development of precise med-\nication. In order to facilitate medical decision-\nmaking and health outcomes, clinical prediction\nmodels (Steyerberg, 2008; Smeden et al., 2021)\n6 12 18 24 30 36 42 48\nNumber of Samples\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Area Under Curve w.r. Best Overall Response\nLogistic Regression\nRandom Forest\nXgBoost\nTransformer\nBaseline\nFigure 1: Pilot study. We evaluate the prediction perfor-\nmance (AUC) of a patient’s probability of immunother-\napy response across multiple cancer types under settings\nwith a small number of training samples on a public clin-\nical dataset from Chowell et al. (2021).\nplay an increasingly crucial role in contemporary\nclinical care by informing professionals, patients,\nand their relatives about outcome risks.\nGiven the fact that most clinical data is stored in\ntabular form, current mainstream machine learning\napproaches (Topol, 2019; Rajkomar et al., 2019)\nto cancer prognosis (Chowell et al., 2021) are\nstill tree-based ensemble models such as boost-\ning (Chen and Guestrin, 2016; Ke et al., 2017) and\nbagging (Breiman, 2004; Ishwaran et al., 2019). In\ncontrast, transformers (Vaswani et al., 2017) have\nrevolutionized enormous ﬁelds including natural\nlanguage processing (NLP) (Devlin et al., 2019;\nBrown et al., 2020) and computer vision (Doso-\nvitskiy et al., 2021). Many attempts to apply\ntransformers on tabular modeling (e.g., TabTrans-\nformer Huang et al., 2020) have also achieved suc-\ncess. Considering that the disparity between clini-\ncal data and other natural tabular data is not large,\nit is appealing that we can also translate this suc-\ncess from other domains to clinical prediction. As\nsuch, we seek to answer the ﬁrst question in this\n332\nCancer\nType Age Albumin Drug \nClass OS Months\nBladder 74 4.1 PD1/PDL1 9.1 \nNSCLC 72 4.2 Combo 27.3 \nInput: A patient has been diagnosed with\nBladder cancer. The age is 74. The albumin is\n4.1. The drug class is PD1/PDL1, etc. \nTemplate: The {attribute} is {value}.\nLLMs    \nEncoder    \nFinetuning\nMulti-Loss\nObjetive\nSerialization\nFigure 2: An illustration of adapting LLMs for clinical prediction.The clinical data entry is ﬁrst serialized into\nsequences of natural language tokens and then fed into the frozen LLMs, followed by a randomly initialized encoder\n(transformers or MLPs or identical blocks) to ﬁnetune with the multi-loss objective same as Eq. 1.\npaper: To what extent can transformers promote\nthe performance of clinical prediction compared to\nconventional machine learning approaches?\nAlthough transformers have advantages in mod-\neling high-dimensional tabular data thanks to the\ncapacity of long-distance dependency modeling,\ntheir efﬁcacy can still be hampered when labeled\ndata is scarce given the nature of data-hungry and\nlow inductive bias (d’Ascoli et al., 2021). This\ncould be vital to predicting many rare disease areas\nwhere historical patient records are extremely lim-\nited (Haendel et al., 2019). Our pilot investigations\n(see Figure 1) also conﬁrmed this. Meanwhile, we\nseek to provide a systematic solution to the clini-\ncal prediction that functions both in the presence\nand absence of much labeled data. Recently, large\nlanguage models (LLMs) built as a stack of trans-\nformers such as BERT (Devlin et al., 2019), GPT-\n3 (Brown et al., 2020) provide a viable direction.\nThe simple and scalable self-supervised learning\n(e.g., masked signal prediction (Devlin et al., 2019;\nChen et al., 2022)) on a nearly unlimited corpus\nof text (e.g., PubMed1, PMC2) has led LLMs to\nnot only continuous performance improvements\nbut also a surprising emergence of in-context learn-\ning capability, which is especially powerful under\nsettings with only a small number of learning sam-\nples also known as few-shot learning (Snell et al.,\n2017; Sanh et al., 2022). Though recent work has\ndemonstrated that LLMs are good few-shot clinical\ninformation extractors (Agrawal et al., 2022), this\nsuccess has yet not been extended to tasks with a\nhigher precision requirement, such as cancer prog-\nnostic prediction. In this work, we therefore seek\nto address this second question: How can language\nmodels boost clinical prediction in few-shot set-\ntings?\nIn addressing these questions, we conduct a\n1https://pubmed.ncbi.nlm.nih.gov/\n2https://www.ncbi.nlm.nih.gov/pmc/\nbenchmarking study on a real-world clinical dataset\nMSK-IMPACT (Chowell et al., 2021) to assess\nthe efﬁcacy of a set of baselines and LLMs on\nprognostic prediction across multiple cancer types\n(melanoma, NSCLC, bladder, etc.). More impor-\ntantly, we explore how different pretrained LLMs\nusing different knowledge resources (domain-\nspeciﬁc or domain-agnostic) may affect the down-\nstream performance of clinical prediction, espe-\ncially under few-shot settings. Our results show sig-\nniﬁcant improvements in accuracy through overall\nsurvival, progression-free survival and best overall\nresponse prediction across multiple disease types.\n2 LLMs for Few-Shot Clinical Prediction\nFigure 2 is an overview of applying LLMs for clin-\nical prediction. As discussed in Section 1, purely\nsupervised learning via transformer encoders is of-\nten hampered when training samples are limited.\nLLMs provide a viable direction with astonishing\nin-context learning capability that exploits knowl-\nedge from other resources to downstream tasks\nwith minimal tuning.\nSerialization. To leverage LLMs on clinical tab-\nular data, the feature columns must be serialized\ninto sequences of natural language tokens that\nLLMs can comprehend and encode. Recently, there\nhave been a few trials (Yin et al., 2020; Bertsi-\nmas et al., 2022) investigating various serializa-\ntion techniques and exploring the corresponding\nperformance across different tasks, which turns\nout that LLMs for tabular modeling rely more on\nthe correct values than the structure of the fea-\ntures (Hegselmann et al., 2022). To avoid repetitive\nwork, in this work, we focus more on how dif-\nferent pretrained LLMs using different knowledge\nsources may affect the prediction performance by\nsimply following a manual serialization template,\nThe {attribute} is {value}., which has been\nproven to generate competitive results compared to\n333\nother LLMs prompting-based regeneration meth-\nods by Hegselmann et al. (2022).\nKnowledge Sources. The pretraining corpus is\nalso known as the knowledge source for LLMs.\nClinical language is notably different from the stan-\ndard NLP text in terms of vocabulary and syn-\ntax (Wu et al., 2019). As a result, following ad-\nvancements in language modeling from the larger\nNLP community, the clinical NLP sub-community\nfrequently trains domain-speciﬁc models on clin-\nical corpora. Following BERT (Devlin et al.,\n2019), various clinical and biomedical versions\nappeared quickly, including BioBERT (Lee et al.,\n2019), ClinicalBERT (Alsentzer et al., 2019), SciB-\nERT (Beltagy et al., 2019), PubMedBERT (Gu\net al., 2020), etc. However, domain-agnostic LLMs\nlike GPT-3 have so far been unable to achieve com-\npetitive results on biomedical NLP tasks (Moradi\net al., 2021; Gutierrez et al., 2022), revealing the\nfact that the relevance and the knowledge reserva-\ntion of pretraining sources have a signiﬁcant impact\nto the knowledge migration in downstream tasks\n(e.g., ﬁnetuning or prompting). Thus, we aim to\nevaluate the downstream performance in few-shot\nsettings with a few different LLMs pretrained on\ndifferent resources and benchmark the gaps.\nOmnivorous Loss Objective. Compared to con-\nventional machine learning approaches, deep learn-\ning allows efﬁcient end-to-end learning of im-\nage/text encoders in the presence of multi-modality\nalong with tabular data beneﬁting from the modu-\nlarized design. More importantly, the customized\nloss objectives corresponding to different tasks can\noften be combined for joint training, also known\nas multi-task learning (Ruder, 2017). The induc-\ntive transfer across related tasks can help improve\na model by introducing an inductive bias, which\ncauses a model to prefer some hypotheses over\nothers, that generally leads to solutions that gen-\neralize better. In cancer prognostic prediction, we\nusually have multiple endpoints to predict. For\nexample, overall survival (OS), progression-free\nsurvival (PFS), and best overall response (BOR),\netc. As such, in this work, we consistently adopt a\njoin learning paradigm that merges multiple end-\npoints into one uniﬁed loss objective Lf for all\nstudies using the following term:\nLf =\nI∑\ni\nαiℓi (1)\nFlattened\nT ransformer\nCategorical\nEmbedding Layer Normalize\nCategorical\nFeatures\nContinuous\nFeatures\nConcat\nCategorical\nEmbedding Layer\nCategorical\nFeatures\nContinuous\nFeatures\nMLPMLP\nContinuous\nEmbedding Layer\nConcat\nT ransformer\nT abT ransformer ClinT aT\nFigure 3: An illustration of ClinTaT (right).Com-\npared to original TabTransformer (left), we add a contin-\nuous embedding layer for modeling continuous features\n(e.g., lab values) and feed the concatenated inputs into\nthe transformer backbone.\nwhere I is the total number of tasks and αi repre-\nsents the soft weight for any task i. More speciﬁ-\ncally, in our experiments, we adopt CrossEntropy\nloss for BOR and CoxPH loss for OS and PFS pre-\ndiction following DeepSurv (Katzman et al., 2018).\n3 Experiments and Results\nData. This dataset is acquired by Memorial\nSloan Kettering Cancer Center (MSKCC) from a\ncomprehensively curated cohort (MSK-IMPACT)\nwith 1,479 patients treated with immune check-\npoint blockade (ICB) across 16 different cancer\ntypes (Chowell et al., 2021), where patients are\neither responder (R) or non-responders (NR) to\nthe treatment (PD-1/PD-L1 inhibitors, CTLA-4\nblockade or a combination) based on Response\nEvaluation Criteria in Solid Tumors (RECIST)\nv1.1 (Eisenhauer et al., 2009) or best overall re-\nsponse on imaging. Each patient was collected\nup to 16 biological features, including genomic,\nmolecular, clinical, and demographic variables.\nThe train set contains 1,184 patients, and the test\nset contains 295 patients. The evaluation target is\nto predict clinical response to immunotherapy (bi-\nnary classiﬁcation) and both overall survival and\nprogression-free survival (regression) in the test\ndata across different cancer.\nTransformers for Tabular Modeling. As we\nneed to compare with transformer baselines, we\nalso introduce ClinTaT (see Figure 3 right) with\nsome improvements based on the original Tab-\nTransformer (Huang et al., 2020), including 1)\nadding a continuous embedding layer which is con-\nsisted of several independent linear layers corre-\nsponding to the number of continuous features; 2)\ndirectly concatenating the embedded categorical\n334\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nMelanoma\nLR (AUC: 0.788)\nRF (AUC: 0.776)\nXGB (AUC: 0.785)\nClinT aT (AUC: 0.797)\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nNSCLC\nLR (AUC: 0.768)\nRF (AUC: 0.825)\nXGB (AUC: 0.828)\nClinT aT (AUC: 0.809)\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nOthers\nLR (AUC: 0.743)\nRF (AUC: 0.785)\nXGB (AUC: 0.791)\nClinT aT (AUC: 0.800)\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nPan-Cancer\nLR (AUC: 0.758)\nRF (AUC: 0.795)\nXGB (AUC: 0.796)\nClinT aT (AUC: 0.815)\nFigure 4: Model performance across multiple cancer types on test data.Comparison of predictive performance\non MSK-IMPACT in terms of ROC curves and AUC between ClinTaT and other baselines in melanoma, NSCLC,\nother cancer types and Pan-cancer.\n0 6 12 18 24 30 36 42 48\nOverall survival (months)\n0.00\n0.25\n0.50\n0.75\n1.00Survival probability\nMelanoma\nGroundtruth R\nGroundtruth NR\nPredicted R\nPredicted NR\nPredicted NR\n   At risk   11\n  Censored     0\n    Events     0\nPredicted R\n   At risk   26\n  Censored     0\n    Events     0\n4\n0\n7\n24\n0\n2\n3\n0\n8\n19\n2\n5\n3\n0\n8\n14\n6\n6\n2\n1\n8\n9\n11\n6\n1\n2\n8\n4\n14\n8\n0\n3\n8\n2\n16\n8\n0\n3\n8\n0\n18\n8\n0\n3\n8\n0\n18\n8\n0 6 12 18 24 30 36 42 48\nOverall survival (months)\n0.00\n0.25\n0.50\n0.75\n1.00Survival probability\nNSCLC\nGroundtruth R\nGroundtruth NR\nPredicted R\nPredicted NR\nPredicted NR\n   At risk   61\n  Censored     0\n    Events     0\nPredicted R\n   At risk   47\n  Censored     0\n    Events     0\n38\n2\n21\n35\n0\n12\n24\n4\n33\n30\n0\n17\n12\n11\n38\n16\n13\n18\n4\n15\n42\n12\n17\n18\n3\n15\n43\n6\n19\n22\n2\n16\n43\n2\n23\n22\n0\n17\n44\n0\n24\n23\n0\n17\n44\n0\n24\n23\n0 6 12 18 24 30 36 42 48\nOverall survival (months)\n0.00\n0.25\n0.50\n0.75\n1.00Survival probability\nPan-Cancer\nGroundtruth R\nGroundtruth NR\nPredicted R\nPredicted NR\nPredicted NR\n   At risk   179\n  Censored       0\n    Events       0\nPredicted R\n   At risk   116\n  Censored       0\n    Events       0\n109\n4\n66\n95\n0\n21\n68\n12\n99\n80\n3\n33\n32\n34\n113\n53\n24\n39\n13\n48\n118\n34\n41\n41\n8\n51\n120\n15\n52\n49\n3\n56\n120\n5\n61\n50\n1\n57\n121\n1\n64\n51\n0\n58\n121\n0\n65\n51\nFigure 5: Model predicts OS and PFS across multiple cancer types on the test data.Comparison of differences\nin overall survival between predicted responders and non-responders across multiple cancer types by ClinTaT.\nand continuous variables together, and feed them\ninto the transformer instead of only categorical vari-\nables.\nTraining settings. For fair comparison, we adopt\na hidden dimensionality of 768 for both ClinTaT\nand BERTs (base versions). Speciﬁcally, ClinTaT\nis a stack of 6 transformer encoder layers with 8\nheads. To prevent overﬁtting, we set the attention\ndropout rate to 0.3 and feedforward dropout rate\nto 0.1. For BERTs, all layers are frozen while\nwe add one independent encoder on top of it to\nﬁnetune. In the main ﬁgures and tables, we utilize\na single linear layer to demonstrate the feasibility\nof LLMs for few-shot regimes. In ablation studies,\nwe also investigate other encoder types such as\nanother small transformer encoder. The optimizer\nof AdamW is adopted consistently for all trainings,\nand the basic learning rates for ClinTaT and BERTs\nare 1.25e−4 and 1.25e−5 with a weight decay of\n0.01, correspondingly. A linear warmup (up to 5\nepochs with a total training of 200 epochs) with\ncosine annealing strategy (warmup learning rate is\nset to 2.5e−7) is also applied. For other machine\nlearning baselines, we utilize the grid search to ﬁnd\nthe optimal hyper-parameters and report the best\nresults. More details can be found in the appedix.\nHow do transformers promote clinical predic-\ntion performance? We ﬁrst calculated the area\nunder the receiver operating characteristic (ROC)\ncurves using the response probabilities computed\nby transformers and other baselines. Our proposed\nClinTaT achieved superior performance on the test\nset, as indicated by the area under the curve (AUC),\nin predicting responders and non-responders across\ncancer types compared to conventional machine\nlearning models such as logistic regression, ran-\ndom forest, and XgBoost, suggesting that the self-\nattention mechanism for long-range dependency\nmodeling contributed to the overall prediction per-\nformance. (Figure 4, Table 1 using all samples).\nFurthermore, the differences in OS between respon-\nders and non-responders predicted by transformers\nwere signiﬁcantly higher than differences between\nresponder and non-responder groups predicted by\nother baselines across various cancer types (Fig-\nure 5). Especially for the predicted non-responders,\nthe predicted survival curves almost ﬁt the ground-\ntruth ones perfectly, while it is interesting to ob-\nserve that transformers tend to underestimate the\nresponse probability with an attempt to balance out\nthe prediction performance across different can-\ncer types compared to other baselines ( 0.809 of\n335\nModel Number of Samples\n6 12 18 24 30 36 42 48 all\nLogRes 0.534 0.535 0.573 0.511 0.527 0.570 0.601 0.678 0.758\nRandomForest0.643 0.527 0.672 0.539 0.594 0.667 0.651 0.701 0.795\nXgBoost 0.500 0.602 0.670 0.586 0.613 0.664 0.651 0.681 0.796\nClinTaTours 0.641 0.619 0.653 0.607 0.584 0.659 0.664 0.676 0.815\nTable 1: Test AUC performance on treatment response prediction of ClinTaT and other baselines on MSK-IMPACT.\nEach column reports the k-shot performance for different values of k. ClinTaT outperforms other traditional\napproaches with all training samples, however not signiﬁcant in the most few-shot regimes.\nModel Number of Samples\n6 12 18 24 30 36 42 48 all\nLogRes 0.500 0.503 0.551 0.511 0.545 0.557 0.549 0.564 0.649\nRandomForest0.637 0.502 0.614 0.536 0.591 0.610 0.626 0.631 0.682\nXgBoost 0.500 0.555 0.601 0.539 0.618 0.628 0.614 0.609 0.688\nClinTaTours 0.583 0.615 0.614 0.639 0.610 0.647 0.643 0.645 0.724\nTable 2: TestC-index performance on Overall Survival prediction of ClinTaT and other baselines on MSK-IMPACT.\nClinTaT generally outperforms other traditional approaches under many settings, however still not signiﬁcant in the\nvery-few-shot regime (e.g., ≤ 6 samples).\nModel Number of Samples\n6 12 18 24 30 36 42 48 all\nLogRes 0.515 0.513 0.538 0.514 0.537 0.549 0.565 0.596 0.648\nRandomForest0.611 0.529 0.612 0.532 0.580 0.619 0.615 0.627 0.666\nXgBoost 0.500 0.514 0.594 0.569 0.600 0.619 0.612 0.620 0.671\nClinTaTours 0.585 0.505 0.547 0.520 0.538 0.553 0.555 0.617 0.684\nTable 3: Test C-index performance on Progression-free Survival prediction of ClinTaT and other baselines on\nMSK-IMPACT. ClinTaT performs better than other approaches only with all training samples.\nClinTaT versus 0.828 of XGB in Fig. 4). It is addi-\ntionally beneﬁcial to rare diseases prediction when\nthe training sample pool is not large.\nTo test whether our approach could also pre-\ndict overall survival (OS) before the administration\nof immunotherapy, we further calculated the con-\ncordance index (C-index) for OS and PFS, which\nranges between 0 and 1 (0.5 being random perfor-\nmance). We found that the C-indices of the ClinTaT\npredictions were signiﬁcantly higher than those\ngenerated by other baselines (Table 2, pan-cancer\nC-index 0.724 for ClinTaT versus 0.688 for Xg-\nBoost versus 0.682 for Random Forest, p <0.05;\nTable 3, pan-cancer C-index 0.684 for ClinTaT ver-\nsus 0.671 for XgBoost versus 0.666 for Random\nForest, p <0.05). These results demonstrate that\nthe transformers can accurately forecast response,\nOS, and PFS before administering immunotherapy.\nHowever, Table 1, 2 and 3 also show that under\nsettings with only a small number of samples, the\nprediction capability of transformers does not gen-\neralize well (e.g., 0.583 for ClinTaT versus 0.637\nfor Random Forest with only 6 samples on OS pre-\ndiction; 0.585 for ClinTaT versus 0.611 for Ran-\ndom Forest with only 6 samples on PFS prediction)\ndue to the nature of data-hungry and low inductive\nbias (discussed in Section 1).\nHow do LLMs boost few-shot learning?Ta-\nble 4 shows the performance of different BERTs\npretrained on different resource corpus followed\nby a single linear layer for ﬁnetuning using only\n[cls] token on MSK-IMPACT test data (averaged\nover three seeds). The PubMedBERT (Gu et al.,\n2020) outperforms all other variants and the base-\nline transformer across all k-shot settings with an\naverage of improvements over 5%. In the very few\nshot settings (4 samples), the language model ﬁne-\ntuning shows signiﬁcant improvements over the\nbaseline (Table 4,9.4%), indicating the beneﬁt of\nthe capability of knowledge transferring to down-\nstream tasks brought by LLMs when samples are\ninsufﬁcient. Also, our results indicate that the sam-\n336\nModel Number of Samples\n4 6 8 10 12 14 16 18\nClinTaTbaseline 0.593 0.641 0.638 0.628 0.619 0.643 0.639 0.653\nBERT (Devlin et al., 2019) 0.590 0.618 0.652 0.636 0.633 0.637 0.632 0.631\nBioBERT (Lee et al., 2019) 0.570 0.512 0.527 0.532 0.536 0.532 0.524 0.530\nSciBERT (Beltagy et al., 2019) 0.506 0.506 0.578 0.577 0.560 0.549 0.513 0.557\nClinBERT (Alsentzer et al., 2019) 0.604 0.550 0.545 0.560 0.567 0.576 0.574 0.558\nPubMedBERT (Gu et al., 2020)0.649(↑9.4%) 0.643(↑0.3%) 0.641(↑0.5%) 0.657(↑4.6%) 0.663(↑7.1%) 0.677(↑5.3%) 0.695(↑8.8%) 0.685(↑4.9%)\nTable 4: Few-shot learning AUC performance of ClinTaT and variants of language models pretrained with different\ncorpus sources on MSK-IMPACT. Best results are in bold and the relative improvements have been marked in\npurple. PubMedBERT (Gu et al., 2020) generally outperforms all the other variants across most settings with an\naverage of improvements over 5%.\nBackbone Encoder AUC C OS CPFS\nBERT linear 0.725 0.593 0.622\ntransformer 0.773 0.699 0.657\nBioBERT linear 0.678 0.590 0.625\ntransformer 0.766 0.707 0.672\nSciBERT linear 0.689 0.588 0.620\ntransformer 0.786 0.711 0.656\nClinBERT linear 0.669 0.591 0.616\ntransformer 0.751 0.719 0.665\nPubMedBERT linear 0.745 0.599 0.634\ntransformer 0.771 0.700 0.662\nTable 5: Ablation study on applying different encoders\nfor ﬁnetuning of treatment response prediction, includ-\ning a simple linear layer and a six-layer transformer\nencoder. Best results across backbones are in bold. Best\nresults across encoders are marked by purple. An addi-\ntional transformer encoder on top of LLMs consistently\nperforms better than a simple linear layer.\nple efﬁciency of using LLMs’ embeddings is highly\ndomain knowledge dependent. The performance\nof SciBERT is worse than that of BioBERT and\nClinicalBERT as SciBERT was pretrained on all\nsemantic scholar 1.14M articles towards a more\ngeneral scientiﬁc knowledge learning.\nIn contrast, BioBERT and ClinicalBERT were\npretrained on the more domain-speciﬁc corpus,\nsuch as PubMed, PMC, and clinical MIMIC III\nnotes3. However, we cannot claim that domain-\nspeciﬁc pretraining is necessary for all clinical pre-\ndiction tasks as Table 4 also reveals that vanilla\nBERT is the second best and performs even bet-\nter than SciBERT pretrained on medical and com-\nputer science articles. As we know, vanilla BERT\nlearns more general knowledge understanding from\ndomain-agnostic corpora such as Wikipedia and\nBook corpus. One of our preliminary conjectures\nis that domain-speciﬁc knowledge transfer is su-\n3https://mimic.mit.edu/\nperior when the pretraining corpus is sufﬁciently\nprofound. However, the generalization capability\nlearned by domain-agnostic models also works un-\nder scenarios where the resource knowledge is nei-\nther domain-agnostic nor morally domain-speciﬁc.\nAdditionally, the performance down gradation\non BioBERT and ClinicalBERT compared to Pub-\nMedBERT released more interesting ﬁndings as\nPubMedBERT was pretraining from scratch. At the\nsame time, the other two models were pretrained\nby inheriting vanilla BERT and BioBERT v1.0 4,\ncorrespondingly. Gu et al. (2020) has also pointed\nout that pretraining only sometimes beneﬁts from\nmore text, including out-domain text. The prior\nbiomedical-related BERT models have yet to be\npretrained using purely biomedical text. Our Ta-\nble 4 also shows that domain-speciﬁc pretraining\nfrom scratch can be superior to mixed-domain pre-\ntraining for downstream applications.\nThough all the results in Table 4 are generated\nby adding one single linear layer on top of LLMs\nfor ﬁnetuning, we conduct more ablation studies in\nTable 5 to evaluate the performance change using\ndifferent encoders (see Figure 2). The transformer\nin Table 5 consists of only the transformer encoder\nof a depth of six layers with a dimension of 768.\nThe results indicate that adding compute complex-\nity to LLMs can still lift the semantic representa-\ntion learning of clinical features, as transformer\narchitecture performs better than a superﬁcial lin-\near layer. It also provides an alternative way to\nreexamine the right size of LLMs and inspires us\nfor the next step, which is to adopt more scaled\nLLMs such as PubMedGPT5, GPT-3 or T5 (Raffel\net al., 2019) for clinical prediction.\n4https://huggingface.co/dmis-lab/biobert-v1.1\n5https://crfm.stanford.edu/2022/12/15/\npubmedgpt.html\n337\n4 Limitations\nThis study is based on a single clinical cohort con-\nsisted of 1479 patients, which may limit the gen-\neralizability of the results to other clinical cohorts.\nThis speciﬁc cohort of patients may not be represen-\ntative enough of the general population, which may\ninject certain level of bias brought by the dissimilar\ndistributions of gender, age, race, etc. While we en-\nvision the generalization capability of the language\nmodels is applicable to other clinical prediction\ntasks, the focus of this work is majorly about prog-\nnostic prediction of cancer immunotherapy, and we\nhereby have not provided solid evidence to prove\nthat the success can also be extended to other rel-\nevant trials. Additionally, we have yet only com-\npared a limited set of transformers and language\nmodels, and it is possible that other models may\nperform better on the tasks evaluated in this study.\nFinally, it is important to note that while the mod-\nels in this study achieve high accuracy in clinical\nprediction, the ultimate value of these models in im-\nproving patient outcomes will depend on how well\nthey are integrated into clinical decision-making\nprocesses and the impact they have on patient care.\n5 Ethical Considerations\nAs this work uses real-world patients’ clinical data\nand molecular proﬁles, which may raise concerns\nabout data privacy and conﬁdentiality. We ensure\nthat all the patients’ data is de-identiﬁed and pro-\ntected from unauthorized access and use. The pub-\nlic patient data 6 was approved by the Memorial\nSloan Kettering Cancer Center (MSKCC) 7 institu-\ntional review board for scientiﬁc use. Researchers\nhave ensured that they obtain proper ethical ap-\nproval and informed consent from patients before\nusing their data. Even though this is a dataset that\nhas been carefully curated to prevent the negative\nimpact brought by human bias, there maybe ex-\nisting a risk of introducing bias into the clinical\ncohort of data we analyze, particularly in the selec-\ntion of patients and the choice of clinical features\nand molecular proﬁles. Additionally, the use of pre-\ndictive models to guide clinical decision-making\nmight raise concerns about fair access to healthcare.\nWe hereby ensure that the use of predictive models\ndoes not result in the inequitable distribution of\nhealthcare resources and that patients from all so-\ncioeconomic backgrounds have equal access to the\n6http://www.cbioportal.org/\n7https://www.mskcc.org/msk-impact\nbest possible care. This study uses natural language\nprocessing and machine learning algorithms to pre-\ndict disease prognosis, which may raise broader\nethical considerations related to the responsible use\nof technology in healthcare. We ensure that the use\nof all approaches discussed in this work is guided\nby general ethical principles, such as transparency,\naccountability, and patient-centered care.\nEven though we focus on relatively large scale\nlanguage models in this work, our ﬁnetuning strat-\negy only requires a considerably small amount of\ncomputation as only the encoder part needs to be\nﬁnetuned. In practice, the single linear layer ﬁne-\ntuning can be obtained in about 2 hours on a ma-\nchine with single Nvidia A10 GPU; training com-\npletes within 5 hours on a machine with one Nvidia\nA10 GPU for another transformer encoder with\na depth of 6 and dimensionality of 768. All the\npretrained language model weights are publicly\navailable (e.g., huggingface).\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David A. Sontag. 2022. Large lan-\nguage models are zero-shot clinical information ex-\ntractors. ArXiv, abs/2205.12689.\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nB. A. McDermott. 2019. Publicly available clinical\nbert embeddings. ArXiv, abs/1904.03323.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\nert: A pretrained language model for scientiﬁc text.\nEMNLP.\nDimitris Bertsimas, Kimberly Villalobos Carballo,\nYu Ma, Liangyuan Na, Léonard Boussioux, Cyn-\nthia Zeng, Luis R. Soenksen, and Ignacio Fuentes.\n2022. Tabtext: a systematic approach to aggregate\nknowledge across tabular data structures. ArXiv,\nabs/2206.10381.\nL. Breiman. 2004. Random forests. Machine Learning,\n45:5–32.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. NeurIPS,\nabs/2005.14165.\n338\nTianqi Chen and Carlos Guestrin. 2016. Xgboost: A\nscalable tree boosting system. SIGKDD.\nZekai Chen, Devansh Agarwal, Kshitij Aggarwal, Wiem\nSafta, Mariann Micsinai Balan, Venkat S. Sethura-\nman, and Kevin Brown. 2022. Masked image mod-\neling advances 3d medical image analysis. WACV,\nabs/2204.11716.\nDiego Chowell, Seong-Keun Yoo, Cristina Valero,\nAlessandro Pastore, Chirag Krishna, Mark Lee, Dou-\nglas R. Hoen, Hongyu Shi, Daniel W. Kelly, Neal Pa-\ntel, Vladimir Makarov, Xiaoxiao Ma, Lynda Vuong,\nErich Sabio, Kate Weiss, Fengshen Kuo, Tobias L.\nLenz, Robert M. Samstein, Nadeem Riaz, Prasad S.\nAdusumilli, Vinod P. Balachandran, George Plitas,\nA. Ari Hakimi, Omar Abdel-Wahab, Alexander N.\nShoushtari, Michael A. Postow, R. Motzer, Marc\nLadanyi, Ahmet Zehir, Michael F. Berger, Mithat\nGönen, Luc G. T. Morris, Nils Weinhold, and Timo-\nthy A. Chan. 2021. Improved prediction of immune\ncheckpoint blockade efﬁcacy across multiple cancer\ntypes. Nature biotechnology.\nStéphane d’Ascoli, Hugo Touvron, Matthew L. Leav-\nitt, Ari S. Morcos, Giulio Biroli, and Levent Sagun.\n2021. Convit: improving vision transformers with\nsoft convolutional inductive biases. ICLR, 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. NAACL, abs/1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. ICLR, abs/2010.11929.\nE. A. Eisenhauer, Patrick Therasse, Jan Bogaerts,\nLawrence H. Schwartz, Daniel J. Sargent, Robert\nFord, Janet E. Dancey, Susan G. Arbuck, S. Gwyther,\nMargaret Mooney, Larry V . Rubinstein, Lalitha K\nShankar, Lori E. Dodd, Richard S. Kaplan, Denis\nLacombe, and Jaap Verweij. 2009. New response\nevaluation criteria in solid tumours: revised recist\nguideline (version 1.1). European journal of cancer,\n45 2:228–47.\nYuxian Gu, Robert Tinn, Hao Cheng, Michael R. Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedical\nnatural language processing. ACM Transactions on\nComputing for Healthcare (HEALTH), 3:1 – 23.\nBernal Jimenez Gutierrez, Nikolas McNeal, Clay Wash-\nington, You Chen, Lang Li, Huan Sun, and Yu Su.\n2022. Thinking about gpt-3 in-context learning for\nbiomedical ie? think again. ArXiv, abs/2203.08410.\nMelissa A. Haendel, N Vasilevsky, Deepak R. Unni,\nCristian G Bologa, Nomi L. Harris, Heidi L. Rehm,\nAda Hamosh, Gareth S. Baynam, Tudor Groza,\nJulie A. McMurry, Hugh J. S. Dawkins, Ana\nRath, Courtney Thaxon, Giovanni Bocci, marcin p.\njoachimiak, Sebastian Köhler, Peter N. Robinson,\nChris J. Mungall, and Tudor I. Oprea. 2019. How\nmany rare diseases are there? Nature Reviews Drug\nDiscovery, 19:77–78.\nStefan Hegselmann, Alejandro Buendia, Hunter Lang,\nMonica Agrawal, Xiaoyi Jiang, and David A. Sontag.\n2022. Tabllm: Few-shot classiﬁcation of tabular data\nwith large language models. ArXiv, abs/2210.10723.\nXin Huang, Ashish Khetan, Milan W. Cvitkovic, and\nZohar S. Karnin. 2020. Tabtransformer: Tabular\ndata modeling using contextual embeddings. ArXiv,\nabs/2012.06678.\nHemant Ishwaran, Udaya B. Kogalur, Eugene H. Black-\nstone, and Michael S. Lauer. 2019. Random survival\nforests. Wiley StatsRef: Statistics Reference Online.\nJared Katzman, Uri Shaham, Alexander Cloninger,\nJonathan Bates, Tingting Jiang, and Yuval Kluger.\n2018. Deepsurv: personalized treatment recom-\nmender system using a cox proportional hazards deep\nneural network. BMC Medical Research Methodol-\nogy, 18.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,\nWei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.\n2017. Lightgbm: A highly efﬁcient gradient boosting\ndecision tree. In NeurIPS.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36:1234 – 1240.\nMilad Moradi, Kathrin Blagec, Florian Haberl, and\nMatthias Samwald. 2021. Gpt-3 models are poor\nfew-shot learners in the biomedical domain. ArXiv,\nabs/2109.02555.\nDrew M. Pardoll. 2012. The blockade of immune check-\npoints in cancer immunotherapy. Nature Reviews\nCancer, 12:252–264.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2019. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text\ntransformer. ArXiv, abs/1910.10683.\nAlvin Rajkomar, Jeffrey Dean, and Isaac S. Kohane.\n2019. Machine learning in medicine. The New Eng-\nland Journal of Medicine, 380:1347–1358.\nSebastian Ruder. 2017. An overview of multi-task learn-\ning in deep neural networks. ArXiv, abs/1706.05098.\n339\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Stella Rose Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. ICLR, abs/2110.08207.\nMaarten Van Smeden, Johannes B. Reitsma, Richard D.\nRiley, Gary Stephen Collins, and Karel G. M. Moons.\n2021. Clinical prediction models: diagnosis ver-\nsus prognosis. Journal of clinical epidemiology ,\n132:142–145.\nJake Snell, Kevin Swersky, and Richard S. Zemel.\n2017. Prototypical networks for few-shot learning.\nNeurIPS, abs/1703.05175.\nEwout Willem Steyerberg. 2008. Clinical prediction\nmodels: A practical approach to development, vali-\ndation, and updating. In Springer.\nSuzanne L. Topalian, Janis M. Taube, Robert A An-\nders, and Drew M. Pardoll. 2016. Mechanism-driven\nbiomarkers to guide immune checkpoint blockade in\ncancer therapy. Nature Reviews Cancer, 16:275–287.\nEric J. Topol. 2019. High-performance medicine: the\nconvergence of human and artiﬁcial intelligence. Na-\nture Medicine, 25:44–56.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. NeurIPS, abs/1706.03762.\nStephen T Wu, Kirk Roberts, Surabhi Datta, Jingcheng\nDu, Zongcheng Ji, Yuqi Si, Sarvesh Soni, Qiong\nWang, Qiang Wei, Yang Xiang, Bo Zhao, and Hua\nXu. 2019. Deep learning in clinical natural language\nprocessing: a methodical review. Journal of the\nAmerican Medical Informatics Association : JAMIA.\nPengcheng Yin, Graham Neubig, Wen tau Yih, and Se-\nbastian Riedel. 2020. Tabert: Pretraining for joint\nunderstanding of textual and tabular data. ACL,\nabs/2005.08314.\n340",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7332261800765991
    },
    {
      "name": "Computer science",
      "score": 0.6787604093551636
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.624414324760437
    },
    {
      "name": "Gradient boosting",
      "score": 0.6015165448188782
    },
    {
      "name": "Machine learning",
      "score": 0.5409371256828308
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5017774105072021
    },
    {
      "name": "Language model",
      "score": 0.453653484582901
    },
    {
      "name": "Random forest",
      "score": 0.2150638997554779
    },
    {
      "name": "Engineering",
      "score": 0.17133885622024536
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}