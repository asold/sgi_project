{
  "title": "Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance",
  "url": "https://openalex.org/W4389524416",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2109424293",
      "name": "Song Wang",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2102304230",
      "name": "Zhen Tan",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2288386125",
      "name": "Ruocheng Guo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2137915745",
      "name": "Jundong Li",
      "affiliations": [
        "University of Virginia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2765407302",
    "https://openalex.org/W1972675781",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4224903910",
    "https://openalex.org/W3123795136",
    "https://openalex.org/W4287286018",
    "https://openalex.org/W4287757781",
    "https://openalex.org/W2963160702",
    "https://openalex.org/W3209002790",
    "https://openalex.org/W3001197829",
    "https://openalex.org/W4312601326",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2743200750",
    "https://openalex.org/W2964292098",
    "https://openalex.org/W4313068152",
    "https://openalex.org/W3200496214",
    "https://openalex.org/W3121661476",
    "https://openalex.org/W2996108195",
    "https://openalex.org/W4387185354",
    "https://openalex.org/W3196904728",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W3201915713",
    "https://openalex.org/W4284892020",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3129603874",
    "https://openalex.org/W2963096987",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W4287183612",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4386071642",
    "https://openalex.org/W2803187616",
    "https://openalex.org/W1493526108",
    "https://openalex.org/W3154813414",
    "https://openalex.org/W4385568170",
    "https://openalex.org/W3103147437",
    "https://openalex.org/W4287815528",
    "https://openalex.org/W4301183982",
    "https://openalex.org/W2945007112",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W3034185248",
    "https://openalex.org/W4380686953",
    "https://openalex.org/W4287332191",
    "https://openalex.org/W3039366696",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2963735582",
    "https://openalex.org/W4225394309",
    "https://openalex.org/W3205500251",
    "https://openalex.org/W3012687255",
    "https://openalex.org/W2978426779",
    "https://openalex.org/W2885593519",
    "https://openalex.org/W3034891697",
    "https://openalex.org/W2964274690",
    "https://openalex.org/W2971296908"
  ],
  "abstract": "Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it essential to develop strategies for fine-tuning PLMs with such noisy labels. To this end, we introduce an innovative approach for fine-tuning PLMs using noisy labels, which incorporates the guidance of Large Language Models (LLMs) like ChatGPT. This guidance assists in accurately distinguishing between clean and noisy samples and provides supplementary information beyond the noisy labels, thereby boosting the learning process during fine-tuning PLMs. Extensive experiments on synthetic and real-world noisy datasets further demonstrate the superior advantages of our framework over the state-of-the-art baselines.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12528‚Äì12540\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nNoise-Robust Fine-Tuning of Pretrained Language Models\nvia External Guidance\nSong Wang\nUniversity of Virginia\nsw3wv@virginia.edu\nRuocheng Guo\nByteDance Research\nruocheng.guo@bytedance.com\nZhen Tan\nArizona State University\nztan36@asu.edu\nJundong Li\nUniversity of Virginia\njundong@virginia.edu\nAbstract\nAdopting a two-stage paradigm of pretraining\nfollowed by fine-tuning, Pretrained Language\nModels (PLMs) have achieved substantial ad-\nvancements in the field of natural language pro-\ncessing. However, in real-world scenarios, data\nlabels are often noisy due to the complex anno-\ntation process, making it essential to develop\nstrategies for fine-tuning PLMs with such noisy\nlabels. To this end, we introduce an innovative\napproach for fine-tuning PLMs using noisy la-\nbels, which incorporates the guidance of Large\nLanguage Models (LLMs) like ChatGPT. This\nguidance assists in accurately distinguishing\nbetween clean and noisy samples and provides\nsupplementary information beyond the noisy la-\nbels, thereby boosting the learning process dur-\ning fine-tuning PLMs. Extensive experiments\non synthetic and real-world noisy datasets fur-\nther demonstrate the superiority of our frame-\nwork over the state-of-the-art baselines.\n1 Introduction\nIn recent years, the development of language mod-\nels has significantly expanded the applications\nwithin the field of natural language processing\n(NLP). Fine-tuning Pretrained Language Models\n(PLMs) like BERT (Devlin et al., 2018) for specific\ndownstream tasks has become an essential step\nin real-world implementations (Alt et al., 2020;\nWang et al., 2021). In general, achieving signifi-\ncant performance gains in fine-tuning PLMs neces-\nsitates the availability of task-specific data for fine-\ntuning (Zhou and Chen, 2021; Wang et al., 2022).\nHowever, obtaining high-quality labeled datasets\nfor this purpose poses significant challenges due\nto the expensive, complex, and labor-intensive na-\nture of the annotation process (Yu et al., 2019; Bae\net al., 2022). For example, large-scale datasets,\noften derived from web-crawling (Li et al., 2017;\nSong et al., 2019) or crowd-sourcing (Yan et al.,\n2014; Williams et al., 2017; Sakaguchi et al., 2021),\nfrequently suffer from the presence of noisy labels.\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c\n/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000026/uni0000004f/uni00000048/uni00000044/uni00000051/uni00000010/uni00000015/uni00000013/uni00000031/uni0000004a\n/uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c/uni00000010/uni00000015/uni00000013/uni00000031/uni0000004a\n/uni00000026/uni0000004f/uni00000048/uni00000044/uni00000051/uni00000010/uni00000024/uni0000002a/uni00000031/uni00000048/uni0000005a/uni00000056\n/uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c/uni00000010/uni00000024/uni0000002a/uni00000031/uni00000048/uni0000005a/uni00000056\nFigure 1: The average confidence of clean and noisy\nsamples during fine-tuning BERT on datasets 20Ng and\nAGNews with 20% noisy labels.\nPrior research (Arpit et al., 2017; Cheng et al.,\n2021; Zhang et al., 2021c; Wang et al., 2023a) has\nshown that PLMs are prone to overfitting and gener-\nally deliver subpar performance when fine-tuned on\ndatasets containing label noise. Since fine-tuning\ninvolves incorporating supervision information to\nenhance performance on downstream tasks, the\npresence of noisy labels will mislead the training\nprocess and significantly impede the efficacy of\nPLMs (Zhu et al., 2022). Therefore, there is an\nimmediate need to design effective algorithms for\nfine-tuning PLMs in the presence of noisy labels.\nIn the context of learning from noisy labels, an\nintuitive approach is to separate clean samples from\nthe noisy ones in the training set for model train-\ning (Han et al., 2018). For the remaining noisy\nsamples, a prevalent strategy is to pseudo-label\nthem based on model predictions to reduce the ad-\nverse impact of noise (Berthelot et al., 2019; Sohn\net al., 2020). However, it remains challenging when\napplying this paradigm to PLMs. This is because\nPLMs, equipped with prior knowledge encoded in\na large size of parameters, tend to easily memo-\nrize noisy samples early on during the fine-tuning\nprocess. As illustrated in Figure 1, PLMs exhibit\nsimilar prediction confidences for both clean and\nnoisy samples, which will result in two challenges\n12528\nwhen utilizing existing methods: (1) Existing ap-\nproaches often rely on confidences generated by\nmodels trained on potentially noisy samples (Li\net al., 2020; Karim et al., 2022). However, as shown\nin Figure 1, it is challenging to accurately distin-\nguish clean and noisy samples solely based on con-\nfidences. (2) Existing methods are susceptible to\nerroneous information during pseudo-labeling, as\nthey directly utilize model predictions as pseudo-\nlabels, which can be detrimental when the predic-\ntions are inaccurate. PLMs, being capable of easily\nremembering noisy samples, further exacerbate the\nrisk of capturing and amplifying erroneous infor-\nmation during pseudo-labeling. For example, in the\n20Ng dataset (Lang, 1995), when an \"autos\" sam-\nple is assigned a wrong label \"hardware\", PLMs\ncan easily memorize the erroneous information and\nthus cannot infer the correct labels in predictions,\nresulting in incorrect pseudo-labeling.\nTo overcome these challenges, we propose a\nnovel framework, named LAFT, that harnesses\nthe power of Large LAnguage Models (LLMs) for\nFine-Tuning PLMs. We leverage LLMs trained\non extensive corpora and fine-tuned with human\ninstructions, such as GPT4 (OpenAI, 2023), to ob-\ntain external guidance in the form of confidence\nvalues. Our approach addresses the first challenge\nof separating reliable samples by utilizing LLM-\ngenerated confidences for each class of all training\nsamples. By comparing these confidences with the\nassigned labels (i.e., the given noisy labels) of train-\ning samples, we categorize them into three disjoint\nsubsets: the Easy Clean (EC) Set, the Hard Clean\n(HC) Set, and the True Noisy (TN) Set. Regarding\nthe second challenge, we propose a novel method\nto incorporate LLM-generated confidence scores\nas robust supervision information for all samples\nto ensure that PLMs learn useful information from\nthem. As LLM-generated confidences are not af-\nfected by label noise, they can provide potentially\nrelevant labels that are useful even if they are not\nentirely accurate. In summary, our contributions\nare as follows:\n‚Ä¢ We are the first to explore the potential of lever-\naging supervision information (i.e., confidence\nscores) generated by LLMs to tackle the noisy\nlabel problem in fine-tuning PLMs.\n‚Ä¢ We propose a novel framework LAFT that can\neffectively separate clean and noisy samples and\nlearn from noisy labels, based on the LLMs-\ngenerated confidences.\n‚Ä¢ We conduct extensive experiments on synthetic\nand real-world noisy datasets and demonstrate\nthe superiority of our framework in fine-tuning\nPLMs with noisy labels.\n2 Related Work\n2.1 Learning from Noisy Labels\nVarious strategies for learning from noisy labels\nhave been proposed, falling primarily into three\ncategories. The first category is Sample Selection\nmethods, which typically employ loss or confi-\ndences to identify reliable samples for model op-\ntimization. These methods generally necessitate a\npredefined threshold (Li et al., 2021; Karim et al.,\n2022) or prior knowledge concerning the noise\nlabel rate (Han et al., 2018; Yu et al., 2019) to\nchoose the instances. The second category, dubbed\nLabel Transitionmethods, aims to learn (Tanaka\net al., 2018) or generate pseudo-labels (Zhang et al.,\n2021c; Sohn et al., 2020) to replace the original\nnoisy labels. Lastly, Regularization methods de-\nsign robust loss functions (Liu et al., 2020; Eng-\nlesson and Azizpour, 2021) or regularization tech-\nniques (Zhang and Sabuncu, 2018) that can effec-\ntively utilize all samples to enhance the model ro-\nbustness against label noise. Nevertheless, these\nmethods generally do not consider the scenario of\nfine-tuning a pretrained model with noisy labels.\n2.2 Confidence-Guided Sample Separation\nTo separate noisy samples, existing works lever-\nage the loss or confidences that provide insights\ninto the model‚Äôs prediction behavior as training\nproceeds (Yu et al., 2019; Karim et al., 2022). In\nthe context of learning from noisy labels, the key\nconcept is to leverage these dynamics as criteria for\nidentifying and separating noisy samples. Several\nworks propose to identify samples with lower train-\ning loss as the clean subset (Li et al., 2020; Han\net al., 2018; Jiang et al., 2018; Zhao et al., 2022;\nWang et al., 2023b), however, they are generally\nsimplistic and inflexible, resulting in the selection\nof only easy samples. To address this limitation,\nalternative approaches have been proposed to effec-\ntively utilize the loss or confidences during training,\nas demonstrated in (Zhang et al., 2021a) and (Nishi\net al., 2021). In contrast to existing methods that\nonly rely on confidences generated by PLMs, we\nleverage LLMs as external guidance that provides\nmore precise separations for fine-tuning PLMs.\n12529\n3 Problem Definition\nIn this work, we study the problem of fine-tuning\nPLMs for text classification with noisy labels. For-\nmally, consider a noisy training dataset contain-\ning nsamples: Dtr = {(xi,Àúyi),i = 1,2,...,n },\nwhere xi is an input sample and Àúyi denotes the\nassigned label of xi. Note that Àúyi is potentially\ncorrupted, and we denote the true label of xi as yi,\nwhich is inaccessible during fine-tuning. Specifi-\ncally, we aim to fine-tune PLMs with samples in\nDtr to achieve satisfactory prediction performance\non test samples, while utilizing LLMs as external\nguidance. Notably, the LLMs remain fixed in our\nframework, which means we can also use black-\nbox LLMs. In practice, we implement PLMs for\ntext classification by attaching an additional classi-\nfier, which will take the output of the PLM as input\nand generate class probabilities for classification.\n4 Methodology\nThe overall framework of LAFT is illustrated in\nFig. 2. In particular, we propose to divide all train-\ning samples into three subsets based on the accor-\ndance among LLM-generated confidences, PLM-\ngenerated confidences, and the assigned labels of\nsamples. In particular, we query an LLM to pro-\nvide confidences for each training sample, spanning\nall classes. Combining confidences obtained from\nboth LLMs and PLMs, we perform two steps of sep-\naration to segregate the entire training set into three\nsubsets: Easy Clean (EC) Set, Hard Clean (HC)\nSet, and True Noisy (TN) Set. Each of these sub-\nsets, displaying unique behaviors, is then subjected\nto specific noise-robust fine-tuning strategies.\n4.1 Confidences\nExisting studies establish a correlation between\nconfidences and the degree to which deep models\nmemorize specific samples during training (Arpit\net al., 2017; Karim et al., 2022). It has been ob-\nserved that as the model‚Äôs memorization of a partic-\nular sample strengthens, the model tends to assign\nhigher confidence for this sample (Li et al., 2023).\nTherefore, these methods generally employ confi-\ndences to distinguish between clean and noisy sam-\nples based on the assumption that the model cannot\neasily memorize noisy samples (Pleiss et al., 2020;\nSwayamdipta et al., 2020). However, applying\nthese strategies to fine-tuning PLMs is suboptimal,\nas PLMs also present high confidence for noisy\nsamples even in the early stage of fine-tuning, as\nNoisy Text DataCoarse-grained SeparationEasy Clean SetDisagreed SetHard Clean SetTrue Noisy Set‚Ñíùíú‚Ñí‚Ñã‚Ñíùí©Assigned LabelLLM Label\nLLMConfidenceFine-grained Separation\nPLM  Confidence\nCoarse-grained SeparationEasy Clean SetDisagreed SetHard Clean Set‚Ñí‚Ñ∞‚Ñí‚Ñã‚Ñíùí©Assigned LabelLLM LabelLLM ConfidenceFine-grained SeparationPLM Confidence\nùë¶‡Øú‡∑ùùë¶‡Øú‡∑• ùëùÃÇùëù‡∑§ Noisy Labeled DataTrue Noisy Set\nFigure 2: The detailed process of our framework LAFT.\nWe perform two steps of separation to divide all training\nsamples into three subsets with different degrees of label\nnoise: Easy Clean Set E, Hard Clean Set H, and True\nNoisy Set N. We further propose three different losses\nto effectively learn from them: LE, LH, and LN .\nshown in Fig. 1. To deal with this issue, we propose\nthe utilization of external guidance, in the form of\nconfidences generated by LLMs. Before delving\ninto the specifics of our framework, we provide a\nformal definition of confidences. Denote the output\nof the final layer (i.e., the classifier) in PLMs for\nsample xi as z(xi) ‚ààRN , where N is the number\nof classes. The confidence of xi for the j-th class\ncj can be represented as follows:\nÀúp(cj; xi) = exp (z(cj; xi))‚àëN\nk=1 exp (z(ck; xi))\n, (1)\nwhere z(cj; xi) ‚ààR is the j-th value in z(xi). No-\ntably, the confidences are obtained fromz(xi) after\na softmax function and thus sum up to one.\nAlthough LLM-generated confidences can pro-\nvide external guidance, they are not completely\naccurate. Thus, We conduct a two-step sample sep-\naration process based on both LLM-generated and\nPLM-generated confidences, with the second step\nproviding a more granular distinction.\n4.2 Coarse-Grained Separation\nFor the first step of separation, we aim to select sam-\nples that are easy to be identified as clean data with\nguidance from LLMs. Thus, we perform Coarse-\ngrained Separation, utilizing confidences gener-\nated by LLMs with the raw text data included in\nthe prompt. Here we provide an example of the\nprompt for querying the LLM to obtain the confi-\ndence value for each class:\n12530\nClassify the following content: {inputtext}.\nSelect the label from {Class 1}, {Class 2},\n... , {ClassN }and output a confidence value\nfor each of them.\nWe denote the LLM-generated confidence for sam-\nple xi regarding class cj as ÀÜp(cj; xi), where cj ‚àà\nC= {c1,c2,...,c N }, and Cis the class set. Then\nthe label obtained by LLM can be represented as\nÀÜyi = argmax\nj=1,2,...,N\nÀÜp(cj; xi). (2)\nTo perform coarse-grained separation, we first pro-\nvide an assumption as the justification:\nAssumption 1. Samples whose assigned labels are\nthe same as LLM-generated labels (i.e.,ÀÜyi = Àúyi)\ncan be considered almost clean.\nNote that Assumption 1 is empirically verified\nin Table 5 in Sec. 5.5. The underlying intuition\nis that the probability of an LLM-generated label\nbeing identical to the assigned label and inconsis-\ntent with the ground-truth label is significantly low\nand thus negligible. In concrete, this assumption\nallows us to segregate the training samples into two\ndistinct subsets based on the concordance between\nthe LLM-generated label ÀÜyi and the assigned (po-\ntentially noisy) label Àúyi of xi. More specifically,\nwe define the resulting two subsets, the Easy Clean\n(EC) Set Eand the Disagreed Set D, as follows:\nE= {xi,Àúyi|ÀÜyi = Àúyi}, D= {xi,Àúyi|ÀÜyi Ã∏= Àúyi}. (3)\nIt is naturally satisfied that E‚à™D = Dtr and E‚à©\nD= ‚àÖ, where Dtr is the training set. Since the\nsamples in Eare already clean, we can directly fine-\ntune PLMs using LLM-generated labels ÀÜyi based\non the cross-entropy loss as follows:\nLE= ‚àí1\n|E|\n|E|‚àë\ni=1\nN‚àë\nj=1\nÀÜyi,j log Àúp(cj; xi), (4)\nwhere Àúp(cj; xi) is the PLM-generated confidence\nfor xi regarding the j-th class cj. Here ÀÜyi,j = 1if\nÀÜyi = cj, and ÀÜyi,j = 0, otherwise.\n4.3 Fine-Grained Separation\nIt is noteworthy that, however, the samples in D\nare not completely noisy. This is because the\nLLM-generated labels are not perfectly correct,\nas shown in Table 5. Thus, the samples that are\nclean but incorrectly classified by LLMs will still\nbe categorized into D. Therefore, although we can\nlearn from samples in Edirectly with their LLM-\ngenerated label Àúyi, it is still challenging to learn\nfrom samples in D, which are only partially clean.\nTherefore, we further propose to separate the sam-\nples in the disagreed set Dinto two subsets: the\nHard Clean (HC) Set Hand the True Noisy (TN)\nSet N, referred to as fine-grained separation.\nThe intuition is that within the disagreed set D,\nthe LLM-generated labels can be incorrect for spe-\ncific hard samples with correct assigned labels, re-\nferred to as Hard Clean (HC) samples. Specifically,\nthe ideal separation for Dis as follows:\nH‚àó= D‚à©{xi,Àúyi|Àúyi = yi},\nN‚àó= D‚à©{xi,Àúyi|Àúyi Ã∏= yi}, (5)\nwhere yi is the true label of xi. Note that although\nthis ideal separation can be completely precise for\nseparating noisy samples in D, it is infeasible in\npractice, as the true labels are unknown. Therefore,\nto precisely separate the true noisy samples in D,\nwe propose two thresholds for LLM-generated and\nPLM-generated confidences, respectively.\nIn order to achieve more robust LLM-generated\nconfidences distributed over the label space C, we\nadopt M different augmentations for each input\nsample to encourage input diversity while keeping\nthe semantics immutable. Denote the augmented\nsamples of xi as vm(xi) , where m= 1,2,...,M ,\nand M is the number of augmentations. We\ncan obtain the M LLM-generated confidences as\nÀÜp(cj; vm(xi)) for xi regarding the j-th class cj,\nwhere m= 1,2,...,M and j = 1,2,...,N . We\naggregate the LLM-generated confidences via the\nM augmentations as follows:\nÀÜpa(cj; xi) = 1\nM\nM‚àë\nm=1\nÀÜp(cj; vm(xi)), (6)\nwhere vm(xi) is the input example after applying\nthe m-th augmentation. As LLM-generated confi-\ndences remain fixed during fine-tuning PLMs, we\nadopt a fixed threshold ÀÜœÑ for fine-grained separa-\ntion based on ÀÜpa(cj; xi).\nOn the other hand, however, PLMs-generated\nconfidences for each sample will change as the fine-\ntuning proceeds, which results in subpar separation\nperformance if we adopt a fixed threshold to select\nhigh-confidence samples as clean ones. Intuitively,\nthe confidences should be lower for HC samples at\nthe beginning of fine-tuning, as the model cannot\n12531\neasily fit these hard samples within several epochs.\nNonetheless, the noisy samples are relatively easier\nto achieve higher confidence, and thus their confi-\ndences will easily achieve higher at the beginning.\nTherefore, we propose an adaptive threshold ÀúœÑ(t)\nthat will increase as fine-tuning proceeds while not\nreaching an excessively high value:\nÀúœÑ(t) =ÀúœÑ ‚àíexp(‚àíŒªt), (7)\nwhere Œªand ÀúœÑ are hyper-parameters that control\nthe value of threshold ÀúœÑ(t) as fine-tuning proceeds.\ntdenotes the current number of fine-tuning epochs.\nCombining the two thresholds, we can perform\nfine-grained separation by selecting the HC set H\nas follows:\nH= D‚à©{xi,Àúyi|max\nc‚ààC\nÀÜpa(c; xi) <ÀÜœÑ}\n‚à©{xi,Àúyi|max\nc‚ààC\nÀúp(c; xi) <ÀúœÑ(t)},\n(8)\nwhere ÀÜœÑ and ÀúœÑ(t) are the thresholds for LLM-\ngenerated confidences ( ÀÜpa(cj; xi)) and PLMs-\ngenerated confidences (Àúp(cj; xi)), respectively. In\nthis manner, the process of separating HC and TN\nsamples, i.e., fine-grained separation, can benefit\nfrom both the LLMs and PLMs. Then the remain-\ning samples are categorized into the True Noisy\n(TN) set N:\nN= D\\H. (9)\n4.4 Learning from the Hard Clean (HC) Set\nNow we have divided the disagreed set Dinto H\nand N. Recall that ideally, samples in Hare hard\nyet correct. Thus, for these samples, we can directly\nutilize their assigned labels Àúy as training labels.\nNevertheless, since the fine-grained separation of\nHand Ncannot be perfect, the samples in Hmay\nstill be noisy. As both LLMs and PLMs fail to\nprovide a confident prediction for samples inH, we\npropose to prioritize the assigned label while also\nincorporating additional information from LLMs\nand PLMs. Specifically, we employ a weighted loss\nbased on cross-entropy. The weight is enlarged if\nthe summed confidence of the LLM and the PLM\nis high. We define the loss as follows:\nLH= ‚àí 1\n|H|\n|H|‚àë\ni=1\nN‚àë\nj=1\n(Àúyi,j + œïi,j) logÀúp(cj; xi),\n(10)\nwhere Àúp(cj; xi) is the PLM-generated confidence\nfor xi regarding the j-th class cj. Here Àúyi,j = 1\nif Àúyi = cj, and Àúyi,j = 0, otherwise. œïi,j acts as a\nweight adjustment that increases when the sum of\nLLM confidence ÀÜpa(cj; xi) and PLM confidence\nÀúp(cj; xi) is larger. Intuitively, if the LLM and PLM\nare with high confidence regarding a specific class,\nthen the information in their confidence can be use-\nful as they are more likely to be correct. Therefore,\nwe define œïi,j as follows:\nœïi,j = max (ÀÜpa(cj; xi) +Àúp(cj; xi) ‚àíŒ±ÀúœÑ(t),0) ,\n(11)\nwhere Œ±> 1 is a hyper-parameter that controls the\nthreshold Œ±ÀúœÑ(t) for œïi,j. As such information can\nstill be inaccurate, we subtract it by the threshold\nŒ±ÀúœÑ(t) to control its magnitude, such that œïi,j also\nacts as an adaptive loss weight for these samples.\n4.5 Learning from the True Noisy (TN) Set\nAfter the fine-grained separation, most samples in\nthe True Noisy (TN) Set should be identified as\nnoisy ones. Nevertheless, it is still challenging\nfor PLMs to use their output as pseudo-labels for\nfine-tuning, as the prediction errors will accumu-\nlate and affect subsequent pseudo-labeling results.\nFortunately, the confidences generated by LLMs\ncan provide additional guidance to identify the po-\ntential labels for samples in the TN set. We first\nprovide Remark 1 to justify the effectiveness of\nusing LLM-generated labels for optimization.\nRemark 1. LLM-generated labels on the True\nNoisy Set preserve the same accuracy as that on the\nwhole dataset, as LLMs are not affected by noise.\nRemark 1, as empirically verified in Sec. 5.5,\ndemonstrates that even when we categorize most\nnoisy samples into True Noisy Set, the LLM-\ngenerated labels can still provide decent guidance\nwithout sacrificing accuracy. Given the confidences\nprovided by LLMs, we employ a loss that can en-\nable the PLMs to benefit from them. Specifically,\nLN = ‚àí 1\n|N|\n|N|‚àë\ni=1\nN‚àë\nj=1\nÀÜpa(cj; xi) logÀúp(cj; xi)\n‚àí 1\n|N|\n|N|‚àë\ni=1\nŒ¥(xi) ¬∑max\nc‚ààC\nÀúp(c; xi) log max\nc‚ààC\nÀúp(c; xi).\n(12)\nHere in the first term, we leverage the confidences\ngenerated by LLMs to learn from potentially cor-\nrect labels. This is because although LLMs can-\nnot completely predict the correct labels, the con-\nfidences still preserve the potentially useful infor-\nmation in other incorrect but relevant labels. Such\n12532\nbenefits cannot be provided by pseudo-labeling,\nwhich tends to output a definitive label. For the\nsecond term, we utilize the model predictions to\nexploit useful information from PLMs. The intu-\nition is that, with the relevant label information\nprovided by LLMs, the PLMs can learn accurate\nlabel information from the noisy samples in TN\nset. Consequently, if the model output tends to be\nconfident on specific samples, we can utilize the\nprediction to further enhance the learning from it.\nThus, we further set a threshold for this term, i.e.,\nŒ¥(xi), defined as follows:\nŒ¥(xi) =\n{1, if max\nc‚ààC\nÀúp(c; xi) >Œ≤ ÀúœÑ(t),\n0, otherwise,\n(13)\nwhere ÀúœÑ(t) is computed by Eq. (7). To reduce the\neffect of confirmation bias, we multiply ÀúœÑ(t) by Œ≤,\na hyper-parameter that controls the final adaptive\nthreshold, i.e., Œ≤ÀúœÑ(t).\n4.6 Fine-tuning Objective\nAfter we separate all training samples into E, H,\nand N, we can combine the three individual losses\nfor them for PLM fine-tuning. Our final fine-tuning\nobjective can be represented as follows:\nL= LE+ ŒªHLH+ ŒªNLN, (14)\nwhere ŒªHand ŒªN are hyper-parameters that con-\ntrol the importance of LHand LN, respectively.\n5 Experiments\n5.1 Experimental Settings\nDatasets. To evaluate the performance of our\nframework, we first conduct experiments on two\nsynthetic-noise datasets: 20Ng (Lang, 1995) and\nAGNews (Li and Roth, 2002; Zhang et al., 2015).\nFollowing existing works on learning from noisy\nlabels (Patrini et al., 2017; Yao et al., 2020; Zhuang\net al., 2023), we adopt three types of synthetic la-\nbel noise: (1) Symmetric Noise (SN) uniformly\nchanges labels to other classes (Han et al., 2018;\nXia et al., 2021). (2) Asymmetric Noise (ASN)\nchanges labels to other similar classes (Tanaka\net al., 2018; Bae et al., 2022). (3) Instance-\nDependent Noise (IDN) changes labels based on a\nprobability in proportion to the specific sample fea-\ntures (Cheng et al., 2021; Yao et al., 2021). More-\nover, we further conduct experiments on three real-\nworld datasets with noisy labels: SemEval (Zhou\net al., 2020), TREC (Awasthi et al., 2020), and\nHausa (Hedderich et al., 2020). More details about\nthese datasets are provided in Appendix B.\nBaselines. We compare our framework to state-\nof-the-art baselines for learning from noisy labels.\nIn particular, we compare to (1) Base (Devlin\net al., 2018) that performs fine-tuning with standard\ncross-entropy loss; (2) Regularization Methods:\nMixup (Zhang et al., 2018) and GCE (Zhang and\nSabuncu, 2018); (3) Sample-selection Methods:\nCo-teaching (Han et al., 2018), Co-teaching+ (Yu\net al., 2019), JoCoR (Wei et al., 2020), CR (Zhou\nand Chen, 2021), and NPC (Bae et al., 2022). Ad-\nditional details are provided in Appendix C.\nImplementation Details. We use BERT (Devlin\net al., 2018) as the text encoder for all datasets\nexcept Hausa, for which we use mBERT. The clas-\nsifier is implemented as a fully-connected layer\nand randomly initialized at the beginning, while\nboth the encoder and the classifier will be updated\nvia gradient descent during fine-tuning. All ex-\nperiments are evaluated on a clean test set, and\nthe average accuracy along with the standard de-\nviation over ten runs is reported for each dataset.\nWe provide more details about the implementa-\ntion in Appendix D, and our code is provided at\nhttps://github.com/SongW-SW/LAFT.\n5.2 Comparison on Synthetic Datasets\nIn this subsection, we compare our framework with\nother baselines on synthetic datasets 20Ng and\nAGNews, considering different noise types and\nratios. Specifically, for Symmetric Noise (SN), we\nconduct experiments on three different noise rates:\n20%, 40%, and 60%. For the other two types of\nnoise, i.e., Asymmetric Noise (AN) and Instance-\nDependent Noise (IDN), we adopt two noise rates:\n20% and 40%. We present the results in Table 1, 2,\nand 3. The key observations drawn from the out-\ncomes are as follows: (1) Regardless of the noise\ntype, our LAFT framework persistently surpasses\nother state-of-the-art baselines, thus showcasing\nits effectiveness in fine-tuning PLMs with noisy\nlabels. (2) LAFT‚Äôs performance improvement over\nother baselines is slightly more pronounced on\nthe 20Ng dataset compared to AGNews. This\nis attributable to 20Ng containing a greater num-\nber of classes ( N = 20) as opposed to AGNews\n(N = 4). Consequently, our strategy of utilizing\nLLM-generated confidences can capitalize on the\nsimilar labels within 20Ng for PLM fine-tuning,\neven if LLM predictions are not entirely accurate.\n12533\nTable 1: The overall performance of various models on synthetic noisy datasets 20Ng and AGNews with Symmetric\nNoise (SN), where accuracy and standard deviation are reported in %, and the best results are in bold.\nDataset 20Ng AGNews\nSN Ratio 20% 40% 60% 20% 40% 60%\nBase 79.15¬±0.29 71.74¬±0.07 61.87¬±1.58 81.77¬±0.33 78.33¬±0.47 73.07¬±1.49\nMixup 79.10¬±0.21 70.58¬±0.22 62.48¬±2.04 81.25¬±0.02 78.24¬±0.77 73.12¬±1.58\nGCE 79.56¬±0.39 70.46¬±0.42 62.61¬±1.01 82.98¬±0.20 78.46¬±0.20 72.75¬±0.27\nCo-teaching 76.68¬±0.86 69.23¬±0.62 61.39¬±3.15 80.99¬±0.88 76.82¬±2.01 71.49¬±2.74\nCo-teaching+ 78.78¬±0.96 71.33¬±2.31 62.90¬±1.55 80.95¬±0.86 77.98¬±0.76 72.73¬±0.98\nJoCoR 80.11¬±0.42 73.52¬±0.88 63.52¬±0.22 83.60¬±0.54 79.45¬±0.38 75.45¬±0.30\nCR 81.80¬±0.48 74.16¬±0.13 64.22¬±1.25 84.09¬±0.39 80.12¬±0.95 75.06¬±1.01\nNPC 80.38¬±0.35 72.93¬±0.23 64.38¬±0.38 84.28¬±0.47 80.26¬±1.39 73.81¬±0.18\nLAFT (Ours) 82.04¬±0.11 76.93¬±0.60 70.79¬±1.98 90.86 ¬±0.02 88.61¬±0.25 80.79¬±2.32\nTable 2: The overall performance on 20Ng and AGNews\nwith Asymmetric Noise (AN).\nDataset 20Ng AGNews\nAN Ratio 20% 40% 20% 40%\nBase 75.34 59.65 82.24 76.35\nMixup 75.84 62.19 82.12 77.45\nGCE 76.41 62.37 83.19 77.49\nCo-teaching 77.49 64.55 84.15 79.16\nCo-teaching+ 77.98 63.48 86.12 81.02\nJoCoR 81.27 70.99 87.63 82.79\nCR 81.08 67.22 88.58 83.07\nNPC 79.03 65.54 86.83 80.21\nLAFT (Ours) 83.70 81.97 91.95 90.12\n(3) Certain regularization methods, such as Mixup\nand GCE, exhibit subpar performance when ap-\nplied to datasets with a higher noise ratio of 60%.\nThis is because these methods rely on all training\nsamples for PLM fine-tuning, making them more\nsusceptible to strong label noises.\n5.3 Comparison on Real-world Datasets\nIn this subsection, we conduct experiments on\nreal-world datasets: SemEval (Zhou et al., 2020),\nTREC (Awasthi et al., 2020), and Hausa (Hed-\nderich et al., 2020). From the results in Table 4,\nwe observe that LAFT outperforms other baseline\nmethods on all three datasets. Moreover, the results\nof LAFT are noticeably competitive on TREC and\nHausa with relatively higher noise ratios, which\nfurther demonstrates the strong robustness and gen-\neralization ability of LAFT to label noise.\n5.4 Ablation Study\nIn this subsection, we systematically remove spe-\ncific components from our framework and ana-\nTable 3: The overall performance on 20Ng and AGNews\nwith Instance-Dependent Noise (IDN).\nDataset 20Ng AGNews\nIDN Ratio 20% 40% 20% 40%\nBase 78.23 70.28 85.40 78.66\nMixup 78.21 70.45 84.71 79.22\nGCE 79.99 71.74 87.11 80.49\nCo-teaching 77.43 70.76 84.09 78.43\nCo-teaching+ 79.89 71.52 88.21 82.30\nJoCoR 81.83 74.04 88.24 82.66\nCR 83.04 75.51 90.72 84.38\nNPC 81.84 74.45 89.90 82.40\nLAFT (Ours) 83.61 80.49 91.94 89.34\nlyze the resulting impact on performance. We\nconduct experiments with the following variants:\n(1) LAFT\\C performs coarse-grained separation\nwithout LLMs and thus only separates the Easy\nClean Set based on PLM predictions. (2) LAFT\\F\nperforms fine-grained separation without LLMs,\nwhich means when separating Hard Clean Set, only\nPLM-generated confidences are used. (3) LAFT\\N\nremoves our proposed loss LN for True Noisy Set\nand replaces it with pseudo-labeling based on the\nmost confident PLM prediction. From the results\npresented in Figure 3, we observe that LAFT out-\nperforms all variants, which verifies the effective-\nness of these designs in LAFT. Specifically, re-\nmoving the learning loss LN leads to significant\nperformance degradation, indicating that such a de-\nsign can effectively alleviate the adverse impact of\nnoisy labels. Moreover, without the fine-grained\nseparation strategy, the performance deteriorates\nrapidly when the noise ratio is larger, indicating\nthe importance of fine-grained separation in the\npresence of higher noise ratios.\n12534\nTable 4: The overall performance of various models on\nthree real-world noisy datasets.\nDataset SemEval TREC Hausa\nNoise Ratio 16.00% 38.56% 50.37%\nBase 70.61 67.42 47.80\nMixup 73.41 68.44 47.66\nGCE 71.91 67.86 48.12\nCo-teaching 72.13 68.19 47.56\nCo-teaching+ 72.24 69.83 48.25\nJoCoR 70.11 66.62 46.48\nCR 72.94 68.90 48.34\nNPC 71.22 67.84 47.48\nLAFT (Ours) 73.56 72.34 51.71\n/uni00000036/uni00000010/uni00000013/uni00000011/uni00000015/uni00000036/uni00000010/uni00000013/uni00000011/uni00000017/uni00000024/uni00000010/uni00000013/uni00000011/uni00000015/uni00000024/uni00000010/uni00000013/uni00000011/uni00000017/uni0000002c/uni00000027/uni00000010/uni00000013/uni00000011/uni00000015/uni0000002c/uni00000027/uni00000010/uni00000013/uni00000011/uni00000017\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018\n/uni0000001b/uni00000013\n/uni0000001b/uni00000018\n/uni0000001c/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni0000002f/uni00000024/uni00000029/uni00000037/uni0000003f/uni00000026\n/uni0000002f/uni00000024/uni00000029/uni00000037/uni0000003f/uni00000029\n/uni0000002f/uni00000024/uni00000029/uni00000037/uni0000003f/uni00000031\n/uni0000002f/uni00000024/uni00000029/uni00000037\nFigure 3: Ablation study of our framework on 20Ng.\nS-0.2 and S-0.4 (A, or ID) refer to the noise ratios of\n20% and 40% for SN (AN, or IDN), respectively.\n5.5 Evaluation of LLMs-generated Labels\nIn this subsection, we evaluate the effectiveness\nof LLMs regarding the quality of generated confi-\ndence, presented in Table 5. Specifically, we report\nthe accuracy of LLM-generated prediction on the\ntraining set in 20Ng and AGNews. The statistics\nare from experiments on 20Ng and AGNews with\n20% Symmetric Noise (SN). From the results, we\ncan observe that: (1) For Easy Clean Set, the LLM-\ngenerated labels exhibit a remarkably high degree\nof correctness. This empirical finding provides the\njustification for Assumption 1, which allows for\nleveraging LLMs to perform coarse-grained sepa-\nration in our framework. (2) For the TN set, LLM-\ngenerated labels are not entirely correct, while still\npreserving similar accuracy compared to that on\nall samples. This result empirically verifies Re-\nmark 1 and justifies our strategy of utilizing LLM-\ngenerated confidences for learning from the TN set.\n(3) Considering the overall result, LLM-generated\nlabels generally exhibit lower accuracy than PLM-\nbased baselines. That being said, LLMs cannot be\nTable 5: The LLM-generated prediction accuracy on the\ntraining set in 20Ng and AGNews in theideal separation\nand real separation (shown in gray) during fine-tuning.\nDataset EC Set HC Set TN Set Overall\n20Ng 99.46% 0% 76.47% 75.34%\n99.46% 3.58% 69.44% 75.34%\nAGNews 99.03% 0% 79.35% 81.41%\n99.03% 2.05% 76.07% 81.41%\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c\n/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000032/uni00000058/uni00000055/uni00000056/uni00000010/uni00000036/uni00000031\n/uni00000026/uni00000035/uni00000010/uni00000036/uni00000031\n/uni00000032/uni00000058/uni00000055/uni00000056/uni00000010/uni00000024/uni00000031\n/uni00000026/uni00000035/uni00000010/uni00000024/uni00000031\n/uni00000032/uni00000058/uni00000055/uni00000056/uni00000010/uni0000002c/uni00000027/uni00000031\n/uni00000026/uni00000035/uni00000010/uni0000002c/uni00000027/uni00000031\nFigure 4: The results of our framework and the best\nbaseline CR on 20Ng with different noise ratios for\nthree types of noise: SN, AN, and IDN.\ndirectly used to replace PLMs for the text classifica-\ntion task when the noise ratio is not extremely high.\nNevertheless, our strategy can effectively leverage\nthe LLM-generated confidences, despite their lack\nof complete correctness, to enhance the fine-tuning\nperformance of PLMs with noisy labels.\n5.6 Results with Different Noise Ratios\nFigure 4 presents the evaluation of our proposed\nframework across a spectrum of noise ratios on\n20Ng, ranging from 0.1 to 0.9, while considering\nthree distinct types of noise: SN, AN, and IDN.\nFrom the results, several significant observations\nare discovered: (1) With increasing noise ratios, all\nbaseline methods consistently show a deterioration\nin performance. This decline can be tied to the\namplified difficulty of label noise during PLM fine-\ntuning. (2) Despite the escalating noise ratios, our\nframework maintains a relatively more robust per-\nformance. This is attributed to our sample separa-\ntion strategy which adeptly discerns clean samples\nwithin the training set, thus alleviating the adverse\neffects of label noise. (3) Our framework demon-\nstrates a slower performance decrease with AN\nnoise compared to other types of noise. This can be\ncredited to the adaptive nature of our approach that\nemploys LLM-generated confidences to effectively\nlearn from similar labels for each sample.\n12535\n6 Conclusion\nThis paper delves into the issue of fine-tuning Pre-\ntrained Language Models (PLMs) with noisy labels\nfor text classification tasks. We address the detri-\nmental effects of label noise by harnessing external\nguidance from Large Language Models (LLMs) in\nthe form of confidences. Our approach entails a\ntwo-step separation strategy that accurately segre-\ngates all training samples into three distinct subsets,\neach treated with innovative noise-resilient strate-\ngies. In summary, our framework adeptly fine-\ntunes PLMs in the face of noisy samples, requiring\nminimal external guidance from LLMs.\nAcknowledgements\nSong Wang and Jundong Li are supported\nby the National Science Foundation under\ngrants (IIS-2006844, IIS-2144209, IIS-2223769,\nCNS2154962, and BCS-2228534), the Common-\nwealth Cyber Initiative awards (VV-1Q23-007, HV-\n2Q23-003, and VV-1Q24-011), the JP Morgan\nChase Faculty Research Award, the Cisco Faculty\nResearch Award, the Jefferson Lab subcontract,\nand the UV A 4-V A collaborative research grant.\nLimitation\nDespite the promising results, our work presents\nseveral limitations that we must acknowledge: (1)\nDependence on Large Language Models (LLMs):\nOur framework leverages the guidance from LLMs\nin the form of confidences. This implies that the\nquality of these confidences and hence the effec-\ntiveness of our model are closely tied to the per-\nformance of the LLM. Unsatisfactory LLM perfor-\nmance could thus directly impact our framework‚Äôs\nefficacy. (2) Noise Types and Ratios: The experi-\nments in this paper mainly focus on three types of\nnoise: Symmetric Noise (SN), Asymmetric Noise\n(AN), and Instance-Dependent Noise (IDN), with\nnoise ratios up to 60%. However, there may be\nother noise types or higher noise ratios in prac-\ntice, and it remains to be investigated how well our\nframework performs under these circumstances. (3)\nLimitations in Sample Separation Strategies: The\nefficacy of our framework relies on accurately di-\nviding the training samples into clean and noisy\nsubsets. If this distinction is not clear or becomes\nmore complex, our current approach may encounter\ndifficulties. (4) Domain-Specific Text Classifica-\ntion: While the experiments are performed on spe-\ncific text classification tasks, we do not investigate\nthe effectiveness of our approach in more domain-\nspecific contexts where the nature of the noise\ncould be different. (5) Computational Costs: Fi-\nnally, our approach entails using LLMs which can\nbe computationally expensive and could thus pose\na challenge when applied to large datasets or in\nresource-constrained environments. In summary,\nfuture research could focus on overcoming these\nlimitations and exploring the adaptability of our\nproposed framework to other noise types, higher\nnoise ratios, more complex noise patterns, as well\nas different task domains.\nEthics Statement\nThis research adheres to the principles of ethical\nconduct in artificial intelligence research. The de-\nvelopment and utilization of our work are carried\nout with full recognition of the potential implica-\ntions. While our method improves the performance\nof Pretrained Language Models (PLMs) with noisy\nlabels in text classification tasks, we acknowledge\nthat it could be potentially used in contexts that\nmay involve ethical concerns, such as disinforma-\ntion or manipulation of opinion on social platforms.\nOur experiments are performed on five publicly\navailable datasets, namely 20Ng, AGNews, Se-\nmEval, TREC, and Hausa. The datasets are already\nanonymized and do not contain any personally iden-\ntifiable information. Moreover, we have complied\nwith all guidelines and standards for the use of\nthese datasets. Furthermore, the use of Large Lan-\nguage Models (LLMs) in our work is subject to the\nterms and conditions put forth by the creators and\nhosts of these models. We only emply them for the\npurpose of enhancing the fine-tuning performance\nof PLMs and do not exploit them for any inappro-\npriate or unauthorized purposes. While our work\nfocuses on improving the robustness of PLMs to\nlabel noise in data, we recognize the broader soci-\netal concerns related to the potential misuse of our\nframework. We advocate for the responsible use of\nour research findings and encourage continued dis-\ncourse around the ethical use of machine learning\nand artificial intelligence technologies in real-world\napplications. Finally, it is crucial to note that the\ndevelopment of technologies and methodologies in\nmachine learning should be done in conjunction\nwith ongoing considerations about their ethical im-\nplications, potential misuse, and societal impacts.\nIt is our responsibility to foster an environment of\nethical vigilance in AI research and development.\n12536\nReferences\nChristoph Alt, Aleksandra Gabryszak, and Leonhard\nHennig. 2020. Tacred revisited: A thorough evalua-\ntion of the tacred relation extraction task. In ACL.\nDevansh Arpit, Stanis≈Çaw JastrzÀõ ebski, Nicolas Ballas,\nDavid Krueger, Emmanuel Bengio, Maxinder S Kan-\nwal, Tegan Maharaj, Asja Fischer, Aaron Courville,\nYoshua Bengio, et al. 2017. A closer look at memo-\nrization in deep networks. In ICML.\nAbhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and\nSunita Sarawagi. 2020. Learning from rules general-\nizing labeled exemplars. In ICLR.\nHeeSun Bae, Seungjae Shin, Byeonghu Na, JoonHo\nJang, Kyungwoo Song, and Il-Chul Moon. 2022.\nFrom noisy prediction to true label: Noisy predic-\ntion calibration via generative model. In ICML.\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow,\nNicolas Papernot, Avital Oliver, and Colin A Raf-\nfel. 2019. Mixmatch: A holistic approach to semi-\nsupervised learning. NeurIPS.\nHao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing\nSun, and Yang Liu. 2021. Learning with instance-\ndependent label noise: A sample sieve approach. In\nICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nErik Englesson and Hossein Azizpour. 2021. Gener-\nalized jensen-shannon divergence loss for learning\nwith noisy labels. NeurIPS.\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao\nXu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama.\n2018. Co-teaching: Robust training of deep neural\nnetworks with extremely noisy labels. NeurIPS.\nMichael A Hedderich, David I Adelani, Dawei Zhu,\nJesujoba Alabi, Udia Markus, and Dietrich Klakow.\n2020. Transfer learning and distant supervision for\nmultilingual transformer models: A study on african\nlanguages. In EMNLP.\nLu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li,\nand Li Fei-Fei. 2018. Mentornet: Learning data-\ndriven curriculum for very deep neural networks on\ncorrupted labels. In ICML.\nNazmul Karim, Mamshad Nayeem Rizve, Nazanin Rah-\nnavard, Ajmal Mian, and Mubarak Shah. 2022. Uni-\ncon: Combating label noise through uniform selec-\ntion and contrastive learning. In CVPR.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nKen Lang. 1995. Newsweeder: Learning to filter net-\nnews. In Machine Learning Proceedings.\nBohan Li, Yutai Hou, and Wanxiang Che. 2022. Data\naugmentation approaches in natural language pro-\ncessing: A survey. AI Open.\nJunnan Li, Richard Socher, and Steven CH Hoi. 2020.\nDividemix: Learning with noisy labels as semi-\nsupervised learning. In ICLR.\nJunnan Li, Caiming Xiong, and Steven CH Hoi. 2021.\nLearning from noisy data with robust representation\nlearning. In ICCV.\nWen Li, Limin Wang, Wei Li, Eirikur Agustsson, and\nLuc Van Gool. 2017. Webvision database: Visual\nlearning and understanding from web data. arXiv\npreprint arXiv:1708.02862.\nXin Li and Dan Roth. 2002. Learning question classi-\nfiers. In COLING.\nYifan Li, Hu Han, Shiguang Shan, and Xilin Chen.\n2023. Disc: Learning from noisy labels via dynamic\ninstance-specific selection and correction. In CVPR.\nSheng Liu, Jonathan Niles-Weed, Narges Razavian, and\nCarlos Fernandez-Granda. 2020. Early-learning reg-\nularization prevents memorization of noisy labels.\nNeurIPS.\nKento Nishi, Yi Ding, Alex Rich, and Tobias Hollerer.\n2021. Augmentation strategies for learning with\nnoisy labels. In CVPR.\nOpenAI. 2023. Gpt-4 technical report.\nGiorgio Patrini, Alessandro Rozza, Aditya Kr-\nishna Menon, Richard Nock, and Lizhen Qu. 2017.\nMaking deep neural networks robust to label noise:\nA loss correction approach. In CVPR.\nGeoff Pleiss, Tianyi Zhang, Ethan Elenberg, and Kil-\nian Q Weinberger. 2020. Identifying mislabeled data\nusing the area under the margin ranking. NeurIPS.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM.\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\nZhang, Han Zhang, Colin A Raffel, Ekin Dogus\nCubuk, Alexey Kurakin, and Chun-Liang Li. 2020.\nFixmatch: Simplifying semi-supervised learning\nwith consistency and confidence. NeurIPS.\nHwanjun Song, Minseok Kim, and Jae-Gil Lee. 2019.\nSelfie: Refurbishing unclean samples for robust deep\nlearning. In ICML.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nEMNLP.\n12537\nDaiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and\nKiyoharu Aizawa. 2018. Joint optimization frame-\nwork for learning with noisy labels. In CVPR.\nSong Wang, Xingbo Fu, Kaize Ding, Chen Chen,\nHuiyuan Chen, and Jundong Li. 2023a. Federated\nfew-shot learning. In SIGKDD.\nSong Wang, Jing Ma, Lu Cheng, and Jundong Li. 2023b.\nFair few-shot learning with auxiliary sets. In ECAI.\nYaqing Wang, Song Wang, Yanyan Li, and Dejing Dou.\n2022. Recognizing medical search query intent by\nfew-shot learning. In SIGIR.\nYaqing Wang, Song Wang, Quanming Yao, and Dejing\nDou. 2021. Hierarchical heterogeneous graph rep-\nresentation learning for short text classification. In\nEMNLP.\nHongxin Wei, Lei Feng, Xiangyu Chen, and Bo An.\n2020. Combating noisy labels by agreement: A joint\ntraining method with co-regularization. In CVPR.\nJason Wei and Kai Zou. 2019. Eda: Easy data augmen-\ntation techniques for boosting performance on text\nclassification tasks. In EMNLP-IJCNLP.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nXiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nan-\nnan Wang, Zongyuan Ge, and Yi Chang. 2021. Ro-\nbust early-learning: Hindering the memorization of\nnoisy labels. In ICLR.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V Le. 2019. Unsupervised data aug-\nmentation for consistency training. arXiv preprint\narXiv:1904.12848.\nYan Yan, R√≥mer Rosales, Glenn Fung, Ramanathan\nSubramanian, and Jennifer Dy. 2014. Learning from\nmultiple annotators with varying expertise. Machine\nLearning.\nYu Yao, Tongliang Liu, Mingming Gong, Bo Han, Gang\nNiu, and Kun Zhang. 2021. Instance-dependent label-\nnoise learning under a structural causal model. In\nNeurIPS.\nYu Yao, Tongliang Liu, Bo Han, Mingming Gong,\nJiankang Deng, Gang Niu, and Masashi Sugiyama.\n2020. Dual t: Reducing estimation error for transi-\ntion matrix in label-noise learning. NeurIPS.\nXingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor\nTsang, and Masashi Sugiyama. 2019. How does\ndisagreement help generalization against label cor-\nruption? In ICML.\nBowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu,\nJindong Wang, Manabu Okumura, and Takahiro\nShinozaki. 2021a. Flexmatch: Boosting semi-\nsupervised learning with curriculum pseudo labeling.\nIn NeurIPS.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. 2018. mixup: Beyond empirical\nrisk minimization. In ICLR.\nJieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yam-\ning Yang, Mao Yang, and Alexander Ratner. 2021b.\nWRENCH: A comprehensive benchmark for weak\nsupervision. In NeurIPS Datasets and Benchmarks\nTrack.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. NIPS.\nYivan Zhang, Gang Niu, and Masashi Sugiyama. 2021c.\nLearning noise transition matrix from only noisy la-\nbels via total variation regularization. In ICML.\nZhilu Zhang and Mert Sabuncu. 2018. Generalized\ncross entropy loss for training deep neural networks\nwith noisy labels. NeurIPS.\nGanlong Zhao, Guanbin Li, Yipeng Qin, Feng Liu, and\nYizhou Yu. 2022. Centrality and consistency: two-\nstage clean samples identification for learning with\ninstance-dependent noisy labels. In ECCV.\nWenxuan Zhou and Muhao Chen. 2021. Learning from\nnoisy labels for entity-centric information extraction.\nIn EMNLP.\nWenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi\nWang, Junyi Du, Leonardo Neves, and Xiang Ren.\n2020. Nero: A neural rule grounding framework for\nlabel-efficient relation extraction. In TheWebConf.\nDawei Zhu, Michael Hedderich, Fangzhou Zhai, David\nAdelani, and Dietrich Klakow. 2022. Is bert robust to\nlabel noise? a study on learning with noisy labels in\ntext classification. In Insights from Negative Results\nin NLP.\nYuchen Zhuang, Yue Yu, Lingkai Kong, Xiang Chen,\nand Chao Zhang. 2023. Dygen: Learning from noisy\nlabels via dynamics-enhanced generative modeling.\narXiv preprint arXiv:2305.19395.\nA Details of Three Subsets\nIn this section, we further provide details about the\nthree subsets of samples in our framework.\n1. Easy Clean (EC) Set. In this subset, the labels\npredicted by LLMs are the same as the assigned\nlabels. Therefore, based on Assumption 1, we\ncan infer that the labels of these samples are al-\nmost clean, which allows us to directly use them\nfor model training. Note that a small portion of\nsamples in this category can still be noisy when\nthe LLM predictions happen to be the same as\nnoisy labels. However, we empirically verify in\nSec. 5.5 that this proportion is small.\n12538\n2. Hard Clean (HC) Set. In this (ideal) category,\nthe labels predicted by LLMs deviate from the\nassigned labels, while the true labels are in ac-\ncordance with the assigned labels. In this case,\nwe are acknowledged that these samples are also\nclean. However, due to the semantic difficulties\nof these samples, the LLMs cannot easily pre-\ndict the correct labels of them. Nevertheless,\nsince the assigned labels are correct as true la-\nbels, we can use assigned labels for model train-\ning on these samples. It is noteworthy that in\npractice, it is infeasible to achieve a perfect sep-\naration of this subset. Therefore, in our frame-\nwork, we propose to leverage LLM-generated\nand PLM-generated confidences to improve the\nseparation performance.\n3. True Noisy (TN) Set. In this (ideal) category,\nthe labels predicted by LLMs and the true la-\nbels are both different from the assigned labels.\nIn other words, these samples are all noisy as\ntheir assigned labels are different from true la-\nbels. Since they are noisy, the labels predicted\nby LLMs are also likely to be different from\nthe assigned labels. As these samples are noisy,\nwe cannot directly use their labels for training.\nHowever, since we narrow down the potential\nrange of noisy samples to this category, we can\nresort to specific techniques to learn from these\nnoisy samples. In our framework, we utilize\nLLM-generated confidences as additional su-\npervision information to effectively learn from\nthese noisy samples.\nB Datasets\nIn this section, we introduce the details of the\ndatasets used in our experiments. In particular,\n20Ng (Lang, 1995) and AGNews (Li and Roth,\n2002; Zhang et al., 2015) are news topic classifi-\ncation datasets that are prevalently used for text\nclassification tasks. We manually inject different\ntypes of noise (SN, AN, and IDN) into these two\ndatasets for evaluation in our experiments. For\nreal-world datasets, we utilize SemEval (Zhou\net al., 2020), TREC (Awasthi et al., 2020), and\nHausa (Hedderich et al., 2020). Specifically, Se-\nmEval is a relation extraction dataset, and we fol-\nlow the process introduced in (Zhou et al., 2020)\nto obtain noisy labels. TREC is a question classifi-\ncation dataset in the weak supervision benchmark\nWRENCH (Zhang et al., 2021b). Moreover, Hausa\nis a text classification dataset in the language of\nTable 6: The detailed statistics of the five datasets used\nin our experiments.\nDataset # Training # Validation # Test # Class\n20Ng 9,051 2,263 7,532 20\nAGNews 40,000 7,600 7,600 4\nSemEval 1,749 200 692 9\nTREC 4,965 500 500 6\nHausa 2,045 290 582 5\nHausa, the second most spoken indigenous lan-\nguage in Africa, with 40 million native speakers.\nFor this dataset, gazetteers are used for automatic\nlabeling, which results in feature-dependent label\nnoise. Here we provide the detailed statistics of\nthese datasets in Table 6.\nC Baselines\nIn this section, we provide the details of the base-\nlines used in our experiments.\n‚Ä¢ Base (Devlin et al., 2018) is the BERT base\nmodel fine-tuned based on the standard cross-\nentropy loss. Note that for dataset Hausa, we\nutilize the multilingual BERT model.\n‚Ä¢ Mixup (Zhang et al., 2018) is a semi-supervised\napproach that performs a linear interpolation be-\ntween clean and noisy samples.\n‚Ä¢ GCE (Zhang and Sabuncu, 2018) denotes Gener-\nalized Cross-Entropy loss, which can be regarded\nas a general loss that combines mean absolute\nerror (MAE) and cross-entropy (CE).\n‚Ä¢ Co-teaching (Han et al., 2018) trains two differ-\nent models and selects small-loss samples to feed\nthem into each other for optimization.\n‚Ä¢ Co-teaching+ (Yu et al., 2019) is an updated ver-\nsion of Co-teaching that explicitly ensures the\ndifference between the two models.\n‚Ä¢ JoCoR (Wei et al., 2020) trains two models and\nselects samples based on the sum of loss from\nthe two models.\n‚Ä¢ CR (Zhou and Chen, 2021) also trains multiple\nmodels with a regularization strategy based on a\nsoft target.\n‚Ä¢ NPC (Bae et al., 2022) utilizes a generative\nmodel to estimate the transition matrix from\nnoisy predictions to the ground-truth labels of\nsamples and uses it to correct noisy labels.\n12539\nD Implementation Details\nIn this section, we introduce the implementation de-\ntails for our experiments. We run the experiments\non a single 48GB Nvidia GeForce RTX A6000\nGPU. The experiments are run 10 times to achieve\nthe average accuracy and the standard deviation.\nFor the augmentations used in our framework,\nfollowing existing works (Xie et al., 2019; Wei and\nZou, 2019; Li et al., 2022), we adopt four typi-\ncal text augmentations: Back Translation, Random\nInsertion, Random Deletion, and Random Swap.\nWe set the threshold ÀÜœÑ for LLM-generated confi-\ndences as 0.8. We set the hyper-parametersŒªand ÀúœÑ\nfor the adaptive threshold ÀúœÑ(t) for PLM-generated\nconfidences as 0.7 and 0.8, respectively. We set\nŒ± as 1.5 and Œ≤ as 0.9. For the loss weight, we\nset ŒªH= ŒªN = 1. We set the model fine-tuning\nlearning rate as 10‚àí4 with a batch size of 32. The\nmaximum length of texts is set as 256. The number\nof training epochs is set as 100 with early stopping\nbased on the performance of the validation set. We\noptimize the model using the Adam (Kingma and\nBa, 2015) optimization strategy. For the LLM, we\nutilize ChatGPT based on GPT4.\nIn addition, we provide more detailed package\nrequirements as listed below.\n‚Ä¢ Python == 3.9.12\n‚Ä¢ torch == 2.0.1\n‚Ä¢ transformers == 4.29.2\n‚Ä¢ huggingface-hub == 0.15.1\n‚Ä¢ keras == 2.12.0\n‚Ä¢ numpy == 1.23.5\n‚Ä¢ scipy == 1.5.3\n‚Ä¢ cuda == 11.6\n‚Ä¢ networkx == 2.5.1\n‚Ä¢ scikit-learn == 0.24.1\n12540",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8114018440246582
    },
    {
      "name": "Fine-tuning",
      "score": 0.6882914900779724
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.6286293864250183
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6246493458747864
    },
    {
      "name": "Noisy data",
      "score": 0.5786709189414978
    },
    {
      "name": "Noise (video)",
      "score": 0.5521480441093445
    },
    {
      "name": "Process (computing)",
      "score": 0.5312765836715698
    },
    {
      "name": "Language model",
      "score": 0.47099679708480835
    },
    {
      "name": "Machine learning",
      "score": 0.46857357025146484
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4570297598838806
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4419512450695038
    },
    {
      "name": "Annotation",
      "score": 0.41825899481773376
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I55732556",
      "name": "Arizona State University",
      "country": "US"
    }
  ],
  "cited_by": 10
}