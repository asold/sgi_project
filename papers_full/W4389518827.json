{
  "title": "Toward Joint Language Modeling for Speech Units and Text",
  "url": "https://openalex.org/W4389518827",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4226677360",
      "name": "Ju-Chieh Chou",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A4225528315",
      "name": "Chung-Ming Chien",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A4209177087",
      "name": "Wei-Ning Hsu",
      "affiliations": [
        "OpenAI (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A320276025",
      "name": "Karen Livescu",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2188726137",
      "name": "Arun Babu",
      "affiliations": [
        "OpenAI (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2555471392",
      "name": "Alexis Conneau",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2893542027",
      "name": "Alexei Baevski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139710560",
      "name": "Michael Auli",
      "affiliations": [
        "OpenAI (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285595742",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3030437843",
    "https://openalex.org/W4300980246",
    "https://openalex.org/W4226103796",
    "https://openalex.org/W4308349017",
    "https://openalex.org/W4226120743",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W3169320628",
    "https://openalex.org/W3119308075",
    "https://openalex.org/W4394671563",
    "https://openalex.org/W4315705838",
    "https://openalex.org/W4313679638",
    "https://openalex.org/W4226065498",
    "https://openalex.org/W3148001440",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3209984917",
    "https://openalex.org/W4385571229",
    "https://openalex.org/W2900260828",
    "https://openalex.org/W4286984129",
    "https://openalex.org/W3095410713",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3140429000",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4223622550",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3207222250",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4221155340",
    "https://openalex.org/W4312121834",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3197580070",
    "https://openalex.org/W2747874407",
    "https://openalex.org/W4312052802",
    "https://openalex.org/W4287079508",
    "https://openalex.org/W4385823130",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4310638188",
    "https://openalex.org/W4375869259",
    "https://openalex.org/W4320194748"
  ],
  "abstract": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model’s learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6582–6593\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nToward Joint Language Modeling for Speech Units and Text\nJu-Chieh Chou1∗, Chung-Ming Chien1, Wei-Ning Hsu2, Karen Livescu1,\nArun Babu2, Alexis Conneau3†, Alexei Baevski4†, Michael Auli2\n1Toyota Technological Institute at Chicago2Meta AI 3OpenAI 4Character AI\njcchou@ttic.edu,wnhsu@meta.com\nAbstract\nSpeech and text are two major forms of hu-\nman language. The research community has\nbeen focusing on mapping speech to text or\nvice versa for many years. However, in the\nfield of language modeling, very little effort\nhas been made to model them jointly. In light\nof this, we explore joint language modeling for\nspeech units and text. Specifically, we com-\npare different speech tokenizers to transform\ncontinuous speech signals into discrete units\nand use different methods to construct mixed\nspeech-text data. We introduce automatic met-\nrics to evaluate how well the joint LM mixes\nspeech and text. We also fine-tune the LM\non downstream spoken language understanding\n(SLU) tasks with different modalities (speech\nor text) and test its performance to assess the\nmodel’s learning of shared representations. Our\nresults show that by mixing speech units and\ntext with our proposed mixing techniques, the\njoint LM improves over a speech-only baseline\non SLU tasks and shows zero-shot cross-modal\ntransferability.\n1 Introduction\nSpeech and language processing research has\nlargely focused on spoken and written language\nseparately. However, the integration of speech\nand text in a single model holds potential bene-\nfits. Speech data contains prosodic information\nthat does not exist in the text, which can help in\nmodeling dialogues. On the other hand, text data\nfrom sources like Wikipedia can provide structural\nknowledge that is not available in most speech\ndatasets. Moreover, the amount of written text\non the internet exceeds the size of any available\nspeech dataset.\nThe impressive performance of text large lan-\nguage models (LLMs) has caused a revolution in\nnatural language processing (Radford et al., 2019;\n∗Work done during an internship at Meta AI.\n†Work done while at Meta AI.\nFigure 1: An illustration of our workflow. We tokenize\nspeech signals into discrete units and mix them with\ntext to create speech-text data. Our SUTLM is then\ntrained on a combination of speech-only, text-only, and\nspeech-text data. More details on the data formats can\nbe found in Table 1.\nBrown et al., 2020). On the other hand, genera-\ntive spoken language models (GSLM) (Lakhotia\net al., 2021), which are LMs trained on discrete\nspeech units derived from self-supervised represen-\ntations (Hsu et al., 2021), are also promising for\nspoken language modeling.\nIn this work, we aim to fill the gap between\ntext-only and speech-only LMs by developing and\nstudying design choices for a joint Speech Unit and\nText Language Model (SUTLM). For speech, we\nuse a self-supervised learning (SSL) speech model,\ni.e. HuBERT (Hsu et al., 2021), to convert con-\ntinuous speech signals into speech units. We then\ncombine the units with text data to train an LM\nthat models speech units and text jointly. We con-\nvert speech-only, mixed speech-text, and text-only\ndata into token sequences (as shown in Figure 1\nand Table 1), and train the model as an LM.\n6582\nTo evaluate the SUTLM, automatic metrics are\ndeveloped to quantify the cross-modal ability of\nthe LMs. We also fine-tune our models on down-\nstream tasks for spoken language understanding.\nWe fine-tune the SUTLMs on either the speech or\ntext data and test them on either speech or text to\nunderstand how well the models learn to align the\ntwo modalities.\nOur main contributions are:\n• We present a joint autoregressive LM trained\non both speech and text (Sec 3).\n• We develop automatic metrics that require no\nfine-tuning for the evaluation of an SUTLM,\nand show that the proposed metrics are indica-\ntive of the model’s cross-modal transfer ability\non downstream tasks (Sec 4).\n• Empirically, we show that units covering a\nlarger span obtained through SentencePiece\ntokenization (Kudo and Richardson, 2018)\noutperform local units learned by existing\nself-supervised models (Hsu et al., 2021)\n(Sec 5.5.1).\n• We find that mixing speech units and text\nwith our proposed techniques (Sec 5.5.3 &\nSec 5.5.4) improves the cross-modal ability of\nthe model. (Sec 5.4).\n2 Related Work\n2.1 SSL speech models\nSelf-supervised pre-training enables speech mod-\nels to learn the information in speech without\npaired text transcriptions and show impressive\nperformance on tasks such as automatic speech\nrecognition (ASR) with minimal supervised fine-\ntuning (Baevski et al., 2020; Hsu et al., 2021; Chen\net al., 2021). As SSL speech models learn pho-\nnetically meaningful speech representations (Pasad\net al., 2023), they can be used as a feature extrac-\ntor (Yang et al., 2021) or a quantizer to transform\ncontinuous speech into discrete units (Lakhotia\net al., 2021; Lee et al., 2021a,b; Lin et al., 2022;\nChen et al., 2022a). In this work, we use the\nHuBERT model (Hsu et al., 2021) along with a\nquantizer to tokenize continuous speech into dis-\ncrete representations. The discrete speech units are\nthen combined with text data to train a single LM\nthat is able to model speech and text jointly.\n2.2 Textless NLP\nTextless NLP (Lakhotia et al., 2021; Polyak et al.,\n2021; Kharitonov et al., 2021) is a framework to\nmodel speech in the absence of textual data. It\nconsists of three components: a speech-to-unit to-\nkenizer, a unit LM (uLM), and a unit-to-speech\ndetokenizer. The tokenizer takes speech signals as\ninputs to generate discrete speech units. A uLM\nis trained to predict the next token in an utterance\ngiven its prior context. Once the uLM is trained, it\ncan be used to generate unit sequences autoregres-\nsively. In the end, the detokenizer is used to convert\nthe generated unit sequences to speech signals.\n2.3 Joint speech-text transformers\nTransformer models have been extremely suc-\ncessful in natural language and speech process-\ning (Vaswani et al., 2017; Gulati et al., 2020),\nwith three major configurations: encoder-decoder\nmodels (Vaswani et al., 2017), encoder-only mod-\nels (Devlin et al., 2018), and decoder-only mod-\nels (Radford et al., 2018).\nPrevious works on speech-text joint transform-\ners mostly adapt the encoder-decoder (Ao et al.,\n2021; Tang et al., 2022; Cheng et al., 2022) or\nencoder-only (Chung et al., 2020; Bapna et al.,\n2021; Chen et al., 2022b; Zhang et al., 2022b) ar-\nchitectures. Compared with decoder-only archi-\ntectures, the training of these models typically re-\nquires multiple losses and explicit alignments be-\ntween paired speech and transcriptions. This makes\nthe hyper-parameter selection time-consuming.\nAlso, encoder-only and encoder-decoder models\nare mostly used in the pre-training + fine-tuning\nparadigm, which limits the use cases of these mod-\nels.\nOn the other hand, decoder-only models on\ntext (Radford et al., 2019; Brown et al., 2020) show\nthe impressive capability of in-context learning,\nwhich also reduces the efforts spent on fine-tuning\npre-trained models. In light of this, we explore\ndecoder-only models for speech-text joint train-\ning. In this under-explored area, the concurrent\nwork V ALL-E (Wang et al., 2023) is the only other\nattempt to build a decoder-only model jointly mod-\neling speech and text. However, V ALL-E’s purpose\nis controllable text-to-speech synthesis (TTS), and\nthe work mainly focuses on the acoustic controlla-\nbility of the generated speech , while our work aims\nto build a general-purpose joint LM and mainly fo-\ncuses on modeling the content of spoken language.\n6583\n3 Method\nWe start with a dataset of sentences D =\n{s1,s2,...,s n}, where a sentence si is composed\nof a sequence of Ti tokens (zi\n1,zi\n2,...,z i\nTi ), where\nzi\nj can be either text or speech units. The SUTLM\nis trained to predict the next tokenzi\nj given its prior\ncontext zi\n<j. We maximize the log-probability of\nthe data\nn∑\ni=1\nTi∑\nj=1\nlog P(zi\nj|zi\n<j) (1)\nIn the following sections, we describe how we con-\nstruct token sequences from speech and text. An\nexample of our data formats can be found in Ta-\nble 1.\n3.1 Speech-only: unit LM (uLM)\nPrior work has shown that discrete speech units\nderived from a pre-trained HuBERT model can be\nused as compact representations to encode speech\ncontent, enabling the training of a unit language\nmodel (Lakhotia et al., 2021). However, when com-\nbining speech with text, the time scales of speech\nunits and text differ. HuBERT units are typically\non the phone or sub-phone level, as shown in Ta-\nble 2. This leads to longer sequences, making it\ndifficult for the model to capture long-term depen-\ndencies. On the other hand, subword tokenizers for\ntext generally break text sequences into chunks of a\nlarger size than speech units. This length mismatch\nbetween speech and text makes it challenging to\nmodel them in a single model. Therefore, we use\na subword tokenizer (Kudo and Richardson, 2018)\nto combine HuBERT units into larger chunks as\nin (Wu et al., 2022) to mitigate the length mis-\nmatch.\nThe process of generating speech units is as\nfollows. Speech signals are first fed into a Hu-\nBERT model. The representations in the final\nlayer are then clustered with the k-means algo-\nrithm. The cluster IDs are used as the discrete\nspeech units after removing consecutive repeating\nunits (Lakhotia et al., 2021).1 These units are then\nfurther combined by the subword SentencePiece\ntokenizer (Kudo and Richardson, 2018). The re-\nsulting average number of tokens per second can\nbe found in Table 2.\n1For example, the unit sequence 13 13 15 80 80 80\nbecomes 13 15 80 after removing repetitions.\n3.2 Text-only: text LM (tLM)\nWe train another SentencePiece tokenizer (Kudo\nand Richardson, 2018) using the text-only cor-\npus Sec 5.1.3 to convert text into subword tokens.\nThe resulting vocabulary size of the subword to-\nkens is around 45k.\n3.3 Concatenated speech-text (CST)\nTo present paired speech-text data to the SUTLM,\nwe first convert speech units and their transcriptions\ninto the uLM and tLM formats, respectively, and\ncombine them into one sequence by simply concate-\nnating them as shown in Table 1. The CST format\nexplicitly tells the model the correspondence be-\ntween paired speech and text and thus encourages\nthe model to learn the dependence between speech\nunits and the corresponding text transcriptions.\n3.4 Alternating speech-text (AST)\nAside from simply concatenating the sequences\nof speech units and text, we also construct mixed\nspeech-text that takes the word-level correspon-\ndence into consideration.\nWe use a pre-trained speech recog-\nnizer (McAuliffe et al., 2017) to force-align\nspeech and its transcription to obtain the word\nboundaries in an utterance. We then randomly\nsample some word boundaries within the utter-\nance2 as the \"switching points\", which divide the\nutterance into several chunks. The alternating\nspeech-text (AST) sequence is then constructed by\nalternatively filling in the chunks with uLM speech\nunits and tLM text tokens, resulting in a sequence\nthat switches modalities at every switching point.\nSpecial tokens <U2T> and <T2U> are inserted when\nswitching from speech units to text and text to\nspeech units, respectively.\n4 Evaluation Metrics\nWe introduce automatic metrics that require no\nfine-tuning to evaluate the SUTLM. Fine-tuning\nis a common approach to assess the quality of pre-\ntrained models (Baevski et al., 2020; Hsu et al.,\n2021; Chen et al., 2021). However, it is a time-\nconsuming process and the reliability of the ex-\nperiments highly depends on the hyper-parameter\nselection process. Furthermore, there is no reliable\nmetric to measure the cross-modal ability of LMs.\n2For a sentence with k words, we uniformly sample ⌊N⌋\nboundaries as the switching points with N ∼N( k\n10 , 1).\n6584\nTask Example\nuLM <U_EN> S12 S66 S17 S18 ... <EOU>\nCST <U_EN> S12 S66 S17 S18 ... <EOU> <T_EN> how are you <EOS>\nCST <T_EN> how are you <EOS> <U_EN> S12 S66 S17 S18 ...<EOU>\nAST <U_EN> S12 S66 <U2T> are you <EOS>\nAST <T_EN> how <T2U> S17 S18 ... <EOU>\ntLM <T_EN> how are you <EOS>\nTable 1: An example of the formats of unpaired (uLM, tLM) and mixed speech-text (CST, AST) data. For the CST\nand AST formats, speech units and text can be present in a sequence in different orders . <U_EN> and <T_EN> are\nused at the beginning of the unit/text sequence. <EOU> and <EOS> are used at the end of the unit/text sequences.\n<U2T> and <T2U> are used when switching from unit to text and text to unit at word boundaries.\nAverage tokens per second\nPhone 20.32\nHuBERT 50.00\n+ deduplication 33.33\n+ SP 10k 17.67\n+ SP 32k 14.33\nTable 2: The average number of tokens per second for\ndifferent types of speech units. SP 10k and 32k refer\nto SentencePiece tokenization (Kudo and Richardson,\n2018) applied to HuBERT units to create a dictionary\nwith 10k and 32k tokens respectively.\nIn light of this, we propose Context Retrieval Ac-\ncuracy (CRA), a new metric that does not require\nfine-tuning, to evaluate the cross-modal ability of\nan SUTLM.\n4.1 Context Retrieval Accuracy (CRA)\nThe motivation of Context Retrieval Accuracy\n(CRA) comes from the intuition that a good LM\nshould learn to predict the next token based on\nits prior context. When we divide a sentence into\nprompt and continuation, a good LM should be\nable to capture the dependence between them. That\nis, it should assign a higher conditional probability\nto the continuation given its corresponding prompt\nthan given a random prompt.\nTo measure CRA, we gather a collection of m\nsentences C= {s1,s2,...,s m}and break si into\na pair of prompt xi and continuation yi. Given an\nSUTLM parameterized by θ, we can measure the\nconditional probabilities Pθ(yi|xi) with Eq 1. The\nCRA is then computed as:\n1\nm\nm∑\ni=1\n1 [arg max\nj∈{1...m}\nPθ(yi|xj) =i], (2)\nThat is, the LM is used as a scorer to classify\nwhether the matched prompt-continuation pair has\nthe highest conditional probability among a pool\nof unmatched prompts.\nCRA also has a pointwise mutual information\n(PMI) interpretation:\narg max\nj∈{1...m}\nPθ(yi|xj) =i\n=⇒log Pθ(yi|xi) ≥ max\nj∈{1...m}\nlog Pθ(yi|xj)\n=⇒log Pθ(yi|xi)\nPθ(yi) ≥ max\nj∈{1...m}\nlog Pθ(yi|xj)\nPθ(yi)\n=⇒PMI(xi,yi) ≥ max\nj∈{1...m}\nPMI(xj,yi)\n(3)\nThat is, correctly identifying the prompt implies\nthe matched prompt-continuation pair has a higher\nPMI than all unmatched prompt-continuation\npairs.\nIdeally, the model should produce similar rep-\nresentations given the same content regardless of\nthe modality. Hence, in addition to the uni-modal\nCRA, we also consider cross-modal CRA, where\nthe prompt and the continuation are in different\nmodalities. In practice, for example, when we use\ntext as the prompts and speech units as the con-\ntinuations, we set the probability of emitting text\ntokens to zero and re-normalize the probability to\nensure that the continuation yi can be only speech\nunits. Cross-modal CRA can be used as a way to\nmeasure whether the SUTLM successfully learns\nshared representations between text and speech.\n4.2 Perplexity under External LM (PELM)\nFollowing previous work, we use the perplexity\nunder external LM (PELM) to measure the quality\nof the content of generated samples (Lakhotia et al.,\n2021). We sample a continuation from the SUTLM\ngiven each ground truth prompt. We then use an\nexternal text LM, OPT-6.7B (Zhang et al., 2022a),\n6585\nto compute the perplexity of the sequence:\nˆyi ∼Pθ(y|xi)\nx′i,y′i = T(xi ∥ˆyi)\nPELM(θ) = 2\n−∑\nilog POPT(y′i|gt(xi))∑\nilen(y′i)\n(4)\nwhere xi and ˆyi refer to the prompt and sam-\npled continuation, and θare the parameters of the\nSUTLM. Similarly to cross-modal CRA, we con-\ntrol the modality of sampled continuations by ze-\nroing out the probability of the tokens in the unde-\nsired modality. Since the prompt and the contin-\nuation can be either speech units or subword text\ntokens, we use a transcriber T(·) to transcribe the\nconcatenated sequences xi ∥ˆyi into text x′i,y′i.3\ngt(·) is a function that outputs a ground truth tran-\nscription when the input is speech units and is an\nidentity function when the input is text. The exter-\nnal LM is then used to measure the perplexity of\nthe continuation part of the text sequence.\n4.3 Evaluation on SLUE tasks\nWe use the SLUE benchmark (Shon et al., 2022)\nto evaluate our models on downstream tasks. The\nbenchmark includes two tasks, sentiment analysis\n(SLUE-SA) and named entity recognition (SLUE-\nNER), with both speech data and transcriptions\nprovided. After pre-training the SUTLM, we fine-\ntune it on the SLUE dataset with either speech or\ntext data as inputs to predict the ground-truth labels,\nand then evaluate it on either speech or text inputs.\nWe evaluate the model on different input modalities\nto understand the cross-modal ability of the model\nas in (Hsu and Shi, 2022; Bapna et al., 2021, 2022).\nFine-tuning details can be found in 5.4.2.\n5 Experiments\n5.1 Data\n5.1.1 Speech-only\nWe use 5% of the dataset used in (Aghajanyan\net al., 2023) to match the size of the mixed speech-\ntext and text-only data. The dataset includes\nMultilingual LibriSpeech (MLS) (Pratap et al.,\n2020), V oxPopuli (Wang et al., 2021), Common-\nV oice (Ardila et al., 2019) and Spotify Podcast &\n3For both speech units and text tokens, we first invert the\nSentencePiece tokenization process to get raw HuBERT units\nand raw text. For speech units, we further use a 12-layer\nTransformer encoder with a CTC head to map HuBERT units\nto text. The transformer is trained on LibriSpeech, with a\nWER of 5.18% on dev-clean, and 11.61% on dev-other.\nPeople’s Speech (Aghajanyan et al., 2023). The\nsubsampled dataset consists of 65k hours of speech.\n5.1.2 Mixed speech-text (CST and AST)\nWe use MLS (Pratap et al., 2020) and V oxPop-\nuli (Wang et al., 2021) to create mixed speech-text\ndata without subsampling. The dataset contains\n45k hours of speech and 2.7B of words.\n5.1.3 Text-only\nWe combine OPT web data (Zhang et al., 2022a),\nWikipedia, and LibriLM (Panayotov et al., 2015),\nand then subsample 5% of it, resulting in a total of\n8.5B subwords.\n5.2 SSL speech tokenizer\nWe use a HuBERT Base model trained on 221K\nhours of unlabeled speech in 8 languages as in (Hsu\net al., 2022; Nguyen et al., 2023). 4 After pre-\ntraining, the representations at the last layer (12th)\nare clustered with k-means using 2000 clusters.\n5.3 Model architecture and training\nWe use the 24-layer transformer implementation in\nfairseq (Ott et al., 2019) with 16 attention heads.\nThe embedding size is 1024, the feed-forward di-\nmension is 4096, and the dropout probability is set\nto 0.1. The weights of the embedding layer are tied\nto the output layer (Press and Wolf, 2016). The\nmodel contains 350M parameters.\nThe model is trained for 500k updates on 32\nV100 GPUs with a batch size of 8192 tokens per\nGPU. We use Adam optimizer (Kingma and Ba,\n2014) with (β1,β2) = (0.9, 0.95). Gradient clipping\nwith a threshold 1.0 and weight decay of 0.1 are\napplied to stabilize the training. Since the data size\nis different for different data formats, we resample\nspeech-only, speech-text, and text-only data equally\n(1/3 for each in every training batch) to prevent the\nmodel from being biased toward any of them.\n5.4 Evaluation setup\n5.4.1 Automatic Metrics\nWe use a subset of the Multilingual Lib-\nriSpeech (Pratap et al., 2020) dev set to evaluate\nthe SUTLM. To provide enough context to the\nSUTLM, we filter out sentences of less than 20\nwords. For each sentence, we use the first 10 words\nas the prompt and the rest as continuation. For the\nCRA experiments, we evaluate the SUTLM with\n4https://dl.fbaipublicfiles.com/hubert/\nmhubert_base_vp_mls_cv_8lang_it3.pt\n6586\nthe 100 shortest utterances in the filtered dataset,\nwhile for the PELM experiments, we use the 500\nshortest utterances. We use fewer utterances in\nCRA experiments as the computation of CRA is\nO(N2) for N utterances. We constrain ourselves\nto sentences with moderate lengths because the\ncontinuation part becomes less coherent with the\nprompt as the sequence length grows, which hurts\nthe sensitivity of the proposed metrics.\nWhen sampling the speech or text continuations\nin the PELM experiments, we use temperature t=\n0.6 and nucleus sampling (Holtzman et al., 2019)\nwith p= 0.95, and truncate the continuation to 10\nwords (identical to the length of the prompts).\n5.4.2 Downstream Tasks\nFor SLUE-SA, we fine-tune SUTLM by adding\na self-attention pooling layer on top of the trans-\nformer model after removing the last output\nlayer (Shon et al., 2022). We fine-tune it with a\nlearning rate of 3e-5 for 30k updates and evaluate\nit with Macro F1 (Shon et al., 2022).\nFor SLUE-NER, we follow the SLUE official\nbaseline to formulate the task as an ASR prob-\nlem and train our model to decode special tokens\naround each named entity (Shon et al., 2022). We\nconcatenate the output (the text transcription with\nspecial tokens before and after each named entity)\nafter the input (speech units when fine-tuned on\nspeech, text tokens when fine-tuned on text) and\nfine-tune our SUTLM as an LM with the same loss\nfunction as Eq 1. The loss is only applied to the out-\nput part of the sequence. We fine-tune the SUTLM\nwith a learning rate of 3e-5 for 50k updates. During\ndecoding, we use a beam size of 5 to generate the\noutputs and evaluate them with Micro F1 (Shon\net al., 2022). For both SLUE tasks, we report re-\nsults on the dev set since the test set is not publicly\navailable. We use the fine-tuned HuBERT as the\nbaseline as in (Shon et al., 2022).\n5.5 Results\n5.5.1 What kind of speech units works the\nbest?\nWe utilize HuBERT units described in Sec 5.2\n(2000 units) and apply SentencePiece tokenizers\non them. Results can be found in rows (A), (B),\n(C) in Table 3 for automatic metrics, Table 4 for\nSLUE-SA and Table 5 for SLUE-NER.\nThe model trained with SP 10k has the best per-\nformance in terms of PELM, SLUE-SA, and SLUE-\nNER, but slightly worse CRA than the model using\nthe original HuBERT units. For CRA for the u2u\ncase (unit prompt, unit continuation), we hypoth-\nesize that the model uses low-level acoustic infor-\nmation to make predictions as the CRAs are nearly\n1.0 for all types of speech units. Also, HuBERT\nuses overlapping windows for neighboring tokens,\nso the first token of the continuation contains infor-\nmation about the previous token.\nFor the speech continuation (PELM) experi-\nments, the SP 10k-based sequences are shorter\nthan HuBERT unit-based sequences, so the model\ntrained with SP 10k (row (B)) can generate more\ncoherent continuations.\n5.5.2 Do we need paired data to learn shared\nrepresentations?\nIn this section, we compare models trained with\nand without paired data to investigate the useful-\nness of paired data. We can compare the results\nin row (D) and (F) in Table 3 for automatic met-\nrics, Table 4 for SLUE-SA and Table 5 for SLUE-\nNER. For cross-modal cases (u2t and t2u), in terms\nof automatic metrics, the model trained with un-\npaired data alone (row (D)) has almost random\nCRAs and high PELMs, indicating a lack of cross-\nmodal ability.\nSimilarly, for SLUE-SA, the model trained with\nunpaired data alone (row (D)) shows almost ran-\ndom macro F1 scores for a 3-way classification task\nwhen tested on the other modality. For SLUE-NER,\nthe model trained without exposure to paired data\n(row (D)) performs worse than models trained with\npaired data (row (F)) when fine-tuned on speech\nand shows no transferability between modalities.\nRow (D) also performs worse than its speech unit-\nonly counterpart (row (B), showing that the model\ntrained solely on unpaired data does not demon-\nstrate any cross-modal transfer ability between\nspeech and text.\n5.5.3 Does concatenated speech-text (CST)\nhelp learn shared representations?\nThe next question we want to answer is whether\nCST is helpful in learning shared representations.\nBuilding on the previous findings (rows (A), (B),\n(C)), we utilize SP 10k as our speech unit vocab-\nulary and present the results in row (E) in Table 3\nfor automatic metrics, Table 4 for SLUE-SA, and\nTable 5 for SLUE-NER. The results show that, com-\npared to using unpaired data alone (row (D)), the\nmodel trained with CST (row(E)) has higher CRAs\nfor u2t and t2u, which indicates that the model cap-\n6587\nu2u t2u u2t t2t\nrow unit uLM CST AST tLM CRA PELM CRA PELM CRA PELM CRA PELM\nGround truth continuation - - - - - - - 101.4\n(A) HuBERT v 1.00 193.3 - - - - - -\n(B) SP 10k v 0.96 163.6 - - - - - -\n(C) SP 32k v 0.96 177.4 - - - - - -\n(D) SP 10k v v 0.94 175.9 0.03 394.9 0.01 1973.3 0.20∗∗ 20.7∗∗\n(E) SP 10k v v 0.95 166.0 0.37 39.1 ∗ 0.26 43.4 ∗ 0.56 34.7\n(F) SP 10k v v v v 0.97 162.8 0.70 124.7 0.81 38.7 0.67 28.2\nTable 3: Automatic metrics (CRA and PELM). \"u2t\" denotes that the prompts are speech units and the continuations\nare text, and so on. (*): for cross-modal cases (u2t and t2u) in row (E), the PELM is low because the continuation\nsimply repeats the prompt. We discuss this issue in Sec 5.6. (**): The low CRA for t2t is due to the use of MLS as\nan evaluation set, resulting in a distribution mismatch from the text-only training data. Similarly, the use of OPT\ndata to train the SUTLM results in better PELM on t2t in row (D).\nFT data SP TXT\nrow unit Eval set SP TXT SP TXT\nBaseline 0.46 - - -\n(A) HuBERT uLM 0.51 - - -\n(B) SP 10k uLM 0.56 - - -\n(C) SP 32k uLM 0.54 - - -\n(D) SP 10k uLM+tLM0.52 0.33 0.35 0.49\n(E) SP 10k uLM+CST0.48 0.42 0.51 0.52\n(F) SP 10k uLM+CST\n+AST+tLM0.49 0.43 0.52 0.56\nTable 4: Macro F1 score on SLUE-SA. FT data indicates\nthe model is fine-tuned on speech (SP) or text (TXT).\nEval set denotes the fine-tuned model is tested on speech\n(SP) or text (TXT).\nFT data SP TXT\nrow unit Eval set SP TXT SP TXT\nBaseline 54.5 - - -\n(A) HuBERT uLM 62.9 - - -\n(B) SP 10k uLM 64.4 - - -\n(C) SP 32k uLM 62.5 - - -\n(D) SP 10k uLM+tLM 63.2 1.5 0.0 66.8\n(E) SP 10k uLM+CST 65.0 3.6 0.5 79.5\n(F) SP 10k uLM+CST\n+AST+tLM66.6 25.2 0.3 77.2\nTable 5: The F1(%) score on SLUE-NER. FT data in-\ndicates the model is fine-tuned on speech (SP) or text\n(TXT). Eval set denotes the fine-tuned model is tested\non speech (SP) or text (TXT).\ntures the relationship between speech and text bet-\nter than models trained with unpaired data alone.\nFor SLUE-SA, the model pre-trained with CST\nshows comparable performance when fine-tuned\non one modality and evaluated on the other. The\nperformance when fine-tuning on text and testing\non speech is even better than directly fine-tuning\non speech (0.51 vs. 0.48). The reason is likely\nto be that text data provides a less noisy supervi-\nsory signal compared to using speech units. The\nmodel trained with extra speech-text data (row (E))\nperforms worse than the model trained with only\nspeech units (row (B)). The reason may be similar\nto the \"curse of multilinguality\" (Conneau et al.,\n2019), where sharing the capacity of the model\nwith other languages or modalities hurts perfor-\nmance.\nFor SLUE-NER, concatenated speech-text im-\nproves performance over the model trained with\nonly speech units (row (B)) when fine-tuned on\nspeech. Unlike SLUE-SA, which is a classifica-\ntion task, here we need to generate the correspond-\ning transcription along with the named entity tags\nfor SLUE-NER. Hence, the model (row (E)) fine-\ntuned on speech benefits directly from the extra\nspeech-text data. We discuss the implications of\nthe fine-tuning results further in Sec 5.7.\nFor speech / text continuation, when only using\nconcatenated speech-text data (CST) as our mixed\ndata, there are no special tokens (<U2T>, <T2U>)\nto trigger modality switching. As shown in Table 6,\nin the u2t case the model trained with CST simply\ntranscribes the speech prompt into its transcription\non u2t and synthesizes the text prompt into speech\nunits, resulting in low PELMs for u2t and t2u in row\n(D) due to the repetition. PELM fails to reflect the\nquality of the continuation accurately. We discuss\nthis limitation further in Sec 5.6.\n5.5.4 Does alternating speech-text (AST) help\nlearn shared representations?\nThis section discusses the benefits of alternating\nspeech-text (AST). The results are presented in\n(row (F)) in Table 3 for automatic metrics, Table 4\n6588\nfor SLUE-SA, and Table 5 for SLUE-NER.\nBy comparing the results of CRA for t2u and\nu2t in row (F) with those in row (E) in Table 3, we\nobserve an improvement in CRA when the data is\ndirectly constructed to switch modalities on word\nboundaries. We can also see that CRA is similar for\nt2u, u2t, and t2t. It suggests that the model learns\nto match context regardless of modality.\nIn row (F), PELM for t2u is lower than PELM\nfor u2u as the text prompt is less noisy than speech\nunits. PELM for u2t is only marginally worse than\nt2t. This shows that the LM trained with AST can\ncontinue a sentence regardless of the modality. The\nworse PELM for u2u and t2u than for u2t and t2t\ncould be attributed to the recognition errors within\nour unit transcriber.\nRegarding SLUE-SA, we can observe that AST\nand tLM further improve the cross-modal transfer\nperformance (trained on the text and evaluated on\nspeech, or vice versa) in row (F).\nIn SLUE-NER, row (F) also shows better per-\nformance than row (E) when fine-tuned on speech\nand evaluated on speech. There is also non-trivial\nspeech-to-text transfer (fine-tuned on speech and\nevaluated on text) in row (F), showing that AST\nhelps in learning transferable features between\nmodalities.\nIn SLUE-NER, when fine-tuned on text and eval-\nuated on speech, there is no transferability between\nspeech and text. The reason can be attributed to the\nfine-tuning task becoming almost trivial. In text\nNER, in our formulation, the input and output are\nnearly identical. The only difference is the named\nentity tags. Further discussion of downstream task\nperformance can be found in Sec 5.7.\n5.6 Limitations of PELM\nWe use PELM as a metric to measure the quality\nof continuations. However, although our SUTLM\n(row (F)) shows the ability to continue after a cross-\nmodal prompt, the resulting continuation is still\nonly locally consistent as shown in Table 6. This\ncan be attributed to the use of a 350M-parameter\nmodel architecture, which is relatively small in the\nera of LLMs.\nThe PELM metric fails to accurately reflect the\nresult in the case of row (E) when the model sim-\nply repeats the prompt. It has been a known phe-\nnomenon that LMs tend to assign a high probability\nto repeated tokens (Holtzman et al., 2019).\nTo quantify repetition, we compute the propor-\ntion of bi-grams in continuations that have ap-\npeared in the prompt transcription. For row (E),\nthe proportions are 0.02, 0.53, 0.42, and 0.02 for\nu2u, u2t, t2u, and t2t, respectively. For row(F), the\nproportions are 0.02, 0.03, 0.01, and 0.03. For row\n(E), the continuations for u2t and t2u are simply\nrepeating the content of the prompt.\nWe can see that the u2t and t2t PELMs are lower\nthan the ground truth PELM. This is because of\nthe use of the temperature of 0.6 in the softmax\nlayer, which likely hurts diversity and coherence as\nin (Caccia et al., 2018; Lakhotia et al., 2021).\n5.7 Implications for SLU Downstream Tasks\nWe show that mixing speech units and text im-\nproves the cross-modal ability of the model. In\nSLUE-SA, the mixed speech-text data enables the\nmodel to zero-shot transfer between speech and\ntext. In SLUE-SA, we remove the output layer\nfrom the SUTLM and attach a classification head\nso the model will always output a valid class.\nIn SLUE-NER, using mixed speech-text data\ndirectly improves the performance. Since this is\na sequence generation task, the mixed speech-text\ndata helps the model generate better text. The trans-\nfer from speech to text is non-trivial but not vice\nversa. This finding aligns with the experiments\nin (Bapna et al., 2022), in which they also find\nnon-trivial transfer from speech to text but not the\nother way around. However, we note that different\nfine-tuning strategies can produce different results,\nas demonstrated in (Liu et al., 2021).\n6 Conclusion\nOur study on joint language modeling for speech\nunits and text involved developing evaluation met-\nrics and fine-tuning the model on speech and text\ndata. We found that using mixed speech-text data\nimproves the model’s cross-modal ability and per-\nformance on both automatic metrics and down-\nstream tasks.\nOur study sheds light on the benefits of consid-\nering both speech and text in building language\nmodels. We hope that this research will motivate\nthe research community to further explore the in-\ntegration of speech and text data for more compre-\nhensive language modeling.\nFuture work in this area could involve investigat-\ning the optimal balance between speech and text\ndata in model training and exploring ways to handle\nmulti-modal data beyond the speech-text domain.\n6589\n7 Limitations\nOur approach involves using a speech tokenizer\nthat can encode phonetic information (HuBERT)\nand an off-the-shelf speech recognizer to generate\nword-level alignment. For other, lower-resource\nlanguages, these components may be harder to ob-\ntain or may not perform as well.\nFor our proposed automatic metrics, the com-\nplexity of CRA grows at a rate of O(N2), which\ncan be expensive when evaluated on a larger num-\nber of utterances or when scaling up the model size.\nPELM, on the other hand, also has limitations as\nstated in Sec 5.6. For the empirical results on down-\nstream tasks, we test our SUTLMs on the SLUE\nbenchmark, which has only two tasks. Extending\nthe experiments to more downstream tasks may\nprovide more insights.\nFinally, we only study relatively small SUTLMs\n(350M parameters). It is unclear how scaling it up\nwould affect the results.\nReferences\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-\nNing Hsu, Karen Hambardzumyan, Susan Zhang,\nStephen Roller, Naman Goyal, Omer Levy, and\nLuke Zettlemoyer. 2023. Scaling laws for genera-\ntive mixed-modal language models. arXiv preprint\narXiv:2301.03728.\nJunyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo\nRen, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang,\net al. 2021. SpeechT5: Unified-modal encoder-\ndecoder pre-training for spoken language processing.\narXiv preprint arXiv:2110.07205.\nRosana Ardila, Megan Branson, Kelly Davis, Michael\nHenretty, Michael Kohler, Josh Meyer, Reuben\nMorais, Lindsay Saunders, Francis M Tyers, and\nGregor Weber. 2019. Common voice: A massively-\nmultilingual speech corpus. arXiv preprint\narXiv:1912.06670.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nAdvances in Neural Information Processing Systems,\n33:12449–12460.\nAnkur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin\nJohnson, Yong Cheng, Simran Khanuja, Jason Riesa,\nand Alexis Conneau. 2022. mSLAM: Massively mul-\ntilingual joint pre-training for speech and text. arXiv\npreprint arXiv:2202.01374.\nAnkur Bapna, Yu-an Chung, Nan Wu, Anmol Gulati,\nYe Jia, Jonathan H Clark, Melvin Johnson, Jason\nRiesa, Alexis Conneau, and Yu Zhang. 2021. SLAM:\nA unified encoder for speech and language model-\ning via speech-text joint pre-training. arXiv preprint\narXiv:2110.10329.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2018. Language gans falling short. arXiv preprint\narXiv:1811.02549.\nPeng-Jen Chen, Kevin Tran, Yilin Yang, Jingfei Du,\nJustine Kao, Yu-An Chung, Paden Tomasello, Paul-\nAmbroise Duquenne, Holger Schwenk, Hongyu\nGong, et al. 2022a. Speech-to-speech translation\nfor a real-world unwritten language. arXiv preprint\narXiv:2211.06474.\nSanyuan Chen, Chengyi Wang, Zhengyang Chen,\nYu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\nKanda, Takuya Yoshioka, Xiong Xiao, et al. 2021.\nWavLM: Large-scale self-supervised pre-training\nfor full stack speech processing. arXiv preprint\narXiv:2110.13900.\nZhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana\nRamabhadran, Pedro Moreno, Ankur Bapna, and\nHeiga Zen. 2022b. MAESTRO: Matched speech text\nrepresentations through modality matching. arXiv\npreprint arXiv:2204.03409.\nYong Cheng, Yu Zhang, Melvin Johnson, Wolfgang\nMacherey, and Ankur Bapna. 2022. Mu 2SLAM:\nMultitask, multilingual speech and language mod-\nels. arXiv preprint arXiv:2212.09553.\nYu-An Chung, Chenguang Zhu, and Michael Zeng.\n2020. SPLAT: Speech-language joint pre-training\nfor spoken language understanding. arXiv preprint\narXiv:2010.02295.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, et al.\n2020. Conformer: Convolution-augmented trans-\nformer for speech recognition. arXiv preprint\narXiv:2005.08100.\n6590\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdel-\nrahman Mohamed. 2021. HuBERT: Self-supervised\nspeech representation learning by masked prediction\nof hidden units. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 29:3451–3460.\nWei-Ning Hsu, Tal Remez, Bowen Shi, Jacob Don-\nley, and Yossi Adi. 2022. ReVISE: Self-supervised\nspeech resynthesis with visual input for universal\nand generalized speech enhancement. arXiv preprint\narXiv:2212.11377.\nWei-Ning Hsu and Bowen Shi. 2022. u-HuBERT: Uni-\nfied mixed-modal speech pretraining and zero-shot\ntransfer to unlabeled modality. InAdvances in Neural\nInformation Processing Systems.\nEugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi,\nJade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Mor-\ngane Rivière, Abdelrahman Mohamed, Emmanuel\nDupoux, et al. 2021. Text-free prosody-aware gen-\nerative spoken language modeling. arXiv preprint\narXiv:2109.03264.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,\nYossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh\nNguyen, Jade Copet, Alexei Baevski, Abdelrahman\nMohamed, et al. 2021. On generative spoken lan-\nguage modeling from raw audio. Transactions of the\nAssociation for Computational Linguistics, 9:1336–\n1354.\nAnn Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu,\nSravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi,\nQing He, Yun Tang, et al. 2021a. Direct speech-to-\nspeech translation with discrete units. arXiv preprint\narXiv:2107.05604.\nAnn Lee, Hongyu Gong, Paul-Ambroise Duquenne,\nHolger Schwenk, Peng-Jen Chen, Changhan Wang,\nSravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu,\nand Wei-Ning Hsu. 2021b. Textless speech-to-\nspeech translation on real data. arXiv preprint\narXiv:2112.08352.\nGuan-Ting Lin, Yung-Sung Chuang, Ho-Lam Chung,\nShu-wen Yang, Hsuan-Jui Chen, Shang-Wen Li, Ab-\ndelrahman Mohamed, Hung-yi Lee, and Lin-shan\nLee. 2022. DUAL: Textless spoken question answer-\ning with speech discrete unit adaptive learning. arXiv\npreprint arXiv:2203.04911.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. arXiv preprint arXiv:2103.10385.\nMichael McAuliffe, Michaela Socolof, Sarah Mihuc,\nMichael Wagner, and Morgan Sonderegger. 2017.\nMontreal forced aligner: Trainable text-speech align-\nment using kaldi. In Interspeech, volume 2017, pages\n498–502.\nTu Anh Nguyen, Wei-Ning Hsu, Antony d’Avirro,\nBowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Re-\nmez, Jade Copet, Gabriel Synnaeve, Michael Hassid,\net al. 2023. Expresso: A benchmark and analysis of\ndiscrete expressive speech resynthesis. Interspeech.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for se-\nquence modeling. arXiv preprint arXiv:1904.01038.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015. Librispeech: an asr cor-\npus based on public domain audio books. In 2015\nIEEE international conference on acoustics, speech\nand signal processing (ICASSP), pages 5206–5210.\nIEEE.\nAnkita Pasad, Bowen Shi, and Karen Livescu. 2023.\nComparative layer-wise analysis of self-supervised\nspeech models. In ICASSP.\nAdam Polyak, Yossi Adi, Jade Copet, Eugene\nKharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-\ndelrahman Mohamed, and Emmanuel Dupoux.\n2021. Speech resynthesis from discrete disentan-\ngled self-supervised representations. arXiv preprint\narXiv:2104.00355.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel\nSynnaeve, and Ronan Collobert. 2020. MLS: A large-\nscale multilingual dataset for speech research. arXiv\npreprint arXiv:2012.03411.\nOfir Press and Lior Wolf. 2016. Using the output em-\nbedding to improve language models. arXiv preprint\narXiv:1608.05859.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSuwon Shon, Ankita Pasad, Felix Wu, Pablo Brusco,\nYoav Artzi, Karen Livescu, and Kyu J Han. 2022.\nSLUE: New benchmark tasks for spoken language un-\nderstanding evaluation on natural speech. In ICASSP\n2022-2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n7927–7931. IEEE.\n6591\nYun Tang, Hongyu Gong, Ning Dong, Changhan\nWang, Wei-Ning Hsu, Jiatao Gu, Alexei Baevski,\nXian Li, Abdelrahman Mohamed, Michael Auli,\net al. 2022. Unified speech-text pre-training for\nspeech translation and recognition. arXiv preprint\narXiv:2204.05409.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu,\nChaitanya Talnikar, Daniel Haziza, Mary Williamson,\nJuan Pino, and Emmanuel Dupoux. 2021. V oxPop-\nuli: A large-scale multilingual speech corpus for rep-\nresentation learning, semi-supervised learning and\ninterpretation. arXiv preprint arXiv:2101.00390.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, et al. 2023. Neural codec\nlanguage models are zero-shot text to speech synthe-\nsizers. arXiv preprint arXiv:2301.02111.\nFelix Wu, Kwangyoun Kim, Shinji Watanabe, Kyu\nHan, Ryan McDonald, Kilian Q Weinberger, and\nYoav Artzi. 2022. Wav2Seq: Pre-training speech-to-\ntext encoder-decoder models using pseudo languages.\narXiv preprint arXiv:2205.01086.\nShu-wen Yang, Po-Han Chi, Yung-Sung Chuang,\nCheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin,\nAndy T Liu, Jiatong Shi, Xuankai Chang, Guan-\nTing Lin, et al. 2021. SUPERB: Speech processing\nuniversal performance benchmark. arXiv preprint\narXiv:2105.01051.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022a. OPT: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nZiqiang Zhang, Sanyuan Chen, Long Zhou, Yu Wu,\nShuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong,\nLirong Dai, Jinyu Li, et al. 2022b. SpeechLM: En-\nhanced speech pre-training with unpaired textual data.\narXiv preprint arXiv:2209.15329.\n6592\nA Appendix\nprompt they need the same aid and they\nmodel modalities Continuation\n(E)\nu2u soon will increase and reach the grave more gradually than ever they seek\ntheir own home and many little people suffer from the hospitality and care of\nthe poor\nu2t they need the same aid and they need the same aid\nt2u they need they they need the sameling the need the need the same sense of\nwill that they need for others others to to to to do they will a and and they to\nhave\nt2t need the same help they can make and are able to make the best of them and\nthe best of them\n(F)\nu2u certainly do need the same money and they do not give themselves out of\neither field or settlement but they do need it they say and they need it\nu2t shall be able to perform it all the same the king of egypt is not free from the\npersecution of the jews he is free from the persecution of the jews\nt2u can no longer face it but though these men believe women now to be the\nprincipal duties the progress made will actually continue\nt2t will be the same as they have been when they were born and they will be the\nsame as they now are if they were not born they would be the same as they\nnow are\nprompt barrack was just coming through the doorway carrying half\nmodel modalities Continuation\n(E)\nu2u doesn papa and several small scotch girls who had been sitting by the fire with\nhim he said to the girls on whom he kept his second mother mother is good to\nme and so i would not let her go away\nu2t barrek was just coming through the doorway carrying half a dozen other things\nt2u round of the door he was just coming through the doorway carrying half\naround of the door they stopped at the back of the doorway and half a around\nof the doorway and half a round of the doorway\nt2t a dozen men and a dozen men and women i believe and i think i could have\nsaved the lives of those who loved me-but i don’t know how i could have saved\nthem\n(F)\nu2u dozen boxes when he saw the stick black inside in his room stepping out of\nhis way into the hall a chuckle of joy fell in the drawing room and he seized\nthe boxes and broke down the door\nu2t a cup of coffee when gertie came up with a basketful of rice and a handful of\nwater and then she came to the house of mrs smiths and she said to gertie\nt2u dozen packs and a wrapper and a light sparkling light across the face of jack\nand a burning gold bullet and a very sharp thumb\nt2t a dozen toy guns and a hundred toy guns and a hundred toy guns\nTable 6: Example for speech and text continuation. Speech continuation has been transcribed by the transcriber.\n6593",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8449952602386475
    },
    {
      "name": "Joint (building)",
      "score": 0.6517214179039001
    },
    {
      "name": "Language model",
      "score": 0.6029713153839111
    },
    {
      "name": "Natural language processing",
      "score": 0.5913195013999939
    },
    {
      "name": "Construct (python library)",
      "score": 0.5575270056724548
    },
    {
      "name": "Speech recognition",
      "score": 0.5225070118904114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5129706263542175
    },
    {
      "name": "Speech analytics",
      "score": 0.4551515579223633
    },
    {
      "name": "Speech corpus",
      "score": 0.44586458802223206
    },
    {
      "name": "Acoustic model",
      "score": 0.4263356029987335
    },
    {
      "name": "Speech processing",
      "score": 0.41883549094200134
    },
    {
      "name": "Speech synthesis",
      "score": 0.41583842039108276
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}