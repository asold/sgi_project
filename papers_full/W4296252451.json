{
  "title": "Omicron detection with large language models and YouTube audio data",
  "url": "https://openalex.org/W4296252451",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3110863413",
      "name": "James T. Anibal",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering",
        "University of Oxford"
      ]
    },
    {
      "id": null,
      "name": "Adam J. Landa",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering"
      ]
    },
    {
      "id": "https://openalex.org/A3096405314",
      "name": "Nguyen T T Hang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3208587540",
      "name": "Miranda J Song",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering"
      ]
    },
    {
      "id": null,
      "name": "Alec K. Peltekian",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2911933313",
      "name": "Ashley Shin",
      "affiliations": [
        "United States National Library of Medicine",
        "National Institutes of Health"
      ]
    },
    {
      "id": "https://openalex.org/A3213326290",
      "name": "Hannah B Huth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4311825531",
      "name": "Lindsey A. Hazen",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering"
      ]
    },
    {
      "id": null,
      "name": "Anna S. Christou",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering"
      ]
    },
    {
      "id": "https://openalex.org/A3214846643",
      "name": "Jocelyne Rivera",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering"
      ]
    },
    {
      "id": null,
      "name": "Robert A. Morhard",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering"
      ]
    },
    {
      "id": "https://openalex.org/A1162469211",
      "name": "Ulas Bagci",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A1996079675",
      "name": "Ming Li",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering"
      ]
    },
    {
      "id": "https://openalex.org/A2785352688",
      "name": "Yael Bensoussan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2057169223",
      "name": "David A Clifton",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A1965007712",
      "name": "Bradford J. Wood",
      "affiliations": [
        "National Institute of Biomedical Imaging and Bioengineering"
      ]
    },
    {
      "id": "https://openalex.org/A3110863413",
      "name": "James T. Anibal",
      "affiliations": [
        "University of Oxford",
        "Society of Interventional Radiology"
      ]
    },
    {
      "id": null,
      "name": "Adam J. Landa",
      "affiliations": [
        "Society of Interventional Radiology"
      ]
    },
    {
      "id": "https://openalex.org/A3208587540",
      "name": "Miranda J Song",
      "affiliations": [
        "Society of Interventional Radiology"
      ]
    },
    {
      "id": null,
      "name": "Alec K. Peltekian",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2911933313",
      "name": "Ashley Shin",
      "affiliations": [
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A4311825531",
      "name": "Lindsey A. Hazen",
      "affiliations": [
        "Society of Interventional Radiology"
      ]
    },
    {
      "id": null,
      "name": "Anna S. Christou",
      "affiliations": [
        "Society of Interventional Radiology"
      ]
    },
    {
      "id": "https://openalex.org/A3214846643",
      "name": "Jocelyne Rivera",
      "affiliations": [
        "Society of Interventional Radiology"
      ]
    },
    {
      "id": null,
      "name": "Robert A. Morhard",
      "affiliations": [
        "Society of Interventional Radiology"
      ]
    },
    {
      "id": "https://openalex.org/A1162469211",
      "name": "Ulas Bagci",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A1996079675",
      "name": "Ming Li",
      "affiliations": [
        "Society of Interventional Radiology"
      ]
    },
    {
      "id": "https://openalex.org/A2057169223",
      "name": "David A Clifton",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A1965007712",
      "name": "Bradford J. Wood",
      "affiliations": [
        "Society of Interventional Radiology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3175510762",
    "https://openalex.org/W2118778378",
    "https://openalex.org/W3096732775",
    "https://openalex.org/W2905587047",
    "https://openalex.org/W4224210841",
    "https://openalex.org/W2011847916",
    "https://openalex.org/W3105033257",
    "https://openalex.org/W3129915047",
    "https://openalex.org/W4200111085",
    "https://openalex.org/W3091468319",
    "https://openalex.org/W3015034944",
    "https://openalex.org/W3109783949",
    "https://openalex.org/W4224253409",
    "https://openalex.org/W4225137254",
    "https://openalex.org/W4210339745",
    "https://openalex.org/W3101195649",
    "https://openalex.org/W4224285589",
    "https://openalex.org/W4381685494",
    "https://openalex.org/W4221107627",
    "https://openalex.org/W3157159055",
    "https://openalex.org/W3206152192",
    "https://openalex.org/W4283399777",
    "https://openalex.org/W4283655498",
    "https://openalex.org/W4292821265",
    "https://openalex.org/W2746791238",
    "https://openalex.org/W3119527628",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W3153912282",
    "https://openalex.org/W4206185298",
    "https://openalex.org/W4200478223",
    "https://openalex.org/W3105837102",
    "https://openalex.org/W3136933888"
  ],
  "abstract": "Abstract Publicly available audio data presents a unique opportunity for the development of digital health technologies with large language models (LLMs). In this study, YouTube was mined to collect audio data from individuals with self-declared positive COVID-19 tests as well as those with other upper respiratory infections (URI) and healthy subjects discussing a diverse range of topics. The resulting dataset was transcribed with the Whisper model and used to assess the capacity of LLMs for detecting self-reported COVID-19 cases and performing variant classification. Following prompt optimization, LLMs achieved accuracies of 0.89, 0.97, respectively, in the tasks of identifying self-reported COVID-19 cases and other respiratory illnesses. The model also obtained a mean accuracy of 0.77 at identifying the variant of self-reported COVID-19 cases using only symptoms and other health-related factors described in the YouTube videos. In comparison with past studies, which used scripted, standardized voice samples to capture biomarkers, this study focused on extracting meaningful information from public online audio data. This work introduced novel design paradigms for pandemic management tools, showing the potential of audio data in clinical and public health applications.",
  "full_text": " \n \nDigital Omicron detection using unscripted voice samples from social media \n \nJames T. Anibal1,2, Adam J. Landa1, Hang T. Nguyen3, Alec K. Peltekian4, Andrew D. Shin5, Miranda J. Song1, \nAnna S. Christou1, Lindsey A. Hazen1, Jocelyne Rivera1, Robert A. Morhard1, Ulas Bagci6, Ming Li1, David A. \nClifton2, Bradford J. Wood1 \n \n1. Center for Interventional Oncology, Radiology and Imaging Sciences, NIH Clinical Center, National Cancer Institute, \nNational Institute of Biomedical Imaging and Bioengineering, National Institutes of Health, 10 Center Dr, Building 10, \nRoom 1C341, MSC 1182, Bethesda, MD 20892-1182 USA \n2. Computational Health Informatics Lab, Oxford Institute of Biomedical Engineering, University of Oxford, Old Road \nCampus Research Building, Headington, Oxford OX3 7DQ, United Kingdom \n3. Oxford University Clinical Research Unit, Centre for Tropical Medicine, 764 Vo Van Kiet, Quan 5, Ho Chi Minh City, \nVietnam \n4. Department of Computer Science, McCormick School of Engineering, Northwestern University, Mudd Hall, 2233 \nTech Drive, Third Floor, Evanston, IL, 60208 USA \n5. National Library of Medicine, National Institutes of Health, 8600 Rockville Pike, Bethesda, MD, 20894 \n6. Feinberg School of Medicine, Northwestern University, 420 E Superior St, Chicago, IL 60611 USA \n \n  \nSummary \nThe success of artificial intelligence in clinical environments relies upon the diversity and \navailability of training data. In some cases, social media data may be used to counterbalance the \nlimited amount of accessible, well-curated clinical data, but this possibility remains largely \nunexplored. In this study, we mined YouTube to collect voice data from individuals with self-\ndeclared positive COVID-19 tests during time periods in which Omicron was the predominant \nvariant1,2,3, while also sampling non-Omicron COVID-19 variants, other upper respiratory \ninfections (URI), and healthy subjects. The resulting dataset was used to train a DenseNet model \nto detect the Omicron variant from voice changes. Our model achieved 0.85/0.80 \nspecificity/sensitivity in separating Omicron samples from healthy samples and 0.76/0.70 \nspecificity/sensitivity in separating Omicron samples from symptomatic non-COVID samples. In \ncomparison with past studies, which used scripted voice samples, we showed that leveraging the \nintra-sample variance inherent to unscripted speech enhanced generalization. Our work \nintroduced novel design paradigms for audio-based diagnostic tools and established the potential \nof social media data to train digital diagnostic models suitable for real-world deployment. \n \n1. Introduction \nCOVID-19 is routinely detected and confirmed through polymerase chain reaction (PCR) using \nnasal or throat swabs; however, turnaround time and resource costs pose a challenge for testing \nin some settings. Some invasive home testing methods have been developed but can require \nexpensive reagents and/or laboratory expertise, restricting accessibility. Moreover, these tests do \nnot offer immediate results, which have become increasingly necessary as societies move \ntowards “living with COVID”4. Serial testing practices have become common, in which \nreasonably sensitive at-home antigen tests are followed by more-specific PCR confirmation. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \n1 \nInstant, non-invasive, and sensitive testing methods may be useful for suggesting confirmatory \ntesting or to track the spread of variants with unique audio phenotypes.  \nPrior AI methods have been unable to successfully detect pre-Omicron variants from unscripted \nor scripted human voice alone, or have been otherwise unsuitable for deployment (e.g., limited \ntraining data, poor generalization)5. Omicron variants, however, are typically milder and affect \nthe upper airway more commonly than prior variants, often resulting in voice changes without a \ncough or other respiratory symptoms6. This presents an opportunity for targeting with AI \nmethods, if robust datasets were available.  \nWorldwide, various social media platforms have over 3.6 billion users, with expectations to \nexceed 4.4 billion by 20257. Over 500 hours of video are uploaded to YouTube alone every \nminute8. While often ignored, much of this data is available to researchers through Python \nlibraries provided by social media companies. Such data more accurately portrays noisy, \nunscripted \"real-world” data, whose broad diversity supports generalizability. Our model was \ntrained using this freely accessible data. Though annotation of training data was based upon \nprevalence assumptions and self-declaration instead of sequencing, these limitations contributed \nto the practicality and cost-effectiveness of the approach. As opposed to other methods that rely \non lab results, AI-based classification tools using voice alone could be instant, accessible, cost-\neffective, and deployable in real-world settings.  \nThere are numerous barriers to practical deployment of AI models for COVID-19 diagnostics. \nPrior attempts have failed due to training on extremely limited datasets, producing overfit models \nthat do not generalize9. Existing models for acoustic, AI-driven diagnostics have also been \nlimited due to a reliance on short, structured samples collected in controlled, scripted \nenvironments. In this report, we broadly applied dataset design and audio-based deep learning \nmethodologies in the context of Omicron classification.  \nContributions: \n \n1. An AI system was constructed using social media data, with training and validation \nstrategies designed for real-world clinical settings. This improves upon prior social media \nAI efforts which simply facilitated narrow tasks in controlled settings involving frequent \nusers of social media.  \n2. A new framework for rapid diagnostic tools from voice/audio data was designed to \nemphasize longer samples and unscripted collection protocols. This strategy relied on the \ndiversity of the input data to better prepare for real-world testing environments. A \nconvolutional neural network (CNN) model trained on long, unscripted samples showed \nimproved generalization in comparison to prior work on short, scripted speaking inputs, \nas well as in our comparative experiments involving short, unscripted inputs. \n3. The first non-invasive model for instant Omicron detection was developed using training \ndata that included past variants, recent subvariants, healthy controls, and other respiratory \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n2 \nillnesses. Our data was compiled from a diverse array of settings and recordings, \ncontaining over 28 hours of unscripted audio from people posting with self-declared \nOmicron. This is several orders of magnitude greater than previous efforts. Identification \nof voice changes in speakers with self-declared Omicron showed that unscripted \nrecordings may be sufficient for detecting Omicron COVID-19, which is a change from \npast variants. \n4. Acoustic biomarkers for COVID-19 were shown to change between non-Omicron \n(primarily Alpha/Delta) variants (lower respiratory symptoms) and the milder Omicron \nvariant (upper respiratory symptoms). Models trained on voice data from pre-Omicron \nvariants experienced a decrease in both sensitivity and specificity when validated on \nOmicron test data. This result emphasized the necessity of variant-specific methods for \ntesting, or continual learning techniques that adapt as pandemics evolve over time.  \n \n2. Related Work \n  \n2.1 Social Media Data for Clinical Tasks \nCost-effective data collection, curation, annotation, and augmentation are critical for enabling AI \nto track or predict illness. Social media is an expansive source of information that does not rely \non intricate searching mechanisms or filtered, delayed reporting, potentially facilitating \nepidemiological surveillance. \nSeveral existing diagnostic models have utilized social media data. A deep learning model \ntrained on Tweets was more predictive of atherosclerotic heart disease mortality than a \nconventional mechanistic input-based model combining demographic and health risk factors10. \nAdditional methods have extracted textual and visual features from Tweets to predict and \nclassify mental health status11,12,13. These models, however, were only useful for active social \nmedia users. Other social media platforms have also been used as data sources for biomedical \napplications, though less frequently than Twitter. Manual analysis of YouTube home videos by \nnon-clinical raters was able to detect and classify autism in children with high performance14. \nYouTube audio, visual, and search-history data have also successfully detected mental illnesses \nincluding depression and OCD15,16. \n \n \n2.2 AI for COVID-19 Testing \nAI methodologies have been applied to numerous COVID-19 datasets to develop deployable \ndiagnostic tools. Coughing and breathing changes associated with COVID-19 may have unique \nfeatures that might be useful for classification, or differentiation from other upper respiratory \nillnesses or infections (URIs). Related work should, however, be contextualized using the criteria \noutlined by Han et al., which pointed out methodological flaws such as mixing training/testing \ndata, exclusion of non-COVID URIs, and overfitting on small datasets5. Prior work has also \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n3 \ntypically lacked stratification by variant, instead considering “COVID” as a whole and assuming \na generally consistent phenotype. \n \nNonetheless, previous efforts highlighted the potential for voice-based diagnostic tools, \nespecially given that COVID-19 breath sounds were characterized by unique time and frequency \ndomain patterns17. A CNN-based model trained on forced-cough recordings in limited numbers \nof patients with and without COVID-19 was able to recognize COVID-19 with high sensitivity, \neven in otherwise asymptomatic subjects18. Audio-based technologies using cough sounds have \nalso been deployed on a smartphone app for COVID-19 detection19,20,21,22,23. A binary classifier \nwas able to differentiate COVID-19 speech from normal speech based on scripted telephone \ndata24. Assessment of spectral features of speech alone in asymptomatic patients with and \nwithout COVID-19 yielded a true positive rate of 70%, though the likelihood of generalization \nwas quite limited due to scripted collection and small sample size. 25 \n \nIn a dynamic pandemic such as COVID-19, crowdsourced datasets allowed for continuous and \nfocused sample collection. “Coswara” is a database containing COVID-19 respiratory sound \nsamples, including cough, breath, and scripted voice data26. Samples recorded and uploaded by \nvolunteers on a smartphone or computer were divided into COVID and non-COVID cohorts27. \nNumerous researchers have used this database to train AI models for COVID-19 detection20,23,28. \nSuch algorithms reported an accuracy of 97% on limited binary datasets, which notably excluded \nother respiratory illnesses29,30. Deep learning models trained on the “Sounds of COVID” dataset, \nwhich contained scripted samples, showed that voice alone performed poorly on pre-Omicron \ndata (0.61 ROC-AUC)5. Further, most studies focused on multi-input models without specifying \nthe variant (via sequencing or demographic statistics). \n \n3. Methods                 \nThe methods were performed in accordance with relevant guidelines and regulations and \napproved by the Institutional Review Board (IRB) at the NIH. The preprocessing, training, and \nvalidation components of the Omicron detection method are outlined in Fig. 1-2.  \n3.1 Data Collection                                                                                                       \nAudio samples were mined from YouTube searches and annotated by cohort based on self-\ndeclaration and presumptive correlation with epidemiological data.  \n \n1. COVID-19 – Omicron variant (presumed by dates, including both symptomatic and \nasymptomatic cases) \n \n2. COVID-19 – non-Omicron variant (presumed by dates, including both symptomatic and \nasymptomatic cases) \n \n3. Symptomatic, non-COVID upper respiratory illnesses and infections (URI) \n \n \n4. Presumably healthy or non-acutely ill (asymptomatic).  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n4 \nA series of heuristics were used to identify relevant videos. For example, if the user said, “I have \nCOVID” or “I tested positive” during a time in which Omicron was the dominant variant, the \naudio sample was labeled as “Omicron”. We excluded videos that contained low-quality audio or \nfeatured multiple speakers. For the URI cohort, we excluded videos where there were no obvious \nrespiratory symptoms. YouTube videos were annotated by cohort and manually verified to \nensure accurate labeling. Since the data mining procedure was standardized but not \ncomprehensive, future studies might benefit from automated mining procedures for dataset \nexpansion. All Omicron videos were from December 20th, 2021 – August 1st, 2022. Omicron was \ndesignated a “variant of concern” on November 26th, 2021 by the WHO, and was estimated to be \nthe dominant variant in the US by late December 202131,32. Omicron was identified as the \ndominant variant globally, accounting for > 98% of sequences shared on GISAID after February \n20222,33. The BA.1 and BA.2 lineages were most common between December 2021 – June 2022, \nwith BA.4/BA.5 becoming more prevalent in July 202234. No sequencing was recorded.  \n  \n3.2 Data Preprocessing \nRaw audio samples extracted from YouTube videos were noisy, often containing background \nnoise, long periods of silence, or low-resolution audio. To reduce potential sources of confusion, \na preprocessing pipeline was implemented: \n   \n1. Audio Denoising: Following audio quality assessments, “noisy” sound was removed \nusing semi-supervised machine learning methods developed by Dolby and accessed \nthrough Dolby Media libraries for Python35. \n  \n2. Removal of Background Noise and Silence: Background noise was removed via a      \nU-Net convolutional neural network architecture36. Extended periods of silence were \nremoved via a voice activity detector that leveraged Gaussian mixture models to identify \nnon-speaking regions37. \n  \n3. Conversion into Mel spectrograms: Samples were converted into a 3-channel matrix, \ncorresponding to 3 Mel spectrograms generated with different window sizes and hop \nlengths. Mel spectrograms represented sound as frequency over time with frequency \nvalues converted to the Mel scale, which represents pitch based on how the human ear \nperceives loudness. This approach ensured that each channel contained different \nfrequency and time information (equivalent to resolution in a standard image \nrepresentation), providing the model with maximal context during training38. \n \n3.3 Augmentation \nIn audio-based diagnostics, there is minimal value in positional context. While past work has \nshown that sounds have variable effectiveness as disease predictors, we further assumed that \nrelevant digital biomarkers of laryngitis should be detectable within a 10 second interval39. Our \nsimple data augmentation strategy relied on the positional invariance of speech samples and the  \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n5 \nFigure 1: Workflow \nfor YouTube data \npreprocessing. (1) \nData was collected \nfrom YouTube and \nannotated based on \nuser declarations. (2) \nA U-Net model was \nused to separate \nvoice from music \nand other \nbackground noise. \n(3) A voice activity \ndetector (VAD), \nbuilt with Gaussian \nmixture models \nremoved extended \nperiods of silence. \n(4) The remaining, \ncleaned voice data \nwas converted into a \nspectrogram.  \n \n \n \nreduced need for modeling long-term dependencies, in the context of laryngitis. We aimed to use \nthe natural diversity of speech to enhance the generalizability of our model and reduce the \nimpact of class imbalance. For each audio recording, we considered the set of possible \ntransformations to be the result of dividing the sample into segments of length n seconds. Time \nand frequency masking were applied to each segment (via SpecAugment) prior to input into the \nCNN model, which is explained in section 3.440. \n \n3.4 DenseNet  \nConvolutional neural networks (CNNs) utilize the convolution operation to model spatial \nrelationships in matrices (e.g., images or spectrograms). These representations are generally \ninput into a standard feed-forward neural network and mapped onto an outcome or embedding \nvector. In most cases, individual layers are connected only to the subsequent layer. DenseNet \nintroduced a new framework wherein each layer was connected to all subsequent layers in a \n“Dense Block”, while allowing for multiple Dense Blocks within the same network41. These \nblocks were connected to each other via convolution and pooling layers which structured the \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n6 \noutputs of one block as inputs for the following block. This approach had multiple advantages \nfor complex tasks, including improved feature propagation and reuse. \n   \n \nFigure 2: Workflow for \ntraining and validation of \nOmicron detection \npipeline. (1) Audio \nrecordings were split \ninto segments and \nconverted to \nspectrograms; (2) \nDenseNet model was \ntrained on the \nspectrograms; (3) trained \nmodel was used to \npredict if segments in a \ntesting dataset were \npositive or negative for \nOmicron COVID-19 \n(majority vote was used \nto assign a label). \n \n \nThe DenseNet model was chosen due to the scalability of the architecture and high top-1 \naccuracy value on the complex ImageNet dataset, indicating that it had learned a generalizable \nrepresentation of images through key shapes/features. A pre-trained model was chosen based on \nprior work which reported that CNN models pre-trained on ImageNet achieved superior \nperformance on audio data compared to randomly initialized models38,42. Other recent \narchitectures (e.g., Vision Transformer) can also be used in our model-invariant system43.  \n \n \n4. Experimental Design \nExperiments were performed to assess the potential of social media data for training models to \ncomplete diagnostic tasks. We also assessed the generalization capacity gained from using the \nlong, free-response inputs as a component of the data augmentation strategy alongside \nSpecAugment (compared to short, standardized inputs). The reported performance metrics were \nmean values from 6-fold cross validation used in each experiment (Table 2).  \n \n4.1 YouTube Dataset                                                                                                                 \nOur YouTube dataset contained 183 subjects with Omicron (28.39 hours), 120 with pre-Omicron \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n7 \nCOVID-19 (22.84 hours), 138 with symptomatic non-COVID URIs (8.09 hours), and 192 that \nwere asymptomatic healthy (33.90 hours). Dataset statistics are listed in Table 1 to emphasize the \nadditional information gained from collecting longer samples of unscripted voice data.  \n \nTo the best of our knowledge, the YouTube dataset currently contains the largest amount of \nvoice data (in hours) for COVID-19 (all variants), the Omicron variants specifically, and, \nparticularly, URI that were confirmed or self-declared non-COVID. This is in comparison to all \npublicly available COVID-19 voice/sounds datasets. The Coswara dataset contained samples \nwhich may be other upper respiratory infections but were not confirmed or self-declared as non-\nCOVID25. We also noted that nearly half the 1,964 negative participants in the “Sounds of \nCOVID” dataset had at least one “COVID” symptom, but many were unrelated to the upper \nrespiratory system (e.g., fever, dizziness)5.  \n \nIn contrast, the YouTube dataset intentionally included illnesses that were designated non-\nOmicron based on self-declaration or date of posting. The samples were selected for the potential \nto impact the upper respiratory system, teaching the model to separate between non-COVID \nillnesses and Omicron. These URI included influenza, strep throat, cold, allergy attack, asthma, \nbronchitis, and others. Prior to training, we applied preprocessing and augmentation strategies to \nthe dataset as described in sections 3.2-3.3. \n \nTable 1: Comparative statistics for two common COVID-19 Voice Datasets. Statistics from \nCoswara were listed based on the amount of data at the time of access (May 18th, 2022). We also \nexcluded any data points which did not meet the criteria for inclusion in the Coswara database \n(e.g., multiple voices, indetectable audio).  \n  \nDataset COVID-19 \nSamples \n(All) \nOmicron \nSamples \nOther URI \n(Symptomatic) \nSamples \nCOVID-19 \ntotal audio \n(hours)  \nOmicron \ntotal audio \n(hours) \nURI total \naudio \n(hours) \nYouTube Dataset 316 183 138 51.23 28.39 8.09 \nCoswara 464 213 102 1.92 0.95 0.47 \n \n4.2 DenseNet Model Training \nFor supervised classification tasks involving voice samples, a cross-entropy loss function was \nused to fine-tune a pre-trained DenseNet. To address imbalances in the dataset, each batch was \ngenerated by oversampling the minority class. For each sample in the batch, a 2.5 second voice \nsegment was selected randomly from the entire audio recording.  \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n8 \n4.2.1 Classification of Omicron and Asymptomatic Healthy Subjects                         \nA DenseNet model was trained to identify healthy subjects (e.g., for testing asymptomatic \nindividuals prior to attending populated events) through the binary classification task of \nseparating asymptomatic healthy voices from all Omicron subjects. \n \n4.2.1 Classification of Omicron and Symptomatic URI Subjects                                                \nFor further classification of users presenting with upper-respiratory symptoms, the model was \ntrained to use voice data to separate Omicron subjects from symptomatic subjects with other, \nnon-COVID URIs. \n \n4.2.3 Model Training with Non-Omicron COVID-19 Data \nWe trained a model with non-Omicron data and attempted to classify COVID/URI and \nCOVID/healthy from test sets containing only Omicron and non-COVID subjects (no other \nvariants were present). This was done to highlight changes in disease phenotype over time and \nshow the need for variant-specific digital testing methods.  \n \n4.2.4 Model Training with Single Segments \nTo demonstrate the importance of collecting longer time-series datasets, we repeated the \nOmicron detection tasks (4.2.1-4.2.2) with short, randomly selected segments (one per sample) \nfrom the YouTube dataset. The same segment was used throughout the training process – the \nremainder of the data was not shown to the model. Due to model overfitting, we froze the base \nlayers of the DenseNet and fine-tuned only the classification head.  \n \n4.3 Model Validation \nWhen validating on a blind test dataset, sensitivity and specificity were calculated on a            \nper-sample basis. Each sample was divided into n-second segments as described in section 3.3. \nFor samples that could be split into multiple segments, a majority vote was used to assign the \nfinal label (“positive” or “negative”). This approach was used to facilitate real-world deployment \nwhere the user might be prompted to supply at minimum 30 seconds of audio, thereby reducing \nthe risk of random noise or vocal shifts that could obscure relevant digital biomarkers. \n \n5. Experimental Results \n \n5.1 Digital Testing on the YouTube Dataset \n \n5.1.1 Extended Voice Samples \nOur model was tested on the YouTube dataset to perform classification tasks (Table 2). Voice \nchanges were used to detect the Omicron variant on “real-world” data from YouTube. Model \nperformance was 85% specific and 80% sensitive for classification of Omicron subjects and \nasymptomatic healthy subjects. Notably, the model yielded a specificity of 76% and a sensitivity \nof 70% on the symptomatic testing task of separating Omicron samples from other, symptomatic \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n9 \nURI samples. These findings indicate the existence of an Omicron-specific laryngitis. This \nfinding suggests that models trained on real-world voice data may have relevance for \nsymptomatic testing, beyond identifying healthy, asymptomatic voices.  \n \nOur model was further tested on the YouTube dataset to determine if voice changes caused by  \nOmicron represented a detectable shift in phenotype compared to Alpha/Delta variants, which \noften presented with lower respiratory symptoms. Like past work, the results show that voice \ndata alone does not facilitate the separation of pre-Omicron COVID-19 samples from \nasymptomatic healthy samples (84% specificity/ 58% sensitivity). The limited signal verifies the \nlimited acoustic biomarkers in subjects with the pre-Omicron variants of COVID-19. The \nimproved results on the COVID/URI classification task (74% specificity / 70% sensitivity) are \nlikely due to the model detecting voice changes or other respiratory symptoms in the patients \nwith URI. This is in contrast with the pre-Omicron COVID-19 patients, which seem to have \nsimilar voices to healthy asymptomatic subjects. Voice data was more effective at detecting the \nOmicron variant in both the healthy/COVID-19 classification task (a difference in sensitivity of \nover 20%), and the COVID-19/other URI classification task.  \n \nTable 2: Model performance on COVID-19 detection (including both Omicron data and non-\nOmicron data) with randomly selected test datasets from the YouTube dataset. \n   \nTask Train Data Test Data Specificity Sensitivity \nAsymptomatic Healthy vs. \nOmicron \nOmicron Omicron 0.85 0.80 \nAsymptomatic Healthy vs. \nOmicron  \nNon-Omicron \nCOVID-19 \nOmicron 0.82 0.71 \nOmicron vs. Symptomatic \nURI \nOmicron Omicron 0.76 0.70 \nOmicron vs. Symptomatic \nURI \nNon-Omicron \nCOVID-19 \nOmicron 0.70 0.65 \nAsymptomatic Healthy vs. \npre-Omicron  \nNon-Omicron \nCOVID-19 \nNon-Omicron \nCOVID-19 \n0.80 0.58 \npre-Omicron vs. \nSymptomatic URI \nNon-Omicron \nCOVID-19 \nNon-Omicron \nCOVID-19 \n0.74 0.70 \n \nFurthermore, our results show that there is performance degradation for the Omicron/URI task \n(5-7% reduction in both sensitivity and specificity) when a non-Omicron model is validated on \nOmicron test data. This is in comparison to an Omicron-trained model that is validated on \nOmicron test data. The likely cause of this result is that the model does not learn to recognize the \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n10 \nOmicron-specific laryngitis that may be an important feature for classification. There was a \nsimilar performance decline (5-10%) on the healthy/Omicron binary classification task (pre-\nOmicron training, Omicron testing), which is, once again, likely resulting from the model being \nboosted by the presence of “Omicron laryngitis” in the training data. We observe that the model \nfor healthy/pre-Omicron classification achieves better performance on Omicron test data than the \nnon-Omicron test data. We speculate that this is due to the limited presence of general laryngitis \nin the training data, which can be transferred to Omicron test data, where upper respiratory \nsymptoms are common. For the non-Omicron test data, laryngitis in the test data, statistically, \nshould be much more sporadic.  \n5.1.2 Single-segment Voice Samples \nWe trained our model to complete both the Omicron detection tasks on the same subjects, with \nonly one segment per subject in the dataset. Despite the use of multiple data augmentation \ntechniques, our results (Table 3) showed a decrease in both sensitivity and specificity (10-20%) \nwhen only a single segment was used, demonstrating the importance of collecting longer          \ntime-series data, even if there are a limited number of subjects. In addition, these extended data \npoints may reduce the risk of error in a real-world digital testing solution. When performing \ninference, the model can check multiple independent segments over a 30+ second input (faster \nthan current rapid testing solutions) to reduce the risk of testing on a corrupted or \nunrepresentative data point. \n \nTable 3: Model performance on Omicron detection with a single segment per sample.  \n \nTask Train Data Test Data Specificity Sensitivity \nHealthy vs. Omicron Omicron Omicron 0.73 0.67 \nOmicron vs. URI Omicron Omicron 0.59 0.64 \n \n \n6. Discussion \n \nIn this report, we show that: \n \n1. Public online data, including unscripted social media data, had potential epidemiologic \nvalue in pandemics, with audio information that could be utilized by AI models for \napplications not specific to social media/Internet users.  \n \n2. In contrast with past variants, voice change was a predictor of the Omicron variant, which \nwas often milder and lacked a cough. Omicron samples were distinguishable not only \nfrom healthy voices, but also from voices with other self-declared upper respiratory \nillness and infections. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n11 \n3. Models trained on pre-Omicron data alone showed a decline in performance when \nvalidated on test datasets containing Omicron samples. This demonstrates the importance \nof testing methods which are continuously updated and validated with variant-specific \ndata. This is relevant for future pandemic preparedness, where perhaps deep learning may \nplay a more meaningful role.  \n \n4. Models trained on longer, unscripted audio samples achieved superior performance \ncompared to shorter scripted inputs used in prior work (e.g., counting to 20) and shorter \nunscripted inputs (section 5.1.2). Lengthy voice samples improved model performance \ndue to the diversity of unscripted data.  \n \nWe introduced the “YouTube COVID-19 voice dataset”, which contains over one full day of \naudio data corresponding to the Omicron variant, and similar quantities for non-Omicron \nCOVID-19 and healthy controls. The dataset also includes over 8 hours of data from other upper \nrespiratory illness and infection, improving upon other COVID-19 audio datasets which were \nnoisy, unbalanced, and contained undiagnosed URI data (unconfirmed COVID negativity). This \ncomparison underscores the value of retrospective data collection from public social media in \nreal-world settings, despite the lack of ground truth verification. Voice changes may represent a \nbiomarker for Omicron. The exact underlying mechanism for characteristic audiogram \nalterations may be due to local laryngitis; however, the central nervous system is also capable of \ncontrolling pitch in speech44,45. \n \n6.1 Real-World Deployment                    \nPotential future directions for this work include dataset expansion through improved mining \nmethods, implementation of a smartphone/web-app system for day-to-day testing or user-specific \ncustomized models, and development of continual learning. Furthermore, regular pre-screening \nin at-risk populations could be used to define the temporal/geographic dynamics of voice \nchanges from variants as a virus evolves. However, model deployment, stability, and impact \ncurrently remain highly speculative due to the limited size of samples from other URIs, lack of \nPCR confirmation testing and sequencing, and cohort annotations based on assumption. \n \n7. Conclusion   \nDigital epidemiology is provocative and understudied in the context of public online data and \nsocial media data; however, its wide availability raises new questions for privacy security, \nregulation, ethics, and real-world validation. Unscripted social media audio data may inherently \nbe more diverse (and ultimately lead to more generalizable models) than narrow-intent scripted \ndata. For symptomatic illnesses, these indicators may be more accurately reflected in longer \nsamples. Even without ground truth from sequencing, the results achieved by this early effort at \nOmicron detection merit further evaluation or smartphone app assessment in specific controlled \npublic health settings with unmet needs. Despite limitations, this work highlights the unique \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n12 \npresentation of laryngitis in patients with the COVID-19 Omicron variant. Social media as a \nsource of unscripted audio data may enable new frameworks to facilitate variant-specific testing. \n  \nData Availability: \nData will be made available for academic research upon requests directed to the corresponding \nemail (james.anibal@reuben.ox.ac.uk).  \n \nDisclosures / Conflicts of Interest:                 \nThe authors declare no competing non-financial interests but the following competing financial \ninterests. NIH may own intellectual property in the field. NIH and BJW receive royalties for \nlicensed patents from Philips, unrelated to this work. FDA and CE Mark “off-label use of \ndevices may be discussed or implied. BW is Principal Investigator on the following CRADA’s = \nCooperative Research & Development Agreements, between NIH and industry: Philips, Philips \nResearch, Celsion Corp, BTG Biocompatibles / Boston Scientific, Siemens, NVIDIA, XACT \nRobotics. Promaxo (in progress). The following industry partners also support research in CIO \nlab via equipment, personnel, devices and/ or drugs: 3T Technologies (devices), Exact Imaging \n(data), AngioDynamics (equipment), AstraZeneca (pharmaceuticals, NCI CRADA), ArciTrax \n(devices and equipment), Imactis (Equipment), Johnson & Johnson (equipment), Medtronic \n(equipment), Theromics (Supplies), Profound Medical (equipment and supplies), QT Imaging \n(equipment and supplies). The content of this manuscript does not necessarily reﬂect the views, \npolicies, or opinions of the National Institutes of Health (NIH), the U.S. Department of Health \nand Human Services, the U.K. National Health Service, the U.K. National Institute for Health \nResearch, the U.K. Department of Health, InnoHK – ITC, or the University of Oxford. The \nmention of commercial products, their source, or their use in connection with material reported \nherein is not to be construed as an actual or implied endorsement of such products by the U.S. \ngovernment.  \n  \nAuthor Contributions: \nJTA, BJW, DAC, ML, UB designed and supervised the research. AJL, HTN, AKP, and ADS \ncollected, annotated, and curated data. JTA and AJL designed and implemented the \npreprocessing pipeline. JTA implemented AI models and performed experiments. JTA, BJW, \nAJL, MJS, ASC, LAH, JR, and RAM wrote/edited the manuscript and outlined future work. \nJTA, BJW and LAH managed the collaborative components of the project and completed IRB \nrequirements.  \n \nFunding: \nThis work was supported by the NIH Center for Interventional Oncology and the Intramural \nResearch Program of the National Institutes of Health, National Cancer Institute, and the \nNational Institute of Biomedical Imaging and Bioengineering, via intramural NIH Grants Z1A \nCL040015 and 1ZIDBC011242.Work also supported by the NIH Intramural Targeted Anti-\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n13 \nCOVID-19 (ITAC) Program, funded by the National Institute of Allergy and Infectious Diseases. \nDAC was supported in part by the National Institute for Health Research (NIHR) Oxford \nBiomedical Research Centre (BRC), and in part by an InnoHK Project at the Hong Kong Centre \nfor Cerebro-cardiovascular Health Engineering (COCHE). DAC is also a funded Investigator in \nthe Pandemic Sciences Institute, University of Oxford, Oxford, UK.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n14 \nReferences: \n \n1. Hodcroft, E. B. CoVariants: SARS-CoV-2 Mutations and Variants of Interest. https://covariants.org/ (2021). \n2. Elbe, S., & Buckland-Merrett, G. Data, disease and diplomacy: GISAID’s innovative contribution to global \nhealth. Global Challenges 1(1), 33–46 (2017).  \n3. Leatherby, L. What the BA.5 Subvariant Could Mean for the United States. The New York Times \nhttps://www.nytimes.com/interactive/2022/07/07/us/ba5-covid-Omicron-subvariant.html (2022). \n4. Ye, Q., Lu, D., Zhang, T., Mao, J., & Shang, S. Recent advances and clinical application in point-of-care \ntesting of SARS-CoV-2. Journal of Medical Virology 94(5), 1866–1875 (2022). \n5. Han, J. et al. (2022). Sounds of COVID-19: exploring realistic performance of audio-based digital testing. Npj \nDigital Medicine 5(1), 16 (2022). \n6. Menni, Cristina, et al. \"Symptom prevalence, duration, and risk of hospital admission in individuals infected \nwith SARS-CoV-2 during periods of omicron and delta variant dominance: a prospective observational study \nfrom the ZOE COVID Study.\" The Lancet 399.10335 (2022): 1618-1624. \n7. Dixon, S. Number of social media users worldwide from 2018 to 2022, with forecasts from 2023 to 2027. \nStatista https://www.statista.com/statistics/278414/number-of-worldwide-social-network-users/ (2022). \n8. Ceci, L. Hours of video uploaded to YouTube every minute as of February 2020. Statista \nhttps://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/ (2022). \n9. Roberts, M. et al. Common pitfalls and recommendations for using machine learning to detect and \nprognosticate for COVID-19 using chest radiographs and CT scans. Nature Machine Intelligence 3(3), 199–\n217 (2021). \n10. Eichstaedt, J. C. et al. Psychological Language on Twitter Predicts County-Level Heart Disease Mortality. \nPsychological Science 26(2), 159–169 (2015). \n11. Sadasivuni, S. T., & Zhang, Y. Using Gradient Methods to Predict Twitter Users’ Mental Health with Both \nCOVID-19 Growth Patterns and Tweets. 2020 IEEE International Conference on Humanized Computing and \nCommunication with Artificial Intelligence (HCCAI), 65–66 (2020). \n12. Gui, T. et al. Cooperative Multimodal Approach to Depression Detection in Twitter. Proceedings of the AAAI \nConference on Artificial Intelligence 33(01), 110–117 (2019). \n13. Chatterjee, M., Samanta, P., Kumar, P., & Sarkar, D. Suicide Ideation Detection using Multiple Feature \nAnalysis from Twitter Data. 2022 IEEE Delhi Section Conference (DELCON), 1–6 (2022). \n14. Fusaro, V. A. et al. The Potential of Accelerating Early Detection of Autism through Content Analysis of \nYouTube Videos. PLOS ONE 9(4), e93533 (2014). \n15. Zhang, B., Zaman, A., Silenzio, V., Kautz, H., & Hoque, E. The Relationships of Deteriorating Depression and \nAnxiety With Longitudinal Behavioral Changes in Google and YouTube Use During COVID-19: \nObservational Study. JMIR Ment Health 7(11), e24012 (2020). \n16. Abhishek, P., Gogoi, V., & Borah, L. Depiction of Obsessive-Compulsive Disorder in YouTube videos. \nInformatics for Health and Social Care 46(3), 256–262 (2021).  \n17. Deshpande, G., & Schuller, B. W. COVID-19 Biomarkers in Speech: On Source and Filter Components. 2021 \n43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), 800–\n803 (2021). \n18. Laguarta, J., Hueto, F., & Subirana, B. COVID-19 Artificial Intelligence Diagnosis Using Only Cough \nRecordings. IEEE Open Journal of Engineering in Medicine and Biology 1, 275–281 (2020).  \n19. Imran, A. et al. AI4COVID-19: AI enabled preliminary diagnosis for COVID-19 from cough samples via an \napp. Informatics in Medicine Unlocked 20, 100378 (2020). \n20. Pahar, M., Klopper, M., Warren, R., & Niesler, T. COVID-19 cough classification using machine learning and \nglobal smartphone recordings. Computers in Biology and Medicine 135, 104572 (2021).  \n21. Rahman, T. et al. QUCoughScope: An Intelligent Application to Detect COVID-19 Patients Using Cough and \nBreath Sounds. Diagnostics 12(4) (2022).  \n22. Chen, Z. et al. Diagnosis of COVID-19 via acoustic analysis and artificial intelligence by monitoring breath \nsounds on smartphones. Journal of Biomedical Informatics 130, (2022). \n23. Alkhodari, M., & Khandoker, A. H. Detection of COVID-19 in smartphone-based breathing recordings: A pre-\nscreening deep learning tool. PLOS ONE 17(1), 1–25 (2022).  \n24. Ritwik, K. V. S., Kalluri, S. B., & Vijayasenan, D. COVID-19 Patient Detection from Telephone Quality \nSpeech Data. Preprint at arXiv https://doi.org/10.48550/ARXIV.2011.04299 (2020). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint \n \n15 \n25. Usman, Mohammed, et al. \"Speech as a Biomarker for COVID-19 Detection Using Machine \nLearning.\" Computational Intelligence and Neuroscience 2022 (2022). \n26. Sharma, N. et al. Coswara—A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis. \nInterspeech 2020 https://doi.org/10.21437/interspeech.2020-2768 (2020). \n27. Sharma, N. K. et al. Towards sound based testing of COVID-19—Summary of the first Diagnostics of \nCOVID-19 using Acoustics (DiCOVA) Challenge. Computer Speech & Language 73, 101320 (2022).  \n28. Chowdhury, N. K., Kabir, M. A., Rahman, Md. M., & Islam, S. M. S. Machine learning for detecting COVID-\n19 from cough sounds: An ensemble-based MCDM method. Computers in Biology and Medicine 145, 105405 \n(2022). \n29. Verde, L. et al. Exploring the Use of Artificial Intelligence Techniques to Detect the Presence of Coronavirus \nCovid-19 Through Speech and Voice Analysis. IEEE Access 9, 65750–65757 (2021).  \n30. Verde, L., de Pietro, G., & Sannino, G. Artificial Intelligence Techniques for the Non-invasive Detection of \nCOVID-19 Through the Analysis of Voice Signals. Arabian Journal for Science and Engineering \nhttps://doi.org/10.1007/s13369-021-06041-4 (2021). \n31. World Health Organization. Tracking SARS-CoV-2 variants. https://www.who.int/activities/tracking-SARS-\nCoV-2-variants (2022). \n32. Centers for Disease Control and Prevention. Potential Rapid Increase of Omicron Variant Infections in the \nUnited States. https://www.cdc.gov/coronavirus/2019-ncov/science/forecasting/mathematical-modeling-\noutbreak.html (2021). \n33. Centers for Disease Control and Prevention. Omicron Variant: What You Need to Know. \nhttps://www.cdc.gov/coronavirus/2019-ncov/variants/Omicron-variant.html (2022). \n34. Centers for Disease Control and Prevention. COVID Data Tracker. https://covid.cdc.gov/covid-data-\ntracker/#variant-proportions (2022). \n35. Dolby.io. Introduction to Media APIs. https://docs.dolby.io/media-apis/docs (2022). \n36. Ronneberger, O., Fischer, P., & Brox, T. U-Net: Convolutional Networks for Biomedical Image \nSegmentation. Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, 234-241 \n(2015). \n37. Wiseman, J. Python interface to the Google WebRTC Voice Activity Detector (VAD). PyPI \nhttps://pypi.org/project/webrtcvad/ (2017). \n38. Palanisamy, K., Singhania, D., & Yao, A. Rethinking CNN Models for Audio Classification. Preprint at arXiv \nhttps://doi.org/10.48550/ARXIV.2007.11154 (2020). \n39. Aly, M., Rahouma, K. H., & Ramzy, S. M. Pay attention to the speech: COVID-19 diagnosis using machine \nlearning and crowdsourced respiratory and speech recordings. Alexandria Engineering Journal 61(5), 3487–\n3500 (2022). \n40. Park, D. S. et al. SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. \nInterspeech 2019 https://doi.org/10.21437/interspeech.2019-2680 (2019). \n41. Iandola, F. et al. DenseNet: Implementing Efficient ConvNet Descriptor Pyramids. Preprint at arXiv \nhttps://doi.org/10.48550/ARXIV.1404.1869 (2014). \n42. Deng, J. et al. ImageNet: A large-scale hierarchical image database. 2009 IEEE Conference on Computer \nVision and Pattern Recognition, 248–255 (2009). \n43. Dosovitskiy, A. et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Preprint \nat arXiv https://doi.org/10.48550/ARXIV.2010.11929 (2020). \n44. Sutherland, M. E. Vocal pitch in the human brain. Nat Hum Behav 2, 613 (2018). \n45. Dichter, B. K., Breshears, J. D., Leonard, M. K., & Chang, E. F. The control of vocal pitch in human laryngeal \nmotor cortex. Cell 174(1), 21-31 (2018). \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 22, 2022. ; https://doi.org/10.1101/2022.09.13.22279673doi: medRxiv preprint ",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.6122249960899353
    },
    {
      "name": "Computer science",
      "score": 0.5552522540092468
    },
    {
      "name": "Sample (material)",
      "score": 0.5215351581573486
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.5120728611946106
    },
    {
      "name": "Software deployment",
      "score": 0.4795417785644531
    },
    {
      "name": "Social media",
      "score": 0.47185561060905457
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.45777153968811035
    },
    {
      "name": "Variance (accounting)",
      "score": 0.41300010681152344
    },
    {
      "name": "Speech recognition",
      "score": 0.3889616131782532
    },
    {
      "name": "Artificial intelligence",
      "score": 0.360847532749176
    },
    {
      "name": "Telecommunications",
      "score": 0.12604135274887085
    },
    {
      "name": "World Wide Web",
      "score": 0.10620957612991333
    },
    {
      "name": "Medicine",
      "score": 0.10044822096824646
    },
    {
      "name": "Mathematics",
      "score": 0.09063568711280823
    },
    {
      "name": "Chemistry",
      "score": 0.08997061848640442
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210140884",
      "name": "National Cancer Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210088243",
      "name": "National Institute of Biomedical Imaging and Bioengineering",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1299303238",
      "name": "National Institutes of Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210155647",
      "name": "National Institutes of Health Clinical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210150114",
      "name": "Oxford University Clinical Research Unit",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I111979921",
      "name": "Northwestern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800548410",
      "name": "United States National Library of Medicine",
      "country": "US"
    }
  ]
}