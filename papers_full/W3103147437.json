{
    "title": "Transfer Learning and Distant Supervision for Multilingual Transformer\\n Models: A Study on African Languages",
    "url": "https://openalex.org/W3103147437",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4224905600",
            "name": "Hedderich, Michael A.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226664605",
            "name": "Adelani, David",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102491900",
            "name": "Zhu Dawei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282320223",
            "name": "Alabi, Jesujoba",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288019697",
            "name": "Markus, Udia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3163108710",
            "name": "Klakow, Dietrich",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2769041395",
        "https://openalex.org/W4287811062",
        "https://openalex.org/W3021001546",
        "https://openalex.org/W2963426978",
        "https://openalex.org/W2970297748",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2970854433",
        "https://openalex.org/W2742113707",
        "https://openalex.org/W3088354264",
        "https://openalex.org/W3018647120",
        "https://openalex.org/W3000409384",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W2997379976",
        "https://openalex.org/W2963907318",
        "https://openalex.org/W2971418718",
        "https://openalex.org/W2952352518",
        "https://openalex.org/W4235216760",
        "https://openalex.org/W2265846598",
        "https://openalex.org/W3100198908",
        "https://openalex.org/W3102505243",
        "https://openalex.org/W2986896820",
        "https://openalex.org/W3014088538",
        "https://openalex.org/W2251689968",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2970961772",
        "https://openalex.org/W2963642612",
        "https://openalex.org/W2970421890",
        "https://openalex.org/W2573062194",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2970457158",
        "https://openalex.org/W3034263272",
        "https://openalex.org/W3021766370",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2606901057",
        "https://openalex.org/W3013840636",
        "https://openalex.org/W4300939116",
        "https://openalex.org/W3100445485",
        "https://openalex.org/W3035689819",
        "https://openalex.org/W2962902328",
        "https://openalex.org/W2579153919",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2952087486"
    ],
    "abstract": "Multilingual transformer models like mBERT and XLM-RoBERTa have obtained\\ngreat improvements for many NLP tasks on a variety of languages. However,\\nrecent works also showed that results from high-resource languages could not be\\neasily transferred to realistic, low-resource scenarios. In this work, we study\\ntrends in performance for different amounts of available resources for the\\nthree African languages Hausa, isiXhosa and Yor\\\\`ub\\\\'a on both NER and topic\\nclassification. We show that in combination with transfer learning or distant\\nsupervision, these models can achieve with as little as 10 or 100 labeled\\nsentences the same performance as baselines with much more supervised training\\ndata. However, we also find settings where this does not hold. Our discussions\\nand additional experiments on assumptions such as time and hardware\\nrestrictions highlight challenges and opportunities in low-resource learning.\\n",
    "full_text": "Transfer Learning and Distant Supervision for Multilingual Transformer\nModels: A Study on African Languages\nMichael A. Hedderich1, David I. Adelani1, Dawei Zhu1, Jesujoba Alabi1,2, Udia Markus3\n& Dietrich Klakow1\n1Saarland University, Saarland Informatics Campus, Germany\n2DFKI GmBH, Saarbr¨ucken, Germany 3Nuhu Bamalli Polytechnic, Zaira, Nigeria\n{mhedderich,didelani,dzhu,dietrich.klakow}@lsv.uni-saarland.de\njesujoba oluwadara.alabi@dfki.de\nAbstract\nMultilingual transformer models like mBERT\nand XLM-RoBERTa have obtained great im-\nprovements for many NLP tasks on a variety\nof languages. However, recent works also\nshowed that results from high-resource lan-\nguages could not be easily transferred to re-\nalistic, low-resource scenarios. In this work,\nwe study trends in performance for differ-\nent amounts of available resources for the\nthree African languages Hausa, isiXhosa and\nYor`ub´a on both NER and topic classiﬁcation.\nWe show that in combination with transfer\nlearning or distant supervision, these models\ncan achieve with as little as 10 or 100 la-\nbeled sentences the same performance as base-\nlines with much more supervised training data.\nHowever, we also ﬁnd settings where this does\nnot hold. Our discussions and additional exper-\niments on assumptions such as time and hard-\nware restrictions highlight challenges and op-\nportunities in low-resource learning.\n1 Introduction\nDeep learning techniques, including contextualized\nword embeddings based on transformers and pre-\ntrained on language modelling, have resulted in\nconsiderable improvements for many NLP tasks.\nHowever, they often require large amounts of la-\nbeled training data, and there is also growing\nevidence that transferring approaches from high\nto low-resource settings is not straightforward.\nIn (Loubser and Puttkammer, 2020a), rule-based\nor linguistically motivated CRFs still outperform\nRNN-based methods on several tasks for South\nAfrican languages. For pretraining approaches\nwhere labeled data exists in a high-resource lan-\nguage, and the information is transferred to a low-\nresource language, Hu et al. (2020) ﬁnd a signif-\nicant gap between performance on English and\nthe cross-lingually transferred models. In a recent\nstudy, Lauscher et al. (2020) ﬁnd that the trans-\nfer for multilingual transformer models is less ef-\nfective for resource-lean settings and distant lan-\nguages. A popular technique to obtain labeled data\nquickly and cheaply is distant and weak supervi-\nsion. Kann et al. (2020) recently inspected POS\nclassiﬁers trained on weak supervision. They found\nthat in contrast to scenarios with simulated low-\nresource settings of high-resource languages, in\ntruly low-resource settings this is still a difﬁcult\nproblem. These ﬁndings also highlight the im-\nportance of aiming for realistic experiments when\nstudying low-resource scenarios.\nIn this work, we analyse multilingual trans-\nformer models, namely mBERT (Devlin et al.,\n2019; Devlin, 2019) and XLM-RoBERTa (Con-\nneau et al., 2019). We evaluate both sequence and\ntoken classiﬁcation tasks in the form of news ti-\ntle topic classiﬁcation and named entity recogni-\ntion (NER). A variety of approaches have been\nproposed to improve performance in low-resource\nsettings. In this work, we study (i) transfer learn-\ning from a high-resource language and (ii) distant\nsupervision. We selected these as they are two of\nthe most popular techniques in the recent literature\nand are rather independent of a speciﬁc model ar-\nchitecture. Both need auxiliary data. For transfer\nlearning, this is labeled data in a high-resource lan-\nguage, and for distant supervision, this is expert\ninsight and a mechanism to (semi-)automatically\ngenerate labels. We see them, therefore, as orthog-\nonal and depending on the scenario and the data\navailability, either one or the other approach might\nbe applicable.\nOur study is performed on three, linguistically\ndifferent African languages: Hausa, isiXhosa and\nYor`ub´a. These represent languages with millions\nof users and active use of digital infrastructure, but\nwith only very limited support for NLP technolo-\ngies. For this aim, we also collected three new\narXiv:2010.03179v1  [cs.CL]  7 Oct 2020\ndatasets that are made publicly available alongside\nthe code and additional material.1\nWe show both challenges and opportunities\nwhen working with multilingual transformer mod-\nels evaluating trends for different levels of resource\nscarcity. The paper is structured into the following\nquestions we are interested in:\n• How do more complex transformer models com-\npare to established RNNs?\n• How can transfer-learning be used effectively?\n• Is distant supervision helpful?\n• What assumptions do we have to consider when\ntargeting a realistic treatment of low-resource\nscenarios?\n2 Languages and Datasets\nIn this work, we evaluate on three African lan-\nguages, namely Hausa, isiXhosa and Yor `ub´a.\nHausa is from the Afro-Asiatic family while\nisiXhosa and Yor`ub´a belong to different branches\nof the large Niger-Congo family. Hausa and Yor`ub´a\nare the second and third most spoken languages in\nAfrica, and isiXhosa is recognized as one of the\nofﬁcial languages in South Africa and Zimbabwe.\nYor`ub´a has been part of the unlabeled training data\nfor the mBERT multilingual, contextual word em-\nbeddings. Texts in Hausa and isiXhosa have been\npart of the XLM-RoBERTa training.\nThe three languages have few or no labeled\ndatasets online for popular NLP tasks like named\nentity recognition (NER) and topic classiﬁcation.\nWe use the NER dataset by Eiselen (2016) for\nisiXhosa and the one by Alabi et al. (2020) for\nYor`ub´a. We collected and manually annotated a\nNER dataset for Hausa and news title topic clas-\nsiﬁcation datasets for Hausa and Yor`ub´a. Table 1\ngives a summary of the datasets. More information\nabout the languages, the datasets and their creation\nprocess can be found in the Appendix.\n3 Experimental Settings\nTo evaluate different amounts of resource-\navailability, we use subsets of the training data\nwith increasing sizes from ten to the maximally\navailable number of sentences. All the models are\ntrained on their corresponding language-model pre-\ntraining. Except if speciﬁed otherwise, the models\nare not ﬁne-tuned on any other task-speciﬁc, la-\nbeled data from other languages. We report mean\n1https://github.com/uds-lsv/\ntransfer-distant-transformer-african\nDataset Name Data Source Full Train/ Val/\nTest sentences\nHausa NER* VOA Hausa 1,014 / 145 / 291\nHausa Topic Class.* VOA Hausa 2,045 / 290 /582\nisiXhosa\nNER (Eiselen,\n2016)\nSADiLaR 5,138 / 608 / 537\nYor`ub´a NER (Alabi\net al., 2020)\nGlobalV oices 816 / 116 / 236\nYor`ub´a Topic Class.* BBC Yoruba 1,340 / 189 / 379\nTable 1: Datasets Summary. *Created for this work.\nF1-score on the test sets over ten repetitions with\nstandard error on the error bars. Additional experi-\nmental details are given in the following sections\nand the Appendix. The code is made publicly avail-\nable online as well as a table with the scores for all\nthe runs.\n4 Comparing to RNNs\nLoubser and Puttkammer (2020a) showed that\nmodels with comparatively few parameters, like\nCRFs, can still outperform more complex, neural\nRNNs models for several task and low-resource\nlanguage combinations. This motivates the ques-\ntion whether model complexity is an issue for these\nlow-resource NLP models. We compare to sim-\nple GRU based (Cho et al., 2014) models as well\nas the popular (non-transformer) combination of\nLSTM-CNN-CRF (Ma and Hovy, 2016) for NER\nand to the RCNN architecture (Lai et al., 2015) for\ntopic classiﬁcation. For these models, we use pre-\ntrained, non-contextual word embeddings trained\nfor the speciﬁc language. Figures 1a+b show that\nan increase in model complexity is not an issue in\nthese experiments. For Hausa and Yor`ub´a and for\nthe low resource settings for isiXhosa, BERT and\nXLM-RoBERTa actually outperform the other base-\nlines, possibly due to the larger amounts of back-\nground knowledge through the language model pre-\ntraining. For larger amounts of task-speciﬁc train-\ning data, the LSTM-CNN-CRF and the transformer\nmodels perform similarly. One should note that for\nisiXhosa, the linguistically motivated CRF (Eise-\nlen, 2016) still outperforms all approaches on the\nfull dataset.\n5 Transfer Learning\nThe mBERT and XLM-RoBERTa models are\ntrained with tasks that can be obtained from unla-\nbeled text, like masked language modelling. Addi-\n10 800 2000 4000\nnumber of clean sentences\n0.0\n0.2\n0.4\n0.6\n0.8test F1 XLM-RoBERTa\nLSTM-CNN-CRF\nGRU\nCRF (Eiselen, 2016)\nLSTM+Aux (Loubser & Puttk., 2020)\n(a) NER isiXhosa\n10 300 700 1340\nnumber of clean sentences\n0.0\n0.2\n0.4\n0.6test F1\nmBERT\nmBERT + en-data (few-shot)\nmBERT + en-data (zero-shot)\nRCNN\n(b) Topic Class. Yor`ub´a\n10 100 250 400 650 800 1014\nnumber of clean sentences\n0.5\n0.6\n0.7\n0.8test F1\nXLM-R\nXLM-R + en-data (few-shot)\nXLM-R + en-data (zero-shot)\n(c) Transfer Learn NER Hausa\nFigure 1: Comparing to RNNs (a+b) and using transfer\nlearning (b+c). Additional plots in the Appendix.\ntionally, the multilingual models can be ﬁne-tuned\non task-speciﬁc, supervised data but from a differ-\nent, high-resource language. There is evidence that\nthe multilingual transformer models can learn par-\nallel concepts across languages (Pires et al., 2019;\nWu and Dredze, 2019; Hu et al., 2020). This allows\nto then apply or evaluate the model directly with-\nout having been ﬁne-tuned on any labeled data in\nthe target language (zero-shot) or on only a small\namount of labeled data in the target language (few-\nshot).\nFor NER, we pre-train on the English CoNLL03\nNER dataset (Tjong Kim Sang and De Meulder,\n2003). For topic classiﬁcation, the models are pre-\ntrained on the English AG News corpus (Zhang\net al., 2015). The texts in the high-resource En-\nglish and the low-resource Hausa and Yor`ub´a tar-\nget datasets share the same domain (news texts).\nOne issue that is visible in these experiments is the\ndiscrepancy between classes. While some classes\nlike “Politics” are shared, the topic classiﬁcation\ndatasets also have language- and location-speciﬁc\nclasses like “Nigeria” and “Africa” which are not\npart of the English ﬁne-tuning dataset. In our exper-\niments, we use the intersection of labels for NER\n(excluding DATE and MISC for Hausa and Yor`ub´a)\nand the union of labels for topic classiﬁcation.\nThe results in Figure 1c and in the Appendix con-\nﬁrm the beneﬁts of ﬁne-tuning on high-resource\nlanguages already shown in past research. They\nshow, however, also the large gains in performance\nthat can be obtained by training on a minimal num-\nber of target instances. While the zero-shot setting\nin (Hu et al., 2020) is interesting from a method-\nological perspective, using a small training set for\nthe target language seems much more beneﬁcial for\na practical application. In our experiments, we get\n- with only ten labeled sentences - an improvement\nof at least 10 points in the F1-score for a shared\nlabel set on NER. For topic classiﬁcation (Figure\n1b) the transfer learning is not beneﬁcial, which\nmight be due to the mismatch in the label sets.\n6 Distant Supervision\nDistant and weak supervision are popular tech-\nniques when labeled data is lacking. It allows a\ndomain expert to insert their knowledge without\nhaving to label every instance manually. The expert\ncan, e.g. create a set of rules that are then used to\nlabel the data automatically (Ratner et al., 2020) or\ninformation from an external knowledge source can\nbe used (Rijhwani et al., 2020). This kind of (semi-)\nautomatic supervision tends to contain more errors\nwhich can hurt the performance of classiﬁers (see\ne.g. (Fang and Cohn, 2016)). To avoid this, it can\nbe combined with label noise handling techniques.\nThis pipeline has been shown to be effective for sev-\neral NLP tasks (Lange et al., 2019; Paul et al., 2019;\nWang et al., 2019; Chen et al., 2019; Mayhew et al.,\n2019), however, mostly for RNN based approaches.\nAs we have seen in Section 4 that these have a\nlower baseline performance, we are interested in\nwhether distant supervision is still useful for the\nbetter performing transformer models. Several of\nthe past works evaluated their approach only on\nhigh-resource languages or simulated low-resource\nscenarios. We are, therefore, also interested in\nhow the distant supervision performs for the actual\nresource-lean African languages we study.\nTo create the distant supervision, native speak-\ners with a background in NLP were asked to write\nlabeling rules. For the NER labels PER, ORG and\nLOC, we match the tokens against lists of entity\nnames. These were extracted from the correspond-\ning categories from Wikidata. For the DATE label,\nthe insight is used that date expressions are usually\npreceded by date keywords in Yor`ub´a, as reported\nby Adelani et al. (2020). We ﬁnd similar patterns\nin Hausa like “ranar”(day), “watan” (month), and\n“shekarar”(year). For example,“18th of May, 2019”\nin Hausa translates to “ranar 18 ga watan Mayu,\nshekarar 2019”. The annotation rules are based on\nthese keywords and further heuristics. Directly ap-\nplying this distant supervision on the NER test sets\nresults in an F1-score of 54% and 62% on Hausa\nand Yor`ub´a, respectively.\nFor the topic classiﬁcation task, the distant super-\nvision rules are based on a dictionary of words relat-\ning to each of the classes. To induce the dictionar-\nies, we collected terms related to different classes\nfrom web sources. For example, for the “Sport”\nlabel, names of sportspeople and sport-related or-\nganizations were collected and similarly for the\n“Africa” label, names of countries, their capitals\nand major cities and their politicians. To label a\nnews headline, the intersection between each class-\ndictionary and the text was computed, and a class\nwas selected with a majority voting scheme. We\nobtain an F1-score of 49% and 55% on the Hausa\nand Yor`ub´a test set respectively when applying\nthe distant supervision directly to the topic classi-\nﬁcation test sets. Additional details on the distant\nsupervision are given in the Appendix.\nFor label noise handling we use the confu-\nsion matrix approach for NER by Hedderich and\nKlakow (2018), marked as cm in the plots. Addi-\ntionally, we propose to combine it with the smooth-\ning concept by Lv et al. (2020).\nThe Figures 2a and in the Appendix show that\nwhen only a small amount of manually labeled data\nis available, distant supervision can be a helpful ad-\ndition. E.g. for the NER task in Yor`ub´a, combining\ndistant supervision and noise handling with 100\nlabeled sentences achieves similar performance to\nusing 400 manually labeled sentences. For label\nnoise handling, combining the confusion matrix\nwith the smoothing approach might be beneﬁcial\nbecause the estimated confusion matrix is ﬂawed\nwhen only small amounts of labeled data are given.\nWhen more manually labeled data is available, the\nnoisy annotations lose their beneﬁt and can become\nharmful to performance. Improved noise-handling\ntechniques might be able to mitigate this.\n10 100 250 400 650 816\nnumber of clean sentences\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70test F1\nmBERT\nmBERT + distant\nmBERT + distant + cm\nmBERT + distant + cm-smooth\n(a) Distant Supervision NER Yor`ub´a\n10 100 300 500 700 900\nnumber of clean sentences\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8test F1\nHausa: limited dev set\nHausa: full dev set\nYoruba: limited dev set\nYoruba: full dev set\n(b) Development Set Topic Class.\n10 250 500 750 1000\nnumber of clean sentences\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7test F1\nTopic Class: mBERT\nTopic Class: Distilled mBERT\nNER: mBERT\nNER: Distilled mBERT\n(c) DistilBERT on Yor`ub´a\nFigure 2: Distant supervision and model variations.\nAdditional plots in the Appendix.\n7 Questioning Assumptions\nIn this section, we want to discuss certain as-\nsumptions taken by us and previous work in low-\nresource learning to see if these hold and what\nchallenges and opportunities they could bring for\nfuture work.\n7.1 Development Set\nKann et al. (2019) criticized that research on low-\nresource often assumes the existence of a devel-\nopment set. Addressing this, we perform hyper-\nparameter optimization on high-resource English\ndata. For early-stopping (to avoid overﬁtting), the\nauthors experiment with obtaining an early-stop-\nepoch from the average of several other languages.\nTo avoid this multi-language set-up and the need to\nobtain labeled data for multiple languages, we sug-\ngest using instead a development set downsized by\nthe same factor as the training data. This approach\nkeeps the ratio between training and development\nset giving the development set a reasonable size\nto obtain in a low-resource setting. For the setting\nwith ten labeled sentences for training, the same\namount was used for the dev set. The results in\nFigure 2b and in the Appendix show that this has\nonly a small effect on the training performance.\n7.2 Hardware Resources\nWhile the multilingual transformer models show\nimpressive improvements over the RNN baselines,\nthey also require more hardware resources. The\nLSTM-CNN-CRF model, e.g. has ca. 5M param-\neters compared to mBERT’s over 150M parame-\nters. The computing capabilities for training and\ndeploying such models might not always be given\nin low-resource scenarios. Through personal con-\nversations with researchers from African countries,\nwe found that this can be an issue. There are ap-\nproaches to reduce model size while keeping a\nsimilar performance quality, e.g. the 25% smaller\nDistilBERT (Sanh et al., 2019). Figure 2c shows\nthat this performs indeed similar in many cases but\nthat there is a signiﬁcant drop in performance for\nNER when only few training sentences are avail-\nable.\n7.3 Annotation Time\nIn (Hu et al., 2020) and (Kann et al., 2020), it is\nassumed that no labeled training data is available\nfor the target language. In the previous sections,\nwe showed that even with ten labeled target sen-\ntences, reasonable model quality can be achieved.\nFor our annotation efforts, we measured on average\n1 minute per annotator per sentence for NER and 6\nseconds per sentence for topic classiﬁcation. We,\ntherefore, think that it is reasonable to assume the\navailability of small amounts of labeled data. Es-\npecially, as we would argue that it is beneﬁcial to\nhave a native speaker or language expert involved\nwhen developing a model for a speciﬁc language.\nFor distant supervision, a trade-off arises given\nthese annotation times. While extracting named\nentities from knowledge bases requires minimal\nmanual effort assuming a set-up system, manual\ncrafting rules took 30 minutes for the DATE label\nand 2.5 hours for each topic classiﬁcation dataset.\nWhen reporting results for distant supervision, the\nperformance beneﬁts should therefore also be com-\npared against manual annotation in the same time\nframe.\n8 Conclusions\nIn this work, we evaluated transfer learning and\ndistant supervision on multilingual transformer\nmodels, studying realistic low-resource settings for\nAfrican languages. We show that even with a small\namount of labeled data, reasonable performance\ncan be achieved. We hope that our new datasets\nand our reﬂections on assumptions in low-resource\nsettings help to foster future research in this area.\nAcknowledgments\nWe would like to thank Toyin Aina and Emmanuel\nOlawuyi for their support in annotating the Hausa\ndata. We also thank Thomas Niesler for his helpful\ninsights as well as Alexander Blatt and the anony-\nmous reviewers for their feedback. Funded by the\nDeutsche Forschungsgemeinschaft (DFG, German\nResearch Foundation) – Project-ID 232722074 –\nSFB 1102, the EU-funded Horizon 2020 projects\nROXANNE under grant number 833635 and COM-\nPRISE under grant agreement No. 3081705.\nReferences\nIdris Abdulmumin and Bashir Shehu Galadanci. 2019.\nhauwe: Hausa words embedding for natural lan-\nguage processing. 2019 2nd International Confer-\nence of the IEEE Nigeria Computer Chapter (Nige-\nriaComputConf).\nDavid Ifeoluwa Adelani, Michael A. Hedderich, Dawei\nZhu, Esther van den Berg, and Dietrich Klakow.\n2020. Distant supervision and noisy label learning\nfor low resource named entity recognition: A study\non hausa and yor`ub´a.\nJesujoba Alabi, Kwabena Amponsah-Kaakyire, David\nAdelani, and Cristina Espana-Bonet. 2020. Massive\nvs. Curated Word Embeddings for Low-Resourced\nLanguages. The Case of Yor `ub´a and Twi. In Pro-\nceedings of The 12th Language Resources and Eval-\nuation Conference , pages 2747–2755, Marseille,\nFrance. European Language Resources Association.\nJunfan Chen, Richong Zhang, Yongyi Mao, Hongyu\nGuo, and Jie Xu. 2019. Uncover the ground-truth re-\nlations in distant supervision: A neural expectation-\nmaximization framework. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 326–336, Hong Kong,\nChina. Association for Computational Linguistics.\nArtem Chernodub, Oleksiy Oliynyk, Philipp Heidenre-\nich, Alexander Bondarenko, Matthias Hagen, Chris\nBiemann, and Alexander Panchenko. 2019. Targer:\nNeural argument mining at your ﬁngertips. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion of Computational Linguistics (ACL’2019), Flo-\nrence, Italy.\nKyunghyun Cho, Bart van Merrienboer, C ¸ aglar\nG¨ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2014, October\n25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a\nSpecial Interest Group of the ACL, pages 1724–1734.\nACL.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale.\nJacob Devlin. 2019. mBERT README ﬁle.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDavid M. Eberhard, Gary F. Simons, and Charles\nD. Fennig (eds.). 2019. Ethnologue: Languages of\nthe world. twenty-second edition.\nRoald Eiselen. 2016. Government domain named en-\ntity recognition for south African languages. In\nProceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16),\npages 3344–3348, Portoro ˇz, Slovenia. European\nLanguage Resources Association (ELRA).\nRoald Eiselen and Martin J. Puttkammer. 2014. Devel-\noping text resources for ten south african languages.\nIn Proceedings of the Ninth International Confer-\nence on Language Resources and Evaluation, LREC\n2014, Reykjavik, Iceland, May 26-31, 2014 , pages\n3698–3703. European Language Resources Associ-\nation (ELRA).\nMeng Fang and Trevor Cohn. 2016. Learning when\nto trust distant supervision: An application to low-\nresource POS tagging using cross-lingual projection.\nIn Proceedings of The 20th SIGNLL Conference on\nComputational Natural Language Learning , pages\n178–186, Berlin, Germany. Association for Compu-\ntational Linguistics.\nMichael A. Hedderich and Dietrich Klakow. 2018.\nTraining a neural network in a low-resource setting\non automatically annotated noisy data. In Proceed-\nings of the Workshop on Deep Learning Approaches\nfor Low-Resource NLP , DeepLo@ACL 2018, Mel-\nbourne, Australia, July 19, 2018 , pages 12–18. As-\nsociation for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalization.\nKatharina Kann, Kyunghyun Cho, and Samuel R. Bow-\nman. 2019. Towards realistic practices in low-\nresource natural language processing: The develop-\nment set. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3342–3349, Hong Kong, China. Association for\nComputational Linguistics.\nKatharina Kann, Oph´elie Lacroix, and Anders Søgaard.\n2020. Weakly supervised pos taggers perform\npoorly on truly low-resource languages.\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.\nRecurrent convolutional neural networks for text\nclassiﬁcation. In Twenty-ninth AAAI conference on\nartiﬁcial intelligence.\nLukas Lange, Michael A. Hedderich, and Dietrich\nKlakow. 2019. Feature-dependent confusion matri-\nces for low-resource NER labeling with noisy labels.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3554–\n3559, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAnne Lauscher, Vinit Ravishankar, Ivan Vulic, and\nGoran Glavas. 2020. From zero to hero: On the\nlimitations of zero-shot cross-lingual transfer with\nmultilingual transformers. ArXiv, abs/2005.00633.\nMelinda Loubser and Martin J. Puttkammer. 2020a. Vi-\nability of neural networks for core technologies for\nresource-scarce languages. Information, 11:41.\nMelinda Loubser and Martin J. Puttkammer. 2020b. Vi-\nability of neural networks for core technologies for\nresource-scarce languages. Information, 11:41.\nBingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing\nZhu, Songfang Huang, Rui Yan, and Dongyan Zhao.\n2017. Learning with noise: Enhance distantly super-\nvised relation extraction with dynamic transition ma-\ntrix. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 430–439, Vancouver,\nCanada. Association for Computational Linguistics.\nXianbin Lv, Dongxian Wu, and Shu-Tao Xia. 2020.\nMatrix smoothing: A regularization for DNN\nwith transition matrix under noisy labels. CoRR,\nabs/2003.11904.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1064–1074, Berlin, Ger-\nmany. Association for Computational Linguistics.\nStephen Mayhew, Snigdha Chaturvedi, Chen-Tse Tsai,\nand Dan Roth. 2019. Named entity recognition\nwith partially annotated training data. In Proceed-\nings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL) , pages 645–655,\nHong Kong, China. Association for Computational\nLinguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal dependencies v2:\nAn evergrowing multilingual treebank collection.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nDebjit Paul, Mittul Singh, Michael A. Hedderich, and\nDietrich Klakow. 2019. Handling noisy labels for\nrobustly learning from self-training data for low-\nresource sequence labeling. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, pages 29–34, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nAlexander Ratner, Stephen H. Bach, Henry R. Ehren-\nberg, Jason A. Fries, Sen Wu, and Christopher R ´e.\n2020. Snorkel: rapid training data creation with\nweak supervision. VLDB J., 29(2):709–730.\nShruti Rijhwani, Shuyan Zhou, Graham Neubig, and\nJaime Carbonell. 2020. Soft gazetteers for low-\nresource named entity recognition.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nStephanie Strassel and Jennifer Tracey. 2016.\nLORELEI language packs: Data, tools, and\nresources for technology development in low\nresource languages. In Proceedings of the Tenth\nInternational Conference on Language Resources\nand Evaluation (LREC’16) , pages 3273–3280,\nPortoroˇz, Slovenia. European Language Resources\nAssociation (ELRA).\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\nJennifer Tracey, Stephanie Strassel, Ann Bies, Zhiyi\nSong, Michael Arrigo, Kira Grifﬁtt, Dana Delgado,\nDave Graff, Seth Kulick, Justin Mott, and Neil\nKuster. 2019. Corpus building for low resource\nlanguages in the DARPA LORELEI program. In\nProceedings of the 2nd Workshop on Technologies\nfor MT of Low Resource Languages , pages 48–55,\nDublin, Ireland. European Association for Machine\nTranslation.\nHao Wang, Bing Liu, Chaozhuo Li, Yan Yang, and\nTianrui Li. 2019. Learning with noisy labels for\nsentence-level sentiment classiﬁcation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 6286–6292, Hong\nKong, China. Association for Computational Lin-\nguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In C. Cortes, N. D. Lawrence, D. D. Lee,\nM. Sugiyama, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 28 , pages\n649–657. Curran Associates, Inc.\nA Languages\nIn this work, we consider three languages: Hausa,\nisiXhosa and Yor`ub´a. These languages are from\ntwo language families: Niger-Congo and Afro-\nAsiatic, according to Ethnologue (Eberhard et al.,\n2019), where the Niger-Congo family has over20%\nof the world languages.\nThe Hausa language is native to the northern part\nof Nigeria and the southern part of the Republic of\nNiger with more than 45 million native speakers\n(Eberhard et al., 2019). It is the second most spoken\nlanguage in Africa after Swahili. Hausa is a tonal\nlanguage, but this is not marked in written text. The\nlanguage is written in a modiﬁed Latin alphabet.\nYor`ub´a, on the other hand, is native to south-\nwestern Nigeria and the Republic of Benin. It has\nover 35 million native speakers (Eberhard et al.,\n2019) and is the third most spoken language in\nAfrica. Yor `ub´a is a tonal language with three\ntones: low, middle and high. These tones are repre-\nsented by the grave (“\\”), optional macron (“−”)\nand acute (“/”) accents respectively. The tones are\nrepresented in written texts along with a modiﬁed\nLatin alphabet.\nLastly, we consider isiXhosa, a Bantu language\nthat is native to South Africa and also recognized\nas one of the ofﬁcial languages in South Africa and\nZimbabwe. It is spoken by over 8 million native\nspeakers (Eberhard et al., 2019). isiXhosa is a tonal\nlanguage, but the tones are not marked in written\ntext. The text is written with the Latin alphabet.\nKann et al. (2020) used as an indicator for a low-\nresource language the availability of data in the\nUniversal Dependency project (Nivre et al., 2020).\nThe languages we study suit their indicator having\nless than 10k (Yor`ub´a) or no data (Hausa, isiXhosa)\nat the time of writing.\nB Datasets\nB.1 Existing Datasets\nThe WikiAnn corpus (Pan et al., 2017) pro-\nvides NER datasets for 282 languages available\non Wikipedia. These are, however, only silver-\nstandard annotations and for Hausa and isiXhosa\nless than 4k and 1k tokens respectively are pro-\nvided. The LORELEI project announced the re-\nlease of NER datasets for several African languages\nvia LDC (Strassel and Tracey, 2016; Tracey et al.,\n2019) but have not yet done so for Hausa and\nYor`ub´a at the time of writing.\nEiselen and Puttkammer (2014) and Eiselen\n(2016) created NLP datasets for South African\nlanguages. We use the latter’s NER dataset for\nisiXhosa. For the Yor`ub´a NER dataset (Alabi et al.,\n2020), we use the authors’ split into training, dev\nand test set of the cased version of their data.2 For\nthe isiXhosa dataset 3, we use an 80%/10%/10%\nsplit following the instructions in (Loubser and\nPuttkammer, 2020b). The split is based on token-\ncount, splitting only after the end of the sentence\n(information obtained through personal conversa-\ntion with the authors). For the ﬁne-tuning of the\nzero- and few-shot models, the standard CoNLL03\nNER (Tjong Kim Sang and De Meulder, 2003) and\nAG News (Zhang et al., 2015) datasets are used\nwith their existing splits.\nB.2 New Datasets\nB.2.1 Hausa NER\nFor the Hausa NER annotation, we collected 250\narticles from VOA Hausa4, 50 articles each from\nthe ﬁve pre-deﬁned categories of the news web-\nsite. The categories are Najeriya (Nigeria), Aﬁrka\n(Africa), Amurka (USA), Sauran Duniya (the rest\nof the world) and Kiwon Laﬁya (Health). We re-\nmoved articles with less than 50 tokens which re-\nsults in 188 news articles (over 37K tokens). We\nasked two volunteers who are native Hausa speak-\ners to annotate the corpus separately. Each volun-\nteer was supervised by someone with experience\nin NER annotation. Following the named entity\nannotation in Yor `ub´a by Alabi et al. (2020), we\nannotated PER, ORG, LOC and DATE (dates and\ntimes) for Hausa. The annotation was based on\nthe MUC-6 Named Entity Task Deﬁnition guide.5\nComparing the annotations of the volunteers, we\nobserve a conﬂict for 1302 tokens (out of 4838 to-\nkens) excluding the non-entity words (i.e. words\nwith ’O’ labels). One of the annotators was better\n2https://github.com/ajesujoba/\nYorubaTwi-Embedding/tree/master/Yoruba/\nYor%C3%B9b%C3%A1-NER\n3https://repo.sadilar.org/handle/20.\n500.12185/312\n4https://www.voahausa.com\n5https://cs.nyu.edu/faculty/grishman/\nNEtask20.book_1.html\nin annotating DATE, while the other was better in\nannotating ORG especially for multi-word expres-\nsions of entities. We resolved all the conﬂicts after\ndiscussion with one of the volunteers. The split of\nannotated data of the Yoruba and Hausa NER data\nis 70%/10%/20% for training, validation and test\nsentences.\nB.2.2 Hausa and Yor `ub´a Text classiﬁcation\nFor the topic classiﬁcation datasets, news titles\nwere collected from VOA Hausa and the BBC\nYoruba news website 6. Two native speakers of\nthe language annotated each dataset. We catego-\nrized the Yor`ub´a news headlines into 7 categories,\nnamely “Nigeria”, “Africa”, “World”, “Entertain-\nment”, “Health”, “Sport”, “Politics”. Similarly, we\nannotated 5 (of the 7) categories for Hausa news\nheadlines, excluding “Sport” and “Entertainment”\nas there was only a limited number of examples.\nThe “Politics” category in the annotation is only for\nNigerian political news headlines. Comparing the\ntwo annotators, there was a conﬂict rate of 7.5%\nfor Hausa and 5.8% for Yor`ub´a. The total number\nof news titles after resolving conﬂicts was 2,917\nfor Hausa and 1,908 for Yor`ub´a.\nC Word Embeddings\nFor the RNN models, we make use of word fea-\ntures obtained from Word2Vec embeddings for\nthe Hausa language and FastText embeddings for\nYor`ub´a and isiXhosa languages. We utilize the\nbetter quality embeddings recently released by Ab-\ndulmumin and Galadanci (2019) and Alabi et al.\n(2020) for Hausa and Yor`ub´a respectively instead\nof the pre-trained embeddings by Facebook that\nwere trained on a smaller and lower quality dataset\nfrom Wikipedia. For isiXhosa, we did not ﬁnd any\nexisting word embeddings, therefore, we trained\nFastText embeddings from data collected from the\nI’solezwe7 news website and the OPUS8 parallel\ntranslation website. The corpus size for isiXhosa is\n1.4M sentences (around 15M tokens). We trained\nFastText embeddings for isiXhosa using Gensim9\nwith the following hyper-parameters: embedding\nsize of 300, context window size of 5, minimum\nword count of 3, number of negative samples ten\nand number of iterations 10.\n6https://www.bbc.com/yoruba\n7https://www.isolezwelesixhosa.co.za/\n8http://opus.nlpl.eu/\n9https://radimrehurek.com/gensim/\nD Distant Supervision\nD.1 Distant supervision for Personal names,\nOrganisation and Locations\nWe make use of lists of entities to annotate PER,\nORG and LOC automatically. In this paper, we ex-\ntract personal names, organizations and locations\nfrom Wikidata as entity lists and assign a corre-\nsponding named entity label if tokens from an un-\nlabeled text match an entry in an entity list.\nFor NER, we use manual heuristics to improve\nmatching. For Yor`ub´a, a minimum token length of\n3 was set for extraction of LOC and PER, while\nthe minimum length for ORG was set to 2. This\nreduces the false positive rate, e.g. preventing\nmatches with function words like “of”.\nApplying this on the test set, we obtained a pre-\ncision of 80%, 38% and 28% for LOC, ORG and\nPER respectively; a recall of 73%, 52% and 14%\nfor LOC, ORG and PER respectively; and an F1-\nscore of 76%, 44% and 19% for LOC, ORG and\nPER respectively.\nFor Hausa NER, a minimum token length of\n4 was set for extraction of LOC, ORG and PER.\nBased on these manual heuristics, on the test set,\nwe obtained a precision of 67%, 12% and 47%\nfor LOC, ORG and PER respectively; a recall of\n63%, 37% and 56% for LOC, ORG and PER re-\nspectively; and an F1-score of 65%, 18% and 51%\nfor LOC, ORG and PER respectively.\nD.2 DATE rules for NER\nRules allow us to apply the knowledge of domain\nexperts without the manual effort of labeling each\ninstance. We asked native speakers with knowledge\nof NLP to write DATE rules for Hausa and Yor`ub´a.\nIn both languages, date expressions are preceded by\ndate keywords, like “ranar” / “o. j´o. ” (day), “watan”\n/ “os. `u” (month), and “shekarar” / “o. d´u. n” (year)\nin Hausa/Yor`ub´a. For example, “18th of Decem-\nber, 2019” in Hausa / Yor`ub´a translates to “ ranar\n18 ga watan Disamba, shekarar 2019 ” / “o. j´o. 18\nos. `u O. p`e. , o. d´un 2019”. The annotation rules are\nbased on these three criteria: (1) A token is a date\nkeyword or follows a date keyword in a sequence.\n(2) A token is a digit, and (3) other heuristics to\ncapture relative dates and date periods connected\nby conjunctions e.g between July 2019 and March\n2020. Applying these rules result in a precision of\n49.30%/51.35%, a recall of 60.61%/79.17% and\nan F1-score of 54.42%/62.30% on Hausa /Yor`ub´a\ntest set respectively.\nD.3 Rules for Topic classiﬁcation\nFor the Yor `ub´a topic classiﬁcation task, we col-\nlected terms that correspond to the different classes\ninto sets. For example, the set for the class Nige-\nria contains names of agencies and organizations,\nstates and cities in Nigeria. The set for the World\nclass is made up of the name of countries of the\nworld, their capitals and major cities and world\naffairs related organizations. Names of sporting\nclubs and sportspeople across the world were used\nfor the Sports class and list of artists and actresses\nand entertainment-related terms for the Entertain-\nment class. Given a news headline to be annotated,\nwe get the union set of 1- and 2-grams and obtain\nthe intersection with the class dictionaries we have.\nThe class with the highest number of intersecting el-\nements is selected. In the case of a tie, we randomly\npick a class out the classes with a tie. Just as we did\nfor Yor`ub´a, we collected the class-related tokens\nfor the Hausa text classiﬁcation. We, however, split\nthe classiﬁcation into two steps, checking some im-\nportant tokens and using the same approach as we\nused for Yor`ub´a. If a headline contains the word\ncutar (disease) , it is classiﬁed as Health, if it con-\ntains tokens such as inec, zaben, pdp,apc (which\nare all politics related tokens) it is classiﬁed as Poli-\ntics. Furthermore, sentences with any of the follow-\ning tokens buhari, legas, kano, kaduna, sokoto are\nclassiﬁed as Nigeria while sentences with afurka,\nkamaru and nijar are classiﬁed as Africa. If none\nof the tokens highlighted above is found, we apply\nthe same approach as we did for the Yor `ub´a set-\nting, which is majority voting of the intersection set\nof the news headline with a keyword set for each\nclass. Applying these rules results in a precision\nof 59.54%/60.05%, a recall of 46.04%/53.66%\nand an F1-score of 48.52%/54.93% on the Hausa\n/Yor`ub´a test set respectively.\nE Experimental Settings\nE.1 General\nAll experiments were repeated ten times with vary-\ning random seeds but with the same data (sub-\nsets). We report mean F1 test score and standard\nerror (σ/\n√\n10). For NER, the score was computed\nfollowing the standard CoNLL approach (Tjong\nKim Sang and De Meulder, 2003) using theseqeval\nimplementation.10 Labels are in the BIO2-scheme.\n10https://github.com/chakki-works/\nseqeval\nFor evaluating topic classiﬁcation, the implemen-\ntation by scikit-learn was used.11 All models are\ntrained for 50 epochs, and the model that performed\nbest on the (possibly size-reduced) development set\nis used for evaluation.\nE.2 BERT and XLM-RoBERTa\nAs multilingual transformer models, mBert and\nXLM-RoBERTa are used, both in the implementa-\ntion by Wolf et al. (2019). The speciﬁc model IDs\nare bert-base-multilingual-cased and xlm-roberta-\nbase.12 For the DistilBERT experiment it is\ndistilbert-base-multilingual-cased. As is standard,\nthe last layer (language model head) is replaced\nwith a classiﬁcation layer (either for sequence or\ntoken classiﬁcation). Models were trained with\nthe Adam optimizer and a learning rate of 5e−5.\nGradient clipping of value 1 is applied. The batch\nsize is 32 for NER and 128 for topic classiﬁca-\ntion. For distant supervision and XLM-RoBERTa\non the Hausa topic classiﬁcation data with 100 or\nmore labeled sentences, we observed convergence\nissues where the trained model would just predict\nthe majority classes. We, therefore, excluded for\nthis task runs where on the development set the\nclass-speciﬁc F1 score was 0.0 for two or more\nclasses. The experiments were then repeated with\na different seed.\nE.3 Other Architectures\nFor the GRU and LSTM-CNN-CRF model, we\nuse the implementation by Chernodub et al. (2019)\nwith modiﬁcations to support FastText embeddings\nand the seqeval evaluation library. Both model\narchitectures are bidirectional. Dropout of 0.5 is\napplied. The batch-size is 10 and SGD with a learn-\ning rate of 0.01, and a decay of 0.05 and momentum\nof 0.9 is used. Gradients are clipped with a value\nof 5. The RNN dimension is 300. For the CNN,\nthe character embedding dimension is 25 with 30\nﬁlters and a window-size of 3.\nFor the topic classiﬁcation task, we experiment\nwith the RCNN model proposed by (Lai et al.,\n2015). The hidden size in the Bi-LSTM is 100\nfor each direction. The linear layer after the Bi-\nLSTM reduces the dimension to 64. The model is\ntrained for 50 epochs.\n11https://scikit-learn.org/stable/\nmodules/generated/sklearn.metrics.\nclassification_report.html\n12https://huggingface.co/transformers/\npretrained_models.html\n10 100 250 400 650 800 1014\nnumber of clean sentences\n0.0\n0.2\n0.4\n0.6\n0.8test F1\nXLM-RoBERTa\nLSTM-CNN-CRF\nGRU\n(a) NER Hausa\n10 100 250 400 650 816\nnumber of clean sentences\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7test F1\nmBERT\nLSTM-CNN-CRF\nGRU (b) NER Yor`ub´a\n10 300 700 2045\nnumber of clean sentences\n0.2\n0.4\n0.6\n0.8test F1\nXLM-RoBERTa\nXLM-RoBERTa + en-data (few-shot)\nXLM-RoBERTa + en-data (zero-shot)\nRCNN (c) Topic Class. Hausa\n10 800 2000 4000\nnumber of clean sentences\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7test F1\nXLM-R\nXLM-R + en-data (few-shot)\nXLM-R + en-data (zero-shot)\n(d) Transfer Learn NER isiXhosa\n10 100 250 400 650 816\nnumber of clean sentences\n0.50\n0.55\n0.60\n0.65\n0.70test F1\nmBERT\nmBERT + en-data (few-shot)\nmBERT + en-data (zero-shot) (e) Transfer Learn NER Yor`ub´a\n10 100 250 400 650 800 1014\nnumber of clean sentences\n0.5\n0.6\n0.7\n0.8test F1\nXLM-R\nXLM-R + distant\nXLM-R + distant + cm\nXLM-R + distant + cm-smooth (f) Distant NER Hausa\n10 300 700 2045\nnumber of clean sentences\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9test F1\nXLM-RoBERTa\nXLM-RoBERTa + distant\nXLM-RoBERTa + distant + cm\nXLM-RoBERTa + distant + cm-smooth\n(g) Distant Topic Class. Hausa\n10 300 700 1340\nnumber of clean sentences\n0.3\n0.4\n0.5\n0.6\n0.7test F1\nmBERT\nmBERT + distant\nmBERT + distant + cm\nmBERT + distant + cm-smooth (h) Distant Topic Class. Yor`ub´a\n10 100 250 500 650 800\nnumber of clean sentences\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8test F1\nHausa: limited dev set\nHausa: full dev set\nisiXhosa: limited dev set\nisiXhosa: full dev set\nYoruba: limited dev set\nYoruba: full dev set (i) Development Set NER\nFigure 3: Additional plots.\nE.4 Transfer Learning\nFor transfer learning, the model is ﬁrst ﬁne-tuned\non labeled data from a high-resource language.\nFollowing (Hu et al., 2020), we use the English\nCoNLL03 NER dataset (Tjong Kim Sang and\nDe Meulder, 2003) for NER. It consists of ca. 8k\ntraining sentences. The model is trained for 50\nepochs and the weights of the best epoch accord-\ning to the development set are taken. The train-\ning parameters are the same as before. On the\nEnglish CoNLL03 test set, the model achieves a\nperformance of 0.90 F1-score. As the Hausa and\nYor`ub´a datasets have slightly different label sets,\nwe only use their intersection, resulting in the labels\nPER, LOC and ORG and excluding MISC from\nCoNLL03 and the DATE label from Hausa/Yor`ub´a.\nFor isiXhosa, the label sets are identical (i.e. also\nincluding MISC). After ﬁne-tuning the model on\nthe high-resource data, the model is directly evalu-\nated on the African test set (for zero-shot) or ﬁne-\ntuned and then evaluated on the African data (for\nfew-shot).\nFor topic classiﬁcation, the AG News corpus\nis used (Zhang et al., 2015). It consists of 120k\ntraining sentences. The model is trained for 20\nepochs and the weights of the best epoch according\nto the test set are used. On this set, an F1-score\nof 0.93 is achieved. The training procedure is the\nsame as above. For the labels, we use the union of\nthe labels of the AG News corpus (Sports, World,\nBusiness and Sci/Tech) and the African datasets.\nE.5 Label Noise Handling\nWe use a confusion matrix which is a common ap-\nproach for handling noisy labels (see, e.g. (Fang\nand Cohn, 2016; Luo et al., 2017; Lange et al.,\n2019; Wang et al., 2019)). The confusion matrix\nmodels the relationship between the true, clean la-\nbel of an instance and its corresponding noisy label.\nWhen training on noisy instances, the confusion\nmatrix is added to the output of the main model\n(that usually predicts clean labels) changing the\noutput label distribution from the clean to the noisy\none. This allows to then train on noisily labeled\ninstances without a detrimental loss obtained by\npredicting the true, clean label but having noisy,\nincorrect labels as targets.\nWe use the speciﬁc approach by Hedderich and\nKlakow (2018) that was developed to work with\nsmall amounts of manually labeled, clean data and\na large amount of automatically annotated, noisy\nlabels obtained through distant supervision. To get\nthe confusion matrix of the noise, the distant super-\nvision is applied on the small set of clean training\ninstances. From the resulting pairs of clean and\nnoisy labels, the confusion matrix can be estimated.\nIn a setting where only a few instances are avail-\nable, the estimated confusion matrix might not be\nclose to the actual change in the noise distribution.\nWe, therefore, combine it with the smoothing ap-\nproach by Lv et al. (2020). Each entry of the prob-\nabilistic confusion matrix is raised to the power of\nβand then row-wise normalized.\nAs studied by Hedderich and Klakow (2018), we\ndo not use the full amount of available, distantly\nsupervised instances in each epoch. Instead, in\neach epoch, only a randomly selected subset of the\nsize of the clean, manually labeled training data\nis used to lessen the negative effects of the noisy\nlabels additionally. For smoothing, β = 0.8 is used\nas this performed best for Lv et al. (2020)."
}