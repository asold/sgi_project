{
  "title": "Fine-tuning and Utilization Methods of Domain-specific LLMs",
  "url": "https://openalex.org/W4390722655",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3087658090",
      "name": "Jeong Cheonsu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4380758928",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4390486238",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3087589180",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extraction, and customer service enhancement, are exemplified. The study explores the potential of LLMs in the financial domain, identifies limitations, and proposes directions for improvement, contributing valuable insights for future research. Ultimately, it advances natural language processing technology in business, suggesting proactive LLM utilization in financial services across industries.",
  "full_text": "- 1 - \nFine-tuning and Utilization Methods of Domain-specific LLMs \nCheonsu Jeong1 \nDr. Jeong is Principal Consultant & the Technical Leader for AI Automation at SAMSUNG SDS  \nAbstract \nRecent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, \nyet research on fine -tuning and employing domain -specific LLMs remains scarce. This study \ninvestigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends \nin LLMs, foundational model s, and methods for domain -specific pre-training. Focusing on the \nfinancial sector, it details dataset selection, preprocessing, model choice, and considerations \ncrucial for LLM fine -tuning in finance. Addressing the unique characteristics of financial data , \nthe study explores the construction of domain -specific vocabularies and considerations for \nsecurity and regulatory compliance. \nIn the practical application of LLM fine -tuning, the study outlines the procedure and \nimplementation for generating domain-specific LLMs in finance. Various financial cases, \nincluding stock price prediction, sentiment analysis of financial news, automated document \nprocessing, research, information extraction, and customer service enhancement, are exemplified. \nThe study explores the potential of LLMs in the financial domain, identifies limitations, and \nproposes directions for improvement, contributing valuable insights for future research. \nUltimately, it advances natural language processing technology in business, suggesting proactive \nLLM utilization in financial services across industries. \n \nKeywords: Domain Specific LLM, PLM  (Pre-trained Language Model), FLM  (Fine-tuning \nLanguage Model), PEFT, Generative AI \n \n \n \n1 Corresponding and first author: jeongcsmon@gmail.com \n- 2 - \n1. Introduction \nIn recent years, advancements in artificial intelligence and deep learning technologies have led \nto remarkable achievements, particularly in the field of natural language processing. Large \nLanguage Models (LLMs), in particular, have reached human -level language generation and \ncomprehension capabilities with their rich context and language understanding. Consequently, \nvarious industries are actively considering or implementing the adoption of LLMs. According to \nMcKinsey, generative AI, including LLMs, is \n anticipated to contribute to productivity enhancement in the banking industry, specifically in \nareas such as marketing/sales, customer support/management, programming, and regulatory \ncompliance. The potential value creation by generative AI in the global banking industry is \nprojected to range from $200 billion to $340 billion, corresponding to 2.8% to 4.7% of the \nindustry's total revenue. Financial institutions are increasingly leveraging LLMs to support and \nautomate employee tasks internally, as well as t o collect and analyze natural language -based \ninformation for strategic decision -making (Financial Focus, 2023). Even in the finance sector, \ntraditionally slow in adopting new technologies, there is a growing expansion of the use of \ngenerative AI, specifically LLMs. \nTherefore, this study aims to explore fine -tuning and utilization cases of LLMs, particularly \nthose applied in the financial domain. The financial sector is characterized by rapidly changing \nmarket environments, diverse financial events, and vast amounts o f financial data. To address \nthese challenges, rapid and accurate information processing and decision-making capabilities are \nessential. Recent research suggests that LLMs can effectively address these challenges, and the \nmarket has witnessed active releas es of general LLMs. However, in -depth research that \ncomprehensively understands and addresses the various issues arising during the construction and \nutilization of LLMs tailored for specific tasks is still lacking. \nThis study seeks to propose specific methods for applying domain-specific LLMs, with a focus \non the financial industry. The financial sector possesses specific characteristics, such as trust, \nconsumer protection regulations, and inclusive finance, distingu ishing it from other industries \n(Financial Focus, 2023). The financial domain, being sensitive to technological innovation while \nprioritizing stability and accuracy, underscores the importance of developing and utilizing \ndomain-specific LLMs to enhance the efficiency and competitiveness of financial businesses. \nThis research, within this context, is expected to provide a deep understanding of the core \n- 3 - \ntechnology and potential applications of domain -specific LLMs in the financial industry, \ndelivering practical value to financial institutions and research organizations. The study \nspecifically addresses LLM fine-tuning and application cases in the financia l domain. However, \nthe financial sector is extensive, comprising various subfields, and further research is needed to \ncover specific areas not addressed in this study. Additionally, the interpretation of research results \nwill consider the limitations and constraints of language models. The primary aim of this research \nis to explore effective methodologies for fine-tuning domain-specific LLMs in the financial sector \nand to study real -world use cases in financial tasks. It is anticipated that language models \nreflecting expertise in the financial domain will demonstrate enhanced performance, contributing \nto increased productivity and decision support in financial tasks. \nThis paper is organized as follows: Chapter 2 examines LLMs and fine -tuning, Chapter 3 \nprovides detailed explanations of approaches to create domain -specific LLMs tailored for the \nfinancial sector. Chapter 4 explores various perspectives on LLM creation me thods and their \napplication in the financial sector, and Chapter 5 concludes the research and discusses future \ndirections.  \n \n2. Related Work \nFor this study, recent major research papers, journals, articles, and books related to generative AI \nand LLM were investigated. In this chapter, an overview of LLMs, generative AI, and foundation \nmodels is discussed, followed by an in -depth exploration of LLM fine-tuning. The application \nareas of language models in the financial domain, as covered in this paper, are also outlined. \n \n2.1. Overview of Large Language Models (LLMs) \nIn recent years, LLMs have witnessed distinctive advancements in the field of Natural Language \nProcessing (NLP). Models such as BERT (Bidirectional Encoder Representations from \nTransformers) and GPT (Generative Pre -trained Transformer), based on the Transf ormer \narchitecture, have demonstrated powerful representations learned through pre-training on massive \ntext data, making them applicable to various NLP tasks. These models showcase excellent \nperformance in understanding text context, diverse grammatical st ructures, and semantic \nrelationships. Generative AI, as a form of artificial intelligence, utilizes extensively trained data \n- 4 - \nmodels to generate new content, including text, images, audio, and video (Jeong C.S., 2023d). In \nthe NLP domain, advancements in Natural Language Understanding (NLU) technologies have \nenabled complex dialogue processing through the utilization of Context models and Transformer \nlanguage models (Jeong, 2023a). Recently, the integration of chatbots  with other solutions such \nas Robotic Process Automation (RPA) and Optical Character Recognition (OCR) has been \nobserved to directly enhance efficiency in various ta sks (Jeong C.S., Jeong J.H., 2020). In \nNovember 2022, OpenAI introduced the ChatGPT model, an AI chatbot trained through a \nreinforcement learning process involving feedback from a group of human experts, enhancing the \nconversational capabilities of the GPT -3.5 model. ChatGPT garnered significant attention, \nsurpassing 100 million monthly users within two months of its release (Jeong C.S., 2023b). In \nopen-domain chatbot conversations like ChatGPT, an interface with LLM models occurs through \nprompts, and caution must be exercised when applying parameters between applications to avoid \nhandling sensitive personal information. Although laws exist to prevent developers from \ncollecting and using user data without consent, users find it challenging to understand how much \ndata developers collect and where this data is stored in real-life situations (Jeong, J. H., and Jeong, \nC. S., 2022). Additionally, in conversation with ChatGPT, the completeness of responses depends \non how detailed the question or request prompt is. Therefore, investigating prompt engineering, \nwhich involves finding combinations of prompt input values from LLMs, is crucial (Jeong C.S., \n2023c). Specifically, in the financial sector, the importance of providing up -to-date information \nthrough customer re sponse chatbots is emphasized, and limitations in the information capacity \nand hallucination issues of LLMs are identified as challenges. Approaches to address these \nchallenges include fine -tuning with new data and directly inserting information into promp t \ncontexts. However, fine-tuning incurs significant costs, and including all information in prompts \nis practically challenging. As an alternative, the Retrieval -Augmented Generation (RAG) model \nhas been proposed, as illustrated in Figure 1. This model invo lves storing information in vector \ndatabases and searching for the required information to be presented to the LLM (Jeong C.S., \n2023e).  \n- 5 - \n \n<Figure 1> RAG-based Vector Store configuration type and processing procedure \nAdditionally, LLMs and generative AI, as depicted in Figure 2, are positioned within AI's deep \nlearning, allowing for the utilization of LLMs based on deep learning to provide generative AI \nservices (Mayank, S., 2023; Jeong C.S., 2023e). \n \n<Figure 2> LLM & Generative AI Relation Diagram  \n2.1.1. Trends in Generative AI  \nIn recent years, as of 2023, the development of super -sized AI technologies has been rapidly \nadvancing. Table 1 illustrates the current status of LLMs and generative AI services introduced in \n2023.  \n\n- 6 - \n<Table 1> Status of generative AI model launches in 2023 \nCountry Company \nFoundation \nModel Parameters Source Release \nDate Service \nUSA \nOpenAI / MS GPT-4 Turbo 1.75T Closed 2023.11 ChatGPT, GPTs / MS Bing AI, MS \nCopilot, MS 365 Copilot \nGoogle Gemini Closed Closed 2023.12 Bard \nMETA LLaMA2 7~65B Open 2023.07 Code Llama \n Stanford Univ. Alpaca 7B Open 2023.03 LLaMA 7B Fine-tuning Model \nNomic GPT4All v2 3~13B Open 2023.04 LLaMA 7B Fine-tuning Model \nHugging Face BLOOM 176B Open 2022.07 - \nSouth \nKorea \nNaver HyperClovaX Closed Closed 2023.08 Polaris Office AI, Lewis, etc. \nLG EXAONE2.0 300B Self-utilization 2023.07 AI artist Tilda, etc. \nNC Soft VARCO Closed Closed 2023.08 VARCO Art/Text/Human/Studio \nSKT A. Enterprise Closed Self-utilization 2023.08 Document summary & creation, etc. \nKT MI:DEUM2 200B Self-utilization 2023.10 GiGA Genie, AICC, AI care service \nSAMSUNG Samsung Gauss Closed Self-utilization 2023.11 Gauss Language/Code/Image \nChina \nHuawei PanGu 3.0 100B Open 2023.07 Pangu-Weather, etc. \nBaidu Ernie 3.5 130B Self-utilization 2023.06 Ernie Bot 3.5 \nAlibaba Tongyi Qianwen 10T Self-utilization \nOpen 2023.04 DingTalk, Tmall Genie \nOpen source service: ModelScope \nUAE TII Falcon 7~180B Open 2023.09 - \n \nIn March, OpenAI unveiled GPT-4, and in May, Google introduced PaLM2. While the parameter \ncount was reduced compared to previous models, they achieved higher performance by training \non approximately five times more tokens (text data). Additionally, in Nove mber, Samsung \npublicly revealed Samsung Gauss, stating plans to gradually incorporate Samsung Gauss into \nproducts such as the Galaxy S24 to be released in the future. Noteworthy companies like Kakao \nare either in the process of constructing or considering developing their own generative AI. There \nis a growing interest in open -source LLMs such as Meta's LLaMA (Large Language Model \nAttention) and Falcon, with a focus on training volume rather than model size (Jeong C.S., 2023e). \n2.1.2. Foundation Model and LLM Pre-training \nGenerative AI models, depending on the type of output they generate, employ language models, \nimage models, video models, etc. However, currently, multi -modal models, capable of \nsimultaneously learning from both images and text, are rapidly evolving, establishing themselves \n- 7 - \nas foundation models (Jeong C.S., 2023e). The data for the foundation model, which encompasses \ntext, images, audio, structured data, 3D signals, etc., is utilized without distinction during training. \nThese models, including human creativity and reasoning abilities, represent a shift in the AI \nparadigm and are referred to as foundation models. Massive amounts of data, obtained through \nunsupervised learning, are deployed and fine -tuned for downstream tasks such as fine -tuning or \nin-context learning, completin g the foundation model to suit the user's desired objectives \n(Bommasani, R., et al., 2021).  \nAmong foundation models, Large Language Models (LLMs) as language models are pre -trained \nlanguage models (PLMs) that utilize vast amounts of general knowledge, such as text data from \nsources like Wikipedia, collected through self -supervised or semi -supervised learning. The \nfoundational process involves gathering the training dataset, which serves as the learning resource \nfor the LLM. The data can be sourced from various outlets, including books, websites, articles, \nand public datasets. To develop a proficient LLM, a text dataset from pre-trained material is used. \nThe sources of pre -trained corpora can be broadly classified into two types: general data and \nspecialized data. General data, such as web pages, books, and conversational texts, is extensive, \ndiverse, and readily accessible, making it predominantly utilized in most LLMs. It enhances the \nlanguage modeling and generalization capabilities of LLMs. Additionally, there are studies \nexpanding the pre-trained corpus to more specialized datasets, such as multilingual data, scientific \ndata, and code, to give LLMs specific task -solving capabilities (Taylor, R., et al., 2022; \nChowdhery, A., et al., 2022; Nijkamp, E., et al., 2022). \n \n<Figure 3> LLM Pre-training Process \nFigure 3 illustrates the typical data collection and pre-training process for LLMs (Zhao, W. X., et \nal., 2023; Jeong C.S., 2023e). Model training involves using supervised learning on pre-processed \ntext data. Given the substantial sizes of both the model and the data, model parallelism is \nemployed to facilitate training by requiring substantial computational power. Training a large -\nscale language model from scratch demands significant investment; therefore, a more economical \nalternative is to fine-tune existing language models for specific use cases (Jeong C.S., 2023e). \n2.1.3. Domain-Specific LLM Pre-training \nWhen aiming to create an LLM specialized in a particular domain, the process involves selecting \n\n- 8 - \nthe target domain, collecting relevant data, injecting specific domain data information into a \ngeneral LLM, and then training a conversational model for the selected domain (Adaptive Pre -\ntraining). \nSeveral criteria guide domain selection, including the likelihood of a domain being already learned, \nthe uniqueness of a niche or recently updated domain, the presence of at least 10,000 characters \nof text data describing the domain, and the verification of learning status for domains with formal \nWikipedia or Namuwiki information. \n \n2.2. LLM Fine-Tuning \nWhile Large Language Models (LLMs) are trained on extensive datasets and possess general \nknowledge, they may not perform optimally in specific tasks without fine -tuning. Therefore, as \ndepicted in Figure 4, the model's performance is enhanced through a fine -tuning process. For \nmodels of a smaller scale, such as BERT (450M) or RoBERTa (1.3G), full fine -tuning was \ntraditionally conducted. However, with the introduction of LLaMA, fine -tuning these large \nmodels in their entirety became computationally challengin g. Approaches like LoRA, which \ninvolves fixing the weights of existing pre -trained layers and only training the weights of new \nlayers, were employed. Recently, it has been validated that there is not a significant difference in \nactual performance, leading to the adoption of the Parameter-efficient Fine-tuning (PEFT) method. \nPEFT involves adding a small number of new parameters to the pre-trained LLM, fine-tuning only \nthe added parameters, and achieving better performance at a lower cost (Raschka, S., 2023).  \n \n<Figure 4> Fine-tuning of LLM \n\n- 9 - \nLoRA, a prominent approach, allows additional learning for specific weight matrices within Large \nLanguage Models (LLMs). Within each Transformer Layer, it determines the target for fine-tuning \nby selecting specific weight matrices. These matrices are repli cated in their original form, and \nfine-tuning proceeds with allowing learning for the chosen weight matrices. The weight matrices \nundergo Low-Rank Matrix Decomposition, forming two matrices, as illustrated in Figure 5. \n \n<Figure 4> LoRA Re-Parameterization \nThe Α matrix is subject to Random Gaussian initialization, while the Β matrix is initialized to \nZero at the beginning of the learning process (Edward, H., et al., 2021). This methodology ensures \nthat fine -tuning is selectively applied to essential weight m atrices, optimizing the LLM for \nspecific tasks. \nGiven that pre-training data significantly influences the performance of Large Language Models \n(LLMs), the demand for high-quality data for model pre-training is even more crucial for LLMs \ncompared to small language models. The model's capacity relies heav ily on the corpus data \ncollected for pre -training and the pre -training processing methods (Jeong, C.S., 2023e). LLMs, \nserving as foundational models that leverage deep learning for Natural Language Processing (NLP) \nand Natural Language Generation (NLG) tas ks, undergo pre -training on extensive datasets to \nlearn the complexity and connectivity of language. Various technologies, including fine -tuning, \nin-context learning, zero/one/few -shot learning, are employed to facilitate this learning process \n(Dilmegani, C., 2023). \nAfter pre-training, the model is evaluated on a test dataset that was not used for training to measure \nits performance. Based on the evaluation results, hyperparameters may be adjusted, architecture \nmodified, or additional training on new data may be performed to fine-tune the model and enhance \n\n- 10 - \nits performance (Jeong, C.S., 2023e). The resulting Fine -tuned Language Model (FLM) is then \napplied in various domains, serving as a specialized small language model (SLM). \n2.2.1. Recent Advances in Fine-Tuning \nThe recent advancements in fine-tuning, particularly the Parameter-efficient Fine-Tuning (PEFT) \nmethod, can be categorized into Prompt Modification, Adapter Methods, and Parameterization. \nPrompt Modification includes Hard Prompt Tuning, Soft Prompt Tuning,  and Prefix -tuning. \nAdapter Methods, such as LLaMA-Adapter, aim to minimize the side effects of fine-tuning on the \nentire model by introducing modularized parameters through adapters. In this process, a single \nadapter module, centered around the Bottleneck Layer, performs linear transformations of Down-\nprojection and Up-projection, while the pre-trained LLM is frozen during fine-tuning. \n \n<Figure 6> Adapter methods \nNotably, QLoRA, an improved version of LoRA, has been introduced. QLoRA provides a method \nto PEFT LLM using QLoRA, allowing testing of LLM models even on personal computers. \nUnlike LoRA, which concatenates additional data by keeping the base model's network intact, \nQLoRA introduces 16-bit network nodes quantized to 4 bits and employs a paging mechanism for \nswapping binary data to handle large models with limited memory. Although there is some \ninformation loss in reducing from 16 bits to 4 bits, it is considered acceptable (Dettmers, T., et al., \n2023). \n\n- 11 - \n \n<Figure 7> QLoRA improves over LoRA \nAnother prevalent fine-tuning approach is Instruction Tuning, where the task is transferred to \nnatural language. Unlike Vanilla-LLM, which simply completes the next text, Instruction Tuning \nLLM completes the next text based on user instructions. This invol ves training the model to \nunderstand and execute task instructions, enabling it to infer new tasks according to the provided \ninstructions. Alpaca LLM, for instance, is a model based on LLaMA 7B, fine -tuned thr ough \ninstruction tuning. \n \n3. Methods \nIn this section, we present a systematic approach for generating finance -specialized LLM. The \nfine-tuning process follows the procedure outlined in Figure 8. The data collection and \npreprocessing stages involve selecting finance -specific datasets and implementing effective \npreprocessing methods. In the model selection and fine-tuning steps, a suitable pre-trained LLM, \nknown as Pre -trained Language Model (PLM), is chosen, and hyperparameters are tuned \naccordingly. Considerations for fine-tuning in the context of finance include addressing financial \ndata characteristics, domain-specific vocabulary, and fine-tuning algorithms. \nNext, in the configuration of LLMs for the finance domain, we introduce approaches for both \ntraining models from scratch and tuning existing models. Lastly, in the evaluation criteria and \nmetrics section, we guide the evaluation of the model using both qua ntitative and qualitative \nperformance metrics. Through this, we aim to comprehensively outline the creation of finance -\nspecialized LLMs. \n\n- 12 - \n \n<Figure 8> Overview of Fine-tuning Procedure \n  \n3.1. Data Collection and Preprocessing  \n3.1.1. Selection of Finance-Specific Datasets \nFirstly, selecting datasets specialized in the finance domain is crucial. To achieve this, various \nsources such as financial research reports, financial news, and market transaction records will be \nutilized. Finance datasets can exist in diverse forms, including financial -related texts, codes, \nimages, audio, etc. Consider the following factors when selecting finance-specific datasets: \n1) Type of Data: Finance datasets can take the form of text, code, images, audio, etc. Each \ntype has distinct characteristics, so choose data that aligns with the intended purpose. \n2) Quantity of Data:  The volume of data significantly influences model performance. \nGenerally, larger datasets lead to improved model performance. \n3) Quality of Data: Data quality impacts model accuracy. Ensure that the data is free from \nerrors or biases, as these can degrade model performance. \nTwo primary methods can be employed to select finance-specific datasets:  \n1) Using Existing Datasets:  Utilize publicly available datasets in the finance domain. \nExamples include: \n\n- 13 - \n. Financial News Dataset: Collected news articles for sentiment analysis related to \nfinancial markets. \n. Financial Document Dataset: Compilation of financial reports, contracts, regulations, \netc., for understanding financial products and services. \n. Financial Code Dataset: Collection of financial-related software code for understanding \nand automating financial systems. \n2) Creating a Custom Dataset: Building a dataset from scratch provides the advantage of \ntailoring it to specific needs. Considerations include: \n. Data Collection Method: Choose methods such as web scraping, database querying, or \nsensor data, depending on the intended use. \n. Data Cleaning Method: Remove errors or biases during data cleaning. \nThese datasets can enhance sentiment analysis of financial news or recommend financial products \nmore effectively based on customer characteristics. \n3.1.2. Data Preprocessing Methods \nThe collected financial data is likely to be irregular and complex. Therefore, preprocessing the \ndata to a format suitable for model training is necessary. Techniques such as text normalization, \ntokenization, stop-word removal, and morphological analysis are applied to refine the data. \nIn the finance domain, commonly used data preprocessing methods include: \n1) Text Data Preprocessing: \n. Character Normalization: Removing special characters, emojis, and extra spaces. \n. Word Tokenization: Breaking sentences into individual words. \n. Stop-word Removal: Eliminating meaningless words. \n. Building V ocabulary: Storing word meanings and features in a dictionary. \n2) Code Data Preprocessing: \n. Code Cleanup: Standardizing code format and correcting errors. \n. Code Structure Analysis: Understanding code structure and dividing it into meaningful \nunits. \n. Code Meaning Analysis: Comprehending code meaning and extracting relevant \n- 14 - \ninformation. \n3) Image Data Preprocessing: \n. Image Normalization: Adjusting image size, brightness, contrast, etc. \n. Feature Extraction: Extracting features from images. \n. Image Classification: Categorizing images. \nThese preprocessing steps ensure that the finance model comprehends the nuances of financial \nproducts more accurately. \n \n3.2. Model Selection and LLM Fine-Tuning Process \nIn this section, the essential steps for selecting a model and fine-tuning a Language Model (LLM) \nfor the finance domain are covered. Firstly, an appropriate pre-trained LLM model is chosen, and \nthen the model undergoes a fine -tuning process to specialize it for the finance domain. This \napproach allows the construction of a language model sensitive to the nuances of the finance \nindustry. \n \n3.2.1. Selection of Pre-trained LLM Model \nTo develop a finance -specific LLM, the choice of a pre -trained language model is crucial. The \nselected model should have language comprehension abilities that extend to the specific \nterminologies and contexts of the finance domain. Consider the following criteria when choosing \na pre-trained LLM model:  \n1) Model Size: The size of the model impacts its performance. Generally, larger models tend \nto deliver better results. \n2) Model Purpose: Different models may be more suitable depending on their intended use. \nFor tasks like sentiment analysis of financial news, a model specialized in natural \nlanguage processing is preferable. \n3) Model Availability: If a model is not publicly available, training the model from scratch \nbecomes necessary. This process can consume considerable time and resources. \nWhen selecting a pre-trained LLM model, consider the following elements: \n- 15 - \n1) Performance and Applicability: Assess the model's performance and suitability for finance \napplications. \n2) Training Data Characteristics: Ensure that the model's training data aligns with the \ncharacteristics of financial data. The model selection should align with both performance \nmetrics and the specific requirements of the finance domain. It's essential to verify that \nthe chosen model understands the nuances of financial terms and contexts. \nOnce a pre-trained model is selected, the next step involves fine-tuning it to adapt to the intricacies \nof the finance domain. The fine -tuning process refines the model's understanding of finance -\nrelated language patterns, enhancing its applicability to tasks within this specific industry. \nSeveral pre-trained Language Models (LLMs) are available for applications in the finance domain. \nHere are some notable models: \n1) GPT-4: GPT-4 is a large-scale language model developed by OpenAI. It has 1.75 trillion \nparameters, making it one of the most powerful language models in the world. GPT-4 can \nbe used in a variety of fields, including natural language processing, machine translation, \nand code generation. It has been shown to be capable of generating human -quality text, \ntranslating languages accurately, and writing different kinds of creative content. \n2) BERT: BERT is a large-scale language model developed by Google AI. It has 100 million \nparameters, making it smaller than GPT-4 but still very powerful. BERT can be used in a \nvariety of fields, including natural language processing, question answering, and \nsentiment analysis. It has been shown to be very good at understanding the meaning of \ntext and answering questions about it. \n3) LLaMA2: LLaMA2 is a large -scale language model developed by Meta. It has 7 to 65 \nbillion parameters, making it a smaller model than GPT -4 but still very powerful. \nLLaMA2 is unique in that it is released as open source, meaning that anyone can use it \nfor research or commercial purposes. This has made it a popular choice for researchers \nand developers who want to build their own language models. \n4) Falcon: Falcon is a large-scale language model developed by TII in the UAE. It has 7 to \n180 billion parameters, making it a very powerful model. Falcon is also released as open \nsource, making it a popular choice for researchers and developers. \n5) BloombergGPT: BloombergGPT is a finance-specific large language model developed by \n- 16 - \nBloomberg. It has 10 billion parameters, making it smaller than some of the other models \non this list but still very powerful. BloombergGPT is trained on a dataset of financial news \nand can be used to perform a variety of tasks in the financial field, such  as sentiment \nanalysis of financial news and recommendation of financial products. \n6) FinBERT: FinBERT is a variant of BERT that is specifically designed for the financial \nfield. It is pre-trained with a dataset of financial data and can be used to perform natural \nlanguage processing tasks in the financial field, such as named entity recogn ition and \nrelationship extraction. \nWhen selecting a pre -trained LLM model for the finance domain, consider factors such as the \nmodel's size, purpose, and availability. The choice between GPT-4, BERT, and recently introduced \nfinance-specialized models like BloombergGPT and FinBERT depends on  the specific \nrequirements and goals of the intended applications. \nAdditionally, models like LLaMA2 and Falcon, which are open -source and versatile, can be \nconsidered based on their availability and suitability for the finance domain. The ultimate decision \nshould align with the intended use case and the model's ability to  capture the intricacies of \nfinancial language and context. \n3.2.2. Hyperparameter Tuning for LLM \nHyperparameter tuning is a crucial step in maximizing the performance of a model. Adjusting key \nhyperparameters appropriately during the construction of a domain-specific LLM enhances the \nmodel's learning and generalization abilities. Here are some essential hyperparameters to consider: \n1) Learning Rate: Determines how much the model's weights should be updated. Set an \nappropriate initial learning rate to control the convergence speed and prevent issues like \ndivergence or insufficient convergence (Example: Set the initial learning rate to 0.0001). \n2) Batch Size: Determines the amount of data processed by the model in each iteration. \nChoose a batch size that balances the trade -off between allowing more updates and \ncontrolling memory usage (Example: Set the batch size to 32 or 64). \n3) Number of Epochs: Represents how many times the entire dataset is passed through the \nmodel. Avoid too few epochs, which may result in insufficient learning, or too many \nepochs, which could increase the risk of overfitting  (Example: Set the number of epochs \nto 10 or 20). \n- 17 - \n4) Dropout Rate: Dropout randomly excludes some neurons during training to prevent the \nmodel from relying too heavily on specific patterns.  Choose an appropriate dropout rate \nto enhance the model's generalization ability (Example: Set the dropout rate to 0.2). \n5) Other Model -Specific Hyperparameters:  Different models may have specialized \nhyperparameters. For instance, in Transformer -based models, adjusting the number of \nattention heads or layers can impact model complexity and performance (Example: Adjust \nthe number of attention heads, Transformer layers, etc.). \nWhen tuning hyperparameters, it is common to conduct experiments to find the optimal \ncombination. Continuously evaluate the performance changes resulting from different settings and \niteratively refine the hyperparameters to achieve the best learning config uration. This iterative \nprocess allows for the discovery of optimal hyperparameter values that contribute to the model's \neffectiveness in the finance domain.  \n3.2.3. Fine-Tuning Execution Environment Setup \nSetting up the fine-tuning execution environment is a critical step for optimizing model training \nand achieving high performance. To fine-tune a model, it's essential to configure an appropriate \nexecution environment. This involves utilizing high-performance computing resources, including \nGPU acceleration, to enhance training speed and establishing an environment for real -time \nmonitoring of the model's performance. By considering various factors and configuring the \nexecution environment accordingly, the efficiency and performance of the model can be improved. \n1) Hardware Selection: Choose high -performance hardware for fine -tuning large datasets \nand complex models. Utilize GPU (Graphics Processing Unit) or TPU (Tensor Processing \nUnit) to accelerate model training and efficiently process massive amounts of data. \n2) Distributed Training: Apply distributed training to shorten the training time of large-scale \nmodels. Technologies like RAY for distributed computing and libraries like Hugging \nFace's DeepSpeed, which specializes in deep learning training optimization, can be \nutilized for efficient parallel processing. \n3) Accelerating Technologies: Leverage hardware -accelerating technologies to speed up \nmodel training. CUDA and cuDNN are libraries that assist in accelerating deep learning \nmodels, especially on NVIDIA GPUs. \n4) Batch Normalization: Enhance model stability and accelerate training by incorporating \n- 18 - \nbatch normalization. Stabilize the distribution between layers in deep learning models to \nimprove performance (e.g., add batch normalization layers to the model architecture). \n5) Data Augmentation: Utilize data augmentation techniques to increase the diversity of text \ndata. Techniques such as shuffling word orders within sentences or inserting synonyms \nhelp the model learn from various contexts. \n6) Experiment Logging and Monitoring: Use tools like TensorBoard  or preferred logging \ntools to monitor performance during training. Log important metrics during training to \nmonitor the model's performance in real -time and quickly identify issues during the \ntraining process. \n7) Hyperparameter Tuning: Use hyperparameter optimization tools to automatically find the \noptimal combination. Techniques like Grid Search or Random Search help adjust \nhyperparameters, maximizing model performance. \nFine-tuning execution environment setup requires careful planning and supervision to ensure \nexperiment efficiency and model convergence. Regularly optimize based on experiment \nresults within the configured environment to achieve the best possible performance. \n \n3.3. Considerations for Fine-Tuning Financial Specialized LLM \nFine-tuning a language model (FLM) for the financial domain involves several steps to ensure the \nmodel's effectiveness in understanding and generating financial text. Here are key considerations. \n3.3.1. Building Domain-Specific Vocabulary Considering Financial Data Characteristics \nFinancial data possesses unique features, including the impact of stock price fluctuations, \nsentiment in financial news, and various other aspects. To fine -tune the model effectively, it's \ncrucial to consider these characteristics: \n1) Usage of Financial Terminology: Incorporate financial jargon and terminology into the \nmodel's vocabulary. Utilize pre -training with financial domain -specific dictionaries or \nglossaries to enhance the model's understanding of terms like \"stock price decline ,\" \n\"exchange rate increase,\" or \"interest rate hike.\" \n2) Handling Numerical Data: Financial data often involves numerical values such as stock \nindices, currency exchange rates, and interest rates. Employ numerical processing \n- 19 - \ncapabilities to ensure accurate handling of numerical information within the text. \n3) Complexity of Rules: Financial data follows intricate rules and patterns. Enhance the \nmodel's ability to comprehend and apply complex financial rules. For instance, \nunderstanding statements like \"KOSPI index at 2,300\" or \"USD exchange rate at 1,300 \nwon\". \n3.3.2. Applying Fine-Tuning Algorithms \nConsider fine-tuning algorithms that not only work well with general text but also address the \nspecific characteristics of financial data: \n1) LSTM (Long Short -Term Memory): Ideal for handling time-series data, LSTM retains \ninformation from the past to process the present. Given the time -sensitive nature of \nfinancial data, LSTM can be beneficial for predicting market situations based on historical \ndata. \n2) Attention Mechanism: Enables the model to focus on specific portions of input text, \nallowing it to learn intricate patterns in financial data effectively. Useful for emphasizing \nthe impact of specific events or news in financial predictions. \n3) Specialized Financial Algorithms: Algorithms tailored for financial tasks, such as \nstatistical methods for risk management or models like Black-Scholes for option valuation. \nIncorporating these algorithms helps the model adapt more effectively to specific financial tasks. \n3.3.3. Security and Regulatory Compliance \nGiven the sensitivity of financial data, ensure compliance with security and regulatory standards \nduring fine-tuning. At this stage, the dataset is selected considering data security and personal \ninformation protection. When undertaking an LLM fine -tuning in finance, you should consider \nthe following: \n1) Data Security: Implement encryption, access controls, and backup measures to secure \nfinancial news data. \n2) Personal Data Protection: Adhere to regulations regarding the collection, use, and \nprocessing of personal information within financial news data. \n3) Regulatory Compliance: Comply with financial regulations such as the Electronic \nFinancial Transactions Act or Foreign Exchange Transactions Act when collecting or \n- 20 - \nutilizing financial information. \nBy addressing these considerations, the fine -tuned LLM can offer high accuracy and utility for \nspecific financial tasks, empowering financial professionals in activities like prediction, research, \nand report generation with the use of LLM in a secure and compliant manner. \n \n3.4. Configuration of LLM for the Financial Specific Domain \nIn the field of finance, models specialized in natural language processing, such as BloombergGPT \nand FinGPT, have gained prominence. These models excel in financial tasks by learning financial \nterms and domain-specific features through fine-tuning on top of general language models. \n \n3.4.1. LLM Pre-trained for Finance from Scratch \nModels initially trained for the financial domain include Fin -T5, released in February 2023, and \nBloombergGPT, introduced in March. Fin-T5 is based on the 770M-T5 model and trained on an \nextensive dataset of 80 billion finance tokens. BloombergGPT, a GPT -based model specializing \nin financial language, is pre-trained using Bloomberg's financial data and news. The model, named \n50B-BLOOM, is initially based on the 363B Finance tokens and 345B public tokens, undergoing \nfine-tuning. Predominantly designed for ta sks like Named Entity Recognition (NER) and \nSentiment Analysis, this model, as reported by Shijie, W., et. al. (2023), demonstrates outstanding \nperformance in financial market trend prediction and news sentiment analysis. \n<Table 2> Evaluation Benchmarks of BloombergGPT. \nSuit Tasks What does it measure? \nPublic Financial Tasks \nBloomberg Financial Tasks \n5 \n12 \nPublic datasets in the financial domain \nNER and sentiment analysis tasks \nBig-bench Hard (Suzgun et al., 2022) \nKnowledge Assessments \nReading Comprehension \nLinguistic Tasks \n23 \n5 \n5 \n9 \nReasoning and general NLP tasks \nTesting closed-book information recall \nTesting open-book tasks \nNot directly user-facing NLP tasks \n \nAdditionally, models pre -trained on financial news data, such as FinBERT , utilize the BERT \narchitecture to further learn words, syntax, and meanings within financial contexts. Primarily \napplied to financial research and trading -related problems, FinBERT successfully extracts \n- 21 - \ndomain-specific information, contributing to its effective utilization in extracting insights from \nfinancial data. \nThese models showcase the effectiveness of training language models from scratch for the \nfinancial domain, providing superior performance in various financial tasks and applications. \n3.4.2. LLM Fine-Tuned for Financial Specific Domain \nModels fine -tuned specifically for the financial sector include FinGPT and Fin -LLaMA, \nintroduced in July 2023. FinGPT, based on OpenAI's GPT architecture, is tailored for finance by \nusing ChatGLM-6B as the base model. It undergoes lightweight fine-tuning with 50,000 samples, \nemploying LoRA technology to learn the intricacies of financial text. FinGPT demonstrates \noutstanding performance in applications such as financial research, predictive analysis, and \nautomated trading. \nFin-LLaMA, on the other hand, utilizes the LLaMA -33B as its base model and undergoes \ninstruction fine-tuning with 16,900 data samples. Known for its high performance, Fin -LLaMA \nexcels in various financial tasks. \nTypically, open -source LLMs like LLaMA2, Falcon, and BLOOM serve as base models, \nincorporating additional financial-specific information through pre -training or fine-tuning. Each \nmodel exhibits unique features, strengths, and weaknesses. FinBERT, for insta nce, excels in \nspecific tasks but may be sensitive to the quantity of data available. BloombergGPT leverages real \nfinancial data and a robust fine -tuning mechanism, making it suitable for practical applications. \nFinGPT, based on the GPT architecture, achie ves high performance through fine -tuning with \nfinance-specific datasets. \nThese LLM models in the financial domain are expected to provide even higher precision and \nefficiency in the future, offering substantial value to financial professionals. \n \n3.5. Evaluation Criteria and Metrics  \nEvaluation criteria and metrics are crucial factors for quantitatively and qualitatively assessing \nthe performance of financial-specialized LLMs. To assess the model's quantitative performance, \nvarious metrics, such as accuracy, precision, recall, and F1 score, are employed. These metrics \nprovide a quantitative measure of how effectively the model performs specific financial tasks. \n- 22 - \n<Table 3> Evaluation Criteria and Metrics for Financial-Specialized Language Models \nEvaluation \nAspect Metrics Criteria \nQuantitative \nPerformance \nSentence Generation \nAccuracy \nSentence Generation Accuracy measures the precision with which generated sentences align \nwith the correct answers. Given that one of the primary objectives of financial-specialized \nLLMs is to produce accurate and meaningful financial sentences, this metric evaluates the \naccuracy of the generated sentences. \nFinancial Prediction \nAccuracy \nFinancial Prediction Accuracy assesses the model's precision in predicting outcomes based \non given financial data. In the financial domain, where tasks such as stock price prediction or \nforecasting financial events are crucial, evaluating the model's accuracy in these predictions \nbecomes paramount. \nSentiment Analysis \nAccuracy \nSentiment Analysis Accuracy evaluates the precision of sentiment analysis results for \nfinancial news or research. It measures how effectively the model analyzes the sentiment of \nfinancial text, ensuring accurate classification of positive or negative sentiments. \nPrecision \nSentiment Analysis Accuracy evaluates the precision of sentiment analysis results for \nfinancial news or research. It measures how effectively the model analyzes the sentiment of \nfinancial text, ensuring accurate classification of positive or negative sentiments. \nRecall \nRecall computes the ratio of true positive predictions to the total instances that actually \nbelong to the positive class. Particularly important in the financial domain, recall measures \nhow well the model can detect positive instances among the actual positive samples, \nproviding insights into the model's ability to capture relevant information. \nF1 Score \nF1 Score, calculated as the harmonic mean of precision and recall, offers a balanced metric \nfor evaluating the model's performance. It serves as an indicator of the model's ability to \nmaintain a balance between precision and recall, providing a comprehensive assessment of \nits overall effectiveness. \nQualitative \nPerformance \nContext Understanding \nContext Understanding assesses how well the model comprehends specific contexts within \nthe financial domain. It evaluates the model's ability to grasp domain-specific vocabulary and \ncontext nuances in given financial sentences. \nDomain-specific \nTerminology Usage \nDomain-specific Terminology Usage measures the model's proficiency in correctly \nemploying specialized financial terms. The accurate utilization of financial terminology \nindicates how well the model has learned the nuances of the domain. \nText Coherence \nText Coherence evaluates the consistency of generated text by the model. Consistent text is \ncrucial in financial reports and predictive analyses, making it a vital criterion for assessing the \nmodel's quality. \nAdaptability to Unusual \nScenarios \nAdaptability to Unusual Scenarios evaluates how effectively the model responds to \nunexpected situations in the financial domain. Given the dynamic and unpredictable nature \nof financial markets, the model's adaptability is a crucial aspect of assessment. \nSubjective Evaluation by \nExperts \nSubjective Evaluation by Experts involves financial professionals assessing the model's \nresults and providing feedback. Leveraging the experience and knowledge of experts, this \nevaluation method gauges how practical and effective the model is in specific financial tasks. \nDomain-specific Evaluation \nMetrics \nDomain-specific Evaluation Metrics introduce metrics tailored to the important features of \nthe financial domain. By incorporating metrics specialized for financial data and tasks, this \nevaluation assesses how well the model fits domain-specific requirements. \nText Generation Evaluation \nText Generation Evaluation utilizes metrics such as BLEU Score and ROUGE Score to \nassess the model's text generation capabilities. This evaluates the model's ability to generate \ngrammatically appropriate and meaningful results, particularly in tasks like financial report \ngeneration and news summarization. \n- 23 - \nIn addition to quantitative evaluation, incorporating subjective assessments from domain experts \nis essential for a comprehensive evaluation of the model's performance. This involves considering \nhow well the model has learned financial domain knowledge and  how effectively it has been \napplied. Table 3 presents example evaluation criteria and metrics that can be utilized to assess \nlanguage models specialized for the financial domain across various aspects. \nBy comprehensively considering these quantitative and qualitative performance metrics, the \nmodel's suitability for the financial domain is assessed. The evaluation results are utilized for \nongoing improvement and optimization of the model. The evaluation c riteria and metrics aid in \nidentifying the strengths and weaknesses of the model, ultimately contributing to the assessment \nof its utility in the financial domain. \n \n4. Fine-tuning Implementation and Utilization Research \nIn this chapter, we discuss the implementation and application of the Fine-tuning method based \non the techniques introduced in Chapter 3. The utilization areas of specialized language models \n(LLM) in the financial domain are diverse. Through this discussion, we aim to confirm the \neffective application possibilities of LLM in the financial sector, presenting areas where LLM can \nenhance decision-making and operational efficiency in the financial domain. \n4.1. Implementation of LLM Fine-tuning \nIn this section, we present the key implemented code based on the Fine-tuning procedure \nillustrated in Figure 8. Python was used as the programming language for implementation, and \nfor tracking parameters such as model weights and biases, the MLOps tool WandB was employed, \nproviding a dashboard. The development infrastructure for t raining utilized Google Colab, \nallowing immediate application without additional installations for GPU and Python. \n4.1.1. Data Collection and Preprocessing \nFor specialized Fine-tuning in the financial domain, one can construct a proprietary dataset or \nutilize open datasets. Figure 9 illustrates preprocessed securities and financial terms data in a \nQuestion-Answer (QA) set format, prepared as a CSV file (e.g., “##Question: What is an Index?## \nAnswer: An Index measures the performance of a group of stocks serving as a benchmark.”). The \nfinancial dataset, named 'FinancialStockTerms_Eng' consisting of preprocessed QA pairs, is \nloaded as the dataset for Fine-tuning, as depicted in Figure 10.  \n- 24 - \n \n<Figure 9> Financial dataset preparation \n \n<Figure 10> Loading dataset for Fine-tuning \n4.1.2. Selection of Pretrained Language Model (PLM) \nIn this study, the selection of the pretrained language model (PLM) takes into consideration the \nmodel's availability for research and commercialization. To facilitate research and application in \nvarious fields, the Falcon 7B model, openly available as ope n source, was chosen and \nimplemented, as depicted in Figure 11. Quantumization information is stored using \nBitsAndBytesConfig to enable quantization and is utilized during model loading \n(AutoModelForCausalLM.from_pretrained). \n\n- 25 - \n \n< Figure 11> PLM Selection \n4.1.3. Hyperparameter Configuration and Fine-Tuning Training \nOnce the preparations for quantization are completed, hyperparameters are set as illustrated in \nFigure 12. For QLoRA-related configurations, LoraConfig is established, and the PEFT model is \nobtained, as demonstrated in Figure 12. \n \n< Figure 12> Hyperparameter Configuration \nThe final step involves the sequence of model training. Figure 13 showcases the code and results \nfor fine-tuning execution, which involves training the PLM with additional datasets. The resulting \nFine-Tuned Language Model (FLM) can be effectively utilized for domain-specific tasks. \n\n- 26 - \n \n< Figure 13> Fine-Tuning Training Execution \n4.1.4. FLM Verification \nTo test the generated FLM, the PEFT FLM is loaded, as depicted in Figure 14. This step ensures \nthat the model operates correctly. \n \n< Figure 14> PEFT Model Loading \n\n- 27 - \n \n< Figure 15> PEFT Model Testing \nWhen posing the question 'What is the Index?' to both the pretrained PLM and the PEFT fine -\ntuned PLM, differences in responses are observed. The pre -fine-tuned PLM generates a generic \nanswer unrelated to stocks (e.g., \"A list of all the pages in a website.\"), whereas the FLM, fine -\ntuned with the specialized securities and finance dataset through PEFT, produces an answer \nrelated to stocks (e.g., \"An index is a measure of the performance of a group of stocks  or the \noverall market.\") \nIn summary, we have explored the method of fine -tuning PLMs with domain -specific datasets, \nvalidated the results, and demonstrated how FLMs can be tailored for specific tasks. Depending \non the domain classification and the scale of business, fine-tuning various LLMs based on specific \nneeds can result in customized FLMs suitable for different applications. \n  \n4.2. Application Fields and Cases of Financial LLM \n4.2.1. Financial Prediction and Trading \nIn the realm of financial prediction and trading, leveraging LLM facilitates predicting market \ntrends through activities like stock price prediction and sentiment analysis of financial news. For \ninstance, models can analyze financial news, corporate reports, and market trends collectively to \nforecast future stock movements. Investors can then make more informed decisions based on these \npredictions. Sentiment analysis of financial news provides insights into the emotions of market \nparticipants. LLMs analyze sentiments in financial news, offering investors information on market \ntrends based on positive or negative news, enabling strategic trading decisions. A notable example \n\n- 28 - \nis Mirae Asset Securities, which has introduced a service utilizing ChatGPT to summarize stock \nmarket conditions (Korea Financial Times, 2023). \n4.2.2. Automated Financial Document Processing \nIn the domain of automated financial document processing, efficiency gains can be achieved \nthrough activities like contract analysis and assistance in financial report generation. Contract \nanalysis involves utilizing LLMs to automatically analyze and extract essential information from \nvarious contracts, enhancing efficiency and reducing errors in financial workflows. Additionally, \nLLMs can aid in financial report generation, summarizing information and assisting financial \nexperts in swiftly producing reports. Real-world examples include Samsung Life's automation of \ninsurance claims payment processes using AI -based Optical Character Recognition (OCR) \n(AITimes, 2023) and JP Morgan's 'COiN,' employing AI to analyze corporate loan contracts, \nclassify types, and extract key phrases, reducing analysis time and improving accuracy (Financial \nFocus, 2023). \n4.2.3. Financial Research and Information Extraction \nIn the realm of financial research and information extraction, LLMs prove valuable in extracting \nuseful information from financial datasets for research purposes or recommending financial \nproducts aligned with customer investment preferences. LLMs efficiently extract information \nnecessary for financial research by collecting and synthesizing financial data from various sources, \nenabling financial experts to access the latest information for more effective decision -making. \nExamples include Korea Investment & Securities' AI-based research service, 'AIR Listin g Index \nFund (ETF),' and DB Insurance's use of AI to analyze relational data, detect insurance fraud, and \nimprove accuracy (Korea Financial Times, 2023; Data Hunt, 2023). \n4.2.4. Customer Interaction and Service Enhancement \nIn the domain of customer interaction and service enhancement, automatic response systems and \npersonalized product recommendations contribute to increased customer satisfaction. Automatic \nresponse systems, powered by LLMs, efficiently address diverse custo mer inquiries, reducing \nresponse times and enhancing efficiency. LLMs can also analyze customer financial histories and \npreferences to recommend tailored financial products, increasing customer satisfaction and \nimproving service quality. Examples include K B Kookmin Card and KBpay's 'Event Q&AI' \nservice, providing marketing event information through natural language dialogue (AITimes, \n2023), Toss's 'Ask GPT' feature for conversational interactions in their app (AITimes, 2023), and \n- 29 - \nNongHyup Bank's 'ARMI AI,' utilizing AI chatbots for automating customer satisfaction surveys \nand extracting statistics and analysis results automatically (Financial Focus, 2023). \nThese applications demonstrate that integrating LLMs into the financial domain automates \nvarious tasks, supports decision -making, and can provide a competitive edge for financial \ninstitutions, enhancing adaptability in the rapidly changing financial market. \n \n5. Discussion and Conclusion \nThis study has presented the fine-tuning of Language Models (LLMs) specialized in the financial \ndomain and explored various applications. By considering the characteristics of financial data, \nfine-tuning was conducted to enhance the model's performance. Th e study delved into applying \nLLMs to diverse tasks in finance, such as financial prediction, automation, research, and customer \ninteraction. Notably, the step -by-step validation results were provided for the procedures of \nfinancial domain dataset selection , data preprocessing, pre -trained LLM model selection, \nhyperparameter tuning, fine -tuning execution environment setup, evaluation metrics for fine -\ntuning performance, and model generation. \nThe study highlighted considerations for fine -tuning in the financial domain, examining areas \nsuch as prediction accuracy improvement, workflow efficiency enhancement through automation, \nand the facilitation of research and customer service. In the context  of financial prediction, an \nenhanced prediction accuracy aids investors in making more precise decisions, while automation \ncontributes to efficiency gains, reducing processing times in tasks like contract analysis and \nfinancial report generation. Addition ally, LLMs can be instrumental in research, efficiently \nextracting information for research reports, thereby supporting decision -making and strategic \nplanning within financial institutions. Regarding customer interaction improvement, automated \nresponse systems and personalized product recommendations can enhance the quality of customer \nservice. \nHowever, the study focuses primarily on the methodology of fine-tuning model creation, leading \nto certain limitations. The dataset's limited diversity may result in the model being overly biased \ntoward specific domains, necessitating a more extensive and representative dataset. Additionally, \novercoming the model's generalization limitations may require further parameter tuning and \noptimization of the deep network structure. The absence of financial domain knowledge in the \n- 30 - \nmodel could be addressed by exploring more effective ways to convey domain -specific \nknowledge. \nFuture research directions could involve expanding the study using more diverse and \nrepresentative financial datasets to strengthen the model's learning. Alongside optimizing the \nmodel structure, reinforcing financial domain knowledge could further enhance  the model's \nperformance. Moreover, exploring the broader application of LLMs in various financial tasks such \nas bankruptcy prediction, investment portfolio optimization, credit scoring, and extending \nresearch to other domains like manufacturing and public services is essential. \nFinally, ethical considerations not covered in this study deserve attention. Research in the financial \nsector involving sensitive information should emphasize aspects such as personal data protection \nand fair decision-making to enhance the model's trustworthiness. \nIn conclusion, this study has explored the possibilities and limitations of applying LLMs to the \nfinancial domain, providing a foundation for specialized domain research. The significance and \nvalue lie in actively utilizing LLMs in financial services within enterprises, paving the way for \nfuture advancements in this specialized field.  \n \n \nReferences \n[1] AITimes. (2023, October 25). Samsung Life, \"Innovation in the Insurance Industry Document \nAutomation!\"... Achieving Document Automation with ' DocAI' AI OCR Solution on the \nUpstage without Human Intervention. \nhttps://www.aitimes.kr/news/articleView.html?idxno=29203 \n[2] AITimes. (2023, October 25). KB Kookmin Card Launches 'Event Q&AI' Beta Service for \nCustomer Experience Innovation Based on LLM. Link \n[3] Bommasani, R., et. al. (2022, July 12). On the Opportunities and Risks of Foundation Models. \nhttps://arxiv.org/pdf/2108.07258.pdf \n[4] Chowdhery, A., et., al. (2022). Palm: Scaling language modeling with pathways. CoRR, vol. \nabs/2204.02311 \n[5] Data Hunt. (2023, October 19). Insurance AI Artificial Intelligence, Applied Technologies, \nand Use Cases. https://www.thedatahunt.com/trend-insight/ai-in-insurance \n[6] Dettmers, T., et. al. (2023, May 23). QLoRA: Efficient Fine-tuning of Quantized LLMs, \nhttps://arxiv.org/pdf/2305.14314.pdf \n[7] Dilmegani, C. (2023, June 21). Large Language Models: Complete Guide in 2023. \nhttps://research.aimultiple.com/large-language-models/ \n- 31 - \n[8] Edward, H., et. al. (2021, October 16). LORA: LOW -RANKADAPTATION OF LARGE \nLANGUAGEMODELS, https://arxiv.org/pdf/2106.09685.pdf \n[9] Financial Focus. (2023, October 31). Birth and Evolution of ChatGPT, Generative AI, \nContributes to Increased Productivity in Banking Business. \nhttp://ffnews.co.kr/detail.php?number=4768&thread=28r38 \n[10]Jeong, C. S., & Jeong, J. H. (2020). A Study on the Method of Implementing an AI Chatbot \nto Respond to the POST COVID-19 Untact Era, Journal of Information Technology Services, \n19(4), 31–47. https://doi.org/10.9716/KITS.2020.19.4.031 \n[11]Jeong, C. S. (2023a). A Study on the RPA Interface Method for Hybrid AI Chatbot \nImplementation, KIPS Transactions on Software and Data Engineering , 12(1), 41 -50. \nhttps://doi.org/10.3745/KTSDE.2023.12.1.41 \n[12]Jeong, C. S. (2023b). A Case Study in Applying Hyperautomation Platform for E2E Business \nProcess Automation, Information Systems Review , 25(2), 31 -56. \nhttps://doi.org/10.14329/isr.2023.25.2.031 \n[13]Jeong, C. S. (2023c). A Study on the Service Integration of Traditional Chatbot and ChatGPT, \nJournal of Information Technology Applications & Management , 3(4), 11 -28. \nhttps://doi.org/10.21219/jitam.2023.30.4.001 \n[14]Jeong, C. S. (2023d). A Study on the Implementation of Generative AI Services Using an  \nEnterprise Data-Based LLM Application Architecture. Advances in Artificial Intelligence and \nMachine Learning, 3(4). 1588-1618. https://dx.doi.org/10.54364/AAIML.2023.1191 \n[15]Jeong, C. S. (2023e). Generative AI service implementation using LLM application \narchitecture: based on RAG model and LangChain framework, Journal of Intelligence and \nInformation Systems, 29(4), 129-164. https://dx.doi.org/10.13088/jiis.2023.29.4.129 \n[16]Jeong, J. H. and Jeong, C. S. (2022). Ethical Issues with Artificial Intelligence (A Case Study \non AI Chatbot & Self -Driving Car), International Journal of Scientific & Engineering \nResearch, 13(1). 468–471. \n[17]Korea Financial Times. (2023, May 2). Securities Industry Riding the AI Wave with \nChatGPT... \"Steady in Research and Investment.\" \nhttps://www.fntimes.com/html/view.php?ud=20230429032803830dd55077bc2_18 \n[18]Mayank, S. (2023, June 30). Generative AI: Empowering Innovation with its Astonishing \nCapabilities. https://shurutech.com/innovating-with-generative-ai/ \n[19]Nijkamp, E., et., al. (2022). Codegen: An  open large language model for code with mtulti -\nturn program synthesis. arXiv preprint arXiv:2203.13474 \n[20]Raschka, S. (2023, May 20). Fine-tuning LLMs Efficiently with Adapters. \nhttps://magazine.sebastianraschka.com/p/Fine-tuning-llms-with-adapters \n[21]Shijie, W., et. al. (2023, May 9). BloombergGPT : A Large Language Model for Finance, \nhttps://arxiv.org/pdf/2303.17564.pdf \n[22]Taylor, R., et., al. (2022). On the Opportunities and Risks of Foundation Models. CoRR, vol. \nabs/2211.09085 \n[23]Zhao, W. X., et al. (2023, June 29). A Survey of Large Language Models. \nhttps://arxiv.org/pdf/2303.18223.pdf \n ",
  "topic": "Financial services",
  "concepts": [
    {
      "name": "Financial services",
      "score": 0.49540627002716064
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.49403172731399536
    },
    {
      "name": "Stock market prediction",
      "score": 0.42302224040031433
    },
    {
      "name": "Computer science",
      "score": 0.40060073137283325
    },
    {
      "name": "Finance",
      "score": 0.3926329016685486
    },
    {
      "name": "Stock market",
      "score": 0.3310082256793976
    },
    {
      "name": "Business",
      "score": 0.26422226428985596
    },
    {
      "name": "Geography",
      "score": 0.0841403603553772
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Context (archaeology)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 23
}