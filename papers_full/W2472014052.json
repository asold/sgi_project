{
    "title": "Incorporating Side Information into Recurrent Neural Network Language Models",
    "url": "https://openalex.org/W2472014052",
    "year": 2016,
    "authors": [
        {
            "id": "https://openalex.org/A2605962998",
            "name": "Cong Duy Vu Hoang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2188741563",
            "name": "Trevor Cohn",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": "https://openalex.org/A1432492132",
            "name": "Gholamreza Haffari",
            "affiliations": [
                "Monash University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W4252684946",
        "https://openalex.org/W2142074148",
        "https://openalex.org/W2474824677",
        "https://openalex.org/W2091812280",
        "https://openalex.org/W2116064496",
        "https://openalex.org/W1753482797",
        "https://openalex.org/W1916589227",
        "https://openalex.org/W2964222437",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2964335273",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2171361956",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W1562955078",
        "https://openalex.org/W2133564696"
    ],
    "abstract": "Cong Duy Vu Hoang, Trevor Cohn, Gholamreza Haffari. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",
    "full_text": "Proceedings of NAACL-HLT 2016, pages 1250–1255,\nSan Diego, California, June 12-17, 2016.c⃝2016 Association for Computational Linguistics\nIncorporating Side Information into Recurrent Neural Network Language\nModels\nCong Duy Vu Hoang\nUniversity of Melbourne\nMelbourne, VIC, Australia\nvhoang2@student.unimelb.edu.au\nGholamreza Haffari\nMonash University\nClayton, VIC, Australia\ngholamreza.haffari@monash.edu\nTrevor Cohn\nUniversity of Melbourne\nMelbourne, VIC, Australia\nt.cohn@unimelb.edu.au\nAbstract\nRecurrent neural network language models\n(RNNLM) have recently demonstrated vast\npotential in modelling long-term dependen-\ncies for NLP problems, ranging from speech\nrecognition to machine translation. In this\nwork, we propose methods for conditioning\nRNNLMs on external side information, e.g.,\nmetadata such as keywords, description, doc-\nument title or topic headline. Our experiments\nshow consistent improvements of RNNLMs\nusing side information over the baselines for\ntwo different datasets and genres in two lan-\nguages. Interestingly, we found that side in-\nformation in a foreign language can be highly\nbeneﬁcial in modelling texts in another lan-\nguage, serving as a form of cross-lingual lan-\nguage modelling.\n1 Introduction\nNeural network approaches to language modelling\n(LM) have made remarkable performance gains over\ntraditional count-based ngram LMs (Bengio et al.,\n2003; Mnih and Hinton, 2007; Mikolov et al., 2011).\nThey offer several desirable characteristics, includ-\ning the capacity to generalise over large vocabular-\nies through the use of vector space representation,\nand – for recurrent models (Mikolov et al., 2011)\n– the ability to encode long distance dependencies\nthat are impossible to include with a limited context\nwindows used in conventional ngram LMs. These\nearly papers have spawned a cottage industry in neu-\nral LM based applications, where text generation\nis a key component, including conditional language\nmodels for image captioning (Kiros et al., 2014;\nVinyals et al., 2015) and neural machine translation\n(Kalchbrenner and Blunsom, 2013; Sutskever et al.,\n2014; Bahdanau et al., 2015).\nInspired by these works for conditioning LMs on\ncomplex side information, such as images and for-\neign text, in this paper we investigate the possibility\nof improving LMs in a more traditional setting, that\nis when applied directly to text documents. Typi-\ncally corpora include rich side information, such as\ndocument titles, authorship, time stamp, keywords\nand so on, although this information is usually dis-\ncarded when applying statistical models. However,\nthis information can be highly informative, for in-\nstance, keywords, titles or descriptions, often in-\nclude central topics which will be helpful in mod-\nelling or understanding the document text. We pro-\npose mechanisms for encoding this side informa-\ntion into a vector space representation, and means\nof incorporating it into the generating process in a\nRNNLM framework. Evaluating on two corpora and\ntwo different languages, we show consistently sig-\nniﬁcant perplexity reductions over the state-of-the-\nart RNNLM models.\nThe contributions of this paper are as follows:\n1. We propose a framework for encoding struc-\ntured and unstructured side information, and its\nincorporation into a RNNLM.\n2. We introduce a new corpus, the RIE corpus,\nbased on the Europarl web archive, with rich\nannotations of several types of meta-data.\n3. We provide empirical analysis showing consis-\ntent improvements from using side information\nacross two datasets in two languages.\n1250\n2 Problem Formulation & Model\nWe ﬁrst review RNNLM architecture (Mikolov et\nal., 2011) before describing our extension in §2.2.\n2.1 RNNLM Architecture\nThe standard RNNLM consists of 3 main layers: an\ninput layer where each input word has its embedding\nvia one-hot vector coding; a hidden layer consisting\nof recurrent units where a state is conditioned recur-\nsively on past states; and an output layer where a\ntarget word will be predicted. RNNLM has an ad-\nvantage over conventional n-gram language model\nin modelling long distance dependencies effectively.\nIn general, an RNN operates from left-to-right\nover the input word sequence; i.e.,\nht = RU (xt, ht−1)\n= f\n(\nW (hh)ht−1 + W (ih)xt + b(h)\n)\nxt+1 ∼ softmax\n(\nW (ho)ht + b(o)\n)\n;\nwhere f(.) is a non-linear function, e.g., tanh, ap-\nplied element-wise to its vector input; ht is the cur-\nrent RNN hidden state at time-step t; and matrices\nW and vectors b are model parameters. The model\nis trained using gradient-based methods to optimise\na (regularised) training objective, e.g. the likelihood\nfunction. In principle, a recurrent unit (RU) can be\nemployed using different variants of recurrent struc-\ntures such as: Long Short Term Memory (LSTM)\n(Hochreiter and Schmidhuber, 1997), Gated Recur-\nrent Unit (GRU) (Cho et al., 2014), or recently\ndeeper structures, e.g. Depth Gated Long Short\nTerm Memory (DGLSTM) – a stack of LSTMs with\nextra connections between memory cells in deep\nlayers (Yao et al., 2015). It can be regarded as\nbeing a generalisation of LSTM recurrence to both\ntime and depth. Such deep recurrent structure may\ncapture long distance patterns at their most general.\nEmpirically, we found that RNNLM with DGLSTM\nstructure appears to be best performer across our\ndatasets, and therefore is used predominantly in our\nexperiments.\n2.2 Incorporating Side Information\nNowadays, many corpora are archived with side in-\nformation or contextual meta-data. In this work, we\nhtht-1\nxt\nxt+1\ne\nhtht-1\nxt\nxt+1\ne\na) b)\nFigure 1:Integration methods for auxiliary information, e: a)\nas input to the RNN, or b) as part of the output softmax layer.\nargue that such information can be useful for lan-\nguage modelling (and presumably other NLP tasks).\nBy providing this auxiliary information directly to\nthe RNNLM, we stand to boost language modelling\nperformance.\nThe ﬁrst question in using side information is how\nto encode these unstructured inputs, y, into a vector\nrepresentation, denoted e. We discuss several meth-\nods for encoding the auxiliary vector:\nBOW additive bag of words, e = ∑\nt yt, and\naverage the average embedding vector,\ne = 1\nT\n∑\nt yt, both inspired by (Hermann\nand Blunsom, 2014a);\nbigram convolution with sum-pooling,\ne = ∑\nt tanh (yt−1 + yt) (Hermann and\nBlunsom, 2014b); and\nRNN a recurrent neural network over the word se-\nquence (Sutskever et al., 2014), using the ﬁnal\nhidden state(s) as e.\nFrom the above methods, we found that BOW\nworked consistently well, outperforming the other\napproaches, and moreover lead to a simpler model\nwith faster training. For this reason we report only\nresults for the BOW encoding. Note that when using\nmultiple auxiliary inputs, we use a weighted combi-\nnation, e = ∑\ni W (ai)e(i).\nThe next step is the integration of e into the\nRNNLM. We consider two integration methods: as\ninput to the hidden state (denoted input), and con-\nnected to the output softmax layer ( output), as\nshown in Figure 1 a and b, respectively. In both\ncases, we compare experimentally the following in-\ntegration strategies:\nadd adding the vectors together, e.g., using xt +\ne as the input to the RNN, such that\n1251\nht = RU (xt + e, ht−1);\nstack concatenating the vectors, e.g., using[\nx⊤\nt e⊤]⊤ for generating the RNN hidden\nstate, such that ht = RU\n([ xt\ne\n]\n, ht−1\n)\n;\nand\nmlp feeding both vectors into an extra perceptron\nwith single hidden layer, using a tanh non-\nlinearity and projecting the output to the re-\nquired dimensionality; i.e.,\nh′\nt = tanh\n(\nW (hh′)ht + W (he)e + b(h′)\n)\nxt+1 ∼ softmax\n(\nW (ho)h′\nt + b(o)\n)\n.\nNote that add requires the vectors to be the same di-\nmensionality, while the other two methods do not.\nThe stack method can be quite costly, given that it\nincreases the size of several matrices, either in the\nrecurrent unit (for input) or the output mapping for\nword generation. This is a problem in the latter case:\ngiven the large size of the vocabulary, the matrix\nW (ho) is already very large and making it larger\n(doubling the size, to become W (h′o)) has a size-\nable effect on training time (and presumably also\npropensity to over-ﬁt). The output+stack method\ndoes however have a compelling interpretation as a\njointly trained product model between a RNNLM\nand a unigram model conditioned on the side in-\nformation, where both models are formulated as\nsoftmax classiﬁers. Considered as a product model\n(Hinton, 2002; Pascanu et al., 2013), the two com-\nponents can concentrate on different aspects of the\nproblem where the other model is not conﬁdent, and\nallowed each model the ability to ‘veto’ certain out-\nputs, by assigning them a low probability.\n3 Experiments\nDatasets. We conducted our experiments on two\ndatasets with different genres in two languages. As\nthe ﬁrst dataset, we use the IWSLT2014 MT track\non TED Talks1 due to its self-contained rich auxil-\niary information, including: title, description, key-\nwords, and author related information. We chose\nthe English-French pair for our experiments 2 . The\nstatistics of the training set is shown in Table 1. We\n1https://wit3.fbk.eu/ (IWSLT’14 MT Track)\n2Our method can be also applied to other language pairs.\ntokens (M) types (K) docs sents (K)\nTED-en 4.0 18.3 1414 179\nTED-fr 4.3 22.6 1414 179\nRIE-en 13.7 15.0 200 460\nRIE-fr 14.9 19.4 200 460\nTable 1:Statistics of the training sets, showing in each cell the\nnumber of word tokens, types, documents (talks or plenaries),\nand sentences. Note that “types” here refers to word frequency\nthresholded at 5 and 15 for TED Talks and RIE datasets, respec-\ntively.\nused dev2010 (7 talks/817 sentences) for early stop-\nping of training neural network models. For evalu-\nation, we used different testing sets over years, in-\ncluding tst2010 (10/1587), tst2011 (7/768), tst2012\n(10/1083).\nAs the second dataset, we crawled the entire Euro-\npean Parliament3 website, focusing on plenary ses-\nsions. Such sessions contain useful structural in-\nformation, namely multilingual texts divided into\nspeaker sessions and topics. We believe that those\ntexts are interesting and challenging for language\nmodelling tasks. Our dataset contains 724 plenary\nsessions over 12.5 years until June 2011 with mul-\ntilingual texts in 22 languages 4. We refer to this\ndataset by RIE 5 (Rich Information Europarl). We\nrandomly select 200/5/30 plenary sessions as the\ntraining/development/test sets, respectively. We be-\nlieve that the new data including side information\npose another challenge for language modelling. Fur-\nthermore, the sizes of our working datasets are an or-\nder of magnitude larger than the standard Penn Tree-\nbank set which is often used for evaluating neural\nlanguage models.\nSet-up and Baselines. We have used cnn6 to im-\nplement our models. We use the same conﬁgura-\ntions for all neural models: 512 input embedding\nand hidden layer dimensions, 2 hidden layers, and\nvocabulary sizes as given in Table 1. We used\nthe same vocabulary for the auxiliary and modelled\ntext. We trained a conventional 5 −gram language\nmodel using modiﬁed Kneser-Ney smoothing, with\nthe KenLM toolkit (Heaﬁeld, 2011). We used the\n3http://www.europarl.europa.eu/\n4We ignored the period from June 2011 onwards, as from\nthis date the EU stopped creating manual human translations.\n5This dataset will be released upon publication.\n6https://github.com/clab/cnn/\n1252\nMethod test2010 test2011 test2012\n5-gram LM 79.9 77.4 89.9\nRNNLM 65.8 63.9 73.0\nLSTM 54.1 52.2 58.4\nDGLSTM 53.1 52.1 58.8\ninput+add+k 52.9 52.1 57.5\ninput+mlp+k 53.3 51.5 57.3\ninput+stack+k 53.7 51.9 58.1\noutput+mlp+k 51.7 50.6 55.8\noutput+mlp+t 52.3 53.5 58.3\noutput+mlp+d 52.0 49.8 56.3\noutput+mlp+k+t 51.4 51.1 56.8\noutput+mlp+k+d 51.2 49.7 55.1\noutput+mlp+t+d 52.6 51.5 57.2\noutput+mlp+k+t+d 51.1 50.6 56.3\nTable 2: Perplexityscores based on the English part of TED\ntalks dataset in IWSLT14 MT. +k, +t, +d: with keywords, title,\nand description as auxiliary side information respectively.bold:\nStatistically signiﬁcant better than the best baseline.\nWilcoxon signed-rank test (Wilcoxon, 1945) to mea-\nsure the statistical signiﬁcance ( p < 0.05) on dif-\nferences between sentence-level perplexity scores\nof improved models compared to the best base-\nline. Throughout our experiments, punctuation, stop\nwords and sentence markers (〈s〉, 〈/s〉, 〈unk〉) are ﬁl-\ntered out in all auxiliary inputs. We observed that\nthis ﬁltering was required for BOW to work rea-\nsonably well. For each model, the best perplexity\nscore on development set is used for early stopping\nof training models, which was obtained after 2-5 and\n2-3 epochs on TED Talks and RIE datasets, respec-\ntively.\nResults & Analysis. The perplexity results on\nTED Talks dataset are presented in Table 2 and\n3. RNNLM variants consistently achieve substan-\ntially better perplexities compared to the conven-\ntional 5−gram language model baseline.7 Of the ba-\nsic RNNLM models (middle), the DGLSTM works\nconsistently better than both the standard RNN and\nthe LSTM. This may be due to better interactions of\nmemory cells in hidden layers. Since the DGLSTM\noutperformed others8, we used it for all subsequent\nexperiments. For TED Talks dataset, there are three\n7For fair comparison, when computing the perplexity with\nthe 5-gram LM, we exclude all test words marked as〈unk〉(i.e.,\nwith low counts or OOVs) from consideration.\n8This concurs with the ﬁnding in (Yao et al., 2015), who\nshowed that DGLSTM produced the state-of-the-art results over\nPenn Treebank dataset.\nMethod test2010 test2011 test2012\n5-gram LM 65.1 60.3 64.8\nLSTM 45.0 42.5 44.0\nDGLSTM 44.0 41.9 43.0\noutput+mlp+t 42.1 40.6 42.5\noutput+mlp+d 40.9 38.9 40.3\noutput+mlp+t+d 41.7 39.8 42.8\noutput+mlp+k 40.8 38.3 39.7\noutput+mlp+d+k 40.2 38.3 39.4\nTable 3: Perplexityscores based on the French part of TED\ntalks dataset in IWSLT14 MT. Note that +k means with key-\nwords in English.\nkinds of side information, including keywords, ti-\ntle, description. We attempted to inject those into\ndifferent RNNLM layers, resulting in model vari-\nants as shown in Table 2. First, we chose “key-\nwords” (+k) information as an anchor to ﬁgure out\nwhich incorporation method works well. Comparing\ninput+add+k, input+mlp+k and input+stack+k, the\nlargest decrease is obtained by output+mlp+k con-\nsistently across all test sets (and development sets,\nnot shown here). We further evaluated the addition\nof other side information (e.g., “description” (+d),\n“title” (+t)), ﬁnding that +d has similar effect as +k\nwhereas +t has a mixed effect, being detrimental for\none test set (test2011). We suspect that it is due to\noften-times short sentences of titles in that test, af-\nter our ﬁltering step, leading to a shortage of useful\ninformation fed into neural network learning. Inter-\nestingly, the best performance is obtained when in-\ncorporating both +k and +d, showing that there is\ncomplementary information in the two auxiliary in-\nputs. Further, we also achieved the similar results\nin the counterpart of English part (in French) using\noutput+mlp with both +t and +d as shown in Ta-\nble 3. In French data, no “keywords” information is\navailable. For this reason, we run additional exper-\niments by injecting English keywords as side infor-\nmation into neural models of French. Interestingly,\nwe found that “keywords” side information in En-\nglish effectively improves the modelling of French\ntexts as shown in Table 3, serving as a new form of\ncross-lingual language modelling.\nWe further achieved similar results by incorpo-\nrating the topic headline in the RIE dataset. The\nconsistently-improved results (in Table 4) demon-\nstrate the robustness of the output+mlp approach.\n1253\nMethod test (en) test (fr)\n5-gram LM 55.7 38.5\nLSTM 40.3 28.5\nDGLSTM 36.4 25.4\noutput+mlp+h 33.3 24.0\nTable 4: Perplexityscores based on the sampled RIE dataset.\n+h: topic headline.\n4 Conclusion\nWe have proposed an effective approach to boost the\nperformance of RNNLM using auxiliary side infor-\nmation (e.g. keywords, title, description, topic head-\nline) of a textual utterance. We provided an empir-\nical analysis of various ways of injecting such in-\nformation into a distributed representation, which\nis then incorporated into either the input, hidden,\nor output layer of RNNLM architecture. Our ex-\nperimental results reveal consistent improvements\nare achieved over strong baselines for different\ndatasets and genres in two languages. Our future\nwork will investigate the model performance on a\nclosely-related task, i.e., neural machine translation\n(Sutskever et al., 2014; Bahdanau et al., 2015). Fur-\nthermore, we will explore learning methods to com-\nbine utterances with and without the auxiliary side\ninformation.\nAcknowledgements\nThe authors would like to thank the reviewers for\nvaluable comments and feedbacks. Cong Duy Vu\nHoang was supported by research scholarships from\nthe University of Melbourne, Australia. Dr Trevor\nCohn was supported by the ARC (Future Fellow-\nship).\nReferences\nD. Bahdanau, K. Cho, and Y . Bengio. 2015. Neural\nMachine Translation by Jointly Learning to Align and\nTranslate. In Proceedings of International Conference\non Learning Representations (ICLR 2015), September.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A Neural Probabilistic Lan-\nguage Model. The Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the Proper-\nties of Neural Machine Translation: Encoder–Decoder\nApproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statisti-\ncal Translation, pages 103–111, Doha, Qatar, October.\nAssociation for Computational Linguistics.\nKenneth Heaﬁeld. 2011. KenLM: Faster and Smaller\nLanguage Model Queries. In Proceedings of the\nEMNLP 2011 Sixth Workshop on Statistical Machine\nTranslation, pages 187–197, Edinburgh, Scotland,\nUnited Kingdom, July.\nK. M. Hermann and P. Blunsom. 2014a. Multilingual\nDistributed Representations without Word Alignment.\nIn Proceedings of International Conference on Learn-\ning Representations (ICLR 2014), December.\nKarl Moritz Hermann and Phil Blunsom. 2014b. Multi-\nlingual Models for Compositional Distributed Seman-\ntics. In Proceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 58–68, Baltimore, Mary-\nland, June. Association for Computational Linguistics.\nGeoffrey E Hinton. 2002. Training Products of Experts\nby Minimizing Contrastive Divergence. Neural com-\nputation, 14(8):1771–1800.\nSepp Hochreiter and Jurgen Schmidhuber. 1997. Long\nShort-Term Memory. Neural Comput., 9(8):1735–\n1780, November.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\nContinuous Translation Models. In Proceedings of\nEmpirical Methods in Natural Language Processing\n(EMNLP 2013).\nRyan Kiros, Ruslan Salakhutdinov, and Rich Zemel.\n2014. Multimodal Neural Language Models. In Pro-\nceedings of the 31st International Conference on Ma-\nchine Learning (ICML-14), pages 595–603.\nT. Mikolov, S. Kombrink, A. Deoras, and J. H. Burget,\nL.and Cernocky. 2011. RNNLM - Recurrent Neural\nNetwork Language Modeling Toolkit. In 2011 IEEE\nWorkshop on Automatic Speech Recognition & Under-\nstanding (ASRU). IEEE Automatic Speech Recogni-\ntion and Understanding Workshop, December.\nAndriy Mnih and Geoffrey Hinton. 2007. Three New\nGraphical Models for Statistical Language Modelling.\nIn Proceedings of the 24th International Conference\non Machine Learning, pages 641–648.\nR. Pascanu, C. Gulcehre, K. Cho, and Y . Bengio. 2013.\nHow to Construct Deep Recurrent Neural Networks.\nArXiv e-prints, December.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Se-\nquence to Sequence Learning with Neural Networks.\nIn Advances in Neural Information Processing Sys-\ntems (NIPS 2014), pages 3104–3112.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. 2015. Show and Tell: A Neural Image\nCaption Generator. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June.\n1254\nFrank Wilcoxon. 1945. Individual Comparisons by\nRanking Methods. Biometrics Bulletin, 1 (6):80–83,\nDec.\nK. Yao, T. Cohn, K. Vylomova, K. Duh, and C. Dyer.\n2015. Depth-Gated LSTM. ArXiv e-prints, August.\n1255"
}