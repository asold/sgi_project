{
  "title": "Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment",
  "url": "https://openalex.org/W4285602376",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5103525627",
      "name": "Takeshi Kojima",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A5090592819",
      "name": "Yutaka Matsuo",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A5063925941",
      "name": "Yusuke Iwasawa",
      "affiliations": [
        "The University of Tokyo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3005560345",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W4288404646",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2991405316",
    "https://openalex.org/W2963217615",
    "https://openalex.org/W4287126759",
    "https://openalex.org/W3173494152",
    "https://openalex.org/W2335728318",
    "https://openalex.org/W2803297029",
    "https://openalex.org/W3034526587",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3214575451",
    "https://openalex.org/W4287116734",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2994088087",
    "https://openalex.org/W3116074996",
    "https://openalex.org/W4287755125",
    "https://openalex.org/W3214432945",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2947707615",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3040572086",
    "https://openalex.org/W3202202633",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3036825277"
  ],
  "abstract": "Vision Transformer (ViT) is becoming more popular in image processing. Specifically, we investigate the effectiveness of test-time adaptation (TTA) on ViT, a technique that has emerged to correct its prediction during test-time by itself. First, we benchmark various test-time adaptation approaches on ViT-B16 and ViT-L16. It is shown that the TTA is effective on ViT and the prior-convention (sensibly selecting modulation parameters) is not necessary when using proper loss function. Based on the observation, we propose a new test-time adaptation method called class-conditional feature alignment (CFA), which minimizes both the class-conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain adaptation (digits datasets and ImageNet-Sketch) show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8% top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation baseline 44.0%. This is a state-of-the-art result among TTA methods that do not need to alter training phase.",
  "full_text": "Robustifying Vision Transformer without Retraining from Scratch\nby Test-Time Class-Conditional Feature Alignment\nTakeshi Kojima∗ , Yutaka Matsuoand Yusuke Iwasawa\nThe University of Tokyo, Japan\n{t.kojima,matsuo,iwasawa}@weblab.t.u-tokyo.ac.jp\nAbstract\nVision Transformer (ViT) is becoming more pop-\nular in image processing. Specifically, we investi-\ngate the effectiveness of test-time adaptation (TTA)\non ViT, a technique that has emerged to correct\nits prediction during test-time by itself. First, we\nbenchmark various test-time adaptation approaches\non ViT-B16 and ViT-L16. It is shown that the\nTTA is effective on ViT and the prior-convention\n(sensibly selecting modulation parameters) is not\nnecessary when using proper loss function. Based\non the observation, we propose a new test-time\nadaptation method called class-conditional feature\nalignment (CFA), which minimizes both the class-\nconditional distribution differences and the whole\ndistribution differences of the hidden representa-\ntion between the source and target in an online\nmanner. Experiments of image classification tasks\non common corruption (CIFAR-10-C, CIFAR-100-\nC, and ImageNet-C) and domain adaptation (digits\ndatasets and ImageNet-Sketch) show that CFA sta-\nbly outperforms the existing baselines on various\ndatasets. We also verify that CFA is model agnostic\nby experimenting on ResNet, MLP-Mixer, and sev-\neral ViT variants (ViT-AugReg, DeiT, and BeiT).\nUsing BeiT backbone, CFA achieves 19.8% top-1\nerror rate on ImageNet-C, outperforming the exist-\ning test-time adaptation baseline 44.0%. This is a\nstate-of-the-art result among TTA methods that do\nnot need to alter training phase.1\n1 Introduction\nInspired by the success in natural language processing, Trans-\nformer [Vaswani et al., 2017 ] is becoming more and more\npopular in various image processing tasks, including image\nrecognition [Dosovitskiy et al., 2020; Touvron et al., 2021],\nobject detection [Carion et al., 2020 ], and video process-\ning [Zhou et al., 2018; Zeng et al., 2020 ]. Notably, [Doso-\nvitskiy et al., 2020 ] proposed Vision Transformer (ViT),\n∗Contact Author\n1Full version (with Appendix) and code are publicly available at\nhttps://github.com/kojima-takeshi188/CFA.\nwhich adapts Transformer architecture to image classifica-\ntion tasks and shows that it achieves comparable or superior\nperformance to that of the conventional convolutional neu-\nral networks (CNNs). Follow-up research also shows that\nViT is more robust to the common corruptions and perturba-\ntions than convolution-based models (e.g., ResNet)[Paul and\nChen, 2021; Morrison et al., 2021 ], which is an important\nproperty for safety-critical applications.\nThis study seeks to answer the following question: can\nwe improve the robustness of ViT without retraining it from\nscratch? Most prior works focused on how to robustify the\nmodels during training. For example, [Hendrycks et al.,\n2019; Hendrycks et al., 2020] demonstrated that several data\naugmentation improves robustness of convolutional neural\nnetworks (CNN). Similarly, [Chen et al., 2022 ] shows that\na sharpness-aware optimizer improves the robustness of ViT.\nUnfortunately, such approaches require retraining the models\nfrom scratch, which entails a massive computational burden\nand training time for large models (such as ViT). Moreover,\nsometimes dataset for pre-training is not publicly available,\nwhich makes it impossible to retrain the models.\nThis study investigates the effectiveness of test-time adap-\ntation (TTA) to robustify ViT. TTA is a recently emerged ap-\nproach for improving the robustness of models without re-\ntraining them from scratch and accessing to training dataset\n[Schneider et al., 2020; Nado et al., 2020; Wanget al., 2020].\nInstead, it corrects the model’s prediction for test data by\nmodulating its parameters during test time. For example,\n[Wang et al., 2020] proposed Tent, which modulates the pa-\nrameters of batch normalization (BN) by minimizing predic-\ntion entropy. It was shown that Tent can significantly improve\nthe robustness of ResNet. TTA has two major advantages\nover usual training-time techniques. First, it does not alter\nthe training phase and thus does not need to repeat the com-\nputationally heavy training phase. Second, it does not require\naccessing to the source data during adaptation, which is im-\npossible in the case of large pre-trained models.\nConceptually speaking, TTA can be applied to arbitrary\nnetwork architectures. However, naively modulating model\nparameters during test-time may cause a catastrophic fail-\nure as discussed in [Wang et al., 2020 ]. To avoid the issue,\nprior works often limited the modulation parameters, which\nresulted in architecture constraints. For example, [Schneider\net al., 2020; Wang et al., 2020 ] modulated statistics and/or\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1009\naffine transformation in batch normalization (BN) layer, but\nthe BN-based method cannot be applied to some modern\nmodels such as ViT since they do not have BN.\nThis study contributes to addressing this research ques-\ntion by following two means. First, we benchmark various\ntest-time adaptation methods on ViT using several robustness\nbenchmark tasks (CIFAR10-C, CIFAR100-C, ImageNet-C,\nand several domain adaptation tasks). We also design mod-\nulation parameters potentially suitable for ViT-based archi-\ntectures. The tested methods include entropy minimization\n[Wang et al., 2020], pseudo-classifier [Iwasawa and Matsuo,\n2021], pseudo-label [Lee and others, 2013 ], diversity regu-\nlarization [Liang et al., 2020], and feature alignments [Liu et\nal., 2021]. Regarding modulation parameters, we sweep over\nthe following four candidates: layer normalization [Ba et al.,\n2016], CLS token, feature extractor [Liang et al., 2020], and\nentire parameters of a model. The results indicate that the\nprior convention in test-time adaptation (i.e., limiting mod-\nulation parameters) is not necessary when using proper loss\nfunction, while it is necessary for pure entropy minimization\nbased approach. This observation is important for applying\nTTA to arbitrary network architectures.\nWe then propose a new loss function: test-time class-\nconditional feature alignment (CFA). Our approach can be\ncategorized into feature alignment approach as with [Liu et\nal., 2021], which minimize the gap of the statistics between\ntraining and test domain. It is worth noting that the feature\nalignment approach for test-time adaptation (e.g., our ap-\nproach and [Liu et al., 2021 ]) assumes that one can access\nto the statistics on the source dataset during the test phase\nbut does not need to access to the source dataset itself and to\nrepeat the computationally heavy training 2. Therefore, this\napproach can be used without source dataset during adapta-\ntion. We show that such complementary information about\nthe source data distribution can stabilize the training without\nselecting the modulation parameters. In addition, we extend\nthe feature alignment approach [Liu et al., 2021] by the fol-\nlowing two means. First, CFA aligns class-conditional statis-\ntics as well as the statistics of overall distribution. Second,\nwe calculate the statistics after properly normalizing the hid-\nden representations. Despite the simplicity, these techniques\nsignificantly boost the performance of test-time adaptation.\nIn summary, our main contributions are as follows.\n• This is the first study that verifies the effectiveness of\ntest-time adaptation methods on ViT. By benchmarking\nseveral test-time adaptation approaches under common\ncorruptions and domain adaptation tasks, we have vali-\ndated that the robustness of ViT model is improved dur-\ning test time without retraining the model from scratch.\n• We introduce a new test-time adaptation method (CFA).\nThroughout the experiment, CFA achieves better results\n2Test-time adaptation generally assumes that the model would be\ndistributed without source data due to bandwidth, privacy, or profit\nreasons [Wang et al., 2020 ]. We argue that the statistics of source\ndata would be distributed even in such a situation since it could\ndrastically compress data size and eliminate sensitive information.\nIn fact, some layers often used in typical neural networks contain\nstatistics of source data (e.g., batch normalization)\nthan existing baselines on multiple datasets. In addition,\nCFA is robust to hyperparameter tuning, which is impor-\ntant in practically setting up test-time adaptation.\n• We show that CFA consistently improves the robustness\non a wide variety of backbone networks during test time.\nIn particular, we achieve the state-of-the-art results of\ntest-time adaptation on ImageNet-C with a 19.8 % top-1\nerror rate when using BeiT-L16 as a backbone network.\n2 Related Work\n2.1 Vision Transformer (ViT)\nTransformer [Vaswani et al., 2017 ], first proposed in natu-\nral language processing (NLP) field, also achieves great per-\nformance in image processing as Vision Transformer (ViT)\n[Dosovitskiy et al., 2020]. ViT divides input image data into\nsmall patches and translates them to embedding vectors, oth-\nerwise known as a ”token”. Extra learnable class embedding\n(CLS token) is added to the sequence of the tokens before\nfeeding them into Transformer Encoder. Transformer En-\ncoder mainly consists of multilayered global self-attention\nblocks and MLP blocks. The blocks include layer normal-\nization [Ba et al., 2016 ] as one function. An MLP head is\nadded to top layer of the CLS token as a classifier.\nSince its invention, ViT has rapidly become popular in the\nfield of computer vision. Many applications and extensions\nhave been proposed thus far. For example, [Touvron et al.,\n2021],[Bao et al., 2021 ], and [Steiner et al., 2021 ] showed\nthat the performance of ViT is respectively improved by disti-\nlation (DeiT), self-supervised learning (BeiT), and data augu-\nmentation (ViT-AugReg) during pre-training phase. More re-\ncently, [Tolstikhin et al., 2021] proposed MLP-Mixer, which\nwas proven to be quite competitive with ViT by replacing\nself-attention blocks with MLP layers.\nThis study is interested in how to robustify ViT to the com-\nmon perturbations. Recent experimental research has ver-\nified that ViT inherently has robustness without any adap-\ntation or any additional data augmentation. Several stud-\nies empirically show that ViT is inherently more robust than\nCNNs [Paul and Chen, 2021; Morrisonet al., 2021; Naseer et\nal., 2021] by using some benchmark datasets. Several stud-\nies have shown that the robustness of ViT can be improved\nby changing the training strategy, such as using a larger\ndata set for the pre-training phase [Paul and Chen, 2021;\nBhojanapalli et al., 2021] or a sharpness-aware optimizer for\nthe training phase [Chen et al., 2022 ]. However, retraining\nsuch a massively pre-trained model from scratch is not de-\nsirable considering the computational burden. The larger the\ndata and model, the higher it costs for retraining. At the same\ntime, however, the model size also matters for robustness, i.e.,\nlarger models tend to be more robust by themselves (See Ap-\npendix E for the detail). This observation motivated us to\ninvestigate a lightweight and model-agnostic way to improve\nthe robustness of the models.\n2.2 Test-Time Adaptation (TTA)\nThis study investigates the effectiveness of the test-time adap-\ntation approaches for Vision Transformer and its variants.\nUnlike most existing works that focus on training phase to\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1010\nimprove the robustness, test-time adaptation focuses on test-\ntime. In other words, test-time adaptation does not alter the\ntraining phase; therefore, we do not need to repetitively run\ncomputationally heavy training to improve robustness.\nThe algorithm of existing test-time adaptation can be sum-\nmarized by following two aspects: (1) adaptation function\nfadapt and (2) modulation parameters ψ. Literally, fadapt\nis a function that determines how to modulate the model pa-\nrameter during the test time. More formally,fadapt receives a\nbatch of unlabeled images Xtest, which is available online at\ntest-time, and updates the target parameter ψ using the data.\nA naive instance of fadapt might use stochastic gradient de-\ncent (SGD) by designing a loss function that can effectively\nincorporate Xtest to correct its prediction. For example, Tent\n[Wang et al., 2020 ], which is a pioneering method for test-\ntime adaptation, minimizes prediction entropy using SGD,\nbased on the assumption that a more confident prediction (i.e.\nlow prediction entropy) leads to a more accurate prediction.\nOne can also use different loss functions (such as pseudo-\nlabel (PL) [Lee and others, 2013 ], diversity regularization\n(SHOT-IM) [Liang et al., 2020 ], and feature alignments\n(TFA) [Liu et al., 2021]), or design optimization-free proce-\ndures to update the model (e.g., T-BN[Schneider et al., 2020;\nNado et al., 2020] and T3A [Iwasawa and Matsuo, 2021]).\nThe second aspect is the selection of modulation param-\neters ψ. As discussed in [Wang et al., 2020 ], updating the\nentire model parameters θ is often ineffective in test-time op-\ntimization because θ is usually the only information of the\nsource data in the setup, and updating all parameters without\nrestriction results in catastrophic failure. (See Table 1 for the\nexperiment result). Consequently, prior works also proposed\nto sensibly select modulation parameters along with the adap-\ntation method fadapt. For example, [Schneider et al., 2020;\nNado et al., 2020 ] proposed to re-estimate the statistics of\nbatch normalization [Ioffe and Szegedy, 2015] during the test\ntime while fixing the other parameters. Similarly, Tent[Wang\net al., 2020] modulated only a set of affine transformation pa-\nrameters of the BN layer. This causes two problems when ap-\nplying test-time adaptation to ViT. First, ViT has significantly\nlarger parameters compared to ResNet which is the standard\ntest bed of prior studies. Consequently, the effectiveness of\nTTA on such a huge model has not been fully investigated.\nSecond, ViT and its variants do not have BN, so they can-\nnot directly take advantage of the common good strategy. In\nother words, there is a lack of knowledge regarding which\nparameter should be updated to effectively robustify ViT.\nIn this study, we avoid the difficulty of sensibly select-\ning the modulation parameters by incorporating the feature-\nalignment approach. More specifically, we explicitly mini-\nmize the difference between some statistics of source distri-\nbution and target (test) distribution, rather than simply modu-\nlating model parameters only given data from target distribu-\ntion. In other words, we leverage the source statistics as aux-\niliary information regarding the source distribution to prevent\nadaptation from causing the aforementioned catastrophic fail-\nure. Note that our method does not rely on the co-existence\nof source and target data and does not violate the setting of\ntest-time adaptation.\nSimilar to our work, [Liu et al., 2021 ] recently proposed\ntest-time feature alignment (TFA), which aligns the hidden\nrepresentation between source and target data by minimiz-\ning the distance of the mean and covariance matrix. Our\nmethod is different from TFA in the following two aspects.\nFirst, we propose to align class-conditional statistics as well\nas the statistics of overall distribution. Second, we propose to\ncalculate the statistics after properly normalizing the hidden\nrepresentations. In §4.4, our experiment results demonstrate\nthat these techniques stably improve the performance of var-\nious tasks based on various backbone networks.\n3 Methods\n3.1 Modulation Parameters\nAs discussed in §2.2, the choice of modulation parameters is\nregarded as important in test-time adaptation but prior BN-\nbased modulation is not applicable to Vision Transformer. To\nfind good candidates for ψ in ViT, we sweep over the fol-\nlowing four candidates: layer normalization [Ba et al., 2016],\nCLS token, feature extractor parameters, and all parameters.\nA Layer normalization (LN) re-estimates the mean and\nstandard deviation of input across the dimensions of the input,\nfollowed by the affine transformation for each dimension. We\nupdate the affine transformation parameters in LN for adap-\ntation. A CLS token is a parameterized vector and proven to\nbe efficient for fine-tuning large models for downstream tasks\nin NLP [Lester et al., 2021]. A feature extractor is defined as\nany module in a model except for its classifier. This term is\nborrowed from [Liang et al., 2020 ]. In the case of ViT, its\nfeature extractor consists of Transformer Encoder, patch em-\nbeddings, and positional embeddings. Updating feature ex-\ntractor parameters is a basic unsupervised domain adaptation\nsetting, while [Wang et al., 2020] claimed that it was ineffec-\ntive in test-time adaptation setup (see §2.2).\nIt is worth noting that these choices of modulation parame-\nters are applicable to many modern architectures, including\nViT, DeiT, MLP-Mixer, and BeiT. This property is impor-\ntant in practice because a better backbone network usually\nprovides significant performance gains. We also empirically\nshow the effectiveness of TTA on such various architectures.\n3.2 Class-Conditional Feature Alignment\nRegarding the adaptation function, this study proposes a\nnew loss function, called class-conditional feature alignment\n(CFA). Similar to the most prior works, our method uses\nstochastic gradient decent to adapt the model during test-time.\nUnlike the prior methods such as Tent, PL, and T3A that mod-\nulate the parameters using the data available at test-time only,\nour method aligns the statistics of features between source\nand target. In other words, we leverage the source statistics\nas an auxiliary information regarding the source distribution\nto prevent the model from suffering a catastrophic failure.\nAssume that a model consists of two components; a linear\nclassifier gω as last layer, and a feature extractor fϕ before\nthe classifier. A set of source training samples is denoted as\nXs = {xs\ni }Ns\ni=1. While prior works often calculate the statis-\ntics of feature output by fϕ, the feature is not always normal-\nized. For example, ViT uses GELU as an activation function\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1011\nAlgorithm 1Online Adaptation using CFA\nInput: Fine-tuned DNN model with parameters θ, Partial pa-\nrameters to be updated during adaptation ψ ⊂θ, Target test\ndataset Xt, m-th ordered batch data Xt,m ⊂Xt, Statistics\nof Eq.(2) (3) (4) calculated from source training dataset.\nOutput:\n1: for m = 1to M do\n2: Predict labels ˆY t,m for Xt,m\n3: Calculate statistics Eq.(6) (7) (8) for Xt,m\n4: Update ψ using Eq.(11)\n5: end for\n6: return ( ˆY t,1, ..,ˆY t,M )\nand LN with elementwise affine transformation before classi-\nfier, which is not bounded. We found that this causes unstable\nbehavior especially when matching higher order moments of\ndistributions. Thus, before calculating the statistics, we nor-\nmalize (bound the minimum and maximum value of) the hid-\nden representation for each sample fϕ(xs\ni ) as follows.\nh(xs\ni ) =Tanh\n(\nLN† (fϕ(xs\ni ))\n)\n, (1)\nwhere LN† is defined as layer normalization without affine\ntransformation. Despite the simplicity, we empirically find\nthat not only matching higher order moment of overall dis-\ntribution is stabilized, but also the performance of class-\nconditional feature alignment is boosted (See Table 5 for the\ndetail). The feature normalization might have a positive effect\non class-conditional distribution matching by highlighting the\ndistribution property of each class.\nAfter the normalization, the mean and higher order central\nmoments of overall distribution on source data are calculated\nand stored in memory as fixed values.\nµs = 1\n|Xs|\n∑\nxs\ni ∈Xs\nh(xs\ni ), (2)\nMs\nk = 1\n|Xs|\n∑\nxs\ni ∈Xs\n(h(xs\ni ) −µs)k, (k = 2, ..., K) (3)\nwhere K denotes the maximum number of moments. Class-\nconditional mean of the normalized hidden representations is\nalso calculated and stored in memory as fixed value as follows\nµs\nc = 1\n|Xsc |\n∑\nxs\ni ∈Xs\nc\nh(xs\ni ), (c = 1, ..., C) (4)\nwhere C denotes the number of classes. Xs\nc ⊂Xs contains\nall the source samples whose ground-truth labels are c. Note\nthat these statistics are calculated before adaptation, i.e., we\ndo not need to access to source data itself in test phase.\nCFA uses these statistics to adapt the model during test\nphase. Assume that a sequence of test data drawn from tar-\nget distribution arrives at our model one after another. Test\ndataset is denoted as Xt = {xt\ni}Nt\ni=1, and a set of test data in\nm-th batch is denoted as Xt,m ⊂Xt, (m = 1, ..., M). For\neach batch, hidden representations of test data are normalized\nand their statistics are calculated in the same way as source.\nh(xt\ni) =Tanh\n(\nLN† (\nfϕ(xt\ni)\n))\n, (5)\nViT-B16 Tent PL SHOT-IM CFA\nLN 50.6±0.5 55.7±1.4 45.7±0.0 43.9±0.0\nCLS 59.4±0.0 60.6±0.0 59.9±0.0 58.2±0.0\nFeature 56.2±2.2 60.8±2.1 43.9±0.0 41.8±0.0\nALL 59.1±1.0 61.4±2.2 44.0±0.0 41.8±0.0\nViT-L16 Tent PL SHOT-IM CFA\nLN 42.3±0.0 44.3±0.0 42.0±0.0 40.2±0.0\nCLS 50.3±0.0 51.3±0.0 50.7±0.1 49.2±0.0\nFeature 43.8±0.6 46.5±0.8 38.4±0.0 36.6±0.0\nALL 44.2±1.1 46.9±0.7 38.4±0.0 36.6±0.0\nTable 1: Modulation parameter choice study. The evaluation metric\nis top-1 error on ImageNet-C averaged over 15 corruption types with\nseverity level of 5. ViT-B16 and ViT-L16 are used as the models.\nCLS: CLS token, LN: layernorm params, Feature: parameters of\nfeature extractor, ALL: all the parameters of ViT.\nµt,m = 1\n|Xt,m|\n∑\nxt\ni∈Xt,m\nh(xt\ni), (6)\nMt,m\nk = 1\n|Xt,m|\n∑\nxt\ni∈Xt,m\n(h(xt\ni) −µt,m)k, (k = 2...K) (7)\nµt,m\nc = 1\n|Xt,m\nc |\n∑\nxt\ni∈Xt,m\nc\nh(xt\ni), (c = 1, ..., C) (8)\nwhere Xt,m\nc , which is a subset of Xt,m, includes all samples\nin the current batch annotated as class c by pseudo-labeling\nargmaxc gω(fϕ(xt\ni)). In this study, the overall distribution\ndistance is defined by the central moment distance (CMD)\n[Zellinger et al., 2017] (see Appendix G for details):\nLF =1\n2||µs −µt,m||2 + 1\n2k\nK∑\nk=2\n||Ms\nk −Mt,m\nk ||2. (9)\nAs for class-conditional distribution matching, following\nprior studies in UDA setting ( [Xie et al., 2018; Deng et al.,\n2019]), we use class-conditional centroid alignment.\nLC = 1\n2|C′|\n∑\nc∈C′\n||µs\nc −µt,m\nc ||2, (10)\nwhere C′ denotes a set of the pseudo labelled classes belong-\ning to the current target minibatch samples. The first-order\nmoment (centroid) is sufficient for class-conditional feature\nalignment when class size is larger than batch size. Parame-\nters ψ of the model are updated by the gradient of the follow-\ning loss function based on the target batch data at hand.\nL=LF + λLC, (11)\nwhere λ is a balancing hyperparameter. Following [Wang et\nal., 2020], for efficient computation, we use the scheme that\nthe parameter update follows the prediction for the current\nbatch. Therefore, the update only affects the next batch. The\nadaptation procedure is summarized in Algorithm 1.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1012\nClass C10→ C100→ ImageNet→ SVHN→ SVHN→ ImageNet→\nMethod Type Cond. C10-C C100-C ImageNet-C MNIST MNIST-M ImageNet-S\nSource - 14.6±0.0 35.1±0.0 61.9±0.0 23.2±0.0 46.2±0.0 64.1±0.0\nT3A gf 13.7±0.0 34.0±0.0 61.2±0.0 17.4±0.3 40.9±0.2 61.7±0.0\nTent fm 10.9±0.2 27.4±0.5 50.6±0.5 15.3±0.2 53.0±1.7 68.3±4.3\nPL fm 11.9±0.0 30.1±0.5 55.7±1.4 15.8±0.7 49.7±1.9 62.2±1.2\nSHOT-IM fm 8.9±0.0 25.6±0.0 45.7±0.0 13.7±0.1 36.6±0.4 56.1±0.1\nTFA(-) fa 8.8±0.0 32.2±0.2 57.8±0.1 16.5±0.1 39.3±0.4 65.7±0.2\nCFA-F (Ours) fa 8.7±0.0 25.2±0.0 46.7±0.0 16.3±0.0 39.9±0.1 57.2±0.1\nCFA-C (Ours) fa ✓ 8.5±0.0 25.3±0.1 45.3±0.0 14.2±0.0 35.8±0.2 57.6±0.1\nCFA (Ours) fa ✓ 8.4±0.0 24.6±0.1 43.9±0.0 14.2±0.1 36.3±0.2 56.1±0.0\nTable 2: Method comparison on each adaptation tasks. The evaluation metric is top-1 error rate. The results of CIFAR-10-C, CIFAR-100-C\nand ImageNet-C are ones averaged over the 15 corruption types with highest severity level (=5). CFA-F : Overall distribution matching only.\nCFA-C : Class-conditional distribution matching only. gf : Gradient free method. fm : Method that controls the output feature representation\n(without depending on feature alignment) by modulation. fa : Method that utilizes feature alignment between source and target by modulation.\nOur proposal (CFA-C and CFA) is the only method utilizing the class-conditional feature alignment during test-time adaptation.\n4 Experiment\n4.1 Datasets and Task Design\nCommon Corruptions. We validate the robustness against\ncommon corruptions on CIFAR-10-C, CIFAR-100-C and\nImageNet-C [Hendrycks and Dietterich, 2019 ] as target\ndatasets. These datasets contain data with 15 types corrup-\ntions with five levels of severity, that is, each dataset has 75\ndistinct corruptions. Most of our experiments use the high-\nest severity(=5) datasets as they can make the difference in\nperformance most noticeable. As source datasets, CIFAR-10,\nCIFAR-100 [Krizhevsky and Hinton, 2009 ] and ImageNet(-\n2012) [Russakovsky et al., 2015] are used, respectively.\nDomain Adaptation. We validate the robustness against\nstyle shift on small-sized datasets and large-sized datasets.\nFor small-sized datasets, we evaluate the adaptation from\nSVHN to MNIST / MNIST-M [Netzer et al., 2011; Le-\nCun et al., 1998; Ganin and Lempitsky, 2015 ]. For large-\nsized datasets, we evaluate the adaptation from ImageNet to\nImageNet-Sketch [Wang et al., 2019]. See Appendix A for a\ndetail description of each dataset.\n4.2 Implementation Details\nVistion Transformer (ViT-B16) is used as a default model\nthroughout the experiment unless an explicit explanation is\nprovided. Images of all the datasets are resized to 224×224\n(see Appendix B for details). Before adaptation, the model\nis fine-tuned on each source dataset (see Appendix C for de-\ntails). In addition, the central moments statistics of hidden\nrepresentation based on source data need to be calculated to\nstore them in memory. For this purpose, we use all the train-\ning data in the source dataset and set the dropout [Srivastava\net al., 2014] off in the model during the calculation.\nAs for default hyperparameters for adaptation on target\ndata, batch size is set as 64, optimizer is set as SGD with a\nconstant learning rate of 0.001, and momentum of 0.9 with\ngradient clipping [Zhang et al., 2020 ] at global norm 1.0\nacross all the experiments (Gradient clipping has the effect\nof preventing adaptation by Tent from catastrophic failure in\nthe severe corruption setting. See “Ablation Study” in §4.4).\nAs for CFA, the balancing parameterλ is set as 1.0, and max-\nimum central moments order K is set as 3. During prediction\nand parameter update, dropout is set off in models.\nAs an evaluation metric, top-1 error of classification is used\nacross all the experiments. We run all the experiments three\ntimes with different seeds for different data ordering by shuf-\nfling. A mean and unbiased standard deviation of the metric\nare reported. Our implementation is in PyTorch [Paszke et\nal., 2019 ]. We use various backbone networks from timm\nlibrary [Wightman, 2019] and torchvision library (Appendix\nD). Every experiment is run on cloud A100 x 1GPU instance.\n4.3 Baseline Methods\nWe compare CFA with some existing baseline test-time adap-\ntation methods that do not need to alter training phase as de-\nscribed in §2.2: Tent, PL, TFA(-)3 , T3A and SHOT-IM.\nIn addition, we report the performance of the model on tar-\nget datasets without any adaptation as Source. T-BN is ex-\ncluded from the baseline because some models (ViT variants\nand MLP-Mixer) do not have a batch normalization layer. For\na fair comparison, we use the same hyperparameters across\nall the methods as described in §4.2.\n4.4 Experiment Result\nModulation Study. Table 1 answers the question about\nwhich set of modulation parameters is the most suitable for\nimproving the performance of test-time adaptations on ViTs.\nThere are two findings. First, updating layer nomalization pa-\nrameters can achieve balanced and high performance across\nall the main methods. Second, SHOT-IM and CFA achieve\n3Original TFA needs to alter training phase (add contrastive\nlearning), while this study focuses on robustifying large-scale mod-\nels without retraining them from scratch. Therefore, we have\nchanged some of the settings from the original TFA so that the model\ndoes not need to alter training phase. The modified version of TFA\nis denoted as TFA(-) in our experiments. See Appendix F for details.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1013\nImageNet-C ImageNet-S\nResNet50 82.0±0.0 75.4±0.0\n+ CFA / SHOT-IM 58.8±0.0/58.8±0.0 70.0±0.2/69.2±0.1\nResNet101 77.4±0.0 72.3±0.0\n+ CFA / SHOT-IM 55.3±0.1/55.7±0.0 66.8±0.0/66.2±0.1\nViT-B16 61.9±0.0 64.1±0.0\n+ CFA / SHOT-IM 43.9±0.0/45.7±0.0 56.0±0.1/56.1±0.1\nViT-L16 53.4±0.0 59.1±0.0\n+ CFA / SHOT-IM 40.2±0.0/42.0±0.0 52.6±0.0/53.6±0.1\nDeiT-S16 59.9±0.0 66.6±0.0\n+ CFA / SHOT-IM 46.0±0.0/46.1±0.0 60.3±0.1/59.4±0.0\nDeiT-B16 52.9±0.0 62.5±0.0\n+ CFA / SHOT-IM 39.9±0.0/39.9±0.0 55.9±0.0/55.4±0.0\nMLP-Mixer-B16 73.3±0.0 74.3±0.0\n+ CFA / SHOT-IM 52.4±0.1/55.1±0.1 64.2±0.1/65.9±0.2\nMLP-Mixer-L16 77.1±0.0 79.8±0.0\n+ CFA / SHOT-IM 56.3±0.0/62.4±0.1 70.8±0.3/72.9±0.3\nViT-B16-AugReg 49.0±0.0 57.0±0.0\n+ CFA / SHOT-IM 37.6±0.0/38.4±0.0 51.5±0.1/51.0±0.2\nViT-L16-AugReg 39.1±0.0 48.2±0.0\n+ CFA / SHOT-IM 32.1±0.0/33.3±0.0 45.2±0.0/45.6±0.1\nBeiT-B16 48.3±0.0 52.6±0.0\n+ CFA / SHOT-IM 35.4±0.0/37.6±0.0 47.5±0.0/49.1±0.0\nBeiT-L16 35.9±0.0 44.2±0.0\n+ CFA / SHOT-IM 26.0±0.0/28.2±0.0 39.9±0.1/41.5±0.0\nTable 3: Adaptation results based on several backbone networks.\nThe evaluation metric of ImageNet-C is the averaged top-1 error\nover 15 corruption types with a severity level of 5. We use publicly\navailable models that were already fine-tuned on ImageNet.\nhigher performance by updating all or feature extractor pa-\nrameters, while Tent and PL deteriorates the performance\nbecause of catastrophic failure (See Appendix I for details).\nThis indicates that a method with more sophisticated strategy\nwithin the adaptation function can work properly without sen-\nsibly selecting modulation parameters. In all the subsequent\nexperiments, we choose layer normalization as modulation\nparameters across all the methods for the fair comparison.\nCFA Outperforms Existing Methods on Several Datasets.\nTable 2 summarizes the adaptation result across datasets for\neach test time adaptation methods. As for CIFAR-10-C,\nCIFAR-100-C, and ImageNet-C, we measure the averaged\ntop-1 error across 15 corruption types for the highest severity\nlevel (=5). CFA (our method) aligns both the overall distri-\nbution and class-conditional distribution between source and\ntarget datasets. In addition to CFA, we have experimented\nclass-conditional distribution matching only method (CFA-\nC) and overall distribution matching only method (CFA-F)\nto measure the contribution of each distribution matching to\nperformance. Specifically, the objective function of CFA-C\nand CFA-F is respectively defined as Eq.(10) and Eq.(9). The\nexperiment results demonstrate that CFA can achieve the best\nor comparable performance against baseline methods across\nall datasets. It is also verified that CFA-F and CFA-C can\nSev\nerity Source Tent SHOT-IM CFA\n1 16.8±0.0\n15.4±0.0 15.8±0.0 15.3±0.0\n2 20.3±0.0 17.9±0.0 18.4±0.0 17.5±0.0\n3 22.5±0.0 19.6±0.4 19.9±0.0 18.7±0.0\n4 27.2±0.0 24.0±1.2 23.2±0.0 21.5±0.0\n5 35.9±0.0 33.6±0.1 28.2±0.0 26.0±0.0\nAv\nerage 24.5±0.0 22.1±0.3 21.1±0.0 19.8±0.0\nTable 4: Top-1 error rate on ImageNet-C averaged across all the\nseverity level and 15 corruption types. BeiT-L16 is used as a model.\nMethod W/.\nEq(1)(5) W/O. Eq(1)(5)\nSource 61.95±0.00\n61.95±0.00\nCFA-F\n(K=1) 46.69±0.02 46.69±0.01\nCFA-F (K=3) 46.66±0.02 47.28±0.03\nCFA-F (K=5) 46.64±0.02 54.51±0.14\nCFA-C 45.31±0.03 47.13±0.06\nCFA\n(K=1) 43.98±0.04 45.28±0.02\nCFA (K=3) 43.90±0.04 44.56±0.03\nCFA (K=5) 43.90±0.04 52.25±0.11\nTable 5: Ablation study of CFA. Top-1 error on ImageNet-C aver-\naged over 15 corruption types with severity level of 5. ViT-B16 is\nused. CFA-F : Overall distribution matching only. CFA-C : Class-\nconditional distribution matching only. K : Maximum # of central\nmoments. K=1 denotes first-order moment (mean) matching only.\nsolely achieve better performance compared to the case with-\nout adaptation (“Source”) as in Table 2. Finally, CFA further\nboosts the performance on most datasets by combining them.\nCFA Is Model Agnostic. Table 3 shows the adaptation\nresults on ImageNet-C and ImageNet-Sketch by CFA (and\nSHOT-IM for comparison) based on various category’s back-\nbone networks. Specifically, we used publicly available mod-\nels that are already fine-tuned on ImageNet-2012 at a reso-\nlution of 224 ×224, including ResNet, ViT, ViT-AugReg,\nDeiT, BeiT, and MLP-Mixer. See Appendix D for details.\nThe modulation parameters are BN for ResNet, and LN for\nthe others. The results indicate that our method (CFA) consis-\ntently improves the performance regardless of the backbone\nnetworks. It is also found that the better performance on the\nsource dataset (ImageNet), the stronger robustness on the tar-\nget dataset (ImageNet-C) the model can gain by adaptation.\nSee Appendix E for visualization of the relationship.\nCFA Achieves SOTA Performance. Among these back-\nbone networks, we select BeiT-L16, which achieved strong\nperformance on ImageNet, and calculate the top-1 error rate\non ImageNet-C averaged over 15 types of corruptions and\nall the severity levels (1-5) for each TTA methods. The re-\nsults described in Table 4 demonstrate that 19.8% using CFA\non BeiT-L16 gives superior performance to the other baseline\nmethods. It also outperforms the existing test-time adaptation\nresult 44.0% using Tent on ResNet50 [Wang et al., 2020 ].\nTherefore, CFA achieves the state-of-the-art (SOTA) perfor-\nmance among TTA methods that do not need to alter training\nphase (See Appendix I for the full results).\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1014\nFigure 1: The effect of changing hyperparameters on Tent and CFA\nperformance. The evaluation metric is the top-1 error on ImageNet-\nC averaged over 15 corruption types with a severity level of 5. ViT-\nB16 is used as a model. Either one of the hyperparameter values is\nchanged from the default described in §4.2.\nAblation Study. Table 5 summarizes the ablation study re-\nsults to analyze the detailed contributions of each components\nin our method on the robustness. Specifically, we analyze the\neffect of the normalization of hidden representation before\ncalculating the distribution statistics by comparing the sce-\nnarios of with/without Eq.(1) and (5). In the case of CFA-F,\nit is verified that the performance deteriorates significantly\nwithout Eq.(1) and (5) especially when the maximum num-\nber of moments K gets larger. This indicates that feature\nnormalization, especially bounding minimum and maximum\nvalue of hidden representation, stabilizes the performance of\nmatching higher order moments. In the case of CFA-C, it\nis verified that the performance deteriorates without Eq.(1)\nand (5). It is speculated that feature normalization, especially\nlayer normalization without affine transformation, might have\na positive effect on class-conditional (centroid) distribution\nmatching by highlighting the distribution property for each\nclass. In addition, it is also verified that using both overall fea-\nture alignment and class-conditional feature alignment (CFA)\nboosts the performance compared to either alone (CFA-F or\nCFA-C) regardless of the value of K.\nHyperparameter Sensitivity. For online adapatation, hy-\nperparameter selection is a challenging issue. Figure 1 shows\nthe experiment results about each hyperparameter sensitivity\non ImageNet-C with the highest severity level (=5) averaged\nover 15 corruption types. We checked 4 hyperparameters by\nchanging either one of the values from the default described\nin §4.2. (a) learning rate, (b) batch size, (c) balancing hyper-\nparameter λ, and (d) whether to enable gradient clipping for\nSGD optimization. The finding is that Tent is more sensitive\nto some hyperparameters than CFA. In particular, enabling\ngradient clipping is essential when applying Tent to ViT to\navoid catastrophic failure, while it is not essential for CFA.\nFurthermore, large learning rate also causes Tent catastrophic\nfailure. In contrast, CFA is robust to all the above hyperpa-\nrameters. This indicates that we can safely use CFA in un-\nknown environments with rough hyperparameter selection.\n5 Conclusion\nThis is the first study that verifies the effectiveness of test-\ntime adaptation methods on ViT to boost their robustness.\nExperiment results demonstrate that the existing methods can\nbe applied to ViT and the prior-convention (sensibly selecting\nmodulation parameters) is not necessary when a proper loss\nfunction is used. This study also proposed a novel method,\nCFA, which is hyperparameter friendly, model agnostic, and\nsurpasses existing baselines. We hope this study becomes a\nmilestone of TTA for current large models and will serve as a\nstepping stone to TTA for larger models in the future.\nAcknowledgements\nThis work has been supported by the Mohammed bin Salman\nCenter for Future Science and Technology for Saudi-Japan\nVision 2030 at The University of Tokyo (MbSC2030). Com-\nputational resource of AI Bridging Cloud Infrastructure\n(ABCI) provided by National Institute of Advanced Industrial\nScience and Technology (AIST) was used for experiments.\nReferences\n[Ba et al., 2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Ge-\noffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[Bao et al., 2021] Hangbo Bao, Li Dong, and Furu Wei. Beit:\nBert pre-training of image transformers. arXiv preprint\narXiv:2106.08254, 2021.\n[Bhojanapalli et al., 2021] Srinadh Bhojanapalli, Ayan\nChakrabarti, Daniel Glasner, Daliang Li, Thomas Un-\nterthiner, and Andreas Veit. Understanding robustness of\ntransformers for image classification. In Proceedings of the\nIEEE/CVF ICCV, pages 10231–10241, 2021.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa, Gabriel\nSynnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers.\nIn ECCV, pages 213–229. Springer, 2020.\n[Chen et al., 2022] Xiangning Chen, Cho-Jui Hsieh, and Bo-\nqing Gong. When vision transformers outperform resnets\nwithout pre-training or strong data augmentations. In ICLR,\n2022.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia\nLi, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchi-\ncal image database. In CVPR, pages 248–255, 2009.\n[Deng et al., 2019] Zhijie Deng, Yucen Luo, and Jun Zhu. Clus-\nter alignment with a teacher for unsupervised domain adapta-\ntion. In Proceedings of ICCV, pages 9944–9953, 2019.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In ICLR,\n2020.\n[Ganin and Lempitsky, 2015] Yaroslav Ganin and Victor Lem-\npitsky. Unsupervised domain adaptation by backpropagation.\nIn ICML, pages 1180–1189. PMLR, 2015.\n[Hendrycks and Dietterich, 2019] Dan Hendrycks and Thomas\nDietterich. Benchmarking neural network robustness to com-\nmon corruptions and perturbations. Proc. of the ICLR, 2019.\n[Hendrycks et al., 2019] Dan Hendrycks, Norman Mu, Ekin D\nCubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-\nnarayanan. Augmix: A simple data processing method\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1015\nto improve robustness and uncertainty. arXiv preprint\narXiv:1912.02781, 2019.\n[Hendrycks et al., 2020] Dan Hendrycks, Steven Basart, Nor-\nman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,\nRahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al.\nThe many faces of robustness: A critical analysis of out-of-\ndistribution generalization. arXiv preprint arXiv:2006.16241,\n2020.\n[Ioffe and Szegedy, 2015] Sergey Ioffe and Christian Szegedy.\nBatch normalization: Accelerating deep network training by\nreducing internal covariate shift. In ICML, pages 448–456.\nPMLR, 2015.\n[Iwasawa and Matsuo, 2021] Yusuke Iwasawa and Yutaka Mat-\nsuo. Test-time classifier adjustment module for model-\nagnostic domain generalization. In Advances in NeurIPS,\n2021.\n[Krizhevsky and Hinton, 2009] Alex Krizhevsky and Geoffrey\nHinton. Learning multiple layers of features from tiny im-\nages. Technical report, University of Toronto, 2009.\n[LeCun et al., 1998] Yann LeCun, L ´eon Bottou, Yoshua Ben-\ngio, and Patrick Haffner. Gradient-based learning ap-\nplied to document recognition. Proceedings of the IEEE,\n86(11):2278–2324, 1998.\n[Lee and others, 2013] Dong-Hyun Lee et al. Pseudo-label:\nThe simple and efficient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges in repre-\nsentation learning, ICML, volume 3, page 896, 2013.\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah\nConstant. The power of scale for parameter-efficient prompt\ntuning. In Proceedings of EMNLP, pages 3045–3059, 2021.\n[Liang et al., 2020] Jian Liang, Dapeng Hu, and Jiashi Feng. Do\nwe really need to access the source data? source hypothesis\ntransfer for unsupervised domain adaptation. In ICML, pages\n6028–6039. PMLR, 2020.\n[Liu et al., 2021] Yuejiang Liu, Parth Kothari, Bastien Ger-\nmain van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and\nAlexandre Alahi. TTT++: When does self-supervised test-\ntime training fail or thrive? In Advances in NeurIPS, 2021.\n[Morrison et al., 2021] Katelyn Morrison, Benjamin Gilby,\nColton Lipchak, Adam Mattioli, and Adriana Kovashka.\nExploring corruption robustness: Inductive biases in\nvision transformers and mlp-mixers. arXiv preprint\narXiv:2106.13122, 2021.\n[Nado et al., 2020] Zachary Nado, Shreyas Padhy, D Scul-\nley, Alexander D’Amour, Balaji Lakshminarayanan, and\nJasper Snoek. Evaluating prediction-time batch normaliza-\ntion for robustness under covariate shift. arXiv preprint\narXiv:2006.10963, 2020.\n[Naseer et al., 2021] Muzammal Naseer, Kanchana Ranas-\ninghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan,\nand Ming-Hsuan Yang. Intriguing properties of vision trans-\nformers. arXiv preprint arXiv:2105.10497, 2021.\n[Netzer et al., 2011] Yuval Netzer, Tao Wang, Adam Coates,\nAlessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading\ndigits in natural images with unsupervised feature learning.\nIn NIPS Workshop, 2011.\n[Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco\nMassa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in NeurIPS, pages 8026–\n8037, 2019.\n[Paul and Chen, 2021] Sayak Paul and Pin-Yu Chen. Vi-\nsion transformers are robust learners. arXiv preprint\narXiv:2105.07581, 2(3), 2021.\n[Rusak et al., 2020] Evgenia Rusak, Lukas Schott, Roland S\nZimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias\nBethge, and Wieland Brendel. A simple way to make neural\nnetworks robust against diverse image corruptions. In ECCV,\npages 53–69. Springer, 2020.\n[Russakovsky et al., 2015] Olga Russakovsky, Jia Deng, Hao\nSu, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,\net al. Imagenet large scale visual recognition challenge.IJCV,\n115(3):211–252, 2015.\n[Schneider et al., 2020] Steffen Schneider, Evgenia Rusak,\nLuisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias\nBethge. Improving robustness against common corruptions\nby covariate shift adaptation. Advances in NeurIPS, 33, 2020.\n[Srivastava et al., 2014] Nitish Srivastava, Geoffrey Hinton,\nAlex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from over-\nfitting. The JMLR, 15(1):1929–1958, 2014.\n[Steiner et al., 2021] Andreas Steiner, Alexander Kolesnikov,\nXiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lu-\ncas Beyer. How to train your vit? data, augmentation,\nand regularization in vision transformers. arXiv preprint\narXiv:2106.10270, 2021.\n[Tolstikhin et al., 2021] Ilya Tolstikhin, Neil Houlsby, Alexan-\nder Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit,\nMario Lucic, et al. Mlp-mixer: An all-mlp architecture for\nvision. arXiv preprint arXiv:2105.01601, 2021.\n[Touvron et al., 2021] Hugo Touvron, Matthieu Cord, Matthijs\nDouze, Francisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. Training data-efficient image transformers & distilla-\ntion through attention. In ICML, pages 10347–10357, 2021.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nIn Advances in NeurIPS, pages 5998–6008, 2017.\n[Wang et al., 2019] Haohan Wang, Songwei Ge, Zachary Lip-\nton, and Eric P Xing. Learning robust global representations\nby penalizing local predictive power. InAdvances in NeurIPS,\npages 10506–10518, 2019.\n[Wang et al., 2020] Dequan Wang, Evan Shelhamer, Shaoteng\nLiu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-\ntime adaptation by entropy minimization. In ICLR, 2020.\n[Wightman, 2019] Ross Wightman. Pytorch image models.\nhttps://github.com/rwightman/pytorch-image-models, 2019.\nAccessed: 2021-12-27.\n[Xie et al., 2018] Shaoan Xie, Zibin Zheng, Liang Chen, and\nChuan Chen. Learning semantic representations for unsuper-\nvised domain adaptation. In ICML, pages 5423–5432, 2018.\n[Zellinger et al., 2017] Werner Zellinger, Thomas Grub-\ninger, Edwin Lughofer, Thomas Natschl ¨ager, and Susanne\nSaminger-Platz. Central moment discrepancy (CMD) for\ndomain-invariant representation learning. In 5th ICLR, 2017.\n[Zeng et al., 2020] Yanhong Zeng, Jianlong Fu, and Hongyang\nChao. Learning joint spatial-temporal transformations for\nvideo inpainting. In ECCV, pages 528–543. Springer, 2020.\n[Zhang et al., 2020] Jingzhao Zhang, Tianxing He, Suvrit Sra,\nand Ali Jadbabaie. Why gradient clipping accelerates train-\ning: A theoretical justification for adaptivity. In ICLR, 2020.\n[Zhou et al., 2018] Luowei Zhou, Yingbo Zhou, Jason J Corso,\nRichard Socher, and Caiming Xiong. End-to-end dense video\ncaptioning with masked transformer. In Proceedings of the\nIEEE Conference on CVPR, pages 8739–8748, 2018.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1016",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7238852977752686
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5398073196411133
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5227680206298828
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.41505953669548035
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.41266050934791565
    },
    {
      "name": "Machine learning",
      "score": 0.3500255346298218
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}