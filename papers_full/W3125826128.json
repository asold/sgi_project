{
    "title": "WangchanBERTa: Pretraining transformer-based Thai Language Models",
    "url": "https://openalex.org/W3125826128",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287569145",
            "name": "Lowphansirikul, Lalita",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287569146",
            "name": "Polpanumas, Charin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287569147",
            "name": "Jantrakulchai, Nawat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742359880",
            "name": "Nutanong, Sarana",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3039089356",
        "https://openalex.org/W3033187248",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3019416653",
        "https://openalex.org/W3013571468",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W3205367839",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3048604374",
        "https://openalex.org/W3089430725",
        "https://openalex.org/W3190860428",
        "https://openalex.org/W3106353077",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2419539795",
        "https://openalex.org/W3210505626",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2971307358",
        "https://openalex.org/W630532510",
        "https://openalex.org/W2784121710",
        "https://openalex.org/W2964583233",
        "https://openalex.org/W2154359981",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2251654371",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2045384855"
    ],
    "abstract": "Transformer-based language models, more specifically BERT-based architectures have achieved state-of-the-art performance in many downstream tasks. However, for a relatively low-resource language such as Thai, the choices of models are limited to training a BERT-based model based on a much smaller dataset or finetuning multi-lingual models, both of which yield suboptimal downstream performance. Moreover, large-scale multi-lingual pretraining does not take into account language-specific features for Thai. To overcome these limitations, we pretrain a language model based on RoBERTa-base architecture on a large, deduplicated, cleaned training set (78GB in total size), curated from diverse domains of social media posts, news articles and other publicly available datasets. We apply text processing rules that are specific to Thai most importantly preserving spaces, which are important chunk and sentence boundaries in Thai before subword tokenization. We also experiment with word-level, syllable-level and SentencePiece tokenization with a smaller dataset to explore the effects on tokenization on downstream performance. Our model wangchanberta-base-att-spm-uncased trained on the 78.5GB dataset outperforms strong baselines (NBSVM, CRF and ULMFit) and multi-lingual models (XLMR and mBERT) on both sequence classification and token classification tasks in human-annotated, mono-lingual contexts.",
    "full_text": "arXiv:2101.09635v2  [cs.CL]  20 Mar 2021\nWA N G C H A N BERTA: P R E T R A IN I N G T R A N S F O R M E R -BA S E D\nTH A I LA N G UAG E MO D E L S\nLalita Lowphansirikul ∗† , Charin Polpanumas ∗ ‡ , Nawat Jantrakulchai † , and Sarana Nutanong †\n‡ PyThaiNLP\ncharin.polpanumas@datatouille.org\n† School of Information Science and T echnology, V idyasirime dhi Institution of Science and T echnology\n{lalital pro, nawatj pro, snutanon }@vistec.ac.th\nMarch 23, 2021\nABSTRACT\nTransformer-based language models, more speciﬁcally BER T -based architectures have achieved\nstate-of-the-art performance in many downstream tasks. Ho wever, for a relatively low-resource\nlanguage such as Thai, the choices of models are limited to tr aining a BER T -based model based on\na much smaller dataset or ﬁnetuning multi-lingual models, b oth of which yield suboptimal down-\nstream performance. Moreover, large-scale multi-lingual pretraining does not take into account\nlanguage-speciﬁc features for Thai. T o overcome these limi tations, we pretrain a language model\nbased on RoBER T a-base architecture on a large, deduplicate d, cleaned training set (78GB in total\nsize), curated from diverse domains of social media posts, n ews articles and other publicly available\ndatasets. W e apply text processing rules that are speciﬁc to Thai most importantly preserving spaces,\nwhich are important chunk and sentence boundaries in Thai be fore subword tokenization. W e also\nexperiment with word-level, syllable-level and SentenceP iece tokenization with a smaller dataset to\nexplore the effects on tokenization on downstream performa nce. Our model wangchanberta-base-\natt-spm-uncased trained on the 78.5GB dataset outperforms strong baselines (NBSVM, CRF and\nULMFit) and multi-lingual models (XLMR and mBER T) on both se quence classiﬁcation and token\nclassiﬁcation tasks in human-annotated, mono-lingual con texts.\nKeywords Language Modeling ·BER T ·RoBER T a·Pretraining ·Transformer ·Thai Language\n1 Introduction\nTransformer-based language models, more speciﬁcally BER T -based architectures [Devlin et al., 2018b],\n[Liu et al., 2019], [Lan et al., 2019], [Clark et al., 2020], a nd [He et al., 2020], have achieved state-of-the-art\nperformance in downstream tasks such as sequence classiﬁca tion , token classiﬁcation, question answering, natural\nlanguage inference and word sense disambiguation [W ang et a l., 2018, W ang et al., 2019]. However, for a relatively\nlow-resource language such as Thai, the choices of models ar e limited to training a BER T -based model based\non a much smaller dataset such as BER T -th [ThAIKeras, 2018] t rained on Thai W ikipedia Dump , or ﬁnetuning\nmulti-lingual models such as XLMR [Conneau et al., 2019] (10 0 languages) and mBER T [Devlin et al., 2018b] (104\n∗ Equal contribution. Listed in random order.\nA P RE P RIN T - M A RCH 23, 2021\nlanguages). Training on a small dataset has a detrimental ef fect on downstream performance. BER T -th underper-\nforms RNN-based ULMFit [Polpanumas and Phatthiyaphaibun, 2021] trained Thai W ikipedia Dump on sequence\nclassiﬁcation task W ongnai Reviews[W ongnai.com, 2018]. For multi-lingual training, we can se e from comparison\nbetween multi-lingual and mono-lingual models such as [Mar tin et al., 2019] that multi-lingual models underperform\nmono-lingual models. Moreover, large-scale multi-lingua l pretraining does not take into account language-speciﬁc\nfeatures for Thai. T o overcome these limitations, we pretra in a language model based on RoBER T a-base architecture\non a large, deduplicated, cleaned training set (78GB in tota l size), curated from diverse domains of social media\nposts, news articles and other publicly available datasets . W e apply text processing rules that are speciﬁc to Thai\nmost importantly preserving spaces, which are important ch unk and sentence boundaries in Thai before subword\ntokenization.\nIn this report, we describe a language model based on RoBER T a -base architecture and SentencePiece\n[Kudo and Richardson, 2018] subword tokenizer on 78GB clean ed and deduplicated data from publicly available so-\ncial media posts, news articles, and other open datasets. W e also pretrain four other language models using different\ntokenizers, namely SentencePiece [Kudo and Richardson, 20 18], dictionary-based word-level and syllable-level tok-\nenizer (PyThaiNLP’s newmm [Phatthiyaphaibun et al., 2020] ), and SEFR tokenizer [Limkonchotiwat et al., 2020], on\nThai Wikipedia Dump to explore how tokens affect downstream performance.\nT o assess the effectiveness of our language model, we conduc ted an extensive set of experimental studies on the\nfollowing downstream tasks: sequence classiﬁcation (mult i-class and multi-label) and token classiﬁcation. Our mode l\nwangchanberta-base-att-spm-uncased outperforms strong baseline models (NBSVM [W ang and Manning, 2012] and\nCRF [Okazaki, 2007]), ULMFit [Howard and Ruder, 2018] (thai 2ﬁt [Polpanumas and Phatthiyaphaibun, 2021]) and\nmulti-lingual transformer-based models (XLMR [Conneau et al., 2019] and mBER T [Devlin et al., 2018a]) on both\nsequence and token classiﬁcation tasks.\nThe remaining sections of this report are organized as follo ws. In Section 2, we describe the methodology in pretraining\nthe language models including raw data, preprocessing, tra in-validation-test split preparation and training the mod els.\nIn Section 3, we introduce the downstream tasks we use to test the performance of our language models. In Section 4,\nwe demonstrate the results of our language modeling and ﬁnet uning for downstream tasks. In Section 5, we discuss\nthe results and next steps for this work.\nThe pretrained language models and ﬁnetuned models 2 are publicly available at Huggingface’s Model Hub. The sour ce\ncode used for the experiments can be found at our GitHub repos itory.3\n2 Methodology\nW e train one language model on the Assorted Thai T exts datasetincluding all available raw datasets and four language\nmodels on the W ikipedia-only dataset, each with a different tokenizer.\n2.1 Raw Data\nThe raw data are obtained from (statistics after preprocess ing):\nDataset name Data size Description\nwisesight-large 51.44GB\n(314M lines)\na large dataset of social media posts provided by the social l istening plat-\nform Wisesight 4 for this study. The dataset contains posts T witter, Faceboo k,\nPantip, Instagram, Y ouTube and other websites sampled from 2019.\n2 https://huggingface.co/airesearch\n3 https://github.com/vistec-AI/thai2transformers\n4 https://wisesight.com/\n2\nA P RE P RIN T - M A RCH 23, 2021\npantip-large 22.35GB\n(95M lines)\na collection of posts and answers of Thailand’s largest onli ne bulletin board\nPantip.com from 2015 to 2019 provided by audience analytics platform\nChaos Theory. 5\nThairath-222k6 1.48GB\n(5M lines)\na collection of articles published on newspaper website Tha irath.com up to\nDecember 2019.\nprachathai-67k7 903.1MB\n(2.7M lines)\na collection of articles published on newspaper website Pra chathai.com from\nAugust 24, 2004 to November 15, 2018.\nThai Wikipedia\nDump8\n515MB\n(843k lines)\nthe Wikipedia articles extracted using Giuseppe Attardi’s WikiExtractor9 in\nSeptember 2020. All HTML tags, bullet points, and tables are removed.\nOpenSubtitles 468.8MB\n(5M lines)\na collection of movie subtitles translated by crowdsourcin g from OpenSubti-\ntles.org [Lison and Tiedemann, 2016]. W e use only the portio ns containing\nThai texts.\nThaiPBS-111k10 372.3MB\n(858k lines)\na collection of articles published on newspaper website Tha iPBS.or.th up to\nDecember 2019.\nThai National\nCorpus\n366MB\n(797k lines)\na 14-million-word corpus of Thai texts containing 75% non-ﬁ ction and 25%\nﬁction works. Media source breakdown is 60% books, 25% magaz ines, and\nthe rest from other publications and writings. Most of the te xts are curated\nfrom 1998 to 2007 [Aroonmanakun et al., 2009].\nscb-mt-en-th-\n2020\n290.4MB\n(947k lines)\na parallel corpus of Englsih-Thai sentence pairs curated ne ws, Wikipedia\narticles, SMS messages, task-based dialogs, web-crawled d ata, government\ndocuments, and machine-generated text [Lowphansirikul et al., 2020].\nJW300 182.8MB\n(727k lines) a parallel corpus of religion texts from jw .org that include s Thai texts.\nwongnai-corpus11 64MB\n(101k lines)\na collection of restaurant reivews and ratings (1 to 5 stars) published on\nW ongnai.com.\nQED 42MB\n(407k lines)\na collection of transcripts for educational videos and lect ures collaboratively\ncreated on the AMARA web-based platform [Abdelali et al., 20 14].\nbibleuedin 2.18MB\n(62k lines)\na multilingual corpus of the Bible created by Christos Chris todoulopoulos\nand Mark Steedman.\nwisesight-\nsentiment\n5.3MB\n(22k lines)\na collection of T witter posts about consumer products and se rvices from\n2016 to early 2019 labeled positive, negative, neutral and q uestion\n[Suriyawongkul et al., 2019].\ntanzil 2.4MB\n(6k lines)\na collection of Quran translations compiled by the T anzil pr oject\n[Tiedemann, 2012].\ntatoeba 1MB\n(2k lines)\na collection of translated sentences from the crowdsourced multilingual\ndataset T atoeba [Tiedemann, 2012].\n5 https://www .facebook.com/ChaosTheoryCompany/\n6 https://github.com/nakhunchumpolsathien/TR-TPBS\n7 https://github.com/PyThaiNLP/prachathai-67k\n8 https://dumps.wikimedia.org/backup-index.html\n9 https://github.com/attardi/wikiextractor/\n10 https://github.com/nakhunchumpolsathien/TR-TPBS\n11 https://github.com/wongnai/wongnai-corpus\n3\nA P RE P RIN T - M A RCH 23, 2021\n2.2 Preprocessing\nW e apply preprocessing rules to the raw datasets before usin g them as our training sets. This effectively demands the\npreprocessing rules to be applied before ﬁnetuning for both domain-speciﬁc language modeling and other downstream\ntasks.\nT ext Processing A large portion of our training data ( wisesight-large and pantip-large) comes from social media,\nwhich usually have a lot of unusual spellings and repetition s. For such noisy data, [Raffel et al., 2020] reports that\npretraining on a cleaned corpus C4 yields better performance in downstream tasks. Therefore, we opted to perform\nthe following processing rules, in order:\n• Replace HTML forms of characters with the actual character s such as nbsp; with a space and <br /> with\na line break [Howard and Ruder, 2018].\n• Remove empty brackets ( (), {}, and []) than sometimes come up as a result of text extraction such as from\nWikipedia.\n• Replace line breaks with spaces.\n• Replace more than one spaces with a single space\n• Remove more than 3 repetitive characters such as ดีมากกก to ดีมาก [Howard and Ruder, 2018].\n• W ord-level tokenization using [Phatthiyaphaibun et al., 2020]’s newmm dictionary-based maximal matching\ntokenizer.\n• Replace repetitive words; this is done post-tokenization unlike [Howard and Ruder, 2018] since there is no\ndelimitation by space in Thai as in English.\n• Replace spaces with <\n>. The SentencePiece tokenizer combines the spaces with othe r tokens. Since\nspaces serve as punctuation in Thai such as sentence boundar ies similar to periods in English, combining it\nwith other tokens will omit an important feature for tasks su ch as word tokenization and sentence breaking.\nTherefore, we opt to explicitly mark spaces with < >.\nFor W ikipedia-only dataset, we only replace non-breaking spaces with spaces, remove an empty parenthesis that occur\nright after the title of the ﬁrst paragraph, and replace spac es with < >.\nSentence Breaking Each row of all datasets are originally delimited by line bre aks. Due to memory constraints, in\norder to train the language model, we need to limit our maximu m sequence length to 416 subword tokens (tokenized\nby SentencePiece [Kudo and Richardson, 2018] unigram model ) or roughly 300 word tokens (tokenized by dictionary-\nbased maximal matching [Phatthiyaphaibun et al., 2020]). I n order to do so, we use the sentence breaking model\nCRFCut ([Lowphansirikul et al., 2020]). CRFCut is a conditi onal random ﬁelds (CRF) model trained on English-\nto-Thai translated texts of [Sornlertlamvanich et al., 199 7] (23,125 sentences), TED transcripts (136,463 sentences ;\n[Lowphansirikul et al., 2020]) and generated product revie ws (217,482 sentences; [Lowphansirikul et al., 2020]). It\nuses English sentence boundary as sentence boundary labels for translated Thai texts. CRFCut has sentence-boundary\nF1 score of 0.69 on [Sornlertlamvanich et al., 1997], 0.71 on TED Transcripts, and 0.96 on generated product reviews.\nW e keep only sentences that are 5 to 300 words long to not excee d 416-subword maximum sequence length and also\nnot have a sequence too short for language modeling.\nT okenizers For the model trained on Assorted Thai T exts dataset, in the same manner as [Martin et al., 2019], we use\nSentencePiece [Kudo and Richardson, 2018] unigram languag e model [Kudo, 2018] to tokenize sentences of training\ndata into subwords. The tokenizer has a vocabulary size of 25 ,000 subwords, trained on 15M sentences. T o construct\nthe training set for the tokenizer, we ﬁrst take 2.5M randoml y sampled sentences from pantip-large, 3.5M randomly\nsampled sentences from wisesight-large and all sentences of the remaining datasets, resulting in 20 ,961,306 total\nsentences. Out of those, we randomly sampled 15M sentences t o train the tokenizer.\n4\nA P RE P RIN T - M A RCH 23, 2021\nFor the models trained on W ikipedia-only dataset, we use four different tokenizers to examine their effects o n language\nmodeling and downstream tasks. W e use the same training set o f 944,782 sentences sampled from Thai W ikipedia\nDump\n• SentencePiece tokenizer ; we train the SentencePiece [Kudo and Richardson, 2018] uni gram language model\n[Kudo, 2018] using 944,782 sentences from Thai W ikipedia Dump, resulting in a tokenizer with vocab size\nof 24,000 subwords.\n• W ord-level tokenizer; the word-level, dictionary-based tokenizer newmm [Phatthiyaphaibun et al., 2020] is\nused to create a tokenizer with vocab size of 97,982 words.\n• Syllable-level tokenizer ; the syllable-level dictionary-based tokenizer syllable\n[Phatthiyaphaibun et al., 2020] is used to create a tokenize r with vocab size of 59,235 syllables.\n• SEFR tokenizer ; Stacked Ensemble Filter and Reﬁne tokenizer ( engine=best) [Limkonchotiwat et al., 2020]\nbased on probabilities from CNN-based deepcut [Kittinaradorn et al., 2019] with a vocab size of 92,177\nwords.\n2.3 T rain-V alidation-T est Splits\nAssorted Thai T exts Dataset After preprocessing and deduplication, we have a training s et of 381,034,638 unique,\nmostly Thai sentences with sequence length of 5 to 300 words ( 78.5GB). The training set has a total of 16,957,775,412\nwords as tokenized by dictionary-based maximal matching [P hatthiyaphaibun et al., 2020], 8,680,485,067 subwords\nas tokenized by SentencePiece [Kudo and Richardson, 2018] t okenizer, and 53,035,823,287 characters.\nW e also randomly sampled 99,181 sentences (19.28MB) as vali dation set and 42,238,656 sentences (8GB) as test set.\nBoth are preprocessed in the same manner as the training set.\nWikipedia-only Dataset From Thai W ikipedia Dump, we extract in a uniformly random manner 944,782 sentences\nfor training set, 24,863 sentences for validation set and 24 ,862 sentences for test set.\n2.4 Language Modeling\nArchitecture W e use the transformer [V aswani et al., 2017] architecture o f BER T (Base) (12 layers, 768 hidden\ndimensions, 12 attention heads) [Devlin et al., 2018b]. Our setup is very similar to [Martin et al., 2019] replacing\nBER T’s W ordPiece tokenizer with a SentencePiece tokenizer , with the exception of preprocessing rules applied before\nsubword tokenization.\nPretraining Objective W e train the model with masked language modeling. T o circumv ent the word boundary\nissues in Thai, we opted to perform this at the subword level i nstead of whole-word level, even though the latter is\nreported to have better performance in English [Joshi et al. , 2020]. In the same manner as BER T [Devlin et al., 2018b]\nand RoBER T a [Liu et al., 2019], for each sequence, we sampled 15% of the tokens and replace them with <mask>\ntoken. Out of the 15%, 80% is replaced with a <mask> token, 10% is left unchanged and 10% is replaced with a\nrandom token. The objective is to predict the tokens replace d with <mask> using cross entropy loss.\nPretraining W e pretrain RoBER T a BASE on both the Assorted Thai T exts dataset and W ikipedia-only dataset. The\nsize of W ikipedia-only dataset is about 0.57 GB which is comparatively low compared to the Assorted Thai T exts\ndataset. Therefore, we manually tune the hyperparamters used for Ro BER T aBASE pretraining for each training set in\norder to control the loss stability. The hyperparameters of the RoBER T aBASE architecture and model pretraining are\nlisted in T able 2.\n5\nA P RE P RIN T - M A RCH 23, 2021\nHyperparameters RoBERT a BASE (Wikipedia-only Dataset) RoBERT aBASE (Assorted Thai T exts Dataset)\nNumber of Layers 12 12\nHidden size 768 768\nFFN hidden size 3,072 3,072\nAttention heads 12 12\nDropout 0.1 0.1\nAttention dropout 0.1 0.1\nMax sequence length 512 416\nEffective batch size 8,192 4,092\nW armup steps 1,250 24,000\nPeak learning rate 7e-4 3e-4\nLearning rate decay Linear Linear\nMax steps 31,250 500,000\nW eight decay 0.01 0.01\nAdam ǫ 1e-6 1e-6\nAdam β1 0.9 0.9\nAdam β2 0.98 0.999\nFP16 training True True\nT able 2: Hyperparameters of RoBER T a BASE used when pretrain on Assorted Thai T exts dataset and W ikipedia-only\ndataset.\nW angchanBERT a W e name our pretrained language models according to their ar chitectures, tokenizers and the\ndatasets on which they are trained on. The models can be found on HuggingFace 12.\nArchitecture Dataset T okenizer\nwangchanberta-base-wiki-spm RoBER T a-base Wikipedia-on ly SentencePiece\nwangchanberta-base-wiki-newmm RoBER T a-base Wikipedia- only word (newmm)\nwangchanberta-base-wiki-syllable RoBER T a-base Wikiped ia-only syllable (newmm)\nwangchanberta-base-wiki-sefr RoBER T a-base Wikipedia-o nly SEFR\nwangchanberta-base-att-spm-uncased RoBER T a-base Assor ted Thai T exts SentencePiece\nT able 3: W angchanBER T a model names\n3 Downstream T asks\nW e evaluate the downstream performance of our pretrained Th ai RoBER T aBASE models on existing Thai sequence-\nclassiﬁcation and token-classiﬁcation benchmark dataset s.\n12 https://huggingface.co/models\n6\nA P RE P RIN T - M A RCH 23, 2021\n3.1 Datasets\nW e use train-valiation-test split as provided by each datas et as hosted on Huggingface Datasets. 13 When not all splits\nare available, namely for W ongnai Reviewsand ThaiNER, we sample respective splits in a uniformly random manner.\nThe descriptive statistics of each datasets are as follows:\nDatasets Label Style T asks Labels Train Eval T est\nwisesight sentiment category informal; social media posts multi-cla ss sequence classiﬁcation 4 21628 2404 2671\nwongnai reviews star rating informal; restaurant reviews multi-class sequence classiﬁcation 5 36000 * 4000* 6203\ngenerated reviews enth review star informal; product reviews multi-class sequence class iﬁcation 5 141369 15708 17453\nprachathai67k tags formal; news multi-label sequence clas siﬁcation 12 54379 6721 6789\nthainer ner tags formal; news and other articles token classiﬁcation 13 ** 5079* 635* 621*,***\nlst20 pos tags formal; news and other articles token classiﬁcation 16 67104 6094 5733\nlst20 ner tags formal; news and other articles token classiﬁcation 10 67104 6094 5733\n*Uniform randomly split with seed = 2020\n**W e replace B-ไม/uni0E48.lowยืนยันand I-ไม/uni0E48.lowยืนยันwhich are extremely rare tags from ThaiNER with O\n***W e removed examples on test set which did not ﬁt in mBERT ma x token length to have a fair comparison among all models\nT able 4: Datasets for downstream tasks\n3.1.1 Sequence Classiﬁcation\nWisesight Sentiment [Suriyawongkul et al., 2019] is a multi-class text classiﬁc ation dataset (sentiment analysis).\nThe data are social media messages in Thailand collected fro m 2016 to early 2019. Each message is annotated as\npositive, neutral, negative, or question.\nW ongnai Reviews [W ongnai.com, 2018] is a multi-class text classiﬁcation da taset (rating classiﬁcation). The data\nare restaurant reviews and their respective ratings from 1 ( worst) to 5 (best) stars.\nGenerated Reviews EN-TH [Lowphansirikul et al., 2020] is a dataset that originally c onsists of product reviews\ngenerated by CTRL [Keskar et al., 2019] in English. It is tran slated to Thai as part of the scb-mt-en-th-2020 machine\ntranslation dataset. Translation is performed both by huma n annotators and models. W e use only the translated Thai\ntexts as a feature to predict review stars from 1 (worst) to 5 ( best).\nPrachathai-67k is a multi-label text classiﬁcation dataset (topic classiﬁ cation) based on news articles of\nPrachathai.com from August 24, 2004 to November 15, 2018 pac kaged by [Phatthiyaphaibun et al., 2020]. W e per-\nform topic classiﬁcation of the headline of each article, wh ich can contain none to all of the following labels: politics ,\nhuman rights, quality of life, international, social, envi ronment, economics, culture, labor, national security, ic t, and\neducation.\n3.1.2 T oken Classiﬁcation\nThaiNER v1.3 [Phatthiyaphaibun, 2019] is a 6,456-sentence named entity recognition (NER) dataset created by\nexpanding an unnamed, 2,258-sentence dataset by [Tirasaro j and Aroonmanakun, 2012]. The NER tags are annotated\nby humans in IOB format.\nLST20 [Boonkwan et al., 2020] is a dataset with 5 layers of linguist ic annotations: word boundaries, POS tagging,\nNER, clause boundaries, and sentence boundaries. NER tags a re in IOBE format. W e use the dataset for POS tagging\nand NER tasks.\n13 https://huggingface.co/datasets\n7\nA P RE P RIN T - M A RCH 23, 2021\n3.2 Benchmarking Models\nW e provide benchmarks using traditional models (NBSVM for s equence classiﬁcation and CRF for token classiﬁca-\ntion), RNN-based models (ULMFit; only for sequence classiﬁ cation) and transformer-based models.\nNBSVM [W ang and Manning, 2012] W e adopt the NBSVM implementation b y Jeremy Howard 14 as our strong\nbaselines for sequence classiﬁcation both multi-class and multi-label. The notable differences are substituting bin a-\nrized ngram features with tf-idf features (uni- and bi-gram s; minimum document frequency of 3, maximum document\nfrequency of 90%). W e also apply the same cleaning rules as th e language model, with the differences being adding\nrepeated character tokens <rep> and repeated word tokens <wrep> instead of space tokens < >.\nW e perform hyperparameter tuning for penalty types (L1 and L 2) and inverse of regularization strength (C=[1.0, 2.0,\n3.0, 4.0]) and choose the models with the highest F1 scores (m icro-averaged for multi-class and macro-averaged for\nmulti-label classiﬁcation). See T able 8. For multi-label c lassifcation, we search for the best set of thresholds (rang ing\nbetween 0.01 – 0.99 with the step size of 0.01) that maximize m acro-average F1-score on validation set.\nULMFit (thai2ﬁt) is an implementation of ULMFit language model ﬁnetuning for text classiﬁcation\n[Howard and Ruder, 2018]. [Polpanumas and Phatthiyaphaibu n, 2021] pretrained a language model with vocab size\nof 60,005 words (tokenized by PyThaiNLP’s newmm) on Thai W ikipedia Dump. W e ﬁnetune the language model on\nthe training set of each dataset for 5 epochs. Then that, we ﬁn etune for the sequence classiﬁcation tasks using gradual\nunfreezing from the last one, two and three parameter groups with discriminative learning rates, for one epoch each.\nAfter that, we ﬁnetune all the weights of the model for 5 epoch s. The checkpoints with the highest accuracy scores\n(validation losses for multi-label classiﬁcation) are cho sen to perform on the test sets. See T able 9. Lastly, we search\nfor the best set of thresholds (ranging between 0.01 – 0.99 wi th the step size of 0.01) that maximize macro-average\nF1-score on validation set.\nConditional Random Fields (CRF) [Lafferty et al., 2001] W e use the CRFSuite implementation [ Okazaki, 2007] of\nconditional random ﬁelds as a strong baseline for POS and NER tagging tasks. W e generate the features by extracting\nunigrams, bigrams and trigrams features within a sliding wi ndow of three timesteps, before and after the current token\n(beginning and ending of sentences are padded with xxpad tokens). W e ﬁnetune L1 and L2 penalty combinations using\n10,000 randomly sampled sentences for LST20 and the entire training set for ThaiNER. With hyperparameters with the\nbest F1 score (micro-averaged) on the validation set, we tra in on the entire training sets and report performances on\nthe test sets. See T able 10. W e run each CRF model for 500 itera tions.\nT ransformer-based models W e use the same ﬁnetuning scheme for all transformer-based m odels, namely XLM-\nRoBER T a-base [Conneau et al., 2019], BER T -base-multiling ual-cased [Devlin et al., 2018a], wangchanberta-base-\nwiki-tokenizer ( spm, newmm, syllable, sefr), and wangchanberta-base-att-spm-uncased. For the seque nce classiﬁ-\ncation task, we preprocess each dataset with the rules descr ibed in 2.2. W e then ﬁnetune each pretrained language\nmodel on downstream tasks for 3 epochs. The criteria to selec t the best epoch is the validation micro-average F1-score\nfor multi-class classiﬁcation and macro-average F1-score for multi-label classiﬁcation. The batch size is set to 16. T he\nThe learning rate is warmed up over the ﬁrst 10% of steps to the value of 3e-5 and linearly decayed to zero. W e ﬁnetune\nmodels with FP16 mixed precision training. All models are op timized with Adam [Kingma and Ba, 2014] ( β1 = 0.9,\nβ2 = 0.999, ǫ =1e-8, L2 weight decay = 0.01) with corrected bias. For multi-label classiﬁcation h ead, we search\nfor the best set of thresholds (ranging between 0.01 – 0.99 wi th the step size of 0.01) that maximize macro-average\nF1-score on validation set.\nFor the token classiﬁcation tasks, we ﬁnetune each pretrain ed language models for 6 epochs. The criteria to select the\nbest epoch is the validation loss. The batch size is set to 32. The learning rate is warmed up over the ﬁrst 10% of steps\nto the value of 3e-5 and linearly decayed to zero. W e ﬁnetune m odels with FP16 mixed precision training. All models\nare optimized with Adam with the parameters as same as the seq uence classiﬁcation task.\n14 https://www .kaggle.com/jhoward/nb-svm-strong-linear-baseline\n8\nA P RE P RIN T - M A RCH 23, 2021\n4 Results\n4.1 Language Modeling\nThe following table shows the performance RoBER T a BASE trained on W ikipedia-only dataset. There are four variations\nof tokenization including subword-level with SentencePie ce [Kudo and Richardson, 2018], word-level and syllable-\nlevel with PyThaiNLP [Phatthiyaphaibun et al., 2020] token izer (denoted as newmm and syllable respectively), and\nstacked-ensemble, word-level tokenizer sefr [Limkonchotiwat et al., 2020].\nModel Name V ocab Size Number of Training Examples Best Checkpoint\nV alidation loss Steps\nPretraining on W ikipedia-only dataset:\nwangchanberta-base-wiki-spm 24,000 116,715 1.5127 7,000\nwangchanberta-base-wiki-newmm 97,982 119,074 1.4990 5,0 00\nwangchanberta-base-wiki-syllable 59,235 167,279 0.8068 8,000\nwangchanberta-base-wiki-sefr 92,177 125,177 1.2995 4,50 0\nPretraining on Assorted Thai T exts dataset(Currently, the model has not reached the max steps):\nwangchanberta-base-att-spm-uncased 25,000 382 M 2.551 360,000\n(latest checkpoint)\nT able 5: The vocab size, number of training examples, and bes t checkpoint of the RoBER T a BASE models trained on\nThai Wikipedia corpus for each type of input tokens and Assor ted Thai T exts dataset.\nFor the RoBER T a BASE trained on Assorted Thai T exts dataset, we only trained with subword token built with Senten-\ncePiece [Kudo and Richardson, 2018] due to the limited compu tational resources.\n4.2 Downstream T asks\nW e choose models to perform on the test set based on their perf ormance on the validation sets. For multi-class\nsequence classiﬁcation and token classiﬁcation, we optimi ze our models for the highest micro-averaged F1 score. For\nmulti-label sequence classiﬁcation, we optimize for the hi ghest macro-averaged F1 score, as it is less affected by clas s\nimbalance. Moreover, for multi-label sequence classiﬁcat ion, we also ﬁnd the best probability threshold for each labe l\nbased on the validation set. W e report the performance of the se optimized models on the test sets.\nFor sequence classiﬁcation tasks, our model trained on the Assorted Thai T exts datasetoutperfroms both strong base-\nlines and other transformer-based architecture on all down stream tasks except Generated Reviews (EN-TH). This may\nbe attributed to the fact that the dataset is translated from generated texts in English, thus multi-lingual pretrainin g of\nXLMR gives it the advantage. 6.\nFor token classiﬁcation tasks, our model trained on the Assorted Thai T exts datasetachieves the highest micro-averaged\nF1 score in all tasks except POS tagging in ThaiNER dataset. This could be attributed to the fact that the POS tag s in\nThaiNER are machine-generated and thus more suited for the baseline model CRF . See T able 7.\n9\nA P RE P RIN T - M A RCH 23, 2021\nModel Wisesight W ongnai Generated Reviews (EN–TH)\nPrachathai(Review rating)\nExisting multilingual models:\nXLMR [Conneau et al., 2019] 73.57 / 62.21 62.57 / 52.75 64.91 / 60.29 68.18 / 63.14\nmBER T [Devlin et al., 2018b] 70.05 / 57.81 47.99 / 12.97 62.14 / 57.20 66.47 / 60.11\nOur baseline models:\nNaive Bayes SVM 72.03 / 54.67 58.38 / 39.75 59.68 / 52.17 66.77 / 60.73\nULMFit (thai2ﬁt) 70.95 / 60.62 61.79 / 48.04 64.33 / 59.33 66. 21 / 60.21\nOur pretrained RoBER T aBASE models:\nwangchanberta-base-wiki-spm 73.94 / 60.13 60.60 / 48.17 63 .43 / 58.43 68.85 / 63.46\nwangchanberta-base-wiki-newmm 72.74 / 55.87 59.81 / 45.75 63.70 / 58.41 68.78 / 63.50\nwangchanberta-base-wiki-syllable 73.42 / 59.12 60.36 / 46 .68 63.53 / 58.73 68.90 / 63.59\nwangchanberta-base-wiki-sefr 70.80 / 59.51 59.83 / 48.21 6 3.31 / 58.85 67.45 / 61.14\nwangchanberta-base-att-spm-uncased 76.19 / 67.05 63.05 / 52.19 64.66 / 59.54 69.78 / 64.90\nT able 6: T est set results for RoBER T a BASE models we pretrain and existing multilingual language models inclduing\nXLM RoBER T aBASE (XLMR) and Multilingual BER T BASE (mBER T). The metrics we report are micro-average and\nmacro-average F1 score.\nModel ThaiNER LST20\nNER POS NER\nExisting multilingual models:\nXLMR [Conneau et al., 2019] 83.25 / 67.23 96.57 / 85.00 73.61 / 68.67\nmBER T [Devlin et al., 2018b] 81.48 / 73.97 96.44 / 85.86 75.05 / 68.25\nOur baseline models:\nConditional Random Fields (CRF) 78.98 / 81.85 96.28 / 81.28 75.94 / 72.13\nOur pretrained RoBER T aBASE models:\nwangchanberta-base-wiki-spm 56.64 / 55.34 96.18 / 83.99 77 .12 / 71.32\nwangchanberta-base-wiki-newmm 58.54 / 47.71 96.14 / 83.11 76.59 / 70.57\nwangchanberta-base-wiki-syllable 83.23 / 76.64 96.06 / 83 .98 76.45 / 70.37\nwangchanberta-base-wiki-sefr 85.04 / 77.73 96.36 / 85.24 7 6.25 / 69.34\nwangchanberta-base-att-spm-uncased 86.49 / 79.29 96.62 / 85.44 78.01 / 72.25\nT able 7: T est set results for RoBER T a BASE models we pretrain and existing multilingual language models inclduing\nXLM RoBER T aBASE (XLMR) and Multilingual BER T BASE (mBER T). The metrics we report are micro-average and\nmacro-average F1 score.\n5 Discussions and Future W orks\nConsistent with previous works on language modeling, we fou nd that training on large datasets such as our Assorted\nThai T exts datasetyield better downstream performance. The only case when a mu lti-lingual model (XLMR) outper-\nforms our largest mono-lingual model is when the training da ta include multi-lingual elements namely the English-to-\nThai translated texts of Generated Reviews EN-TH. From our experiments on the W ikipedia-only dataset, we did not\nﬁnd any notable diferrence in downstream performance for se quence classiﬁcation or token classiﬁcation tasks.\n10\nA P RE P RIN T - M A RCH 23, 2021\nAnother area we will explore in the future is the inherent bia ses on our relatively large language models. Previous\nworks including [Sheng et al., 2019] [Nadeem et al., 2020] [N angia et al., 2020] have detected social biases within\nlarge language models trained in English. Our next step in th is direction is to create similar bias-measuring datasets i n\nThai contexts to detect the biases in our language models.\nW e pretrain our language models on publicly available datas ets. T wo main concerns that have been raised about\nsimilar models are copyrights and privacy. All datasets use d to train our models are based on publicly available\ndata. Publicly available social media data are packaged and provided to use by Wisesight 15 (wisesight-large) and\nChaos Theory 16 (pantip-large). Unless speciﬁed otherwise in the distribution of dataset s, all rights belong to the\ncontent creators. W e provide the weights of our pretrained l anguage models under CC-BY -SA 4.0. Our models are\ntrained as feature extractors for downstream tasks, and not generative tasks. Reproduction of training data can happen\n[Carlini et al., 2020] albeit at much lower chance than langu age models trained speciﬁcally for generative tasks.\n6 Acknowledgements\nW e thank Wisesight 16 , Chaos Theory 17 and Pantip.com for providing what has become, to the best of o ur knowledge,\nthe largest and most diverse high-quality training data in T hai for language modeling.\nReferences\n[Abdelali et al., 2014] Abdelali, A., Guzman, F ., Sajjad, H. , and V ogel, S. (2014). The AMARA corpus: Building\nparallel language resources for the educational domain. In Proceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC’14) , pages 1856–1862, Reykjavik, Iceland. European Language\nResources Association (ELRA).\n[Aroonmanakun et al., 2009] Aroonmanakun, W ., T ansiri, K., and Nittayanuparp, P . (2009). Thai national corpus: a\nprogress report. In Proceedings of the 7th W orkshop on Asian Language Resources(ALR7), pages 153–160.\n[Boonkwan et al., 2020] Boonkwan, P ., Luantangsrisuk, V ., P haholphinyo, S., Kriengket, K., Leenoi, D., Phrombut,\nC., Boriboon, M., Kosawat, K., and Supnithi, T . (2020). The a nnotation guideline of lst20 corpus. arXiv preprint\narXiv:2008.05055 .\n[Carlini et al., 2020] Carlini, N., Tramer, F ., W allace, E., Jagielski, M., Herbert-V oss, A., Lee, K., Roberts, A., Brow n,\nT ., Song, D., Erlingsson, U., et al. (2020). Extracting trai ning data from large language models. arXiv preprint\narXiv:2012.07805 .\n[Clark et al., 2020] Clark, K., Luong, M.-T ., Le, Q. V ., and Ma nning, C. D. (2020). Electra: Pre-training text encoders\nas discriminators rather than generators. arXiv preprint arXiv:2003.10555.\n[Conneau et al., 2019] Conneau, A., Khandelwal, K., Goyal, N ., Chaudhary, V ., W enzek, G., Guzm ´ an, F ., Grave, E.,\nOtt, M., Zettlemoyer, L., and Stoyanov, V . (2019). Unsuperv ised cross-lingual representation learning at scale.\narXiv preprint arXiv:1911.02116.\n[Devlin et al., 2018a] Devlin, J., Chang, M., Lee, K., and T ou tanova, K. (2018a). BER T: pre-training of deep bidirec-\ntional transformers for language understanding. CoRR, abs/1810.04805.\n[Devlin et al., 2018b] Devlin, J., Chang, M.-W ., Lee, K., and T outanova, K. (2018b). Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n[He et al., 2020] He, P ., Liu, X., Gao, J., and Chen, W . (2020). Deberta: Decoding-enhanced bert with disentangled\nattention. arXiv preprint arXiv:2006.03654.\n[Howard and Ruder, 2018] Howard, J. and Ruder, S. (2018). Uni versal language model ﬁne-tuning for text classiﬁca-\ntion. arXiv preprint arXiv:1801.06146.\n15 https://wisesight.com\n16 https://www .facebook.com/ChaosTheoryCompany/\n11\nA P RE P RIN T - M A RCH 23, 2021\n[Joshi et al., 2020] Joshi, M., Chen, D., Liu, Y ., W eld, D. S., Zettlemoyer, L., and Levy, O. (2020). Spanbert: Im-\nproving pre-training by representing and predicting spans . T ransactions of the Association for Computational\nLinguistics, 8:64–77.\n[Keskar et al., 2019] Keskar, N. S., McCann, B., V arshney, L. R., Xiong, C., and Socher, R. (2019). CTRL: A\nconditional transformer language model for controllable g eneration. CoRR, abs/1909.05858.\n[Kingma and Ba, 2014] Kingma, D. and Ba, J. (2014). Adam: A met hod for stochastic optimization. International\nConference on Learning Representations.\n[Kittinaradorn et al., 2019] Kittinaradorn, R., Achakulvi sut, T ., Chaovavanich, K., Srithaworn, K., Chormai, P .,\nKaewkasi, C., Ruangrong, T ., and Oparad, K. (2019). DeepCut : A Thai word tokenization library using Deep\nNeural Network.\n[Kudo, 2018] Kudo, T . (2018). Subword regularization: Impr oving neural network translation models with multiple\nsubword candidates. arXiv preprint arXiv:1804.10959.\n[Kudo and Richardson, 2018] Kudo, T . and Richardson, J. (201 8). SentencePiece: A simple and language indepen-\ndent subword tokenizer and detokenizer for neural text proc essing. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System D emonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\n[Lafferty et al., 2001] Lafferty, J., McCallum, A., and Pere ira, F . C. (2001). Conditional random ﬁelds: Probabilistic\nmodels for segmenting and labeling sequence data.\n[Lan et al., 2019] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P ., and Soricut, R. (2019). Albert: A lite bert\nfor self-supervised learning of language representations . arXiv preprint arXiv:1909.11942.\n[Limkonchotiwat et al., 2020] Limkonchotiwat, P ., Phatthi yaphaibun, W ., Sarwar, R., Chuangsuwanich, E., and Nu-\ntanong, S. (2020). Domain adaptation of thai word segmentat ion models using stacked ensemble. Association for\nComputational Linguistics.\n[Lison and Tiedemann, 2016] Lison, P . and Tiedemann, J. (201 6). OpenSubtitles2016: Extracting large parallel\ncorpora from movie and TV subtitles. In Proceedings of the T enth International Conference on Langu age Re-\nsources and Evaluation (LREC’16) , pages 923–929, Portoro ˇ z, Slovenia. European Language Re sources Associa-\ntion (ELRA).\n[Liu et al., 2019] Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M ., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L.,\nand Stoyanov, V . (2019). Roberta: A robustly optimized bert pretraining approach. arxiv 2019. arXiv preprint\narXiv:1907.11692 .\n[Lowphansirikul et al., 2020] Lowphansirikul, L., Polpanu mas, C., Rutherford, A. T ., and Nutanong, S. (2020). scb-\nmt-en-th-2020: A large english-thai parallel corpus. arXiv preprint arXiv:2007.03541.\n[Martin et al., 2019] Martin, L., Muller, B., Su ´ arez, P . J. O ., Dupont, Y ., Romary, L., de la Clergerie, ´E. V ., Seddah,\nD., and Sagot, B. (2019). Camembert: a tasty french language model. arXiv preprint arXiv:1911.03894.\n[Nadeem et al., 2020] Nadeem, M., Bethke, A., and Reddy, S. (2 020). Stereoset: Measuring stereotypical bias in\npretrained language models.\n[Nangia et al., 2020] Nangia, N., V ania, C., Bhalerao, R., an d Bowman, S. R. (2020). Crows-pairs: A challenge\ndataset for measuring social biases in masked language mode ls.\n[Okazaki, 2007] Okazaki, N. (2007). Crfsuite: a fast implem entation of conditional random ﬁelds, 2007.\n[Phatthiyaphaibun, 2019] Phatthiyaphaibun, W . (2019). wa nnaphongcom/thai-ner: Thainer 1.3.\n[Phatthiyaphaibun et al., 2020] Phatthiyaphaibun, W ., Cha ovavanich, K., Polpanumas, C., Suriyawongkul, A., Low-\nphansirikul, L., and Chormai, P . (2020). Pythainlp/pythai nlp: Pythainlp 2.1.4.\n[Polpanumas and Phatthiyaphaibun, 2021] Polpanumas, C. an d Phatthiyaphaibun, W . (2021). thai2ﬁt: Thai language\nimplementation of ulmﬁt.\n12\nA P RE P RIN T - M A RCH 23, 2021\n[Raffel et al., 2020] Raffel, C., Shazeer, N., Roberts, A., L ee, K., Narang, S., Matena, M., Zhou, Y ., Li, W ., and Liu,\nP . J. (2020). Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine\nLearning Research, 21(140):1–67.\n[Sheng et al., 2019] Sheng, E., Chang, K.-W ., Natarajan, P ., and Peng, N. (2019). The woman worked as a babysitter:\nOn biases in language generation. arXiv preprint arXiv:1909.01326.\n[Sornlertlamvanich et al., 1997] Sornlertlamvanich, V ., C haroenporn, T ., and Isahara, H. (1997). Orchid: Thai part-\nof-speech tagged corpus. National Electronics and Computer T echnology Center T echnical Report, pages 5–19.\n[Suriyawongkul et al., 2019] Suriyawongkul, A., Chuangsuw anich, E., Chormai, P ., and Polpanumas, C. (2019).\nPythainlp/wisesight-sentiment: First release.\n[ThAIKeras, 2018] ThAIKeras (2018). Thaikeras bert. https://github.com/ThAIKeras/bert.\n[Tiedemann, 2012] Tiedemann, J. (2012). Parallel data, too ls and interfaces in opus. In Chair), N. C. C., Choukri,\nK., Declerck, T ., Dogan, M. U., Maegaard, B., Mariani, J., Od ijk, J., and Piperidis, S., editors, Proceedings of the\nEight International Conference on Language Resources and E valuation (LREC’12) , Istanbul, Turkey. European\nLanguage Resources Association (ELRA).\n[Tirasaroj and Aroonmanakun, 2012] Tirasaroj, N. and Aroon manakun, W . (2012). Thai ner using crf model based\non surface features. pages 176–180. SNLP-A OS 2011.\n[V aswani et al., 2017] V aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\nPolosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems , pages\n5998–6008.\n[W ang et al., 2019] W ang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Michael, J., Hill, F ., Levy, O., and Bowman,\nS. (2019). Superglue: A stickier benchmark for general-pur pose language understanding systems. In Advances in\nneural information processing systems, pages 3266–3280.\n[W ang et al., 2018] W ang, A., Singh, A., Michael, J., Hill, F . , Levy, O., and Bowman, S. R. (2018). Glue: A multi-task\nbenchmark and analysis platform for natural language under standing. arXiv preprint arXiv:1804.07461.\n[W ang and Manning, 2012] W ang, S. I. and Manning, C. D. (2012) . Baselines and bigrams: Simple, good sentiment\nand topic classiﬁcation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics\n(V olume 2: Short P apers), pages 90–94.\n[W ongnai.com, 2018] W ongnai.com (2018). W ongnai-corpus. https://github.com/wongnai/wongnai-corpus.\n13\nA P RE P RIN T - M A RCH 23, 2021\n7 Appendix\ndatasets[features:labels] penalty C f1\nwisesight sentiment[texts:category]\nl2 3 0.720466\nl2 2 0.718386\nl2 4 0.715474\nl1 2 0.710067\nl1 3 0.707571\nl2 1 0.707571\nl1 1 0.706323\nl1 4 0.705075\nwongnai reviews[review body:star rating]\nl1 2 0.57125\nl2 4 0.5705\nl2 3 0.56675\nl1 3 0.5635\nl2 2 0.5605\nl1 4 0.55425\nl1 1 0.55325\nl2 1 0.54\ngenerated reviews enth[translation[th]:review star]\nl2 2 0.593265\nl2 3 0.591609\nl2 1 0.590718\nl2 4 0.5904\nl1 2 0.590209\nl1 1 0.590018\nl1 3 0.58467\nl1 4 0.577222\nprachathai67k[title:multilabel]\nl2 1 0.61105\nl2 2 0.607425\nl2 3 0.60561\nl2 4 0.601663\nl1 1 0.59017\nl1 2 0.585137\nl1 3 0.578731\nl1 4 0.574738\nT able 8: NBSVM Hyperparameter Tuning Results\n14\nA P RE P RIN T - M A RCH 23, 2021\ndatasets[features:labels] ﬁnetuning unfreezing epoch tr ain loss valid loss accuracy\nwisesight sentiment[texts:category]\nLM all 0 4.459779 4.256248 0.330593\nLM all 1 4.261896 4.081441 0.348543\nLM all 2 4.042319 3.979969 0.358797\nLM all 3 3.854878 3.939566 0.364824\nLM all 4 3.754257 3.932823 0.365743\nCLS last 1 0 0.90485 0.800313 0.653078\nCLS last 2 0 0.834293 0.762427 0.675957\nCLS last 3 0 0.783797 0.724123 0.68594\nCLS all 0 0.729717 0.73506 0.673877\nCLS all 1 0.744124 0.707241 0.702579\nCLS all 2 0.721162 0.694311 0.714642\nCLS all 3 0.719528 0.698624 0.710899\nCLS all 4 0.659977 0.691418 0.711314\nwongnai reviews(review body:star rating)\nLM all 0 3.844957 3.675409 0.358546\nLM all 1 3.640318 3.511868 0.375098\nLM all 2 3.521275 3.422731 0.383874\nLM all 3 3.423584 3.377162 0.388852\nLM all 4 3.333537 3.370303 0.389565\nCLS last 1 0 1.039886 0.981237 0.54275\nCLS last 2 0 0.952058 0.913627 0.58675\nCLS last 3 0 0.917949 0.884318 0.595\nCLS all 0 0.881919 0.882625 0.595\nCLS all 1 0.879615 0.883927 0.59825\nCLS all 2 0.865561 0.889925 0.58675\nCLS all 3 0.831835 0.894447 0.602\nCLS all 4 0.808713 0.895076 0.59925\ngenerated reviews enth[translation[th]:review star]\nLM all 0 3.562119 3.389167 0.347284\nLM all 1 3.425128 3.265404 0.362213\nLM all 2 3.312375 3.198505 0.370227\nLM all 3 3.235396 3.164119 0.374517\nLM all 4 3.184817 3.157655 0.375286\nCLS last 1 0 1.097455 0.98512 0.586516\nCLS last 2 0 0.976767 0.902084 0.62204\nCLS last 3 0 0.925023 0.874969 0.631653\nCLS all 0 0.892837 0.870975 0.637\nCLS all 1 0.884311 0.859921 0.636555\nCLS all 2 0.852318 0.856317 0.638464\nCLS all 3 0.840453 0.85957 0.64012\nCLS all 4 0.827038 0.859206 0.639674\nprachathai[title:multilabel]\nLM all 0 4.347134 4.142264 0.347872\nLM all 1 4.150784 3.989359 0.359503\nLM all 2 3.950324 3.895626 0.370871\nLM all 3 3.784429 3.858943 0.37453\nLM all 4 3.709645 3.854859 0.374904\nCLS last 1 0 0.263054 0.240299 NA\nCLS last 2 0 0.246976 0.22738 NA\nCLS last 3 0 0.234152 0.217878 NA\nCLS all 0 0.224458 0.214642 NA\nCLS all 1 0.219356 0.211842 NA\nCLS all 2 0.213312 0.2097 NA\nCLS all 3 0.206874 0.208715 NA\nCLS all 4 0.203129 0.208968 NA\nﬁnetuning LM: languagel model, CLS: classiﬁcation; unfree zing all: all layers, last X: last X layer groups\nT able 9: ULMFit (thai2ﬁt) Hyperparameter Tuning Results\n15\nA P RE P RIN T - M A RCH 23, 2021\ndatasets[features:labels] l1 penalty l2 penalty f1 micro f1 macro\nlst20[tokens:ner tags]\n0.5 0 0.717296 0.627721\n1 0 0.716445 0.622451\n0 0 0.688666 0.615289\n0 0.5 0.703625 0.602803\n0.5 0.5 0.699979 0.590872\n0 1 0.694041 0.586303\n1 0.5 0.692479 0.585022\n0.5 1 0.686875 0.580285\n1 1 0.679075 0.574405\nlst20[tokens:pos tags]\n1 0 0.952271 0.803645\n0.5 0 0.951856 0.80205\n0 0.5 0.950801 0.801114\n0.5 0.5 0.950444 0.798502\n1 0.5 0.949195 0.797815\n0.5 1 0.94809 0.796299\n0 1 0.948829 0.796092\n1 1 0.946645 0.795668\n0 0 0.934692 0.790179\nthainer[tokens:ner tags]\n0.5 0 0.810651 0.76763\n1 0 0.799159 0.770863\n0.5 0.5 0.776165 0.749212\n0 0.5 0.79007 0.745939\n1 0.5 0.762203 0.741308\n0 0 0.766292 0.739058\n0 1 0.770964 0.732445\n0.5 1 0.755218 0.727801\n1 1 0.742729 0.722583\nT able 10: CRF Hyperparameter Tuning Results\n16\nA P RE P RIN T - M A RCH 23, 2021\nT ag Precision Recall F1-score Support\nDA TE 0.8758 0.8221 0.8481 163\nEMAIL 1.0000 1.0000 1.0000 1\nLA W 0.9000 0.6000 0.7200 15\nLEN 0.9412 0.8000 0.8649 20\nLOCA TION 0.7747 0.6770 0.7226 452\nMONEY 1.0000 0.9138 0.9550 58\nORGANIZA TION 0.8550 0.7400 0.7934 550\nPERCENT 0.9375 0.9375 0.9375 16\nPERSON 0.8816 0.7941 0.8356 272\nPHONE 0.7500 0.6000 0.6667 10\nTIME 0.8154 0.6235 0.7067 85\nURL 1.0000 0.8571 0.9231 7\nZIP 1.0000 0.5000 0.6667 2\nMicro avg 0.8458 0.7408 0.7898 1651\nMacro avg 0.9024 0.7589 0.8185 1651\nW eighted avg 0.8450 0.7408 0.7889 1651\nT able 11: CRF – per-class precision, recall and F1-score on t est set of ThaiNER\nLST20 (POS) LST20 (NER)\nT ag Precision Recall F1-score Support\nNN 0.9699 0.9780 0.9740 58568\nVV 0.9567 0.9670 0.9618 42586\nPU 0.9999 0.9998 0.9999 37973\nCC 0.9485 0.9671 0.9577 17613\nPS 0.9413 0.9421 0.9417 10886\nAX 0.9514 0.9427 0.9470 7556\nA V 0.8881 0.7889 0.8356 6722\nFX 0.9955 0.9928 0.9941 6918\nNU 0.9684 0.9559 0.9621 6256\nAJ 0.8974 0.8506 0.8734 4403\nCL 0.8781 0.8422 0.8598 3739\nPR 0.8479 0.8523 0.8501 2139\nNG 1.0000 0.9953 0.9976 1694\nP A 0.8122 0.8918 0.8501 194\nXX 0.0000 0.0000 0.0000 27\nIJ 0.0000 0.0000 0.0000 4\nAccuracy 0.9628 207278\nMacro avg 0.8160 0.8104 0.8128 207278\nW eighted avg 0.9624 0.9628 0.9624 207278\nT ag Precision Recall F1-score Support\nBRN 0.4286 0.1915 0.2647 47\nDES 0.9090 0.8665 0.8872 1176\nDTM 0.7128. 0.6657 0.6884 1331\nLOC 0.7340. 0.6509 0.6900 2349\nMEA 0.6669. 0.6639 0.6654 3166\nNUM 0.6745. 0.6267 0.6497 1243\nORG 0.7772. 0.6682 0.7186 4261\nPER 0.9007. 0.8680 0.8840 3272\nTRM 0.8835. 0.7109 0.7879 128\nTTL 0.9673. 0.9862 0.9767 1379\nMicro avg 0.7873 0.7335 0.7594 18352\nMacro avg 0.7654 0.6898 0.7213 18352\nW eighted avg 0.7856 0.7335 0.7579 18352\nT able 12: CRF – per-class precision, recall and F1-score on t est set of LST20\n17\nA P RE P RIN T - M A RCH 23, 2021\nThaiNER (NER)\nT ag Precision Recall F1-score Support\nDA TE 0.2297 0.3988 0.2915 163\nEMAIL 1.0000 1.0000 1.0000 1\nLA W 0.2593 0.4667 0.3333 15\nLEN 0.1053 0.2000 0.1379 20\nLOCA TION 0.7635 0.8429 0.8013 452\nMONEY 0.1038 0.1897 0.1341 58\nORGANIZA TION 0.7496 0.8491 0.7962 550\nPERCENT 0.3077 0.5000 0.3810 16\nPERSON 0.2414 0.4118 0.3043 272\nPHONE 0.6923 0.9000 0.7826 10\nTIME 0.1824 0.3176 0.2318 85\nURL 1.0000 1.0000 1.0000 7\nZIP 1.0000 1.0000 1.0000 2\nMicro avg 0.4922 0.6669 0.5664 1651\nMacro avg 0.5104 0.6213 0.5534 1651\nW eighted avg 0.5511 0.6669 0.5994 1651\nT able 13: wangchanberta-thwiki-spm – per-class precision , recall and F1-score on test set of ThaiNER\nLST20 (POS) LST20 (NER)\nT ag Precision Recall F1-score Support\nAJ 0.8847 0.8378 0.8606 4403\nA V 0.8881 0.7885 0.8353 6722\nAX 0.9399 0.9423 0.9411 7556\nCC 0.9552 0.9601 0.9576 17613\nCL 0.8311 0.8804 0.8551 3739\nFX 0.9938 0.9929 0.9933 6918\nIJ 0.5000 0.5000 0.5000 4\nNG 0.9988 0.9953 0.9970 1694\nNN 0.9780 0.9706 0.9743 58568\nNU 0.9565 0.9735 0.9649 6256\nP A 0.7521 0.9072 0.8224 194\nPR 0.8082 0.8668 0.8365 2139\nPS 0.9348 0.9433 0.9391 10886\nPU 0.9998 0.9974 0.9986 37973\nVV 0.9532 0.9715 0.9623 42586\nXX 0.0000 0.0000 0.0000 27\nAccuracy 0.9618 207278\nMacro avg 0.8359 0.8455 0.8399 207278\nW eighted avg 0.9617 0.9618 0.9616 207278\nT ag Precision Recall F1-score Support\nBRN 0.2692 0.2979 0.2828 47\nDES 0.8658 0.8776 0.8716 1176\nDTM 0.6494 0.7055 0.6763 1331\nLOC 0.6523 0.7395 0.6931 2349\nMEA 0.6657 0.7505 0.7056 3166\nNUM 0.6673 0.5857 0.6238 1243\nORG 0.6978 0.7705 0.7323 4261\nPER 0.9186 0.9523 0.9352 3272\nTRM 0.6780 0.6250 0.6504 128\nTTL 0.9403 0.9826 0.9610 1379\nMicro avg 0.7453 0.7988 0.7712 18352\nMacro avg 0.7004 0.7287 0.7132 18352\nW eighted avg 0.7480 0.7988 0.7718 18352\nT able 14: wangchanberta-thwiki-spm – per-class precision , recall and F1-score on test set of LST20\n18\nA P RE P RIN T - M A RCH 23, 2021\nThaiNER (NER)\nT ag Precision Recall F1-score Support\nDA TE 0.2456 0.4233 0.3108 163\nEMAIL 0.0000 0.0000 0.0000 1\nLA W 0.2800 0.4667 0.3500 15\nLEN 0.1053 0.2000 0.1379 20\nLOCA TION 0.7933 0.8407 0.8163 452\nMONEY 0.1028 0.1897 0.1333 58\nORGANIZA TION 0.7764 0.8836 0.8265 550\nPERCENT 0.3200 0.5000 0.3902 16\nPERSON 0.2567 0.4228 0.3194 272\nPHONE 0.6429 0.9000 0.7500 10\nTIME 0.1985 0.3176 0.2443 85\nURL 1.0000 0.8571 0.9231 7\nZIP 1.0000 1.0000 1.0000 2\nMicro avg 0.5135 0.6808 0.5854 1651\nMacro avg 0.4401 0.5386 0.4771 1651\nW eighted avg 0.5725 0.6808 0.6177 1651\nT able 15: wangchanberta-thwiki-newmm – per-class precisi on, recall and F1-score on test set of ThaiNER\nLST20 (POS) LST20 (NER)\nT ag Precision Recall F1-score Support\nAJ 0.8882 0.8358 0.8612 4403\nA V 0.8888 0.7873 0.8350 6722\nAX 0.9281 0.9453 0.9367 7556\nCC 0.9526 0.9566 0.9546 17613\nCL 0.8339 0.8850 0.8587 3739\nFX 0.9941 0.9929 0.9935 6918\nIJ 0.5000 0.2500 0.3333 4\nNG 0.9994 0.9953 0.9973 1694\nNN 0.9773 0.9717 0.9745 58568\nNU 0.9528 0.9714 0.9620 6256\nP A 0.8037 0.9072 0.8523 194\nPR 0.8102 0.8719 0.8399 2139\nPS 0.9338 0.9407 0.9372 10886\nPU 0.9998 0.9972 0.9985 37973\nVV 0.9549 0.9701 0.9625 42586\nXX 0.0000 0.0000 0.0000 27\nAccuracy 0.9614 207278\nMacro avg 0.8386 0.8299 0.8311 207278\nW eighted avg 0.9613 0.9614 0.9612 207278\nT ag Precision Recall F1-score Support\nBRN 0.2075 0.2340 0.2200 47\nDES 0.8455 0.8793 0.8620 1176\nDTM 0.6442 0.7047 0.6731 1331\nLOC 0.6463 0.7288 0.6851 2349\nMEA 0.6627 0.7423 0.7002 3166\nNUM 0.6555 0.6307 0.6429 1243\nORG 0.6786 0.7590 0.7165 4261\nPER 0.9245 0.9511 0.9376 3272\nTRM 0.6587 0.6484 0.6535 128\nTTL 0.9483 0.9848 0.9662 1379\nMicro avg 0.7377 0.7964 0.7659 18352\nMacro avg 0.6872 0.7263 0.7057 18352\nW eighted avg 0.7411 0.7964 0.7673 18352\nT able 16: wangchanberta-thwiki-newmm – per-class precisi on, recall and F1-score on test set of LST20\n19\nA P RE P RIN T - M A RCH 23, 2021\nThaiNER (NER)\nT ag Precision Recall F1-score Support\nDA TE 0.8114 0.8712 0.8402 163\nEMAIL 0.0000 0.0000 0.0000 1\nLA W 0.7333 0.7333 0.7333 15\nLEN 0.8261 0.9500 0.8837 20\nLOCA TION 0.7792 0.8274 0.8026 452\nMONEY 0.8833 0.9138 0.8983 58\nORGANIZA TION 0.7800 0.8636 0.8197 550\nPERCENT 0.9375 0.9375 0.9375 16\nPERSON 0.8869 0.9228 0.9045 272\nPHONE 0.7143 1.0000 0.8333 10\nTIME 0.7527 0.8235 0.7865 85\nURL 0.8571 0.8571 0.8571 7\nZIP 1.0000 0.5000 0.6667 2\nMicro avg 0.8026 0.8643 0.8323 1651\nMacro avg 0.7663 0.7846 0.7664 1651\nW eighted avg 0.8041 0.8643 0.8327 1651\nT able 17: wangchanberta-thwiki-syllable – per-class prec ision, recall and F1-score on test set of ThaiNER\nLST20 (POS) LST20 (NER)\nT ag Precision Recall F1-score Support\nAJ 0.8901 0.8428 0.8658 4403\nA V 0.8899 0.7876 0.8356 6722\nAX 0.9382 0.9419 0.9400 7556\nCC 0.9495 0.9593 0.9544 17613\nCL 0.8367 0.8783 0.8570 3739\nFX 0.9936 0.9857 0.9896 6918\nIJ 0.5000 0.5000 0.5000 4\nNG 0.9976 0.9935 0.9956 1694\nNN 0.9753 0.9699 0.9726 58568\nNU 0.9550 0.9711 0.9630 6256\nP A 0.7719 0.9072 0.8341 194\nPR 0.8116 0.8560 0.8332 2139\nPS 0.9333 0.9395 0.9364 10886\nPU 0.9998 0.9976 0.9987 37973\nVV 0.9528 0.9700 0.9613 42586\nXX 0.0000 0.0000 0.0000 27\nAccuracy 0.9606 207278\nMacro avg 0.8372 0.8438 0.8398 207278\nW eighted avg 0.9605 0.9606 0.9604 207278\nT ag Precision Recall F1-score Support\nBRN 0.1837 0.1915 0.1875 47\nDES 0.8360 0.8801 0.8575 1176\nDTM 0.6479 0.7175 0.6809 1331\nLOC 0.6379 0.7335 0.6824 2349\nMEA 0.6602 0.7236 0.6905 3166\nNUM 0.6678 0.6420 0.6546 1243\nORG 0.6765 0.7641 0.7177 4261\nPER 0.9177 0.9514 0.9343 3272\nTRM 0.7207 0.6250 0.6695 128\nTTL 0.9429 0.9826 0.9624 1379\nMicro avg 0.7352 0.7964 0.7645 18352\nMacro avg 0.6891 0.7211 0.7037 18352\nW eighted avg 0.7384 0.7964 0.7658 18352\nT able 18: wangchanberta-thwiki-syllable – per-class prec ision, recall and F1-score on test set of LST20\n20\nA P RE P RIN T - M A RCH 23, 2021\nThaiNER (NER)\nT ag Precision Recall F1-score Support\nDA TE 0.8480 0.8896 0.8683 163\nEMAIL 0.0000 0.0000 0.0000 1\nLA W 0.5625 0.6000 0.5806 15\nLEN 0.8182 0.9000 0.8571 20\nLOCA TION 0.8038 0.8518 0.8271 452\nMONEY 0.9483 0.9483 0.9483 58\nORGANIZA TION 0.8242 0.8782 0.8504 550\nPERCENT 0.9375 0.9375 0.9375 16\nPERSON 0.8961 0.9191 0.9074 272\nPHONE 0.8750 0.7000 0.7778 10\nTIME 0.6970 0.8118 0.7500 85\nURL 0.7500 0.8571 0.8000 7\nZIP 1.0000 1.0000 1.0000 2\nMicro avg 0.8275 0.8746 0.8504 1651\nMacro avg 0.7662 0.7918 0.7773 1651\nW eighted avg 0.8290 0.8746 0.8509 1651\nT able 19: wangchanberta-thwiki-sefr – per-class precisio n, recall and F1-score on test set of ThaiNER\nLST20 (POS) LST20 (NER)\nT ag Precision Recall F1-score Support\nAJ 0.9143. 0.8188 0.8639 4403\nA V 0.8847. 0.8079 0.8446 6722\nAX 0.9688. 0.9277 0.9478 7556\nCC 0.9497. 0.9678 0.9586 17613\nCL 0.8479. 0.8869 0.8669 3739\nFX 0.9954. 0.9915 0.9934 6918\nIJ 1.0000. 0.5000 0.6667 4\nNG 0.9994. 0.9941 0.9967 1694\nNN 0.9763. 0.9747 0.9755 58568\nNU 0.9569. 0.9731 0.9650 6256\nP A 0.7344. 0.9124 0.8138 194\nPR 0.8346. 0.8518 0.8431 2139\nPS 0.9295. 0.9475 0.9384 10886\nPU 0.9999. 0.9987 0.9993 37973\nVV 0.9566. 0.9713 0.9639 42586\nXX 0.0000. 0.0000 0.0000 27\nAccuracy 0.9636 207278\nMacro avg 0.8718 0.8453 0.8524 207278\nW eighted avg 0.9634 0.9636 0.9633 207278\nT ag Precision Recall F1-score Support\nBRN 0.2857 0.1702 0.2133 47\nDES 0.8795 0.8631 0.8712 1176\nDTM 0.5623 0.7085 0.6270 1331\nLOC 0.6727 0.7263 0.6985 2349\nMEA 0.6378 0.7596 0.6934 3166\nNUM 0.7173 0.5551 0.6259 1243\nORG 0.6728 0.7813 0.7230 4261\nPER 0.9022 0.9560 0.9283 3272\nTRM 0.6476 0.5312 0.5837 128\nTTL 0.9609 0.9797 0.9702 1379\nMicro avg 0.7302 0.7979 0.7625 18352\nMacro avg 0.6939 0.7031 0.6934 18352\nW eighted avg 0.7364 0.7979 0.7636 18352\nT able 20: wangchanberta-thwiki-sefr – per-class precisio n, recall and F1-score on test set of LST20\n21\nA P RE P RIN T - M A RCH 23, 2021\nThaiNER (NER)\nT ag Precision Recall F1-score Support\nDA TE 0.8198 0.8650 0.8418 163\nEMAIL 0.0000 0.0000 0.0000 1\nLA W 0.5263 0.6667 0.5882 15\nLEN 0.8261 0.9500 0.8837 20\nLOCA TION 0.8143 0.8827 0.8471 452\nMONEY 0.7895 0.7759 0.7826 58\nORGANIZA TION 0.8777 0.9000 0.8887 550\nPERCENT 0.9375 0.9375 0.9375 16\nPERSON 0.8897 0.9485 0.9181 272\nPHONE 1.0000 1.0000 1.0000 10\nTIME 0.7500 0.7765 0.7630 85\nURL 0.8571 0.8571 0.8571 7\nZIP 1.0000 1.0000 1.0000 2\nMicro avg 0.8430 0.8879 0.8649 1651\nMacro avg 0.7760 0.8123 0.7929 1651\nW eighted avg 0.8439 0.8879 0.8652 1651\nT able 21: wanchanberta-base-att-spm-uncased – per-class precision, recall and F1-score on test set of ThaiNER\nLST20 (POS) LST20 (NER)\nT ag Precision Recall F1-score Support\nAJ 0.9027 0.8685 0.8853 4403\nA V 0.9163 0.7849 0.8455 6722\nAX 0.9494 0.9464 0.9479 7556\nCC 0.9561 0.9611 0.9585 17613\nCL 0.8430 0.8930 0.8673 3739\nFX 0.9949 0.9928 0.9938 6918\nIJ 0.4286 0.7500 0.5455 4\nNG 1.0000 0.9970 0.9985 1694\nNN 0.9819 0.9744 0.9781 58568\nNU 0.9685 0.9823 0.9753 6256\nP A 0.7662 0.9124 0.8329 194\nPR 0.8240 0.9018 0.8612 2139\nPS 0.9383 0.9486 0.9434 10886\nPU 0.9999 0.9999 0.9999 37973\nVV 0.9566 0.9765 0.9665 42586\nXX 1.0000 0.0370 0.0714 27\nAccuracy 0.9662 207278\nMacro avg 0.9016 0.8704 0.8544 207278\nW5eighted avg 0.9663 0.9662 0.9660 207278\nT ag Precision Recall F1-score Support\nBRN 0.2424 0.1702 0.2000 47\nDES 0.8724 0.8776 0.8749 1176\nDTM 0.6307 0.6762 0.6526 1331\nLOC 0.7107 0.7322 0.7213 2349\nMEA 0.6390 0.7015 0.6688 3166\nNUM 0.6641 0.6251 0.6440 1243\nORG 0.7436 0.7834 0.7630 4261\nPER 0.9364 0.9630 0.9495 3272\nTRM 0.8505 0.7109 0.7745 128\nTTL 0.9640 0.9898 0.9767 1379\nMicro avg 0.7651 0.7957 0.7801 18352\nMacro avg 0.7254 0.7230 0.7225 18352\nW eighted avg 0.7664 0.7957 0.7805 18352\nT able 22: wanchanberta-base-att-spm-uncased – per-class precision, recall and F1-score on test set of LST20\n22\nA P RE P RIN T - M A RCH 23, 2021\nThaiNER (NER)\nT ag Precision Recall F1-score Support\nDA TE 0.8258 0.9018 0.8622 163\nEMAIL 0.0000 0.0000 0.0000 1\nLA W 0.6471 0.7333 0.6875 15\nLEN 0.7391 0.8500 0.7907 20\nLOCA TION 0.7571 0.8208 0.7877 452\nMONEY 0.9298 0.9138 0.9217 58\nORGANIZA TION 0.7905 0.8436 0.8162 550\nPERCENT 0.6000 0.7500 0.6667 16\nPERSON 0.8551 0.8897 0.8721 272\nPHONE 0.9091 1.0000 0.9524 10\nTIME 0.6019 0.7294 0.6596 85\nURL 0.8750 1.0000 0.9333 7\nZIP 1.0000 0.5000 0.6667 2\nMicro avg 0.7857 0.8462 0.8148 1651\nMacro avg 0.7331 0.7640 0.7397 1651\nW eighted avg 0.7878 0.8462 0.8155 1651\nT able 23: mBER T – per-class precision, recall and F1-score o n test set of ThaiNER\nLST20 (POS) LST20 (NER)\nT ag Precision Recall F1-score Support\nAJ 0.9112 0.8626 0.8862 4403\nA V 0.9177 0.7792 0.8428 6722\nAX 0.9525 0.9453 0.9489 7556\nCC 0.9598 0.9654 0.9626 17613\nCL 0.8140 0.9128 0.8606 3739\nFX 0.9958 0.9928 0.9943 6918\nIJ 1.0000 0.5000 0.6667 4\nNG 1.0000 0.9953 0.9976 1694\nNN 0.9791 0.9700 0.9745 58568\nNU 0.9655 0.9783 0.9718 6256\nP A 0.8178 0.9021 0.8578 194\nPR 0.8023 0.9392 0.8654 2139\nPS 0.9357 0.9612 0.9483 10886\nPU 0.9997 0.9979 0.9988 37973\nVV 0.9547 0.9693 0.9620 42586\nXX 0.0000 0.0000 0.0000 27\nAccuracy 0.9644 207278\nMacro avg 0.8754 0.8545 0.8586 207278\nW eighted avg 0.9648 0.9644 0.9643 207278\nT ag Precision Recall F1-score Support\nBRN 0.1923 0.1064 0.1370 47\nDES 0.8680 0.8776 0.8727 1176\nDTM 0.5735 0.6950 0.6284 1331\nLOC 0.6591 0.7407 0.6975 2349\nMEA 0.5751 0.7290 0.6430 3166\nNUM 0.6737 0.4883 0.5662 1243\nORG 0.7245 0.7517 0.7378 4261\nPER 0.9019 0.9248 0.9132 3272\nTRM 0.7849 0.5703 0.6606 128\nTTL 0.9750 0.9616 0.9682 1379\nMicro avg 0.7264 0.7762 0.7505 18352\nMacro avg 0.6928 0.6845 0.6825 18352\nW eighted avg 0.7347 0.7762 0.7519 18352\nT able 24: mBER T – per-class precision, recall and F1-score o n test set of LST20\n23\nA P RE P RIN T - M A RCH 23, 2021\nThaiNER (NER)\nT ag Precision Recall F1-score Support\nDA TE 0.8047 0.8344 0.8193 163\nEMAIL 0.0000 0.0000 0.0000 1\nLA W 0.4375 0.4667 0.4516 15\nLEN 0.7895 0.7500 0.7692 20\nLOCA TION 0.7761 0.8053 0.7904 452\nMONEY 0.9310 0.9310 0.9310 58\nORGANIZA TION 0.7977 0.8964 0.8442 550\nPERCENT 0.7222 0.8125 0.7647 16\nPERSON 0.9078 0.9412 0.9242 272\nPHONE 0.9091 1.0000 0.9524 10\nTIME 0.7561 0.7294 0.7425 85\nURL 0.6667 0.8571 0.7500 7\nZIP 0.0000 0.0000 0.0000 2\nMicro avg 0.8087 0.8577 0.8325 1651\nMacro avg 0.6537 0.6942 0.6723 1651\nW eighted avg 0.8077 0.8577 0.8315 1651\nT able 25: XLMR – per-class precision, recall and F1-score on test set of ThaiNER\nLST20 (POS) LST20 (NER)\nT ag Precision Recall F1-score Support\nAJ 0.8750 0.8858 0.8804 4403\nA V 0.8831 0.8026 0.8409 6722\nAX 0.9686 0.9275 0.9476 7556\nCC 0.9514 0.9742 0.9626 17613\nCL 0.8603 0.8874 0.8736 3739\nFX 0.9949 0.9929 0.9939 6918\nIJ 0.5000 0.5000 0.5000 4\nNG 0.9994 0.9970 0.9982 1694\nNN 0.9836 0.9705 0.9770 58568\nNU 0.9750 0.9771 0.9760 6256\nP A 0.8812 0.9175 0.8990 194\nPR 0.8753 0.8069 0.8397 2139\nPS 0.9420 0.9522 0.9471 10886\nPU 0.9998 1.0000 0.9999 37973\nVV 0.9512 0.9778 0.9643 42586\nXX 0.0000 0.0000 0.0000 27\nAccuracy 0.9657 207278\nMacro avg 0.8526 0.8481 0.8500 207278\nW eighted avg 0.9656 0.9657 0.9655 207278\nT ag Precision Recall F1-score Support\nBRN 0.2258 0.1489 0.1795 47\nDES 0.7478 0.8852 0.8107 1176\nDTM 0.5643 0.6627 0.6095 1331\nLOC 0.6714 0.7029 0.6868 2349\nMEA 0.5479 0.4972 0.5213 3166\nNUM 0.5900 0.7651 0.6662 1243\nORG 0.7105 0.7759 0.7418 4261\nPER 0.8890 0.9520 0.9194 3272\nTRM 0.8017 0.7266 0.7623 128\nTTL 0.9556 0.9840 0.9696 1379\nMicro avg 0.7123 0.7616 0.7361 18352\nMacro avg 0.6704 0.7100 0.6867 18352\nW eighted avg 0.7107 0.7616 0.7339 18352\nT able 26: XLMR – per-class precision, recall and F1-score on test set of LST20\n24"
}