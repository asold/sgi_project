{
    "title": "MDMMT: Multidomain Multimodal Transformer for Video Retrieval",
    "url": "https://openalex.org/W3139129996",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4221631067",
            "name": "Dzabraev, Maksim",
            "affiliations": [
                "Moscow State University",
                "Lomonosov Moscow State University"
            ]
        },
        {
            "id": "https://openalex.org/A4221631066",
            "name": "Kalashnikov, Maksim",
            "affiliations": [
                "Moscow State University",
                "Lomonosov Moscow State University"
            ]
        },
        {
            "id": null,
            "name": "Komkov, Stepan",
            "affiliations": [
                "Lomonosov Moscow State University",
                "Moscow State University"
            ]
        },
        {
            "id": "https://openalex.org/A3189776739",
            "name": "Petiushko Aleksandr",
            "affiliations": [
                "Lomonosov Moscow State University",
                "Moscow State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2425121537",
        "https://openalex.org/W6676647902",
        "https://openalex.org/W2016053056",
        "https://openalex.org/W2963916161",
        "https://openalex.org/W2963293463",
        "https://openalex.org/W2984008963",
        "https://openalex.org/W2753311918",
        "https://openalex.org/W6684090549",
        "https://openalex.org/W6794013807",
        "https://openalex.org/W2808399042",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W3129719298",
        "https://openalex.org/W2968848930",
        "https://openalex.org/W2803088946",
        "https://openalex.org/W3122640483",
        "https://openalex.org/W1572567476",
        "https://openalex.org/W2732026016",
        "https://openalex.org/W2952138345",
        "https://openalex.org/W2883429621",
        "https://openalex.org/W2565656701",
        "https://openalex.org/W2949380904",
        "https://openalex.org/W3035635319",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W3034875620",
        "https://openalex.org/W2796207103",
        "https://openalex.org/W2963703197",
        "https://openalex.org/W2526286384",
        "https://openalex.org/W2164290393",
        "https://openalex.org/W2931316642",
        "https://openalex.org/W2769342113",
        "https://openalex.org/W2619947201",
        "https://openalex.org/W2980037812",
        "https://openalex.org/W2965458216",
        "https://openalex.org/W2308045930",
        "https://openalex.org/W2975813532",
        "https://openalex.org/W2625366777",
        "https://openalex.org/W2990503944",
        "https://openalex.org/W2949650786",
        "https://openalex.org/W3043840704",
        "https://openalex.org/W2961193895",
        "https://openalex.org/W2112912048",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3102887392",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2948859046",
        "https://openalex.org/W3158782016"
    ],
    "abstract": "We present a new state-of-the-art on the text to video retrieval task on MSRVTT and LSMDC benchmarks where our model outperforms all previous solutions by a large margin. Moreover, state-of-the-art results are achieved with a single model on two datasets without finetuning. This multidomain generalisation is achieved by a proper combination of different video caption datasets. We show that training on different datasets can improve test results of each other. Additionally we check intersection between many popular datasets and found that MSRVTT has a significant overlap between the test and the train parts, and the same situation is observed for ActivityNet.",
    "full_text": "MDMMT: Multidomain Multimodal Transformer for Video\nRetrieval\nA PREPRINT\nMaksim Dzabraev1,2, Maksim Kalashnikov1, Stepan Komkov1,2, Aleksandr Petiushko1,2\n1Lomonosov Moscow State University\n2Huawei Moscow Research Center\ndzabraev.maksim@intsys.msu.ru, kalashnikov.maxim@intsys.msu.ru,\nstepan.komkov@intsys.msu.ru, petyushko.alexander1@huawei.com\nABSTRACT\nWe present a new state-of-the-art on the text to video re-\ntrieval task on MSRVTT and LSMDC benchmarks where\nour model outperforms all previous solutions by a large\nmargin. Moreover, state-of-the-art results are achieved\nwith a single model on two datasets without ﬁnetuning.\nThis multidomain generalisation is achieved by a proper\ncombination of different video caption datasets. We show\nthat training on different datasets can improve test results\nof each other. Additionally we check intersection between\nmany popular datasets and found that MSRVTT has a sig-\nniﬁcant overlap between the test and the train parts, and\nthe same situation is observed for ActivityNet.\nKeywords video, language, retrieval, multi-modal,\ncross-modal, temporality, transformer, attention\n1 Introduction\nVideo is a quite popular data format, 500+ hours of video\nare uploaded on YouTube every minute. Many personal\nmobile phones have gigabytes of video. Since video format\ngets more popular every year the importance of modern\nsearch methods is increasing as well.\nIn this work we present our research about text to video\nretrieval task. In this task system should return for a given\ntextual query the most relevant video segments from a\ngallery. The query is a textual description of what we\nwant to ﬁnd in the video. The query may describe objects,\nactions, sounds, ..., and relations between them.\nSuch search methods are a promising direction for mobile\ndevices because every year manufacturers increase the\navailable memory on devices. The large part of the memory\nis ﬁlled by media data. For end users it is getting difﬁcult\nto search for a video made one or two years ago. But users\ncan easily describe the content of the video using natural\nlanguage, which can be effectively used as a search query.\nThere are two major directions which allow calculate the\nrelevance between a textual search query and a video seg-\nment. The ﬁrst direction is single stream approaches [32],\nwhere a query and a video together are given to a net-\nwork and then become fused from the beginning of the\nprocessing. The schematic illustration of this approach is\npresented in Fig. 1a.\n(a) Scheme for a single-stream\nneural network.\n(b) Scheme for a two-stream\nneural network.\nFigure 1: Two types of fusion\nThis type of approaches have access to all input data from\nthe beginning of its processing and can make a strong\nverdict about data. But these approaches have a signiﬁcant\ndrawback because it is not scalable: every new query the\nsearch system should calculate the full forward pass for\nthis query and for each video segment from the gallery.\nAnother direction is two stream neural networks [24], [8],\nwhere a textual query and a video are processed by two\ndifferent neural networks. As a result the networks pro-\nduce embeddings inside the same embedding space, where\nsemantically close textual queries and video segments will\nbe placed next to each other. The schematic illustration is\npresented in Fig. 1b.\nThe two stream approach is scalable, it allows to precom-\npute video embeddings for all videos from the gallery,\nand to do only one forward pass with the text network for\narXiv:2103.10699v1  [cs.CV]  19 Mar 2021\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\neach new query and then to compute the cosine similarity\nbetween the new query embedding and all precomputed\nembeddings.\nTo make a strong video retrieval solution it is important to\nshow to the model a lot of situations, actions and objects\nfrom real life. There exist a lot of video datasets, but none\nof them cover a signiﬁcant portion of real life situations.\nOne of the ﬁrst steps to tackle this problem is to formulate\nthe rules for combining different existing datasets to a\nsingle large train database.\nText to video retrieval is a modern direction, where one\nof the ﬁrst works was published at 2016 [33]. One of the\nmost universal solution for video retrieval task is Multi\nModal Transformer [8] architecture which uses BERT [4]\nbackbone for a video network. It allows in a natural way to\nprocess the temporal dependencies inside the multi modal\ndata source.\nTo train a text to video retrieval neural network the training\ndatabase should consists of pairs: (a video segment, a\ntextual description of this video segment). Traditionally\nsuch sort of datasets was created for a video captioning\ntask. But it turns out that these datasets perfectly can\nbe used for a video retrieval task. One of the ﬁrst video\ncaptioning dataset was MSVD, which was created in 2010.\nToday there exist more than a dozen of different video\ncaptioning datasets.\nThe most popular datasets for text to video retrieval is\nMSRVTT [39], ActivityNet [17] and LSMDC [29]. Many\nresearchers test their solutions mostly on these three\ndatasets.\nOur main contributions in this work are the following:\n• We present a new state-of-the-art (SotA) result on\nMSRVTT and LSMDC benchmarks;\n• We present a model which shows good results on three\ndifferent benchmarks without ﬁnetuning: MSRVTT\n(SotA), LSMDC (SotA) and ActivityNet at the same\ntime;\n• We present a practical approach which helps us to ﬁnd\nthe overlap between the train and the test parts of used\ndatasets.\n2 Related work\n2.1 Datasets\nMSRVTT [39] was created in 2016. This dataset is tradi-\ntionally used by researchers as the main dataset for testing\ntext to video retrieval models. This dataset consists of\n10k video segments, each segment has 20 captions. The\nauthors collected 257 popular search queries and gathered\nfrom YouTube 118 most relevant videos for each of them.\nThe dataset has 42 hours of video. The captions were made\nby 1327 amazon workers.\nToday there are three different test/train splits. The ofﬁcial\nsplit is called full split, where the train part has 7k videos\nand the test part has 3k videos. There are two important\nproperties of this split: 1. there are no two video segments\ncropped from the same video so as the ﬁrst segment is\nplaced in the train part and the second segment is placed in\nthe test part; 2. there are no two video segments, retrieved\nfrom the same query so as the ﬁrst one is placed in the\ntrain part and the second one is placed in the test part.\nAnother two splits are called 1k-A [40] (sometimes called\njsfusion) and 1k-B [21] (sometimes called miech). Both\nof them have different 1k videos for testing. They were\ncreated by randomly sampling 1k videos from the original\ntest part (full split). 1k-A train part consists of the original\ntrain split and the rest of the videos from the test part, so it\nhas 1k videos for the test part and 9k videos for the train\npart. 1k-B has 1k videos for the test part and 6.5k videos\nfor the train. Additionally both splits use only one caption\nper segment (instead of 20 captions).\nUnfortunately 1k-A and 1k-B mixed up the train and test\nparts. This led to violation in properties 1. and 2. which\nthe full split satisﬁes.\nAnother problem is that all these splits have the overlap\nbetween the test and train parts, see C.2 for details. To\nbe strict we remove the overlap between the test part and\nthe train part of MSRVTT full split. We called this split\nMSRVTT full clean, and refer to it as M c. It is worth\nto mention that we do not modify the test part, we only\nremove some videos from the train part.\nThe Large Scale Movie Description Challenge\n(LSMDC) [29] is the extension of two independent\ndatasets: MPII Movie Description Dataset (MPII-\nMD) [28], and Montreal Video Annotation Dataset\n(M-V AD) [34].\nVideo segments for this dataset were cropped from movies,\nwhere movie textualized transcriptions were used as cap-\ntions. A movie transcription is an audio description of a\nvideo segment that helps blind people to watch movies by\ndescribing what happens, who appears in this time, what\nis on background right now and so on.\nIn this work for testing we use LSMDC public test, which\nconsists of 1k video segments.\nActivityNet captions dataset [17] consists of 20k videos\nand 100k captions, where captions cover the full video\nlength for the most of videos, and neighbour captions may\nintersect. The annotations were made with Amazon Me-\nchanical Turk.\nThe situation when some video segments may overlap\nmakes a problem for text to video retrieval testing. Suppose\nwe have two video-caption pairs (S1,C1) and (S2,C2)\nwhere the video segment S1 has a non empty overlap with\nthe video segment S2. Now suppose that for query C1 the\nsystem returns the video segment S2. Is it mistake or not?\nWhat to do in this case?\nMany previous works used ActivityNet test dataset in a\nparagraph retrieval mode. In this mode all captions for all\n2\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nvideo segments are concatenated, then the concatenated\ntext is used as a textual query and the whole video should\nbe retrieved for this query. Such mode has two drawbacks.\nThe ﬁrst one is that paragraph retrieval is not a classical\nvideo retrieval mode, it is another task. One can ask: if\na model is good in paragraph retrieval will it be good\nfor video retrieval? The second drawback is that queries\nwill be long, video segments will be long (compared to\na classical video retrieval mode). This issue requires to\nenlarge the input for the model.\nAnother way to use the test part of ActivityNet is just to\nsample once a single random segment from each video. As\na result we will have non intersected video segments and\ncaptions with usual length. We use ActivityNet test part\nin this way. We take all videos from val1 and val2 parts,\nand sample a single random segment from each video. All\nresults on ActivityNet are reported on this split.\nAdditionally in this work the following datasets are used:\nNIST TRECVID Twitter vines [1], TGIF [18], MSVD [2],\nYouCook2 [43], Something-something V2 [10], Kinetics\n700 [31], HowTo100M [23].\n2.2 Prior Art\nA dominant approach to train video retrieval models is\ncontrastive learning. The idea of this approach is that we\nhave a set of pairs (videoi,texti) and elements of each\npair should be placed next to each other in some metric\nspace: distance(videoi,texti) →0, at the same time the\nelement videoi should be far from all other textj),j ̸= i:\ndistance(videoi,textj) →+∞. The bi-directional max-\nmargin ranking loss [13] represents this idea.\nWhen training data have a lot of noise the MIL NCE\nloss [22] can be applied in the training procedure. Sup-\npose that we know that a videoi should be close to one of\n(or several) texts texti1, ..., textik. This approach tries to\nreduce the distance between the video i and all texti1, ...,\ntextik at the same time.\nAll video captions datasets have the following problem.\nSuppose the distance between (videoi,texti) is to be min-\nimized while the distance between (videoi,textj),j ̸= i\nis to be maximized, but texti and textj are quite similar\n(from the semantical point of view). Maybe the optimal\nscenario in this situation is to minimize the distance be-\ntween (videoi,textj),j ̸= i. In [25] the authors show the\napproach which deals with this problem.\nAs far as an input video is the temporal sequence of tokens\n(frames or video segments) it is important to efﬁciently\naggregate the information from all tokens. Many ideas for\nsuch aggregation in the previous works are borrowed from\nthe natural language processing. Convolution ﬁlters for\naggregation are used in [25], a transformer encoder as a\nvideo aggregator is used in [8], many different aggregation\nfunctions are tested in [26].\nWe think that the most promising aggregation method is\nMulti Modal Transformer (MMT) [8]. MMT is a two\nstream solution designed for a text to video retrieval task.\nThe extraction of features from the input video stream is\ndone in the following way. An input video is preprocessed\nby several pretrained frozen neural networks (these net-\nworks are called experts). Original solution uses seven\nmodalities: motion, RGB, scene, face, OCR, speech, au-\ndio, and one pretrained network for each modality is used.\nThe motion modality is processed with video recognition\nnetworks like S3D, SlowFast, irCSN, where several input\nframes are used as a single input. The RGB modality uses\na single frame as an input. The audio modality uses the raw\ninput sound from a video. After embeddings are extracted\nfrom input data by these experts, it will be augmented by\nadding positional encoding tokens (representing time) and\nexpert tokens. Then the augmented embeddings are passed\nthrough MMT backbone. MMT backbone is a standard\ntransformer encoder architecture. Each input modality pro-\nduces one embedding, so in total there are seven output\nembedding from MMT.\nFor encoding the textual query the authors use pretrained\nBERT model where the output [CLS] token is used. The\noutput is postprocessed with shallow networks (one net-\nwork per modality) to extract the modality related infor-\nmation, in total seven feature vectors will be produced. In\naddition to embeddings from the text query seven weights\nrepresenting how much the query describes one of seven\nmodalities are produced. For example, if a query does\nnot represent the sound, the small weight for the audio\nmodality should be produced.\nThe ﬁnal similarity score is done by a sum of seven\nweighted dot products of embeddings.\nThe MMT is trained with the bi-directional max-margin\nranking loss [13]:\n1\nB\nB∑\ni=1\n∑\nj̸=i\n[\nmax(0,sij−sii+m)+max(0,sji−sii+m)\n]\nwhere B,sij,m represent the batch size, the similarity\nbetween the i-th query and the j-th video inside this batch,\nand some predeﬁned margin correspondingly.\n3 Methodology\nOur work is mostly based on MMT. We use the same loss\nand a similar architecture, but with different hyperparame-\nters. In this work we study the following questions:\n• Which publicly available pretrained motion expert is the\nbest for text to video retrieval nowadays, Sec. 3.1.\n• How to combine several video caption datasets in or-\nder to train a strong model without specialisation for a\nparticular dataset, Sec. 3.2.\n• How to ﬁnd and prevent the overlap between the test and\ntrain parts when combining datasets, Sec. C.\n3\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\n3.1 Motion experts\nThe MMT video backbone does not process the raw in-\nput video stream, and instead the input video stream is\nprocessed by one or more pretrained experts, where each\nexpert produces time series of features. The most impor-\ntant modality is motion: a motion expert processes several\nvideo frames as a single input unit and extracts the infor-\nmation about actions and objects within a segment.\nWe may say that the motion modality is the basis of the\nMMT. If a motion expert doesn’t extract some information,\nthere is a high probability that MMT won’t know about\nsome events in the video stream. That’s why improving\nthe motion expert is very important.\nWe consider several best solutions from Kinetics [15]\nbenchmark as well as several promising video recogni-\ntion models and check which one works in the best way as\na motion expert. We present all details in Sec. 4.2.\n3.2 Dataset creation\nIt is possible to train a video retrieval model by two means.\nThe ﬁrst way is the way of specialization for a single\ndomain. For example: create model that will work good\nonly for MSRVTT benchmark (or domain) but at the same\ntime this model will show poor results on other datasets\n(domains). In this way MMT [8] was trained. The authors\ntrained three different models for MSRVTT, ActivityNet\nand LSMDC datasets. Each of these three networks works\ngood on domain X if and only if it was trained on X, but\nat the same time works poor on another domain Y ̸= X.\nA proof of this statement we provide in Tab. 6.\nThe second way is to create a model that will work good\nfor all domains at the same time. We use this way.\nObviously the model trained in the ﬁrst way can’t work\ngood with real users, because the event when a user writes\na search query similar to some caption from a small train\ndatabase is very rare.\nThe second drawback here is that each video retrieval train\ndataset is not that big, and it causes the situation that model\ndoesn’t see many words and real life situations during\ntraining. For example, MSRVTT has only 9k videos and\n200k captions in total for training, obviously this is not\nenough to train a neural network that will know most of\nreal life situations, different items and persons. To tackle\nwith this problem we can take several datasets with videos\nand captions and concatenate it.\nDifferent datasets have the different number of videos, the\ndifferent number of captions per video, some datasets may\nhave long captions, some may have short captions, differ-\nent rules for creating captions were used by human writers,\nand so on. Due to these factors some datasets may contain\nmore information and require longer training time, some\ndatasets may contain less information and require shorter\ntraining time. On the other hand, if we use long training\ntime for a small dataset it could lead to overﬁtting on this\ndataset (the data will be memorized). The \"information\nsizes\" of some used datasets are illustrated in Fig. 2.\nnumber of unique captions\ntotal video duration\nMSRVTT\nActivityNet\nLSMDC\nVines\nYC2\nTGIF\nMSVD\n10K 50K 100K 166K\n466h\n102h\n41h\nFigure 2: Radius of the ball represent the “information\nsize” of dataset. The biggest balls have more diversity in\ndata.\nFig. 2 is made with a simple algorithm. We take the orig-\ninal training procedure of MMT and for a given dataset\nwe change the number of examples that will be shown to a\nnetwork during training. We deﬁne the radius of the ball\nas the number of training examples after which the perfor-\nmance gets saturated (i.e. increasing the training time does\nnot give the better model).\nThe key question is: what is the proper way for sampling\nexamples from several datasets taking into account the\ndifferent information size?\nWe use these obvious rules:\n1. If a dataset X is larger than Y, we should sample from\nX more often than from Y;\n2. Training on X and Y combined requires longer train\nthan training solely on X or Y;\n3. Training on X and Y combined may require a deeper\nmodel than for X or Y.\nIf we achieve the same results on X after combining X\nand Y it is still good because model gets better on Y.\nOur experiments show that the proper usage of rules 1–3\noften improves the results for a speciﬁc test dataset (e.g.\nMSRVTT) after extending the train dataset.\nWe managed to combine the following datasets: MSRVTT,\nActivityNet, LSMDC, TwitterVines, YouCook2, MSVD,\nTGIF and Something to something V2 (SomethingV2). In\ntotal we increase the number of video segments by 40 times\nand the number of unique captions by 4 times compared\nwith MSRVTT dataset. In Tab. 1 we summarize the sizes of\nused datasets. We separate SomethingV2 dataset from all\nother datasets because: 1. all video segments are created\nartiﬁcially, 2. the structure of text captions is quite limited.\nAt the same time videos for all other datasets are collected\n4\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nfrom the Internet and captions being created by humans\nhave quite a rich structure.\nDataset\nNum Num Num Has\nvideo pairs unique YouTube\ncaptions Id\nMSRVTT 10k 200k 167k Yes\nActivityNet 14k 70k 69k Yes\nLSMDC 101k 101k 101k No\nTwitterVines 6.5k 23k 23k No\nYouCook2 1.5k 12k 12k Yes\nMSVD 1.5k 80k 64k Yes\nTGIF 102k 125k 125k No\nSum above 236k 611k 561k —\nSomethingV2 193k 193k 124k No\nSum above 429k 804k 685k —\nTable 1: The \"Num video\" column represents the number\nof video clips in the dataset, the \"Num pairs\" column rep-\nresents the total number of video-caption pairs, the \"Num\nunique captions\" column represents the number of unique\ncaptions in the dataset.\n3.3 Intersection\nIt is important to extend the training database carefully, not\nallowing the addition to the train part of video segments\nthat already exist in the test part.\nTo ﬁnd the intersection between the test part and the train\npart we use the two stage ﬁltration. The ﬁrst stage is to use\nthe YouTube ID, if it is available. We should not allow to\nuse in the test and train parts simultaneously any two video\nsegments sampled from the same video. In the second\nstage we compute the similarity score between each video\nfrom the test part and each video from the train part, then\nwe manually assess the pairs with the highest scores. In\ntotal we assessed more than 100K pairs of the most relevant\nsegments, see Sec. C.1 for details.\nWe found the signiﬁcant overlap between the MSRVTT\n1k-A test and train parts, and the similar situation is with\nthe 1k-B test and train parts and the less signiﬁcant overlap\nis found between the MSRVTT full split test and train\nparts. The similar situation is with the ActivityNet train\nand validation 1,2 parts.\nAdditionally we estimate (but did not ﬁnd) the overlap\nbetween HowTo100M and MSRVTT, and found that it\nmay be signiﬁcant. Our approach allows to approximately\nestimate the total number of videos in the intersection\nwithout ﬁnding the exact intersection, please see the details\nin Sec. C.1.2. The similar estimation is for ActivityNet\nand Kinetics700, an our approximation shows that there\nmay be a signiﬁcant overlap, see all details in Sec. C.1.\nAbbreviate Composition\nM MSRVTT full split\nMc MSRVTT full clean split, see Sec. 2.1\nM1k-A MSRVTT 1k-A split\nM1k-B MSRVTT 1k-B split\nA ActivityNet\nAval1 ActivityNet val1 validation set\nAval2 ActivityNet val2 validation set\nAp/r ActivityNet paragraph retrieval, see Sec. 2.1\nL LSMDC\nK Kinetics700\nV Twitter Vines\nY YouCook2\nHT100M HowTo100M\nMALV MSRVTT + ActivityNet +\nLSMDC + TwitterVines\nMALVYMT\nMSRVTT + ActivityNet +\nLSMDC + TwitterVines +\nYouCook2 + MSVD + TGIF\nMALVYMTS\nMSRVTT + ActivityNet +\nLSMDC + TwitterVines +\nYouCook2 + MSVD + TGIF +\nSomething to Something V2\nTable 2: The left column represents the abbreviate name\nfor the set of datasets from the right column.\n4 Experiments\n4.1 Architecture\nWe use exactly the same neural network architecture as\noriginal MMT [8], our method is signiﬁcantly based on\ntheir codebase. The difference is in the following: 1. we\nuse the more aggressive dropout equals to 0.2 for the text\nBERT and the video BERT (against the original value of\n0.1); 2. we found that the deeper and wider transformer\nencoder for a video network gives better results — we use\n6 layers and 8 heads for the motion only modality and 9\nlayers and 8 heads for the motion + audio setting (against\n4 layer and 4 head in the original implementation).\n4.2 Stronger motion experts\nAs the input data for MMT is embeddings from experts,\nthe obvious question can arise: if a better expert is used,\nwill we have a stronger model? To answer this question\nwe train MMT on MSRVTT dataset with the only motion\nmodality. For motion experts we try several architectures\npretrained on different datasets, these models are presented\nin Tab. 3. We take the architectures which show the best re-\nsults on Kinetics 400 benchmark having publicly available\npretrained weights: [38] [7] [36] [35] [9] .\nThe results in Tab. 3 are made with the same hyperparame-\nters as in [8]. For the train dataset we use only MSRVTT\n5\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nfull clean split. The ﬁrst line in Tab. 3 represents the motion\nfeature extractor from the original MMT paper.\nAs we can see, usually stronger models provide better\nresults, but not always. Refer to r(2+1)d 152 rows, this\nnetwork demonstrates one of the best performance on Ki-\nnetics 400 benchmark, but works poorly as motion expert.\nMaybe this network is over specialized for Kinetics 400.\nMore shallow analogue of r(2+1)d 152 is r(2+1)d 34 which\nshows much better results.\nAn interesting observation is that the best results are\nachieved with the networks trained in the unsupervised\nmanner. CLIP and models trained on IG65M outperform\nall other models trained on Kinetics in the supervised man-\nner. Another weakly supervised dataset is Sports1M [14].\nModels trained on this dataset provide weak embeddings\nsimilar to the weak s3d model trained on Kinetics dataset.\nThe CLIP [27] (ViT-B/32) image feature extractor with\na large margin outperforms all other models. The model\ns3dg MIL-NCE is a video encoder from the work [22].\nThis network was trained from scratch on HowTo100M\ndataset.\nAs we show in Sec. C Kinetics dataset has an overlap with\nMSRVTT dataset, and we don’t know whether it affects to\noverﬁtting or not. Also it is worth to mention that IG65M\nand CLIP datasets are not publicly available, so we do not\nknow if there is an overlap with MSRVTT and other video\nretrieval datasets.\nFor more details about our usage of pretrained video ex-\nperts please refer to Sec. A.\n4.3 Datasets combination\nIn this section we show our experiments about the com-\nbination of different datasets. Nowadays video-caption\ndatasets are not big enough to capture all real life situa-\ntions, also some datasets may be biased. The combination\nof different datasets may help to tackle this problem.\nOur experiments show that the proper combination of\ndatasets allows to train a single model that can capture\nthe knowledge from all used datasets. Important thing here\nis that in most cases the model trained on the combination\nof datasets is better than the model trained on a single\ndataset.\nIn our experiments we combine all datasets presented in\nTab. 5. The important thing is how to sample minibatches\nduring training. In our experiments we ﬁrst sample a\ndataset, then we uniformly sample a video segment, if\nthis sampled video segment has more than one caption\nwe sample a single caption uniformly. Column weight\nin Tab. 5 describes the probability of sampling the corre-\nsponding dataset. To obtain the probability of sampling\nthe dataset with the weight wwe should divide wby the\nsum of all weights.\nThe weights for all datasets are manually adjusted. It is\nimportant to ﬁnd a good weight combination, because if\nsome weight will be larger than needed, this dataset will\nbe overseen and as a result the performance will be lower\ncomparing to the optimal case. The opposite case is when\na small weight was selected, this causes the situation when\nduring training a network does not see the required number\nof examples from this dataset.\nFor experiments in this section we use MMT with the only\nmotion modality. Embeddings for the motion modality\nare computed with irCSN152 pretrained on IG65M. All\nconﬁgurations are trained with 50 epochs and different\nnumber examples per epoch. The initial learning rate is 5e-\n5. After each epoch we multiply learning rate by 0.95. The\nMALVYMTS (see Tab. 2 for abbreviations.) conﬁguration\nis trained with 150K examples per epoch. Conﬁgurations\nwith the less number of datasets are trained with the less\nnumber of examples per epoch. The number of examples\nper epoch can be represented as a product of 150K by a sum\nof normalized weights (weights from Tab. 5 divided by a\nsum of all weights) for each dataset (the initial sum equals\nto 1): 150K = 150K×(pMSRVTT + pActivityNet + pLSMDC +\npTwitter Vines + pYouCook2 + pMSVD + pTGIF + pSomething V2). If\nsome dataset is removed from the training, we remove the\ncorresponding coefﬁcient from this sum, so the resulting\nlength will be 150K multiplied by a value less than 1.\nAs far as we use the conﬁgurations Mc, A, L as the base-\nlines, we need to be sure that the results for these con-\nﬁgurations are the optimal values. In addition to the rule\ndescribed above we try several values for a number of ex-\namples per epoch parameter, and report the results for the\nbest found value.\nTab. 6 summarizes our experiments on the datasets combi-\nnation (for more details please refer to Sec. B). The main\npoint here is that the proper combination of datasets leads\nto the best solution.\n4.4 Final result\nIn this section we compare our solution with the prior art.\nOur two best solution uses three modalities: the audio, the\nmotion and the RGB. To fuse modalities we use MMT ar-\nchitecture with 9 layers and 8 heads. As a feature extractor\nfor the audio stream the vggish [12] network is used. For\nthe video encoding we use CLIP ViT-B/32 (RGB modal-\nity) and irCSN152 (motion modality) pretrained on IG65M\ndataset. The details about preprocessing videos for both\nnetworks are presented in Sec. A.\nAdditionally we report separate results for motion + audio\nencoders and RGB + audio encoders because we do not\nknow whether the IG65M or CLIP train database has a\nsigniﬁcant overlap with any of the test datasets or not.\nAll our models presented in Tab. 4,7 and 8 are trained\nbased on the pretrain HowTo100M model. We present the\ndetails about pretraining in Sec. E.\nThe results for MSRVTT are presented in Tab. 7. As\nwe can see our solution MDMMT(MALVYMTS) L9H8\nCLIP+irCSN152+audio signiﬁcantly outperforms all pre-\n6\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nVideo expert Dataset Text →Video\nR@1↑ R@5↑ R@10↑ MnR↓ MdR↓\ns3d Kinetics 600 7.7±0.1 24.0±0.2 34.9±0.2 129.6±1.0 23.7±0.5\nSlowFast 32x2 R101 Kinetics 600 9.3±0.1 27.5±0.1 39.1±0.1 110.8±1.1 18.7±0.5\nipCSN152 IG65M 9.5±0.1 27.9±0.2 39.6±0.2 106.1±1.1 18.0±0.0\nipCSN152 IG65M →K400 8.3±0.1 25.2±0.1 36.5±0.2 124.3±0.2 21.0±0.0\nipCSN152 Sports1M 7.4±0.2 22.4±0.1 32.7±0.2 140.6±1.0 27.0±0.0\nipCSN152 Sports1M →K400 7.8±0.1 24.2±0.1 35.2±0.1 129.9±0.2 23.0±0.0\nirCSN152 IG65M 9.5±0.1 27.9±0.2 39.5±0.2 105.5±0.4 18.0±0.0\nirCSN152 IG65M →K400 8.4±0.1 25.3±0.1 36.5±0.2 120.4±0.4 21.0±0.0\nirCSN152 Sports1M 6.9±0.1 21.6±0.1 31.6±0.1 141.9±0.4 28.7±0.5\nirCSN152 Sports1M →K400 7.7±0.1 24.1±0.1 35.1±0.1 127.6±0.6 23.0±0.0\nr(2+1)d 152 IG65M 5.7±0.1 18.5±0.1 27.8±0.1 178.5±1.5 37.7±0.9\nr(2+1)d 152 IG65M →K400 5.5±0.1 18.1±0.1 27.3±0.1 184.1±1.2 39.3±0.5\nr(2+1)d 152 Sports1M →K400 5.3±0.1 17.3±0.1 26.0±0.1 193.4±3.6 42.3±0.5\nr(2+1)d 34 IG65M 9.1±0.2 27.2±0.2 38.7±0.2 108.1±0.0 19.0±0.0\nr(2+1)d 34 IG65M →K400 8.2±0.2 25.3±0.3 36.7±0.1 120.8±0.7 21.0±0.0\nCLIP CLIP 14.4±0.1 37.4±0.3 50.2±0.3 70.3±0.3 10.3±0.5\ns3dg MIL-NCE HowTo100M 8.6±0.4 26.3±0.5 37.9±0.7 104.4±2.2 19.3±0.5\nTable 3: Comparison of the best available pretrain models as the motion experts for MMT. IG65M →K400 means that\nmodel was trained on IG65M and then ﬁne tuned on Kinetics400. Results for each experiment are computed over three\nruns with random seeds. The results are reported on MSRVTT full clean split.\nmodel ActivityNet text →video\nR@1↑ R@5↑ R@10↑ MnR↓ MdR↓\nCLIP [27] 0.02 0.06 0.2 2210 2251\nMMT (Ap/r) motion+audio [8] 7.3 22.5 31 283.9 30\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio 15.1±0.1 38.3±0.1 51.5±0.3 92.4±2.3 10.0±0.0\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio 17.7±0.1 41.6±0.3 54.3±0.2 76.0±1.0 8.3±0.5\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+irCSN152+audio 20.1±0.5 45.1±0.5 58.0±0.6 70.8±0.1 7.0±0.0\nTable 4: Test results on our split (see Sec. 2.1) on ActivityNet.\nDataset Weight\nMSRVTT 140\nActivityNet 100\nLSMDC 70\nTwitter Vines 60\nYouCook2 9\nMSVD 9\nTGIF 102\nSomething V2 169\nTable 5: These datasets were used in our train procedure.\nThe \"Weight\" column describes how often we sample ex-\namples from the dataset. The probability of obtaining an\nexample from the dataset with the weight wequals to w\ndivides by a sum of all weights.\nvious solutions on all splits: full, 1k-A and 1k-B. Our\nsolution is better than the previous SotA (on R@5) on\n8.7%, 10.5% and 14.4% on full, 1k-A and 1k-B corre-\nspondingly. It is also worth to mention that our MDMMT\n(using only the motion, the RGB and the audio modalities)\nDataset Test Text→Video R@5 ↑\nMSRVTT ActivityNet LSMDC\nMc 29.0±0.2 13.4±0.3 12.9±0.6\nA 14.7±0.1 30.9±0.6 10.4±0.3\nL 8.8±0.1 7.2±0.2 24.7±0.6\nMcALV 32.1±0.1 32.0±0.2 26.5±0.7\nMcALVYMT 33.8±0.1 32.3±0.2 27.3±0.4\nMcALVYMTS 34.5±0.1 32.4±0.5 27.4±0.6\nTable 6: See abbreviations for the ﬁrst column in Tab. 2.\nThe ﬁrst three rows Mc,A,L report the quality of models\ntrained on a single domain, and tested on other domains.\nItalic means that the model did not see data from this do-\nmain during training. In this table the only motion modality\n(irCSN152) is used.\noutperforms the original MMT (the motion, the RGB and\nthe audio and 4 other modalities) by 8.7%, 10.5% and 14.4\n(R@5) on full, 1k-A and 1k-B correspondingly.\nWe also report the results for the original CLIP [27]. The\nCLIP model has an image encoder and a text encoder, both\n7\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\npretrained in an unsupervised way. To test the CLIP model\nwe take a single frame from the middle of the video (this\nis the original testing protocol for CLIP). The row CLIP\nagg [26] represents the usage of CLIP model with several\nframes using some speciﬁc aggregation procedure from\nthis work.\nIn Tab. 8 we report the results on LSMDC. On this bench-\nmark we outperform the previous SotA solution by 8.6%.\nAs we mention in Sec. 2.1 we do not use the standard\nActivityNet paragraph retrieval test protocol. Instead we\nuse the text to video retrieval protocol. To compare our\nsolution with the previous work we take the previous SotA\napproach (MMT) in text to video retrieval and test it on\nour split. The results are reported in Tab. 4. Our solution\noutperforms MMT by 22.6%. The row MMT (A p/r) mo-\ntion+audio means that this network was trained only on\nActivityNet dataset with the paragraph retrieval mode. It is\nalso worth to mention that CLIP shows very bad results on\nthis benchmark. We try to aggregate with the mean pool-\ning of 2, 4 and 16 uniformly taken embeddings, take the\nﬁrst 10, 20 and 70 words from a caption, and no method\nimproves the results.\nThe important property of our model is that we train a\nsingle model and test it on different test sets. The authors\nof previous SotA approach (MMT) trained three different\nmodels for MSRVTT, ActivityNet and LSMDC, while in\nTab. 6 we show that the model trained in such a manner\nhas poor generalization and can show good performance\non the test part of the datasetXif and only if it was trained\non the train part of the dataset X.\n5 Conclusions and Discussion\nIn this work we present a new text to video retrieval state-\nof-the-art model on MSRVTT and LSMDC benchmarks.\nWe do not use ActivityNet dataset in the paragraph retrieval\nmode as many previous works do, so we can’t compare\nwith them. But we show that on ActivityNet in the video\nretrieval mode we outperform the previous state-of-the-art\nmodel (MMT) by a large margin. Our model has captured\nknowledge from many video caption datasets, thus it is\nable to show the best results on several datasets at the same\ntime without ﬁnetuning.\nWe also present a practical approach to ﬁnd the overlap\nbetween two different video datasets. Using this approach\nwe ﬁnd the overlap between several datasets. Especially\nwe ﬁnd a large overlap between the MSRVTT test and\ntrain parts, and between the ActivityNet test and train\nparts. Removing this overlap from the MSRVTT train part\nsigniﬁcantly decreases the performance of previous best\nmodels on MSRVTT benchmark.\nAcknowledgments. We would like to thank Andrey\nIvanyuta and other colleagues from Intelligent Systems\nand Data Science Lab for helping to ﬁnd the overlap be-\ntween datasets.\n8\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nmodel\nsplit\nMSRVTT text →video\nR@1↑ R@5↑ R@10↑ MnR↓ MdR↓\nRandom baseline\nfull\n0.0 0.2 0.3 1500 1500\nVSE [24] 5.0 16.4 24.6 — 47\nVSE++ [24] 5.7 17.1 24.8 — 65\nMulti Cues [24] 7.0 20.9 29.7 — 38\nW2VV [5] 6.1 18.7 27.5 — 45\nDual Enc. [6] 7.7 22.0 31.8 — 32\nCE [19] 10.0±0.1 29.0±0.3 41.2±0.2 86.8±0.3 16.0±0.0\nMMT (M) 7mod [8] 10.7±0.2 31.1±0.1 43.4±0.2 88.2±0.7 15.0±0.0\nCLIP [27] 15.1 31.8 40.4 184.2 21\nCLIP agg [26] 21.5 41.1 50.4 — 4\nOurs MDMMT(MALVYMTS) L9H8 irCSN152+audio 15.7±0.1 38.8±0.1 51.1±0.2 76.0±0.7 10.0±0.0\nOurs MDMMT(MALVYMTS) L9H8 CLIP+audio 21.7±0.2 47.6±0.3 59.8±0.1 55.9±0.2 6.0±0.0\nOurs MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio 23.1±0.1 49.8±0.1 61.8±0.1 52.8±0.2 6.0±0.0\nMMT (Mc) 7mod [8]\nfullclean\n10.4±0.1 30.2±0.4 42.3±0.2 89.4±0.6 15.7±0.5\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio 15.8±0.1 38.9±0.1 51.0±0.1 76.4±0.5 10.0±0.0\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio 21.5±0.1 47.4±0.2 59.6±0.1 57.7±0.4 6.0±0.0\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+irCSN152+audio 22.8±0.2 49.5±0.1 61.5±0.1 53.8±0.3 6.0±0.0\nRandom baseline\n1k-A\n0.1 0.5 1.0 500.0 500.0\nJSFusion [40] 10.2 31.2 43.2 — 13\nE2E [22] 9.9 24.0 32.4 — 29.5\nHT [23] 14.9 40.2 52.8 — 9\nCE [19] 20.9±1.2 48.8±0.6 62.4±0.8 28.2±0.8 6.0±0.0\nCLIP [27] 22.5 44.3 53.7 61.7 8\nMMT (M1k-A) 7mod [8] 26.6±1.0 57.1±1.0 69.6±0.2 24.0±0.8 4.0±0.0\nA VLnet[30] 27.1 55.6 66.6 — 4\nSSB [25] 30.1 58.5 69.3 — 3.0\nCLIP agg [26] 31.2 53.7 64.2 — 4\nOurs MDMMT(M 1k-AALVYMTS) L9H8 irCSN152+audio 31.3±0.1 60.4±1.2 71.8±1.0 24.0±0.4 3.0±0.0\nOurs MDMMT(M 1k-AALVYMTS) L9H8 CLIP+audio 38.9±1.0 68.3±0.7 78.8±0.2 17.3±0.5 2.0±0.0\nOurs MDMMT(M 1k-AALVYMTS) L9H8 CLIP+irCSN152+audio 38.9±0.6 69.0±0.1 79.7±0.6 16.5±0.4 2.0±0.0\nRandom baseline\n1k-B\n0.1 0.5 1.0 500.0 500.0\nMEE [21] 13.6 37.9 51.0 — 10.0\nJPose [37] 14.3 38.1 53.0 — 9\nMEE-COCO [21] 14.2 39.2 53.8 — 9.0\nCE [19] 18.2±0.7 46.0±0.4 60.7±0.2 35.3±1.1 7.0±0.0\nMMT (M1k-B) 7mod [8] 24.5±0.5 54.4±0.8 68.0±0.5 26.6±0.2 4.7±0.5\nCLIP [27] 24.5 46.2 56.8 60.9 7\nOurs MDMMT(M 1k-BALVYMTS) L9H8 irCSN152+audio 28.8±0.9 58.8±0.3 71.2±0.3 28.5±0.5 3.7±0.5\nOurs MDMMT(M 1k-BALVYMTS) L9H8 CLIP+audio 35.1±0.1 66.5±0.9 77.6±0.3 21.5±0.4 2.7±0.5\nOurs MDMMT(M 1k-BALVYMTS) L9H8 CLIP+irCSN152+audio 37.4±1.5 68.8±0.4 79.4±0.4 21.3±0.4 2.0±0.0\nTable 7: Results on MSRVTT dataset.\nmodel LSMDC text →video\nR@1↑ R@5↑ R@10↑ MnR↓ MdR↓\nCT-SAN [41] 5.1 16.3 25.2 — 46\nJSFusion [40] 9.1 21.2 34.1 — 36\nMEE [21] 9.3 25.1 33.4 — 27\nMEE-COCO [21] 10.1 25.6 34.6 — 27\nCE [19] 11.2±0.4 26.9±1.1 34.8±2.0 96.8±5.0 25.3±3.1\nCLIP agg [26] 11.3 22.7 29.2 — 56.5\nCLIP [27] 12.4 23.7 31.0 142.5 45\nMMT (L) 7mod [8] 12.9±0.1 29.9±0.7 40.1±0.8 75.0±1.2 19.3±0.2\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio 13.1±0.5 31.3±0.3 40.1±0.0 74.5±0.7 19.3±0.5\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio 17.2±0.6 34.9±0.4 45.3±1.0 65.6±0.8 14.0±0.8\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+irCSN152+audio 18.8±0.7 38.5±0.4 47.9±0.7 58.0±1.1 12.3±0.5\nTable 8: Test results on LSMDC public test (1k video)\n9\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nReferences\n[1] George Awad et al. “TRECVID 2020: comprehen-\nsive campaign for evaluating video retrieval tasks\nacross multiple application domains”. In: Proceed-\nings of TRECVID 2020. NIST, USA. 2020.\n[2] David Chen and William Dolan. “Collecting Highly\nParallel Data for Paraphrase Evaluation”. In: Pro-\nceedings of the 49th Annual Meeting of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies. Portland, Oregon, USA: Asso-\nciation for Computational Linguistics, June 2011,\npp. 190–200. URL : https://www.aclweb.org/\nanthology/P11-1020.\n[3] J. Deng et al. “ImageNet: A Large-Scale Hierarchi-\ncal Image Database”. In: CVPR09. 2009.\n[4] Jacob Devlin et al.BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Understand-\ning. 2019. arXiv: 1810.04805 [cs.CL].\n[5] Jianfeng Dong, Xirong Li, and Cees G. M. Snoek.\n“Predicting Visual Features From Text for Image and\nVideo Caption Retrieval”. In:IEEE Transactions on\nMultimedia 20.12 (2018), 3377–3388. ISSN : 1941-\n0077. DOI : 10.1109/tmm.2018.2832602 . URL :\nhttp : / / dx . doi . org / 10 . 1109 / TMM . 2018 .\n2832602.\n[6] Jianfeng Dong et al. Dual Encoding for Zero-\nExample Video Retrieval. 2019. arXiv:1809.06181\n[cs.CV].\n[7] Christoph Feichtenhofer et al. SlowFast Networks\nfor Video Recognition. 2019. arXiv: 1812.03982\n[cs.CV].\n[8] Valentin Gabeur et al. Multi-modal Transformer\nfor Video Retrieval . 2020. arXiv: 2007 . 10639\n[cs.CV].\n[9] Deepti Ghadiyaram et al. Large-scale weakly-\nsupervised pre-training for video action recognition.\n2019. arXiv: 1905.00561 [cs.CV].\n[10] Raghav Goyal et al. The \"something something\"\nvideo database for learning and evaluating vi-\nsual common sense . 2017. arXiv: 1706 . 04261\n[cs.CV].\n[11] Kaiming He et al. Deep Residual Learning for\nImage Recognition . 2015. arXiv: 1512 . 03385\n[cs.CV].\n[12] Shawn Hershey et al. CNN Architectures for Large-\nScale Audio Classiﬁcation . 2017. arXiv: 1609 .\n09430 [cs.SD].\n[13] Andrej Karpathy, Armand Joulin, and Li Fei-Fei.\n“Deep Fragment Embeddings for Bidirectional Im-\nage Sentence Mapping”. In:Proceedings of the 27th\nInternational Conference on Neural Information\nProcessing Systems - Volume 2. NIPS’14. Montreal,\nCanada: MIT Press, 2014, 1889–1897.\n[14] Andrej Karpathy et al. “Large-scale Video Classi-\nﬁcation with Convolutional Neural Networks”. In:\nCVPR. 2014.\n[15] Will Kay et al. The Kinetics Human Action Video\nDataset. 2017. arXiv: 1705.06950 [cs.CV].\n[16] Giorgos Kordopatis-Zilos et al. “Near-duplicate\nvideo retrieval with deep metric learning”. In:Pro-\nceedings of the IEEE International Conference on\nComputer Vision Workshops. 2017, pp. 347–356.\n[17] Ranjay Krishna et al. “Dense-Captioning Events in\nVideos”. In: International Conference on Computer\nVision (ICCV). 2017.\n[18] Yuncheng Li et al. “TGIF: A New Dataset and\nBenchmark on Animated GIF Description”. In: The\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). 2016.\n[19] Yang Liu et al.Use What You Have: Video Retrieval\nUsing Representations From Collaborative Experts.\n2020. arXiv: 1907.13487 [cs.CV].\n[20] Dhruv Kumar Mahajan et al. “Exploring the Lim-\nits of Weakly Supervised Pretraining”. In: ECCV.\n2018.\n[21] Antoine Miech, Ivan Laptev, and Josef Sivic.Learn-\ning a Text-Video Embedding from Incomplete and\nHeterogeneous Data. 2020. arXiv: 1804 . 02516\n[cs.CV].\n[22] Antoine Miech et al. End-to-End Learning of Vi-\nsual Representations from Uncurated Instructional\nVideos. 2020. arXiv: 1912.06430 [cs.CV].\n[23] Antoine Miech et al. “HowTo100M: Learning a\nText-Video Embedding by Watching Hundred Mil-\nlion Narrated Video Clips”. In: ICCV. 2019.\n[24] Niluthpol Chowdhury Mithun et al. “Learning joint\nembedding with multimodal cues for cross-modal\nvideo-text retrieval”. In: Proceedings of the 2018\nACM on International Conference on Multimedia\nRetrieval. 2018, pp. 19–27.\n[25] Mandela Patrick et al. Support-set bottlenecks for\nvideo-text representation learning . 2021. arXiv:\n2010.02824 [cs.CV].\n[26] Jesús Andrés Portillo-Quintero, José Carlos Ortiz-\nBayliss, and Hugo Terashima-Marín. A Straightfor-\nward Framework For Video Retrieval Using CLIP.\n2021. arXiv: 2102.12443 [cs.CV].\n[27] Alec Radford et al. “Learning Transferable Visual\nModels From Natural Language Supervision”. In:\nImage 2 (), T2.\n[28] Anna Rohrbach et al. A Dataset for Movie Descrip-\ntion. 2015. arXiv: 1501.02530 [cs.CV].\n[29] Anna Rohrbach et al. Movie Description . 2016.\narXiv: 1605.03705 [cs.CV].\n[30] Andrew Rouditchenko et al. AVLnet: Learning\nAudio-Visual Language Representations from In-\nstructional Videos . 2020. arXiv: 2006 . 09199\n[cs.CV].\n[31] Lucas Smaira et al. A Short Note on the Kinetics-\n700-2020 Human Action Dataset . 2020. arXiv:\n2010.10864 [cs.CV].\n10\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\n[32] Chen Sun et al. VideoBERT: A Joint Model for\nVideo and Language Representation Learning. 2019.\narXiv: 1904.01766 [cs.CV].\n[33] Atousa Torabi, Niket Tandon, and Leonid Si-\ngal. Learning Language-Visual Embedding for\nMovie Understanding with Natural-Language. 2016.\narXiv: 1609.08124 [cs.CV].\n[34] Atousa Torabi et al. Using Descriptive Video Ser-\nvices to Create a Large Data Source for Video\nAnnotation Research. 2015. arXiv: 1503 . 01070\n[cs.CV].\n[35] Du Tran et al. A Closer Look at Spatiotemporal\nConvolutions for Action Recognition. 2018. arXiv:\n1711.11248 [cs.CV].\n[36] Du Tran et al. Video Classiﬁcation with Channel-\nSeparated Convolutional Networks . 2019. arXiv:\n1904.02811 [cs.CV].\n[37] Michael Wray et al. Fine-Grained Action Re-\ntrieval Through Multiple Parts-of-Speech Embed-\ndings. 2019. arXiv: 1908.03477 [cs.CV].\n[38] Saining Xie et al. Rethinking Spatiotemporal Fea-\nture Learning: Speed-Accuracy Trade-offs in Video\nClassiﬁcation. 2018. arXiv: 1712.04851 [cs.CV].\n[39] Jun Xu et al. “MSR-VTT: A Large Video Descrip-\ntion Dataset for Bridging Video and Language”. In:\nIEEE International Conference on Computer Vision\nand Pattern Recognition (CVPR), 2016.\n[40] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A\nJoint Sequence Fusion Model for Video Question\nAnswering and Retrieval. 2018. arXiv:1808.02559\n[cs.CV].\n[41] Youngjae Yu et al.End-to-end Concept Word Detec-\ntion for Video Captioning, Retrieval, and Question\nAnswering. 2017. arXiv: 1610.02947 [cs.CV].\n[42] Bolei Zhou et al. “Places: A 10 million Image\nDatabase for Scene Recognition”. In:IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence\n(2017).\n[43] Luowei Zhou, Nathan Louis, and Jason J. Corso.\nWeakly-Supervised Video Object Grounding from\nText by Loss Weighting and Object Interaction. 2018.\narXiv: 1805.02834 [cs.CV].\nA Pretrain experts usage\nThe important data preparing stage is how to sample frames\nfrom a video to achieve the best performance. For s3d ex-\nperiments the input video is converted to 30 frames per sec-\nond, for all other experiments we convert the input video\nto 32 frames per second. As a result we compute a single\nembedding for each second, having 1 second window with\n1 second shift (no overlapping).\nThe input frame size is important. We use the different\nsizes for the different models. For each model we use the\nrecommended input size. For s3d we resize a video to\n256 on the short side and then take a 224x224 center crop.\nFor SlowFast 32x2 R101 we resize a video to 256 on the\nshort side and then take a 256x256 center crop. For ipCSN\n152 and irCSN 152 we resize a video to 224 on the short\nside and take a 224x224 center crop. For r(2+1)d 152 and\nr(2+1)d 34 we resize a video to 112 on the short side and\nthen take a 112x112 center crop.\nPretrained models for ipCSN, irCSN and r(2+1)d are avail-\nable here1, for SlowFast 32x2 R101 here 2, and for s3d\nhere3.\nFor the CLIP model [27] we resize a video to 224 on the\nshort side and take a center crop, then we extract 1 frame\nper second. We use a publicly available image encoder.\nWe do not use the text encoder from CLIP.\nModel s3dg MIL-NCE is a video encoder from the\nwork [22]. This network was trained from scratch on\nHowTo100M dataset. For this network we resize the input\nvideo stream to the size of 228x228 pixels, then take a\ncenter crop.\nB Datasets combination\nIn Fig. 3,4,5 we present 6 models. Abbreviations McALV ,\nMcALVYMTS and M cALVYMTS represent the same\nthree models on these ﬁgures. The ﬁrst model, called\nMc, is trained on the MSRVTT full clean split only, the\nsecond one, called A, is trained on ActivityNet only.\nAnd the third model, called L, is trained on LSMDC\nonly. These three models are taken as baselines. Adding\nmore datasets should be not worse than these baseline.\nThe forth model is called McALV . This model is trained\non the combination of MSRVTT, ActivityNet, LSMDC\nand TwitterVines. As we can see M c→McALV gives\n+3.07% on MSRVTT (full clean split), A→McALV gives\n+1.06% on ActivityNet, and L→McALV gives +1.77% on\nLSMDC. The next model is called McALVYMT and it is\ntrained on combination of MSRVTT, ActivityNet, LSMDC,\nTwitterVines, YouCook2, MSVD, TGIF. The transitions\nMc→McALVYMT, A→McALVYMT, L→McALVYMT\ngive +4.85%, +1.45% and +2.63% correspondingly. The\nlast transitions Mc→McALVYMTS, A→McALVYMTS,\nL→McALVYMTS slightly improve the performance on\nActivityNet and LSMDC and signiﬁcantly improve the\nperformance on MSRVTT. Finally, the combination of all\ndatasets gives +5.5% for MSRVTT, +1.47% for Activi-\ntyNet and +2.74% for LSMDC.\nC Test and train intersection\nIn this section we present our analysis of overlapping of\npopular text to video datasets. Since we compose the train\ndataset from several different datasets it is important to be\nsure that there is no the same video segment in the train\npart and in the test part. Our aim is to ﬁnd the overlap\n1. https://github.com/facebookresearch/VMZ\n2. https://github.com/facebookresearch/SlowFast/blob/master/MODEL_ZOO.md\n3. https://github.com/princeton-vl/d3dhelper/blob/master/d3d_helper.ipynb\n11\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nMSRVTT full clean R@5↑\nMc\n29.0\nMcALV\n32.1\nMcALVYMT\n33.8\nMcALVYMTS\n34.5\nFigure 3: Increasing R@5 metric on the MSRVTT full\nclean split while enriching the train part.\nActivityNet R@5↑\nA\n30.9\nMcALV\n32.0\nMcALVYMT McALVYMTS\n32.4\nFigure 4: Increasing R@5 metric on the ActivityNet test\nset while enriching the train part.\nbetween the train part of used datasets — MSRVTT, Activ-\nityNet, LSMDC, YouCook2, MSVD, TGIF, TwitterVines,\nHowTo100M, Kinetics700 and the test parts of MSRVTT,\nActivityNet and LSMDC, and then to remove found dupli-\ncates from the train parts.\nNote that for training we use Something to Something V2\ndataset, but we do not try to ﬁnd overlap between it and\ntest datasets because this dataset is artiﬁcially created, thus\nthe probability to ﬁnd duplicates is very low.\nWe decided to ﬁnd the overlap only for MSRVTT, Activ-\nityNet and LSMDC because these are the most popular\ndatasets and we do not have enough human resources to\nﬁnd the overlap for the test part of all other datasets.\nOur cleaning method consists of two stages. The ﬁrst stage\nis to match video segments by the YouTube ID (if the ID is\nLSMDC R@5↑\nL\n24.7\nMcALV\n26.5\nMcALVYMT McALVYMTS\n27.4\nFigure 5: Increasing R@5 metric on the LSMDC test set\nwhile enriching the train part.\navailable) and remove from train parts all video segments\nthat have the corresponding pair in test parts. In Tab. 1\nthe information about the availability of YouTube IDs in\ndatasets is presented. We collect the YouTube ID for all\nvideos from MSRVTT full test and ActivityNet validation\n1,2 and remove corresponding video segments from the\ntrain part.\nThe second stage is based on matching frames by embed-\ndings. For each video we compute several embeddings\nthen we compute the similarity between each video from\nthe train part and the test part. After we manually assess\nseveral thousands of video segments with highest scores\nfor each pair of datasets. Then we extend found duplicates\nby either the YouTube ID or the internal dataset ID. This\nmeans that if a video V1 is marked as a duplicate and a\nvideo V2 is not marked as a duplicate, but they have the\nsame YouTube ID or same internal dataset ID, we will\nremove V1 and V2 from the train part. In case of LSMDC\nwe do not have the YouTube ID, but have the name of the\nmovie from which the video segment was taken, so if a\nvideo segment V1 is marked as a duplicate, we remove\nall segments taken from the movie of V1. The detailed\ndescription of the second stage is described in Sec. C.1.\nSurprisingly we found that the MSRVTT test has a signiﬁ-\ncant overlap with the MSRVTT train part. This problem is\nrelevant for the full, 1k-A and 1k-B splits. The ActivityNet\ndataset suffers from the same problem.\nFor large datasets like HowTo100M and Kinetics700 we\ncan not ﬁnd the whole intersection, but we estimate the\napproximate number of videos in the intersection. We\nfound that HowTo100M may have about 300 (10% of the\nMSRVTT full test part) video segments that can be in the\nMSRVTT full test part.\nThe similar situation is about Kinetics700 and ActivityNet\ndatasets. Kinetics700 may have approximately 500-600\n12\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nvideo segments (10% of the ActivityNet test) that may have\nduplicates in ActivityNet validation 1,2. Another problem\nwith the Kinetics dataset is that many motion models are\npretrained on it.\nThis circumstance means that researchers should carefully\nuse HowTo100M and Kinetics700 along with MSRVTT\nand ActivityNet correspondingly, because for today we\ndon’t know whether a neural network overﬁts for some\nportion of this intersection or not.\nAll duplicates can be considered as two groups of pairs.\nPairs from the ﬁrst group have the same videos, but dif-\nferent brightness, aspect ratio, size, presence/absence of\na logo and so on. The second group has pairs with quite\nsimilar videos, for example it can be the same person on\nthe same background, doing the same things, but wearing\ndifferent clothes. We think that it is better to remove such\nvideos from the train part to prevent overﬁtting. Several\nfound examples are presented in Fig. 6.\nC.1 Near duplicate video search\nC.1.1 Approach\nIn this section we explain our approach that is used to ﬁnd\nthe same or quite similar video segments in test and train\nparts.\nSuppose we have two sets of videos Q= {q1,....,q k}and\nG= {g1,...,g n}called the query set and the gallery set.\nWe want to ﬁnd all pairs (qi,gj) where qi and gj have a\ncommon video segment.\nFrom each qi and gj we extract 1 frame per second. Each\nvideo is then represented by a sequence of pictures: qi =\n[q1\ni ,....,q si\ni ] and gj = [g1\nj ,...,g pj\nj ]. Then a 2D pretrained\nneural network is used to extract features from each image:\n¯qb\na = neuralnet(qb\na) and ¯gb\na = neuralnet(gb\na).\nThen we compute the matrix of cosines between the fea-\ntures from Q and G: sab\nij =\n<¯qa\ni ,¯gb\nj>\n||¯qba||2||¯gba||2\n.\nNow each pair (qi,gj) is represented by the matrix:\ng1\nj ... g pj\nj\nq1\ni s11\nij ... s 1pj\nij\n...\nqsi\ni ssi1\nij ... s sipj\nij\nSuppose that videos qi and gj are intersected at time mo-\nments tq and tg, it is naturally to assume that the next sev-\neral seconds tq +1,...,t q +K−1 and tg +1,...,t g +K−1\n(K ≤ min(si,pj)) represent the same video segment.\nMotivated by this fact we compute the mean cosine for\neach interval of K seconds (we use K=4): Stqtj\nij =\ns\ntqtg\nij +...+s\ntq+K−1,tg+K−1\nij\nK . The sum in the numerator is\nthe sum of diagonal elements started with stqtj\nij .\nWe deﬁne the intersection score between (qi,gj) as\nSij = max\na=1,...,si−K\nb=1,...,pj−K\nSab\nij\nand the corresponding video segments as\n(a,a + K),(b,b + K)\nwhere\na,b = argmax\na=1,...,si−K\nb=1,...,pj−K\nSab\nij\nFinally we sorted all Sij in the descending order and man-\nually assess candidate pairs.\nC.1.2 Number of pairs to assess\nSuppose we search duplicates in datasets Q and G and we\nhave seen N pairs with the highest scores and ﬁnd M pairs\nwith duplicates. The important question is: what is the\ntotal number of duplicates and how many percents of them\nhave we found.\nFor each pair of Q and G we construct the following test\nprocedure. The ﬁrst step is to augment Q, and let us call\nthe result of augmentation as ˆQ. To augment a dataset we\napply two transformations: 1. we randomly crop a side of\neach video, where each side can be 70%–100% of original\nside length (aspect ratio can be changed); 2. we randomly\nshift the start of the video by a random value between 0\nand 1 seconds.\nHaving Q, ˆQ and G we compute sets of positive and neg-\native scores: Pos and Neg. The Pos is the set of scores\nbetween i-th video from Q and the corresponding aug-\nmented video from ˆQ. Neg is the set of scores between\neach video from Q and G. Having Pos and Neg sets we can\nplot a curve, where xaxis represents the fraction of found\npairs with duplicates and Y axis represents the number of\nnegative pairs that we need to assess to ﬁnd fraction xof\npositive pairs, call this curve F(x). We present the algo-\nrithm that computes F using Pos and Neg sets in Lst. 1.\nSuppose we have seen N + M pairs and have found M\npairs with duplicates. The total number of pairs with dupli-\ncates can be estimated as M/F−1(N). By the deﬁnition\nF(x) connects the fraction of found positive pairs with the\nnumber of seen negative pairs. The value F−1(N) repre-\nsents approximation of the fraction of found positive pairs.\nSo if we know, thatMis approximately 100∗F−1(N)% of\npositive pairs, then we can approximately compute 100%\nof positive pairs as M/F−1(N).\n13\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nFigure 6: The left image is taken from the MSRVTT test split and the right one from MSRVTT Train. The numbers in\nthe upper left corner represent the MSRVTT video ID. The faces are blurred in order to avoid legal claims.\n# first element is highest\nP = np. sort (P )[:: -1] # Pos\nN = np. sort (N )[:: -1] # Neg\nxs = []\nys = []\nfor x, p in enumerate (P):\n# how many negative scores\n# greater than p ?\nj = np. searchsorted (N, p)\nxs. append (x)\nys. append (j)\nListing 1: Numpy pseudocode for building the search curve\nF(x)\nC.1.3 Best 2D feature extractor\nThe key component of a duplicate search system is a fea-\nture extractor. A good feature extractor signiﬁcantly re-\nduces the number of pairs for manual assessment. To\ncompare different 2D feature extractors we use the fol-\nlowing test procedure. The test consists of two datasets.\nThe ﬁrst dataset is the train part from the MSRVTT full\nsplit. The second dataset is random 596k videos from the\nHowTo100M dataset. From each video of the taken part of\nHowTo100M we take a random 30 seconds segment. We\napply random augmentation to MSRVTT, as described\nin Sec. C.1.2. Deﬁne MSRVTT as Q, the augmented\n14\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nMSRVTT dataset as ˆQand the taken part of HowTo100M\nas G. For each feature extractor we compute curve F(x),\nas described in Sec. C.1.2.\nThe best expert has the lowest curve. For example, if we\nwant to ﬁnd 95% of duplicates, we should see many of\ncandidates, some of them are duplicates, but majority of\nthem are not. So, the value F(0.95) is the approximation\nof how many not duplicates we need to see to ﬁnd 95% of\nduplicates. Ideally F(0.95) = 0, where all seen candidates\nare duplicates. So, a lower value F(0.95) requires to see\nless number of false candidates, that’s why the lower curve\nis better.\nWe consider several feature extractors: resnet18 and\nresnet101 [11] pretrained on ImageNet [3], resnet50\npretrained on Places365 [42] and resnext101-32x8d,\nresnext101-32x32d, resnext101-32x48d pretrained on one\nbillion images from Instagram [20] and ﬁnetuned on Ima-\ngeNet. We report search curves F(x) for these pretrained\nnetworks in Fig. 7.\nThere exist networks [27] [16] trained especially for match\nthe duplicate frames or video segments, but they are not\npublicly available.\n0 0.2 0.4 0.6 0.8 1\n104\n105\n106\nresnet18-imagenet\nresnet50-places365\nresnet101-imagenet\nresnext101-32x8d-wsl\nresnext101-32x16d-wsl\nresnext101-32x48d-wsl\n# of negative pairs\nratio of positive pairs\nFigure 7: Search curves F for different pretrained models.\nCurve F is used to estimate the minimal number of negative\npairs (y = F(x)) that human assessors need to inspect\nbefore they ﬁnd the fraction x of positive pairs. The lower\nthe curve F the better (need to inspect manually less pairs).\nThe curves are built with the query set Q = MSRVTT\nfull train, the gallery set G = random 596k videos from\nHowTo100M.\nAs we see resnext101-32x48d-wsl shows the best result.\nWe use this network for searching for duplicates.\nIt is worth to mention that here we just compare different\nnetworks on a ﬁxed benchmark, and pick the best one. But\nthe search curve F(x) signiﬁcantly depends on data. This\ncurve should be estimated for each used pair of datasets Q\nand G.\nC.1.4 Black frames\nOften two consecutive video segments are glued with sev-\neral black frames. The cosine similarity of embeddings\nof two black or near black frames are close to 1. In this\ncase the most probable candidates for duplicates are black\nvideo segments. To prevent this we apply the following\nrule. Suppose we have a frame U and the unit length em-\nbedding vcomputed from U. We ﬁnd the prevalent color\nin U and compute the area S0 ﬁlled by this color. Then\nwe compute the value S0/(hw), where hand w are the\nheight and width of U. If this fraction is greater than 0.7\nwe deﬁne µv = 1 −S0/(hw), otherwise µv = 1 . To\ncalculate similarity between embeddings v1 and v2 we use\nweighted cosine similarity: µ1µ2cosv1,v2, instead of clas-\nsical cosine similarity. This rule removes majority of all\nnear black frames from the most relevant candidates for\nduplicates.\nC.1.5 Screensavers detection\nMany videos from ActivityNet, HowTo100m, YouCook2\ncontain screensavers at the beginning or at the end. It\ncauses a problem like mentioned above with near black\nframes, because most of relevant proposals are the same\nscreensavers, but the video content of the remainder video\npart are different.\nUsing the system described in Sec. C.1.6 we search for du-\nplicates in the ActivityNet dataset, where a lot of the most\nrelevant segments are screensavers. We collect several hun-\ndreds of screensavers and then compute embeddings for\neach of them. Let us call the resulting set of embeddings as\nE. Then we apply the following rule: if some embedding\nvhas the similarity greater that 0.9 to one of embeddings\nfrom E, we set v = 0 . So if the video segment has a\npart of a screensaver, it will never be in the most relevant\nproposals.\nC.1.6 GUI\nThe important part of the video duplicate search system is\nthe user interface. Without ergonomic and fast interface it\nis impossible to assess tens thousands of video pairs. Our\nsystem is presented in Fig. 8.\nThe system shows video pairs with the highest scores on\ntop. A user needs to scroll down a web page (new videos\nare loaded dynamically with ajax), and if a video duplicate\nis detected, a user should press the Duplicate button, if\nthere are no duplicates in the current viewport, no action is\nrequired. When a user scrolls a web page, all non-duplicate\npairs automatically are saved to a log ﬁle. Additionally\nseveral users at the same time can assess video pairs.\n15\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nFigure 8: Web system used to ﬁnd duplicates. Images on\nthe ﬁrst and third row are not duplicates and the second\nrow contains duplicate.\ndataset M A L\ntest train test train test train\nM 114 223 6 10 0 0\nA 10 6 127 163 0 0\nL 6 2744 0 0 0 0\nYouCook2 13 27 7 10 0 0\nMSVD 1 1 1 1 1 1\nTGIF 6 8 0 0 0 0\nTwitter Vines 3 3 0 0 0 0\nKinetics700 4 5 456 464 0 0\nHowTo100M 177 154 209 209 0 0\nTable 9: The leftmost column represents train parts of\ndatasets, and the upper row represents test parts of datasets.\nColumn \"test\" means how many video segments are in\nthe test part that have the corresponding pair in the train\npart either with the same YouTube ID or manually marked\nas a duplicate. Column \"train\" represents the number of\nvideo segments in the train part that have corresponding\npair in the test dataset either with the same YouTube ID or\nmanually marked as a duplicate. All segments counted in\nthe \"train\" column are removed from the train part. For ex-\nample consider the column \"A\" and the row \"M\". train=10\nmeans that the MSRVTT train part contains 10 video seg-\nments that have a pair in the ActivityNet test part. These\n10 videos must be removed from train part when dataset\nare combined. test=6 means that ActivityNet test has 6\nvideo segments that have a pair in the MSRVTT test part.\nC.2 Cleaning results\nRecall that our cleaning method consists of two stages. In\nthe ﬁrst stage we throw out from the train part all video\nsegments that have a pair with the same YouTube ID in\ntest parts of MSRVTT or ActivityNet. The second stage\nis matching video segments by embeddings and manually\nassess several thousands pairs with the highest score.\nIn Tab. 9 we report how many duplicates are found for\neach pairs of datasets. This table represents the ﬁnal result\nafter applying these two stages.\nSeparate results for the ﬁrst and the second stages are\nreported in Sec. C.2.1.\nNote that columns \"test\" and \"train\" in Tab. 9 may have\ndifferent values. Consider the situation when the test part\nhave a video segment A, and the train part have two video\nsegments A1 and A2. And both are marked as duplicates\nwith A. In this case the video segment A brings +1 to the\n\"test\" column and A1, A2 bring +2 to the \"train\" column.\nThe most problematic datasets in terms of the number of\nduplicates are MSRVTT and ActivityNet. These datasets\noverlap with itself (e.g. MSRVTT test overlap with\nMSRVTT train). We found more than 100 duplicate\npairs for both of them. Other problematic datasets are\nHowTo100M and Kinetics700, these datasets are large,\nso we can’t assess the required number of video pairs\nto ﬁnd 95% or 99% of duplicates. But we can assess a\nsmaller number of pairs and using search curves F (see\nSec. C.1.2) can extrapolate this value to 100%. We found\nthat HowTo100M may have the intersection with MSRVTT\ntest full by about 300 videos (10% of the MSRVTT test\nfull). The similar situation is about the ActivityNet test set\nand Kinetics700, the intersection could be near 500-600\nvideos (10% of the ActivityNet test set).\nIn Tab. 10 we report results on MSRVTT for MMT retrain-\ning with no cleaning, after cleaning by the YouTube ID and\ncleaning combination by the YouTube ID and the manual\nassessment. The manual cleaning for 1k-A and 1k-B is\nincomplete because we only do cleaning for the full split.\nThe following situation takes place for 1k-A, 1k-B splits:\nwhen 1k videos from the full test are taken for test and\nthe remaining 2k videos are moved to the train part, the\nadditional overlapping is introduced, because these 1k and\n2k videos are overlapping. We do not remove this overlap\nin this research.\nsplit no by ID by ID +\nclean manual\nfull 31.1 ±0.1 31.1±0.1 30.2±0.4\n1k-A 54.8 ±0.5 50.7±0.9 49.4±0.5\n1k-B 51.1 ±0.9 46.1±0.1 46.4±0.6\nTable 10: Comparison for original MMT trained (7 modal-\nities) on MSRVTT without cleaning, with cleaning by the\nYouTube ID only, and with cleaning by the YouTube ID\nplus the manual assessment.\nAs you can see after cleaning the performance is signif-\nicantly decreased on 1k-A and 1k-B splits for original\nMMT.\nC.2.1 Intersection by YouTube ID and embeddings\nIn Tab. 11 we report the intersection by the YouTube ID\nbetween test parts of MSRVTT (full, 1k-A, 1k-B) and\nActivityNet with train parts of MSRVTT (full, 1k-A, 1k-\nB), ActivityNet, Kinetics700, YouCook2, HowTo100m,\nMSVD.\n16\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\ndata M M1k-a M1k-b A\nset test train test train test train test train\nM 0 0 0 0 104 179 2 4\nM1k-a 2362 1990 372 415 827 1007 2 4\nM1k-b 1689 1367 563 634 380 407 2 4\nA 0 0 0 0 0 0 0 0\nK 5 4 1 1 0 0 408 408\nY 8 4 2 2 2 2 3 3\nHT100M 147 117 39 38 57 53 175 175\nMSVD 3 1 2 1 0 0 1 1\nTable 11: First stage. The leftmost column represents train\nparts of datasets, and the upper row represents test parts\nof datasets. Column \"test\" represents number of video\nsegments in test part that have corresponding video in train\npart with the same YouTube ID. Column \"train\" represents\nnumber of video in train part that have corresponding pair\nin test part with the same ID. For example: if we com-\nbine M and YouCook2, we should remove 4 video from\nYouCook2 train.\nIt is worth to mention that MSRVTT 1k-A test and 1k-B\ntest have a large overlap ratio by the YouTube ID with the\n1k-A train and the 1k-B train parts correspondingly. Both\nsplits have the overlap ratio of about 38% between the train\npart and the test part. We also emphasize that the original\nMSRVTT full split does not overlap by the YouTube ID\nbetween the test and train parts.\ndata M A L\nset seen found total seen found total seen found total\nM 10k 114 114 1k 6 6 1k 0 0\nA 10k 10 10 15k 127 142 1k 0 0\nL 3k 6 6 2k 0 0 — — —\nY 2k 13 13 1k 7 7 1k 0 0\nMSVD 1k 1 1 1k 1 1 1k 1 1\nT 2k 6 6 2k 0 0 3k 0 0\nV 2k 3 3 0k 0 0 1k 0 0\nK 2k 1 2 30k 227 539 2k 0 0\nHT100M 5k 15 320 — — — — — —\nTable 12: Second stage. The leftmost column represents\ntrain parts of datasets, and the upper row represents test\nparts of datasets. Column \"seen\" represents the number of\nvideo segments that we manually assess for a given pair of\ndatasets. Column \"found\" represents the number of videos\nin the test part for which there exists the corresponding\nduplicate video segment in the train part. Column \"total\"\nrepresents the approximately estimated total number of\nvideos from the test part that have a duplicate pair in the\ntrain part. Symbol \"—\" means that the intersection is not\ncomputed because it requires too much human resources.\nIn Tab. 12 we report the statistics for the second deduplica-\ntion stage (searching by embeddings). We do not compute\nan intersection for MSRVTT 1k-A and 1k-B splits.\nIn this table we present the number of manually found\nduplicates and the estimated maximum number of dupli-\ncates for a given pair of datasets. We managed to ﬁnd the\nintersection for almost all pairs of datasets.\nThe maximum number of duplicates is computed based\non the search curve F(x). As we told in Sec. C.1.2 the\nsearch curve signiﬁcantly depends on data. We compute\nthe search curve for all pairs of datasets in Tab. 12. The\nsearch curve for each particular pair of datasets is build\nexactly in the same way as described in Sec. C.1.2. For\nexample, to compute the search curve for MSRVTT test\nand ActivityNet train we deﬁne MSRVTT test as Q, Activ-\nityNet train as G, then augment Qto produce ˆQ, and use\nthe algorithm described in Sec. C.1.2.\nUsing the column \"seen\" from Tab. 12 we can compute\nhow many pairs need to be assessed to ﬁnd the full over-\nlap between datasets. For example, inspect 5k pairs for\nHowTo100M dataset and MSRVTT (the row \"HT100M\"\nand the column \"M\"), we found 15 duplicates, so the ap-\nproximate maximum number of duplicates is 320: 5k *\n(320 / 15) = 106k. So, to ﬁnd the full overlap using the\ncurrent version of algorithm it is needed to manually assess\n106k video pairs and it is too much, that’s why we do not\nﬁnd full intersection for this speciﬁc pair of datasets.\nD Hyperparameters\nTo train our best networks (MMT(MALVYMTS)\nL9H8 CLIP+audio, MDMMT(MALVYMTS) L9H8\nirCSN152+audio and MMT(MALVYMTS) L9H8\nCLIP+irCSN152+audio) we use 50 epochs and deﬁne a\nsingle epoch as 150K examples per GPU (in total 1.2M\nexamples per epoch on 8 GPUs). We use Adam optimizer\nwithout weight decay, the initial value for a learning rate is\n5e-5, after each epoch we multiply the learning rate by\n0.95. Batch size of 32 examples per GPU is used. We\ndo not exchange embeddings between GPUs. We use\nbi-directional max-margin ranking loss with margin 0.05.\nIn Bert and the video transformer encoder we use dropout\n0.2 in attention and in FFN block. We use 8 Nvidia V100\n32GB GPUs. The training time is about 14 hours.\nE Pretrained model\nThe well known method to boost the performance in video\nretrieval tasks is to use a pretrained model. First the neural\nnetwork is trained on some large dataset, then at second\nstage it is ﬁnetuned for target target dataset. In video\nretrieval task HowTo100M dataset is often used for pre-\ntraining. In this work we use HowTo100M for pretraining\nin the same way.\nIn our training procedure we use 8 Nvidia V100 32Gb\nGPUs, we train for 200 epochs where one epoch is deﬁned\nas 80k examples on each GPU (in total network sees 640k\nexamples on 8 GPUs per epoch). We use batch size 64\nfor each GPU and do not exchange embeddings between\n17\nMDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT\nmodel\npretr\nMSRVTT full clean text →video\nR@1↑ R@5↑ R@10↑ MnR↓ MdR↓\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio yes 15.8±0.1 38.9±0.1 51.0±0.1 76.4±0.5 10.0±0.0\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio no 14.5±0.1 36.8±0.3 48.8±0.3 82.2±0.6 11.0±0.0\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio yes 21.5±0.1 47.4±0.2 59.6±0.1 57.7±0.4 6.0±0.0\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio no 20.0±0.1 45.1±0.1 57.3±0.1 63.1±0.1 7.0±0.0\nTable 13: Performance on the MSRVTT full clean split with and without pretrained model (HowTo100m).\nmodel\npretr\nActivityNet text →video\nR@1↑ R@5↑ R@10↑ MnR↓ MdR↓\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio yes 15.1±0.1 38.3±0.1 51.5±0.3 92.4±2.3 10.0±0.0\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio no 12.0±0.1 33.7±0.4 46.3±0.3 119.9±2.1 13.0±0.0\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio yes 17.7±0.1 41.6±0.3 54.3±0.2 76.0±1.0 8.3±0.5\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio no 15.2±0.3 37.9±0.3 50.1±0.2 93.4±2.0 10.3±0.5\nTable 14: Performance on ActivityNet with and without pretrained model (HowTo100m). The performance reported for\nthe text to video retrieval task on our own subset of the original ActivityNet test part. See Sec. 2.1 for details.\nmodel\npretr\nLSMDC text →video\nR@1↑ R@5↑ R@10↑ MnR↓ MdR↓\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio yes 13.1±0.5 31.3±0.3 40.1±0.0 74.5±0.7 19.3±0.5\nOurs MDMMT(M cALVYMTS) L9H8 irCSN152+audio no 12.6±0.7 30.2±1.5 39.6±0.9 76.1±0.8 19.7±1.3\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio yes 17.2±0.6 34.9±0.4 45.3±1.0 65.6±0.8 14.0±0.8\nOurs MDMMT(M cALVYMTS) L9H8 CLIP+audio no 16.2±1.1 35.4±1.3 45.1±0.7 64.9±1.9 14.7±0.5\nTable 15: Performance on LSMDC with and without pretrained model (HowTo100m).\nGPU. Initial learning rate is 5e-5. After each epoch we\nmultiply learning rate by 0.98. We use the full HowTo00M\ndataset. The model is trained either with two modalities:\nmotion/RGB and audio or with three modalities: motion,\nRGB and audio, depending on how many modalities are\nused in ﬁnal model. The total training time is about 24\nhours. We use bi-directional max-margin ranking loss with\nmargin 0.05.\nIn Tab. 13, 14 and 15 we compare two our models:\nMDMMT(McALVYMTS) L9H8 irCSN152+audio and\nMDMMT(McALVYMTS) L9H8 CLIP+audio when they\nare trained from the pretrained model or not. In these three\ntables we present the same four models (no special ﬁnetun-\ning for the target dataset) tested on different datasets.\nAs we can see in Tab. 13 the pretrained model increases R1\nmetric by 1% and R5 by 2%. The pretrained model also\nincrease performance on ActivityNet dataset, see Tab. 14.\nFor R1 metric the improvement is about 2% and for R5\nmetric is about 4%. For LSMDC dataset, see Tab 15,\nwe have approximately the same results with and without\npretraining.\n18"
}