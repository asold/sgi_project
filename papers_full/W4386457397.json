{
  "title": "LET-Net: locally enhanced transformer network for medical image segmentation",
  "url": "https://openalex.org/W4386457397",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1985544887",
      "name": "Na Ta",
      "affiliations": [
        "Jilin University",
        "Hulunbuir University",
        "Jilin Province Science and Technology Department"
      ]
    },
    {
      "id": "https://openalex.org/A2105556960",
      "name": "Haipeng Chen",
      "affiliations": [
        "Jilin Province Science and Technology Department",
        "Jilin University"
      ]
    },
    {
      "id": "https://openalex.org/A2125391649",
      "name": "Xianzhu Liu",
      "affiliations": [
        "Changchun University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2313469795",
      "name": "Nuo Jin",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A1985544887",
      "name": "Na Ta",
      "affiliations": [
        "Jilin University",
        "Hulunbuir University"
      ]
    },
    {
      "id": "https://openalex.org/A2105556960",
      "name": "Haipeng Chen",
      "affiliations": [
        "Jilin University"
      ]
    },
    {
      "id": "https://openalex.org/A2125391649",
      "name": "Xianzhu Liu",
      "affiliations": [
        "Changchun University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2313469795",
      "name": "Nuo Jin",
      "affiliations": [
        "University of Southampton"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979515228",
    "https://openalex.org/W4292939762",
    "https://openalex.org/W3087353686",
    "https://openalex.org/W4206693420",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4287183752",
    "https://openalex.org/W4280596596",
    "https://openalex.org/W3215835978",
    "https://openalex.org/W3034144495",
    "https://openalex.org/W2990613241",
    "https://openalex.org/W3214287966",
    "https://openalex.org/W3181882949",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W3213290340",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3203497085",
    "https://openalex.org/W3027739765",
    "https://openalex.org/W3196075503",
    "https://openalex.org/W3092462072",
    "https://openalex.org/W4312258849",
    "https://openalex.org/W4312880622",
    "https://openalex.org/W3209646389",
    "https://openalex.org/W3199684609",
    "https://openalex.org/W4295934721",
    "https://openalex.org/W3013198566",
    "https://openalex.org/W2996290406",
    "https://openalex.org/W3110440461",
    "https://openalex.org/W2985469567",
    "https://openalex.org/W3196107618",
    "https://openalex.org/W4312617484",
    "https://openalex.org/W4312907595",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W3090492687",
    "https://openalex.org/W3199543531",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W4205189720",
    "https://openalex.org/W4311088269",
    "https://openalex.org/W4319300975",
    "https://openalex.org/W4200015473",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W4220686172",
    "https://openalex.org/W3162418282",
    "https://openalex.org/W4250482878",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W3173203577",
    "https://openalex.org/W2998449272",
    "https://openalex.org/W3135262214",
    "https://openalex.org/W3092344722",
    "https://openalex.org/W3181013887",
    "https://openalex.org/W4214489053",
    "https://openalex.org/W2008359794",
    "https://openalex.org/W2997286550",
    "https://openalex.org/W2285968993",
    "https://openalex.org/W2021088830",
    "https://openalex.org/W2560328367",
    "https://openalex.org/W2991372685",
    "https://openalex.org/W2962767316",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W3148874463"
  ],
  "abstract": "Abstract Medical image segmentation has attracted increasing attention due to its practical clinical requirements. However, the prevalence of small targets still poses great challenges for accurate segmentation. In this paper, we propose a novel locally enhanced transformer network (LET-Net) that combines the strengths of transformer and convolution to address this issue. LET-Net utilizes a pyramid vision transformer as its encoder and is further equipped with two novel modules to learn more powerful feature representation. Specifically, we design a feature-aligned local enhancement module, which encourages discriminative local feature learning on the condition of adjacent-level feature alignment. Moreover, to effectively recover high-resolution spatial information, we apply a newly designed progressive local-induced decoder. This decoder contains three cascaded local reconstruction and refinement modules that dynamically guide the upsampling of high-level features by their adaptive reconstruction kernels and further enhance feature representation through a split-attention mechanism. Additionally, to address the severe pixel imbalance for small targets, we design a mutual information loss that maximizes task-relevant information while eliminating task-irrelevant noises. Experimental results demonstrate that our LET-Net provides more effective support for small target segmentation and achieves state-of-the-art performance in polyp and breast lesion segmentation tasks.",
  "full_text": "Vol.:(0123456789)1 3\nMultimedia Systems (2023) 29:3847‚Äì3861 \nhttps://doi.org/10.1007/s00530-023-01165-z\nSPECIAL ISSUE PAPER\nLET‚ÄëNet: locally enhanced transformer network for¬†medical image \nsegmentation\nNa¬†Ta1,2,3¬†¬∑ Haipeng¬†Chen1,3¬†¬∑ Xianzhu¬†Liu4¬†¬∑ Nuo¬†Jin5\nReceived: 13 April 2023 / Accepted: 12 August 2023 / Published online: 5 September 2023 \n¬© The Author(s) 2023\nAbstract\nMedical image segmentation has attracted increasing attention due to its practical clinical requirements. However, the \nprevalence of small targets still poses great challenges for accurate segmentation. In this paper, we propose a novel locally \nenhanced transformer network (LET-Net) that combines the strengths of transformer and convolution to address this issue. \nLET-Net utilizes a pyramid vision transformer as its encoder and is further equipped with two novel modules to learn more \npowerful feature representation. Specifically, we design a feature-aligned local enhancement module, which encourages \ndiscriminative local feature learning on the condition of adjacent-level feature alignment. Moreover, to effectively recover \nhigh-resolution spatial information, we apply a newly designed progressive local-induced decoder. This decoder contains \nthree cascaded local reconstruction and refinement modules that dynamically guide the upsampling of high-level features \nby their adaptive reconstruction kernels and further enhance feature representation through a split-attention mechanism. \nAdditionally, to address the severe pixel imbalance for small targets, we design a mutual information loss that maximizes \ntask-relevant information while eliminating task-irrelevant noises. Experimental results demonstrate that our LET-Net pro-\nvides more effective support for small target segmentation and achieves state-of-the-art performance in polyp and breast \nlesion segmentation tasks.\nKeywords Medical image segmentation¬†¬∑ Feature alignment¬†¬∑ Local-induced decoder¬†¬∑ Mutual information¬†¬∑ Transformer\n1 Introduction\nMultimodal medical image segmentation aims to accu-\nrately identify and annotate regions of interest from images \nproduced by various medical devices, such as segmenting \npolyps from colonoscopy images [1 ], breast lesions from \nultrasound images [2 ], and focal cortical dysplasia lesions \nfrom magnetic resonance images [3]. It has been an essential \nprocedure for computer-aided diagnosis [4 ], which assists \nclinicians in making accurate diagnoses, planning surgical \nprocedures, and proposing treatment strategies. Hence, the \ndevelopment of automatic, accurate, and robust medical \nimage segmentation methods is of great value to clinical \npractice.\nHowever, medical image segmentation still encounters \nsome challenges, one of which is the prevalence of small \nlesions. Figure¬† 1 illustrates small lesion samples and size \ndistribution histograms for several different benchmarks, \nwhere the ratio of lesion area to whole image is signifi-\ncantly concentrated in a smaller range, with proportions in \ndescending order: 0 ‚àí0.1 first, 0.1‚àí0.2¬†s. Specifically, a vast \nmajority of polyps and breast lesions occupy only a small \nproportion of the entire medical image. Meanwhile, some \nsmall lesions, e.g., early stage polyps, exhibit an inconspicu-\nous appearance. These small targets inevitably pose great \ndifficulties for accurate segmentation for several reasons. \nFirst, small targets are prone to being lost during repeated \n * Nuo Jin \n jinnuo0412@163.com\n1 College of¬†Computer Science and¬†Technology, Jilin \nUniversity, Changchun¬†130012, China\n2 College of¬†Computer, Hulunbuir University, \nHulunbuir¬†021008, China\n3 Key Laboratory of¬†Symbolic Computation and¬†Knowledge \nEngineering of¬†Ministry of¬†Education, Jilin University, \nChangchun¬†130012, China\n4 National and¬†Local Joint Engineering Research Center \nof¬†Space Optoelectronics Technology, Changchun University \nof¬†Science and¬†Technology, Changchun¬†130022, China\n5 Southampton Business School, University of¬†Southampton, \nSouthampton¬†SO17¬†1BJ, United¬†Kingdom\n3848 N.¬†Ta et al.\n1 3\ndownsampling operations and are hard to recover. Second, \nthere is a significant class imbalance problem in the number \nof pixels between the foreground and background, leading \nto a biased network and suboptimal performance. Whereas, \nthe ability of computer-aided diagnosis to identify small \nobjects is highly desired, as early detection and diagnosis \nof small lesions are crucial for successful cancer prevention \nand treatment.\nNowadays, the development of medical image segmenta-\ntion has greatly advanced due to the efficient feature extrac-\ntion ability of convolutional neural networks (CNNs) [5‚Äì7]. \nModern CNN-based methods typically utilize a U-shaped \nencoder‚Äìdecoder structure, where the encoder extracts \nsemantic information and the decoder restores resolution to \nfacilitate segmentation. Additionally, skip connections are \nemployed to compensate for detailed information. Some \nadvanced U-shaped works focus on the following studies, \nwhich include designing novel encoding blocks [8 ‚Äì10] to \nenhance feature representation ability, adopting attention \nmechanisms to further recalibrate features [11, 12], extract-\ning and fusing multi-scale reasonable context information to \nimprove accuracy [13‚Äì15], and so on. Despite their promis-\ning performance, these methods share a common flaw, i.e., \nlacking global contexts essential for better recognition of \ntarget objects.\nDue to their superior ability to model global contexts, \nTransformer-based architectures have become popular in \nsegmentation tasks while achieving promising performance. \nRecent works [16‚Äì 18] utilize vision transformers (ViT) as \na backbone to incorporate global information. Despite their \ngood performance, ViT produces single-scale low-resolu-\ntion features and has a very high computational cost, which \nhampers their performance in dense prediction. In contrast \nto ViT, pyramid vision transformer (PVT) [19] inherits the \nadvantages of both CNN and Transformer and produces \nhierarchical multi-scale features that are more favorable for \nsegmentation. Unfortunately, Transformer-based methods \ndestroy part of local features when modeling global contexts, \nwhich may result in imprecise predictions for small objects.\nIn the field of small target segmentation, a couple of \napproaches have been devised to improve the sensitivity \nof small objects. They overcome the segmentation difficul-\nties brought by small objects from multiple aspects, such as \nexploiting the complementarity between low-level spatial \ndetails and high-level semantics [20], multi-scale feature \nlearning [21, 22], and augmenting spatial dimension strat-\negies [23‚Äì 25]. Although their skip connections can com-\npensate for detail loss to some extent and even eliminate \nsomewhat irrelevant noises by extra equipping with attention \nmechanisms, these methods are still insufficient, as some \nlocal contexts may be overwhelmed by dominant seman-\ntics due to feature misalignment issues. In addition, another \nimportant factor that has been overlooked is how to effec-\ntively restore spatial information of downsampled features. \nMost methods adopt common upsampling operations, such \nas nearest-neighbor interpolation and bilinear interpolation, \nwhich may still lack local spatial awareness to handle small \nobject positions. As a result, they are not compatible with \nthe recovery of target objects and produce suboptimal seg-\nmentation performance.\nIn this paper, we propose a novel locally enhanced trans-\nformer network (LET-Net) for medical image segmentation. \nBy leveraging the merits of both Transformer and CNN, \nour LET-Net can accurately segment small objects and pre-\ncisely sharpen local details. First, the PVT-based encoder \nproduces hierarchical multi-scale features where low-level \nfeatures tend to retain local details, while high-level features \nprovide strong global representations. Second, to further \nemphasize detailed local contexts, we propose a feature-\naligned local enhancement (FLE) module, which can learn \ndiscriminative local cues from adjacent-level features on \nthe condition of feature alignment and then utilize the local \nenhancement block equipped with local receptive fields to \nfurther recalibrate features. Third, we design a progressive \nFig. 1  An illustration of small lesion samples and size distributions \nfor different medical image datasets, including polyp coloscopy \nimages and breast ultrasound images. Ground truth for each image is \nrepresented by a green line. In a histogram, the horizontal axis repre-\nsents the proportion of the entire image occupied by the lesion area, \nwhile the vertical axis indicates the proportion of samples with a par -\nticular lesion size relative to the total sample\n3849LET-Net: locally enhanced transformer network for¬†medical image segmentation  \n1 3\nlocal-induced decoder that contains cascaded local recon-\nstruction and refinement (LRR) modules to achieve effective \nspatial recovery of high-level features under the adaptive \nguidance of reconstruction kernels and optimization of a \nsplit-attention mechanism. Moreover, to alleviate the class \nimbalance between foreground and background, we design \na mutual loss based on an information-theoretic objective, \nwhich can impose task-relevant restrictions while reducing \ntask-irrelevant noises.\nThe contributions of this paper mainly include: \n(1) We put forward a novel LET-Net, which combines the \nstrengths of Transformer and CNN for accurate medical \nimage segmentation.\n(2) We propose two novel modules, FLE and LRR, to \nenhance the sensitivity of small objects. FLE can \nextract discriminative local cues under the alignment \nof adjacent-level features, while LRR enables effective \nspatial recovery by guiding upsampling of high-level \nfeatures via its adaptive reconstruction kernels and \nrecalibrating features through a split-attention mecha-\nnism.\n(3) To mitigate the class imbalance caused by small targets, \nwe design a mutual information loss, which enables our \nmodel to extract task-relevant information while reduc-\ning task-irrelevant noises.\n(4) By evaluating our LET-Net in challenging colorectal \npolyp segmentation and ultrasound breast segmenta-\ntion, we demonstrate its state-of-the-art segmentation \nability and strong generalization capability.\n2  Related work\n2.1  Medical image segmentation\nWith the great development of deep learning, especially \nconvolutional neural networks (CNNs), various CNN-based \nmethods, such as U-Net [7 ], have significantly improved \nthe performance of medical image segmentation. These \napproaches possess the popular U-shaped encoder‚Äìdecoder \nstructure. To further assist precise segmentation, a battery of \ninnovative improvements based on encoder‚Äìdecoder archi-\ntecture has emerged [26‚Äì 30]. One direction is to design a \nnew module for enhancing the encoder or decoder ability. \nFor instance, Dai et¬†al. [26] designed Ms RED network, \nwhich, respectively, employs a multi-scale residual encod-\ning fusion module (MsR-EFM) and a multi-scale residual \ndecoding module (MsR-DFM) in the encoder and decoder \nstages to improve skin lesion segmentation. In the work [27], \na selective receptive filed module (SRFM) was designed to \nobtain suitable sizes of receptive fields, thereby boosting \nbreast mass segmentation. Another direction is optimizing \nskip connection to facilitate the recovery of spatial informa-\ntion. UNeXt [28] proposed an encoder‚Äìdecoder structure \ninvolving convolutional stages and tokenized MLP stages, \nachieving better segmentation performance while also \nimproving the inference speed. However, these methods \ndirectly fuse unaligned features from different levels, which \nmay hamper accuracy, especially for small objects. In this \npaper, we propose a powerful feature-aligned local enhance-\nment module, which ensures that feature maps at adjacent \nlevels can be well aligned and then explore substantial local \ncues to optimally enhance the discriminative details.\n2.2  Feature alignment\nFeature alignment has drawn much attention and is now \nan active research topic in computer vision. Numerous \nresearchers have devoted considerable effort to addressing \nthis challenge [6, 31‚Äì37]. For instance, SegNet [6 ] utilized \nmax-pooling indices computed in the encoder to perform an \nupsampling operation in the corresponding decoder stage. \nMazzini et¬†al. [32] proposed a guided upsampling module \n(GUM) that generates learnable guided offsets to enhance \nthe upsampling operation. IndexNet [33] built a novel index-\nguided encoder‚Äìdecoder structure in which pooling and \nupsampling operators are guided by self-learned indices. \nAlignSeg [34] learned 2D transformation offsets by a simple \nlearnable interpolation strategy to alleviate feature misalign-\nment. Huang et¬†al. [35] designed an FaPN framework con-\nsisting of feature alignment and feature selection modules, \nachieving substantial and consistent performance improve-\nments on dense prediction tasks. SFNet [31] presented a \nflow alignment module that effectively broadcasts high-level \nsemantic features to high-resolution detail features by its \nsemantic flow. Our method shares a similar aspect with the \nwork [31], in which efficient spatial alignment is achieved by \nlearning offsets. However, unlike these methods, we further \nenhance discriminative representations by subtraction under \nthe premise of aligning low-resolution and high-resolution \nfeatures, which facilitates excavating imperceptible local \ncues related to small objects.\n2.3  Attention mechanism\nAttention-based algorithms have been developed to assist \nin segmentation. In general, attention mechanisms can be \ncategorized into channel attention, spatial attention, and self-\nattention according to different focus perspectives. Inspired \nby the success of SENet [38], various networks [39‚Äì 41] \nhave incorporated the squeeze-and-excitation (SE) module \nto recalibrate features by modeling channel relationships, \nthereby improving segmentation performance. K. Wang \net¬†al. [42] proposed a dual attention network (DANet), which \n3850 N.¬†Ta et al.\n1 3\ncombines position spatial attention and channel attention \nmodules to capture rich contexts.\nAdditionally, Transformer networks based on self-atten-\ntion have been popular in medical image segmentation \n[43‚Äì48]. For instance, TransUnet [43] inserted Transformer \nlayers between CNN-based encoder and decoder stages to \nmodel global contexts, achieving excellent performances in \nmulti-organ and cardiac segmentation. Wu et¬†al. [48] pro-\nposed FAT-Net with a dual encoder that is, respectively, \nbased on CNNs and Transformers for skin lesion segmenta-\ntion. However, the loss of local contexts may still hinder the \nprediction accuracy of Transformer-based methods. In this \npaper, we propose a feature-aligned local enhancement mod-\nule and progressive local-induced decoder, which, respec-\ntively, emphasize local information and adaptively recover \nspatial information to improve predictions.\n3  Method\nFigure¬†2 illustrates our proposed LET-Net, which combines \nTransformer and CNN architectures to achieve accurate \nsegmentation. In the encoder stage, we utilize a pre-trained \npyramid vision transformer (PVT) [19] as the backbone to \nextract hierarchical multi-scale features. Then, three feature-\naligned local enhancement (FLE) modules are inserted in the \nskip connections to enhance discriminative local features. \nAfterward, we employ a novel progressive local-induced \ndecoder composed of cascaded local reconstruction and \nrefinement (LRR) modules to effectively recover spatial \nresolution and produce the final segmentation maps. In what \nfollows, we elaborate on the key components of our model.\n3.1  PVT‚Äëbased encoder\nAlthough CNN-based methods have achieved great success \nin medical image segmentation, they have general limita -\ntions in modeling global contexts. In contrast, pyramid \nvision transformer (PVT) [19] inherits the advantages of \nboth Transformer and CNN while proving to be more effec-\ntive for segmentation. Thus, we choose PVT as the backbone \nto obtain global receptive fields and learn effective multi-\nscale features.\nAs shown in Fig.¬† 2, the PVT-based encoder has four \nstages with a similar architecture. Each stage contains a \npatch embedding layer and multiple Transformer layers. \nBenefiting from its progressive shrinking pyramid and spa-\ntial-reduction attention strategy, the PVT-based encoder can \nproduce multi-scale feature maps with fewer memory costs. \nSpecifically, given an input image X ‚àà ‚ÑùH√óW √ó3 , it produces \nfeatures \n/braceleft.s1Ei /uni007C.var1 ‚â§ i ‚â§ 4/braceright.s1\n , in which Ei‚àà ‚ÑùH‚àï2i+1√óW ‚àï2i+1√óCi . \nTherefore, we obtain high-resolution detail features and \nlow-resolution semantic features, which are beneficial for \nsegmentation.\n3.2  Feature‚Äëaligned local enhancement module\nThe powerful global receptive field of PVT-based encoder \nmakes it challenging for our model to adequately capture \ncritical local details. Although low-level features can pro-\nvide some local context, directly transmitting them to the \ndecoder via a simple skip connection is problematic, as \nthis may introduce a large amount of irrelevant background \ninformation. As a solution, leveraging high-level features \nis an effective way, but one significant issue, i.e., feature \nSpacial\nReduction\nMulti-Head\nAttention\nNorm\nFeed Forward\nNorm\nPatch \nEmbeddingPVT\nStage 2\nPVT\nStage 3\nPVT\nStage 4\nFLE\nInput\nPrediction FLEFLE\nPPAV SDLL\n2E 3E\nLocal reconstruction and refinement moduleLRRFLE Feature-aligned local enhancement module\nPVT\nStage 1\nLRR\n1F\n3D2D1D\nSupervision\nLRRLRR\nPVT-based encoder PVT\nStage i\nProgressive local-induced decoder\nN\nReshape\n1E 4E\nTransformer\nPositional embedding\n2F 3F\nFig. 2  The pipeline of our proposed LET-Net\n3851LET-Net: locally enhanced transformer network for¬†medical image segmentation  \n1 3\nalignment, should be fully considered in this procedure to \nprevent local contexts from being overshadowed by global \ncontexts. To this end, we propose a feature-aligned local \nenhancement (FLE) module, in which informative detailed \nfeatures are effectively captured under the premise of fea-\nture alignment, producing discriminative representation. \nThe internal structure of FLE is illustrated in Fig.¬† 3, and it \nconsists of two steps: feature-aligned discriminative learning \nand local enhancement.\nFeature-aligned discriminative learning  Due to the \ninformation gap between semantics and resolution, feature \nrepresentation is still suboptimal when directly upsam-\npling high-level feature maps to guide low-level features. \nTo obtain strong feature representations, more attention \nand effort should be given to position offset between low-\nlevel and high-level features. Inspired by previous work \n[31], we propose a feature-aligned discriminative learning \n(FDL) block that aligns adjacent-level features and further \nexcavates discriminative features, leading to high sensitiv -\nity to small objects. Within FDL, two 1 √ó 1 convolution \nlayers are first employed to compress adjacent-level fea-\ntures (i.e., Ei and Ei‚àí1 ) into the same channel depth. Then, \na semantic flow field is calculated by a 3 √ó 3 convolution \noperation, as described in Eq.¬† 1\nwhere fs√ós (‚ãÖ) indicates s √ó s convolution layer followed by \nbatch normalization and a ReLU activation function, while \n¬© and U(‚ãÖ) , respectively, represent concatenation and upsam-\npling operation. Next, according to learned semantic flow \n/u1D6E5i‚àí1 , we obtain a feature-aligned high-resolution feature ÃÉEi \nwith semantic cues, Mathematically\nwhere Warp(‚ãÖ) indicates the mapping function, Ei is a C i \ndimensional feature map defined on the spatial grid /u1D6FAi of \nthe specific size \n/parenleft.s1H ‚àï2i+1,W ‚àï2i+1/parenright.s1\n . Schematically as shown \nin Fig.¬† 4, the warp procedure consists of two steps. In the \n(1)/u1D6E5i‚àí1 = f3√ó3\n/parenleft.s1f1√ó1\n/parenleft.s1Ei‚àí1\n/parenright.s1¬©U /parenleft.s1f1√ó1\n/parenleft.s1Ei\n/parenright.s1/parenright.s1/parenright.s1,\n(2)ÃÉEi = Warp (f1√ó1\n/parenleft.s1Ei\n/parenright.s1,ùõ•i‚àí1),\nFig. 3  The architecture of feature-aligned local enhancement module, \nwhich performs two steps: First, feature-aligned discriminative learn-\ning initially produces a flow field to align adjacent features and then \nconstructs discriminative representation using subtraction and a resid-\nual connection. Second, local enhancement with a dense connection \nstructure is adopted to highlight local details\nFig. 4  An illustration of the \nwarp procedure\n\n3852 N.¬†Ta et al.\n1 3\nfirst step, each point p i‚àí1 on the spatial grid /u1D6FAi‚àí1 is mapped \nto p i on low-resolution feature, which is formulated by Eq.¬†3\nIt is worth mentioning that due to the resolution gap between \nthe flow field and features (see Fig.¬† 4), Eq.¬† 3 contains a \nhalved operation to reduce the resolution. In the second step, \nwe adopt the differentiable bilinear sampling mechanism \n[49] to approximate the final feature ÃÉEi by linearly inter -\npolating the scores of four neighboring points (top-right, \ntop-left, bottom-right, and bottom-left) of p i.\nAfter that, to enhance the discriminative local context \nrepresentation, we further utilize subtraction, absolute value, \nand residual learning procedures. Conclusively, the final \noptimized feature ÃÇEi‚àí1 can be expressed as follows:\nLocal enhancement In the PVT-based encoder, attention is \nestablished between each patch, allowing information to be \nblended from all other patches, even if their correlation is \nnot high. Meanwhile, since small targets only occupy a por-\ntion of the entire image, the global interaction in transformer \narchitecture cannot fully meet the requirements of small \ntarget segmentation where more detailed local contexts are \nneeded. Considering that the convolution operation with a \nfixed receptive field can blend the features of each patch‚Äôs \nneighboring patches, we construct a local enhancement \n(LE) block to increase the weights associated with adja -\ncent patches to the center patch using convolution, thereby \nemphasizing the local features of each patch.\nAs shown in Fig.¬† 3, LE has a convolution-based struc-\nture and consists of four stages. Each stage includes a 3 √ó 3 \nconvolutional layer followed by batch normalization and \na ReLu activation layer (denoted as f3√ó3(‚ãÖ) ). Additionally, \n(3)p i = p i‚àí1 + /u1D6E5i‚àí1(p i‚àí1)\n2 .\n(4)ÃÇEi‚àí1 =‚à£ Ei‚àí1 ‚àí ÃÉEi ‚à£+ Ei‚àí1 .\ndense connections are added to encourage feature reuse and \nstrengthen local feature propagation. As a result, the fea-\nture map obtained by LE contains rich local contexts. Let \nx0 denote the initial input, and the outputs of ith stage within \nLE can be formulated as follows:\nwhere []  represents the concatenation operation. In sum-\nmary, LE utilizes the local receptive field of the convo-\nlution operation and dense connections to achieve local \nenhancement.\n3.3  Progressive local‚Äëinduced decoder\nEfficient recovery of spatial information is critical in medical \nimage segmentation, especially for small objects. Inspired \nby previous works [50, 51], we propose a progressive local-\ninduced decoder to adaptively restore feature resolution and \ndetailed information. As shown in Fig.¬† 2, the decoder con-\nsists of three cascaded local reconstruction and refinement \n(LRR) modules. The internal structure of LRR is illustrated \nin Fig.¬† 5, where two steps are performed: local-induced \nreconstruction (LR) and split-attention-based refinement \n(SAR).\nLocal-induced reconstruction LR aims to transfer the \nspatial detail information from low-level features into high-\nlevel features, thereby facilitating accurate spatial recovery \nof high-level features. As shown in Fig.¬†5, LR first produces \na reconstruction kernel /u1D705‚àà ‚Ñùk2√óH i‚àí1√óW i‚àí1 based on low-level \nfeature Fi‚àí1 and high-level feature Di , in which k indicates \nthe neighborhood size for reconstructing local features. The \nprocedure of generating the reconstruction kernel /u1D705 can be \nexpressed as follows:\n(5)x i =\n/braceleft.s3f3√ó3\n/parenleft.s1x 0\n/parenright.s1, i = 1,\nf3√ó3\n/parenleft.s1/bracketleft.s1x 0,‚ãØ,x i‚àí1\n/bracketright.s1/parenright.s1,2 ‚â§ i ‚â§ 4,\nFig. 5  The structure of local reconstruction and refinement module. It contains two blocks: local-induced reconstruction and split-attention-\nbased refinement\n3853LET-Net: locally enhanced transformer network for¬†medical image segmentation  \n1 3\nwhere fs√ós (‚ãÖ) represents an s √ó s convolution layer followed \nby batch normalization and a ReLU activation function. U(‚ãÖ) , \n¬© , and Soft(‚ãÖ) , respectively, indicate upsampling, concatena-\ntion, and Softmax activation operations. Meanwhile, another \n3 √ó 3 convolution and upsampling operation are applied on \nDi to obtain ÃÉDi with the same resolution size as Fi‚àí1 . Math-\nematically, ÃÉDi = U/parenleft.s1f3√ó3\n/parenleft.s1Di\n/parenright.s1/parenright.s1\n . Note that, D4 = E4 here. Next, \nwe optimize pixel ÃÉDi[u, v] under the guidance of reconstruc-\ntion kernel /u1D705[u,v] ‚àà ‚Ñùk√ók , producing refined local feature \nÃÇDi[u, v] . This can be written as Eq.¬†7, where r = ‚åäk‚àï2‚åã\nSubsequently, ÃÇDi and Fi‚àí1 are concatenated together and then \npassed through two convolutional layers to produce an opti-\nmized feature. Conclusively, LR overcomes the limitations \nof traditional upsampling operations in precisely recover -\ning pixel-wise prediction, since it takes full advantage of \nlow-level features to adaptively predict reconstruction kernel \nand then effectively combines semantic contexts with spa-\ntial information toward accurate spatial recovery. This can \nstrengthen the recognition of small objects.\nSplit-attention-based refinement To enhance feature \nrepresentation, we implement an SAR block in which \ngrouped sub-features are further split and fed into two par -\nallel branches to capture channel dependencies and pixel-\nlevel pairwise relationships through two types of attention \nmechanisms. As shown in Fig.¬† 5, SAR is composed of two \nbasic components: a spatial attention block and a chan-\nnel attention block. Given an input feature map M , SAR \nfirst divides it along the channel dimension to produce \nM = /braceleft.s1M1 ,M2 ,‚ãÖ‚ãÖ‚ãÖ ,MG\n/braceright.s1 . For each M i , valuable responses \nare specified by attention mechanisms. Specifically, M i is \nsplit into two features, denoted as M 1\ni and M 2\ni , which are \nseparately fed into the channel attention block and spatial \nattention block to reconstruct features. This allows our \nmodel to focus on ‚Äúwhat‚Äù and ‚Äúwhere‚Äù are valuable through \nthese two blocks.\nIn channel attention block, global average pooling \n(denoted as GAP(‚ãÖ) ) is performed to produce channel-wise \nstatistics, which can be formulated as\nThen, channel-wise dependencies are captured according to \nthe guidance of a compact feature, which is generated by a \nSigmoid function (i.e., Sig(‚ãÖ) ). Mathematically\n(6)/u1D705= Soft /parenleft.s1f3√ó3\n/parenleft.s1U(f1√ó1\n/parenleft.s1Di\n/parenright.s1)¬©f 1√ó1\n/parenleft.s1/u1D405i‚àí1\n/parenright.s1/parenright.s1/parenright.s1,\n(7)ÃÇDi[u, v] =\nr/uni2211.s1\nm=‚àír\nr/uni2211.s1\nn=‚àír\nùúÖ[u,v][m, n] √ó ÃÉDi[u + m, v + n].\n(8)S= GAP(M 1\ni)= 1\nH √ó W\nH/uni2211.s1\nm=1\nW/uni2211.s1\nn=1\nM 1\ni(m, n).\n(9)ÃÉM 1\ni = Sig/parenleft.s1W 1 √ó S+ b1\n/parenright.s1√ó M 1\ni,\nin which parameters W1 and b1 are used for scaling and shift-\ning S.\nIn spatial attention block, spatial-wise statistics are cal-\nculated using Group Norm (GN) [52] on M 2\ni . The pixel-wise \nrepresentation is then strengthened by another compact fea-\nture calculated by two parameters W2 and b2 and a Sigmoid \nfunction. This process can be expressed as\nNext, ÃÉM 1\ni and ÃÉM 2\ni are optimized by an additional consistency \nembedding path and then concatenated. This procedure is \nrepresented as\nAfter aggregating all sub-features, a channel shuffle [53] \nis performed to facilitate cross-group information exchange \nalong the channel dimension.\n3.4  Mutual information loss\nAs stated in the previous study [ 54], training models with \nonly pixel-wise loss may limit segmentation performance, \nespecially resulting in prediction errors for small objects. \nThis is due to class imbalance between foreground and back-\nground, such that task-relevant information is overwhelmed \nby irrelevant noise. Therefore, to facilitate preserving task-\nrelevant information, we explore novel supervision at the \nfeature level to further assist accurate segmentation. Let X \nand Y denote the input medical image and its correspond-\ning ground truth, respectively. Z represents the deep feature \nextracted from input X.\nMutual information (MI) Mutual information is a funda-\nmental quantity that measures the amount of information \nshared between two random variables. Mathematically, the \nstatistical dependency of Y  and Z can be quantified by MI, \nwhich is expressed as\nwhere p(Y, Z) is the probability distribution between Z  and \nY, while p(Z) and p(Y) are their marginals.\nMutual Information Loss Our primary objective is to \nmaximize the amount of task-relevant information about Y \nin the latent feature Z while reducing irrelevant information. \nThis is achieved by two mutual information terms [55, 56]. \nFormally \nOwing to the notorious difficulty of the conditional MI com-\nputations, these terms are estimated by existing MI estima-\ntors [56, 57]. In detail, the first term is accomplished through \n(10)ÃÉM 2\ni = Sig/parenleft.s1W 2 √ó GN /parenleft.s1M 2\ni\n/parenright.s1+ b2\n/parenright.s1√ó M 2\ni.\n(11)ÃÉM i = /parenleft.s1ÃÉM 1\ni + M 1\ni √ó M 2\ni\n/parenright.s1¬© /parenleft.s1ÃÉM 2\ni + M 1\ni √ó M 2\ni\n/parenright.s1.\n(12)I(Y ;Z) = /u1D53Cp(Y ,Z)\n/bracketleft.s3\nlog p(Y ,Z)\np(Y )p(Z)\n/bracketright.s3\n,\n(13)IB(Y,Z)= Max I(Z;Y) ‚àí I(Z;X).\n3854 N.¬†Ta et al.\n1 3\nthe use of Pixel Position Aware (PPA) loss [57] ( LPPA ). Since \nPPA loss assigns different weights to different positions, it \ncan better explore task-relevant structure information and \ngive more attention to important details. The second term is \nestimated by Variational Self-Distillation (VSD) [56] ( LVSD ) \nthat uses KL-divergence to compress Z  and remove irrel-\nevant noises, thereby addressing the effect of imbalances in \nthe number of foreground and background pixels caused by \nsmall targets. Thus, our total loss can be expressed as\n \n4  Experiments\n4.1  Experimental setup\n4.1.1  Implementation details\nWe implement our experiments based on the hardware envi-\nronment with NVIDIA GeForce RTX 3090. The AdamW \nalgorithm is chosen to optimize our model‚Äôs parameters, and \nthe initial learning rate is set to 1e-4. During training, a multi-\nscale training strategy is employed, in which input images \nare reshaped according to a ratio of [0.75, 1, 1.25]. The total \nnumber of epochs and batch size are set to 200 and 16, respec-\ntively. In the pre-processing step, all images and corresponding \nground truths are resized to 352 √ó 352 in our experiments.\n4.1.2  Datasets\nTo verify the capability of our proposed model, we evalu-\nate LET-Net in two medical image segmentation tasks. For \npolyp segmentation, we utilize five public benchmarks: CVC-\nClinicDB [62], Kvasir [63], CVC-ColonDB [64], ETIS-\nLaribPolypDB [ 65], and CVC-300 [66 ]. To ensure a fair \ncomparison, we follow the work [59] and divide large-scale \nCVC-ClinicDB and Kvasir datasets into training, validation, \nand testing datasets in a ratio of [8:1:1], while the remaining \nthree datasets are used only for testing to evaluate the model‚Äôs \ngeneralization abilities. For breast lesion segmentation task, \nwe choose the public breast ultrasound dataset (BUSIS) [67] to \nassess the effectiveness of our LET-Net. This dataset includes \n133 normal cases, 437 benign cases, and 210 malignant cases. \nWe follow the same settings as work [2] to separately conduct \nexperiments on benign and malignant samples.\n4.1.3  Evaluation metrics\nAs done in recent related work of polyp segmentation [20], \nwe employ both mean Dice (mDice) and mean IoU (mIoU) \nto quantitatively evaluate the performance of our model and \n(14)L total = L PPA + L VSD .\nother state-of-the-art methods on polyp benchmarks. For \nbreast lesion segmentation, we adopt four widely used met-\nrics, including Accuracy, Jaccard index, Precision, and Dice \nto validate the segmentation performance in our study. Theo-\nretically, high scores for all metrics indicate better results.\n4.2  Experimental results\nTo investigate the effectiveness of our proposed method, we \nvalidate LET-Net in two applications: polyp segmentation \nfrom coloscopy images and breast lesion segmentation from \nultrasound images.\n4.2.1  Polyp segmentation\nQuantitative comparison To demonstrate the effectiveness \nof our LET-Net, we compare it to several state-of-the-art \nmethods on five polyp benchmarks. Table¬† 1 summarizes \nthe quantitative experimental results in detail. From it, we \ncan see that our LET-Net outperforms the other methods \non all datasets. Concretely, on the seen CVC-ClinicDB \ndataset, it achieves significantly higher mDice and mIoU \nscores (94.5% and 89.9%, respectively). On Kvasir dataset, \nour method exceeds SANet [20] and BLE-Net [61] by 2.2% \nand 2.1% mDice improvements, respectively. The underly -\ning reason for their limited performance is that these two \nmethods follow a pure CNN architecture, which lacks global \nlong-range dependencies. By contrast, our method captures \nglobal contexts by its PVT-based encoder, and further exca-\nvates valuable local information using FLE module, demon-\nstrating superior segmentation ability. Most importantly, our \nLET-Net still exhibits excellent generalization capabilities \nwhen applied to unseen datasets (i.e., CVC-ColonDB, ETIS-\nLaribPolypDB, and CVC-300). Specifically, LET-Net gets \nahead of the CNN-based SOTA CaraNet [22] by 2.2% and \n2.8% in terms of mDice and mIoU on CVC-ColonDB. Com-\npared with other Transformer-based approaches, our LET-\nNet also presents excellent segmentation and generalization \nabilities. Concretely, on ETIS-LaribPolypDB dataset, we can \nobserve that LET-Net achieves 4.7% and 4.2% higher mDice \nthan SETR-PUP [18] and TransUnet [43], respectively. This \nperformance improvement can be attributed to two factors. \nOne is that our proposed FLE module compensates for the \nloss of local details in the Transformer architecture. The \nother is that the LRR module effectively recovers spatial \ninformation.\nVisual Comparison To further evaluate the proposed \nLET-Net intuitively, we visualize some segmentation maps \nproduced by our model and other methods in Fig.¬† 6. It is \napparent that our LET-Net can not only clearly highlight \npolyp regions but also identify small polyps more accu-\nrately than other counterparts. This is mainly because our \nmethod effectively leverages and combines global and local \n3855LET-Net: locally enhanced transformer network for¬†medical image segmentation  \n1 3\ncontexts. In addition, we introduce mutual information loss \nas an assistant to learning task-relevant representation. Fur-\nthermore, we find that our LET-Net successfully deals with \nother challenging cases, including cluttered backgrounds \n(Fig.¬†6 (b),(c), (g), (i)) and low contrast (Fig.¬†6 (a),(h)). For \nexample, as illustrated in Fig.¬† 6 (b),(i), ACSNet [39] and \nPraNet [59] misidentify background tissues as polyps, but \nour LET-Net overcomes this drawback. Due to combining \nthe strengths of CNN and Transformer, our LET-Net pro-\nduces good segmentation performance in these scenarios. \nOverall, our model achieves leading performance.\n4.2.2  Breast lesion segmentation\nQuantitative comparison To further evaluate the effective -\nness of our method, we conduct extensive experiments \nin breast lesion segmentation and perform a comparative \nanalysis with ten segmentation approaches. Table¬† 2 pre-\nsents the detailed quantitative comparison among differ -\nent methods on BUSIS dataset. Obviously, our LET-Net \nexhibits excellent performance in both benign and malig-\nnant lesion segmentation. In benign lesion segmentation, \nLET-Net achieves 97.7% Accuracy, 74% Jaccard, 83.5% \nPrecision, and 81.5% Dice. Compared with other com-\npetitors, LET-Net significantly outperforms them by a \nlarge margin. In detail, it, respectively, excels C-Net [2 ], \nCPF-Net [29], and PraNet [59] by 1.6%, 4.1%, and 4.9% \nin terms of Jaccard. Meanwhile, in malignant lesion seg-\nmentation, we obtain an Accuracy score of 93% and a Dice \nscore of 72.7%, respectively, demonstrating the superiority \nof our LET-Net over other methods. In particular, LET-Net \npresents a significant improvement of 1.8% in Jaccard and \n2.8% in Dice compared with C-Net [ 2]. The reason behind \nthis is that although C-Net constructs a bidirectional atten-\ntion guidance network to capture both global and local \nfeatures, long-range dependencies are not fully modeled \ndue to the limitations of convolution.\nVisual comparison: To intuitively demonstrate the per -\nformance of our model, we present segmentation results of \ndifferent methods in Fig.¬† 7. We observe that other methods \noften produce segmentation maps with incomplete lesion \nstructures or false positives, while our prediction maps \nare superior to others. This is mainly due to our FLE‚Äôs \nability to facilitate discriminative local feature learning \nand the effectiveness of our proposed LRR module for \nspatial reconstruction. In addition, it is worth noting that \nour LET-Net performs well in handling various shapes \n[Fig.¬† 7(a)‚Äì(h)] and low-contrast images [Fig.¬† 7(d)(h)], \nwhich can be attributed to the powerful and robust feature \nlearning ability of LET-Net.\nTable 1  Comparisons between \ndifferent method in polyp \nsegmentation task. The best \nresults are highlighted in bold\nMethod Seen dataset Unseen dataset\nCVC-ClinicDB Kvasir CVC-ColonDB ETIS- Larib-\nPolypDB\nCVC-300\nmDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU\nFCN [5] 0.825 0.747 0.775 0.686 0.578 0.481 0.379 0.313 0.660 0.558\nU-Net [7] 0.842 0.775 0.818 0.746 0.512 0.444 0.398 0.335 0.710 0.627\nUNet++ [30] 0.846 0.774 0.821 0.743 0.599 0.499 0.456 0.375 0.707 0.624\nAttentionU-Net [11] 0.809 0.744 0.782 0.694 0.614 0.524 0.440 0.360 0.686 0.580\nDCRNet [58] 0.896 0.844 0.886 0.825 0.704 0.631 0.556 0.496 0.856 0.788\nSegNet [8] 0.915 0.857 0.878 0.814 0.647 0.570 0.612 0.529 0.841 0.773\nSFA [1] 0.700 0.607 0.723 0.611 0.469 0.347 0.297 0.217 0.467 0.329\nPraNet [59] 0.899 0.849 0.898 0.840 0.709 0.640 0.628 0.567 0.871 0.797\nACSNet [39] 0.912 0.858 0.907 0.850 0.709 0.643 0.609 0.537 0.862 0.784\nEU-Net [60] 0.902 0.846 0.908 0.854 0.756 0.681 0.687 0.609 0.837 0.765\nSANet [20] 0.916 0.859 0.904 0.847 0.753 0.670 0.750 0.654 0.888 0.815\nBLE-Net [61] 0.926 0.878 0.905 0.854 0.731 0.658 0.673 0.594 0.879 0.805\nCaraNet [22] 0.936 0.887 0.918 0.865 0.773 0.689 0.747 0.672 0.903 0.838\nSETR-PUP [18] 0.934 0.885 0.911 0.854 0.773 0.690 0.726 0.646 0.889 0.814\nTransUnet [43] 0.935 0.887 0.913 0.857 0.781 0.699 0.731 0.660 0.893 0.824\nLET-Net(Ours) 0.945 0.899 0.926 0.876 0.795 0.717 0.773 0.698 0.907 0.839\n3856 N.¬†Ta et al.\n1 3\n4.3  Ablation study\nIn this section, we conduct a series of ablation studies \nto verify the effectiveness of each critical component in \nour proposed LET-Net, including FLE, LRR, and mutual \ninformation loss.\nFig. 6  Visualization results of our LET-Net and several other methods on five polyp datasets. From top to down, the images are from CVC-\nClinicDB, Kvasir, CVC-ColonDB, ETIS-LaribPolypDB, and CVC-300, which are separated by red dashed lines\nTable 2  Comparison with \ndifferent state-of-the-art \nmethods on BUSIS dataset\nMethod Benign lesion Malignant lesion\nAccuracy Jaccard Precision Dice Accuracy Jaccard Precision Dice\nU-Net [7] 0.966 0.615 0.750 0.705 0.901 0.511 0.650 0.635\nSTAN [21] 0.969 0.643 0.744 0.723 0.910 0.511 0.647 0.626\nAttentionU-Net [11] 0.969 0.650 0.752 0.733 0.912 0.511 0.616 0.630\nAbraham et¬†al. [68] 0.969 0.667 0.767 0.748 0.915 0.541 0.675 0.658\nUNet++ [30] 0.971 0.683 0.759 0.756 0.915 0.540 0.655 0.655\nUNet3+ [69] 0.971 0.676 0.756 0.751 0.916 0.548 0.658 0.662\nSegNet [8] 0.972 0.679 0.770 0.755 0.922 0.549 0.638 0.659\nPraNet [59] 0.972 0.691 0.799 0.763 0.925 0.582 0.763 0.698\nCPF-Net [29] 0.973 0.699 0.801 0.766 0.927 0.605 0.755 0.716\nC-Net [2] 0.975 0.724 0.827 0.794 0.926 0.597 0.757 0.699\nLET-Net(Ours) 0.977 0.740 0.835 0.815 0.930 0.615 0.772 0.727\n3857LET-Net: locally enhanced transformer network for¬†medical image segmentation  \n1 3\n4.3.1  Impact of¬†FLE and¬†LRR modules\nTo validate the effectiveness of FLE and LRR modules, \nwe remove them individually from our full net, resulting \nin two variants, namely w/o FLE and w/o LRR. As shown \nin Table¬† 3, the variant without FLE (w/o FLE) achieves \na 93.6% mDice score on CVC-ClinicDB dataset. When \nwe apply the FLE module, the mDice score increases to \n94.5%. Moreover, it boosts mDice by 1.6%, 2.2%, and \n2.3% on CVC-ColonDB, ETIS-LaribPolypDB, and CVC-\n300 datasets, respectively. These results indicate that our \nFLE module effectively supports accurate segmentation \ndue to its ability to learn discriminative local features \nunder the feature alignment condition. Furthermore, when \ncomparing the second and third lines of Table¬† 3, it can \nbe seen that LRR module is also conducive to segmenta-\ntion, with performance gains of 1.6% and 1.7% in terms \nof mDice and mIoU on Kvasir dataset. The main reason is \nFig. 7  Visual comparison among different methods in breast lesion segmentation, where the segmentation results of benign and malignant \nlesions are separated by a red dashed line\n3858 N.¬†Ta et al.\n1 3\nthat LRR module is capable of effective spatial recovery \nvia its dynamic reconstruction kernels and split-attention \nmechanism, thereby facilitating segmentation.\n4.3.2  Effectiveness of¬†mutual information loss\nTo validate the effectiveness and necessity of our mutual \ninformation loss, we retrain our proposed LET-Net with \ndifferent loss settings. Specifically, we denote three vari-\nants, i.e., w/o LPPA , w/o LVSD , and w/o LPPA& LVSD , each of \nwhich removes the corresponding loss item. Note that we \napply conventional binary-cross entropy loss to supervise \nour model when removing LPPA . Table¬†4 reports the quan-\ntitative evaluation. Comparing the first and fourth lines in \nTable¬†4, we can observe that our model performs poorly \nwithout PPA loss supervision, obtaining a 1.1% lower mIoU \non CVC-ClinicDB dataset. Also, a similar dropping situation \noccurs with the variant w/o LVSD . Specifically, our model has \nwitnessed performance degradation without LVSD , decreas-\ning mIoU by 1.5%, 1%, and 1.3%, respectively, on CVC-\nColonDB, ETIS-LaribPolypDB, and CVC-300 datasets. \nThis confirms that each term in our total loss is effective for \nsegmentation. The reasons can be summarized as: first, in \ncontrast to binary-cross entropy loss, PPA loss can guide our \nmodel to pay more attention to local details by synthesizing \nlocal structure information of a pixel, resulting in superior \nperformance. Second, LVSD assists task-relevant feature \nlearning, thereby improving the sensitivity of small objects. \nIn addition, it can be seen that our method outperforms w/o \nLPPA& LVSD by a large margin, achieving 2.3% mDice and \n2.5% mIoU performance gains with the help of our mutual \ninformation loss on CVC-ColonDB dataset. In summary, our \nexperimental results fully demonstrate that mutual informa-\ntion loss is beneficial for LET-Net.\n5  Conclusion\nIn this work, we propose a novel locally enhanced trans -\nformer network for accurate medical image segmentation. \nOur model adopts a PVT-based encoder to extract global \ncontexts and utilizes a feature-aligned local enhancement \nmodule to highlight detailed local contexts while effectively \nrecovering high-resolution spatial information by its pro-\ngressive local-induced decoder. In addition, we design a \nmutual information loss to encourage our LET-Net to learn \npowerful representations from the task-relevant perspective. \nLET-Net is validated in polyp and breast lesion segmenta-\ntion and achieves state-of-the-art performance, especially \ndemonstrating its ability for small target segmentation. In \nfuture work, we aim to apply our proposed LET-Net to other \nmedical image segmentation tasks with different modali-\nties or anatomies, thereby developing our model to be more \nrobust.\nAcknowledgements This research is supported by the National Natural \nScience Foundation of China (62276112), the National Natural Sci-\nence Foundation of China Regional Joint Fund of NSFC (U19A2057), \nJilin Province Science and Technology Development Plan Key R &D \nProject (20230201088GX), and Collaborative Innovation Project of \nAnhui Universities (GXXT-2022-044).\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nTable 3  Ablation analysis w.r.t \nthe effectiveness of FLE and \nLRR modules. The best results \nare shown in bold\nMethod Seen dataset Unseen dataset\nCVC-ClinicDB Kvasir CVC-ColonDB ETIS- Larib-\nPolypDB\nCVC-300\nmDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU\nw/o FLE 0.936 0.887 0.918 0.871 0.779 0.698 0.751 0.674 0.884 0.816\nw/o LRR 0.940 0.894 0.910 0.859 0.790 0.711 0.759 0.681 0.890 0.821\nLET-Net 0.945 0.899 0.926 0.876 0.795 0.717 0.773 0.698 0.907 0.839\nTable 4  Ablation analysis of \nmutual information loss. The \nbest results are shown in bold\nLoss setting Seen dataset Unseen dataset\nCVC-ClinicDB Kvasir CVC-ColonDB ETIS- Larib-\nPolypDB\nCVC-300\nmDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU\nw/o LPPA 0.937 0.888 0.917 0.864 0.782 0.697 0.737 0.663 0.885 0.812\nw/o LVSD 0.940 0.892 0.923 0.872 0.785 0.702 0.762 0.688 0.895 0.826\nw/o LPPA& LVSD 0.934 0.882 0.914 0.861 0.772 0.692 0.716 0.648 0.879 0.807\nLET-Net 0.945 0.899 0.926 0.876 0.795 0.717 0.773 0.698 0.907 0.839\n3859LET-Net: locally enhanced transformer network for¬†medical image segmentation  \n1 3\nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article‚Äôs Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article‚Äôs Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n 1. Fang, Y., Chen, C., Yuan, Y., Tong, R.K.: Selective feature aggre-\ngation network with area-boundary constraints for polyp segmen-\ntation. In: International Conference on Medical Image Computing \nand Computer-Assisted Intervention, pp. 302‚Äì310 (2019). https:// \ndoi. org/ 10. 1007/ 978-3- 030- 32239-7_ 34\n 2. Chen, G., Dai, Y., Zhang, J.: C-net: Cascaded convolutional neural \nnetwork with global guidance and refinement residuals for breast \nultrasound images segmentation. Comput. Methods. Programs \nBiomed. 225, 107086 (2022)\n 3. Thomas, E., Pawan, S., Kumar, S., Horo, A., Niyas, S., Vinay -\nagamani, S., Kesavadas, C., Rajan, J.: Multi-res-attention unet: a \ncnn model for the segmentation of focal cortical dysplasia lesions \nfrom magnetic resonance images. IEEE J. Biomed. Health Infor-\nmat. 25(5), 1724‚Äì1734 (2020)\n 4. Wang, R., Lei, T., Cui, R., Zhang, B., Meng, H., Nandi, A.K.: \nMedical image segmentation using deep learning: A survey. IET \nImage Process. 16(5), 1243‚Äì1267 (2022). https:// doi. org/ 10. 1049/ \nipr2. 12419\n 5. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks \nfor semantic segmentation. In: Proceedings of the IEEE Confer -\nence on Computer Vision and Pattern Recognition, pp. 3431‚Äì3440 \n(2015). https:// doi. org/ 10. 1109/ CVPR. 2015. 72989 65\n 6. Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: A deep \nconvolutional encoder-decoder architecture for image segmenta-\ntion. IEEE Trans. Pattern Anal. Mach. Intell. 39(12), 2481‚Äì2495 \n(2017). https:// doi. org/ 10. 1109/ TPAMI. 2016. 26446 15\n 7. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional \nnetworks for biomedical image segmentation. In: International \nConference on Medical Image Computing and Computer-assisted \nIntervention, pp. 234‚Äì241 (2015). https:// doi. org/ 10. 1007/ 978-3- \n319- 24574-4_ 28\n 8. Lou, A., Guan, S., Loew, M.: Cfpnet-m: A light-weight encoder-\ndecoder based network for multimodal biomedical image real-time \nsegmentation. Comput. Biol. Med. 154, 106579 (2023)\n 9. Xie, X., Pan, X., Zhang, W., An, J.: A context hierarchical inte-\ngrated network for medical image segmentation. Comput. Elect. \nEng. 101, 108029 (2022). https:// doi. org/ 10. 1016/j. compe leceng. \n2022. 108029\n 10. Wang, R., Ji, C., Zhang, Y., Li, Y.: Focus, fusion, and rectify: \nContext-aware learning for covid-19 lung infection segmentation. \nIEEE Trans. Neural Netw. Learn. Syst. 33(1), 12‚Äì24 (2021)\n 11. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., \nMisawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., \net¬†al.: Attention u-net: Learning where to look for the pancreas. \narXiv preprint arXiv: 1804. 03999 (2018)\n 12. Cheng, J., Tian, S., Yu, L., Lu, H., Lv, X.: Fully convolutional \nattention network for biomedical image segmentation. Artif. \nIntell. Med. 107, 101899 (2020)\n 13. Wang, X., Jiang, X., Ding, H., Liu, J.: Bi-directional dermoscopic \nfeature learning and multi-scale consistent decision fusion for skin \nlesion segmentation. IEEE Trans. Image Processing 29, 3039‚Äì\n3051 (2019)\n 14. Wang, X., Li, Z., Huang, Y., Jiao, Y.: Multimodal medical image \nsegmentation using multi-scale context-aware network. Neuro-\ncomputing 486, 135‚Äì146 (2022). https:// doi. org/ 10. 1016/j. neu-\ncom. 2021. 11. 017\n 15. Liang, X., Li, N., Zhang, Z., Xiong, J., Zhou, S., Xie, Y.: Incor -\nporating the hybrid deformable model for improving the perfor -\nmance of abdominal ct segmentation via multi-scale feature fusion \nnetwork. Med. Image Anal. 73, 102156 (2021)\n 16. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for \ndense prediction. In: Proceedings of the IEEE/CVF International \nConference on Computer Vision, pp. 12179‚Äì12188 (2021)\n 17. Li, Y., Wang, Z., Yin, L., Zhu, Z., Qi, G., Liu, Y.: X-net: a dual \nencoding‚Äìdecoding method in medical image segmentation. The \nVisual Computer, pp. 1‚Äì11 (2021)\n 18. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., \nFeng, J., Xiang, T., Torr, P.H., et¬†al.: Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with transformers. \nIn: Proceedings of the IEEE/CVF Conference on Computer Vision \nand Pattern Recognition, pp. 6881‚Äì6890 (2021)\n 19. Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., \nLuo, P., Shao, L.: Pyramid vision transformer: A versatile back -\nbone for dense prediction without convolutions. In: Proceedings \nof the IEEE/CVF International Conference on Computer Vision, \npp. 568‚Äì578 (2021)\n 20. Wei, J., Hu, Y., Zhang, R., Li, Z., Zhou, S.K., Cui, S.: Shallow \nattention network for polyp segmentation. In: International Con-\nference on Medical Image Computing and Computer-Assisted \nIntervention, pp. 699‚Äì708 (2021). Springer\n 21. Shareef, B., Xian, M., Vakanski, A.: Stan: Small tumor-aware \nnetwork for breast ultrasound image segmentation. In: 2020 \nIEEE 17th International Symposium on Biomedical Imaging \n(ISBI), pp. 1‚Äì5 (2020)\n 22. Lou, A., Guan, S., Ko, H., Loew, M.H.: Caranet: context axial \nreverse attention network for segmentation of small medical \nobjects. In: Medical Imaging 2022: Image Processing, vol. \n12032, pp. 81‚Äì92 (2022)\n 23. Valanarasu, J.M.J., Sindagi, V.A., Hacihaliloglu, I., Patel, V.M.: \nKiu-net: Towards accurate segmentation of biomedical images \nusing over-complete representations. In: International Confer -\nence on Medical Image Computing and Computer-assisted \nIntervention, pp. 363‚Äì373 (2020). Springer\n 24. Pang, Y., Zhao, X., Xiang, T.-Z., Zhang, L., Lu, H.: Zoom in \nand out: A mixed-scale triplet network for camouflaged object \ndetection. In: Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition, pp. 2160‚Äì2170 \n(2022)\n 25. Jia, Q., Yao, S., Liu, Y., Fan, X., Liu, R., Luo, Z.: Segment, \nmagnify and reiterate: Detecting camouflaged objects the hard \nway. In: Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition, pp. 4713‚Äì4722 (2022)\n 26. Dai, D., Dong, C., Xu, S., Yan, Q., Li, Z., Zhang, C., Luo, N.: \nMs red: A novel multi-scale residual encoding and decoding \nnetwork for skin lesion segmentation. Med. Image Anal. 75, \n102293 (2022)\n 27. Xu, C., Qi, Y., Wang, Y., Lou, M., Pi, J., Ma, Y.: Arf-net: An adap-\ntive receptive field network for breast mass segmentation in whole \n3860 N.¬†Ta et al.\n1 3\nmammograms and ultrasound images. Biomed. Signal Process \nControl 71, 103178 (2022)\n 28. Valanarasu, J.M.J., Patel, V.M.: Unext: Mlp-based rapid medical \nimage segmentation network. arXiv preprint arXiv: 2203. 04967  \n(2022)\n 29. Feng, S., Zhao, H., Shi, F., Cheng, X., Wang, M., Ma, Y., Xiang, \nD., Zhu, W., Chen, X.: Cpfnet: Context pyramid fusion network \nfor medical image segmentation. IEEE Trans. Med. Imaging \n39(10), 3008‚Äì3018 (2020). https:// doi. org/ 10. 1109/ TMI. 2020. \n29837 21\n 30. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: \nRedesigning skip connections to exploit multiscale features in \nimage segmentation. IEEE Trans. Med. Imaging 39 (6), 1856‚Äì\n1867 (2019)\n 31. Li, X., You, A., Zhu, Z., Zhao, H., Yang, M., Yang, K., Tan, S., \nTong, Y.: Semantic flow for fast and accurate scene parsing. In: \nEuropean Conference on Computer Vision, pp. 775‚Äì793 (2020)\n 32. Mazzini, D.: Guided upsampling network for real-time semantic \nsegmentation. arXiv preprint arXiv: 1807. 07466 (2018)\n 33. Lu, H., Dai, Y., Shen, C., Xu, S.: Indices matter: Learning to index \nfor deep image matting. In: Proceedings of the IEEE/CVF Inter -\nnational Conference on Computer Vision, pp. 3266‚Äì3275 (2019)\n 34. Huang, Z., Wei, Y., Wang, X., Liu, W., Huang, T.S., Shi, H.: \nAlignseg: Feature-aligned segmentation networks. IEEE Trans. \nPattern Anal. Mach. Intell. 44(1), 550‚Äì557 (2021)\n 35. Huang, S., Lu, Z., Cheng, R., He, C.: Fapn: Feature-aligned pyra-\nmid network for dense image prediction. In: Proceedings of the \nIEEE/CVF International Conference on Computer Vision, pp. \n864‚Äì873 (2021)\n 36. Wu, J., Pan, Z., Lei, B., Hu, Y.: Fsanet: Feature-and-spatial-\naligned network for tiny object detection in remote sensing \nimages. IEEE Trans. Geosci. Remote Sens. 60, 1‚Äì17 (2022)\n 37. Hu, H., Chen, Y., Xu, J., Borse, S., Cai, H., Porikli, F., Wang, X.: \nLearning implicit feature alignment function for semantic segmen-\ntation. In: European Conference on Computer Vision, pp. 487‚Äì505 \n(2022)\n 38. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: \nProceedings of the IEEE Conference on Computer Vision and \nPattern Recognition, pp. 7132‚Äì7141 (2018)\n 39. Zhang, R., Li, G., Li, Z., Cui, S., Qian, D., Yu, Y.: Adaptive con-\ntext selection for polyp segmentation. In: International Conference \non Medical Image Computing and Computer-Assisted Interven-\ntion, pp. 253‚Äì262 (2020). https:// doi. org/ 10. 1007/ 978-3- 030- \n59725-2_ 25\n 40. Tomar, N.K., Jha, D., Riegler, M.A., Johansen, H.D., Johansen, \nD., Rittscher, J., Halvorsen, P., Ali, S.: Fanet: A feedback atten-\ntion network for improved biomedical image segmentation. IEEE \nTrans. Neural Netw. Learn, Syst (2022)\n 41. Shen, Y., Jia, X., Meng, M.Q.-H.: Hrenet: A hard region enhance-\nment network for polyp segmentation. In: International Confer -\nence on Medical Image Computing and Computer-Assisted Inter-\nvention, pp. 559‚Äì568 (2021)\n 42. Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H.: Dual \nattention network for scene segmentation. In: Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 3146‚Äì3154 (2019)\n 43. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., \nYuille, A.L., Zhou, Y.: Transunet: Transformers make strong \nencoders for medical image segmentation. arXiv preprint arXiv: \n2102. 04306 (2021)\n 44. Zhang, Y., Liu, H., Hu, Q.: Transfuse: Fusing transformers \nand cnns for medical image segmentation. In: International \nConference on Medical Image Computing and Computer-Assisted \nIntervention, pp. 14‚Äì24 (2021)\n 45. He, X., Tan, E.-L., Bi, H., Zhang, X., Zhao, S., Lei, B.: Fully \ntransformer network for skin lesion analysis. Med. Image Anal. \n77, 102357 (2022)\n 46. Yuan, F., Zhang, Z., Fang, Z.: An effective cnn and transformer \ncomplementary network for medical image segmentation. Pattern \nRecogn 136, 109228 (2023)\n 47. Heidari, M., Kazerouni, A., Soltany, M., Azad, R., Aghdam, E.K., \nCohen-Adad, J., Merhof, D.: Hiformer: Hierarchical multi-scale \nrepresentations using transformers for medical image segmen-\ntation. In: Proceedings of the IEEE/CVF Winter Conference on \nApplications of Computer Vision, pp. 6202‚Äì6212 (2023)\n 48. Wu, H., Chen, S., Chen, G., Wang, W., Lei, B., Wen, Z.: Fat-net: \nFeature adaptive transformers for automated skin lesion segmenta-\ntion. Med. Image Anal. 76, 102327 (2022)\n 49. Jaderberg, M., Simonyan, K., Zisserman, A., et¬†al.: Spatial trans-\nformer networks. Adv. Neural Info. Processing Syst 28, (2015)\n 50. Song, J., Chen, X., Zhu, Q., Shi, F., Xiang, D., Chen, Z., Fan, \nY., Pan, L., Zhu, W.: Global and local feature reconstruction for \nmedical image segmentation. IEEE Trans. Med. Imaging (2022)\n 51. Zhang, Q.-L., Yang, Y.-B.: Sa-net: Shuffle attention for deep con-\nvolutional neural networks. In: ICASSP 2021-2021 IEEE Inter -\nnational Conference on Acoustics, Speech and Signal Processing \n(ICASSP), pp. 2235‚Äì2239 (2021)\n 52. Wu, Y., He, K.: Group normalization. In: Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), pp. 3‚Äì19 (2018)\n 53. Ma, N., Zhang, X., Zheng, H.-T., Sun, J.: Shufflenet v2: Practical \nguidelines for efficient cnn architecture design. In: Proceedings \nof the European Conference on Computer Vision (ECCV) (2018)\n 54. Zhao, S., Wang, Y., Yang, Z., Cai, D.: Region mutual information \nloss for semantic segmentation. Adv. Neural Info. Processing Syst. \n32, (2019)\n 55. Alemi, A.A., Fischer, I., Dillon, J.V., Murphy, K.: Deep vari -\national information bottleneck. arXiv preprint arXiv: 1612. 00410 \n(2016)\n 56. Tian, X., Zhang, Z., Lin, S., Qu, Y., Xie, Y., Ma, L.: Farewell to \nmutual information: Variational distillation for cross-modal per -\nson re-identification. In: Proceedings of the IEEE/CVF Confer -\nence on Computer Vision and Pattern Recognition, pp. 1522‚Äì1531 \n(2021)\n 57. Wei, J., Wang, S., Huang, Q.: F 3net: fusion, feedback and focus for \nsalient object detection. In: Proceedings of the AAAI Conference \non Artificial Intelligence, vol. 34, pp. 12321‚Äì12328 (2020)\n 58. Yin, Z., Liang, K., Ma, Z., Guo, J.: Duplex contextual relation \nnetwork for polyp segmentation. In: 2022 IEEE 19th Interna-\ntional Symposium on Biomedical Imaging (ISBI), pp. 1‚Äì5 (2022). \nhttps:// doi. org/ 10. 1109/ ISBI5 2829. 2022. 97614 02\n 59. Fan, D.-P., Ji, G.-P., Zhou, T., Chen, G., Fu, H., Shen, J., Shao, L.: \nPranet: Parallel reverse attention network for polyp segmentation. \nIn: International Conference on Medical Image Computing and \nComputer-assisted Intervention, pp. 263‚Äì273 (2020). https:// doi. \norg/ 10. 1007/ 978-3- 030- 59725-2_ 26\n 60. Patel, K., Bur, A.M., Wang, G.: Enhanced u-net: A feature \nenhancement network for polyp segmentation. In: 2021 18th Con-\nference on Robots and Vision (CRV), pp. 181‚Äì188 (2021). https:// \ndoi. org/ 10. 1109/ CRV52 889. 2021. 00032\n 61. Ta, N., Chen, H., Lyu, Y., Wu, T.: Ble-net: boundary learning and \nenhancement network for polyp segmentation. Multimed. Syst. \n1‚Äì14 (2022)\n 62. Bernal, J., S√°nchez, F.J., Fern√°ndez-Esparrach, G., Gil, D., \nRodr√≠guez, C., Vilari√±o, F.: Wm-dova maps for accurate polyp \n3861LET-Net: locally enhanced transformer network for¬†medical image segmentation  \n1 3\nhighlighting in colonoscopy: Validation vs. saliency maps from \nphysicians. Comput. Med. Imaging Graph. 43, 99‚Äì111 (2015). \nhttps:// doi. org/ 10. 1016/j. compm edimag. 2015. 02. 007\n 63. Jha, D., Smedsrud, P.H., Riegler, M.A., Halvorsen, P., Lange, T.d., \nJohansen, D., Johansen, H.D.: Kvasir-seg: A segmented polyp \ndataset. In: International Conference on Multimedia Modeling, \npp. 451‚Äì462 (2020)\n 64. Tajbakhsh, N., Gurudu, S.R., Liang, J.: Automated polyp detection \nin colonoscopy videos using shape and context information. IEEE \nTrans. Med. Imaging 35(2), 630‚Äì644 (2016). https:// doi. org/ 10. \n1109/ TMI. 2015. 24879 97\n 65. Silva, J., Histace, A., Romain, O., Dray, X., Granado, B.: Toward \nembedded detection of polyps in WCE images for early diagnosis \nof colorectal cancer. Int. J. Comput. Assist. Radiol. Surg. 9 (2), \n283‚Äì293 (2014). https:// doi. org/ 10. 1007/ s11548- 013- 0926-3\n 66. V√°zquez, D., Bernal, J., S√°nchez, F.J., Fern√°ndez-Esparrach, \nG., L√≥pez, A.M., Romero, A., Drozdzal, M., Courville, A.C.: A \nbenchmark for endoluminal scene segmentation of colonoscopy \nimages. J. Healthc. Eng. (2017)\n 67. Al-Dhabyani, W., Gomaa, M., Khaled, H., Fahmy, A.: Dataset of \nbreast ultrasound images. Data in brief 28, 104863 (2020)\n 68. Abraham, N., Khan, N.M.: A novel focal tversky loss function \nwith improved attention u-net for lesion segmentation. In: 2019 \nIEEE 16th International Symposium on Biomedical Imaging \n(ISBI 2019), pp. 683‚Äì687 (2019)\n 69. Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., \nHan, X., Chen, Y.-W., Wu, J.: Unet 3+: A full-scale connected \nunet for medical image segmentation. In: ICASSP 2020-2020 \nIEEE International Conference on Acoustics, Speech and Signal \nProcessing (ICASSP), pp. 1055‚Äì1059 (2020)\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7998319268226624
    },
    {
      "name": "Segmentation",
      "score": 0.6789138913154602
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6635143756866455
    },
    {
      "name": "Encoder",
      "score": 0.5772005319595337
    },
    {
      "name": "Upsampling",
      "score": 0.57672518491745
    },
    {
      "name": "Feature learning",
      "score": 0.5547512769699097
    },
    {
      "name": "Discriminative model",
      "score": 0.5183880925178528
    },
    {
      "name": "Transformer",
      "score": 0.4845268130302429
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4592130184173584
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.4535771310329437
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4356079697608948
    },
    {
      "name": "Computer vision",
      "score": 0.4352788031101227
    },
    {
      "name": "Image segmentation",
      "score": 0.41519659757614136
    },
    {
      "name": "Image (mathematics)",
      "score": 0.14323276281356812
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210134929",
      "name": "Jilin Province Science and Technology Department",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I194450716",
      "name": "Jilin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I106645853",
      "name": "Changchun University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I43439940",
      "name": "University of Southampton",
      "country": "GB"
    }
  ]
}