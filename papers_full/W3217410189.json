{
  "title": "Grounded Situation Recognition with Transformers",
  "url": "https://openalex.org/W3217410189",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5013096795",
      "name": "Junhyeong Cho",
      "affiliations": [
        "Pohang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5026261889",
      "name": "Youngseok Yoon",
      "affiliations": [
        "Pohang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5103014894",
      "name": "Hyunjun Lee",
      "affiliations": [
        "Pohang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5060343759",
      "name": "Suha Kwak",
      "affiliations": [
        "Pohang University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2560747010",
    "https://openalex.org/W2922476711",
    "https://openalex.org/W3003365594",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W1524680991",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3096682293",
    "https://openalex.org/W2963346996",
    "https://openalex.org/W3162565403",
    "https://openalex.org/W3173181410",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2911810010",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3035399403",
    "https://openalex.org/W2604673901",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2423576022",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W2583194072",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2886970679",
    "https://openalex.org/W3180562345",
    "https://openalex.org/W2950898568",
    "https://openalex.org/W2579549467",
    "https://openalex.org/W2134670479",
    "https://openalex.org/W1895577753"
  ],
  "abstract": "Grounded Situation Recognition (GSR) is the task that not only classifies a salient action (verb), but also predicts entities (nouns) associated with semantic roles and their locations in the given image. Inspired by the remarkable success of Transformers in vision tasks, we propose a GSR model based on a Transformer encoder-decoder architecture. The attention mechanism of our model enables accurate verb classification by capturing high-level semantic feature of an image effectively, and allows the model to flexibly deal with the complicated and image-dependent relations between entities for improved noun classification and localization. Our model is the first Transformer architecture for GSR, and achieves the state of the art in every evaluation metric on the SWiG benchmark. Our code is available at https://github.com/jhcho99/gsrtr .",
  "full_text": "CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 1\nGrounded Situation Recognition\nwith Transformers\nJunhyeong Cho*1\njunhyeong99@postech.ac.kr\nY oungseok Y oon*1\nyys8646@postech.ac.kr\nHyeonjun Lee*2\nhyeonjun1882@postech.ac.kr\nSuha Kwak1,2\nsuha.kwak@postech.ac.kr\n1 Department of CSE\nPOSTECH\nPohang, Republic of Korea\n2 Graduate School of AI\nPOSTECH\nPohang, Republic of Korea\nAbstract\nGrounded Situation Recognition (GSR) is the task that not only classiﬁes a salient\naction (verb), but also predicts entities ( nouns) associated with semantic roles and their\nlocations in the given image. Inspired by the remarkable success of Transformers in\nvision tasks, we propose a GSR model based on a Transformer encoder-decoder archi-\ntecture. The attention mechanism of our model enables accurate verb classiﬁcation by\ncapturing high-level semantic feature of an image effectively, and allows the model to\nﬂexibly deal with the complicated and image-dependent relations between entities for\nimproved noun classiﬁcation and localization. Our model is the ﬁrst Transformer archi-\ntecture for GSR, and achieves the state of the art in every evaluation metric on the SWiG\nbenchmark. Our code is available at https://github.com/jhcho99/gsrtr.\nFigure 1: Predictions of our model on the SWiG dataset.\n1 Introduction\nDeep learning models have achieved or even surpassed human-level performance on basic\nvision tasks such as classiﬁcation of objects [7, 16], actions [19, 32], and places [6, 13, 33].\n© 2021. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms. * Equal contribution.\narXiv:2111.10135v1  [cs.CV]  19 Nov 2021\n2 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\nFigure 2: The overall architecture of our model (GSRTR). It mainly consists of two com-\nponents: Transformer Encoder for verb prediction, and Transformer Decoder for grounded\nnoun prediction. Diagram is best viewed in colored version.\nHowever, it still remains challenging and less explored to expand such models for detailed\nand comprehensive understanding of natural scenes,e.g., recognizing what happens and who\nare involved with which roles. Image captioning [8, 23, 30] and scene graph generation [12,\n26, 27] have been studied in this context. These tasks aim at reasoning about image contents\nin detail and describing them through natural language captions or relation graphs of objects.\nHowever, quality evaluation of natural language captions is not straightforward, and scene\ngraphs are limited in terms of expressive power as they represent an action only by a triplet\nof subject, predicate, and object.\nGrounded Situation Recognition (GSR) [17] is a comprehensive scene understanding\ntask that resolves the above limitations. It originates from Situation Recognition (SR) [28],\nthe task of predicting a salient action, entities taking part of the action, and their roles alto-\ngether given an image. In SR, an action and entities are called verb and nouns, respectively,\nand the set of semantic roles of the entities in an action is termed frame; a frame is deﬁned\nfor each verb as prior knowledge by FrameNet [4], a lexical database of English. Then SR\nis typically done by predicting a verb then assigning a noun to each role given by the frame\nof the verb. GSR has been introduced to further address localization ( i.e., bounding box es-\ntimation) of the nouns in the image, which is missing in SR. It is thus more challenging yet\nenables more detailed scene understanding in comparison with SR.\nThe major challenge in GSR is two-fold. The ﬁrst is the difﬁculty of verb prediction.\nThis is caused by the fact that a verb is a high-level concept embodied by multiple entities;\nas illustrated in Fig. 1, images of the same verb often vary signiﬁcantly due to different\nentities interacting in different ways. The second is the difﬁculty of modeling complicated\nrelations between entities. Since an action ( i.e., verb) is performed by multiple entities (i.e.,\nnouns) related to each other, individual noun recognition per role is deﬁnitely suboptimal;\nrelations between nouns have to be considered for improved noun prediction and localiza-\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 3\ntion. However, modeling such relations is challenging since they are latent and depending\non an input image.\nInspired by the recent success of Transformers [1, 3, 22], we present in this paper a new\nmodel, dubbed GSRTR, that addresses the aforementioned challenges through the attention\nmechanism. As illustrated in Fig. 2, it has an encoder-decoder architecture based on Trans-\nformer. The encoder takes as input a verb token and image features from a CNN backbone.\nThe token then goes through self-attention blocks in the encoder and is ﬁnally processed by\na verb classiﬁer on top. Thanks to the self-attention with the image features, the encoder can\ncapture rich and high-level semantic information for accurate verb prediction. Meanwhile,\nthe decoder predicts a grounded noun per role, where target roles are determined by the frame\nof the target verb. It thus takes as inputsemantic role queries of target roles as well as image\nfeatures given by the encoder; a semantic role query is obtained by a concatenation of two\nembedding vectors, one for its role and the other for a verb, which are learnable parameters\ndedicated to the role and verb, respectively. Each semantic role query is converted to a fea-\nture vector through attention blocks, then used to predict a noun class, a box coordinate and\na box existence probability of its role. The attention blocks in our decoder allow to capture\ncomplicated and image-dependent relations among roles effectively and ﬂexibly.\nContributions: Our GSRTR is the ﬁrst Transformer architecture dedicated to GSR. Further-\nmore, its encoder-decoder architecture is carefully designed to address major challenges of\nthe task. The efﬁcacy of GSRTR is validated on the SWiG dataset [17], the standard bench-\nmark for GSR, where it clearly outperforms existing models [17] in every evaluation metric.\nWe also provide in-depth analysis on behaviors of GSRTR, which demonstrates that it has\nthe capability of drawing attentions on local areas relevant to verb and grounded nouns.\n2 Related Work\nSituation Recognition: Situation Recognition (SR) is the task of predicting a salient ac-\ntion ( verb) and entities ( nouns) taking part of the action. Yatskar et al . [28] present the\nimSitu dataset as benchmark of Situation Recognition and propose Conditional Random\nField (CRF) model. Their following work [29] ﬁgures out that sparsity of training exam-\nples compared to large output space could be problematic, and alleviates it through tensor-\ncomposition function. Since then, there have been attempts to model the relations among\nsemantic roles. Inspired by image captioning task, Mallya and Lazebnik [15] adopt a Re-\ncurrent Neural Network (RNN) architecture to model the relations in the predeﬁned order.\nLi et al. [9] use a Gated Graph Neural Network (GGNN) [10] to capture relations among\nroles, and Suhail and Sigal [20] propose a modiﬁed GGNN to learn context-aware relations\namong roles depending on the content of the image. Cooray et al. [2] formulate the relation\nmodeling as an interdependent query based visual reasoning problem.\nGrounded Situation Recognition: Recently, Grounded Situation Recognition (GSR) has\nbeen introduced by Prattet al. [17] to further address localization of entities, which is missing\nin SR. They propose the Situation With Groundings (SWiG) dataset that provides bounding\nbox annotations in addition to theimSitu dataset. They also propose Joint Situation Localizer\n(JSL) model which consists of a verb classiﬁer and a RNN based object detector. The object\ndetector sequentially produces noun and its bounding box prediction via the predeﬁned role\norder. Compared with JSL, our GSRTR can ﬂexibly capture the relations among the semantic\nroles rather than the predeﬁned order. Furthermore, the verb prediction process in our model\ncan capture long-range interactions of semantic concepts via a Transformer encoder.\n4 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\nTransformer in Vision Tasks: Dosovitskiy et al. [3] propose a standard Transformer en-\ncoder architecture [22] for image classiﬁcation task. This model, called ViT, takes image\npatches ﬂattened, linearly transformed, and combined with positional encodings as input\nwith a classiﬁcation token. On the other hand, the encoder of GSRTR takes image features\nfrom a CNN backbone as input, and is combined with a decoder for grounded noun predic-\ntion. Carion et al. [1] view object detection as a direct set prediction and bipartite matching\nproblem, and propose a Transformer encoder-decoder architecture for object detection ac-\ncordingly. Their model, called DETR, introduces learnable embeddings calledobject queries\nas inputs of the decoder, each of which is in charge of a certain image region and a set of\nbounding box candidates. Instead of the object queries, GSRTR uses semantic role queries,\neach of which focuses on entities taking part of a speciﬁed action with a speciﬁc role.\nSimilar follow-ups to DETR: There have been attempts, including our GSRTR, to apply\nDETR to other domains such as video instance segmentation [24], video action recogni-\ntion [31] and human-object-interaction detection [34]. Their models use latent queries for\na Transformer decoder in the similar way, but GSRTR has notable differences. While their\nmodels employ a ﬁxed number of latent queries in the decoder, GSRTR constructs a variable\nnumber of queries depending on a given image. Also, to the best of our knowledge, GSRTR\nis the ﬁrst attempt to explicitly leverage the output of a Transformer encoder for building\nqueries used in a Transformer decoder; semantic role queries use the verb embedding corre-\nsponding to the predicted verb from the encoder output at inference time.\n3 Proposed Method\nInspired by ViT [3] and DETR [1], we propose a novel model called Grounded Situation\nRecognition TRansformer (GSRTR) to address the challenging GSR task; the architecture\nof GSRTR is illustrated in Fig. 2. This section ﬁrst provides a formal deﬁnition of GSR, then\ndescribes details of our model architecture, training and inference procedures.\n3.1 Task Deﬁnition\nLet V, R, and Ndenote the sets of verbs, roles, and nouns deﬁned in the task, respectively.\nFor each verb v ∈V, a set of semantic roles, denoted by Rv ⊂R, is predeﬁned as its frame\nby FrameNet [4]. For example, the frame of a verb Catching is a set of semantic roles\nRCatching = {Agent, Caught Item , Tool, Place} ⊂R. Also, a pair of a noun n ∈N and its\nbounding box b ∈R4 is called a grounded noun. The goal of GSR is to predict a verbv of an\ninput image and assign a grounded noun to each role in Rv. Formally speaking, a prediction\nof GSR is in the form of S = (v,Fv), where Fv = {(r,nr,br)|nr ∈N∪ {/ 0n}, br ∈R4 ∪\n{/ 0b}for r ∈Rv}; / 0n and / 0b mean unknown and not grounded, respectively. For example,\nthe prediction for the leftmost image in Fig. 1 is given byS =\n(\nCatching,\n{\n(Agent, Bear, □),\n(Caught Item , Fish, □), (Tool, Mouth, □), (Place, River, / 0b)\n})\n.\n3.2 Encoder for Verb Prediction\nA CNN backbone ﬁrst processes an input image to extract its feature map Ximg ∈Rc×h×w,\nwhere c is the number of channels and h ×w is the resolution of Ximg. Then Ximg is fed to a\n1 ×1 convolution layer for reducing the channel size to d, and ﬂattened, leading to ﬂattened\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 5\nimages features Fimg ∈Rd×hw. Like the classiﬁcation token used in ViT [3], we append a\nlearnable verb embedding fv ∈Rd to Fimg, forming an input of the encoder F ∈Rd×(1+hw).\nThe encoder is a stack of six layers, each of which consists of a Multi-Head Self-\nAttention (MHSA) block and a Feed Forward Network (FFN) block. Also, we apply Pre-\nLayer Normalization (Pre-LN) [25] before the MHSA and FFN blocks. Positional encodings\nare added to the input of each encoder layer. Please refer to the supplementary material for\nmore details of the encoder.\nThe output of the encoder, denoted by E ∈Rd×(1+hw), is split into a verb feature ev ∈Rd\nand hw image features Eimg ∈Rd×hw. The former is fed to the verb classiﬁer, which in turn\nproduces a logit vector zv ∈R|V| as a result of verb classiﬁcation. On the other hand, the\nlatter will be used as observations for the decoder. Note that by exploiting the attention\nmechanism through the encoder layers, the verb token can effectively aggregate relevant\nsemantic features of an image for accurate verb classiﬁcation.\n3.3 Decoder for Grounded Noun Prediction\nIn addition to the image features Eimg given by the encoder, the decoder takes as input se-\nmantic role queries to predict corresponding nouns and their bounding boxes, inspired by the\nobject queries in DETR [1]. To be speciﬁc, a semantic role query w(v,r) ∈Rd is obtained by\na concatenation of a verb embedding vector wv ∈Rdv and a role embedding vector wr ∈Rdr\n(d = dv + dr), both of which are learnable parameters; v is the ground-truth verb at training\ntime and the predicted verb at inference time, while r ∈Rv. The number of semantic role\nqueries fed to the decoder is thus |Rv|.\nThe decoder is a stack of six layers, each of which consists of a MHSA block, a Multi-\nHead Attention (MHA) block, and a FFN block; Pre-LN is applied before each of the blocks.\nThe ﬁrst decoder layer input is set to zero. In each decoder layer, each semantic role query\nw(v,r) is added to each key and query of the MHSA block and added to each query of the\nMHA block. The image features Eimg serve as keys and values in the MHA block of each de-\ncoder layer. Through the MHSA block in each decoder layer, semantic role queries ﬂexibly\ncapture the role relations (Fig. 4). From the MHA block in each decoder layer, each semantic\nrole query attends to image features considering image-dependent relations (Fig. 3).\nThrough the decoder, each semantic role query w(v,r) is converted to an output feature.\nThe output feature of each role r ∈Rv is in turn fed to three branches: One for noun clas-\nsiﬁcation, another for bounding box regression, and the other for predicting existence of its\nbounding box. The noun classiﬁer produces a noun logit vector znr ∈R|N∪{/ 0n}|. The bound-\ning box regressor predicts ˆb′\nr = (ˆcx, ˆcy, ˆw, ˆh) ∈[0,1]4, indicating the normalized center coor-\ndinate, height, and width of a box relative to the image size. This predicted box coordinate\nis transformed into top-left and bottom-right coordinate representation ˆbr = (ˆx1, ˆy1, ˆx2, ˆy2) ∈\nR4. Finally, the box existence predictor produces a box existence probability pbr ∈[0,1].\nPlease refer to the supplementary material for more details of the decoder.\n3.4 Training and Inference\nThe total loss for training GSRTR is a linear combination of ﬁve losses: A verb classiﬁcation\nloss, a noun classiﬁcation loss, a bounding box existence loss, aL1 box regression loss, and a\nGeneralized IoU (GIoU) [18] box regression loss. The verb classiﬁcation lossLv is the cross\nentropy between the verb prediction probabilitypv = Softmax(zv) and the ground-truth verb\n6 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\ndistribution. The noun classiﬁcation loss Ln is formulated as the average of individual noun\nclassiﬁcation losses over the semantic roles, and is given by\nLn = 1\n|Rv| ∑\nr∈Rv\nCrossEntropy(pnr ,tnr ), (1)\nwhere pnr denotes the noun prediction probability for each roler and tnr indicates the ground-\ntruth noun distribution for each role r. The bounding box existence loss Lexist is the average\nof individual bounding box existence loss over the semantic roles, and is given by\nLexist = 1\n|Rv| ∑\nr∈Rv\nCrossEntropy(pbr ,tbr ), (2)\nwhere pbr denotes the bounding box existence probability for each role r and tbr ∈{0,1}\nspeciﬁes the existence of the ground-truth bounding box for each role r (i.e., tbr = 1 when\nbr ̸= / 0b). The L1 box regression loss LL1 is deﬁned as the average of individual L1 distances\nbetween predicted and ground-truth bounding boxes over semantic roles for which ground-\ntruth bounding boxes exist, and are given by\nLL1 = 1\n|˜Rv|∑\nr∈˜Rv\n∥ˆb′\nr −b′\nr∥1, (3)\nwhere ˜Rv = {r |r ∈Rv and br ̸= / 0b}is the set of roles associated with bounding boxes.\nFinally, the GIoU box regression loss LGIoU [18] is formulated as the average of individual\nGIoU losses over roles for which ground-truth bounding boxes exist, and are given by\nLGIoU = 1\n|˜Rv|∑\nr∈˜Rv\n(\n1 −\n(\n|br ∩ˆbr|\n|br ∪ˆbr|\n−|C(br, ˆbr)\\br ∪ˆbr|\n|C(br, ˆbr)|\n))\n, (4)\nwhere C(ˆbr,br) denotes the smallest box enclosing predicted box ˆbr and ground-truth box\nbr for each role r. GIoU loss is a scale-invariant loss and it compensates for scale-variant\nL1 loss. The total loss Ltotal is formulated as Ltotal = λvLv +λnLn +λexistLexist +λL1 LL1 +\nλGIoU LGIoU , where λv,λn,λexist,λL1 ,λGIoU > 0 are hyperparameters.\nAt inference time, our method predicts a verb ˆ v = argmaxv pv then constructs corre-\nsponding semantic role queries w(ˆv,r) for all r ∈Rˆv. Each w(ˆv,r) is used by the decoder to\nproduce corresponding output noun logit znr , bounding box ˆb′\nr and bounding box existence\nprobability pbr . Note that if pbr < 0.5, the predicted bounding box ˆb′\nr is ignored.\n4 Experiments\n4.1 Dataset and Metrics\nSWiG [17] dataset is composed of 75k, 25k and 25k images for the train, development and\ntest set respectively. There are |V|= 504 verbs, |R|= 190 roles, and 1 ≤|Rv|≤6 semantic\nroles per verb. We use about 10 k nouns, the number of noun classes in the train set. The\nannotation for each image consists of a verb, a bounding box for each semantic role, and\nthree nouns (from three annotators) for each semantic role.\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 7\nTable 1: Requirements for each metric.\nrequirement\ncorrect verb correct noun correct nouns correct bounding boxcorrect bounding boxes\nmetric for a semantic rolefor all semantic rolesfor a semantic rolefor all semantic roles\nverb !\nvalue ! !\nvalue-all ! ! !\ngrounded-value ! ! !\ngrounded-value-all! ! ! ! !\nThe predicted verb and grounded nouns are measured by ﬁve metrics:verb, value, value-\nall, grounded-value, and grounded-value-all. The verb metric denotes a verb prediction ac-\ncuracy. The value metric denotes a noun prediction accuracy from its semantic role. The\nvalue-all metric denotes that all nouns corresponding to semantic roles are correctly pre-\ndicted. The grounded-value metric denotes a grounded noun prediction accuracy for its\nsemantic role. Note that the grounded noun prediction is considered correct if it correctly\npredicts noun and bounding box. The bounding box prediction is considered correct if it\ncorrectly predicts bounding box existence and the predicted box has an Intersection-over-\nUnion (IoU) value of at least 0.5 with the ground-truth box. The grounded-value-all metric\ndenotes that all grounded nouns corresponding to semantic roles are correctly predicted. The\nrequirements for each metric are summarized in Table 1. Because the number of roles per\nverb is different and the number of images per verb could be different, all above metrics are\ncalculated for each verb and then averaged over them.\nSince these metrics depend heavily on the verb accuracy, the metrics are reported in 3\nsettings: top-1 predicted verb , top-5 predicted verbs and ground-truth verb. In top-1\npredicted verb setting, ﬁve metrics are reported: a top-1 predicted verb accuracy, two noun\nmetrics and two grounded noun metrics. If the top-1 predicted verb is incorrect, the noun\nand grounded noun metrics are considered incorrect. In top-5 predicted verbs setting, ﬁve\nmetrics are reported: a top-5 predicted verbs accuracy, two noun metrics and two grounded\nnoun metrics. If the ground-truth verb is not included in the top-5 predicted verbs, the noun\nand grounded noun metrics are considered incorrect, too. Inground-truth verb setting, four\nmetrics are reported: two noun metrics and two grounded noun metrics. From the ground-\ntruth verb assumed to be known, noun and grounded noun predictions are taken from the\nmodel by conditioning on the ground-truth verb.\n4.2 Implementation Details\nFollowing previous work [17], we use ImageNet-pretrained ResNet-50 backbone [7] except\nFeature Pyramid Network (FPN) [11]. The ResNet-50 backbone produces the image fea-\ntures Ximg ∈Rc×h×w from the input image where c = 2048. The hidden dimensions of each\nsemantic role query, verb token and image feature are 512 ( d = 512). The verb embedding\ndimension and role embedding dimension are 256 ( dv = dr = 256). We use learnable 2D\nembeddings for the positional encodings. The number of heads for all MHSA and MHA\nblocks is 8. We use 2 fully connected layers with ReLU activation function for the four\nfollowings: the FFN blocks in the encoder and decoder, the verb classiﬁer, the noun clas-\nsiﬁer, and the bounding box existence predictor. The size of hidden dimensions are 2048,\n2d, 2d, and 2d, respectively. The dropout rates are 0.15, 0.3, 0.3, and 0.2, respectively. The\nbounding box regressor is 3 fully connected layers with ReLU activation function and 2 d\nhidden dimensions, using 0.2 dropout rate. The label smoothing regularization [21] is used\n8 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\nfor the target verb and noun labels with label smoothing factor 0 .3 and 0 .2, respectively.\nWe use AdamW [14] optimizer with the learning rate 10−4 (10−5 for the backbone), weight\ndecay 10−4, β1 = 0.9 and β2 = 0.999. We set the max gradient clipping value to 0 .1 and\ntrain the BatchNorm layers in the backbone. The training epoch is 40 with batch size 16 per\nGPU on four 12GB TITAN Xp GPUs, which takes about 20 hours. The loss coefﬁcients are\nλv = λn = 1 and λexist = λL1 = λGIoU = 5.\nData Augmentation: Random Color Jittering, Random Gray Scaling, Random Scaling and\nRandom Horizontal Flipping are used. The hue, saturate and brightness scale in random\ncolor jittering set to 0.1. The scale of random gray scaling sets to 0.3. The scales of random\nscaling set to 0.5, 0.75 and 1.0. The probability of random horizontal ﬂipping sets to 0.5.\nFinal Noun Loss: In SWiG, three noun annotations exist per role. For each noun annotation,\nwe calculate the loss (Eq. 1). The ﬁnal noun loss is the summation of the three noun losses.\nBatch Training: The number of semantic roles ranges from 1 to 6 depending on the frame\nof a verb. In GSRTR, the semantic role queries are constructed as much as the number of\nsemantic roles. To ensure batch training, zero padding is used for each output of grounded\nnoun prediction branches. We ignore the padded outputs in the loss computation.\n4.3 Experiment Results\nQuantitative Comparison with Previous Work:Table 2 quantitatively compares our model\nwith previous work on the dev and test splits of SWiG dataset. In all evaluation metrics,\nGSRTR achieves the state-of-the-art accuracy. In the dev set, compared with JSL, GSRTR\nachieves the top-1 predicted verb and top-5 predicted verbs accuracies of 41.06% (+1.46%p)\nand 69.46% (+1.75%p), respectively. In ground-truth verb setting, GSRTR achieves the\nvalue and grounded-value accuracies 74.27% (+0.74%p) and 58.33% (+0.83%p), respec-\ntively. Note that previous work uses two ResNet-50 backbones and FPN, while our GSRTR\nonly uses a single ResNet-50 backbone without FPN. Existing models in [17] have about 108\nmillion parameters, but our GSRTR only has about 83 million parameters. Although GSRTR\nhas less backbone capacity and less parameters, it achieves the state-of-the-art accuracy in\nevery evaluation metric. In addition, the reason for the small improvement by GSRTR in\nterms of grounded-value metrics is that these metrics require correct predictions of verb,\nnoun and bounding box as shown in Table 1.\nExisting models in [17] are trained separately in terms of verb prediction part and grounded\nnoun prediction part, while our GSRTR is trained in an end-to-end manner. For this reason,\nit is difﬁcult to fairly compare the training time of ours with existing models. However, we\ncan reasonably guess that GSRTR takes less training time than others. GSRTR takes about\n20 hours with four 12GB TITAN Xp GPUs for whole training, but other models take about\n20 hours with four 24GB TITAN RTX GPUs only for training of grounded noun prediction\npart. For the comparison of inference time, we compare GSRTR with JSL which was the\nprevious state-of-the-art. We evaluate the models on the test set in the same environment\nwith one 2080Ti GPU. GSRTR takes 21.69 ms (46.10 FPS) and JSL takes 80.00 ms (12.50\nFPS) on the average of 10 trials.\nEffect of Verb Embedding Concatenation: We also quantitatively show the effect of verb\nembedding concatenation in the semantic role query. If we do not concatenate the verb em-\nbedding (i.e., dv = 0 and dr = d), the accuracies in the ground-truth verb setting decrease by\naround 1.3 ∼2.3%p (GSRTR w/o VE in Table 2). It demonstrates that the verb embedding\nconcatenation is helpful for grounded noun prediction.\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 9\nTable 2: Quantitative evaluation on the SWiG dataset.top-1 predicted verb top-5 predicted verbs ground-truth verbgrnd grnd grnd grnd grnd grndset model verb value value-all value value-allverb value value-all value value-allvalue value-all value value-all\ndev\nISL [17] 38.83 30.47 18.23 22.47 7.6465.74 50.29 28.59 36.90 11.6672.77 37.49 52.92 15.00JSL [17] 39.60 31.18 18.85 25.03 10.1667.71 52.06 29.73 41.25 15.0773.53 38.32 57.50 19.29GSRTR w/o VE (Ours)40.81 32.05 19.31 25.64 10.3169.33 53.09 29.78 42.01 15.3672.55 37.07 57.00 18.93GSRTR (Ours)41.06 32.52 19.63 26.04 10.4469.46 53.69 30.66 42.61 15.9874.27 39.24 58.33 20.19\ntest\nISL [17] 39.36 30.09 18.62 22.73 7.7265.51 50.16 28.47 36.60 11.5672.42 37.10 52.19 14.58JSL [17] 39.94 31.44 18.87 24.86 9.6667.60 51.88 29.39 40.60 14.7273.21 37.82 56.57 18.45GSRTR w/o VE (Ours)40.61 31.87 19.01 25.21 9.6969.75 53.25 29.67 41.65 14.9372.32 36.75 56.03 18.02GSRTR (Ours)40.63 32.15 19.28 25.49 10.1069.81 54.13 31.01 42.50 15.8874.11 39.00 57.45 19.67\nFigure 3: Role Attention Map on Image Features for a Sketching image from the MHA\nblock in each decoder layer. The left labels are the semantic roles of the verbSketching. The\nrightmost column images and labels are predicted bounding boxes and nouns of our model.\nRole Attention Map on Image Features: In Figure 3, each column shows the difference of\nattention maps among semantic roles. For example, at Layer 6, the roleAgent focuses on the\nwoman, and the role Place focuses on the road and yard. Each row shows the transition of\nattention maps through the decoder layers. For example, in the role Material , the attention\nmap gradually focuses on the paper in the image through the decoder layers. It shows that\nthe semantic role queries can focus on the region related to them.\nVisualization on Role Relations: In Figure 4, two images show different context for a\nverb Swinging. The role Agent and Carrier in Fig. 4(a) focus on the role Place, i.e., the\nforest (Place) is highly related to the monkey (Agent) and the vine (Carrier) given the verb\nSwinging. Meanwhile, the role Place in Fig. 4(b) focuses on the role Carrier, i.e., the golf\nclub (Carrier) is highly related to the golf course (Place) given the verb Swinging. It shows\nthat the relations among roles can be adaptively captured depending on the context of a given\nimage.\n10 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\n(a) (b)\nFigure 4: Visualization on Role Relations for two Swinging images. We visualize the\nattention scores between semantic role pairs computed in the MHSA block of the last decoder\nlayer. Attention scores are represented as column-wise sum to 1.\nFigure 5: Verb Token Attention Map on Image Features for three Tugging images. Each\nrow consists of an image and attention maps from the MHSA block in each encoder layer.\nVerb Token Attention Map on Image Features: In Figure 5, the rightmost column shows\nthe semantic regions where the verb token focuses on are similar. The verb token can capture\nthe key feature (e.g., tugged item) to infer the salient action. Each row shows the transition\nof attention maps through the encoder layers, e.g., focusing on the tugged item gradually.\n5 Discussion\nThere have been many studies in image retrieval by computing the similarities between the\nvisual representations of images. But, they do not work well for getting the retrieval results\nwhich have similar situations with respect to semantics or object arrangements. Grounded-\nSemantic-Aware Image Retrieval enables image retrieval in the aspects of main activity and\nkey objects with their arrangements, as shown in Figure 6. This retrieval uses the results of\nverb prediction and grounded noun prediction instead of visual representations. The predic-\ntions of main activity (verb) and entities (nouns) enable image retrieval for similar semantics,\nand the predictions of entity locations enable image retrieval for similar object arrangements.\nIn this retrieval, we compute the GrSitSim(I,J) [17] as similarity score function between im-\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 11\nFigure 6: Grounded-Semantic-Aware Image Retrieval results on the dev set. The retrieval\nresults have similar semantics and object arrangements with the query image. In this re-\ntrieval, the similarity between two images is computed by the results of verb prediction and\ngrounded noun prediction as in [17].\nage I and J. For an image I, we compute the top-5 verb predictions ˆvI\n1, ..., ˆvI\n5. For each verb\nprediction ˆvI\ni , we predict nouns ˆnI\ni,1, ..., ˆnI\ni,|RˆvI\ni\n| and bounding boxes ˆbI\ni,1, ..., ˆbI\ni,|RˆvI\ni\n|. Note\nthat we ignore the predicted bounding box if its existence probability is less than 0.5. We\ncalculate the similarity between two images I and J as follows:\nGrSitSim(I,J)\n= max\n\n\n\n1 [ˆvI\ni =ˆvJ\nj ]\n2 ·i ·j ·|RˆvI\ni\n|\n|RˆvI\ni\n|\n∑\nk=1\n1 [ˆnI\ni,k=ˆnJ\nj,k] ·\n(\n1 +IoU(ˆbI\ni,k, ˆbJ\nj,k)\n)⏐⏐⏐⏐⏐1 ≤i, j ≤5\n\n\n\n. (5)\nGrSitSim(I,J) is computed by the results of verb prediction and grounded noun prediction\nfor image I and J. The similarity is not zero when at least one verb is shared in the top-5\nverb predictions for image I and J. The similarity is maximized when the top-1 verb predic-\ntions and noun predictions of two images are same, and the sizes and locations of predicted\nbounding boxes are same. For this reason, we can get the retrieval result which has similar\nsemantics and object arrangements in Grounded-Semantic-Aware Image Retrieval. Thus, we\ncan apply this image retrieval to the applications where semantics and object arrangements\nare important, e.g., search engine using semantics and object arrangements of images.\nGrounded Situation Recognition models produce complete predictions with respect to\nthe semantic roles corresponding to a verb. Thus, the models can answer the following\nquestions more strictly, “What is the main activity” (verb), “Who is participating in the main\nactivity” (role Agent), “What does the actor use in the main activity” (role Tool), “Where is\nthe actor in the image” (entity location of role Agent), etc. For this reason, the models are\nuseful for predetermined questions on situations. Taking advantages of these properties, we\ncan apply the models for industry such as unmanned surveillance system or service robot.\n12 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\n6 Conclusion\nWe propose the ﬁrst Transformer architecture for GSR, which achieves the state-of-the-art\naccuracy on every evaluation metric. Our model, GSRTR, can capture high-level semantic\nfeature, and ﬂexibly deal with the complicated and image-dependent role relations. We\nperform extensive experiments and qualitatively illustrate the effectiveness of our method.\nAcknowledgement: This work was supported by the NRF grant and the IITP grant funded\nby Ministry of Science and ICT, Korea (No.2019-0-01906 Artiﬁcial Intelligence Graduate\nSchool Program–POSTECH, NRF-2021R1A2C3012728–50%, IITP-2020-0-00842–50%).\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kir-\nillov, and Sergey Zagoruyko. End-to-End Object Detection with Transformers. InPro-\nceedings of the European Conference on Computer Vision (ECCV) , pages 213–229,\n2020.\n[2] Thilini Cooray, Ngai-Man Cheung, and Wei Lu. Attention-Based Context Aware Rea-\nsoning for Situation Recognition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 4736–4745, 2020.\n[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale. In International Conference on Learn-\ning Representations (ICLR), 2021.\n[4] Charles J. Fillmore, Christopher R. Johnson, and Miriam R.L. Petruck. Background to\nFramenet. International Journal of Lexicography, 16(3):235–250, 2003.\n[5] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feed-\nforward neural networks. In Proceedings of the Thirteenth International Conference\non Artiﬁcial Intelligence and Statistics, pages 249–256, 2010.\n[6] Yunchao Gong, Liwei Wang, Ruiqi Guo, and Svetlana Lazebnik. Multi-scale Orderless\nPooling of Deep Convolutional Activation Features. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 392–407, 2014.\n[7] He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian. Deep Residual\nLearning for Image Recognition. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 770–778, 2016.\n[8] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on Attention\nfor Image Captioning. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 4634–4643, 2019.\n[9] Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun, and Sanja Fi-\ndler. Situation Recognition with Graph Neural Network. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), pages 4173–4182, 2017.\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 13\n[10] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated Graph Se-\nquence Neural Networks. In International Conference on Learning Representations\n(ICLR), 2016.\n[11] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge\nBelongie. Feature Pyramid Networks for Object Detection. InProceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 2117–2125,\n2017.\n[12] Hengyue Liu, Ning Yan, Masood Mortazavi, and Bir Bhanu. Fully Convolutional Scene\nGraph Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 11546–11556, 2021.\n[13] Shaopeng Liu, Guohui Tian, and Yuan Xu. A novel scene classiﬁcation model combin-\ning ResNet based transfer learning and data augmentation with a ﬁlter. Neurocomput-\ning, 338:191–206, 2019.\n[14] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In Inter-\nnational Conference on Learning Representations (ICLR), 2019.\n[15] Arun Mallya and Svetlana Lazebnik. Recurrent Models for Situation Recognition. In\nProceedings of the IEEE International Conference on Computer Vision (ICCV), pages\n455–463, 2017.\n[16] Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V . Le. Meta Pseudo Labels. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 11557–11568, 2021.\n[17] Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and Aniruddha Kembhavi.\nGrounded Situation Recognition. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 314–332, 2020.\n[18] Rezatoﬁghi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and\nReid, Ian and Savarese, Silvio. Generalized Intersection Over Union: A Metric and a\nLoss for Bounding Box Regression. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 658–666, 2019.\n[19] Marjaneh Safaei and Hassan Foroosh. Still Image Action Recognition by Predicting\nSpatial-Temporal Pixel Evolution. In 2019 IEEE Winter Conference on Applications of\nComputer Vision (WACV), pages 111–120, 2019. doi: 10.1109/W ACV .2019.00019.\n[20] Mohammed Suhail and Leonid Sigal. Mixture-Kernel Graph Attention Network for\nSituation Recognition. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 10363–10372, 2019.\n[21] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.\nRethinking the Inception Architecture for Computer Vision. InProceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 2818–2826,\n2016.\n[22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances\nin Neural Information Processing Systems (NIPS), 2017.\n14 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\n[23] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and Tell: A\nNeural Image Caption Generator. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 3156–3164, 2015.\n[24] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao\nShen, and Huaxia Xia. End-to-End Video Instance Segmentation With Transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 8741–8750, 2021.\n[25] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai\nZhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On Layer Normalization in the\nTransformer Architecture. In International Conference on Machine Learning (ICML),\npages 10524–10533. PMLR, 2020.\n[26] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene Graph Generation\nby Iterative Message Passing. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 5410–5419, 2017.\n[27] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. Graph R-CNN\nfor Scene Graph Generation. In Proceedings of the European Conference on Computer\nVision (ECCV), pages 670–685, 2018.\n[28] Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. Situation Recognition: Visual Se-\nmantic Role Labeling for Image Understanding. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR), pages 5534–5542, 2016.\n[29] Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, and Ali Farhadi. Commonly Un-\ncommon: Semantic Sparsity in Situation Recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 7196–7205,\n2017.\n[30] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image Cap-\ntioning with Semantic Attention. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4651–4659, 2016.\n[31] Chuhan Zhang, Ankush Gupta, and Andrew Zisserman. Temporal Query Networks for\nFine-grained Video Understanding. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 4486–4496, 2021.\n[32] Zhichen Zhao, Huimin Ma, and Shaodi You. Single Image Action Recognition using\nSemantic Body Part Actions. In Proceedings of the IEEE International Conference on\nComputer Vision (ICCV), pages 3391–3399, 2017.\n[33] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva.\nLearning Deep Features for Scene Recognition using Places Database. In Advances\nin Neural Information Processing Systems (NIPS), 2014.\n[34] Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu Zhao, Boxun Li, Chen-\nguang Zhang, Chi Zhang, Yichen Wei, et al. End-to-End Human Object Interaction\nDetection with HOI Transformer. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 11825–11834, 2021.\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 15\nA Appendix\nThis provides more details of our model, further analyses on it, additional ablation studies\nand experimental results. Section A.1 describes the transformer architecture of GSRTR in\ndetail, Section A.2 performs the ablation studies on GSRTR, and Section A.3 provides more\nqualitative examples of the total prediction of GSRTR. Finally, a more thorough qualitative\nanalysis on attention of GSRTR is illustrated in Section A.4.\nA.1 Detailed Transformer Architecture\nTransformer Encoder-Decoder: The detailed transformer architecture of GSRTR is given\nin Figure A1. The encoder takes as input a verb token and ﬂattened image features, and\nthen produces a verb feature and image features. Along with image features given by the\nencoder, the decoder takes as input semantic role queries, and then produces output features\ncorresponding to the semantic roles. The encoder is a stack of six encoder layers and the\ndecoder is a stack of six decoder layers. Each encoder layer consists of a Multi-Head Self-\nAttention (MHSA) block and a Feed-Forward Network (FFN) block. Each decoder layer\nconsists of a MHSA block, a Multi-Head Attention (MHA) block, and a FFN block. We use\nPre-Layer Normalization (Pre-LN) [25], i.e., LayerNorm is used before each MHSA block,\nMHA block, and FFN block, and also before the verb feature and before the decoder output\nfeatures corresponding to the semantic roles. The skip connection, using 0 .15 dropout rate,\nis given by:\nx +Dropout(Block(LayerNorm(x))), (A.1)\nwhere x ∈Rd and Block denotes one of the MHSA block, MHA block, and FFN block.\nNote that we use d = 512. The FFN block is 2 fully-connected layers with ReLU activation\nfunction and 2048 hidden dimensions, using 0.15 dropout rate, and it is given by:\nFFN(x) =W2 (Dropout(max(W1x +b1,0)))+ b2, (A.2)\nwhere x ∈Rd, W1 ∈R2048×d, b1 ∈R2048, W2 ∈Rd×2048, and b2 ∈Rd. We use Xavier\ninitialization [5] for the learnable parameters in the encoder and decoder.\nMulti-Head Attention: MHA takes as input a query sequence XQ ∈Rd×nQ and a key-value\nsequence XKV ∈Rd×nKV , where nQ denotes the query sequence length and nKV denotes the\nkey-value sequence length. MHSA corresponds to the case when the query sequence is same\nwith the key-value sequence in MHA,i.e., when XQ = XKV in MHA. MHA is formulated as:\nMHA(XQ,XKV ) =WO\n[\nHead1 (XQ,XKV );···;HeadH (XQ,XKV )\n]\n, (A.3)\nwhere H is the number of heads, [;] denotes a concatenation and WO ∈Rd×d denotes an\noutput projection. Note that we use H = 8. Head m denotes each attention function with\nlinear projections for m = 1,···,H, and it is given by:\nHeadm(XQ,XKV ) =Attn\n(\nWm\nQ XQ,Wm\nK XKV ,Wm\nV XKV\n)\n, (A.4)\nwhere Wm\nQ ,Wm\nK ,Wm\nV ∈Rd′×d denotes linear projection of mth head for key, query, and value,\nrespectively. The linear projection matrices are learnable parameters, which are not shared\nacross the MHA and MHSA blocks in the encoder and decoder layers. Note that we use\n16 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\nFFNFFNLayerNormLayerNormEncoder\nLayerNormMulti-Head Self-Attention\nDecoder\nLayerNormMulti-Head Self-AttentionLayerNormMulti-Head AttentionVLayerNormVerbNounBox ExistenceBox\n66\nverb tokenflattened image featuressemantic role querieszero input\npositional encodingsV K QV K QV K Q\nLayerNorm\nFigure A1: The detailed transformer architecture of GSRTR. A verb token and ﬂattened\nimage features are used for the ﬁrst encoder layer input (black line in Encoder). Zero input\nis used for the ﬁrst decoder layer input (black line in Decoder). Positional encodings are\nadded to the keys and queries of the MHSA block in each encoder layer and the keys of the\nMHA block in each decoder layer (red line). Semantic role queries are added to the keys and\nqueries of the MHSA block in each decoder layer and the queries of the MHA block in each\ndecoder layer (blue line). We omit Dropout in this diagram.\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 17\nd′= 64, where d′= d\nH . Attn denotes an attention function which transforms a query sequence\nQ ∈Rd′×nQ into an output sequence, whose element is a weighted sum of a value sequence\nV ∈Rd′×nKV . For ith query qi ∈Rd′\n, each weight of the sum is computed by a softmax\nfunction (i.e., Softmax) after a scaled dot-product between theith query qi and a key sequence\nK ∈Rd′×nKV . In other words, the ith element of the attention function output from the query\nsequence Q, key sequence K, and value sequence V is given by:\nAttni(Q,K,V ) =∑\nj\nSoftmaxj\n( 1√\nd′qiK\n)\nvj, (A.5)\nwhere Softmax j denotes the jth output of the softmax function and vj ∈Rd′\ndenotes the jth\nvalue.\nThe MHSA block in the encoder: The encoder takes as input a verb token and ﬂattened\nimage features. The positional encodings P ∈Rd×hw are used, where hw denotes the length\nof ﬂattened image features. The positional encodings P are 2D learnable embeddings, and\nthey are used at the attention function of each MHSA block in the encoder. To be speciﬁc,\nthe positional encodings are added to the corresponding image features, which are used as\nthe key and query inputs at the attention function. For the verb token, we append zero to\nthe positional encodings, leading to P′∈Rd×(1+hw). As a result, the positional encodings P′\nare added to the key and query inputs of the attention function in each MHSA block of the\nencoder. Thus, the mth attention function in each MHSA block of the encoder is given by:\nHeadm(XQ,XKV ) =Attn\n(\nWm\nQ\n(\nXQ +P′)\n,Wm\nK\n(\nXKV +P′)\n,Wm\nV XKV\n)\n, (A.6)\nwhere XQ = XKV and XQ ∈Rd×(1+hw).\nThe MHSA and MHA blocks in the decoder: Along with the image features given by the\nencoder, the decoder takes as input a sequence of the semantic role queries. Additionally\nto Section 3.3, each semantic role query w(v,r) per semantic role r ∈Rv can formulate a se-\nquence with arbitrary role orders, leading to the semantic role query sequence Sv ∈Rd×|Rv|.\nNote that the initial decoder input is set to zero. In each MHSA block of the decoder, the se-\nmantic role query sequence Sv is added to the query and key inputs of the attention function.\nIn other words, the mth attention function in each MHSA block of the decoder is given by:\nHeadm (XQ,XKV ) =Attn\n(\nWm\nQ (XQ +Sv),Wm\nK (XKV +Sv),Wm\nV XKV\n)\n, (A.7)\nwhere XQ = XKV and XQ ∈Rd×|Rv|. In each MHA block of the decoder, the semantic role\nquery sequence Sv are added to the query inputs of the attention function, and positional\nencodings P are added to the key inputs of the attention function. In other words, the mth\nattention function in each MHA block of the decoder is given by:\nHeadm (XQ,XKV ) =Attn\n(\nWm\nQ (XQ +Sv),Wm\nK (XKV +P),Wm\nV XKV\n)\n, (A.8)\nwhere XQ ∈Rd×|Rv|and XKV ∈Rd×hw.\n18 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\nTable A1: Ablation studies on our model (GSRTR).top-1 predicted verb top-5 predicted verbs ground-truth verbgrnd grnd grnd grnd grnd grndset model verb value value-all value value-allverb value value-all value value-allvalue value-all value value-all\ndev\nGSRTR w/ 4 layers40.26 31.88 19.20 25.44 10.2069.34 53.52 30.33 42.29 15.6974.09 38.88 57.97 19.75GSRTR w/ 8 layers40.49 32.10 19.46 25.69 10.3969.11 53.34 30.62 42.35 15.8874.07 39.12 58.27 19.92GSRTR w/ Post-LN40.18 31.50 18.54 25.20 9.8968.82 52.72 29.30 41.79 15.2773.30 37.60 57.50 19.34GSRTR 41.06 32.52 19.63 26.04 10.4469.46 53.69 30.66 42.61 15.9874.27 39.24 58.33 20.19\ntest\nGSRTR w/ 4 layers40.87 32.2119.13 25.35 9.8369.8753.78 30.25 41.97 15.2273.89 38.42 57.00 18.88GSRTR w/ 8 layers40.83 32.20 19.1725.4910.0369.47 53.40 30.07 41.99 15.3573.75 38.54 57.20 19.19GSRTR w/ Post-LN40.31 31.72 18.69 25.03 9.5669.86 53.57 29.89 41.99 15.1473.33 37.76 56.70 18.78GSRTR 40.63 32.1519.28 25.49 10.1069.8154.13 31.01 42.50 15.8874.11 39.00 57.45 19.67\nA.2 Ablation Studies\nWe study the effect on the number of layers and the location of LayerNorm in GSRTR. Our\nexperiments are evaluated on the dev and test splits of SWiG dataset [17], and the results are\ncompared with the proposed model and setting in Section 4.2.\nThe effect on the number of layers in the encoder and decoder is shown at the ﬁrst and\nsecond row of each set in Table A1. GSRTR w/ 4 layers denotes that each of the transformer\nencoder and decoder has four layers, and GSRTR w/ 8 layers denotes that each has eight\nlayers. In ground-truth verb setting, the noun and grounded noun accuracies of both models\ndecrease. The top-1 predicted verb and top-5 predicted verbs accuracies of both models\nmarginally ﬂuctuate.\nThe effect on the location of LayerNorm in GSRTR is shown at the third row of each set\nin Table A1. GSRTR w/ Post-LN denotes that LayerNorm is placed between skip connec-\ntions, leading to Post-Layer Normalization (Post-LN) [25] transformer architecture. In all\nevaluation metrics of each set, the accuracies of GSRTR w/ Post-LN decrease.\nA.3 More Qualitative Results of Our Model\nIn top-1 predicted verb setting on the test split of the SWiG dataset, the prediction results of\nGSRTR are shown in Figure A2, Figure A3 and Figure A4. The SWiG dataset has three noun\nannotations for each semantic role. The noun prediction is considered correct if the predicted\nnoun matches one of the three noun annotations. The box prediction is considered correct\nif the model correctly predicts box existence and the predicted box has an Intersection-over-\nUnion (IoU) value of at least 0.5 with the ground-truth box. Note that the grounded noun\nprediction is considered correct if the predicted noun and predicted box are correct.\nFigure A2 shows the correct grounded noun prediction results. Figure A3 shows the\nfailure cases of box prediction. There are incorrect box predictions when bounding boxes\nhave extreme aspect ratios ( e.g., the boxes of the role Tool in the Surﬁng and the Coloring\nimage), or small scales (e.g., the box of the role Agent in the Mowing image and the box of\nthe role Tool in the Helping image). Figure A4 shows the failure cases of noun prediction,\nincluding incorrect box predictions. Even in the failure cases, there are the cases where\nGSRTR reasonably predicts nouns. For example, in the Tilting image, GSRTR predicts that\nthe noun of the role Place is Outdoors, which is similar to the ﬁrst annotation Outside. In\nthe Curling image, GSRTR predicts that the nouns of the role Agent and Place are Person\nand / 0, which are enough to describe the given image. There is also the case where GSRTR\ninappropriately predicts nouns. In the Chasing image, GSRTR predicts that the noun of the\nrole Chasee is Zebra, whereas the three noun annotations are Bull, Calf, and Cow.\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 19\nFigure A2: Correct grounded noun predictions of GSRTR in top-1 predicted verb setting on\nthe test set. For each semantic role, three annotators record noun annotations.\nFigure A3: Incorrect box predictions of GSRTR in top-1 predicted verb setting on the test\nset. The dashed box denotes incorrect box prediction.\n20 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\nFigure A4: Incorrect noun predictions of GSRTR in top-1 predicted verb setting on the test\nset. The incorrect noun predictions are highlighted in red color. The dashed box denotes\nincorrect box prediction.\nFigure A5: Role Attention Map on Image Features for a Decorating image from the MHA\nblock in each decoder layer.\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 21\nA.4 Qualitative Analysis on Attention\nRole Attention Map on Image Features: In Figure A5, Figure A6 and Figure A7, each\ncolumn shows the difference of attention maps among roles. Each row shows the transition\nof attention maps through the decoder layers. In Figure A5, the role Decorated focuses on\nthe decorated stuff and the role Item focuses on the decoration item. Figure A6 shows that\nGSRTR can understand the given image and distinguish between the roleAgent and the role\nVictim. Figure A6 and Figure A7 show that GSRTR can ﬁgure out the background for the\nrole Place in the given image.\nFigure A6: Role Attention Map on Image Features for a Apprehending image from the\nMHA block in each decoder layer.\nFigure A7: Role Attention Map on Image Features for a Smelling image from the MHA\nblock in each decoder layer.\n22 CHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS\nVisualization of Role Relations: GSRTR captures the relations among roles in the similar\nway if the situations of the given images are similar. In Figure A8, the role Vehicle focuses\non the role Place, i.e., the runway (Place) and the railway station (Place) are highly related\nto the airplane ( Vehicle) and the train ( Vehicle) given the verb Boarding, respectively. In\nFigure A9, the role Obstacle and the role Tool focus on the role Place, i.e., the cliff (Place)\nis highly related to the rock (Obstacle) and the rope (Tool) given the verb Climbing.\nFigure A8: Visualization on Role Relations for two Boarding images from the MHSA block\nin the last decoder layer. Attention scores are represented as column-wise sum to 1.\nFigure A9: Visualization on Role Relations for two Climbing images from the MHSA block\nin the last decoder layer. Attention scores are represented as column-wise sum to 1.\nFigure A10: Verb Token Attention Map on Image Features for three Biting images. Each\nrow consists of an image and attention maps from the MHSA block in each encoder layer.\nCHO ET AL.: GROUNDED SITUATION RECOGNITION WITH TRANSFORMERS 23\nVerb Token Attention Map on Image Features: GSRTR can capture the key feature to\ninfer the salient action. Figure A10 and Figure A11 show that GSRTR focuses on the bit-\nten part and the falling agent, respectively. The rightmost column shows that the semantic\nregions where the verb token focuses on are similar for the same verb. Each row shows the\ntransition of attention maps through the encoder layers.\nFigure A11: Verb Token Attention Map on Image Features for three Falling images. Each\nrow consists of an image and attention maps from the MHSA block in each encoder layer.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7563494443893433
    },
    {
      "name": "Computer science",
      "score": 0.7264479398727417
    },
    {
      "name": "Encoder",
      "score": 0.620304524898529
    },
    {
      "name": "Noun",
      "score": 0.5813348889350891
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5698479413986206
    },
    {
      "name": "Salient",
      "score": 0.5620666146278381
    },
    {
      "name": "Verb",
      "score": 0.560093104839325
    },
    {
      "name": "Architecture",
      "score": 0.508796751499176
    },
    {
      "name": "Natural language processing",
      "score": 0.4560149013996124
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3466359078884125
    },
    {
      "name": "Engineering",
      "score": 0.10750904679298401
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I123900574",
      "name": "Pohang University of Science and Technology",
      "country": "KR"
    }
  ],
  "cited_by": 7
}