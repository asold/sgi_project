{
  "title": "Open-source LLMs for text annotation: a practical guide for model setting and fine-tuning",
  "url": "https://openalex.org/W4405552797",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2105367364",
      "name": "Meysam Alizadeh",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A3119245614",
      "name": "Maël Kubli",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2921410335",
      "name": "Zeynab Samei",
      "affiliations": [
        "Institute for Research in Fundamental Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2791963985",
      "name": "Shirin Dehghani",
      "affiliations": [
        "Allameh Tabataba'i University"
      ]
    },
    {
      "id": "https://openalex.org/A5106848705",
      "name": "Mohammadmasiha Zahedivafa",
      "affiliations": [
        "Iran University of Science and Technology"
      ]
    },
    {
      "id": null,
      "name": "Juan D. Bermeo",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A3095465386",
      "name": "Maria Korobeynikova",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A1818728020",
      "name": "Fabrizio Gilardi",
      "affiliations": [
        "University of Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4301183006",
    "https://openalex.org/W4386074242",
    "https://openalex.org/W3034824379",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2251617662",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W4385571411",
    "https://openalex.org/W4386102168",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4389524317",
    "https://openalex.org/W4384261711",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4400789854",
    "https://openalex.org/W4402304442",
    "https://openalex.org/W4390947766",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4321760789",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W4385570354",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4366277658",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W2979691890",
    "https://openalex.org/W4366733439",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W4310930460"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nJournal of Computational Social Science (2025) 8:17\nhttps://doi.org/10.1007/s42001-024-00345-9\nRESEARCH ARTICLE\nOpen‑source LLMs for text annotation: a practical guide \nfor model setting and fine‑tuning\nMeysam Alizadeh1  · Maël Kubli1 · Zeynab Samei2 · Shirin Dehghani3 · \nMohammadmasiha Zahedivafa4 · Juan D. Bermeo1 · Maria Korobeynikova1 · \nFabrizio Gilardi1\nReceived: 9 April 2024 / Accepted: 19 November 2024 / Published online: 18 December 2024 \n© The Author(s) 2024\nAbstract\nThis paper studies the performance of open-source Large Language Models (LLMs) \nin text classification tasks typical for political science research. By examining tasks \nlike stance, topic, and relevance classification, we aim to guide scholars in mak -\ning informed decisions about their use of LLMs for text analysis and to establish a \nbaseline performance benchmark that demonstrates the models’ effectiveness. Spe-\ncifically, we conduct an assessment of both zero-shot and fine-tuned LLMs across a \nrange of text annotation tasks using news articles and tweets datasets. Our analysis \nshows that fine-tuning improves the performance of open-source LLMs, allowing \nthem to match or even surpass zero-shot GPT −3.5 and GPT-4, though still lagging \nbehind fine-tuned GPT −3.5. We further establish that fine-tuning is preferable to \nfew-shot training with a relatively modest quantity of annotated text. Our findings \nshow that fine-tuned open-source LLMs can be effectively deployed in a broad spec-\ntrum of text annotation applications. We provide a Python notebook facilitating the \napplication of LLMs in text annotation for other researchers.\nKeywords ChatGPT · LLMs · Open source · FLAN · LLaMA · NLP · Text \nannotation\n * Meysam Alizadeh \n alizadeh@ipz.uzh.ch\n1 Department of Political Science, University of Zurich, 8050 Zurich, Switzerland\n2 Department of Computer Science, Institute for Fundamental Research, Tehran, Iran\n3 Department of Computer Engineering, Allameh Tabataba’i University, Tehran, Iran\n4 Department of Computer Science, Iran University of Science and Technology, Tehran, Iran\n Journal of Computational Social Science (2025) 8:17\n17 Page 2 of 25\nIntroduction\nGenerative Large Language Models (LLMs) such as GPT-3 and GPT-4 have dem-\nonstrated substantial potential for text-annotation tasks common to many Natu-\nral Language Processing (NLP) and political science applications [11]. Recent \nresearch reports impressive performance metrics for these models. For instance, \nstudies demonstrate that GPT −3.5 exceeds the performance of crowd-workers in \ntasks encompassing relevance, stance, sentiment, topic identification, and frame \ndetection [13], that it outperforms trained annotators in detecting the political \nparty affiliations of Twitter users [37], and that it achieves accuracy scores over \n0.6 for tasks such as stance, sentiment, hate speech detection, and bot identifica-\ntion [49]. Notably, GPT −3.5 also demonstrates the ability to correctly classify \nmore than 70% of news as either true or false [15], which suggests that LLMs \nmight potentially be used to assist content moderation processes.\nWhile the performance of LLMs for text annotation is promising, several \naspects remain unclear and require further research. Before proceeding to classify \ndocuments, the researcher must: (1) choose whether to employ an LLM without \nfurther training (zero-shot) or to manually annotate a subset of data and use a \nfew-shot or fine-tuning strategy, (2) decide how many instances of data to anno-\ntate for few-shot or fine-tuning, (3) choose between GPT −3.5, GPT-4, and open-\nsource LLMs such as LLaMA and FLAN. In the sections below, we offer empiri-\ncal evidence demonstrating the significance of these decisions in practical terms \nand provide recommendations on how to best make their decisions. Throughout, \nour goal is to highlight the capabilities of open-source LLMs and fine-tuning \napproach, and provide a practical guide for researchers.\nZero-Shot Learning enables models to generalize to hitherto unseen tasks with-\nout the requirement for labeled examples, while Few-Shot Learning leverages a \nminimal set of annotated instances to adapt the model to new tasks. Despite their \napplicability, the conditions under which one paradigm outperforms the other \nremain an open question. On the other hand, fine-tuning constitutes the retrain-\ning of LLMs on a specialized, domain-specific dataset to augment task-specific \nperformance. This process helps in endowing the LLM with domain-specific \nknowledge, while also potentially reducing some of the model biases. However, \nthis may introduce biases present in the fine-tuning dataset, thereby requiring \ncareful consideration of the data employed for this purpose. The extent to which \nFine-Tuning and Few-Shot Learning methodologies substantively improve model \nperformance remains indeterminate. Specifically, unresolved issues include the \nquantity of annotated data requisite for significant performance gains in the Fine-\nTuning process, and whether Few-Shot Learning yields statistically meaningful \nimprovements.\nMoreover, the role of open-source LLMs deserves more attention. While mod-\nels like GPT −3.5 have democratized the field by offering a more cost-effective \nalternative to traditionally more expensive annotation methods involving human \nannotations, open-source LLMs represent a further step towards greater acces-\nsibility. Beyond cost, the advantages of open-source LLMs include degrees of \nJournal of Computational Social Science (2025) 8:17 Page 3 of 25 17\ntransparency and reproducibility that are typically not provided by commercial \nmodels. open-source LLMs can be scrutinized, tailored, and enhanced by a wider \nuser base, fostering a diverse group of contributors and improving the overall \nquality and fairness of the models. Furthermore, open-source LLMs offer sig-\nnificant data protection benefits. They are designed not to share data with third \nparties, enhancing security and confidentiality. For these reasons, the academic \ncommunity is increasingly advocating for the use of open-source LLMs [27, 36]. \nThis transition would not only broaden researchers’ access to these tools but also \npromote a more open and reproducible research culture.\nTo address these questions, we extend previous research [13] to compare the per-\nformance of two widely-used open-source LLMs, LLaMA and FLAN, with that of \nGPT 3.5 as well as MTurkers, using eleven text annotation tasks distributed across \nfour datasets. Each model is tested using different settings: varied model sizes for \nFLAN, and distinct temperature parameters in both zero-shot, few-shot, and fine-\ntuning approaches for GPT 3.5, LLaMA-1 (through HuggingChat), and LLaMA-2. \nWe then compare their accuracy, using agreement with trained annotators as a met-\nric, against that of MTurk as well as amongst themselves.\nRelated work\nThe practice of fine-tuning transformer models for specialized tasks has become a \ncornerstone in the field of natural language processing (NLP). With the advent of \nlarge language models (LLMs) like GPT−3.5/4 and Bard (now Gemini), a burgeon-\ning body of research is emerging to evaluate their performance and utility across \nvarious tasks.\nLLMs have catalyzed a significant paradigmatic shift in the field of NLP, notably \nin the area of text annotation. Their capability to mimic human behavior, understand \ncontext, and adapt makes them effective for data annotation tasks [45]. This adapt-\nability is primarily conditioned through intricate prompt engineering methods, rang-\ning from zero-shot to few-shot promoting techniques, thereby focusing on generat-\ning accurate outputs [24].\nPrevious studies have extensively explored the capabilities of LLMs in diverse \nNLP applications such as text classification, text alignment, and semantic similar -\nity tasks. [13] has shown that GPT −3.5 excels in multiple annotation tasks com-\npared to human annotators. [38] further substantiates the efficacy of LLMs in the \npolitical arena by demonstrating that ChatGPT-4 outperforms both expert classifiers \nand crowd workers in classifying the political affiliation of Twitter posts, even when \nrequiring reasoning based on contextual knowledge and author intentions. Promis-\ning results in named entity recognition, fact-checking and various text annotation \ntasks have also been reported [11, 12, 16]. However, the focus has largely been on \npractical NLP applications, leaving unexplored the potential in corpus pragmatics \nand corpus-assisted discourse studies [45].\nHowever, it is imperative to acknowledge that the performance of LLMs is highly \ncontingent upon both the quality of the dataset provided to be annotated and the \n Journal of Computational Social Science (2025) 8:17\n17 Page 4 of 25\nproficiency of the model itself in this area [29]. Therefore, the ongoing refinement \nand fine-tuning of these models are imperative.\nIn terms of fine-tuning methodologies, various approaches have been devel-\noped. Notably, Parameter-Efficient Fine Tuning (PEFT) has garnered attention for \noptimizing LLMs in resource-constrained settings [20]. Another method that has \nreceived scholarly attention is the ’chain of thought’ technique, as validated by [43, \n48]. The prevailing methodology, however, continues to be the attachment of task-\nspecific heads to existing architectures, followed by domain-specific training and \nperformance evaluation [18]. Moreover, several papers suggest that fine-tuning large \nlanguage models can be effective in improving their performance and also reduce \ntheir size. [41] propose a structured pruning approach based on low-rank factori-\nzation and L0 norm regularization, which achieves significant inference speedups \nwhile maintaining or surpassing the performance of unstructured pruning methods. \n[5] explores the possibility of turning large language models into cognitive mod-\nels by fine-tuning them on psychological experiment data, showing that they can \naccurately represent human behavior and outperform traditional cognitive models in \ndecision-making tasks.\nThe ascendancy of proprietary LLMs has engendered a series of ethical and prac-\ntical concerns, particularly pertaining to cost, transparency, and data protection. In \ncontrast, open-source LLMs offer compelling advantages, including cost-effective-\nness, methodological transparency, replicability, and stringent data protection stand-\nards [23, 27, 36].\nGiven the rapidly evolving landscape of LLM applications in text annotation and \nthe critical role of prompt engineering in their performance, our work aims to evalu-\nate the efficiency of fine-tuned open-source LLMs across a broader range of text \nannotation tasks. We also offer a side-by-side performance comparison with fine-\ntuned GPT−3.5.\nMaterials and methods\nData\nThe analysis relies on four distinct datasets. The first dataset consists of 2,978 ran-\ndomly selected tweets from a more extensive collection of 2.6 million tweets related \nto content moderation, spanning from January 2020 to April 2021. The second data-\nset comprises 3,006 tweets posted by members of the US Congress between 2017 \nand 2022, sampled from a dataset of 20 million tweets. The third dataset consists \nof 2,480 newspaper articles on content moderation published from January 2020 to \nApril 2021, drawn from a dataset of 980k articles obtained via LexisNexis. Sample \nsizes were determined based on the number of texts required to construct training \nsets for machine-learning classifiers. Finally, the fourth dataset replicates the data \ncollection process of the first dataset. Specifically, it focused on January 2023, com-\nprising a random sample of 1,313 Tweets from a dataset of 1.3 million tweets.\nFor the fine-tuning section of the three LLMs, we aim to allocate at least 15 % or \nmore of the dataset to the evaluation set and the remaining 85 % to the training sets. \nJournal of Computational Social Science (2025) 8:17 Page 5 of 25 17\nWe structure the training sets in increments of 50, 100, 250, 500, 1000, and 1500 \nsamples, depending on the dataset’s size. If the evaluation contains fewer than 100 \nrows or less than two instances of the minority class (the least frequent class), we \nadjust its proportion upwards until these conditions are fulfilled. Beyond meeting \nthese prerequisites, the evaluation set proportion is incrementally expanded by 5 % \nas long as it does not compromise the planned training set sizes. This approach aims \nto optimize the evaluation sample size across an array of training set dimensions, \nthereby enabling a comprehensive assessment of how varying training data volumes \nimpact the performance of all three LLMs. This approach also facilitates a compara-\ntive analysis using identical datasets for zero-shot learning, few-shot learning, and \nfine-tuning (Table 1).\nData annotation tasks\nWe implemented several annotation tasks: (1) relevance: whether a tweet is about \ncontent moderation or, in a separate task, about politics; (2) topic detection: whether \na tweet is about a set of six pre-defined topics (i.e. Section 230, Trump Ban, Com-\nplaint, Platform Policies, Twitter Support, and others); (3) stance detection: whether \na tweet is in favor of, against, or neutral about repealing Section 230 (a piece of US \nlegislation central to content moderation); (4) general frame detection: whether a \ntweet contains a set of two opposing frames (“problem’ and “solution”). The solu-\ntion frame describes tweets framing content moderation as a solution to other issues \n(e.g., hate speech). The problem frame describes tweets framing content moderation \nas a problem on its own as well as to other issues (e.g., free speech); (5) policy frame \ndetection: whether a tweet contains a set of fourteen policy frames proposed in [7]. \nThe full text of instructions for the five annotation tasks is presented in Appendix \nS1. We used the exact same wordings for LLMs and MTurk.\nTable 1  Comprehensive \nOverview of Datasets Employed \nfor Fine-Tuning Models \nAcross Varied Tasks: Content \nModeration Tweets from 2021 \n[A], Content Moderation \nTweets from 2023 [B], Content \nModeration News Articles from \n2021 [C], and Tweets from the \nU.S. Congress spanning 2017 to \n2021 [D]\nSpecifications encompass Task Categories, Evaluation Dataset \nDimensions, and Varied Sample Sizes Utilized in Fine-Tuning \nAssessment\nDataset Task Eval. Size Fine-Tuning Size\nA Relevance 387 50,100,250,500,1000,1500\nA Problem/Solution 328 50,100,250,500\nA Policy Frames 843 50,100,250,500\nA Stance Detection 277 50,100,250,500,1000\nA Topics 307 50,100,250\nB Relevance 144 50,100,250,500,1000\nB Problem/Solution 100 50,100,250,500\nC Relevance 559 50,100,250,500,1000,1500\nC Problem Solution 196 50,100,250,500,1000\nD Relevance 836 50,100,250,500,1000,1500\nD Policy Frames 341 50,100,250,500\n Journal of Computational Social Science (2025) 8:17\n17 Page 6 of 25\nTrained annotators\nWe trained three political science students to conduct the annotation tasks. For each \ntask, they were given the same set of instructions described above and detailed in \nAppendix S1. Importantly, to minimize inter-coder discrepancies and enhance the \nrobustness of the annotation process, each of the students operated independently \nand systematically annotated the dataset task by task, adhering to a uniform code-\nbook and shared procedural guidelines.\nCrowd‑workers\nTo maintain a consistent comparative framework, we engaged workers from Ama-\nzon’s Mechanical Turk (MTurk) to execute the identical tasks administered to \ntrained human annotators and Large Language Models (LLMs). These MTurk work-\ners operated under the same instructional guidelines elaborated in Appendix S1. To \nensure the quality and reliability of annotations, we imposed several restrictions on \nworker eligibility. Specifically, we limited task access to individuals designated as \n\"MTurk Masters\" by Amazon. Additionally, these workers were required to have a \nHuman Intelligence Task (HIT) approval rate exceeding 90 % and a minimum of 50 \napproved HITs. We further restricted their geographic location to the United States. \nTo mitigate the risk of undue influence from individual workers on the annotations \nfor a specific task, we instituted a cap, ensuring that no single worker could contrib-\nute annotations to more than 20 % of the tweets allocated to a given task. Similar to \nour approach with trained human annotators, each tweet underwent annotation by \ntwo distinct MTurk workers to bolster the integrity and robustness of the collected \ndata.\nLLM selection and settings\nIn our endeavor to evaluate the annotation performance and cost efficiency of \nvarious large language models (LLMs), we selected four distinct LLMs. The first \nmodel chosen was OPENAI’s GPT-4, GPT−3.5 (‘gpt−3.5-turbo’ version), a propri-\netary, closed-source LLM. Complementing this, we incorporated Meta’s LLaMA-1 \n(‘oasst-sft-6-LLaMA-30b’ version) and the more recent LLaMA-2 in two configura-\ntions: ‘LLaMA-2 13b’ and ‘LLaMA-2 70b as well as LLaMA-3 in its LLaMA-3 8b \nconfiguration. The selection was rounded off with FLAN-T5, a model we opted for \ndue to its demonstrated promise in prior research [8, 50]. For FLAN-T5, available in \nsizes ranging from 80 M to 20B parameters, we experimented with the L, XL, and \nXXL variants to explore zero-shot capabilities (see Figure S1 in Appendix). Ulti-\nmately, we selected the FLAN-XL model for fine-tuning due to its advantageous \nbalance of computational resource demands and text processing capabilities. This \nselection was driven by the model’s ability to provide a sophisticated understanding \nand processing of text, which is essential for optimal annotation performance.\nIn our Zero-Shot versus Few-Shot analysis, we employed only GPT −3.5 and \nLLaMA-1 (via HuggingChat) with the temperature set to 0.2. For the fine-tuning \nJournal of Computational Social Science (2025) 8:17 Page 7 of 25 17\nphase, we utilized the bare LLaMA-1 model, FLAN-XL, and other selected mod-\nels, with the temperature set to 0.0. Our earlier findings informed this decision, \nwhere we observed a high level of agreement between runs with a temperature of \n0.2, eliminating the need to run each model twice. Adopting a temperature of 0.0 \nfor fine-tuning ensured the maximum level of output determinism, thereby enabling \na more effective and efficient comparison between zero-shot and fine-tuned model \nperformances.\nPrompt engineering\nFor zero-shot tests, we intentionally avoided adding any prompt engineering to \nensure comparability between LLMs and MTurk crowd-workers. After testing sev -\neral variations, we decided to feed tweets one by one to GPT −3.5 using the follow-\ning prompt: “Here’s the tweet I picked, please label it as [Task Specific Instruction \n(e.g. ‘one of the topics in the instruction’)].” The corresponding prompts for each \ntask are reported in Appendix S2. For few-shot tests, we employ Chain-of-Thought \n(CoT) prompting [43], where large language models (LLMs) are provided with both \nthe question and a step-by-step reasoning answer as examples. Specifically, follow -\ning previous research [21], we use GPT −3.5 to generate two CoT-prompted exam-\nples per class per annotation task. More specifically, we supplied GPT −3.5 with \nexamples annotated by human experts, requesting an annotation and a substantiating \nexplanation for the given annotation. Should the annotation provided by GPT −3.5 \nalign with our human-generated labels-which serve as the ground truth-we subse-\nquently incorporated both the example and GPT−3.5’s explanatory rationale into the \nprompt architecture for the few-shot learning experiment. Finally, we redeploy the \nzero-shot prompts in the fine-tuning phase to facilitate a comprehensive comparison \nbetween zero-shot and fine-tuned model performances.\nLLM fine‑tuning\nPretraining Large Language Models (LLMs) on extensive corpora enables them to \nperform competently across a wide range of tasks with minimal examples, often \nachieving results that rival those of fine-tuned transformer models [6]. Specifically, \nin contexts where the LLM has not been sufficiently trained on task-relevant data, \nsupervised fine-tuning can offer advantages. This involves supplementing the model \nwith an additional dataset of labeled task-specific examples and selectively updating \na subset of its weight parameters [28, 42]. However, while effective, fine-tuning such \nextensive models, particularly those with tens to hundreds of billions of parameters, \ncan be computationally intensive, often requiring large-scale GPU clusters. How -\never, recent advancements have made it feasible to fine-tune these models on single-\nGPU systems by employing techniques such as 4-bit or 8-bit quantization and add-\ning lower-rank adapter layers to the original architecture [9].\nIn GPT−3.5, the fine-tuning process bridges the generalized learning acquired \nfrom pre-training and the specialized learning required for domain-specific Tasks. \nOpenAI’s GPT−3.5 architecture permits fine-tuning via its specialized Application \n Journal of Computational Social Science (2025) 8:17\n17 Page 8 of 25\nProgramming Interface (API), encompassing a multi-step workflow. Initially, the \nprocedure necessitates the preparation of domain-specific datasets, generally con-\nstituting labeled instances. The transformation of this data involves segregating the \ninput into three distinct components as mandated by the API. The first segment com-\nprises the system prompt, articulating the overarching task instruction (e.g., Defini-\ntion, Steps, Examples, etc.). Subsequently, the second segment encapsulates the user \nprompt, laden with domain-specific data and the instruction that requires labeling. \nThe final segment incorporates the assistant prompt, directly indicating the target \nlabels. To make sure the uploaded data adheres to the requested format, OpenAI \nprovides a function to check for compatibility.1\nActing as a facilitative mechanism, the API enables users to delineate the training \nconfiguration, offering the liberty to customize hyperparameters, such as the number \nof epochs. Once the configuration is established and data uploaded, the API initi-\nates the fine-tuning process. The model parameters are then updated iteratively to \nminimize the loss on the fine-tuning dataset. However, the inner workings of the \nfine-tuning process remain somewhat opaque, limiting interpretability and poten-\ntial improvements. Post-fine-tuning, the model is evaluated on a held-out dataset \nto ascertain its performance on the target task. The fine-tuned model can then be \ndeployed for the desired application.\nWe employed a combination of techniques to achieve efficient adaptation for \nfine-tuning the Open Source Models. Low-Rank Adaptation (LoRA) significantly \nreduces the number of trainable parameters by introducing low-rank matrices into \neach layer that capture task-specific adjustments [19]. Additionally, 4-bit quantiza-\ntion compresses the pre-trained model weights from 32-bit floating-point numbers \nto a more memory-efficient 4-bit representation, as described in [10]. This combina-\ntion allows us to perform supervised fine-tuning on large models like FLAN-T5-xl \n[42], ‘oasst-sft-6-LLaMA-30b’ and ‘LLaMA-2 13b/70b’ [22] with better efficiency \nand potentially faster training times. We selected the xl-version of FLAN due to its \nenhanced capabilities relative to its smaller counterparts while keeping the computa-\ntional demands reasonable. We used adapter layers on the Query and Value attention \nblocks in all three cases. As the training sets for each task are small and all are text \nclassification tasks, we chose r = 16 and /u1D6FC= 32 as hyperparameters for the adapter \nlayers added. A lower rank was chosen to avoid overfitting to the training set, while \nthe /u1D6FC was selected to produce a scaling of 2 and give more weight to the output of \nthe adaptive layers and force the LLMs to follow the format used in the training set \nexamples. As for the hyperparameters during training, we chose the default param-\neters of the Seq2SeqTrainer and SFTTrainer from huggingface [44].\nTo fine-tune ‘flan-t5-xl’, we used a single 80GB A100 GPU. The training exam-\nples had as input the zero-shot prompt with the coding or labeling guidelines fol-\nlowed by the text to label, and as output the letter that identified the label that should \nbe assigned to the text (i.e: in the Relevance task ’ A’ for relevant texts or ’B’ for \nirrelevant texts). For oasst-LLaMA, two 80GB A100 GPUs were needed to fine-tune \n1 Additional information for Data preparation and analysis for chat model fine-tuning can be found here: \nhttps:// cookb ook. openai. com/ examp les/ chat_ finet uning_ data_ prep.\nJournal of Computational Social Science (2025) 8:17 Page 9 of 25 17\nthe model. For the trainset examples, we followed the original prompt format used to \nfine-tune LLaMA-1 and produce the ‘oasst-sft-6-LLaMA-30b’ model 2. In the seg-\nment designated for the prompter section, we reincorporated the zero-shot prompt \nand the corresponding text requiring labeling. Conversely, in the assistant section \nof the prompt, we included the target label to which the text should be mapped to. \nFor LLaMA-2, we needed three 80GB A100 GPUs for the 70b model while only \nusing one for the smaller 13b model. Furthermore, we maintained consistency with \nprevious fine-tuning efforts by adhering to the original prompt format established \nfor the \"oasst-sft-6-LLaMA-30b\" model. To optimize training efficiency, the input \ntext for both models (LLaMA-1 & LLaMA-2) was left-truncated at a maximum of \n4096 tokens. This combination of prompt design, targeted allocation of computa-\ntional resources, and adherence to established prompting strategies facilitated effec-\ntive fine-tuning of the LLaMA models. We trained all three LLMs for three epochs \nwith a batch size of four. To further improve efficiency, we implemented a technique \ncalled gradient accumulation, where the model weights were only updated after \naccumulating gradients from every second batch.\nEvaluation metrics\nWe computed average accuracy (i.e. percentage of correct predictions), that is, the \nnumber of correctly classified instances over the total number of cases to be clas-\nsified, using trained human annotations as our gold standard and considering only \ntexts that both annotators agreed upon. Second, in applicable cases, we computed \nintercoder agreement, measured as the percentage of instances for which both anno-\ntators in a given group report the same class.\nFor the internal evaluation of the fine-tuning process, we employed additional \nmetrics to garner more profound insights into the models’ learning and generaliza-\ntion across classes. Specifically, we calculated each class’s precision, recall, and F1 \nscore to ensure that the fine-tuning process was comprehensive and aimed at enhanc-\ning performance across all classes. Precision is the ratio of correctly predicted posi-\ntive observations to the total predicted positives, providing insight into the models’ \nability to identify positive instances accurately. On the other hand, recall is the ratio \nof correctly predicted positive observations to all observations in actual class, shed-\nding light on the models’ capability to identify all possible positive instances. The \nF1-score is the weighted average of precision and recall, thereby balancing the two \nmetrics, especially in cases where one may have more significance than the other. \nEmploying these metrics facilitated a robust evaluation, ensuring that the models \nwere not biased towards the majority class and that the fine-tuning process effec-\ntively enhanced the models’ performance across all classes.\n2 https:// github. com/ LAION- AI/ Open- Assis tant/ blob/ main/ model/ MESSA GE_ AND_ TOKEN_ FOR-\nMAT. md.\n Journal of Computational Social Science (2025) 8:17\n17 Page 10 of 25\nResults\nAll results in this paper extend a previous study which compared GPT −3.5’s zero-\nshot annotation performance with that of MTurk [13]. We rely on extended datasets \n(n = 9,777), which include tweets and news articles that were collected and anno-\ntated manually on the discourse around content moderation [2], as well as a new \nsample of tweets posted in 2023 to address the concern that LLMs might be merely \nreproducing texts that could have been part of their training data. While the previ-\nous study used only GPT−3.5 for text classification, our analysis conducts the same \nclassifications using GPT-4 as well as two open-source LLMs (LLaMA and FLAN), \nusing the same codebook that was originally constructed for the research assistants \nand MTurkers (see S1).\nChoosing the training approach: zero‑shot versus few‑shot\nProbably the first decision with respect to using LLMs for text annotation is whether \nto first manually annotate a subset of data and use it for few-shot learning or just \ndirectly proceed with a LLM in a zero-shot setting (see [6] for more background). \nMoreover, even if a researcher decides to have some manually annotated data, then \nthe question is whether to use crowd-workers, or to recruit expert research assistants. \nThe later question has already been answered in a previous study [13], in which \nthe authors showed that GPT−3.5 outperforms crowd workers for several annotation \ntasks, including relevance, stance detection, topic modeling, and frame detection. \nAcross the four datasets and a total of 12 annotation tasks, the zero-shot accuracy of \nGPT−3.5 exceeds that of crowd workers by about 25 percentage points on average.\nFor the purpose of answering the question of whether to go zero-shot or few-\nshot, before comparing these approaches, we would like to highlight the necessity \nof measuring the accuracy of LLMs with a priori annotated data. In fact, no matter \nhow well GPT −3.5 or other LLMs have been reported to perform across various \ndatasets and text annotation tasks, whenever a researcher is using a new dataset or \nneed to implement a new annotation task, it is recommended to measure the accu-\nracy of LLMs on a small subset of manually annotated data. The size of the test set \nvaries in different papers (e.g. [13] and [50]). However, we recommend that a human \nexpert manually annotate at least 100 data points.\nTo understand whether it is safe for researchers to seamlessly use a LLM for \ntext annotation in a zero-shot setting or not, here we extend the previous analysis \nof [13] to include few-shot learning. We conduct chain of thought (CoT) prompt-\ning [43]. This few-shot approach involves providing LLMs with question and step-\nby-step reasoning answer examples. In addition, we include results obtained from \nGPT-4 and HuggingChat (which uses LLaMA-1) as well. We chose HuggingChat \ndue to its popularity and ease of use. The corresponding prompts are reported in S2. \nThe results are illustrated in Fig.  1. Overall, in Fig.  1, we can see that the few-shot \nresults are mixed, with some tasks slightly benefiting from few-shot learning, some \nperforming lower, and some with no difference. For GPT-4 and GPT −3.5, we see \nJournal of Computational Social Science (2025) 8:17 Page 11 of 25 17\nperformance gain for few-shot learning in 6 tasks and performance reduction in 5 \ntasks, though it varies across tasks. As for LLaMA-1 (HuggingChat), we see perfor-\nmance gain for few-shot learning in 4 tasks and performance reduction in 7 tasks.\nWith respect to what explains the mixed performance of few-shot learning across \nvarious text annotation tasks, we could not find any conclusive pattern. For example, \nlet’s consider the number of classes as the measure for the complexity of the classi-\nfication tasks. We see that the LLaMA-1 (HuggingChat) and GPT-4 benefited from \nfew-shot learning in majority of more complex tasks, including topic modeling (6 \nclasses), and framing detection (4 classes). However, GPT −3.5 saw a performance \nreduction in these three tasks. On the other hand, for the less complex tasks of rel-\nevance (first two plots from the top left in Fig.  1), we see that LLaMA-1 (Hugging-\nChat) experienced significant performance gain from few-shot learning, but GPT-4 \nFig. 1  Comparing zero- and few-shot text annotation of GPT −3.5, GPT-4, and LLaMA-1 (Hugging-\nChat). The x-axis shows the accuracy. The y-axis displays the two models grouped by the model configu-\nration, including Zero-Shot and Few-Shot. Facets represent distinct tasks and/or datasets for evaluating \nmodel configurations\n Journal of Computational Social Science (2025) 8:17\n17 Page 12 of 25\nand GPT−3.5 results are mixed. As another example, exploring different types of \ndatasets (tweets vs. news articles), we see that all GPT-4, GPT −3.5, and Hugging-\nChat are showing less accuracy for few-shot learning in the relevance tasks, but in \nthe framing detection task, the results are mixed, with GPT −3.5 benefiting from \nfew-shot learning and GPT-4 and HuggingChat losing performance from it.\nChoosing a Training Approach: Zero- vs. Few-Shot\nAdvantages:\nZero-Shot: Off-the-shelf usage; no annotation cost; academic benchmarks on performance gain with \nfew-shot learning are inconclusive.\nFew-Shot: Possibility of in-context learning with minimal examples; might lead to performance \nimprovement.\nFindings:\nIn our tests, few-shot results are mixed, some tasks benefited from it and some lost performance. No \nexplicit pattern with respect to task complexity, model selection, and data type.\nAdvice:\nAlways manually annotate 100-250 data points to measure the accuracy of LLMs, specailly If working \non a new dataset or task. We do not recommend spending much money and time on few-shot learn-\ning.\nTemperature setting: higher versus lower\nAnother important decision that a researcher should make about using LLMs for \ntext annotation is about the value of the temperature parameter. Both GPT −3.5 and \nLLaMA-1 (HuggingChat) have a temperature parameter which controls the degree \nof randomness, and thus the creativity, of the output. A higher temperature will \nresult in more diverse and unexpected responses, while a lower temperature will \nresult in more conservative and predictable responses. The default temperature value \nis 1.0 for GPT−3.5 and 0.9 for HuggingChat. Previous research showed that a lower \ntemperature value may be preferable for text annotation tasks, as it seems to increase \nconsistency without decreasing accuracy [13]. Here, we extend the previous research \nresults by assessing the effect of a lower temperature in LLaMA-1 (HuggingChat). \nSimilar to [13], we set the temperature at its default value and 0.2 and compare the \noutputs with respect to accuracy and intercoder agreement. We conducted two sets \nof annotations for each temperature value to compute LLM’s intercoder agreement.\nOur results demonstrate that lower temperature settings significantly enhance \nintercoder agreement, underscoring the deterministic and repeatable nature of \nthe annotations. For instance, GPT −3.5’s average intercoder agreement increased \nfrom 91.7% to 97.6% when the temperature was reduced from 1 to 0.2 in the \nzero-shot setting and from 92.3% to 95.4% in the few-shot setting. Similarly, for \nLLaMA-1 (HuggingChat), the agreement surged from 46.7% to 84.8% in the \nzero-shot and from 47.1% to 83.1% in the few-shot settings when the temperature \nwas lowered from 0.9 to 0.2. These substantial improvements in intercoder agree-\nment with lower temperatures provide a compelling case for their use in ensuring \nmore deterministic and reliable LLM annotations. Moreover, our examination of \nJournal of Computational Social Science (2025) 8:17 Page 13 of 25 17\naccuracy values further supports the preference for lower temperature settings. \nNotably, the accuracy in the task of ’Stance’ classification on Tweets from 2020-\n2021 increased remarkably from 53.7% to 70.7% as the temperature lowered from \n0.9 to 0.2. Additionally, in the ’Relevance’ classification for News Articles from \n2020-2021, we observed a significant accuracy boost from 56.6% to 72.3% when \nthe temperature setting was reduced. These examples underscore that a more \ndeterministic approach, achieved by lowering the temperature, improves consist-\nency and enhances the overall quality of results in various classification tasks, \naffirming the efficacy of employing lower temperature settings for improved LLM \nperformance (Fig. 3 ).\nAcross the four datasets, we report HuggingChat’s zero-shot performance for \ntwo different metrics: accuracy and intercoder agreement (Fig.  2). Accuracy is \nmeasured as the percentage of correct annotations (using our trained annotators \nas a benchmark), while the intercoder agreement is computed as the percentage \nof tweets that were assigned the same label by two different annotators (research \nassistant, crowd-workers, or GPT −3.5’s runs). Figure  2 shows that while the \naccuracy and intercoder agreement are, on average, lower than those reported for \nGPT−3.5 in [13], the pattern for the effect of temperature is the same. Across all \nfour datasets and eleven annotation tasks, decreasing the temperature significantly \nimproved the intercoder agreement scores without decreasing the accuracy. The \nonly exception is for the relevance task in Tweet (2020-2021) dataset (first row in \nthe top left plot in Fig.  2) which decreasing the temperature to 0.2 increased the \nintercoder agreement but led to reduction in accuracy. Interestingly, the GPT −3.5 \nresults in 3  shows the same pattern for this particular task and dataset.\nFig. 2  Analyzing the effect of LLaMA-1 (HuggingChat)’s temperature parameter on accuracy and inter -\ncoder agreement in text annotation tasks\n Journal of Computational Social Science (2025) 8:17\n17 Page 14 of 25\nTemperature Setting: High vs. Low\nAdvantages:\nLow Temperature: Less randomness; less creative answers; more deterministic output.\nHigh Temperature: More randomness, more creative answers, more variations in outputs.\nFindings:\nOur analyses show that in almost all annotation tasks, lower temperature setting for GPT-3.5 and Hug-\ngingChat increases the intercoder agreement rate without decreasing the accuracy.\nAdvice:\nWe recommend setting the temperature parameter at zero (no randomness).\nModel selection: proprietary versus open‑source LLMs\nProprietary closed-source LLMs such as ChatGPT and Bard (Gemini) are more \nconvenient and safe to use for general audiences due to their heavy fine-tuning \nto align with human preferences [36]. Although the training methodology is \nstraightforward and simple, the extensive computational demands have restricted \nthe creation of LLMs to a select few. That is why none of the open-source LLMs \nsuch as BLOOM, LLaMA-1 and Falcon could made a suitable substitutes for \nclosed-source LLMs [39]. More recently, responding to this demand, LLaMA-2 \nand LLaMA-3 were introduced, which are a family of pretrained and fine-tuned \nLLMs at scales up to 70B parameters [39]. LLaMA-2 evaluations showed it out-\nperforms LLaMA-1, Falcon, and MPT in standard academic benchmarks includ-\ning commonsense reasoning, world knowledge, reading comprehension, and \nFig. 3  Analyzing the effect of GPT −3.5’s temperature parameter on accuracy and intercoder agreement \nin text annotation tasks\nJournal of Computational Social Science (2025) 8:17 Page 15 of 25 17\nmath, and performs on par with GPT −3.5 in math and popular aggregated bench-\nmarks but fall short in Python code writing benchmarks [39].\nText annotation has always been costly. Although previous findings showed \nthat GPT −3.5 outperforms Amazon Mechanical Turk crowd-workers (MTurker) \nand costs almost thirty times cheaper [13], it is not free of charge. Hence, it is \ntempting for researchers to explore the extent to which open-source LLMs are \ncapable for text annotation tasks. In addition to cost-effectiveness, open-source \nLLMs are increasingly recognized for their transparency, reproducibility, and \nenhanced data protection features [27, 36]. However, the academic benchmarks \nreported above lack the text annotation tasks, especially that of political text. To \nassess how well open-source LLMs perform in text annotation tasks, we compare \nGPT (3.5 & 4) results with those of LLaMA (1 & 2), Llama-3 (8b) and FLAN \n(T5 XL). Considering the reported training data size, scaled-up parameters, and \nreading comprehension benchmarks, we expect the Llama-2 (70b) model to per -\nform well on our text annotation tasks.\nFigure  4 compares the text annotation accuracy of GPT-4, GPT −3.5, \nLLaMA-1 (HuggingChat), LLaMA-1 (30b), LLaMA-2 (13b), LLaMA-2 (70b), \nLlama-3 (8b), FLAN-T5 (XL), and MTurkers. Seven observations stand out in \nthis analysis: (1) no LLM outperforms others across all 11 annotation tasks; (2) \nonly GPT-4 and GPT −3.5 outperform MTurkers in all 11 annotation tasks; (3) \namong the open-source LLMs, the best performing one in terms of the number \nof tasks that it outperforms MTurkers is LLaMA-2 (70b), with 9 out of 11 anno-\ntation tasks that it outperforms the crowd-workers; (4) among the open-source \nLLMs, the worst performing one in terms of the number of tasks that it outper -\nforms MTurkers is LLaMA-2 (13b), with only 6 out of 11 annotation tasks that it \noutperforms the crowd-workers; (5) among the open-source LLMs, the best per -\nforming one in terms of outperforming GPT −3.5 is LLaMA-2 (70b), with 5 out \nof 11 annotation tasks it outperforms GPT −3.5; (6) Llama-3 (8b), which consid-\ners as a light LLM, performs on-par with GPT-4 on binary classification tasks, \nand outperform crowd-workers in 10 out of 11 tasks; (7) across the 9 annotation \ntasks that are related to datasets published before 2023, all open-source LLMs \noutperform crowd-workers where the number of classes is three and greater. In \nother words, when the data falls within an open-source LLM’s cutoff date and the \nannotation task is not a binary classification, all open-source LLMs perform bet-\nter than MTurkers.\nOverall, these findings underscore that while open-source LLMs are not consist-\nently the superior choice, they generally outperform crowd-sourced annotations and \nare approaching the performance levels of GPT 3.5. Even an out-of-the-box tool \nsuch as HuggingChat, which looks and works very much like ChatGPT, outperforms \nAmazon Mechanical Turk crowd-workers in 8 out of all annotation tasks and per -\nforms close enough to GPT 3.5 in 6 out of 11 annotation tasks. While there is no \nuniversal answer to the question of what is the best open-source LLM for political \ntext annotation, and the best performing LLM varies across dataset, task, and model \nsize, our results show that a collection of open-source LLMs could almost perform \non par with GPT −3.5. Therefore, we recommend to (1) use open-source LLMs for \ntext annotation in social sciences research; and (2) compare the performance of 2-3 \n Journal of Computational Social Science (2025) 8:17\n17 Page 16 of 25\nopen-source LLMs, such as LLaMA-2 (70b), FLAN-T5 (XL), and LLaMA-1 (Hug-\ngingChat), and pick the best performing one.\nModel Selection: GPT-3.5/4 vs. Open-Source LLMs\nAdvantages:\nGPT-3.5/4: Off-the-shelf usage; more convenient and safe to use; heavy fine-tuning to align with \nhuman preferences.\nOpen-Source LLMs: No cost (GPT-3.5, and especially GPT-4 can become expensive for researchers \nwithout large research budgets); more transparency; ethical way to do research due to data privacy \nconcerns; more reproducibility.\nFindings:\nFig. 4  Accuracy of GPT−3.5, GPT-4, open-source LLMs, and MTurk. Accuracy means agreement with \ntrained annotators. Bars indicate average accuracy, while whiskers range from minimum to maximum \naccuracy across models with different parameters and/or prompts (zero vs few shot)\nJournal of Computational Social Science (2025) 8:17 Page 17 of 25 17\nModel Selection: GPT-3.5/4 vs. Open-Source LLMs\nOur results show that while open-source LLMs are not consistently the superior choice, they gener-\nally outperform crowd-sourced annotations and are approaching the performance levels of GPT 3.5 \n(ChatGPT).\nAdvice:\nUse both LLaMA-2 (70b) and a light LLM from an easy-to-use open-source interface such as Hug-\ngingChat if resources allow for running heavy-size models, and pick the best-performing model. Use \nLLaMA-3 (8b) if high-performance computing is not available. Do not use LLaMA-2 (13b).\nAnnotation size: how much annotation is enough for fine‑tuning?\nIn this section, we are interested in testing the effect of fine-tuning on the perfor -\nmance of both closed- and open-source LLMs. Advanced large language models \n(LLMs), such as GPT-4 and LLaMA-2, often demonstrate new capabilities and can \nlearn from context with minimal examples, enabling them to perform complex tasks \n[35, 43]. However, fine-tuning these models is still necessary to unlock their full \npotential for creative and specialized tasks, aligning their performance with human \npreferences [34, 46]. Here, we would like to answer three important questions: (1) \nWould LLM exhibit performance gain in text annotation tasks when they get fine-\ntuned with human expert annotated data?; (2) If fine-tuning improves the perfor -\nmance of LLMs in text annotation accuracy, how big the training data should be?; \nand (3) Does the effect of fine-tuning on LLMs’ text annotation accuracy varies \nbetween close- and open-source LLMs?\nSeveral factors may influence the efficacy of fine-tuning LLMs, including but not \nlimited to 1) pretraining conditions; and 2) fine-tuning conditions. Pretraining fac-\ntors include the size of the LLM and the volume of pretraining data, which are criti-\ncal in determining the quality of the representation and knowledge encoded in the \npretrained LLMs. On the other hand, fine-tuning conditions such as the nature of \nthe downstream task, the size of the fine-tuning dataset, and the specific fine-tuning \nmethodologies employed can significantly impact the extent of knowledge transfer to \nthe targeted task [47]. Prior research has extensively investigated the scaling of LLM \npretraining or training from scratch [17] as well as the development of advanced \nmethods for fine-tuning [14]. However, the issue of whether and how the fine-tuning \nof LLMs scales with the fine-tuning conditions has been largely overlooked.\nIn this section, we are interested to explore the effect of LLM selection, LLM \nsize, and the size of the fine-tuning data on the accuracy of LLMs on our running \ntext annotation text. A recent study, based on three downstream tasks on translation \nand summarization, finds that the augmentation of the LLM model’s size exerts a \nmore substantial influence on fine-tuning compared to increasing the size of fine-\ntuning data [47]. Furthermore, their results show that the effectiveness of fine-tun-\ning varies across tasks and datasets, making the selection of the best fine-tuning \napproach for a particular downstream task less definitive. Considering these recent \nfindings, we should expect to see a similar pattern, in which the performance gain \nfrom fine-tuning is being dependent on the text annotation task, model, model size, \nand the size of fine-tuning data.\n Journal of Computational Social Science (2025) 8:17\n17 Page 18 of 25\nFigure  5 compares fine-tuning accuracy across five LLMs (LLaMA-1 (30b), \nLLaMA-2 (13b), LLaMA-2 (70b), FLAN-T5 (XL), and GPT 3.5), 11 text anno-\ntation tasks, between 4 to 7 different sizes of fine-tuning data (corresponding \nF1-scores are reported in Fig  S2 in the Appendix). Several observations stand \nout in Fig.  5. First, with the exception of LLaMA-1 (30b), we see a general \ntrend in other LLMs, in which the accuracy and F1-score of models improves \nwith the size of fine-tuning data. Second, one of our salient findings in Fig.  5 \nand Figure S2 centers on the capabilities of FLAN-T5 XL. Our analysis reveals \nthat, when fine-tuned, FLAN-T5 (XL) matches or even surpasses the zero-shot \naccuracy and F1-score of ChatGPT across all tasks and datasets, with a singular \nexception. It falls short in classifying the Problem-Solution frames in the 2023 \ntweets.\nThe third, and arguably the most compelling, finding in Fig.  5 and Fig.  S2 \ncenters on the differential rates and magnitudes of improvement observed across \nmodels during the fine-tuning process. ChatGPT demonstrates remarkable perfor -\nmance gains, even when fine-tuned with a minimal dataset of just 50 instances. \nSpecifically, it registers an average accuracy increase of 15.7 %, which further \nescalates to 19.1 % when the training set comprises 100 cases. In a stark con-\ntrast, open-source models such as FLAN-T5 (XL) and LLaMA-1 exhibit a more \nincremental progression in performance. Intriguingly, the LLaMA-1 model ini-\ntially sees a dip in accuracy on certain tasks but tends to recover and improve as \nthe dataset expands to around 250 instances. By this point, both FLAN-T5 (XL) \nand LLaMA-1 come close to matching the zero-shot accuracy levels achieved by \nChatGPT. It is worth noting that the average accuracy gains for FLAN-T5 (XL) \nFig. 5  Performance (accuracy) of GPT−3.5, LLaMA-1, LLaMA-2, and FLAN-T5 (XL), as a function of \nthe training data size for fine-tuning. The x-axis shows different sizes of training datasets, ranging from \nzero-shot (no fine-tuning) to 50, 100, 250, 500, and 1,000 rows used for fine-tuning the models. The \ny-axis displays the accuracy of the models in percentages. Facets represent distinct tasks and/or datasets \nfor evaluating the models. Pink dots represent zero-shot GPT-4 accuracy for the sake of comparison\nJournal of Computational Social Science (2025) 8:17 Page 19 of 25 17\nstand at 7.7 % and 12.4 % when fine-tuned with 50 and 100 instances, respec-\ntively. This underscores the point that open-source models, too, stand to benefit \nsignificantly from fine-tuning.\nThese findings illuminate the complex dynamics at play in the fine-tuning of \nLLMs for text annotation tasks. They highlight the variable performance across dif-\nferent models and tasks, the rapid yet plateauing gains for commercial models like \nChatGPT, and the more gradual but sustained improvements for open-source mod-\nels. Next, fine-tuning is particularly helpful when using GPT −3.5 Turbo, even with \nas little as 50 rows of training data. The results are mixed for open-source models \nlike FLAN-T5 (XL) and LLaMA-1. FLAN-T5 (XL) generally benefits from fine-\ntuning, while LLaMA-1’s performance is inconsistent. However, as shown in Fig. 5, \nincreasing the amount of training data does improve LLaMA-1’s performance over \nits zero-shot capabilities, just like for all other models.\nIn our study, we observed that the cost of fine-tuning commercial models like \nChatGPT is quite reasonable. The total expenditure for fine-tuning all our Chat-\nGPT models was only $311, with an additional $34 for the evaluation of these mod-\nels. Specifically, fine-tuning GPT −3.5 costs merely $1.2 for 100 rows across three \nepochs, with subsequent usage costs being only 16 Cents per 100 rows. These fig-\nures illustrate that even when relying on commercial models, fine-tuning is an eco-\nnomically viable option, especially when compared to the costs of zero-shot usage. \nWhile open-source LLMs can offer significant cost savings when suitable infrastruc-\nture is available, fine-tuning commercial models like ChatGPT remains an afford-\nable and efficient alternative for those without access to such resources.\nIn summary, our empirical findings robustly advocate for fine-tuning as the pri-\nmary strategy for enhancing classification accuracy across varying tasks and mod-\nels. Although zero-shot and few-shot learning paradigms may offer utility under spe-\ncific circumstances, fine-tuning emerges as the most consistently effective approach. \nHowever, it is important to note that the efficacy of fine-tuning is not universally \nhigh across all task complexities. Specifically, the availability of substantial, high-\nquality training datasets becomes imperative for achieving optimal performance lev-\nels for intricate classification tasks. Despite fine-tuning, it remains possible that the \nmodel’s performance may not meet the thresholds required for specific specialized \napplications. This is particularly relevant as more labeled datasets become availa-\nble, making fine-tuning a practical choice for those aiming to optimize classification \ntasks.\nAnnotation Size: How much annotation is enough for fine-tuning?\nAdvantages:\n50-100 Manual Annotation: Less cost; less time consuming.\n250-500 Manual Annotation: Potential for higher performance gain.\nFindings:\nIn general, LLMs’ accuracy increases with the size of fine-tuning data. FLAN-T5 (XL) matches or even \nsurpasses GPT-3.5’s zero-shot performance across all tasks and datasets (except one task). Open-\nsource LLMs and GPT-3.5 differ in the optimal number of required fine-tuning data.\nAdvice:\n Journal of Computational Social Science (2025) 8:17\n17 Page 20 of 25\nAnnotation Size: How much annotation is enough for fine-tuning?\nOur empirical findings robustly advocate for fine-tuning as the primary strategy for enhancing clas-\nsification accuracy. If using GPT-3.5, 50 annotated data points are enough. If using an open-source \nLLM, go with 250.\nA note on explainability of LLMs performance\nThroughout the paper, we presented some mixed results about which LLM outper -\nforms others in each annotation task. For example, for comparison between zero- \nand few-shot annotation, we witnessed very mixed results with no universal pattern \nexplaining what drives the performance differences. We anticipate that various fac-\ntors will contribute to explaining the behaviors and capabilities of large language \nmodels (LLMs) in text annotation tasks. These factors include but are not limited to, \nthe topic at hand and the model’s knowledge of it, the cut-off date for the model’s \ntraining data, the quality and diversity of the data used in training, the complexity of \nthe tasks presented, the inherent stochasticity of LLMs, and the extent of fine-tuning \napplied by the developers after pre-training. several notes are of importance here: \n1. Capabilities vs. Behaviors: LLMs’ capabilities stem from the pre-training stage, \nwhich is a resource-intensive and often static process. In contrast, their behavior, \nparticularly in tasks such as question answering, is shaped by fine-tuning, which \nis more cost-effective and can be updated more frequently [26].\n2. Transparency and Interpretability: In many cases, there is limited transparency \nregarding the specific data on which LLLMs have been trained, both in terms of \nthe textual corpus utilized during pre-training and the supplementary instruc-\ntions provided in fine-tuning stage. This opacity presents significant challenges in \nexplaining why a particular input yields a specific output, particularly in relation \nto the model’s internal weight structure and the training data employed. Conse-\nquently, we advise researchers to exercise caution when employing these models \nin domains where interpretability and explainability are critical [32].\n3. Unintended consequences of Fine-Tuning: Fien Tuning can sometimes lead to \nunintended performance degradation or improvements, which developers con-\nstantly work to address [26].\n4. Nondeterministic Behaviors: The combination of fine-tuning and the inherent \nnondeterminism in LLMs can result in unpredictable outcomes. For example, it \nhas been shown that GPT-4, when asked to find prime numbers in a given set, may \nskip part of the reasoning process, failing to thoroughly evaluate each number. \nSuch behavior could similarly occur in our experiments, potentially contributing \nto the mixed results we observed [26].\n5. It is shown that the topic of the conversation has sometimes induce a significant \neffect on the performance of the LLMs [1 ]. It is possible that if we expand the \ndiversity of the topics, we could find a pattern on when and why we see some \nmixed results and why LLMs produce what the produce in text annotation.\nJournal of Computational Social Science (2025) 8:17 Page 21 of 25 17\nA potential approach for assessing the impact of the aforementioned factors influ-\nencing the LLMs is the use of the ‘integrated gradients’ method [33]. This technique \nfocuses on identifying the most influential input tokens rather than exhaustively con-\nsidering all possible factors. However, as the primary objective of this paper is to \noffer practical guidelines for social science researchers on initiating the use of LLMs \nfor text annotation tasks, as well as to establish a baseline performance benchmark \nthat demonstrates the models’ effectiveness in this domain, we reserve this experi-\nment for future research.\nConclusion\nAutomated classification of short and long text is central to a growing number of \nresearch questions in the social sciences. Previous research advocated for using \nsupervised machine learning methods over dictionary-based approaches and pro-\nvided best practices for human annotation of text [4]. However, the emergence of \nlarge language models (LLM) and their ability to outperform crowd-workers in text \nannotation and yielding acceptable accuracy compared to human expert evaluation \n[13] provide researchers with new opportunities to skip the crowd-sourcing or even \ntraining their own supervised machine learning models for text classification. Nev -\nertheless, in the rush to take advantage of these opportunities, one can easily neglect \nto consider crucial questions and underestimate the implications of certain choices.\nIn this paper, we have tried to walk the researchers through critical decisions they \nneed to make for using LLMs in text annotation tasks (e.g. relevance, topic detec-\ntion, and framing detection) and provided them with some practical advice backed \nby our empirical results. Our most surprising finding is the substantial effect of fine-\ntuning on increasing LLMs text annotation performance. We demonstrate that open-\nsource LLMs such as LLaMA-1, LLaMA-2, and FLAN represent a competitive \nalternative for text annotation tasks, exhibiting performance metrics that generally \nexceed those of Amazon Mechanical Turk crowd-workers and rival those of GPT \n3.5 (ChatGPT). An important appeal of open-source LLMs is that they offer consid-\nerable cost advantages. While ChatGPT provides substantial cost-efficiency, being \nabout thirty times more affordable per annotation compared to MTurk [13], open-\nsource LLMs surpass this by being freely available. This constitutes a significant \nimprovement in the accessibility of such models, extending their reach to a broader \nrange of researchers irrespective of financial constraints.\nOpen-source LLMs present benefits that go beyond cost-efficiency. One key \nadvantage is that they help reduce reliance on proprietary models operated by for-\nprofit companies, which may conflict with research ethics and the reproducibility \nstandards [27, 36]. Furthermore, open-source LLMs provide distinct benefits for \ndata protection, as they are designed in such a way that data do not need to be shared \nwith any third-party entities [40]. This feature ensures that sensitive information \nremains secure and confidential because it is not sent to or stored by an external \nparty. The elimination of data sharing in open-source LLMs provides an extra layer \nof protection against potential data breaches or unauthorized access. This feature \nbecomes especially beneficial in scenarios where sensitive data is involved, such as \n Journal of Computational Social Science (2025) 8:17\n17 Page 22 of 25\nin the legal or medical fields, where confidentiality is of utmost importance [3, 30, \n31], but also in social science research involving data protected under the European \nUnion’s General Data Protection Regulation (GDPR), or covered by non-disclosure \nagreements (NDAs) [25].\nWe conclude with four general pieces of advice for text analysts: (1) manually \nannotate 250-500 data points and use half for fine-tuning and half for accuracy test-\ning; (2) use fine-tuned open-source LLMs for text annotation due to their cost-effec-\ntiveness, transparency, and reproducibility; (3) always validate the output of LLMs \nby human expert evaluation; (4) run LLMs at least twice per task and report average \naccuracy and intercoder agreement; and (5) set the temperature of GPT or LlaMA \nmodels at zero or a low value to get higher intercoder agreement without loss in \naccuracy.\nWhile the findings presented in this study regarding the performance of open-\nsource large language models (LLMs) in text annotation tasks are encouraging, it \nis crucial to underscore that these results should not be broadly generalized across \nall forms of text annotation tasks or datasets. The specificity of the tasks and data-\nsets used in our experiments may not fully represent the diversity of potential use \ncases in other research contexts. Therefore, we strongly recommend that researchers \nexercise caution when drawing conclusions from our results and conduct their own \nempirical evaluations. This is particularly important when engaging with distinct \ndatasets or addressing novel annotation tasks, as variations in data characteristics or \ntask complexities may significantly influence the performance of LLMs. The deci-\nsions and recommendations discussed in this paper should be viewed as a frame-\nwork for further experimentation, rather than definitive guidance applicable to all \nannotation scenarios.\nSupplementary Information The online version contains supplementary material available at https:// doi. \norg/ 10. 1007/ s42001- 024- 00345-9.\nAcknowledgements We thank Darya Zare, Fabio Melliger, Mohammadamin Alizadeh, Paula Moser, \nMahdis Abbasi, Sophie van IJzendoorn, and Zahra Baghshahi for excellent research assistance.\nFunding Open access funding provided by University of Zurich. This project received funding from the \nEuropean Research Council (ERC) under the European Union’s Horizon 2020 research and innovation \nprogram (grant agreement nr. 883121).\nData Availability As part of our commitment to transparency and reproducibility, we have made all the \nnecessary files available to replicate the analyses presented in this manuscript. The replication package \nincludes datasets, jupyter Python notebook for using and fine-tuning open-source LLMs, and additional \nsupplementary materials used in our study. The replication files can be accessed at the following URL: \nhttps:// osf. io/ ctgqx/.\nDeclarations \nConflict of interest None.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nJournal of Computational Social Science (2025) 8:17 Page 23 of 25 17\nto the material. If material is not included in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permis-\nsion directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\nReferences\n 1. Alghisi, S., Rizzoli, M., Gabriel, R., Seyed MM., & Giuseppe R. (2024) Should we fine-tune or rag? \nevaluating different techniques to adapt llms for dialogue. arXiv preprint arXiv: 2406. 06399\n 2. Alizadeh, M., Gilardi, F., Emma H., K.üser, K.J., Kubli, M., & Marchal, N. (2022) Content modera-\ntion as a political issue: the twitter discourse around trump’s ban. Journal of Quantitative Descrip-\ntion: Digital Media, 2,\n 3. Alizadeh, Meysam, Hoes, Emma, & Gilardi, Fabrizio. (2023). Tokenization of social media engage-\nments increases the sharing of false (and other) news but penalization moderates it. Scientific \nReports, 13(1), 13703.\n 4. Barberá, Pablo, Boydstun, Amber E., Linn, Suzanna, McMahon, Ryan, & Nagler, Jonathan. (2021). \nAutomated text classification of news articles: a practical guide. Political Analysis, 29(1), 19–42.\n 5. Binz, M., & Eric S. (2023) Turning large language models into cognitive models. arXiv preprint \narXiv: 2306. 03917.\n 6. Brown, Tom, Mann, Benjamin, Ryder, Nick, Subbiah, Melanie, Kaplan, Jared D., Dhariwal, \nPrafulla, Neelakantan, Arvind, Shyam, Pranav, Sastry, Girish, Askell, Amanda, et  al. (2020). \nLanguage models are few-shot learners. Advances in neural information processing systems, 33, \n1877–1901.\n 7. Card, D., Boydstun, A., Gross, J.H., Resnik, P., & Smith, N.A. (2015). The media frames corpus: \nannotations of frames across issues. In Proceedings of the 53rd annual meeting of the association for \ncomputational linguistics and the 7th international joint conference on natural language processing \n(volume 2: short papers), 438-444.\n 8. Chung, H., Hou, L., Longpre, S., Zoph, B., YiTay, F., William, L., Eric, X., Dehghani, M., Brahma, \nS. et al. (2022). Scaling instruction-finetuned language models. arXiv preprint arXiv: 2210. 11416.\n 9. Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023) Qlora: efficient finetuning of \nquantized llms. arXiv:  2305. 14314 [cs.LG].\n 10. Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2024). Qlora: efficient finetuning of \nquantized llms. Advances in Neural Information Processing Systems 36.\n 11. Ding, B., Qin, C., Liu, L., Chia, YK., Joty, S., Li, B., & Bing, L. (2023). Is GPT-3 a Good Data \nAnnotator? In Proceedings of the 61th annual meeting of the association for computational linguis-\ntics. June. Accessed June 30, 2023.\n 12. Frei, Johann, & Kramer, Frank. (2023). Annotated dataset creation through large language models \nfor non-english medical nlp. Journal of Biomedical Informatics, 145, 104478.\n 13. Gilardi, Fabrizio, Alizadeh, Meysam, & Kubli, Maël. (2023). ChatGPT outperforms crowd workers \nfor text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30), e2305016120.\n 14. He, J., Zhou, C, Ma, X, Berg-Kirkpatrick, T., & Neubig, G. (2021). Towards a unified view of \nparameter-efficient transfer learning. arXiv preprint arXiv: 2110. 04366.\n 15. Hoes, E, Altay, S, & Bermeo, J. (2023). Using ChatGPT to Fight Misinformation: ChatGPT Nails \n72% of 12,000 Verified Claims.\n 16. Hoes, E., Altay, S., & Bermeo, J. n.d. Using chatgpt to fight misinformation: chatgpt nails 72% of \n12,000 verified claims.\n 17. Hoffmann, J, Borgeaud, S., Mensch, A, Buchatskaya, E, Cai, Trevor, R, Eliza, C, Diego de L., Hen-\ndricks, L.A., Welbl, J., Clark, A., et  al.(2022). Training compute-optimal large language models. \narXiv preprint arXiv: 2203. 15556.\n 18. Howard, J., & Ruder, S. (2018). Universal language model fine-tuning for text classification. arXiv \npreprint arXiv: 1801. 06146.\n 19. Hu, E.J., Shen, Y, PhillipWallis, A.Z., Zeyuan, L., Yuanzhi, S.W., LuWang, & Chen, W. (2021). \nLora: low-rank adaptation of large language models. arXiv preprint arXiv: 2106. 09685.\n Journal of Computational Social Science (2025) 8:17\n17 Page 24 of 25\n 20. Hu, Z., Lan, Y., Wang, L., Xu, W., Lim, E.P., Lee, R.K.W., Bing, L., & Poria, S. (2023). Llm-adapt-\ners: an adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint \narXiv: 2304. 01933.\n 21. Kojima, T., Gu, S.S,, Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models are zero-\nshot reasoners. arXiv preprint arXiv: 2205. 11916.\n 22. Köpf, Andreas, K., Yannic, von Rütte, Dimitri, A., Sotiris, T., Zhi-Rui, S., Keith, B., Abdullah, et al. \n(2023). Openassistant conversations - democratizing large language model alignment. arXiv:  2304. \n07327 [cs.CL].\n 23. Liesenfeld, A., Lopez, A., & Dingemanse, M. (2023). Opening up chatgpt: tracking openness, trans-\nparency, and accountability in instruction-tuned text generators. In Proceedings of the 5th interna-\ntional conference on conversational user interfaces, 1-6.\n 24. Liu, Pengfei, Yuan, Weizhe, Jinlan, Fu., Jiang, Zhengbao, Hayashi, Hiroaki, & Neubig, Graham. \n(2023). Pre-train, prompt, and predict: a systematic survey of prompting methods in natural lan-\nguage processing. ACM Computing Surveys, 55(9), 1–35.\n 25. Marchal, N., Hoes, E., Klüser, K.J., Hamborg, F., Alizadeh, M., Kubli, M., & Katzenbach, C. \n(2024). How negative media coverage impacts platform governance: evidence from facebook, twit-\nter, and youtube. Political Communication, 1-19.\n 26. Narayanan, Arvind, & Kapoor, Sayash. (2024). Ai snake oil: what artificial intelligence can do, \nwhat it can’t, and how to tell the difference. Princeton University Press.\n 27. Ollion, Étienne., Shen, Rubing, Macanovic, Ana, & Chatelain, Arnault. (2024). The dangers of \nusing proprietary LLMs for research. Nature Machine Intelligence, 6(1), 4–5.\n 28. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C. et al.( 2022). \nTraining language models to follow instructions with human feedback. In Advances in neural infor-\nmation processing systems, edited by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, \nand A. Oh, 35:27730-27744. Curran Associates, Inc. https:// proce edings. neuri ps. cc/ paper_ files/ \npaper/ 2022/ file/ b1efd e53be 364a7 3914f 58805 a0017 31- Paper- Confe rence. pdf.\n 29. Pangakis, N., Samuel W., & Fasching, N. (2023). Automated annotation with generative ai requires \nvalidation. arXiv preprint arXiv: 2306. 00176.\n 30. Paul, M., Maglaras, L., Ferrag, Mohamed A., & AlMomani, I. (2023). Digitization of healthcare \nsector: a study on privacy and security concerns. ICT Express.\n 31. Ray, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key challenges, \nbias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems, 3(121), \n54.\n 32. Rudin, Cynthia. (2019). Stop explaining black box machine learning models for high stakes deci-\nsions and use interpretable models instead. Nat Mach Intell, 1(206), 215.\n 33. Sarti, G., Feldhus, N., Sickert, L., DerWal, Oskar Van, Nissim, M., & Bisazza, A. (2023). Inseq: an \ninterpretability toolkit for sequence generation models. arXiv preprint arXiv: 2302. 13942.\n 34. Schick, T., Dwivedi-Yu, J., Dessı, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., \nCancedda, N., & Scialom, T. (2024). Toolformer: language models can teach themselves to use \ntools. Advances in Neural Information Processing Systems 36.\n 35. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., & Zhuang, Y. (2024). Hugginggpt: solving ai tasks with \nchatgpt and its friends in hugging face. Advances in Neural Information Processing Systems 36.\n 36. Spirling, Arthur. (2023). Why open-source generative AI models are an ethical way forward for sci-\nence. Nature, 616(7957), 413–413.\n 37. Törnberg, P. (2023a). ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political \nTwitter Messages with Zero-Shot Learning.\n 38. Törnberg, P. (2023b). Chatgpt-4 outperforms experts and crowd workers in annotating political twit-\nter messages with zero-shot learning. arXiv preprint arXiv: 2304. 06588.\n 39. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., \nBhargava, P., & Bhosale, S., et al.( 2023). Llama 2: open foundation and fine-tuned chat models. \narXiv preprint arXiv: 2307. 09288.\n 40. Dis, Van, Eva, A. M., Bollen, Johan, Zuidema, Willem, van Rooij, Robert, & Bockting, Claudi L. \n(2023). Chatgpt: five priorities for research. Nature, 614(7947), 224–226.\n 41. Wang, Z., Wohlwend, J., & Lei, T. (2020). Structured pruning of large language models. In Proceed-\nings of the 2020 conference on empirical methods in natural language processing (emnlp), 6151-\n6162. Online: Association for Computational Linguistics, November. https:// doi. org/ 10. 18653/ v1/ \n2020. emnlp- main. 496.\nJournal of Computational Social Science (2025) 8:17 Page 25 of 25 17\n 42. Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, Adams W., Lester, B., Du, N., Dai, A.M., & Le, Q.V. \n(2022). Finetuned language models are zero-shot learners. arXiv:  2109. 01652 [cs.CL].\n 43. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022). Chain of \nthought prompting elicits reasoning in large language models. arXiv preprint arXiv: 2201. 11903.\n 44. Werra, Leandro von, Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., & Huang, S. \n(2020). Trl: transformer reinforcement learning. https:// github. com/ huggi ngface/ trl.\n 45. Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., Yin, B., & Hu, X. (2023). Harnessing the \npower of llms in practice: a survey on chatgpt and beyond. arXiv preprint arXiv: 2304. 13712.\n 46. Yang, W., Li, C., Zhang, J., & Zong, C. (2023). Bigtrans: augmenting large language models with \nmultilingual translation capability over 100 languages. arXiv preprint arXiv: 2305. 18098.\n 47. Zhang, B., Liu, Z., Cherry, C., & Firat, O. (2024). When scaling meets llm finetuning: the effect of \ndata, model and finetuning method. arXiv preprint arXiv: 2402. 17193.\n 48. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., & Smola, A. (2023). Multimodal chain-of-\nthought reasoning in language models. arXiv preprint arXiv: 2302. 00923.\n 49. Zhu, Y., Zhang, P., Haq, EU., Hui, P., & Tyson, G. (2023). Can ChatGPT Reproduce Human-Gener-\nated Labels? A Study of Social Computing Tasks.\n 50. Ziems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., & Yang, D. (2023). Can large language models \ntransform computational social science? arXiv preprint arXiv: 2305. 03514.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.",
  "topic": "Annotation",
  "concepts": [
    {
      "name": "Annotation",
      "score": 0.7329964637756348
    },
    {
      "name": "Computer science",
      "score": 0.5436659455299377
    },
    {
      "name": "Open source",
      "score": 0.5407069325447083
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5396243929862976
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3147953748703003
    },
    {
      "name": "Geography",
      "score": 0.1218029260635376
    },
    {
      "name": "Cartography",
      "score": 0.07878288626670837
    },
    {
      "name": "Software",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I202697423",
      "name": "University of Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210146419",
      "name": "Institute for Research in Fundamental Sciences",
      "country": "IR"
    },
    {
      "id": "https://openalex.org/I200432940",
      "name": "Allameh Tabataba'i University",
      "country": "IR"
    },
    {
      "id": "https://openalex.org/I67009956",
      "name": "Iran University of Science and Technology",
      "country": "IR"
    }
  ],
  "cited_by": 28
}