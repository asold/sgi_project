{
  "title": "Calibration of Pre-trained Transformers",
  "url": "https://openalex.org/W3012209028",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2129022426",
      "name": "Shrey Desai",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A1978278429",
      "name": "Greg Durrett",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963384319",
    "https://openalex.org/W2963266575",
    "https://openalex.org/W2528491735",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2798984401",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2984218712",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2963508788",
    "https://openalex.org/W2073241381",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2130715829",
    "https://openalex.org/W2600383743",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2144212877",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2616311551",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2918914336",
    "https://openalex.org/W2012672587",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1826524928",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963729324",
    "https://openalex.org/W3090047754",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2047635155",
    "https://openalex.org/W2137556846",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W2963693742",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2158840489"
  ],
  "abstract": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 295–302,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n295\nCalibration of Pre-trained Transformers\nShrey Desai and Greg Durrett\nDepartment of Computer Science\nThe University of Texas at Austin\nshreydesai@utexas.edu gdurrett@cs.utexas.edu\nAbstract\nPre-trained Transformers are now ubiquitous\nin natural language processing, but despite\ntheir high end-task performance, little is\nknown empirically about whether they are cal-\nibrated. Speciﬁcally, do these models’ poste-\nrior probabilities provide an accurate empir-\nical measure of how likely the model is to\nbe correct on a given example? We focus\non BERT (Devlin et al., 2019) and RoBERTa\n(Liu et al., 2019) in this work, and analyze\ntheir calibration across three tasks: natural\nlanguage inference, paraphrase detection, and\ncommonsense reasoning. For each task, we\nconsider in-domain as well as challenging out-\nof-domain settings, where models face more\nexamples they should be uncertain about. We\nshow that: (1) when used out-of-the-box, pre-\ntrained models are calibrated in-domain, and\ncompared to baselines, their calibration error\nout-of-domain can be as much as 3.5 ×lower;\n(2) temperature scaling is effective at further\nreducing calibration error in-domain, and us-\ning label smoothing to deliberately increase\nempirical uncertainty helps calibrate posteri-\nors out-of-domain.1\n1 Introduction\nNeural networks have seen wide adoption but are\nfrequently criticized for being black boxes, offer-\ning little insight as to why predictions are made\n(Ben´ıtez et al., 1997; Dayhoff and DeLeo, 2001;\nCastelvecchi, 2016) and making it difﬁcult to di-\nagnose errors at test-time. These properties are\nparticularly exhibited by pre-trained Transformer\nmodels (Devlin et al., 2019; Liu et al., 2019; Yang\net al., 2019), which dominate benchmark tasks like\nSuperGLUE (Wang et al., 2019), but use a large\nnumber of self-attention heads across many layers\nin a way that is difﬁcult to unpack (Clark et al.,\n1Code and datasets available at https://github.\ncom/shreydesai/calibration\n2019; Kovaleva et al., 2019). One step towards un-\nderstanding whether these models can be trusted is\nby analyzing whether they are calibrated (Raftery\net al., 2005; Jiang et al., 2012; Kendall and Gal,\n2017): how aligned their posterior probabilities are\nwith empirical likelihoods (Brier, 1950; Guo et al.,\n2017). If a model assigns 70% probability to an\nevent, the event should occur 70% of the time if the\nmodel is calibrated. Although the model’s mech-\nanism itself may be uninterpretable, a calibrated\nmodel at least gives us a signal that it “knows what\nit doesn’t know,” which can make these models\neasier to deploy in practice (Jiang et al., 2012).\nIn this work, we evaluate the calibration of two\npre-trained models, BERT (Devlin et al., 2019)\nand RoBERTa (Liu et al., 2019), on three tasks:\nnatural language inference (Bowman et al., 2015),\nparaphrase detection (Iyer et al., 2017), and com-\nmonsense reasoning (Zellers et al., 2018). These\ntasks represent standard evaluation settings for pre-\ntrained models, and critically, challenging out-of-\ndomain test datasets are available for each. Such\ntest data allows us to measure calibration in more\nrealistic settings where samples stem from a dis-\nsimilar input distribution, which is exactly the sce-\nnario where we hope a well-calibrated model would\navoid making conﬁdent yet incorrect predictions.\nOur experiments yield several key results. First,\neven when used out-of-the-box, pre-trained models\nare calibrated in-domain. In out-of-domain settings,\nwhere non-pre-trained models like ESIM (Chen\net al., 2017) are overconﬁdent, we ﬁnd that pre-\ntrained models are signiﬁcantly better calibrated.\nSecond, we show that temperature scaling (Guo\net al., 2017), multiplying non-normalized logits by\na single scalar hyperparameter, is widely effective\nat improving in-domain calibration. Finally, we\nshow that regularizing the model to be less certain\nduring training can beneﬁcially smooth probabili-\nties, improving out-of-domain calibration.\n296\n2 Related Work\nCalibration has been well-studied in statistical ma-\nchine learning, including applications in forecast-\ning (Brier, 1950; Raftery et al., 2005; Gneiting\net al., 2007; Palmer et al., 2008), medicine (Yang\nand Thompson, 2010; Jiang et al., 2012), and com-\nputer vision (Kendall and Gal, 2017; Guo et al.,\n2017; Lee et al., 2018). Past work in natural lan-\nguage processing has studied calibration in the non-\nneural (Nguyen and O’Connor, 2015) and neural\n(Kumar and Sarawagi, 2019) settings across sev-\neral tasks. However, past work has not analyzed\nlarge-scale pre-trained models, and we additionally\nanalyze out-of-domain settings, whereas past work\nlargely focuses on in-domain calibration (Nguyen\nand O’Connor, 2015; Guo et al., 2017).\nAnother way of hardening models against out-\nof-domain data is to be able to explicitly detect\nthese examples, which has been studied previously\n(Hendrycks and Gimpel, 2016; Liang et al., 2018;\nLee et al., 2018). However, this assumes a discrete\nnotion of domain; calibration is a more general\nparadigm and gracefully handles settings where\ndomains are less quantized.\n3 Posterior Calibration\nA model is calibrated if the conﬁdence estimates\nof its predictions are aligned with empirical likeli-\nhoods. For example, if we take 100 samples where\na model’s prediction receives posterior probability\n0.7, the model should get 70 of the samples correct.\nFormally, calibration is expressed as a joint distribu-\ntion P(Q,Y ) over conﬁdences Q∈R and labels\nY ∈Y, where perfect calibration is achieved when\nP(Y = y|Q= q) =q. This probability can be em-\npirically approximated by binning predictions into\nkdisjoint, equally-sized bins, each consisting of bk\npredictions. Following previous work in measur-\ning calibration (Guo et al., 2017), we use expected\ncalibration error(ECE), which is a weighted aver-\nage of the difference between each bin’s accuracy\nand conﬁdence: ∑\nk\nbk\nn|acc(k) −conf(k)|. For the\nexperiments in this paper, we use k= 10.\n4 Experiments\n4.1 Tasks and Datasets\nWe perform evaluations on three language under-\nstanding tasks: natural language inference, para-\nphrase detection, and commonsense reasoning. Sig-\nniﬁcant past work has studied cross-domain robust-\nModel Parameters Architecture Pre-trained\nDA 382K LSTM \u0017\nESIM 4M Bi-LSTM \u0017\nBERT 110M Transformer \u0013\nRoBERTa 110M Transformer \u0013\nTable 1: Models in this work. Decomposable Atten-\ntion (DA) (Parikh et al., 2016) and Enhanced Sequen-\ntial Inference Model (ESIM) (Chen et al., 2017) use\nLSTMs and attention on top of GloVe embeddings\n(Pennington et al., 2014) to model pairwise semantic\nsimilarities. In contrast, BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019) are large-scale, pre-trained\nlanguage models with stacked, general purpose Trans-\nformer (Vaswani et al., 2017) layers.\nness using sentiment analysis (Chen et al., 2018;\nPeng et al., 2018; Miller, 2019; Desai et al., 2019).\nHowever, we explicitly elect to use tasks where\nout-of-domain performance is substantially lower\nand challenging domain shifts are exhibited. Be-\nlow, we describe our in-domain and out-of-domain\ndatasets.2 For all datasets, we split the development\nset in half to obtain a held-out, non-blind test set.\nNatural Language Inference. The Stanford\nNatural Language Inference (SNLI) corpus is a\nlarge-scale entailment dataset where the task is to\ndetermine whether a hypothesis is entailed, con-\ntradicted by, or neutral with respect to a premise\n(Bowman et al., 2015). Multi-Genre Natural Lan-\nguage Inference (MNLI) (Williams et al., 2018)\ncontains similar entailment data across several do-\nmains, which we can use as unseen test domains.\nParaphrase Detection. Quora Question Pairs\n(QQP) contains sentence pairs from Quora that are\nsemantically equivalent (Iyer et al., 2017). Our out-\nof-domain setting is TwitterPPDB (TPPDB), which\ncontains sentence pairs from Twitter where tweets\nare considered paraphrases if they have shared\nURLs (Lan et al., 2017).\nCommonsense Reasoning. Situations With Ad-\nversarial Generations (SW AG) is a grounded com-\nmonsense reasoning task where models must se-\nlect the most plausible continuation of a sentence\namong four candidates (Zellers et al., 2018). Hel-\nlaSW AG (HSW AG), an adversarial out-of-domain\ndataset, serves as a more challenging benchmark\nfor pre-trained models (Zellers et al., 2019); it is\n2Dataset splits are detailed in Appendix A. Furthermore,\nout-of-domain datasets are strictly used for evaluating the\ngeneralization of in-domain models, so the training split is\nunused.\n297\nModel Accuracy ECE\nID OD ID OD\nTask: SNLI/MNLI\nDA 84.63 57.12 1.02 8.79\nESIM 88.32 60.91 1.33 12.78\nBERT 90.04 73.52 2.54 7.03\nRoBERTa 91.23 78.79 1.93 3.62\nTask: QQP/TwitterPPDB\nDA 85.85 83.36 3.37 9.79\nESIM 87.75 84.00 3.65 8.38\nBERT 90.27 87.63 2.71 8.51\nRoBERTa 91.11 86.72 2.33 9.55\nTask: SWAG/HellaSWAG\nDA 46.80 32.48 5.98 40.37\nESIM 52.09 32.08 7.01 19.57\nBERT 79.40 34.48 2.49 12.62\nRoBERTa 82.45 41.68 1.76 11.93\nTable 2: Out-of-the-box calibration results for in-\ndomain (SNLI, QQP, SW AG) and out-of-domain\n(MNLI, TwitterPPDB, HellaSW AG) datasets using the\nmodels described in Table 1. We report accuracy and\nexpected calibration error (ECE), both averaged across\n5 ﬁne-tuning runs with random restarts.\ndistributionally different in that its examples ex-\nploit statistical biases in pre-trained models.\n4.2 Systems for Comparison\nTable 1 shows a breakdown of the models used in\nour experiments. We use the same set of hyper-\nparameters across all tasks. For pre-trained mod-\nels, we omit hyperparameters that induce brittle-\nness during ﬁne-tuning, e.g., employing a decaying\nlearning rate schedule with linear warmup (Sun\net al., 2019; Lan et al., 2020). Detailed information\non optimization is available in Appendix B.\n4.3 Out-of-the-box Calibration\nFirst, we analyze “out-of-the-box” calibration; that\nis, the calibration error derived from evaluating a\nmodel on a dataset without using post-processing\nsteps like temperature scaling (Guo et al., 2017).\nFor each task, we train the model on the in-domain\ntraining set, and then evaluate its performance on\nthe in-domain and out-of-domain test sets. Quanti-\ntative results are shown in Table 2. In addition, we\nplot reliability diagrams (Nguyen and O’Connor,\n2015; Guo et al., 2017) in Figure 1, which visualize\nthe alignment between posterior probabilities (con-\nﬁdence) and empirical outcomes (accuracy), where\na perfectly calibrated model has conf(k) =acc(k)\nfor each bucket of real-valued predictions k. We\nFigure 1: In-domain calibration of BERT and\nRoBERTa when used out-of-the-box. Models are both\ntrained and evaluated on SNLI, QQP, and SW AG, re-\nspectively. Z ERO ERROR depicts perfect calibration\n(e.g., expected calibration error = 0). Note that low-\nconﬁdence buckets have zero accuracy due to a small\nsample count; however, as a result, these buckets do\nnot inﬂuence the expected error as much.\nremark on a few observed phenomena below:\nNon-pre-trained models exhibit an inverse re-\nlationship between complexity and calibration.\nSimpler models, such as DA, achieve competitive\nin-domain ECE on SNLI (1.02) and QQP (3.37),\nand are notably better than pre-trained models on\nSNLI in this regard. However, the more complex\nESIM, both in number of parameters and architec-\nture, sees increased in-domain ECE despite having\nhigher accuracy on all tasks.\nHowever, pre-trained models are generally\nmore accurate and calibrated. Rather surpris-\ningly, pre-trained models do not show character-\nistics of the aforementioned inverse relationship,\ndespite having signiﬁcantly more parameters. On\nSNLI, RoBERTa achieves an ECE in the ballpark\nof DA and ESIM, but on QQP and SWAG, both\n298\nBERT and RoBERTa consistently achieve higher\naccuracies and lower ECEs. Pre-trained models\nare especially strong out-of-domain, where on Hel-\nlaSWAG in particular, RoBERTa reduces ECE by\na factor of 3.4 compared to DA.\nUsing RoBERTa always improves in-domain\ncalibration over BERT. In addition to obtain-\ning better task performance than BERT, RoBERTa\nconsistently achieves lower in-domain ECE. Even\nout-of-domain, RoBERTa outperforms BERT in all\nbut one setting (TwitterPPDB). Nonetheless, our\nresults show that representations induced by ro-\nbust pre-training (e.g., using a larger corpus, more\ntraining steps, dynamic masking) (Liu et al., 2019)\nlead to more calibrated posteriors. Whether other\nchanges to pre-training (Yang et al., 2019; Lan\net al., 2020; Clark et al., 2020) lead to further im-\nprovements is an open question.\n4.4 Post-hoc Calibration\nThere are a number of techniques that can be ap-\nplied to correct a model’s calibration post-hoc. Us-\ning our in-domain development set, we can, for ex-\nample, post-process model probabilities via temper-\nature scaling (Guo et al., 2017), where a scalar tem-\nperature hyperparameter T divides non-normalized\nlogits before the softmax operation. As T →0,\nthe distribution’s mode receives all the probability\nmass, while as T →∞, the probabilities become\nuniform.\nFurthermore, we experiment with models trained\nin-domain with label smoothing (LS) (Miller et al.,\n1996; Pereyra et al., 2017) as opposed to conven-\ntional maximum likelihood estimation (MLE). By\nnature, MLE encourages models to sharpen the\nposterior distribution around the gold label, lead-\ning to conﬁdence which is typically unwarranted in\nout-of-domain settings. Label smoothing presents\none solution to overconﬁdence by maintaining un-\ncertainty over the label space during training: we\nminimize the KL divergence with the distribution\nplacing a 1 −αfraction of probability mass on the\ngold label and α\n|Y|−1 fraction of mass on each other\nlabel, where α∈(0,1) is a hyperparameter.3 This\nre-formulated learning objective does not require\nchanging the model architecture.\nFor each task, we train the model with either\nMLE or LS (α= 0.1) using the in-domain training\nset, use the in-domain development set to learn\n3For example, the one-hot target [1, 0, 0] is transformed\ninto [0.9, 0.05, 0.05] when α= 0.1.\nFigure 2: In-domain calibration of BERT and\nRoBERTa with temperature scaling (TS). Both\ntemperature-scaled models are much better calibrated\nthan when used out-of-the-box, with BERT especially\nshowing a large degree of improvement.\nan optimal temperature T, and then evaluate the\nmodel (scaled with T) on the in-domain and out-\nof-domain test sets. From Table 3 and Figure 2, we\ndraw the following conclusions:\nMLE models with temperature scaling achieve\nlow in-domain calibration error.MLE models\nare always better than LS models in-domain, which\nsuggests incorporating uncertainty when in-domain\nsamples are available is not an effective regulariza-\ntion scheme. Even when using a small smoothing\nvalue (0.1), LS models do not achieve nearly as\ngood out-of-the-box results as MLE models, and\ntemperature scaling hurts LS in many cases. By\ncontrast, RoBERTa with temperature-scaled MLE\nachieves ECE values from 0.7-0.8, implying that\nMLE training yields scores that are fundamentally\ngood but just need some minor rescaling.\nHowever, out-of-domain, label smoothing is\ngenerally more effective. In most cases, MLE\nmodels do not perform well on out-of-domain\n299\nMethod\nIn-Domain Out-of-Domain\nSNLI QQP SW AG MNLI TPPDB HSW AG\nMLE LS MLE LS MLE LS MLE LS MLE LS MLE LS\nModel: BERT\nOut-of-the-box 2.54 7.12 2.71 6.33 2.49 10.01 7.03 3.74 8.51 6.30 12.62 5.73\nTemperature scaled 1.14 8.37 0.97 8.16 0.85 10.89 3.61 4.05 7.15 5.78 12.83 5.34\nModel: RoBERTa\nOut-of-the-box 1.93 6.38 2.33 6.11 1.76 8.81 3.62 4.50 9.55 8.91 11.93 2.14\nTemperature scaled 0.84 8.70 0.88 8.69 0.76 11.4 1.46 5.93 7.86 5.31 11.22 2.23\nTable 3: Post-hoc calibration results for BERT and RoBERTa on in-domain (SNLI, QQP, SW AG) and out-of-\ndomain (MNLI, TwitterPPDB, HellaSW AG) datasets. Models are trained with maximum likelihood estimation\n(MLE) or label smoothing (LS), then their logits are post-processed using temperature scaling (§4.4). We report\nexpected calibration error (ECE) averaged across 5 runs with random restarts. Darker colors imply lower ECE.\nFigure 3: Out-of-domain calibration of RoBERTa ﬁne-\ntuned on SW AG with different learning objectives and\nused out-of-the-box on HellaSW AG. Without seeing\nHellaSW AG samples during ﬁne-tuning, RoBERTa-\nLS achieves signiﬁcantly lower calibration error than\nRoBERTa-MLE.\ndatasets, with ECEs ranging from 8-12. How-\never, LS models are forced to distribute probability\nmass across classes, and as a result, achieve signiﬁ-\ncantly lower ECEs on average. We note that LS is\nparticularly effective when the distribution shift is\nstrong. On the adversarial HellaSWAG, for exam-\nple, RoBERTa-LS obtains a factor of 5.8 less ECE\nthan RoBERTa-MLE. This phenomenon is visually\ndepicted in Figure 3 where we see RoBERTa-LS is\nsigniﬁcantly closer to the identity function despite\nbeing used out-of-the-box.\nOptimal temperature scaling values are\nbounded within a small interval. Table 4\nreports the learned temperature values for BERT-\nMLE and RoBERTa-MLE. For in-domain tasks,\nthe optimal temperature values are generally in\nModel In-Domain Out-of-Domain\nSNLI QQP SW AG MNLI TPPDB HSW AG\nBERT 1.20 1.34 0.99 1.41 2.91 3.61\nRoBERTa 1.16 1.39 1.10 1.25 2.79 2.77\nTable 4: Learned temperature scaling values for\nBERT and RoBERTa on in-domain (SNLI, QQP,\nSW AG) and out-of-domain (MNLI, TwitterPPDB, Hel-\nlaSW AG) datasets. Values are obtained by line search\nwith a granularity of 0.01. Evaluations are very fast as\nthey only require rescaling cached logits.\nthe range 1-1.4. Interestingly, out-of-domain,\nTwitterPPDB and HellaSWAG require larger\ntemperature values than MNLI, which suggests the\ndegree of distribution shift and magnitude of T\nmay be closely related.\n5 Conclusion\nPosterior calibration is one lens to understand the\ntrustworthiness of model conﬁdence scores. In this\nwork, we examine the calibration of pre-trained\nTransformers in both in-domain and out-of-domain\nsettings. Results show BERT and RoBERTa cou-\npled with temperature scaling achieve low ECEs\nin-domain, and when trained with label smoothing,\nare also competitive out-of-domain.\nAcknowledgments\nThis work was partially supported by NSF Grant\nIIS-1814522 and a gift from Arm. The authors\nacknowledge a DURIP equipment grant to UT\nAustin that provided computational resources to\nconduct this research. Additionally, we thank R.\nThomas McCoy for answering questions about DA\nand ESIM.\n300\nReferences\nJose M. Ben ´ıtez, Juan Luis Castro, and Ignacio Re-\nquena. 1997. Are Artiﬁcial Neural Networks Black\nBoxes? IEEE Transactions on Neural Networks and\nLearning Systems.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A Large An-\nnotated Corpus for Learning Natural Language In-\nference. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nGlenn W. Brier. 1950. Veriﬁcation of Forecasts Ex-\npressed in Terms of Probability. Monthly Weather\nReview.\nDavide Castelvecchi. 2016. Can We Open the Black\nBox of AI? Nature News.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM\nfor Natural Language Inference. In Proceedings of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nXilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,\nand Kilian Weinberger. 2018. Adversarial Deep Av-\neraging Networks for Cross-Lingual Sentiment Clas-\nsiﬁcation. Transactions of the Association for Com-\nputational Linguistics (TACL).\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What Does BERT\nLook at? An Analysis of BERT’s Attention. In Pro-\nceedings of the Workshop on BlackboxNLP.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather\nThan Generators. In Proceedings of the Inter-\nnational Conference on Learning Representations\n(ICLR).\nJudith E. Dayhoff and James M. DeLeo. 2001. Artiﬁ-\ncial Neural Networks: Opening the Black Box. Can-\ncer: Interdisciplinary International Journal of the\nAmerican Cancer Society.\nShrey Desai, Hongyuan Zhan, and Ahmed Aly.\n2019. Evaluating Lottery Tickets Under Distribu-\ntional Shifts. In Proceedings of the Workshop on\nDeep Learning Approaches for Low-Resource NLP\n(DeepLo).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL).\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A Deep Semantic Natural Language Pro-\ncessing Platform. In Proceedings of the Workshop\nfor NLP Open Source Software (NLP-OSS).\nTilmann Gneiting, Fadoua Balabdaoui, and Adrian E.\nRaftery. 2007. Probabilistic Forecasts, Calibration\nand Sharpness. Journal of the Royal Statistical Soci-\nety: Series B (Statistical Methodology).\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On Calibration of Modern Neural Net-\nworks. In Proceedings of the International Confer-\nence on Machine Learning (ICML).\nDan Hendrycks and Kevin Gimpel. 2016. A Baseline\nfor Detecting Misclassiﬁed and Out-of-Distribution\nExamples in Neural Networks. In Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR).\nShankar Iyer, Nikhil Dandekar, and Korn ´el Csernai.\n2017. Quora Question Pairs.\nXiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lu-\ncila Ohno-Machado. 2012. Calibrating Predictive\nModel Estimates to Support Personalized Medicine.\nIn Journal of the American Medical Informatics As-\nsociation (JAMIA).\nAlex Kendall and Yarin Gal. 2017. What Uncertainties\nDo We Need in Bayesian Deep Learning for Com-\nputer Vision? In Proceedings of the Conference on\nNeural Information Processing Systems (NeurIPS).\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the Dark Se-\ncrets of BERT. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nAviral Kumar and Sunita Sarawagi. 2019. Calibration\nof Encoder Decoder Models for Neural Machine\nTranslation. In Proceedings of the Workshop on De-\nbugging Machine Learning Models.\nWuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. A\nContinuously Growing Dataset of Sentential Para-\nphrases. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Proceed-\nings of the International Conference on Learning\nRepresentations (ICLR).\nKimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin.\n2018. Training Conﬁdence-calibrated Classiﬁers for\nDetecting Out-of-Distribution Samples. In Proceed-\nings of the International Conference on Learning\nRepresentations (ICLR).\n301\nShiyu Liang, Yixuan Li, and R. Srikant. 2018. Enhanc-\ning the Reliability of Out-of-distribution Image De-\ntection in Neural Networks. In Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization. In Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nDavid J. Miller, Ajit V . Rao, Kenneth Rose, and Allen\nGersho. 1996. A Global Optimization Technique for\nStatistical Classiﬁer Design. IEEE Transactions on\nSignal Processing.\nTimothy Miller. 2019. Simpliﬁed Neural Unsupervised\nDomain Adaptation. In Proceedings of the Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL).\nKhanh Nguyen and Brendan O’Connor. 2015. Poste-\nrior Calibration and Exploratory Analysis for Nat-\nural Language Processing Models. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nTim Palmer, Francisco Doblas-Reyes, Antje\nWeisheimer, and Mark Rodwell. 2008. Toward\nSeamless Prediction: Calibration of Climate Change\nProjections using Seasonal Forecasts. Bulletin of\nthe American Meteorological Society.\nAnkur Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A Decomposable Attention\nModel for Natural Language Inference. In Proceed-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nMinlong Peng, Qi Zhang, Yu gang Jiang, and Xuanjing\nHuang. 2018. Cross-Domain Sentiment Classiﬁca-\ntion with Target Domain Speciﬁc Information. In\nProceedings of the Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. GloVe: Global Vectors for Word\nRepresentation. In Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP).\nGabriel Pereyra, George Tucker, Jan Chorowski,\nŁukasz Kaiser, and Geoffrey Hinton. 2017. Regu-\nlarizing Neural Networks by Penalizing Conﬁdent\nOutput Distributions. In Proceedings of the Inter-\nnational Conference on Learning Representations\n(Workshop).\nAdrian E. Raftery, Tilmann Gneiting, Fadoua Bal-\nabdaoui, and Michael Polakowski. 2005. Using\nBayesian Model Averaging to Calibrate Forecast En-\nsembles. Monthly Weather Review.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to Fine-Tune BERT for Text Classiﬁca-\ntion? arXiv preprint arXiv:1905.05583.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Proceedings of the Conference on Neu-\nral Information Processing Systems (NeurIPS).\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. SuperGLUE:\nA Stickier Benchmark for General-Purpose Lan-\nguage Understanding Systems. In Proceedings of\nthe Conference on Neural Information Processing\nSystems (NeurIPS).\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A Broad-Coverage Challenge Corpus\nfor Sentence Understanding through Inference. In\nProceedings of the Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics (NAACL).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. arXiv preprint arXiv:1910.03771.\nHuiqin Yang and Carl Thompson. 2010. Nurses’ Risk\nAssessment Judgements: A Conﬁdence Calibration\nStudy. Journal of Advanced Nursing.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R. Salakhutdinov, and Quoc V . Le.\n2019. XLNet: Generalized Autoregressive Pretrain-\ning for Language Understanding. In Proceedings\nof the Conference on Neural Information Processing\nSystems (NeurIPS).\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. SW AG: A Large-Scale Adversarial\nDataset for Grounded Commonsense Inference. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSW AG: Can\na Machine Really Finish Your Sentence? In Pro-\nceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL).\n302\nA Dataset Splits\nDataset splits are shown in Table 5.\nDataset Train Dev Test\nSNLI 549,368 4,922 4,923\nMNLI 392,702 4,908 4,907\nQQP 363,871 20,216 20,217\nTwitterPPDB 46,667 5,060 5,060\nSW AG 73,547 10,004 10,004\nHellaSW AG 39,905 5,021 5,021\nTable 5: Training, development, and test dataset sizes\nfor SNLI (Bowman et al., 2015), MNLI (Williams et al.,\n2018), QQP (Iyer et al., 2017), TwitterPPDB (Lan et al.,\n2017), SW AG (Zellers et al., 2018), and HellaSW AG\n(Zellers et al., 2019).\nB Training and Optimization\nFor non-pre-trained model baselines, we use the\nopen-source implementations of DA (Parikh et al.,\n2016) and ESIM (Chen et al., 2017) in AllenNLP\n(Gardner et al., 2018), except in the case of\nSWAG/HellaSWAG, where we run the baselines\navailable in the authors’ code.4 For BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019), we\nuse bert-base-uncased and roberta-base, re-\nspectively, from HuggingFace Transformers (Wolf\net al., 2019). BERT is ﬁne-tuned with a maximum\nof 3 epochs, batch size of 16, learning rate of 2e-5,\ngradient clip of 1.0, and no weight decay. Simi-\nlarly, RoBERTa is ﬁne-tuned with a maximum of\n3 epochs, batch size of 32, learning rate of 1e-5,\ngradient clip of 1.0, and weight decay of 0.1. Both\nmodels are optimized with AdamW (Loshchilov\nand Hutter, 2019). Other than early stopping on\nthe development set, we do not perform additional\nhyperparameter searches. Finally, all experiments\nare conducted on NVIDIA V100 32GB GPUs, with\nthe total time for ﬁne-tuning all models being under\n24 hours.\nFurthermore, temperature scaling line searches\nare performed in the range [0.01, 5.0] with a gran-\nularity of 0.01. These searches are quite fast and\ncan be performed on a CPU; we simply evaluate\ncalibration error by rescaling cached logits. On a\nIntel Xeon E3-1270 v3 CPU, all searches can be\ncompleted in under 15 minutes.\nC Reproducibility\nTable 6 shows the accuracy and expected calibra-\ntion error (ECE) of BERT and RoBERTa on the\n4https://github.com/rowanz/swagaf\nModel Accuracy ECE\nID OD ID OD\nTask: SNLI/MNLI\nBERT 90.18 74.04 3.43 8.18\nRoBERTa 91.20 79.17 1.18 1.41\nTask: QQP/TwitterPPDB\nBERT 90.22 86.02 4.68 11.30\nRoBERTa 89.97 86.17 3.09 9.57\nTask: SWAG/HellaSWAG\nBERT 78.82 38.01 2.51 2.24\nRoBERTa 81.85 59.03 3.02 5.71\nTable 6: Out-of-the-box calibration development setre-\nsults for in-domain (SNLI, QQP, SW AG) and out-of-\ndomain (MNLI, TwitterPPDB, HellaSW AG) datasets\nusing pre-trained models.\ndevelopment sets of the datasets we consider. We\ndo not report post-hoc calibration results using the\ndevelopment set since these require tuning on the\ndevelopment set itself.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8054215312004089
    },
    {
      "name": "Inference",
      "score": 0.7008967995643616
    },
    {
      "name": "Calibration",
      "score": 0.6218410134315491
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6038946509361267
    },
    {
      "name": "Machine learning",
      "score": 0.5023417472839355
    },
    {
      "name": "Transformer",
      "score": 0.4903087913990021
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.48812755942344666
    },
    {
      "name": "Smoothing",
      "score": 0.4831194579601288
    },
    {
      "name": "Task (project management)",
      "score": 0.4538194239139557
    },
    {
      "name": "Paraphrase",
      "score": 0.41372233629226685
    },
    {
      "name": "Natural language processing",
      "score": 0.3727908134460449
    },
    {
      "name": "Statistics",
      "score": 0.15115952491760254
    },
    {
      "name": "Computer vision",
      "score": 0.14183756709098816
    },
    {
      "name": "Mathematics",
      "score": 0.1058841347694397
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 9
}