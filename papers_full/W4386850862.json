{
  "title": "Evaluating the Efficacy of Supervised Learning vs. Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media",
  "url": "https://openalex.org/W4386850862",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2129459229",
      "name": "Guang-Hui Fu",
      "affiliations": [
        "Sorbonne University Abu Dhabi"
      ]
    },
    {
      "id": "https://openalex.org/A2104650034",
      "name": "Hongzhi Qi",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095722350",
      "name": "Qing Zhao",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2123639189",
      "name": "Changwei Song",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1995163484",
      "name": "Wei Zhai",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106992842",
      "name": "Dan Luo",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A1872988133",
      "name": "Liu Shuo",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A4380556227",
      "name": "Yi-Jing Yu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2080195193",
      "name": "Fan Wang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2263872999",
      "name": "Huijing Zou",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2756905266",
      "name": "Bing-Xiang Yang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2122459095",
      "name": "Jianqiang Li",
      "affiliations": [
        "Beijing University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2916226351",
    "https://openalex.org/W2924664363",
    "https://openalex.org/W1922396948",
    "https://openalex.org/W2165343833",
    "https://openalex.org/W2889391310",
    "https://openalex.org/W3196324893",
    "https://openalex.org/W3032735579",
    "https://openalex.org/W3153411045",
    "https://openalex.org/W3021738738",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W6854308750",
    "https://openalex.org/W3089862415",
    "https://openalex.org/W3193582868",
    "https://openalex.org/W3139172628",
    "https://openalex.org/W2969739743",
    "https://openalex.org/W3037313435",
    "https://openalex.org/W3094709634",
    "https://openalex.org/W4384920109",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W4319301446",
    "https://openalex.org/W4313447794",
    "https://openalex.org/W6850135255",
    "https://openalex.org/W4318464200",
    "https://openalex.org/W4319455199",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W4384926409",
    "https://openalex.org/W4376122720",
    "https://openalex.org/W4378464713",
    "https://openalex.org/W4386302642",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W6854692045",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2971092323",
    "https://openalex.org/W6846002521",
    "https://openalex.org/W4226364033",
    "https://openalex.org/W4324308091",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4322631505",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4253506849",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4385373745",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4387356888"
  ],
  "abstract": "Abstract Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tuning. Our findings revealed a discernible performance gap between the large language models and traditional supervised learning approaches, primarily attributed to the models' inability to fully grasp subtle categories. Notably, while GPT-4 outperforms its counterparts in multiple scenarios, GPT-3.5 shows significant enhancement in suicide risk classification after fine-tuning. To our knowledge, this investigation stands as the maiden attempt at gauging large language models on Chinese social media tasks. This study underscores the forward-looking and transformative implications of using large language models in the field of psychology. It lays the groundwork for future applications in psychological research and practice.",
  "full_text": "Evaluating the E\u0000cacy of Supervised Learning vs.\nLarge Language Models for Identifying Cognitive\nDistortions and Suicidal Risks in Chinese Social\nMedia\nGuanghui FU  (  aslanfu123@gmail.com )\nSorbonne university\nHongzhi Qi \nFaculty of Information Technology, Beijing University of Technology, Beijing, 100124, China\nQing Zhao \nBeijing University of Technology\nChangwei Song \nBeijing University of Technology\nWei Zhai \nFaculty of Information Technology, Beijing University of Technology, Beijing, 100124, China\nDan Luo \nWuhan University\nLiu Shuo \nWuhan University\nYi Jing Yu \nCenter for Wise Information Technology of Mental Health Nursing Research, School of Nursing, Wuhan\nUniversity, Wuhan, China\nFan Wang \nWuhan University\nHuijing Zou \nCenter for Wise Information Technology of Mental Health Nursing Research, School of Nursing, Wuhan\nUniversity, Wuhan, China\nBing Xiang Yang \nWuhan University\nJianqiang Li \nBeijing University of Technology\nArticle\nKeywords: Large language model, Deep learning, Mental health, Social media\nPosted Date: September 20th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3355484/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nEvaluating the Eﬃcacy of Supervised Learning vs. Large\nLanguage Models for Identifying Cognitive Distortions and\nSuicidal Risks in Chinese Social Media\nHongzhi Qi a, Qing Zhao a, Changwei Song a, Wei Zhai a, Dan Luo b, Shuo Liu b, Yi Jing Yu b, Fan\nWangb, Huijing Zou b, Bing Xiang Yang b, Jianqiang Li a, and Guanghui Fu *c\naFaculty of Information Technology, Beijing University of Te chnology, Beijing, 100124, China\nbCenter for Wise Information Technology of Mental Health Nursin g Research, School of\nNursing, Wuhan University, Wuhan, China\ncSorbonne Universit´ e, Institut du Cerveau - Paris Brain Inst itute - ICM, CNRS, Inria, Inserm,\nAP-HP, Hˆ opital de la Piti´ e Salpˆ etri` ere, Paris, France\nABSTRACT\nLarge language models, particularly those akin to the rapidly progressing G PT series, are gaining traction\nfor their expansive inﬂuence. While there is keen interest in t heir applicability within medical domains such\nas psychology, tangible explorations on real-world data remain scant. Concur rently, users on social media\nplatforms are increasingly vocalizing personal sentiments; under s peciﬁc thematic umbrellas, these sentiments\noften manifest as negative emotions, sometimes escalating to suicidal i nclinations. Timely discernment of such\ncognitive distortions and suicidal risks is crucial to eﬀectively i ntervene and potentially avert dire circumstances.\nOur study ventured into this realm by experimenting on two pivot al tasks: suicidal risk and cognitive distortion\nidentiﬁcation on Chinese social media platforms. Using supervised learning as a baseline, we examined and\ncontrasted the eﬃcacy of large language models via three distinct strate gies: zero-shot, few-shot, and ﬁne-\ntuning. Our ﬁndings revealed a discernible performance gap between the large language models and traditional\nsupervised learning approaches, primarily attributed to the mode ls’ inability to fully grasp subtle categories.\nNotably, while GPT-4 outperforms its counterparts in multiple scen arios, GPT-3.5 shows signiﬁcant enhancement\nin suicide risk classiﬁcation after ﬁne-tuning. To our knowledge, t his investigation stands as the maiden attempt\nat gauging large language models on Chinese social media tasks. This study u nderscores the forward-looking\nand transformative implications of using large language models in the ﬁel d of psychology. It lays the groundwork\nfor future applications in psychological research and practice.\nKeywords: Large language model, Deep learning, Mental health, Social media\n1. INTRODUCTION\nThe omnipresent specter of mental illness, particularly depress ion, continues to impose signiﬁcant challenges\nglobally.\n1 According to the World Health Organization (WHO), an estimated 3.8% of the global population\nexperiences depression. 1 Speciﬁcally in China, the prevalence of depression is notably high, w ith estimates\naround 6.9%, 2 underscoring the escalating mental health concerns in the nation. Su ch severe depression can\noften precipitate suicidal behaviors. 3 As digital avenues for communication ﬂourish, social media platforms li ke\nTwitter and Sina Weibo have evolved into reﬂective mirrors, oﬀer ing glimpses into the emotional landscapes of\ncountless users. 4 Within these platforms, a speciﬁc subset of topics recurrently s urfaces, with users frequently\nconveying deep-seated negative emotions and, alarmingly, pronounced su icidal inclinations. 5, 6\nArtiﬁcial intelligence (AI), especially the branch underscored by deep learning and natural language processing\ntechnique, is an avenue that holds promise in addressing this chal lenge.7 Over recent years, AI research has\nresulted in the formulation of a plethora of algorithms tailored for emotion recognition within textual data. 8\nHowever, these advancements are not without obstacles. 9 Constructing a potent deep learning model often\nFurther author information: (Send correspondence to Guanghui Fu , guanghui.fu@inria.fr)\n1\ndemands considerable time and ﬁnancial resources. The intricacies of data labeling, predominantly the need to\nenlist domain experts and the model’s variance in performance when s hifted across diﬀerent application areas,\nhighlight pressing challenges. 10 This underscores a compelling need for more agile and adaptable algorithmi c\nsolutions especially in medical domain. 11 It is in this context that the emergence and proliferation of large\nlanguage models are particularly noteworthy.\nLarge language models, characterized by their expansive parameters and th e depth of their training datasets,\nstand as vanguards in the realm of computational linguistics. 12 Their prowess lies in their ability to comprehend\nand emulate human-like text nuances. Despite their promising pote ntial, several studies have sought to validate\ntheir practical implications. For instance, Xu et al. 13 examined four public datasets related to online social media\nsentiment detection. However, their study focused solely on Engl ish data, and the classiﬁcation granularity was\nrelatively broad. To date, there is a notable gap in research concerning the Chinese context, particularly in\nthe area of ﬁne-grained emotion recognition, which is often of greater signi ﬁcance. The lack of comprehensive\nevaluations and practical tests has inadvertently led to a cautious appr oach, especially in sectors demanding high\nreliability, like medicine and healthcare. 14 Motivated by these challenges and opportunities, our research embarks\non a nuanced exploration of two psychology-related tasks using data from Ch inese social media platforms. We\noﬀer the following contributions:\n• To our best knowledge, our research presents the ﬁrst time a detail ed comparison between supervised\nlearning methodologies and large language models using datasets from Chin ese social media, with a speciﬁc\nfocus on psychological tasks. These tasks are not just academic exercise s; they have real-world implications,\npotentially shaping strategies for suicide prevention and interven tions for cognitive distortions.\n• We move beyond the foundational aspects and critically assess the perf ormance of large language models,\npaying particular attention to the role of prompt. This endeavor aims t o shed light on the optimal strategies\nfor employing these models in speciﬁc contexts, oﬀering a roadmap f or future researchers.\n• Lastly, our study pioneers the exploration of ﬁne-tuning capabilities of GPT-3.5, leveraging real-world data.\nThis endeavor seeks to determine the adaptability and specialized performance enhancements possible with\nthe model, a facet largely uncharted in current literature.\n2. RELATED WORK\nThe intertwining of artiﬁcial intelligence (AI) with myriad ﬁelds has spurred innovations and transformations\nat an unprecedented scale. A quintessential example is the fusion of natural language processing (NLP) tools,\nnotably deep learning based model, with domains as critical as the ment al health ﬁeld. 15 Additionally, as digital\ninteractions burgeon, especially on social media, the urgency to unde rstand and analyze human sentiments\nbecomes paramount. In this section, we will delve into deep learnin g techniques for sentiment analysis utilizing\ntext data (Section 2.1). Subsequently, we will discuss the evolution, potential, and cu rrent research on large\nlanguage models in this domain (Section 2.2).\n2.1 Text sentiment analysis\nIn the swiftly evolving digital era, social networking platforms hav e emerged as pivotal channels for expressing\nemotions globally. These platforms generate vast amounts of unstructured data every second. Accurately and\npromptly discerning the emotions embedded within this data pres ents a formidable challenge to computational\nalgorithms.\n8 Fu et al. 16 presented a distant supervision method designed to build syste ms that classify high\nand low suicide risk levels using Chinese social media data. This approach minimizes the need for human\nexperts of varying expertise levels to perform annotations. By inte grating this model with crucial psychological\nfeatures extracted from user blogs, they attained an F1 score of 77.98%. Singh et al. 17 employed a BERT-\nbased model for sentiment analysis on tweets sourced globally and another dataset speciﬁcally from India, both\nfocusing on the topic of COVID-19. They reported achieving an accuracy of 94% . Wan 18 introduced a method for\nsentiment analysis of comments on Weibo platforms, leveraging deep ne ural networks. The data undergoes feature\nextraction through multilevel pooling and convolution layers. Commen ts are preprocessed and transformed into\ntext representations using the word2vec algorithm. Subsequently, key features are extracted from the feature\n2\nmatrix using a CNN. For the ﬁnal classiﬁcation and sentiment analysis, th e softmax logistic regression method\nis employed. Zhang et al. 19 explored the correlations among emotion labels, social interactions, and temporal\npatterns within an annotated Twitter dataset. They introduced a fact or graph-based emotion recognition model\nthat seamlessly integrates these correlations into a uniﬁed framewor k. This model adeptly identiﬁes multiple\nemotions by applying a multi-label learning approach to Twitter datase ts. Wang et al. 20 introduced a topic\nmodeling technique, termed LDA, to examine the primary concerns e xpressed on Weibo during the COVID-19\npandemic. They assessed the emotional inclinations of these topics, d etermined their proportional distributions,\nand conducted user behavior analysis based on metrics such as likes, c omments, and retweets. Furthermore, they\nexplored shifts in user concerns and variations in engagement among resi dents from diﬀerent regions of mainland\nChina. Such insights guide public sentiment and actions during heal th emergencies, emphasizing the importance\nof vigilant social media monitoring.\nAlthough deep learning algorithms typically demonstrate impressive r esults, they often require a signiﬁcant\nvolume of labeled data to perform optimally. The distant supervision ap proach highlighted in Fu et al.’s research 16\naims to reduce the need for labeling, but it still requires the inv olvement of three diﬀerent expert groups at\nvarious expertise levels to yield desired results. Nonetheless , when applying these models to new datasets or\ntasks, domain adaptation issues often arise. These trained models can se e a decline in their eﬃcacy, making\ndeep learning algorithms both costly and inﬂexible. Given these hur dles, there’s a growing demand for eﬃcient\nand user-centric methods to assist individuals in emotion detect ion on social media platforms. The recent\nadvancements in large language models present a potential solution to thi s challenge, but their precise impact\nstill warrants examination from multiple perspectives and special ists.\n2.2 Large language model and its applications in medical domain\nThe advent of Large Language Models (LLMs), such as OpenAI’s ChatGPT,\n12 has revolutionized the ﬁeld of\nnatural language processing. 21 These LLMs demonstrate emergent abilities that signiﬁcantly outperfor m those\nof their smaller, pre-trained models. 22 Initially conceived for understanding and generating human-like t ext,\nLLMs have found diverse applications ranging from content generation, 23 medical report assistant, 24 coding\nassistance,25 education,26 and answering medical related questions. 27 The sheer scale of these models enables\nthem to generate complex, contextually relevant content. LLMs have garn ered signiﬁcant attention in medical\ndomain.14 For instance, Jiang et al. 28 developed a clinical LLM named NYUTron to assist physicians and health-\ncare administrators in making time-sensitive decisions. This mod el can process on unstructured clinical notes\nfrom electronic health record. And it can achieve good performance with AUC score ranging from 78.7–94.9%.\nThe model has been successfully deployed in a prospective trial , indicating its potential for real-world application\nin providing point-of-care guidance to physicians.\nConcurrently, research in psychology-related domains has also been con ducted by other researcher. 29, 29 Qin\net al. 30 devised an interpretable and interactive depression detection sy stem employing large language models\n(LLMs). This innovative approach allows for the detection of mental healt h indicators through social media\nactivity and encourages users to interact with the system using natu ral language. While this facilitates a more\npersonalized understanding of an individual’s mental state, it also r aises ethical concerns. The absence of human\noversight could lead to biased outcomes, thereby posing potential ris ks to users. Additionally, if this system\nwere to become a foundational diagnostic tool for future psychological couns eling, issues related to user privacy\ncould become a point of concern. Chen et al. 31 developed a tool designed to improve the realism of psychiatrist-\npatient simulations using ChatGPT-based chatbots. Their approach inv olved using distinct prompts to enable\nlarge language models (LLMs) to emulate the roles of both a depressed patien t and a psychiatrist. The study\nconﬁrmed the feasibility of utilizing ChatGPT-driven chatbots in psychiatric contexts. However, the research\nalso acknowledged limitations: individual patients and counselors ha ve unique communication styles, and some\npatients may be reluctant to engage in conversation. These nuances pre sent a challenge for achieving truly realistic\nsimulations with ChatGPT. Addressing the simulation of diverse per sonalities in a meaningful way remains a\nkey area for further investigation. Fu et al. 32 developed a counseling support system designed to augment\nthe capabilities of non-professional counselors. The system provide s multiple features, including mental health\nanalysis, evaluation of therapist responses, and suggested interventi ons. This application serves as a valuable use\ncase for language models in the mental health sector. Ten professional psy chologists assessed the system on ﬁve\ncritical dimensions, and the ﬁndings were favorable, with a 78% exper t approval rate indicating that the system\n3\ncan deliver eﬀective treatment strategies. Ayers et al. 33 developed a ChatGPT-based chatbot and compared its\nresponses with those of physicians to patient inquiries on a social m edia forum. Notably, 78.6% of the evaluators\npreferred the chatbot’s responses, citing their speed and greater empathetic tone. However, a key limitation\nof this study lies in its exclusive focus on interactions within on line forums. Such settings may not accurately\nreﬂect the nuances of real-world patient-physician dialogues, as phys icians often tailor their responses based on\npre-existing relationships and the context of a clinical setting. I n summary, there is active research into the\nutilization of LLMs in the ﬁeld of psychology, and these research demonstr ate considerable potential. However,\ndelineating the limitations of LLMs remains a crucial issue that warrant s further investigation. Additional studies\nare needed to comprehensively evaluate the capabilities and boundari es of LLMs in psychological applications.\nXu et al. 13 present a pioneering evaluation of multiple Large Language Models (LLMs) acros s various\nmental health prediction tasks using four publicly available online text datasets. Their insights oﬀer guidance to\npractitioners on optimizing the use of LLMs for speciﬁc applications. Wh ile their research stands as a monumental\nveriﬁcation of LLMs’ potential in the mental health domain, it is noteworth y that their datasets are exclusively\nin English and do not address multi-label classiﬁcation tasks. Yang et al .34 assessed ChatGPT’s capabilities\nin mental health analysis and emotional reasoning by evaluating its perfor mance on 11 datasets across ﬁve\ntasks. The study also investigated the impact of diﬀerent emotion-bas ed prompting strategies. Experimental\nresults indicate that while ChatGPT surpasses traditional neural ne twork-based approaches, it still lags behind\nmore advanced, task-speciﬁc methods. Nevertheless, ChatGPT demon strates signiﬁcant potential in the area of\nexplainable mental health analysis. In conclusion, while the integrat ion of LLMs in medicine presents compelling\nprospects, there’s an imperative to ensure privacy and uphold ethi cal standards. Responses generated may\nnot always be ﬂawless. 35 Particularly in mental health, relying solely on LLM-driven system s for diagnosis or\nsupport introduces numerous unpredictable variables. It’s cruc ial to recognize that LLMs warrant meticulous\nscrutiny and validation. 36 Evaluation should be considered an essential discipline to facilitat e the more eﬀective\ndevelopment of large language models (LLMs). 37\n3. METHODS\nWe conducted experiments to classify suicide risk and cognitive d istortions on Chinese social media data using\nsupervised learning methods and large language models (LLMs). Within t he realm of supervised learning,\nwe explored two models BERT 38 and LSAN 39 as baseline, detailed in Section 3.1. For the large language\nmodels, we utilized zero-shot prompt, few-shot prompt, and ﬁne-tu ning methods. Subsequent sections provide a\ncomprehensive introduction of these methods.\n3.1 Baseline supervised learning model\nWe experimented with two representative models: LSAN\n39 and BERT. 38 LSAN is adept at uncovering the\nrelationships between labels, making it particularly suitable for ou r cognitive distortion recognition task. On the\nother hand, BERT represents a groundbreaking pre-trained model arc hitecture that had achieved state-of-the-art\n(SOTA) on 11 distinct NLP tasks. We discuss each in detail below:\n• LSAN: The LSAN model is engineered to utilize label semantics for ident ifying the relationships between\nlabels and documents, thereby creating a label-speciﬁc documen t representation. The model also employs\na self-attention mechanism to focus on this representation, which is derived from the document’s content.\nAn adaptive fusion strategy integrates these components eﬀectively, f acilitating the generation of a compre-\nhensive document representation suitable for multi-label text c lassiﬁcation. The LSAN model has proven\neﬀective, particularly in predicting low-frequency labels.\n• BERT: Bidirectional Encoder Representations from Transformers (B ERT) has been a pivotal development\nin natural language processing (NLP). Unlike traditional NLP models that pro cess text unidirectionally,\nBERT uses a bidirectional approach, facilitated by the Transformer arc hitecture, to understand the full\ncontext of each word. It is pre-trained using a masked language model obje ctive, where random words\nare replaced with a ’[MASK]’ token and the model predicts the original word. This design has enabled\nBERT to set new performance standards in diverse NLP tasks, such as que stion-answering and sentiment\nanalysis, especially when ﬁne-tuned on speciﬁc task data.\n4\n3.2 Large language models\nGiven that our data is in Chinese, we explored the open-source model s ChatGLM2-6B and GLM-130B,\n40 both of\nwhich support Chinese language processing. The primary distinction between these two models lies in the number\nof parameters they possess. GPT-3.5 41 stands as a ﬂagship large-scale language model. We experimented with\nvarious prompt word constructions and sought to integrate prior knowledge from the psychological domain, along\nwith the most recent public ﬁne-tuning functionalities. GPT-4, 42 being the latest iteration, was also included in\nour assessment. Detailed introduction on these models are provide d in the subsequent sections.\n• ChatGLM2-6B: ChatGLM2-6B is an open-source bilingual language model with 6.2 billion parame ters,\noptimized for Chinese question-answering and dialogue. It employs s imilar technology to ChatGPT and\nis trained on roughly 1 terabyte of Chinese and English text data. The mod el can be ﬁne-tuned through\nvarious techniques like supervised learning and human feedback. I t also features an eﬃcient tuning method\nbased on P-Tuning v2, requiring at least 7GB of GPU memory for customizati on. Due to quantization\ntechniques, it can run on consumer-grade graphics cards with only 6GB of m emory.\n– Source code: https://github.com/thudm/chatglm2-6b\n– Unoﬃcial demo: https://huggingface.co/spaces/mikeee/chatglm2-6b-4bit\n• GLM-130B: GLM-130B is a bilingual pre-trained language model optimized for both Englis h and Chinese,\nboasting a substantial 130 billion parameters. This model aims to provi de an open-source alternative of a\nscale comparable to GPT-3, while shedding light on the complexities of training such large-scale models.\nDuring its development, the team faced a host of technical and enginee ring challenges, including issues\nrelated to loss spikes and model convergence. Impressively, GLM-130B surpasses GPT-3 175B on multiple\nEnglish benchmarks and outperforms ERNIE TITAN 3.0 260B, 43 the largest existing Chinese language\nmodel, on relevant benchmarks. A distinctive feature of GLM-130B is it s capability for INT4 quantization\nwithout substantial performance degradation, thus facilitating eﬃcie nt inference on widely available GPUs.\n– Source code: https://github.com/THUDM/GLM-130B\n– Oﬃcial online demo: https://chatglm.cn/detail\n• GPT-3.5: GPT-3.5 is a cutting-edge language model developed by OpenAI, designed to oﬀer enhanced\nconversational capabilities. Building on the foundation of its predece ssor, GPT-3, this iteration introduces\nimprovements in both performance and cost-eﬃciency. OpenAI’s comm itment to reﬁning and advancing\nthe capabilities of their models is evident in GPT-3.5, which prov ides users with a more coherent, context-\naware, and responsive conversational experience. As part of OpenAI’s mi ssion to ensure that artiﬁcial\ngeneral intelligence beneﬁts all of humanity, GPT-3.5 is a testament to the organization’s dedication to\ninnovation and excellence in the realm of natural language processing.\n– Web application (also for GPT 4): https://chat.openai.com/\n– Fine-tuning details: https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates\n• GPT 4: GPT-4 is a groundbreaking multimodal model capable of processing bot h image and text inputs\nto generate text-based outputs. Marking a signiﬁcant advancement ove r its predecessors, GPT-4 exhibits\nhuman-level performance across a range of professional and academic benchm arks, including a top 10% score\non a simulated bar exam. Built upon the Transformer architecture, the model is initially trained to predict\nsubsequent tokens in a given sequence and later undergoes a post-tr aining alignment process to improve\nits factuality and behavior. A critical component of the project invol ved the development of scalable\ninfrastructure and optimization techniques that function consiste ntly across various sizes, allowing the\nteam to extrapolate GPT-4’s performance metrics based on smaller model s. Despite its notable capabilities,\nGPT-4 does inherit certain limitations from earlier versions, suc h as occasional content ”hallucinations”\nand a constrained context window.\nThe large language model is widely recognized as being pre-trained on vast amounts of text data. However,\nthe manner in which prompt are inputted is crucial, as it directly i nﬂuences the LLM’s comprehension and\noutput for a given task. In light of this, we have formulated the followi ng prompts.\n5\nLLM Zero-shot Prompting We initiate our exploration with prompt design tailored for tasks withi n a\nzero-shot paradigm. This process encompasses various strategies, incl uding direct task requests (acting as the\nbasic), role-deﬁnition, scene-deﬁnition, and hybrid approaches. F or illustrative purposes, the cognitive distortion\nclassiﬁcation task serves as the focal point. The design is elaborated as f ollows:\n1. basic: A direct task directive devoid of speciﬁc contextual emphasis.\n(a) English translation: ”Please conduct a multi-classiﬁcation task to as certain if it encompasses any of\nthe speciﬁed 12 cognitive distortions ([list of cognitive distortions ]). ”\n(b) Formulaic representation: M(T,12CD), where M stands for multi-classiﬁcation, T symbolizes the\ntask, and 12 CD represents the 12 cognitive distortions.\n2. Role-deﬁnition Prompting: The prompt delineates the role of the respondent (in this case, a psy chol-\nogist) and emphasizes reliance on psychological insights.\n(a) English translation: ”Assuming the role of a psychologist and leveragin g psychological insights, please\nconduct a multi-classiﬁcation task to discern if it integrates any of t he 12 cognitive distortions ([list\nof cognitive distortions]).”\n(b) Formulaic representation: R(M(T,12CD)), where R embodies the role-deﬁnition of being a psychol-\nogist.\n3. Scene-deﬁnition Prompting: The context of a social media setting is introduced, highlighting user\nidentiﬁers to preclude ambiguity.\n(a) English translation: ”Considering the provided user ID and the ass ociated posts on social media,\nplease based on the post content, engage in a multi-classiﬁcation task to d etermine the presence of\nany of the 12 cognitive distortions ([list of cognitive distortions]).”\n(b) Formulaic representation: S(M(T,12CD)), with S denoting the scene, which in this scenario, pertains\nto the user’s ID and corresponding social media posts.\n4. Hybrid Prompting: A synthesis of both role and scene deﬁnitions, oﬀering an integrative i nstruction.\n(a) English translation: ”With the given user ID and their respective social media posts, and adopting\nthe role of a psychologist fortiﬁed with psychological expertise, pleas e execute a multi-classiﬁcation\ntask to verify the inclusion of any of the 12 cognitive distortions ([lis t of cognitive distortions]).”\n(b) Formulaic representation: S + R(M(T,12CD)), intertwining the scene context ( S) with the role-\ndeﬁnition ( R).\nLLM Few-shot Prompting In this segment, few-shot prompting is construed as the provision of prior knowl-\nedge or a batch of n training instances to LLMs, thereby enabling them to internalize t his information and adeptly\nexecute the stipulated task. This methodology unfolds as:\n1. Background Knowledge: The model is furnished with psychological deﬁnitions supplement ed by em-\nblematic cases, followed by one of the four prompting strategies devis ed from zero-shot prompting. Prompts\nthat integrate background knowledge and employ the hybrid strategy from zero-shot prompting are detailed\nas follows:\n(a) English translation: ”Given the deﬁnitions of cognitive distortions denoted by D and the prototypical\ncases represented by C, and in light of the supplied user ID and associated social media posts , you are\nassumed to be a psychological expert well-versed in the aforemention ed deﬁnitions and cases. Drawing\nfrom this backdrop, please conduct a multi-classiﬁcation task to evalu ate the correlation with any of\nthe 12 cognitive distortions ([list of cognitive distortions]).”\n6\n(b) Formulaic representation: D +C +S +R(M(T,12CD)), where D encapsulates the background deﬁni-\ntion, and C signiﬁes the prototypical instances from academic literature, S represents scene-deﬁnition\nand R stands for role-deﬁnition.\n2. Training with n Samples per Category: In this approach, n training instances are randomly selected\nfor each category to train the LLM, followed by one of the four prompting strat egies designed from zero-shot\nprompting. These instances are represented as trainn in the following tables. Prompts that incorporate\nthe training instances employ the hybrid strategy from zero-shot pr ompting are detailed as follows:\n(a) English translation: ”You are provided with learning samples denote d by T .In light of the supplied\nuser ID and associated social media posts, and assuming your role as a psy chologist with the rele-\nvant expertise.Drawing from this backdrop, please conduct a multi -classiﬁcation task to evaluate the\ncorrelation with any of the 12 cognitive distortions ([list of cognitive di stortions]).”\n(b) Formulaic representation: T + S + R(M(T,12CD)), integrating T as the training set with the scene-\ndeﬁnition ( S) and role-deﬁnition ( R).\n3. Background knowledge and training with n samples per category: This approach investigates\nwhether enhancing sample diversity in few-shot prompting augment s the LLM’s comprehension of psycho-\nlogical health tasks. It incorporates psychological deﬁnitions, symbolic examples, and provides n training\ninstances per category for LLM training. A command is subsequently issue d using a previously described\nfew-shot prompting strategy. The following example integrates backgr ound knowledge and training in-\nstances, and poses a query using the hybrid strategy from zero-shot pr ompting:\n(a) English translation: ”Given the deﬁnitions of cognitive distortions represented by D and the proto-\ntypical cases denoted by C, you are also provided with learning samples represented by T . Assuming\nyour expertise as a psychological expert familiar with the aforemention ed deﬁnitions and cases, and\nin consideration of the supplied user ID and associated social media p osts, please conduct a multi-\nclassiﬁcation task to evaluate the correlation with any of the 12 cognitive d istortions ([list of cognitive\ndistortions]).”\n(b) Formulaic representation: D + C + T + S + R(M(T,12CD)), where D encapsulates the background\ndeﬁnition, C signiﬁes the prototypical instances from academic literature, T is integrated as the\ntraining set, S denotes the scene-deﬁnition, and R represents the role-deﬁnition.\nLLM Fine-tuning Fine-tuning represents a potent paradigm provided by OpenAI, enab ling users to optimize\nthe aptitude of pre-trained models, such as GPT-3.5. While GPT-3.5 i s inherently trained on an expansive text\ncorpus, the ﬁne-tuning process sharpens its proﬁciency for spec ialized tasks by exposing it to additional task-\nspeciﬁc instances. Details can be seen in the oﬃcial document: https://platform.openai.com/docs/guides/\nfine-tuning. Following the ﬁne-tuning, our evaluation paradigm retained the role, s cene and hybrid deﬁnitions\nfrom the zero-shot prompting for consistency and comparative assessmen t:\n1. Role-deﬁnition Prompting: Post ﬁne-tuning with relevant training samples, we employed the prompt\ndelineated in the role-deﬁnition section (refer to Section 3.2).\n2. Scene-deﬁnition Prompting: Analogously, after the ﬁne-tuning process, we reverted to the promp t\nillustrated in the scene-deﬁnition segment of the zero-shot prompt ing.\n3. Hybrid Prompting: Similarly, after the ﬁne-tuning process, we adopted the prompt pr esented in the\nhybrid strategy segment of the zero-shot prompting.\n4. EXPERIMENTS AND RESULTS\n4.1 Datasets and Evaluation Metrics\nWe undertook two psychology-related classiﬁcation tasks: suicide ri sk and cognitive distortion. The suicide risk\ntask primarily diﬀerentiates between high and low suicide risks, while the cognitive distortion task focuses on\n7\nclassiﬁcations deﬁned by Burns. 44 We sourced our data by crawling comments from the “Zoufan” ∗ blog within\nthe Weibo social platform. Subsequently, a team of qualiﬁed psychol ogists were enlisted to annotate the data.\nGiven that this data is publicly accessible, there are no concerns related to privacy breaches.\nFor the suicide detection data, there were 645 records with low suici de risk and 601 records with high suicide\nrisk. The dataset for cognitive distortion consists of a total of 910 entries . The classiﬁcation labels employed for\nthis data are as follows: all-or-nothing thinking, over-generalization , mental ﬁlter, disqualifying the positive, mind\nreading, the fortune teller error, magniﬁcation, emotional reasoning, sh ould statements, labeling and mislabeling,\nblaming oneself and blaming others. For both sets of data, the training se t and test set are divided according to\nthe ratio of 4:1. We utilize three evaluation metrics to measure the pe rformance of diﬀerent algorithms for our\ntwo tasks: precision, recall, and F1 score. Precision is the ratio of correctly predicted positive obser vations to the\ntotal predicted positives and recall (or Sensitivity) represents the ratio of correctly predicted positive observations\nto all the actual positives. These two metrics provide a comprehen sive view of the algorithm’s performance in\nterms of its positive predictions. The F1 score oﬀers a more holistic view of the model’s performance, especial ly\nwhen the distribution of the true positive and true negative rates is uneven.\n4.2 Experiment design\nOur experimental methodology is both hierarchical and greedy. Using cogn itive distortions as an example to\nshow our points, our evaluations spanned several dimensions:\n• Prompt Design Perspective: Initially, we assessed four prompting strategies within the zero-shot learning\nframework. Subsequently, based on their performance metrics, the top two strategies were selected for\nfurther evaluation in the few-shot learning setting across various LLMs .\n• LLM Performance Perspective: Across all zero-shot prompts, ChatGLM2-6B’s performance was found to\nbe lacking, resulting in our decision to omit it from subsequent few -shot prompting experiments. For\nGPT-3.5, its token limitation prevented us from entering ﬁve sample s for each category during few-shot\nprompting. Consequently, we reserved the train 5 approach exclusively for GPT-4.\n• Fine-tuning Perspective: A discernible performance chasm exi sts between GPT-3.5 and GPT-4. However,\nOpenAI’s recent introduction of ﬁne-tuning capabilities for GPT- 3.5 and reports from oﬃcial channels\nsuggest that, under speciﬁc conditions, GPT-3.5 might outperform GPT -4 post ﬁne-tuning. Consequently,\nour attention was centered on the ﬁne-tuning of GPT-3.5. Regrettably, t he current iteration of GPT-4\nlacks ﬁne-tuning functionalities, curtailing our capacity to asses s its potential in this dimension.\nThe detailed experimental setup is as follows:\n• LSAN: We used word2vec to train 300-dimensional embeddings for both document an d randomly-initialized\nlabel texts. The attention mechanism helped us compute word contrib utions to labels and create label-\nspeciﬁc document representations. Dot products between these document and label vectors reﬁned these\nrelationships further. These two types of document representat ions were then fused using weighted combi-\nnations. For predictions, we employed a fully connected layer, fol lowed by RELU and a sigmoid function.\nLosses were calculated using a cross-entropy function during traini ng.\n• BERT: We employ BERT to extract 768-dimensional vectors from Chinese sente nces. To mitigate over-\nﬁtting, a dropout function is applied to these sentence vectors. Su bsequently, a fully connected layer is\nintroduced to independently classify suicide risk and cognitiv e distortions. The sigmoid function serves as\nthe activation function for the output layer. Both the BERT layer and t he fully connected layer are trained\nsimultaneously.\n• LLM-zero shot: Both GPT-3.5 and GPT-4 are closed-source and available through API provid ed by\nOpenAI. We picked the gpt-3.5-turbo, one of the most capable and cost-eﬀec tive models in the GPT-3.5\nfamily, and the gpt-4, more capable than any GPT-3.5 model, able to do more c omplex tasks, and optimized\n∗ https://www.weibo.com/xiaofan116?is_all=1\n8\nfor chat. As for the GLM models, we employed the smaller, open-source v ariant, ChatGLM2-6B, suitable\nfor deployment on consumer-grade hardware. Given the extensive param eter count of GLM-130B, it posed\ndeployment challenges due to its elevated operational costs. Furthe rmore, its API lacked the capability to\nhandle cognitive distortion multi-label classiﬁcation task, leading u s to conduct tests via its oﬃcial website.\nAcknowledging the inherent variability in LLM’s outputs, our experi mental design involved averaging the\noutcomes over ﬁve runs. For GPT-3.5, GPT-4, and ChatGLM2-6B, we adjusted t he temperature to values\nof 0.1, 0.3, 0.5, 0.7, and 0.9, conducting experiments at each setting. Given th e absence of a temperature\nsetting for GLM-130B on its platform, we simply executed ﬁve repeated r uns and computed the mean\nperformance. For zero-shot evaluations, we initiated performance valid ation on the basic strategy across\nthe LLMs, subsequently examining the eﬃcacy of role-deﬁnition, sce ne-deﬁnition, and hybrid strategies,\naiming to discern the inﬂuence of domain-speciﬁc information on LLM’s p erformance.\n• LLM-few shot: We conducted a assessment using the top two performing prompt strat egies from the zero-\nshot tests, determined by their F1-scores.The impact on performanc e was assessed when augmenting these\nstrategies with background, trainn , and their combination ( background +trainn). Speciﬁcally, background\ndenotes the incorporation of prior knowledge, trainn represents the addition of training samples, where n\nis the number of positive samples chosen for each category. background + trainn suggests simultaneous\nenrichment with prior knowledge and training samples. Given the v arying token input constraints among\ndiﬀerent models, the sample size selected for each model diﬀere d. In addition, we also experimented with\nthe integration of basic, role, scene, and hybrid strategies in the zer o-shot prompting scenario.\n• LLM-ﬁne-tunning: We ﬁne-tuned the GPT-3.5 Turbo model for predicting suicide ri sk and cognitive\ndistortions using the API interface provided by OpenAI. We utiliz ed three types of prompts: role-based,\nscene-based, and hybrid strategies.\n5. RESULTS\nIn our study, we focused on two speciﬁc tasks: suicide classiﬁcati on and multi-label classiﬁcation of cognitive\ndistortions. And the results can be seen in Table 1 and Table 2 respectively. Our analysis examined these two\ntasks in Section 5.1 and Section 5.2 respectively from three distinct aspects: training strategy, th e construction of\nprompt, and a comparative evaluation across various LLMs. Ultimately, we asse ssed and compared the model’s\nperformance on these two psychological tasks to draw conclusions in Sec tion 5.3.\n5.1 Suicide Risk\nTraining strategies In our training strategy comparison, we observed varying degrees of eﬀec tiveness across\ndiﬀerent models. The LSAN model showed a modest improvement ov er the pre-trained BERT model, with a\n1.59% F1-score increase in performance; however, this diﬀerence was n ot statistically signiﬁcant. In contrast,\nﬁne-tuning GPT-3.5 led to a substantial performance gain, achieving an F1-score of 78.44%. This represented\na notable 11.42% improvement in F1-score when compared to its base model ( ﬁne-tuning hybrid vs.zero-shot\nhybrid), bringing its performance closer to that of supervised lear ning models.\nDesign of prompts Our investigation into prompt design for large language models revealed nu anced out-\ncomes across diﬀerent strategies and models. In the realm of zero-shot p rompts, we found that while a mixed\nstrategy yielded satisfactory results, the performance diﬀerence s among various types of prompts were not sta-\ntistically signiﬁcant. For few-shot prompts, adding more data did not c onsistently improve performance; this\nwas evident in the ChatGLM2-6B model where additional data sometimes re duced eﬀectiveness. Conversely,\nGPT-4’s performance remained stable irrespective of the data size. Notably, the background+train n+hybrid\nstrategy emerged as the most eﬀective across multiple models.\nWe also studied the impact of extra training data in few-shot scenari os and observed that using role-deﬁne and\ntrainn+role-deﬁne prompts often led to diminished performance. The role of background knowledge was model-\ndependent; in smaller models like ChatGLM2-6B, incorporating backgrou nd knowledge led to a performance\nincrease from 53.75% to 64.41%. However, this could not be universally veriﬁ ed due to token limitations. Finally,\nour comparison between few-shot and zero-shot prompts showed that few- shot prompts did not signiﬁcantly\noutperform their zero-shot counterparts.\n9\nComparison of LLMs In our comparative analysis of large language models, we observed several tre nds that\nhighlight the complexities of model performance. Generally, GPT-4 outperformed GPT-3.5, and GLM-130B\nexcelled over ChatGLM2-6B, suggesting the beneﬁts of larger model arch itectures and more extensive training\ndata. However, this narrative was disrupted when GPT-3.5 was ﬁne-tun ed, as it surpassed GPT-4 by a margin\nof 2.63%. Additionally, GLM-130B demonstrated a performance comparable to GPT- 4 and superior to GPT-3.5\nfor the speciﬁc task under study. These ﬁndings indicate that whil e larger models typically oﬀer advantages,\nﬁne-tuning and task-speciﬁc capabilities can alter the performance landscape signiﬁcantly.\n5.2 Cognitive Distortion\nTraining strategies Our investigation into training strategies for large language models reve aled nuanced\nperformance outcomes. Initially, the pre-trained BERT model demon strated a 2.83% performance advantage\nover LSAN trained from scratch. However, this diﬀerence was not statis tically signiﬁcant, implying that the\nobserved discrepancy may not be meaningful. On the other hand, ﬁne-t uning GPT-3.5 surprisingly led to a\ndecrease in performance rather than the anticipated improvement. T his underscores the complexity of model\ntraining and the need for careful consideration when implementing ﬁ ne-tuning strategies.\nDesign of prompts In the design of prompts for large language models, our study examined the p erformance\nof both zero-shot and few-shot prompts. For zero-shot prompts, we found th at a meticulous design focusing on\nscene and role settings is crucial; otherwise, a basic task-oriented prompt is generally more eﬀective. In the realm\nof few-shot prompts, we observed that prompts providing speciﬁc dat a points outperformed those that simply\noﬀered background knowledge. Interestingly, increasing the traini ng data in these prompts did not lead to better\nperformance. A comparative analysis revealed that although few-shot promp ts outperformed zero-shot prompts,\nthey still fell short of fully meeting the task requirements, as e videnced by GPT-4’s F1-score of approximately\n30%.\nComparison of LLMs Consistently, larger models like GPT-4 outperformed their smalle r counterparts such as\nGPT-3.5. When it came to the complex tasks in our study, The performanc e of ChatGLM2-6B was insuﬃcient\nfor handling complex tasks, while GLM-130B fared better but was still out done by GPT-4. Given that our\ndataset consists of comments from social networks, the text is generall y concise. As a result, token length did\nnot substantially aﬀect the performance of the models in our tasks. Rath er, the selection of representative data\nfor prompt construction emerged as a more crucial factor than merely incr easing the number of tokens.\n5.3 Cross-Task Comparison\nAs task complexity increased from binary to multi-label classiﬁcation , large language models did not sustain their\nperformance. In contrast, supervised learning models maintained a relatively stable F1-score close to 80% across\nboth types of tasks. This highlights the limitations of large language model s in replacing supervised learning\nfor specialized tasks. While ﬁne-tuning may beneﬁt simpler task s, it does not adequately address the challenges\nposed by complex tasks, calling for further investigation into ﬁne-t uning mechanisms for large language models.\n6. DISCUSSION\nOur study systematically evaluated the eﬀectiveness of large language m odels (LLMs) across two mental health\nrelated tasks on Chinese social media: suicide risk classiﬁcation and cognitive distortion multi-label classiﬁcation.\nOur results also reveal the nuanced role of prompt design. While the ’ hybrid’ prompt performed well in zero-\nshot settings, the beneﬁts of increasing data in few-shot prompts we re not universally beneﬁcial. For more\nstraightforward tasks, adding background knowledge appeared to help smal ler models (ChatGLM2-6B), but its\nutility diminished in more complex models or tasks. This calls for a more customized approach to prompt\nengineering tailored to the speciﬁc task and the size of the model be ing used. If high-quality data is unavailable\nor prompt design proves challenging, allowing a LLM to directly handle t he task may still yield acceptable\nperformance. Larger language models like GPT-4 and GLM-130B generally outperf orm smaller variants such as\nGPT-3.5 and ChatGLM2-6B. However, it’s important to note that these large m odels are not always competent\n10\nTable 1. Result for suicide binary classiﬁcation task\nModel category Model name Type Sub-type Train data Test data Precision Recall F1-score\nSupervised learning\nLSAN train from scarch - 999\n250\n81.86% 82.29% 82.07%\nBERT ﬁne-tuning - 999 80.16% 80.80% 80.48%\nLLM\nChatGLM2-6B\nzero-shot\nbasic 0 69.07% 37.10% 48.07%\nrole-deﬁne 0 65.77% 35.81% 46.15%\nscene-deﬁne 0 64.52% 45.16% 53.01%\nhybrid 0 65.68% 46.13% 53.74%\nfew-shot\nbackground+scene-deﬁne 0 58.56% 47.26% 51.45%\nbackground+hybrid 0 60.37% 70.64% 64.41%\ntrain12+scene-deﬁne 24 67.19% 59.52% 63.04%\ntrain12+hybrid 24 64.29% 49.20% 55.56%\nbackground+train12+scene-deﬁne 24 49.74% 56.61% 52.70%\nbackground+train12+hybrid 24 58.91% 73.23% 64.78%\ntrain30+scene-deﬁne 60 57.71% 26.77% 36.14%\ntrain30+hybrid 60 50.60% 24.84% 32.60%\nbackground+train30+scene-deﬁne 60 62.97% 52.90% 57.02%\nbackground+train30+hybrid 60 60.14% 47.10% 51.88%\nGLM-130B\nzero-shot\nbasic 0 54.58% 95.81% 69.52%\nrole-deﬁne 0 55.51% 94.84% 70.02%\nscene-deﬁne 0 55.05% 93.87% 69.40%\nhybrid 0 57.37% 97.42% 72.20%\nfew-shot\nbackground+role-deﬁne 0 56.55% 90.32% 69.55%\nbackground+hybrid 0 56.91% 92.42% 70.43%\ntrain12+role-deﬁne 24 53.18% 83.23% 64.89%\ntrain12+hybrid 24 55.30% 88.39% 68.02%\nbackground+train12+role-deﬁne 24 57.84% 83.38% 68.30%\nbackground+train12+hybrid 24 60.88% 90.00% 72.61%\nGPT-3.5\nzero-shot\nbasic 0 52.00% 88.23% 65.42%\nrole-deﬁne 0 53.31% 96.13% 68.59%\nscene-deﬁne 0 52.16% 89.03% 65.76%\nhybrid 0 52.55% 92.26% 66.95%\nfew-shot\nbackground+role-deﬁne 0 54.90% 88.55% 67.76%\nbackground+hybrid 0 55.27% 89.03% 68.19%\ntrain12+role-deﬁne 0 56.34% 83.39% 67.22%\ntrain12+hybrid 0 57.19% 84.68% 68.27%\nbackground+train12+role-deﬁne 0 59.37% 81.61% 68.71%\nbackground+train12+hybrid 24 58.26% 82.90% 68.41%\nﬁne-tuning\nrole-deﬁne 999 84.76% 71.77% 77.29%\nscene-deﬁne 999 63.20% 83.15% 72.14%\nhybrid 999 84.25% 73.38% 78.44%\nGPT-4\nzero-shot\nbasic 0 57.43% 95.48% 71.72%\nrole-deﬁne 0 57.29% 97.26% 72.10%\nscene-deﬁne 0 58.81% 97.58% 73.39%\nhybrid 0 57.47% 97.42% 72.30%\nfew-shot\nbackground+scene-deﬁne 0 64.91% 73.55% 68.86%\nbackground+hybrid 0 63.24% 84.03% 72.05%\ntrain12+scene-deﬁne 24 60.70% 94.35% 73.87%\ntrain12+hybrid 24 59.77% 84.19% 69.87%\nbackground+train12+scene-deﬁne 24 65.44% 81.77% 72.63%\nbackground+train12+hybrid 24 65.65% 78.87% 71.60%\ntrain30+scene-deﬁne 60 61.11% 92.42% 73.56%\ntrain30+hybrid 60 60.79% 89.03% 72.22%\nbackground+train30+scene-deﬁne 60 63.86% 83.06% 72.16%\nbackground+train30+hybrid 60 70.16% 82.58% 75.81%\n11\nTable 2. Result for cognitive distortion multi-label classiﬁ cation task.\nModel category Model name Type Sub-type Train data Test data Precision Recall F1-score\nSupervised learning LSAN train from scratch - 728\n182\n76.79% 77.95% 76.08%\nBERT ﬁne-tuning - 728 79.85% 80.49% 78.91%\nLLM\nGLM-130B\nzero-shot\nbasic 0 9.56% 57.39% 16.39%\nrole-deﬁne 0 9.00% 65.31% 15.78%\nscene-deﬁne 0 8.75% 60.43% 15.19%\nhybrid 0 9.93% 67.83% 17.31%\nfew-shot\ntrain1+basic 12 7.59% 39.31% 12.69%\ntrain1+hybrid 12 8.18% 44.09% 13.73%\nGPT-3.5\nzero-shot\nbasic 0 10.25% 4.87% 6.49%\nrole-deﬁne 0 12.58% 4.35% 6.18%\nscene-deﬁne 0 9.2% 3.91% 5.34%\nhybrid 0 8.61% 5.39% 6.38%\nfew-shot background+hybrid 0 2.39% 7.91% 11.63%\nbackground+basic 0 24.21% 10.09% 14.06%\ntrain2+hybrid 24 13.32% 10.09% 11.46%\ntrain2+basic 24 12.51% 10.00% 11.10%\nﬁne-tuning\nrole-deﬁne 728 10.79% 8.26% 6.78%\nscene-deﬁne 728 13.17% 10.43% 7.88%\nhybird 728 10.79% 8.26% 6.27%\nGPT-4\nzero-shot\nbasic 0 16.46% 46.09% 24.18%\nrole-deﬁne 0 16.69% 42.09% 23.86%\nscene-deﬁne 0 18.12% 43.22% 25.43%\nhybrid 0 16.26% 38.87% 22.84%\nfew-shot\nbackground+basic 0 21.96% 31.04% 25.54%\nbackground+scene-deﬁne 0 22.89% 34.18% 26.59%\ntrain2+basic 24 31.25% 34.17% 32.47%\ntrain2+scene-deﬁne 24 27.00% 27.74% 27.06%\nbackground+train2+basic 24 24.35% 32.00% 27.63%\nbackground+train2+scene-deﬁne 24 24.39% 28.09% 25.94%\ntrain5+basic 60 25.62% 35.65% 29.57%\ntrain5+scene-deﬁne 60 29.46% 34.61% 31.57%\nat handling complex tasks and should not be seen as replacements for supe rvised learning algorithms. For simpler\ntasks, such as the suicide risk classiﬁcation task examined in our stu dy, the performance of LLMs is satisfactory.\nInterestingly, after ﬁne-tuning, GPT-3.5 even outperforms GPT-4, achieving results that are nearly on par with\nthose obtained through supervised learning methods. While there i s often a preference for large input limits in\nlarge language models (LLMs), it’s crucial to tailor these settings to the speciﬁc task at hand. For tasks involving\nshorter text, such as our study on sentiment analysis of social network data, the long input capability of an LLM\nmay not be a primary concern. Our experiments indicate that extend ing the input data to construct few-shot\nprompts does not necessarily lead to improved performance. Theref ore, it is important to carefully consider the\nnature of the task when conﬁguring the input parameters of an LLM.\nOur study does have some limitations. For instance, due to token cons traints, we were unable to conduct\ncertain tests—particularly those involving smaller models supple mented with background knowledge—across all\ntasks. Looking ahead, we plan to conduct more comprehensive studies th at encompass a wider variety of tasks\nand models. This will allow us to draw more deﬁnitive conclusions r egarding the comparative eﬀectiveness of\nlarge language models and supervised learning algorithms. Additionally, t he ﬁne-tuning mechanisms of LLMs\nwarrant further exploration, particularly for more eﬃcient handling of complex tasks. The development of\nadvanced prompt engineering techniques could also help optimize th e performance of LLMs across various tasks.\n7. CONCLUSION\nIn this study, we evaluated the performance of multiple large language mo dels (LLMs) in two psychology-related\ntasks and compared their eﬃcacy with that of supervised learning algorit hms. Although LLMs show promise\nin various natural language processing applications, they are not yet a comp rehensive substitute for supervised\nlearning, particularly in domain-speciﬁc tasks. Fine-tuning LLMs can enhance performance on simpler tasks\nbut is less eﬀective for more complex challenges. The success of diﬀ erent training strategies and prompt designs\nis highly contingent on both the task and the size of the model, undersc oring the necessity for task-speciﬁc\ncustomization. In summary, our research suggests that while LLMs oﬀer cons iderable potential, signiﬁcant work\nremains to make them universally eﬀective across a broad array of comple x tasks.\n12\n8. DATA A V AILABILITY\nThe experimental texts are sourced from comments on a Sina Weibo post by the user ”Zoufan.” The post can\nbe accessed at: https://www.weibo.com/xiaofan116?is_all=1.\n9. CODE A V AILABILITY\nThe code for this manuscript is being sorted out and will be made publ ic after the article is accepted.\n10. ACKNOWLEDGMENTS\nThis work was supported by grants from the National Natural Science Foundation of China (grant num-\nbers:72174152, 72304212 and 82071546), Fundamental Research Funds for the Central Universit ies (grant num-\nbers: 2042022kf1218; 2042022kf1037), the Young Top-notch Talent Cultivation Program of Hubei Pr ovince.\nGuanghui Fu is supported by a Chinese Government Scholarship prov ided by the China Scholarship Council\n(CSC).\n11. AUTHOS CONTRIBUTIONS\nHongzhi Qi were responsible for the experiment design and programming. Q ing Zhao and Jianqiang Li col-\nlaborated in the proposal of the AI-related aspects of the project, with Z hao focusing on data analysis and\ninterpretation and Li serving as the leader of the computer science asp ect of the project. Both also reviewed\nthe manuscript. Dan Luo and Huijing Zou contributed to the manuscript writing, carried out experimental\nveriﬁcation, and collected data. Changwei Song and Wei Zhai were respons ible for code development and served\nas auxiliary programmers. Guanghui Fu proposed the central idea of the stu dy and was a major contributor in\nwriting the manuscript. Shuo Liu, Yi Jing Yu and Fan Wang took the lead i n result evaluation and contributed\npsychological perspectives to the idea proposal. Bing Xiang Yang propose d the psychological aspects of the\nidea, performed experimental veriﬁcation, and led the project from the psychology angle. All authors read and\napproved the ﬁnal manuscript.\n12. COMPETING INTERESTS\nAll authors declare no ﬁnancial or non-ﬁnancial competing interests.\nREFERENCES\n[1] World health organization, “Depressive disorder (depression),” ( 2023).\n[2] Huang, Y., Wang, Y., Wang, H., Liu, Z., Yu, X., Yan, J., Yu, Y., Kou, C., Xu, X., Lu , J., et al., “Prevalence\nof mental disorders in China: a cross-sectional epidemiological study ,” The Lancet Psychiatry 6(3), 211–224\n(2019).\n[3] World health organization, “Suicide,” (2023).\n[4] Keles, B., McCrae, N., and Grealish, A., “A systematic review: th e inﬂuence of social media on depression,\nanxiety and psychological distress in adolescents,” International journal of adolescence and youth 25(1),\n79–93 (2020).\n[5] Robinson, J., Cox, G., Bailey, E., Hetrick, S., Rodrigues, M., Fisher, S., and Herrman, H., “Social media\nand suicide prevention: a systematic review,” Early intervention in psychiatry 10(2), 103–121 (2016).\n[6] Luxton, D. D., June, J. D., and Fairall, J. M., “Social media and sui cide: a public health perspective,”\nAmerican journal of public health 102(S2), S195–S200 (2012).\n[7] Coppersmith, G., Leary, R., Crutchley, P., and Fine, A., “Natural l anguage processing of social media as\nscreening for suicide risk,” Biomedical informatics insights 10, 1178222618792860 (2018).\n[8] Nandwani, P. and Verma, R., “A review on sentiment analysis and emotion detection from text,” Social\nNetwork Analysis and Mining 11(1), 81 (2021).\n[9] Acheampong, F. A., Wenyu, C., and Nunoo-Mensah, H., “Text-based emotio n detection: Advances, chal-\nlenges, and opportunities,” Engineering Reports 2(7), e12189 (2020).\n13\n[10] Saunders, D., “Domain adaptation and multi-domain adaptation for neural mac hine translation: A survey,”\nJournal of Artiﬁcial Intelligence Research 75, 351–424 (2022).\n[11] Laparra, E., Bethard, S., and Miller, T. A., “Rethinking domain adaptat ion for machine learning over\nclinical language,” JAMIA open 3(2), 146–150 (2020).\n[12] Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Z hang, J., Dong, Z., et al.,\n“A survey of large language models,” arXiv preprint arXiv:2303.18223 (2023).\n[13] Xu, X., Yao, B., Dong, Y., Yu, H., Hendler, J., Dey, A. K., and Wang, D., “Leve raging large language\nmodels for mental health prediction via online text data,” arXiv preprint arXiv:2307.14385 (2023).\n[14] Thirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez , L., Tan, T. F., and Ting, D. S. W., “Large\nlanguage models in medicine,” Nature Medicine , 1–11 (2023).\n[15] Le Glaz, A., Haralambous, Y., Kim-Dufor, D.-H., Lenca, P., Billot, R., Ry an, T. C., Marsh, J., Devylder,\nJ., Walter, M., Berrouiguet, S., et al., “Machine learning and natural l anguage processing in mental health:\nsystematic review,” Journal of Medical Internet Research 23(5), e15708 (2021).\n[16] Fu, G., Song, C., Li, J., Ma, Y., Chen, P., Wang, R., Yang, B. X., and Huang, Z ., “Distant supervision for\nmental health management in social media: suicide risk classiﬁcation s ystem development study,” Journal\nof medical internet research 23(8), e26119 (2021).\n[17] Singh, M., Jakhar, A. K., and Pandey, S., “Sentiment analysis on the i mpact of coronavirus in social life\nusing the bert model,” Social Network Analysis and Mining 11(1), 33 (2021).\n[18] Wan, F., “Sentiment analysis of weibo comments based on deep neural network,” in [ 2019 international\nconference on communications, information system and comput er engineering (CISCE) ], 626–630, IEEE\n(2019).\n[19] Zhang, X., Li, W., Ying, H., Li, F., Tang, S., and Lu, S., “Emotion detection i n online social networks: a\nmultilabel learning approach,” IEEE Internet of Things Journal 7(9), 8133–8143 (2020).\n[20] Wang, J., Zhou, Y., Zhang, W., Evans, R., and Zhu, C., “Concerns expre ssed by chinese social media users\nduring the COVID-19 pandemic: content analysis of sina weibo microbl ogging data,” Journal of medical\nInternet research 22(11), e22152 (2020).\n[21] Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., and Mc Hardy, R., “Challenges and applica-\ntions of large language models,” arXiv preprint arXiv:2307.10169 (2023).\n[22] Wei, J., Tay, Y., Bommasani, R., Raﬀel, C., Zoph, B., Borgeaud, S., Y ogatama, D., Bosma, M., Zhou, D.,\nMetzler, D., et al., “Emergent abilities of large language models,” arXiv preprint arXiv:2206.07682 (2022).\n[23] Liebrenz, M., Schleifer, R., Buadze, A., Bhugra, D., and Smith, A., “Generating scholarly content with\nChatGPT: ethical challenges for medical publishing,” The Lancet Digital Health 5(3), e105–e106 (2023).\n[24] Jeblick, K., Schachtner, B., Dexl, J., Mittermeier, A., St ¨ uber, A. T., Topalis, J., Weber, T., Wesp, P., Sabel,\nB., Ricke, J., et al., “ChatGPT makes medicine easy to swallow: An exp loratory case study on simpliﬁed\nradiology reports,” arXiv preprint arXiv:2212.14882 (2022).\n[25] Surameery, N. M. S. and Shakor, M. Y., “Use ChatGPT to solve programming b ugs,” International Journal\nof Information Technology & Computer Engineering (IJITC) ISSN: 24 55-5290 3(01), 17–22 (2023).\n[26] Kasneci, E., Seßler, K., K¨ uchemann, S., Bannert, M., Dement ieva, D., Fischer, F., Gasser, U., Groh,\nG., G¨ unnemann, S., H¨ ullermeier, E., et al., “Chatgpt for good? on opp ortunities and challenges of large\nlanguage models for education,” Learning and individual diﬀerences 103, 102274 (2023).\n[27] Yeo, Y. H., Samaan, J. S., Ng, W. H., Ting, P.-S., Trivedi, H., Vipani, A., Ay oub, W., Yang, J. D., Liran,\nO., Spiegel, B., et al., “Assessing the performance of chatgpt in answer ing questions regarding cirrhosis and\nhepatocellular carcinoma,” medRxiv , 2023–02 (2023).\n[28] Jiang, L. Y., Liu, X. C., Nejatian, N. P., Nasir-Moin, M., Wang, D., Abidin, A. , Eaton, K., Riina, H. A.,\nLaufer, I., Punjabi, P., et al., “Health system-scale language models are al l-purpose prediction engines,”\nNature , 1–6 (2023).\n[29] Farhat, F., “ChatGPT as a complementary mental health resource: a bo on or a bane,” Annals of Biomedical\nEngineering , 1–4 (2023).\n[30] Qin, W., Chen, Z., Wang, L., Lan, Y., Ren, W., and Hong, R., “Read, diagnose and chat: To-\nwards explainable and interactive LLMs-augmented depression detecti on in social media,” arXiv preprint\narXiv:2305.05138 (2023).\n14\n[31] Chen, S., Wu, M., Zhu, K. Q., Lan, K., Zhang, Z., and Cui, L., “LLM-emp owered chatbots for psychiatrist\nand patient simulation: Application and evaluation,” arXiv preprint arXiv:2305.13614 (2023).\n[32] Fu, G., Zhao, Q., Li, J., Luo, D., Song, C., Zhai, W., Liu, S., Wang, F., W ang, Y., Cheng, L., et al.,\n“Enhancing psychological counseling with large language model: A multifac eted decision-support system\nfor non-professionals,” arXiv preprint arXiv:2308.15192 (2023).\n[33] Ayers, J. W., Poliak, A., Dredze, M., Leas, E. C., Zhu, Z., Kelle y, J. B., Faix, D. J., Goodman, A. M.,\nLonghurst, C. A., Hogarth, M., et al., “Comparing physician and artiﬁcial inte lligence chatbot responses to\npatient questions posted to a public social media forum,” JAMA internal medicine (2023).\n[34] Yang, K., Ji, S., Zhang, T., Xie, Q., Kuang, Z., and Ananiadou, S., “Towar ds interpretable mental health\nanalysis with ChatGPT,” (2023).\n[35] Vaishya, R., Misra, A., and Vaish, A., “ChatGPT: Is this version good f or healthcare and research?,”\nDiabetes & Metabolic Syndrome: Clinical Research & Reviews 17(4), 102744 (2023).\n[36] Editorial, “Will ChatGPT transform healthcare?,” Nature Medicine , 505–506 (2023).\n[37] Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., et al.,\n“A survey on evaluation of large language models,” arXiv preprint arXiv:2307.03109 (2023).\n[38] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K., “BERT: Pre-t raining of deep bidirectional trans-\nformers for language understanding,” arXiv preprint arXiv:1810.04805 (2018).\n[39] Xiao, L., Huang, X., Chen, B., and Jing, L., “Label-speciﬁc document repr esentation for multi-label text\nclassiﬁcation,” in [ Proceedings of the 2019 conference on empirical methods in natura l language processing\nand the 9th international joint conference on natural language pr ocessing (EMNLP-IJCNLP) ], 466–475\n(2019).\n[40] Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zhen g, W., Xia, X., et al.,\n“GLM-130b: An open bilingual pre-trained model,” arXiv preprint arXiv:2210.02414 (2022).\n[41] OpenAI, “Introducing chatgpt,” (2023).\n[42] OpenAI, “Gpt-4 technical report,” (2023).\n[43] Wang, S., Sun, Y., Xiang, Y., Wu, Z., Ding, S., Gong, W., Feng, S., Shang, J ., Zhao, Y., Pang, C., et al.,\n“Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-trai ning for language understanding and\ngeneration,” arXiv preprint arXiv:2112.12731 (2021).\n[44] Burns, D. D., [ Feeling good], Signet Book (1981).\n15",
  "topic": "Social media",
  "concepts": [
    {
      "name": "Social media",
      "score": 0.5403223037719727
    },
    {
      "name": "Cognition",
      "score": 0.5126215815544128
    },
    {
      "name": "Transformative learning",
      "score": 0.5099538564682007
    },
    {
      "name": "Psychology",
      "score": 0.4989464282989502
    },
    {
      "name": "Cognitive psychology",
      "score": 0.48969319462776184
    },
    {
      "name": "Popularity",
      "score": 0.4381476938724518
    },
    {
      "name": "Computer science",
      "score": 0.3753211498260498
    },
    {
      "name": "Social psychology",
      "score": 0.3717026114463806
    },
    {
      "name": "Artificial intelligence",
      "score": 0.322334349155426
    },
    {
      "name": "Developmental psychology",
      "score": 0.15408837795257568
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210141120",
      "name": "Sorbonne University Abu Dhabi",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I37796252",
      "name": "Beijing University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    }
  ],
  "cited_by": 2
}