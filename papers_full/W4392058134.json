{
  "title": "GenAI against humanity: nefarious applications of generative artificial intelligence and large language models",
  "url": "https://openalex.org/W4392058134",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2147903670",
      "name": "Emilio Ferrara",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2147903670",
      "name": "Emilio Ferrara",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2804927761",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4380769213",
    "https://openalex.org/W2963145442",
    "https://openalex.org/W4388488349",
    "https://openalex.org/W4380887356",
    "https://openalex.org/W4391018561",
    "https://openalex.org/W2944176689",
    "https://openalex.org/W4385071300",
    "https://openalex.org/W4385452929",
    "https://openalex.org/W2335888457",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2953522645",
    "https://openalex.org/W3164113067",
    "https://openalex.org/W4223466411",
    "https://openalex.org/W4392023408",
    "https://openalex.org/W4384072523",
    "https://openalex.org/W4290033826",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W4386947678",
    "https://openalex.org/W4386276963",
    "https://openalex.org/W4375819566",
    "https://openalex.org/W4384434186",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W2072410439",
    "https://openalex.org/W2790166049",
    "https://openalex.org/W4399121933",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4251465078",
    "https://openalex.org/W3123655755"
  ],
  "abstract": "Abstract Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we’ll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI’s nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.",
  "full_text": "Vol.:(0123456789)\nJournal of Computational Social Science (2024) 7:549–569\nhttps://doi.org/10.1007/s42001-024-00250-1\n1 3\nSURVEY ARTICLE\nGenAI against humanity: nefarious applications \nof generative artificial intelligence and large language \nmodels\nEmilio Ferrara1 \nReceived: 10 November 2023 / Accepted: 21 January 2024 / Published online: 22 February 2024 \n© The Author(s) 2024\nAbstract\nGenerative Artificial Intelligence (GenAI) and Large Language Models (LLMs) \nare marvels of technology; celebrated for their prowess in natural language pro-\ncessing and multimodal content generation, they promise a transformative future. \nBut as with all powerful tools, they come with their shadows. Picture living in a \nworld where deepfakes are indistinguishable from reality, where synthetic identities \norchestrate malicious campaigns, and where targeted misinformation or scams are \ncrafted with unparalleled precision. Welcome to the darker side of GenAI applica-\ntions. This article is not just a journey through the meanders of potential misuse of \nGenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. \nAs we navigate the seas of misinformation campaigns, malicious content generation, \nand the eerie creation of sophisticated malware, we’ll uncover the societal implica-\ntions that ripple through the GenAI revolution we are witnessing. From AI-pow -\nered botnets on social media platforms to the unnerving potential of AI to gener -\nate fabricated identities, or alibis made of synthetic realities, the stakes have never \nbeen higher. The lines between the virtual and the real worlds are blurring, and the \nconsequences of potential GenAI’s nefarious applications impact us all. This article \nserves both as a synthesis of rigorous research presented on the risks of GenAI and \nmisuse of LLMs and as a thought-provoking vision of the different types of harmful \nGenAI applications we might encounter in the near future, and some ways we can \nprepare for them.\nKeywords AI · Generative AI · Large Language Models · Risks · Social media\n * Emilio Ferrara \n emiliofe@usc.edu\n https://scholar.google.com/citations?user=0r7Syh0AAAAJ\n1 Thomas Lord Department of Computer Science, University of Southern California, \nLos Angeles, CA 90007, USA\n550 Journal of Computational Social Science (2024) 7:549–569\n1 3\nIntroduction\nIn March 2019, a UK-based energy firm’s CEO was duped out of $243,000. The cul-\nprit? Not a seasoned con artist, but an AI-generated synthetic voice so convincingly \nmimicking the company’s German parent firm’s CEO that it led to a costly misstep \n(see Table 1A). This incident, while startling, is just the tip of the iceberg when it \ncomes to the nefarious potential applications of Generative Artificial Intelligence \n(GenAI) and Large Language Models (LLMs).\nGenAI and LLMs are transforming the landscape of natural language process-\ning, content generation, and understanding. Their potential seems endless, promis-\ning innovations that could redefine the way humans and machines interact with each \nother or partner to work together [10, 26]. However, lurking in the shadows of these \nadvancements are challenges that threaten the very fabric of our cybersecurity, eth-\nics, and societal structures [15].\nThis paper ventures into the darker alleys of GenAI, with a focus on LLMs. From \ntheir potential role in scaling up misinformation campaigns, to the creation of tar -\ngeted scams or custom-tailor-made alibies, the risks are profound [5]. From the \nsubtle perpetuation of biases to the blatant reinforcement of stereotypes [2, 6, 21], \nGenAI can become mirrors reflecting and amplifying the imperfections of our soci-\nety [1, 8].\nImagine a world where AI-powered botnets dominate social media [7, 29], where \nharmful or radicalizing content is churned out by algorithms [23], and where the \nlines between reality and AI-generated content blur [3]. A world where the same \ntechnology that can be used to restore lost pieces of art or ancient documents [4], \ncan also be used to fabricate evidence, craft alibis, and conceive the “perfect crime” \n(see Table 1B). Many of these scenarios that until recently we would have ascribed \nto futuristic science fiction are already enabled by GenAI and LLMs.\nAs we navigate the complexities of these issues, this paper highlights the urgency \nof robust mitigation strategies, ethical guidelines, and continuous monitoring for \nGenAI and LLM systems [9, 14]. This exploration not only aims to summarize rig-\norous research on GenAI abuse, but also to ignite a discourse on the dual nature of \nthese technologies.\nDefinition and mechanisms of generative AI and LLMs\nGenerative AI refers to artificial intelligence systems that can generate new content, \nincluding text, images, and audio, based on existing data [3 ]. Unlike traditional AI, \nwhich focuses on recognizing patterns or making predictions, GenAI actively creates \nnovel outputs. This involves complex algorithms and models that learn from large \ndatasets, recognize underlying structures, and emulate them in unique ways [4]. Large \nLanguage Models (LLMs), a subset of GenAI systems, specifically deal with textual \ndata [10]. They are trained on extensive corpora of text, learning language patterns, \nsyntax, and context. LLMs like GPT (Generative Pretrained Transformer) are capable \nof producing coherent, contextually relevant text, resembling human writing. Their \n551\n1 3Journal of Computational Social Science (2024) 7:549–569 \nmechanisms involve understanding input queries, accessing their extensive training \ndata, and generating appropriate textual responses, which can range from answering \nquestions to creating content.\nTechnological advancements and democratization\nThe democratization of Generative AI represents a pivotal change in AI technology. \nThe early 2020s period witnessed significant advancements in the technical capabili-\nties of GenAI, marked by improvements in machine learning algorithms, particularly in \nneural networks. These advancements led to the creation of more sophisticated and effi-\ncient models that are capable of understanding and generating complex data patterns.\nAt the same time, there was a marked decrease in the cost of developing and deploy-\ning GenAI systems. This was due to both the falling prices of computing power and the \nincreased availability of open-source tools and platforms, making GenAI accessible to \na wider range of users and developers. Furthermore, the proliferation of user-friendly \ninterfaces and cloud-based services has made GenAI technologies more accessible to \nnon-specialists. This broader access has catalyzed a wave of innovation and creativity \nacross various sectors, allowing individuals and smaller organizations to take advantage \nof GenAI for various applications, thus democratizing the field of artificial intelligence.\nTogether, these factors have differentiated the current landscape of GenAI from pre-\nvious generations of AI technologies, both in terms of technological sophistication and \nsocietal impact.\nRegulatory landscape\nThe European Union (EU) and China have actively engaged in discussions to regulate \nGenerative AI (GenAI). In the EU, the focus has been on establishing frameworks that \nensure the ethical use of AI, focusing on data privacy, transparency, and accountability. \nThe proposed regulations aim to categorize AI systems according to their risk levels \nand apply the corresponding oversight measures. China, on the other hand, has focused \non harnessing the potential of GenAI while safeguarding national security and social \nstability. The Chinese approach includes stringent data control measures and guide-\nlines to prevent the misuse of AI technologies, especially in areas like surveillance and \ncensorship.\nThese divergent approaches reflect the complexities and varying priorities in GenAI \ngovernance, illustrating the challenges in creating a universally accepted regulatory \nframework. The policies in these regions are likely to influence global standards and \npractices in the GenAI domain [14].\nUnderstanding GenAI abuse: a taxonomy\nFigure 1 offers an overview of the potential dangers associated with the misuse of \ngenerative AI models by charting the intersection between the type of harm that can \nbe inflicted and the underlying intentions of malicious actors.\n552 Journal of Computational Social Science (2024) 7:549–569\n1 3\nThe types of harm encompass threats to an individual’s personal identity, such as \nidentity theft, privacy breaches, or personal defamation, which we term as “Harm to \nthe Person.” Then, we have the potential for financial loss, fraud, market manipula-\ntion, and other economic harms, which fall under “Financial and Economic Dam-\nage.” The distortion of the information ecosystem, including the spread of misin-\nformation, fake news, and other forms of deceptive content [28], is categorized as \n“Information Manipulation.” Lastly, broader harms that can impact communities, \nsocietal structures, and critical infrastructures, including threats to democratic pro-\ncesses, social cohesion, and technological systems, are captured under “Societal, \nSocio-technical, and Infrastructural Damage.”\nOn the other side of the matrix, we have the goals (i.e., intent) of malicious \nactors. “Deception” involves misleading individuals or entities for various purposes, \nsuch as scams, impersonation, or other fraudulent activities [16]. “Propaganda” is \nthe intent to promote a particular political, ideological, or commercial agenda, often \nby distorting facts or manipulating emotions. And “Dishonesty” covers a range of \nactivities where the truth is concealed or misrepresented for personal gain, competi-\ntive advantage, or other ulterior motives. Naturally, this dimension does not fully \nencompass the goals or motivations behind all possible types of misuse of GenAI, \nbut it serves as a guide to frame nefarious applications with respect to their intent to \nharm.\nIn this 3 × 4 matrix, each cell represents a unique combination of harm and mali-\ncious intent, illustrating the multifaceted forms of abuse possible with generative \nAI. For instance, AI-generated impersonation for identity theft might be found at \nthe intersection of “Harm to the Person” and “Deception.” Similarly, AI-driven fake \nnews campaigns to influence public opinion could be represented at the crossroads \nof “Information Manipulation” and “Propaganda.”\nFig. 1  Charting the landscape of nefarious applications of generative artificial intelligence and large lan-\nguage models\n553\n1 3Journal of Computational Social Science (2024) 7:549–569 Table 1  News articles about nefarious GenAI and LLM applications\nRef. News title Media outlet URL\nA Fraudsters used AI to mimic CEO’s voice in unusual cybercrime \ncase\nWall Street Journal https:// www. wsj. com/ artic les/ fraud sters- use- ai- to- mimic- ceos- voice- \nin- unusu al- cyber crime- case- 11567 157402\nB People are creating records of fake historical events using AI Vice https:// www. vice. com/ en/ artic le/ k7zqdw/ people- are- creat ing- recor \nds- of- fake- histo rical- events- using- ai\nC ‘I don’t want to upset people’: Tom Cruise deepfake creator speaks \nout\nThe Guardian https:// www. thegu ardian. com/ techn ology/ 2021/ mar/ 05/ how- start ed- \ntom- cruise- deepf ake- tiktok- videos\nD Do these A.I.-created fake people look real to you? New York Times https:// www. nytim es. com/ inter active/ 2020/ 11/ 21/ scien ce/ artifi  cial- \nintel ligen ce- fake- people- faces. html\nE Generative AI: A blessing or a curse for cybersecurity? InWeb3 https:// www. inweb3. com/ gener ative- ai-a- bless ing- or-a- curse- for- \ncyber secur ity/\nF Real-world AI threats in cybersecurity aren’t science fiction VentureBeat https:// ventu rebeat. com/ ai/ real- world- ai- threa ts- in- cyber secur ity- \narent- scien ce- ficti on/\nG AI amplifies scam calls and other deceptions Marketplace https:// www. marke tplace. org/ 2023/ 07/ 14/ ai- ampli fies- scam- calls- \nand- other- decep tions/\nH Scammers use AI to mimic voices of loved ones in distress CBS News https:// www. cbsne ws. com/ news/ scamm ers- ai- mimic- voices- loved- \nones- in- distr ess\nI Fake or fact? The disturbing future of AI-generated realities forbes https:// www. forbes. com/ sites/ berna rdmarr/ 2023/ 07/ 27/ fake- or- fact- \nthe- distu rbing- future- of- ai- gener ated- reali ties\nJ Disinformation researchers raise alarms about A.I. chatbots New York Times https:// www. nytim es. com/ 2023/ 02/ 08/ techn ology/ ai- chatb ots- disin \nforma tion. html\nK GPT-4 produces misinformation more frequently, and more per-\nsuasively, than its predecessor\nNewsGuard https:// www. newsg uardt ech. com/ misin forma tion- monit or/ march- \n2023/\nL The age of AI surveillance is here Quartz https:// qz. com/ 10606 06/ the- age- of- ai- surve illan ce- is- here\nM The biggest threat of deepfakes isn’t the deepfakes themselves MIT Technology Review https:// www. techn ology review. com/ 2019/ 10/ 10/ 132667/ the- bigge \nst- threat- of- deepf akes- isnt- the- deepf akes- thems elves/\nN Mushroom pickers urged to avoid foraging books on Amazon that \nappear to be written by AI\nThe Guardian https:// www. thegu ardian. com/ techn ology/ 2023/ sep/ 01/ mushr oom- \npicke rs- urged- to- avoid- forag ing- books- on- amazon- that- appear- to- \nbe- writt en- by- ai\n554 Journal of Computational Social Science (2024) 7:549–569\n1 3\nTable 2 summarizes proof-of-concept examples of scenarios in which GenAI and \nLLMs can be intentionally misused for dishonest, propagandist, or deceiving pur -\nposes. By understanding this framework, stakeholders can better anticipate potential \nthreats and devise specific mitigation strategies to protect against the malicious use \nof generative AI.\nA glimpse into days of future past\nPretend for a moment that you were Tom Cruise, and on a day like any other (back \nin 2021) you tap into your social media feed just to see videos of yourself playing \ngolf and prat-falling around your home (see Table  1C). What would your reaction \nbe if you never actually recorded and posted those videos? The malicious use of \ntechnological advancements is barely news: each new powerful technology comes \nwith abuse. The problem of tampered footage or photoshopped multimedia is not \nnew, but GenAI and deepfake technologies have brought about a wealth of new chal-\nlenges [22].\nThe ability to create deepfakes, provide plausible deniability, and spread sub-\nliminal messages or deceiving content makes GenAI a potent tool in the hands of \nmalicious actors. Let us unpack some of the most salient nefarious applications of \nGenAI technologies. Figure 2 provides a map of such plausible and known applica-\ntions. In Table  3, we summarized several proof-of-concept examples of scenarios \nwhere GenAI and LLMs can be abused to cause personal and financial harm to peo-\nple, distort the information ecosystem, and manipulate sociotechnical systems and \ninfrastructures.\nThe rise of deepfakes\nGenAI can produce images of people that look very real, as if they could be seen on \nplatforms like Facebook, Twitter, or Tinder. Although these individuals do not exist \nin reality, these synthetic identities are already being used in malicious activities \n(see Table 1D).\nAI‑generated faces\nThere are businesses that offer “fake people” for purchase. For instance, on the web-\nsite Generated.Photos, one can buy a “unique, worry-free” fake person for $2.99 or \neven 1000 people for $1000. If someone needs a few fake individuals, perhaps for \na video game or to diversify a company website, they can obtain their photos for \nfree from ThisPersonDoesNotExist.com. There is even a company named Rosebud.\nAI that can animate these fake personas and make them talk (the stated goal is for \ngames and art, but the technology can be easily abused).\n555\n1 3Journal of Computational Social Science (2024) 7:549–569 Table 2  Examples of intentional malicious deployments of LLMs and GenAI in the real world\nGoal Application Example Proof-of-concept\nDishonesty Automated essay writing and academic dishonesty Students could use LLMs to generate essays, \nresearch papers, or assignments, bypassing the \nlearning process and undermining academic \nintegrity\nInputting a prompt like “Write a 2000-word essay \non the impact of the Industrial Revolution on \nEuropean society” into an LLM and receiving a \ndetailed, well-structured essay in return\nGenerating fake research papers LLMs can be used to produce fake research papers \nwith fabricated data, results, and references, \npotentially polluting academic databases or \nmisleading researchers\nFeeding an LLM a prompt such as “Generate a \nresearch paper on the effects of a drug called \n‘Zyphorin’ on Alzheimer’s disease” and obtaining \na seemingly legitimate paper\nPropaganda Impersonating celebrities or public figures LLMs can generate statements, tweets, or mes-\nsages that mimic the style of celebrities or public \nfigures, leading to misinformation or defamation\nInputting “Generate a tweet in the style of [Celeb-\nrity Name] discussing climate change” and getting \na fabricated tweet that appears genuine\nAutomated propaganda generation Governments or organizations could use LLMs to \nproduce propaganda material at scale, targeting \ndifferent demographics or regions with tailored \nmessages\nInputting “Generate a propaganda article promot-\ning the benefits of a fictional government policy \n‘GreenFuture Initiative”’ and receiving a detailed \narticle\nCreating Fake Historical Documents or Texts LLMs can be used to fabricate historical docu-\nments, letters, or texts, potentially misleading \nhistorians or altering public perception of events\nPrompting an LLM with “Generate a letter from \nNapoleon Bonaparte to Josephine discussing his \nstrategies for the Battle of Waterloo” to produce a \nfabricated historical document\n556 Journal of Computational Social Science (2024) 7:549–569\n1 3\nTable 2  (continued)\nGoal Application Example Proof-of-concept\nDeception Generating fake product reviews Businesses could use LLMs to generate positive \nreviews for their products or negative reviews for \ncompetitors, misleading consumers\nInputting “Generate 10 positive reviews for a \nfictional smartphone brand ‘NexaPhone”’ and \nobtaining seemingly genuine user reviews\nGenerating realistic but fake personal stories or \ntestimonies\nLLMs can be used to craft personal stories or \ntestimonies for use in deceptive marketing, false \nlegal claims, or to manipulate public sentiment\nInputting “Generate a personal story of someone \nbenefiting from a fictional health supplement \n‘VitaBoost”’ to obtain a convincing but entirely \nfabricated testimony\nCrafting convincing scam emails LLMs can be used to craft highly personalized \nscam emails that appear to come from legitimate \nsources, such as banks or service providers\nFeeding the model information about a fictional \nuser and a prompt like “Generate an email from \na bank notifying the user of suspicious account \nactivity” to produce a scam email\nCrafting legal documents with hidden clauses Unscrupulous entities could use LLMs to generate \nlegal documents that contain hidden, misleading, \nor exploitative clauses\nPrompting an LLM with “Generate a rental agree-\nment that subtly gives the landlord the right to \nincrease rent without notice” to produce a decep-\ntive legal document\n557\n1 3Journal of Computational Social Science (2024) 7:549–569 \nUse of synthetic personas\nAI-generated identities are beginning to appear on the Internet and are being used \nby real people with malicious intentions. Examples include spies using attrac-\ntive faces to infiltrate intelligence communities, right-wing propagandists hid-\ning behind fake profiles, and online harassers using a friendly face to troll their \ntargets.\nThe perfect alibi: plausible deniability and attribution problems\nThe ability to generate fictitious images and videos can not only lend itself to \nabuse such as deepfake-fueled non-consensual porn generation, or the creation of \nmisinformation for the sake of harassment or slander. Researchers are concerned \nthat the same technologies could be used to construct alibis or fabricate criminal \nevidence in scalable and inexpensive ways [25]. Generative AI poses potential \nthreats, especially in the realm of generating fake evidence or alibis. An article \npublished by InWeb3 put it best in words (see Table 1 E):\n“These possibilities undermine trust, credibility, and accountability. They \ncreate plausible deniability, the ability to deny responsibility or involvement \nin an action, by generating fake evidence or alibis. They also create attribu-\ntion problems, the difficulty of identifying the source or origin of an action, \nby generating fake identities or locations. Ethical dilemmas also arise, the \nconflict between moral principles or values, by generating content that vio-\nlates human rights or norms.”\nGenAI against the people\nThe potential threats posed by GenAI in the realm of cybersecurity include ad \nhominem attacks [11], such as automated online harassment and personalized \nscams (see Table 1 F).\nFig. 2  Mind map of abuse and malicious applications of GenAI and large language models\n558 Journal of Computational Social Science (2024) 7:549–569\n1 3\nTable 3  Proof-of-concept Scenarios Highlighting the Potential for Different Type of Harms in Malicious GenAI Applications\nHarm Application Example Proof-of-concept\nInfo. manipulation Automated social media manipulation LLMs can be used to operate multiple social \nmedia accounts, creating an illusion of \ngrassroots movements or artificially ampli-\nfying certain narratives\nDeploying an LLM to manage hundreds of \nTwitter accounts, all pushing a specific \npolitical agenda or spreading misinformation \nabout a public health issue\nGenerating fake medical advice or informa-\ntion\nLLMs can produce misleading medical infor-\nmation, potentially endangering individuals \nwho might act on this false advice\nAsking an LLM to “Provide natural remedies \nfor a heart condition” and receiving poten-\ntially harmful or ineffective suggestions\nCrafting deceptive advertisements LLMs can be used to generate advertise-\nments that exaggerate product capabilities \nor make false claims\nInputting “Create an advertisement for a fic-\ntional skincare product that provides instant \nresults” and obtaining a misleading ad that \npromises unrealistic outcomes\nFinancial harm Creating fake financial reports or data LLMs can be used to generate false financial \ndata or reports, potentially misleading \ninvestors or manipulating stock prices\nPrompting an LLM with “Generate a quarterly \nfinancial report for a fictional tech company \n‘TechNova’ showing a X% profit increase” \nto obtain a detailed but fabricated financial \ndocument\nGenerating scripts for scam calls LLMs can produce scripts for scam calls, \nmaking them sound more genuine and \nincreasing the likelihood of deceiving \nindividuals\nAsking an LLM to “Create a script for a call \nclaiming to be from the IRS, notifying the \nrecipient of unpaid taxes” to produce a \nconvincing scam script\n559\n1 3Journal of Computational Social Science (2024) 7:549–569 Table 3  (continued)\nHarm Application Example Proof-of-concept\nPersonal and identity harm Fake personal profiles and identities LLMs can craft detailed personal profiles, \ncomplete with background stories, for use \nin scams, catfishing, or espionage\nPrompting an LLM with “Generate a detailed \nprofile of a fictional journalist named ‘Alexa \nMorgan”’ and receiving a comprehensive \nbackstory, educational history, and career \nachievements\nAutomated online harassment LLMs can be deployed to target individu-\nals online, sending them personalized and \nharmful messages at scale\nUsing an LLM to manage multiple online \naccounts that continuously post derogatory \ncomments on a specific individual’s social \nmedia posts\nGenerating fake evidence or alibis LLMs can craft detailed narratives or digital \ncontent that serve as false evidence or \nalibis in legal cases\nAsking an LLM to “Provide a detailed alibi for \nsomeone claiming to be at a conference in \nBoston from June 1–5, 2023” and receiving \na comprehensive itinerary, complete with \nfictional events and interactions\nTecno-social harm Fake technical support scams LLMs can be used to generate scripts or \nguides that mislead individuals into think-\ning they’re receiving legitimate technical \nsupport, leading them to compromise their \ndevices or data\nPrompting an LLM with “Create a guide for \nfixing a computer virus” and obtaining a \nguide that, instead, instructs users to down-\nload malicious software\nGenerating biased or prejudiced content LLMs, if not properly fine-tuned, can pro-\nduce content that reflects societal biases, \npotentially perpetuating stereotypes or \nprejudice\nAsking an LLM about descriptions of different \ncultures or groups and receiving outputs that \ncontain biased or stereotypical information\n560 Journal of Computational Social Science (2024) 7:549–569\n1 3\nAI against users\nThe primary targets of AI-powered attacks are not just vulnerable systems, but \nalso human users behind those systems. AI technology can scrape personal iden-\ntifiable information (PII) and gather social media data about potential victims. \nThis enhanced data collection can help criminals craft more detailed and convinc-\ning social engineering efforts than traditional human attackers.\nBespoke spear phishing\nWhile “phishing” involves generic email lures, “spear phishing” involves collecting \ndata on a target and crafting a personalized email [12]. Historically, spear phishing \nwas primarily used against governments and businesses. However, with AI tools that \ncan scrape data from various sources, spear phishing will become more common \nand more effective.\nAutomated harassment\nBeyond data theft and blackmail, GenAI can be used for automated harassment. \nCybercriminals, as well as individuals with malicious intent, can use GenAI tech-\nnology to launch harassment campaigns that result in service disruptions, ruined \nreputations, or more traditional forms of online harassment. Victims could range \nfrom businesses to private individuals or public figures. Tactics might include the \ncreation of fake social media accounts used to spread lies or automated phone calls \nusing voice over IP (VoIP) services. The automation of harassment processes could \ncreate a relentless and potentially untraceable campaign against victims.\nFake people, real consequences\nThe use of LLMs in conjunction with other GenAI tools can bring to life synthetic \npersonas used for scams, swindles, and other deceptions (see Table 1G).\nFake users, real money scams\nGenAI can be used to scale up the generation of synthetic personal data, including \nfake accounts and fake transactions (see Table  1G). For example, JPMorgan Chase \ndiscovered that its acquisition of a college financial aid platform included numer -\nous fictitious accounts. The platform was believed to contain 4.25 million customer \naccounts, but the bank later found that only 300,000 were legitimate. The platform \nvendor allegedly hired a data scientist to fabricate the majority of the accounts. Sim-\nilarly, Wells Fargo faced penalties when it was revealed that employees had opened \nat least 3.5 million new accounts using data from existing customers without their \nconsent. By creating fake PINs and email addresses, funds were transferred from \nlegitimate to fraudulent accounts. Fake accounts have also been a problem in the \nsocial media and online retail sectors, leading to issues like spamming, fake reviews, \n561\n1 3Journal of Computational Social Science (2024) 7:549–569 \nand user-spoofing-powered fraud. For instance, PayPal disclosed that it believed 4.5 \nmillion of its accounts were not legitimate and possibly fraudulent.\nKidnapped by a bot?\nGenerative AI can copy voices and likenesses, making it possible for individuals to \nappear as if they are saying or doing almost anything. This technology is similar to \n“deepfake” videos but applies to voices.\nAI-generated voices in scams: AI-generated voices are being used to enhance \nscams, making them more convincing (see Table  1H). For instance, people have \nreceived calls from what sounds like a relative asking for money, but the voice was \ngenerated by artificial intelligence as part of a fraudulent scheme.\nVoice spoofing and ransom: Threat actors can easily obtain a few seconds of \nsomeone’s voice from social media or other audio sources and use generative AI \nto produce entire scripts of whatever they want that person to say. This has led to \nscams in which children appear to call their parents asking for a wire transfer for \nransom (see Table 1I).\nVoice authentication: AI can be used to bypass voice authentication systems. For \nexample, some financial services companies allow users to download information \nbased on voice recognition. AI can potentially be used to mimic these voices and \ngain unauthorized access.\nOpening the floodgates to disinformation\nLLMs have the ability to craft persuasive content that can parrot false narratives \nand conspiracy theories, effectively and at scale (see Table  1J). Some concerned \nresearchers recently described Large Language Models like ChatGPT as weapons \nof mass deception [24]. It seems undeniable that the potential for GenAI and LLMs \nto craft fictitious, nonfactual, inaccurate, or deceiving content is unparalleled [17].\nLLMs and disinformation\nSoon after the launch of ChatGPT, researchers tested its ability to produce content \nbased on questions filled with conspiracy theories and false narratives. The AI-gen-\nerated content was so convincing that Gordon Crovitz, a co-chief executive of News-\nGuard (a company that tracks online misinformation), stated, “This tool is going to \nbe the most powerful tool for spreading misinformation that has ever been on the \nInternet.”\nChatGPT’s capabilities\nChatGPT can produce convincing content rapidly without revealing its sources. \nWhen supplied with disinformation-loaded questions, it can generate clean varia-\ntions of the content en masse within seconds. When researchers from NewsGuard \nasked ChatGPT to produce content based on false narratives, the AI complied about \n562 Journal of Computational Social Science (2024) 7:549–569\n1 3\n80% of the time (see Table 1K). For instance, when asked to write from the perspec-\ntive of conspiracy theorist Alex Jones about the Parkland shooting, ChatGPT pro-\nduced content that falsely claimed the mainstream media and the government used \n“crisis actors” to push a gun-control agenda.\nAll systems down\nYet, the potential misuse of GenAI could have its most catastrophic consequences \nwhen looking at socio-technical systems and infrastructures. When deployed at a \nplanetary scale, GenAI’s influence extends beyond mere technological advance-\nments: it has the potential to profoundly impact the very foundations of our econ-\nomy, democracy, and infrastructure. Targeted surveillance, censorship, and synthetic \nrealities have been topics of concern in research community.\nHyper‑targeted surveillance\nEnhanced by GenAI, surveillance capabilities, such as facial recognition systems, \ncan reach unprecedented levels of accuracy. When integrated with other individual \ninformation and online data, these systems could not only recognize but also predict \nindividual behaviors. Such advancements, while promising in the context of secu-\nrity, raise alarming concerns about privacy and individual rights. We may be soon \nbe entering an age of ubiquitous GenAI-driven surveillance (see Table 1L).\nTotal information control\nThe intersection of GenAI with content moderation and censorship poses signifi-\ncant challenges to democratic values [30]. While LLMs can efficiently detect and \nremove harmful content from digital platforms, the potential for misuse, especially \nby authoritarian regimes, is concerning. The risk of suppressing dissenting voices \nand curating a single narrative threatens the very essence of democracy.\nEntirely synthetic realities\nIn the era of synthetic realities–augmented reality (AR), virtual reality (VR), and the \nexpansive metaverse–Generative Artificial Intelligence (GenAI) stands as a power -\nful architect. With its capability to craft intricate and indistinguishable virtual envi-\nronments, GenAI has the potential to redefine our perception of reality itself. How -\never, this transformative power is not without its pitfalls. As these synthetic realities \nbecome increasingly immersive and indistinguishable from our physical world, there \nlies a profound risk of manipulation. Unscrupulous entities could exploit GenAI-\ngenerated environments to influence individuals’ beliefs, emotions, and behaviors. \nFrom subtly altering virtual advertisements to resonate more with individual prefer -\nences, to creating entire virtual narratives that push specific agendas or ideologies, \nthe potential for psychological and behavioral manipulation is vast. As we embrace \nthe wonders of synthetic realities, it becomes imperative to remain vigilant, ensuring \n563\n1 3Journal of Computational Social Science (2024) 7:549–569 \nthat the line between the virtual and the real remains discernible, and that our agency \nwithin these realms is preserved.\nSystemic aberrations\nLastly, the ability of GenAI to manipulate public opinion can have cascading effects \non planetary scale systems. From influencing stock markets to swaying election out-\ncomes, the ramifications are vast and varied. In conclusion, as we navigate the intri-\ncate landscape of GenAI, it is imperative to recognize its massive scale implications. \nWhile the opportunities are immense, the challenges are equally daunting. Address-\ning the ethical, security, and societal concerns associated with GenAI is not just a \ntechnological endeavor but a global responsibility.\nDual nature technologies: GenAI’s double‑edged sword\nGenAI systems, with their ability to generate content, simulate voices, and even rec-\nreate historical artifacts, have opened up a plethora of opportunities across various \nsectors. However, with great power comes great responsibility, and the dual nature \nof these technologies necessitates a comprehensive understanding of their risks and \nbenefits. Table 4 illustrates a few application scenarios where a cost-benefit analysis \nshould inform whether the opportunity created by using GenAI far outweighs the \npotential danger and risks that it will enable.\nTake, for instance, the restoration of historical artifacts. Generative AI has shown \npromise in recreating or restoring damaged historical artifacts and paintings, breath-\ning new life into our shared cultural heritage [4]. Museums and historians can lever-\nage this technology to provide a more immersive experience for visitors, allowing \nthem to witness history in its full glory. Yet, the same capability can be misused to \nfabricate fake artifacts, misleading historians and collectors, or attempting to rewrite \nor distort our understanding of the past. Similarly, scalable and cheap creation of \nfake identities, fabricated documentation, or fraudolent evidence, might be enabled \nby GenAI’s ability to create seemingly legitimate documents, whose quality might \nmatch that of costly custom-made fakes (see Table 5, left figure). The medical field, \ntoo, is not immune to the double-edged sword of generative AI. While AI-generated \nmedical images can provide invaluable training resources for medical students with-\nout compromising patient privacy [20], the potential for fabricating medical images \nposes risks of misdiagnoses, fraudulent research, and insurance scams.\nThe realm of personalized content generation offers both promise and danger. \nOn the one hand, GenAI-driven curation can enhance user experiences on stream-\ning platforms, tailoring content to individual preferences, and ensuring more enjoy -\nable and bespoke experiences. On the other hand, this personalization can be weap-\nonized to spread misinformation or propaganda, manipulating individual beliefs \nand behaviors to serve malicious agendas. Take the DeepTomCruise example from \nearlier: Although these particular videos were relatively harmless, the proof-of-\nconcept highlighted the potential misuse in more sensitive areas like politics. There \nhave been concerns that deepfakes are being used to create fake endorsements or to \n564 Journal of Computational Social Science (2024) 7:549–569\n1 3\nTable 4  Antithetic scenarios demonstrating the dual nature of GenAI’s capabilities\nScenario Opportunity Danger\nFabrication of historical artifacts GenAI can be used to recreate or “restore” historical artifacts or paintings The danger lies in the potential misuse of this capability to create fake histori-\ncal artifacts and sell them as genuine, misleading historians and collectors\nPersonalized content generation GenAI can curate content tailored to individual preferences, enhancing user \nexperience on platforms like streaming services or online shopping sites\nThe same technology can be exploited to create hyper-targeted misinfor-\nmation or propaganda campaigns, manipulating individuals’ beliefs or \nbehaviors\nVoice synthesis and cloning GenAI can be used to recreate voices of historical figures or digital assistive \ncaretakers, allowing for unique educational or therapeutic experiences\nThis capability can be misused to generate fake audio recordings, leading to \nscams, misinformation, or even potential security breaches\nVoice-based services LLMs can enhance voice-based services, providing users with natural and \nengaging interactions\nLLMs, when combined with voice synthesis tools, can be used for scam calls, \ngenerating scripts that sound convincing\nMedical image generation GenAI can generate medical images for training and educational purposes, \nproviding medical students with diverse cases without compromising \npatient privacy\nThe technology can be exploited to fabricate medical images, leading to \nmisdiagnoses, fraudulent research, or insurance scams\nVR and AR enhancements GenAI can enhance VR and AR experiences, making them more immersive \nand realistic for education, training, or entertainment\nMisuse can lead to the creation of manipulated realities that distort historical \nevents, spread false information, or even create harmful psychological \nexperiences\nLanguage translation GenAI can break down language barriers, allowing for real-time translation \nand fostering global communication\nIt can be misused to generate misleading translations with the intent of caus-\ning misunderstandings, conflicts, or spreading fabricated narratives\nAutomated social media content LLMs can be used to automate content generation for businesses on social \nmedia, ensuring consistent engagement and timely responses to user \nqueries\nLLMs can be deployed to operate multiple social media accounts, creating \nan illusion of grassroots movements or artificially amplifying certain nar-\nratives\nMedical information LLMs can assist in providing general medical information to users, helping \nspread awareness about common health issues and preventive measures\nLLMs can produce misleading medical information, potentially endangering \nindividuals who might act on this false advice\nAdvertisements LLMs can assist businesses in crafting engaging advertisements and \ndetailed product descriptions\nLLMs can be used to generate deceptive advertisements that exaggerate \nproduct capabilities or make false claims\nFinancial reports LLMs can assist financial analysts in generating reports, offering insights \ninto market trends and predictions\nLLMs can be used to generate false financial data or reports, misleading \ninvestors or manipulating stock prices\n565\n1 3Journal of Computational Social Science (2024) 7:549–569 \nspread misinformation during election campaigns (see Table 1M). Digital book mar-\nketplaces have been flooded by AI-generated books, many of which lack any basic \nfact-checking and quality assurance, sometimes providing dangerous (i.e., halluci-\nnated [13]) information to an inattentive reader (see Table 1N). GenAI could be used \nto depict never-occurred events with serious public diplomacy consequences (see \nTable 5, center figure). Furthermore, these technologies are sufficiently advanced to \nimplement old-school infuence strategies like the injection of subliminal messages, \nin a fully automated way (see Table 5, right figure).\nRecommendations\nIn the box “Recommendations to Mitigate GenAI Abuse”, we provide a non-exhaus-\ntive list of plausible technical and socio-technical approaches that might help in mit-\nigating GenAI abuse. It should be noted that most of these approaches would work \nfor complying actors, in other words, entities who are willing to comply with regula-\ntions, rather than having mischievous or illicit intents [19]. Given the inherent com-\nplexities, a robust risk mitigation strategy is imperative. This involves continuous \nmonitoring of AI output, the establishment of ethical guidelines for the implemen-\ntation of GenAI, and the promotion of transparency in AI-driven processes. Stake-\nholders must also be educated about the potential pitfalls of generative AI, ensuring \ninformed decision-making at every step.\nFurthermore, a comprehensive risk–benefit analysis should be conducted before \ndeploying any generative AI system. This analysis should weigh the potential advan-\ntages against the possible harms, considering both short-term and long-term impli-\ncations. Only by understanding and addressing these challenges head-on can we har-\nness the full potential of generative AI while safeguarding our societal values and \nnorms.\nIn conclusion, as we navigate the intricate tapestry of generative AI, a balanced \napproach that recognizes both its transformative potential and inherent risks is \ncrucial. By adopting proactive risk mitigation strategies and conducting thorough \nTable 5  (L) From fake IDs to synthetic identities, GenAI can foster a boom of fabricated documents and \npersonas (source: Superbad ©). (C) MidJourney v5 is already capable of generating lifelike depictions of \nnever-occurred events (prompt: president biden and supreme leader of iran shaking hands). (R) Sublimi-\nnal messages can be incorporated into generated content (the optical illusion reads OBEY)\n\n566 Journal of Computational Social Science (2024) 7:549–569\n1 3\nrisk-benefit analyses, we can ensure that generative AI serves as a force for good, \ndriving innovation while preserving the integrity of our digital and physical worlds.\nRecommendations to mitigate GenAI abuse proof of identity: Proof of identity \nrefers to the verification of an individual’s or entity’s identity using specific docu-\nments or digital methods. Examples include technologies like humanID, OpenID, \nnext-generation CAPTCHAs [27], etc. In GenAI:\n• Proof of Identity can ensure that AI-generated content or actions can be traced \nback to a legitimate source.\n• Methods might include multi-factor authentication, biometric verification, or \ndigital certificates.\nAuthentication protocols: Authentication protocols are processes or systems \nused to confirm the identity of an individual, system, or entity. In the context of \nGenAI:\n• These protocols can verify whether content, actions, or requests generated by \nan AI system are legitimate [18].\n• Methods can include blockchain-based authentication, token-based systems, \nor cryptographic methods.\nAudience disclaimers: Audience disclaimers are explicit notifications provided to \naudiences to inform them about the nature of the content they are consuming.\n• For AI-generated content, it’s crucial to inform audiences that what they’re \nviewing, reading, or listening to was produced by an algorithm [18].\n• This promotes transparency and allows consumers to critically assess the con-\ntent.\nContent labeling: Content labeling involves tagging content to indicate its nature, \nsource, or other relevant attributes.\n• AI-generated content can be labeled to distinguish it from human-generated, \nensuring users are aware of its origin.\n• Labels can be visual tags, metadata, or even auditory cues.\nSource verification and provenance: Source verification is the process of confirm-\ning the authenticity and origin of a piece of information or content.\n• Provenance refers to the chronology of the ownership, custody, or location of \nan item or piece of content.\n• In GenAI, ensuring the provenance of data or content helps in maintaining \nits integrity and trustworthiness. Blockchain technology, for instance, can be \nused to trace the provenance of AI-generated content.\n567\n1 3Journal of Computational Social Science (2024) 7:549–569 \nDigital watermarking: Digital watermarking involves embedding a digital sig-\nnal or pattern into data, making it possible to verify its authenticity or detect \ntampering.\n• For AI-generated content, watermarking can help in identifying and distinguish-\ning it from human-generated content.\n• It provides a layer of security and traceability, ensuring that any alterations to the \noriginal content can be detected.\nConclusions\nLarge Language Models (LLMs) and other Generative Artificial Intelligence \n(GenAI) systems have emerged as a transformative force offering unprecedented \ncapabilities in natural language processing and multimodal content generation, and \nunderstanding. While the potential benefits of these technologies are vast, their rapid \nproliferation has created a myriad of malicious applications that pose significant \nthreats to cybersecurity, ethics, and society at large.\nThis paper explores the darker side of generative AI applications, with a special \nemphasis on LLMs. We discuss potential misuse in misinformation campaigns, the \ngeneration of malicious content that can bypass traditional security filters, and the \ncreation of sophisticated malware, including the use of LLMs as intermediaries for \nmalware attacks. We then examine the societal implications of GenAI and LLMs, \nfrom their role in AI-powered botnets on social media to their potential to generate \nharmful or radicalizing content.\nOur findings underscore the pressing need for robust mitigation strategies, ethi-\ncal guidelines, and continuous monitoring to ensure the responsible deployment \nand use of GenAI and LLMs. Our aim is to raise awareness of the dual-edge nature \nof GenAI and LLMs, and to advocate for a balanced approach that harnesses their \ncapabilities while safeguarding against their nefarious applications.\nAcknowledgements Work supported in part by DARPA (contract #HR001121C0169).\nFunding Open access funding provided by SCELC, Statewide California Electronic Library Consortium.\nData availibility No data was used in this work.\nDeclarations \nConflict of interest The author declares no conflict of interest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \n568 Journal of Computational Social Science (2024) 7:549–569\n1 3\ndirectly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\nReferences\n 1. Baeza-Yates, R. (2018). Bias on the web. Communications of the ACM, 61(6), 54–61.\n 2. Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language cor-\npora contain human-like biases. Science, 356(6334), 183–186.\n 3. Cao, Y., Li, S., Liu, Y., Yan, Z., Dai, Y., Yu, P. S., & Sun, L. (2023). A comprehensive survey of AI-gen-\nerated content (AIGC): A history of generative AI from GAN to ChatGPT. arXiv preprint. arXiv: 2303. \n04226 v1 [cs.AI]\n 4. Epstein, Z., Hertzmann, A., Investigators of Human Creativity, Akten, M., Farid, H., Fjeld, J., Frank, \nM. R., Groh, M., Herman, L., Leach, N., et al. (2023). Art and the science of generative AI. Science, \n380(6650), 1110–1111.\n 5. Ferrara, E. (2019). The history of digital spam. Communications of the ACM, 62(8), 82–91.\n 6. Ferrara, E. (2023). Should ChatGPT be biased? Challenges and risks of bias in large language models. \nFirst Monday, 28(11).\n 7. Ferrara, E. (2023). Social bot detection in the age of ChatGPT: Challenges and opportunities. First Mon-\nday, 28(6).\n 8. Ferrara, E. (2024). The butterfly effect in artificial intelligence systems: Implications for AI bias and fair-\nness. Machine Learning with Applications, 15, 100525.\n 9. Floridi, L. (2019). Establishing the rules for building trustworthy AI. Nature Machine Intelligence, 1(6), \n261–262.\n 10. Fui-Hoon Nah, F., Zheng, R., Cai, J., Siau, K., & Chen, L. (2023). Generative AI and ChatGPT: Appli-\ncations, challenges, and AI-human collaboration. Journal of Information Technology Case and Appli-\ncation Research, 25(3), 277–304.\n 11. Gupta, M., Akiri, C., Aryal, K., Parker, E., & Praharaj, L. (2023). From ChatGPT to ThreatGPT: \nImpact of generative AI in cybersecurity and privacy. IEEE Access, 11, 80218–80245.\n 12. Jagatic, T. N., Johnson, N. A., Jakobsson, M., & Menczer, F. (2007). Social phishing. Communications \nof the ACM, 50(10), 94–100.\n 13. Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., & Fung, P. (2023). \nSurvey of hallucination in natural language generation. ACM Computing Surveys, 55(12), 1–38.\n 14. Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature \nMachine Intelligence, 1(9), 389–399.\n 15. Köbis, N., Bonnefon, J.-F., & Rahwan, I. (2021). Bad machines corrupt good morals. Nature Human \nBehaviour, 5(6), 679–685.\n 16. Kshetri, N. (2022). Scams, frauds, and crimes in the nonfungible token market. Computer, 55(4), \n60–64.\n 17. Mazurczyk, W., Lee, D., & Vlachos, A. (2024). Disinformation 2.0 in the age of AI: A cybersecurity \nperspective. arXiv preprint arXiv: 2306. 05569.\n 18. Menczer, F., Crandall, D., Ahn, Y.-Y., & Kapadia, A. (2023). Addressing the harms of AI-generated \ninauthentic content. Nature Machine Intelligence, 2023, 1–2.\n 19. Mozes, M., He, X., Kleinberg, B., & Griffin, L. D. (2023). Use of LLMs for illicit purposes: Threats, \nprevention measures, and vulnerabilities. arXiv: 2308. 12833.\n 20. Ricci Lara, M. A., Echeveste, R., & Ferrante, E. (2022). Addressing fairness in artificial intelligence for \nmedical imaging. Nature Communications, 13(1), 4581.\n 21. Schramowski, P., Turan, C., Andersen, N., Rothkopf, C. A., & Kersting, K. (2022). Large pre-trained \nlanguage models contain human-like biases of what is right and wrong to do. Nature Machine Intel-\nligence, 4(3), 258–268.\n 22. Seymour, M., Riemer, K., Yuan, L., & Dennis, A. R. (2023). Beyond deep fakes. Communications of \nthe ACM, 66(10), 56–67.\n 23. Shaw, A. (2023). Social media, extremism, and radicalization. Science Advances, 9(35), eadk2031.\n 24. Sison, A. J. G., Daza, M. T., Gozalo-Brizuela, R., & Garrido-Merchán, E. C. (2023). ChatGPT: More \nthan a “weapon of mass deception\" ethical challenges and responses from the human-centered artificial \nintelligence (HCAI) perspective. International Journal of Human-Computer Interaction. https:// doi. \norg/ 10. 2139/ ssrn. 44238 74\n569\n1 3Journal of Computational Social Science (2024) 7:549–569 \n 25. Treleaven, P., Barnett, J., Brown, D., Bud, A., Fenoglio, E., Kerrigan, C., Koshiyama, A., Sfeir-Tait, S., \n& Schoernig, M. (2023). The future of cybercrime: AI and emerging technologies are creating a cyber-\ncrime tsunami. Social Science Research Network. https:// doi. org/ 10. 2139/ ssrn. 45072 44\n 26. Van Dis, E. A. M., Bollen, J., Zuidema, W., van Rooij, R., & Bockting, C. L. (2023). ChatGPT: Five \npriorities for research. Nature, 614(7947), 224–226.\n 27. Von Ahn, L., Blum, M., & Langford, J. (2004). Telling humans and computers apart automatically. \nCommunications of the ACM, 47(2), 56–60.\n 28. Vosoughi, S., Roy, D., & Aral, S. (2018). The spread of true and false news online. Science, 359(6380), \n1146–1151.\n 29. Yang, K.-C., & Menczer, F. (2023). Anatomy of an AI-powered malicious social botnet. arXiv: 2307. \n16336.\n 30. Ziems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., & Yang, D. (2023). Can large language models \ntransform computational social science? arXiv: 2305. 03514.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.",
  "topic": "Humanity",
  "concepts": [
    {
      "name": "Humanity",
      "score": 0.7465347647666931
    },
    {
      "name": "Generative grammar",
      "score": 0.6856371164321899
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47137606143951416
    },
    {
      "name": "Computer science",
      "score": 0.43304675817489624
    },
    {
      "name": "Cognitive science",
      "score": 0.35382384061813354
    },
    {
      "name": "Philosophy",
      "score": 0.25847312808036804
    },
    {
      "name": "Psychology",
      "score": 0.2313227355480194
    },
    {
      "name": "Theology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 120
}