{
  "title": "Fine-Tuning Large Language Model Based Explainable Recommendation with Explainable Quality Reward",
  "url": "https://openalex.org/W4393148128",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2562268280",
      "name": "Mengyuan Yang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2132859106",
      "name": "Mengying Zhu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1484673654",
      "name": "Yan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2277856354",
      "name": "Linxun Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118351096",
      "name": "Yilei Zhao",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2224477586",
      "name": "Xiuyuan Wang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2095736400",
      "name": "Bing Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108036699",
      "name": "Xiaolin Zheng",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2124632089",
      "name": "Jianwei Yin",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1484673654",
      "name": "Yan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2277856354",
      "name": "Linxun Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095736400",
      "name": "Bing Han",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2143177362",
    "https://openalex.org/W7005109661",
    "https://openalex.org/W2949553967",
    "https://openalex.org/W2903340942",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3153594481",
    "https://openalex.org/W4312780785",
    "https://openalex.org/W6774931820",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094497946",
    "https://openalex.org/W3164495165",
    "https://openalex.org/W4221158409",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W4225109328",
    "https://openalex.org/W4382202702",
    "https://openalex.org/W2798331900",
    "https://openalex.org/W6929778926",
    "https://openalex.org/W4297433271",
    "https://openalex.org/W3094210847",
    "https://openalex.org/W4367047369",
    "https://openalex.org/W2575006718",
    "https://openalex.org/W4309618087",
    "https://openalex.org/W3175536494",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W4382202735",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W4360612299",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W3104353018"
  ],
  "abstract": "Large language model-based explainable recommendation (LLM-based ER) systems can provide remarkable human-like explanations and have widely received attention from researchers. However, the original LLM-based ER systems face three low-quality problems in their generated explanations, i.e., lack of personalization, inconsistency, and questionable explanation data. To address these problems, we propose a novel LLM-based ER model denoted as LLM2ER to serve as a backbone and devise two innovative explainable quality reward models for fine-tuning such a backbone in a reinforcement learning paradigm, ultimately yielding a fine-tuned model denoted as LLM2ER-EQR, which can provide high-quality explanations. LLM2ER-EQR can generate personalized, informative, and consistent high-quality explanations learned from questionable-quality explanation datasets. Extensive experiments conducted on three real-world datasets demonstrate that our model can generate fluent, diverse, informative, and highly personalized explanations.",
  "full_text": "Fine-Tuning Large Language Model Based Explainable Recommendation with\nExplainable Quality Reward\nMengyuan Yang1, Mengying Zhu1*, Yan Wang2, Linxun Chen3, Yilei Zhao1, Xiuyuan Wang1,\nBing Han3, Xiaolin Zheng1, Jianwei Yin1\n1 Zhejiang University, China\n2 School of Computing, Macqaurie University, Australia\n3 MYbank, Ant Group, China\n{yangmy412, mengyingzhu, yilei zhao, xiuyuanwang, xlzheng}@zju.edu.cn, yan.wang@mq.edu.au\n{linxun.clx, hanbing.hanbing}@antgroup.com, zjuyjw@cs.zju.edu.cn\nAbstract\nLarge language model-based explainable recommendation\n(LLM-based ER) systems can provide remarkable human-\nlike explanations and have widely received attention from\nresearchers. However, the original LLM-based ER systems\nface three low-quality problems in their generated explana-\ntions, i.e., lack of personalization, inconsistency, and ques-\ntionable explanation data. To address these problems, we pro-\npose a novel LLM-based ER model denoted as LLM2ER to\nserve as a backbone and devise two innovative explainable\nquality reward models for fine-tuning such a backbone in a\nreinforcement learning paradigm, ultimately yielding a fine-\ntuned model denoted as LLM2ER-EQR, which can provide\nhigh-quality explanations. LLM2ER-EQR can generate per-\nsonalized, informative, and consistent high-quality explana-\ntions learned from questionable-quality explanation datasets.\nExtensive experiments conducted on three real-world datasets\ndemonstrate that our model can generate fluent, diverse, in-\nformative, and highly personalized explanations.\nIntroduction\nExplainable recommendation (ER) systems aim to provide\nhigh-quality explanations to help users understand the rec-\nommendations and make decisions. According to the phe-\nnomenon reported in an exhaustive survey of explanation\nquality (Lu et al. 2023), generating human-like explanations\ncan significantly improve the adoption rate of recommended\nitems. Among various explanation forms, such as tags (Yan\net al. 2020), reasoning paths (Wang et al. 2019) and images\n(Chen et al. 2019a), the textual explanation generated by\nlarge language models (LLMs) has attracted increasing at-\ntention due to their remarkable human-like natural language\ngeneration capabilities.\nIntuitively, a straightforward solution for an LLM-based\nER system is to feed an ER-related instructional prompt to\nan LLM. In Figure 1, we show an ER process for a movie\nnamed “X-Men” using a simple instruction with ChatGPT.\nWe can observe that the explanation given by ChatGPT as-\nsumes that the user is already a fan of the X-Men series\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The explanation generated by ChatGPT (GPT-3.5)\nwith an ER-related instructional prompt on July 10, 2023.\nand contains a lot of terms that appear in the movie, which\nmay be difficult to understand for the user who, in fact, is\nnot familiar with the movie. Based on this example, we can\nconclude that such an explanation suffers from low-quality\nproblem, making this explanation unable to improve user\nsatisfaction. Moreover, the widely recognized issue of hal-\nlucination (Ji et al. 2023) in LLM can lead to factual inac-\ncuracies within explanations, thereby compounding the low-\nquality problem. Therefore, directly using an LLM to fulfill\nthe ER task cannot be one-size-fits-all.\nWith the above insightful study of LLM-based ER, we\nanalyze that the causes of the low-quality problem are three-\nfold.\nCause 1: Prompts without integrating personalized in-\nformation trigger a lack of personalization in explanations.\nLLM is usually pre-trained on generic data and lacks per-\nsonalized information from users, and thus it may gener-\nate explanations that do not match user preferences when\npersonalized user–item information is not integrated into\nthe prompt, i.e., the model input. Existing studies (Geng\net al. 2022; Li, Zhang, and Chen 2023) construct person-\nalized prompts with personalized information to guide LLM\nin generating explanations. However, the personalization de-\ngree in such generated explanations is limited, because they\nincorporate user and item information separately rather than\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9250\nincorporating collaborative user-item information.\nCause 2: Generating information-overloaded explana-\ntions reduces consistency. LLMs generate informative but\nlengthy explanations, which contain a considerable amount\nof irrelevant information about the item features that a user\ndoes not care about, resulting in a worse user experience. Ex-\nisting studies (Chen et al. 2019b; Hada and Shevade 2021)\nleverage both user’s and item’s historical reviews to learn\nthe user preferences and item features to guide LLM in gen-\nerating precise explanations. However, the user preferences\nimplied in the user’s historical reviews partially match the\nitem features implied in the corresponding item’s historical\nreviews. Jointly leveraging the historical reviews from the\ndual sides will mix in some words that do not match user\npreferences and item features simultaneously, resulting in a\nreduction in the precision of the generated explanations.\nCause 3:Lack of sufficient high-quality explanation data\nfor fine-tuning limits the adaptability of LLM to the ER task.\nTo better adapt to the ER task, LLM requires fine-tuning\nwith explanation data. Existing studies (Hada and Shevade\n2021; Li, Zhang, and Chen 2023) fine-tune LLM by aligning\npaired generated explanation and review from the same user-\nitem pair, i.e., denoting the review as the ground truth expla-\nnation. However, the review dataset is questionable, and the\nground truth corresponding to a generated explanation is not\nnecessarily of high quality. For example, we often encounter\nhollow reviews that lack substantial information, and more\nimportantly, based on such reviews, it is difficult to fine-tune\nthe model to generate convincing explanations.\nIn order to improve the quality of explanations for LLM-\nbased ER, we tend to apply reinforcement learning from hu-\nman feedback (RLHF) as a training paradigm to obtain an\nER-oriented LLM, which is a popular technique to alleviate\nthe hallucinations and low-quality problems of LLMs. How-\never, RLHF requires handcrafted evaluation with expensive\nmanual efforts, which is impractical in recommender sys-\ntems. Fortunately, the recommendation system contains a\nwealth of implicit information that can evaluate the quality\nof explanations from multiple perspectives, allowing us to\nfine-tune the LLM in an unsupervised manner.\nBased on the above analysis, in this paper, we propose a\nnovel LLM-based ER backbone named LLM2ER and fine-\ntune such a backbone in a reinforcement learning paradigm\nwith two novel explainable quality reward models, where the\nfine-tuned model is named LLM2ER-EQR. LLM2ER model\na concept graph extracted from reviews to achieve the fol-\nlowing two purposes: (1) predict the rating based on the het-\nerogeneous graph model and map the rating to the user’s\nsentiment for the target item; (2) infer the reasoning paths\nbetween target user-item pair from the concept graph to col-\nlect personalized and consistent candidate concepts to im-\nprove the precision of explanations (for addressing Causes\n1 and 2). The LLM2ER-EQR additionally includes two re-\nward models to further enhance the explanation quality: (1)\nconcept consistent reward model leverages sentiment-wise\ncandidate concepts to preserve paired user preferences and\nitem features in the generated explanations based on con-\ntrastive learning (for addressing Cause 2); (2) high-quality\nalignment reward model aligns the generated explanations to\nunpaired high-quality explanations based on the generative\nadversarial network (for addressing Cause 3).\nTo the best of our knowledge, this is the first work to\nfine-tune an LLM in a reinforcement learning paradigm for\nexplainable recommendations. Our main contributions are\nsummarized as follows: (1) Effective model design: we pro-\npose a novel fine-tuned LLM-based ER modelLL2ER-EQR\nto address three low-quality problems in ER systems; (2)\nNovel fine-tuning strategy: we devise an efficient and fea-\nsible RL-based fine-tuning strategy for unsupervised fine-\ntuning of LLMs that can generate high-quality explana-\ntions without a handcrafted evaluation; (3) Extensive ex-\nperiments: we conduct extensive experiments on three real-\nworld datasets, which demonstrate our model significantly\noutperforms the state-of-the-art methods and can generate\nfluent, diverse, informative, and highly personalized expla-\nnations.\nRelated Work\nWith natural language processing technology having\nachieved remarkable performance in text generation, ER\nbased on text generation has received increasing attention\nbecause of its good human-like language generation capabil-\nities. The early studies propose methods to generate expla-\nnations based on pre-defined templates (Wang et al. 2018)\nor association rules (Gao et al. 2019), which require exten-\nsive manual labor and cannot assemble diverse, personal-\nized, convincing explanations. Subsequently, there are meth-\nods (Chen et al. 2019b; Li, Zhang, and Chen 2020; Hu et al.\n2022; Zhang et al. 2023) to generate synthetic text explana-\ntions by recurrent neural network-based (RNN-based) lan-\nguage models. However, their models’ training processes\nare confronted with a shortage of sufficient training sam-\nples, i.e., explanation texts, resulting in a lack of robustness\nin generated explanations. In addition, the RNN-based lan-\nguage models are not trained on a vast corpus, which makes\nthe fluency of generated explanations questionable.\nRecently, a burgeoning body of research started to study\nLLM-based ER models. (Li, Zhang, and Chen 2021; Geng\net al. 2022; Li, Zhang, and Chen 2023) mainly focus on\ndesigning prompts to guide LLMs to directly generate ex-\nplanations. However, LLMs face the hallucination problem\nresulting in generating low-quality explanations. Some stud-\nies address part of low-quality problems, such as question-\nable review data and personalization, by controlled text gen-\neration (Hada and Shevade 2021), personalized variational\nautoencoder (Cai and Cai 2022; Wang et al. 2023), and re-\ntrieval model (Xie et al. 2023). So far, there is no research\nthat comprehensively addresses the low-quality problems of\nexplanations generated by LLM-based ER models.\nIn addition, there are a series of studies orthogonal to our\nwork, i.e., review-based recommendation models (Zheng,\nNoroozi, and Yu 2017; Shuai et al. 2022), which integrates\nthe explanation process into the recommendation model to\nimprove recommendation performance rather than generat-\ning explainable text and is not our main focus.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9251\nRating Prediction ModulePersonalized Prompt Learning ModuleExplanationGeneration Module\nUsersItems\nPrediction Loss (Lr)\nConcept graph\nconceptuseritemrelation between conceptsuser-item interaction\nuser embeddingitem embedding\nPredicted Rating\nu1 u2 u3\nv1 v2 v3 v4\nc1\nc3\nc6\nc4\nc8c7\nc2 c5\nc9 Find top-lpaths\n……\nStep 1: user-item pairStep 2: sentiment\nStep 3: candidate concepts\nLinear\nc4c6\nv2 su1\nPretrained Causal Language Model (e.g., GPT-2, ……)\nGenerated Explanations\nUserHeadItemSent.ConceptsExplanation text[BOS]\nc4 c6sv2u1I ……\n……\nExplanationLoss (Le)GT Explanations\nGT Rating\nConcept Consistent Reward Model\nHigh-Quality Alignment Reward ModelGenerated Explanations\nGT Explanations(High quality)\nDiscriminator loss (fake)  \nDiscriminator loss (real)  \n0000\n1111\nEQR Reward\nCCR Reward\n(Negative Generator loss)Data flowsTraining flows\n+c4 c6v2u1\nc1 c3v1u1\nc7 c8v2u3\nGeneratedExplanationss\nBert\nmaximizemaximize\nminimize(Similarity Distance)\nLLM2ER EQR\nContrastive loss\nPPO\n[EOS]\nHGT\nIntegrate and assemble prompt\nsentiment mappingLinear\nu1\nv1\nv2\nc6 u2c4 c4\nc6\nc40.100.080.050.01\n0.180.080.040.120.030.08\n0.10\nLookupCandidate Concepts\nDiscriminator\nFigure 2: The architecture of LLM2ER-EQR. The left part is the backbone for the ER task named LLM2ER, and the right part\nis the explainable quality reward models named EQR.\nPreliminaries and Task Formulation\nPreliminaries\nThe decision process for the RL problem can be defined as a\ntuple (S, A, T , R), where S denotes a finite state space, A\ndenotes a finite set of actions, T (s′|s, a) is a state transition\nfunction that defines the next state s′ given the current state\ns and action a, and R(s, a) is a reward function. A policy\nπ(a|s) determines an action a given the current state s.\nReward learning enables the application of reinforcement\nlearning (RL) to fine-tune an LLM. InstructGPT (Ouyang\net al. 2022) apply an RL training paradigm using Proxi-\nmal Policy Gradient (PPO) (Schulman et al. 2017) as the\nfine-tuning strategy. Specifically, in InstructGPT, the RL\nagent πRL\nϕ is initialized with the supervised pre-trained LLM\nπSFT, which aims to maximize the following combined ob-\njective function fRL:\nfRL =E(x,ˆe)∼DπRL\nϕ\n[Rθ(x,ˆe)| {z }\nreward\n−βlog(πRL\nϕ (ˆe | x)/πSFT(ˆe | x))| {z }\nKL penalty\n]−\nγ Ex∼Dpretrain[−log(πRL\nϕ (x))]| {z }\npre-training loss\n, (1)\nwhere x is the prompt, ˆe is the generated text, the reward\nRθ(x, ˆe) is calculated by a reward model,Dpretrain is the pre-\ntraining distribution, and the coefficientsβ and γ control the\nKL penalty and pre-training loss, respectively.\nExplainable Recommendation Task\nWe formulate the ER task as follows.\nInput. The input consists of the user set U, the item set V\nand the concept graph:\n(1) each user is represented by its ID u ∈ Uand each\nitem is represented by the item ID v ∈ V;\n(2) the rating ru,v ∈ R>0 is a positive value given by\nuser u to item v. Let Ω = {(u, v) : user u rates item v}.\nNote that unavailable ratings are represented by 0, namely,\nru,v = 0 where (u, v) /∈ Ω. Specifically, we treatΩtrain, Ωtest\nas the training and test dataset, respectively.\n(3) the explanation eu,v is a short text, which is extracted\nfrom a review of the user-item pair (u, v). For each expla-\nnation denoted as eu,v, we assemble a corresponding set of\nconcepts represented as {c}u,v. These concepts are system-\natically extracted from the explanation text. Note that the el-\nements within this concept set are not arbitrary; they are key-\nwords pertinent to recommendation explanations, including\norganizations, careers, characteristics, companies, brands,\nservices, products, etc. These keywords are instrumental in\nmirroring user preferences and item attributes. Specifically,\nwe extract several concepts {c}u,v according to the follow-\ning two steps: (i) extract 1-gram to 4-grams from the expla-\nnation; (ii) match them to Microsoft Concept Graph and get\nthe matched words as concepts.\n(4) the concept graphG is a heterogeneous graph, which\nincludes user nodes U, item nodes V, and concept nodes C.\nIn order to enrich concept relations, we add the 1-hop neigh-\nbors of all concepts into C. We organize the above informa-\ntion in the form of knowledge graph G = {(h, r, t)|h, t∈\nC ∪ U ∪ V, r∈ M ∪ I}, where M is the relation set from\nMicrosoft Concept Graph, I is the interaction set of user-\nexplanation-item interactions. Note that the concept graph is\nonly constructed based on the training dataset.\nOutput. Given a user-item pair (u, v) /∈ Ωtrain, our model\npredicts: (1) the rating ˆru,v and (2) the explanation ˆeu,v for\nthe target user-item pair (u, v) .\nNote that at the training stage, the input data comprise\nusers, items, ground truth ratings, ground truth explanations,\nand the concept graph, while during the testing stage, only\nusers, items, and the concept graph are exposed to the model.\nMethod\nThe architecture of LLM2ER-EQR is presented in Figure 2.\nIn the following subsections, we first present an LLM-based\nexplainable recommendation model named LLM2ER as the\nbackbone for the ER task. After that, to further improve the\nquality of explanations, we devise the fine-tuning process of\nthe LLM-based backbone based on a reinforcement learning\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9252\nparadigm, where the backbone denotes the action model. We\nelaborate on two types of reward models corresponding to\naddress consistency problem and questionable explanation\ndata problem. Finally, we present the whole training process\nof LLM2ER-EQR .\nPre-trained Language Models, Prompt Learning,\nand Explanation Generation\nWe introduce three modules of the LLM2ER backbone ac-\ncording to the process of an ER task.\nCollaborative Concept-based Rating Prediction Mod-\nule. For each user-item pair, we process this module in the\nfollowing two steps. Firstly, we learn the user and item em-\nbedding based on the concept graph, which contains rich\nuser preferences and item features. Specifically, we adopt\nHGT (Hu et al. 2020) to aggregate the interaction and con-\ncept information from heterogeneous concept graphs and\nobtain the corresponding node embeddings h. Secondly, we\nemploy a multi-layer perception (MLP) to predict the rating\nthat the user u rates the item v as follows:\nˆru,v = MLP([hu, hv]), (2)\nwhere hu and hv are the user and item node embedding,[·, ·]\ndenotes the concatenation of vectors.\nFor the rating prediction task, we aim to minimize the\nmean squared error loss between ground truth ratings and\nthe predicted ones as follows:\nLr = 1\n|Ωtrain|\nX\nu,v∈Ωtrain\n(ru,v − ˆru,v)2. (3)\nPersonalized Prompt Learning Module.Plugging per-\nsonalized information into the LLM by prompt learning\ncan control the LLM to generate personalized explana-\ntions. To construct such a personalized prompt, we create an\nER prompt template following the instruction-based prompt\nschema (Geng et al. 2022), which contains detailed ER task\ndescriptions and personalized information as follows:\nPrompt head I provides detailed instructions in the natu-\nral language format for the ER task.\nUser-item embeddings pair (pu, qv) are important iden-\ntifiers for personalization. We use linear projection to con-\nform the dimension of user and item embeddings to that of\nthe LLM token embedding, as follows:\n(pu, qv) = (Linear(hu), Linear(hv)). (4)\nSentiment su,v reflects the user’s attitude about the item,\nwhich is indicated by rating. Following (Li, Zhang, and\nChen 2020), we map the rating to “negative” if the rating\nis less than 3 (5-scale rating), and vice versa.\nCandidate concepts {ˆc}u,v are a set composed of dis-\ntinct concepts collected from reasoning paths for consis-\ntency. Benefiting from the attention mechanism in HGT, we\ninfer the reasoning paths from the target useru to target item\nv by performing a weighted search on the concept graph.\nSpecifically, we adopt beam search to heuristically search\nthe top-l paths with the highest cumulative attention weights\nand with no more than h hops. To limit prompt length, we\nstop the search once the size of the candidate concept set\n|{ˆc}u,v| reaches 10.\nFor each target user u and target item v, we assemble a\npersonalized prompt xu,v = [I, pu, qv, su,v, {ˆc}u,v], where\nall the text are tokenized by the LLM’s tokenizer.\nExplanation Generation Module. We adopt a pre-\ntrained causal language model, such as GPT-2, to generate\nthe explanation ˆeu,v = (ˆeu,v,1, ˆeu,v,2, . . . ,ˆeu,v,n), where n\nis the explanation length. For the i-th decoding, we have\nˆeu,v,i = arg max\nˆeu,v,i\npM(ˆeu,v,i|xu,v, ˆeu,v,<i), (5)\nwhere decoding output ˆeu,v,i is conditioned on both input\nprompt xu,v and previous outputs ˆeu,v,<i.\nFor the explanation generation task, we aim to the nega-\ntive log-likelihood loss between the ground truth explanation\nand the generated explanation as follows:\nLe = 1\n|Ωtrain|\nX\nu,v∈Ωtrain\n1\nn\nnX\nt=1\n−log p(eu,v,i), (6)\nwhere p(eu,v,i) is the predicted probability of token eu,v,i.\nConcept Consistent Reward Model (CCR)\nIn order to further alleviate low-quality problems of gener-\nated explanations, we fine-tune the backbone LLM2ER in an\nRL paradigm followed by InstructGPT (Ouyang et al. 2022),\nwith an explainable quality reward model.\nTo improve the generated explanation consistent with user\npreferences and item features, we propose a concept con-\nsistent reward model (CCR) based on BERT (Kenton and\nToutanova 2019). The CCR calculates the similarity distance\nbetween the pair of generated explanations and the corre-\nsponding sentiment-wise candidate concepts as the reward.\nSpecifically, CCR first encodes the generated explanations\nˆeu,v and candidate concepts {ˆc}u,v by BERT, and then cal-\nculates the cosine similarity of the averaged word embed-\nding over the words of ˆeu,v and {ˆc}u,v. The reward is calcu-\nlated by:\nRCCR(ˆeu,v, {ˆc}u,v)\n= cosin(avg(BERT(ˆeu,v)), avg(BERT({ˆc}u,v))). (7)\nBefore we\nplug the CCR model into the RL paradigm,\nwe need to train it by reward learning. We construct con-\ntrastive learning to train the CCR model from the perspec-\ntive of consistency. Specifically, to form the positive pair,\nwe take the generated explanation ˆeu,v of a target user-item\npair (u, v) and the corresponding candidate concepts {ˆc}u,v\nas a positive pair. To form the negative pair, we select some\nuser-item pairs as negative samples Nu′,v′ and pair ˆeu′,v′\nwith the candidate concepts {ˆc}u′,v′ of each negative user-\nitem pair (u′, v′). The negative user-item pairs are selected\nby the following rules: (1) the user or item in such user-item\npair has one that matches the target user-item pair; (2) the\nnegative user-item pairs have the same sentiment as that in\nthe target user-item pair. Finally, following (Gao, Yao, and\nChen 2021), we train the CCR model by minimizing the con-\ntrastive loss as:\nLc = 1\n|Ωtrain|\nX\n(u,v)∈Ωtrain\nlog exp(rCCR(ˆeu,v, {ˆc}u,v))P\n(u′,v′)∈Nu,v exp(rCCR(ˆeu′,v′ , {ˆc}u′,v′ )). (8)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9253\nHigh-Quality Alignment Reward Model (HQAR)\nBecause the ground truth corresponding to the generated\nexplanation is not necessarily of high quality, we propose\na high-quality alignment reward model (HQAR) based on\ngenerative adversarial network (GAN) (Goodfellow et al.\n2014) to align the explanations generated by LLM2ER to\nunpaired high-quality explanations that contain rich infor-\nmation during the training process.\nSpecifically, we reformat LLM2ER and HQAR into a\nGAN framework, where LLM2ER is the generator and\nHQAR is the discriminator. The primary objective of\nLLM2ER within this GAN framework is to generate high-\nquality explanations with the intention of deceiving the\nHQAR. Conversely, the HQAR’s goal is to discern these\nhigh-quality explanations with accuracy. This dynamic es-\ntablishes a competitive interaction between theLLM2ER and\nHQAR, central to the effectiveness of the LLM2ER. Note\nthat HQAR includes an LLM with the same structure as the\nLLM in LLM2ER. We feed the generated explanation ˆeu,v\nto the HQAR, and the HQAR outputs the probability p that\nthe generated explanation came from a high-quality expla-\nnations dataset. We denote the negative generative loss as\nthe high-quality alignment reward, as follows:\nRHQAR(ˆeu,v) = −lg(ˆeu,v) = −\nnX\ni=1\nlog(1 − pˆeu,v,i ), (9)\nwhere i is the token index.\nWe train the HQAR model by following two steps. Firstly,\nwe collect a high-quality explanation dataset H as a real\nsample dataset. For each ground truth explanation in the\ntraining dataset, we define a concept proportion rate of each\nground truth explanation to measure the proportion of words\nin the explanation belonging to the corresponding candi-\ndate concepts. The concept proportion rate is calculated as\noe = |{c}u,v ∩ eu,v|/n, where n is the explanation length.\nThen we set a threshold δ and select those explanations\nwith oe ≥ δ to form H. Note that we set the δ empiri-\ncally. Secondly, we feed the generated explanation ˆeu,v and\nhigh-quality explanation h sampled from H to HQAR (dis-\ncriminator), and HQAR outputs the probability p that the\nexplanation came from H. We adopt a token-level binary\ncross-entropy loss as a discriminator loss to train the HQAR,\nwhich can be calculated as:\nLd = − 1\n|Ωtrain| + |H|(\nX\n(\nu,v)∈Ωtrain\nnX\ni=1\nlog(1 − pˆeu,v,i ) +\nX\nh∈H\nnX\ni=1\nlog phi), (10)\nwhere i is the\ntoken index.\nModel Training\nThe training process of the LLM2ER-EQR consists of four\nsteps. The first step is training the collaborative concept-\nbased rating prediction module by minimizing the Lr. The\nsecond step is training the LLM2ER by minimizing the\nLLLM2ER, where LLLM2ER = λrLr + λeLe. The third step is\ntraining the two reward models, i.e., CCR and HQAR, under\na frozen LLM2ER by minimizing the LEQR, where LEQR =\nλcLc + λdLd. The fourth step is fine-tuning the LLM2ER.\nSpecifically, training the LLM2ER under two frozen reward\nmodels by maximizing the combined objective in Eq. (1),\nwhere the reward function is defined as follows:\nR(xu,v, ˆeu,v) = λCCRRCCR(\nˆeu,v, {ˆc}u,v) + λHQARRHQAR(ˆeu,v), (11)\nwhere {ˆc}u,v is in\nprompt xu,v. Note that the λr, λe, λc, λd,\nλCCR and λHQAR are coefficients for balancing the impor-\ntance of different loss terms. We perform the third and fourth\nsteps alternately until the fine-tuned LLM2ER and the two\nreward models reach Nash equilibrium (Goodfellow et al.\n2014).\nExperiment\nIn this section, we present extensive experiments to answer\nthe following question:\nQ1: How does LLM2ER-EQR perform on explanation\ngeneration task?\nQ2: How do LLM2ER-EQR’s key components contribute\nto its performance?\nQ3: How do the concept selection affect the performance\nof LLM2ER-EQR?\nQ4: How about the quality of explanations generated by\nLLM2ER-EQR?\nExperimental Settings\nDataset Descriptions and Collection.To evaluate the effec-\ntiveness of LLM2ER-EQR, we adopt three benchmark rec-\nommendation datasets, which are publicly available explain-\nable contents and vary in terms of domain, size, and sparsity.\nThe three datasets are from Amazon (Movie & TV) 1, and\nYelp (2019)2, and TripAdvisor3, respectively, and their cor-\nresponding recommendation explanation data are collected\nfrom the GitHub repository4 of (Li, Zhang, and Chen 2021).\nTo ensure the quality of the concept graphs, we then filter out\nthe rare concepts and domain-dependent frequent concepts.\nStatistics of the datasets are shown in Table 1.\nFollowing the previous study (Li, Zhang, and Chen 2020,\n2021; Wang et al. 2023), each dataset is randomly divided\ninto training, validation, and testing sets with a ratio of 8:1:1.\nWe repeat all experiments 5 times independently, with each\niteration involving a re-division of the dataset. The mean of\ntest performance is reported.\nComparison Methods.To demonstrate the effectiveness\nof LLM2ER-EQR, we compare seven LM-ER baselines,\nall of which generate explanations by the language model.\nCAML (Chen et al. 2019b) andNETE (Li, Zhang, and Chen\n2020) are two conventional GRU-based ER models to gen-\nerate explanations and template-controlled explanations, re-\nspectively. ReXPlug (Hada and Shevade 2021) is a state-\nof-the-art ER model and applies a plug-and-play language\nmodel to generate controlled text explanations.PETER (Li,\nZhang, and Chen 2021) is a state-of-the-art LLM-based ER\nmodel to enhance explanation generation by prompt learn-\ning. PEV AE(Cai and Cai 2022) is a state-of-the-art LLM-\nbased ER model extending a hierarchical variational auto-\nencoder to overcome the data sparsity. CV AEs(Wang et al.\n1http://jmcauley.ucsd.edu/data/amazon\n2https://www.yelp.com/dataset\n3https://www.tripadvisor.com\n4https://github.com/lileipisces/PETER\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9254\nDataset\nAmazon\n(Movies\n&\nTV)\nYelp\n(2019)\nTrip-\nAdvisor\n# Users 7,506\n27,147 9,765\n# Items 7,316 18,172 5,429\n# exp. 432,075 1,189,056 296,118\n# con. 10,141 14,657 10,948\n# Triplets 7,223,673 31,417,433 6,992,065\nAvg. # exp./user 57.56 43.80 30.32\nAvg. # exp./item 59.06 65.43 54.54\nAvg. # con./exp. 7.33 11.46 9.73\nTable 1: Statistics of the datasets. The “exp.” is the abbrevi-\nation of “explanations” and the “con.” is the abbreviation of\n“concepts”\n2023) is a state-of-the-art LLM-based ER model to extract\nadequate characteristics for controllable generation. PRAG\n(Xie et al. 2023) is a state-of-the-art LLM-based ER model\nfor addressing factual hallucinations problem.\nEvaluation Metrics.We evaluate the performance of ex-\nplanation generation from the following two perspectives.\nIn terms of the text quality perspective, we adopt two\nwidely used evaluation metrics, i.e., BLEU (Chen and\nCherry 2014) and ROUGE (Lin 2004). Specifically, we\nreport the results of BLEU-3 and BLEU-4, denoting the\nsmoothed BLEU scores for 3-grams and 4-grams, respec-\ntively. We present the result of ROUGE-1 and ROUGE-L,\ndenoting the ROUGE scores for 1-grams and longest com-\nmon subsequence, respectively.\nIn terms of the explanation quality perspective, we adopt\nthree metrics for comprehensive evaluation. From a per-\nsonalization aspect, we adopt Distinct-1 and Distinct-2 for\nsentence-wise diversity and propose a new metric named\nConcept Overlapping Ratio (COR) for word-level diversity.\nCOR measures the overlap of concepts in any two generated\nexplanations with the same item. COR is calculated as:\nCOR(v) = 1\n|Utestv |(|Utestv | − 1)\nX\nu\n∈Utest\ni ,u′∈Utest\ni\n|(ˆeu,v ∩ {c}u,v) ∩ (ˆeu′,v ∩ {c}u′,v)|\n|(ˆeu,v ∩\n{c}u,v) ∪ (ˆeu′,v ∩ {c}u′,v)|,\nCOR = 1\n|Vtest|\nX\nv∈Vtest\nCOR(v),\n(12)\nwhere Utest\nv denotes the\nset of users that have interactions\nwith v in the testing data, and Vtest denotes the item set in\ntesting data. A lower COR indicates a smaller overlap be-\ntween explanations and, thus, a higher diversity. From a con-\nsistency aspect, we propose Concept Matching Ratio (CMR)\nto measure how many concepts are contained in the gener-\nated explanation, which is calculated as follows:\nCMR = 1\n|Ωtest|\nX\n(u,v)∈Ωtest\nX\nc∈{c}u,v\nδ(c ∈ ˆeu,v), (13)\nwhere δ(·) is an indicator function that is true when concept\nc matches 1-gram to 4-grams in ˆeu,v. Higher CMR indicates\nbetter performance.\nPersonalized Prompt.The personalized ER prompt used\nin our LLM2ER is as follows, which contains four placehold-\ners, i.e., sentiment, candidate concepts, user embedding, and\nitem embedding.\nER pr\nompt template:\nPrompt head:Please generate\nrecommendation explanation.\nQuery input: Based on the {sentiment:\npositive\\negative } sentiment\nand candidate concepts including\n{candidate concepts, e.g., user\npreferences, item features}, please\nprovide explanations to recommend\n{item embedding} to {user embedding}.\nQuery output: {explanation}\nFor\neach user-item pair, we assemble the sentiment, can-\ndidate concepts, user embedding, and item embedding into\nthe above ER prompt template to construct the entire per-\nsonalized prompt.\nQuantitative on Explanation Tasks (for Q1)\nWe compare LLM2ER-EQR with seven LM-ER meth-\nods. The evaluation results of the generated explanations\non three datasets are shown in Table 2, which shows\nLLM2ER-EQR consistently outperforms baselines on all\nmetrics. From the results, we can demonstrate the effec-\ntiveness of LLM2ER-EQR from the following three as-\npects. Firstly, LLM2ER-EQR generates explanations with\nthe highest text quality, as it achieves the highest BLEU\nand ROUGE scores, with an average improvement of\n5.496% and 4.835%, respectively, compared to the best-\nperforming baseline. Secondly, LLM2ER-EQR exhibits the\nability to generate diverse and personalized explanations,\nas is reflected by the highest Distinct scores and COR\nscores among all baselines, with an average improvement of\n3.940% and 12.756% compared to the best-performing base-\nline, respectively. Thirdly, in terms of consistency, we ob-\nserve that LLM2ER-EQR achieves the highest CMR scores\nwith an average improvement of 5.843% over the best-\nperforming baseline, which demonstrates the CCR model in\nLLM2ER-EQR has the capability to alleviate the consistency\nproblem in the generated explanations.\nAblation Studies (for Q2)\nTo investigate the effectiveness of LLM2ER-EQR’s key\ncomponents, we experiment on three simplified variants\nof our own model, i.e., LLM2ER, LLM2ER-CCR and\nLLM2ER-HQAR, only backbone, only with concept con-\nsistent reward model, and only with high-quality align-\nment reward model, respectively. As shown in Table 2,\nthe absence of any key component will lead to a de-\ncline in performance. Specifically, LLM2ER-EQR outper-\nforms LLM2ER on all eight explainable metrics, which em-\npirically demonstrates the importance of improving expla-\nnation quality. LLM2ER-EQR outperform LLM2ER-HQAR\nwith average improvements of 9.460% on CMR, which\ndemonstrates LLM2ER-EQR has the strong ability to pre-\nserve the consistent concepts that reflect both user pref-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9255\nCategories Methods BLEU (%) ROUGE (%) Distinct (%) NEW Defined\nBLEU-1 BLEU-4 ROUGE-1 ROUGE-L Distinct-1 Distinct-2 COR CMR\nAmazon (Movie & TV)\nLM-ER\nCAML 15.834 1.439 15.686 12.104 18.256 61.254 0.078 2.078\nNETE 13.847 1.186 14.891 11.880 14.854 48.431 0.099 0.759\nRexPlug 15.940 1.436 17.406 13.040 17.167 57.233 0.101 2.040\nPETER 15.850 1.403 16.514 13.395 18.135 60.290 0.082 2.042\nPEV AE 16.288 1.476 17.051 13.345 18.650 62.710 0.077 1.894\nPRAG 15.756 1.389 16.417 13.316 19.003 64.420 0.069 2.146\nCV AEs 17.319 1.504 17.114 14.536 18.714 62.367 0.080 2.023\nOur\nModel\nLLM2ER 16.378 1.193 16.874 14.176 18.400 64.982 0.066 2.171\nLLM2ER-HQAR 16.973 1.237 17.669 14.614 19.089 66.228 0.071 2.196\nLLM2ER-CCR 17.319 1.564 18.029 14.747 18.622 65.631 0.057 2.301\nLLM2ER-EQR 17.571 ∗ 1.572∗∗ 18.291∗∗ 15.157∗∗ 19.370∗∗ 67.044∗∗ 0.058∗∗ 2.353∗∗\nImprovement1 1.457% 4.520% 5.089% 4.276% 1.931% 4.073% 16.403% 9.647%\nYelp (2019)\nLM-ER\nCAML 13.533 1.283 15.080 11.837 16.677 56.830 0.090 2.422\nNETE 15.414 1.322 18.040 14.226 13.345 45.466 0.127 1.756\nRexPlug 15.978 1.368 19.960 15.668 16.760 59.680 0.093 2.860\nPETER 14.989 1.026 18.720 14.246 16.274 52.852 0.080 2.805\nPEV AE 16.219 1.524 18.867 15.012 17.210 60.570 0.081 2.542\nPRAG 16.612 1.146 20.747 15.789 16.901 60.045 0.071 2.897\nCV AEs 17.804 1.511 20.064 16.839 16.764 59.979 0.090 2.549\nOur\nModel\nLLM2ER 18.075 1.523 21.427 16.827 16.879 59.916 0.071 2.800\nLLM2ER-HQAR 17.841 1.307 21.487 16.730 17.617 63.050 0.076 2.746\nLLM2ER-CCR 17.968 1.653 21.640 16.849 16.897 62.317 0.063 2.953\nLLM2ER-EQR 18.013 ∗ 1.662∗∗ 21.693∗∗ 16.901∗ 17.707∗∗ 64.150∗∗ 0.064∗∗ 2.995∗∗\nImprovement 1.170% 9.042% 4.563% 0.365% 2.888% 5.911% 11.276% 3.365%\nTripAdvisor\nLM-ER\nCAML 16.385 1.151 17.468 14.832 19.388 63.583 0.060 3.510\nNETE 15.783 1.138 17.403 14.521 15.616 52.130 0.075 1.694\nRexPlug 16.726 1.216 20.472 15.939 20.051 67.263 0.064 3.978\nPETER 17.023 1.197 20.445 16.084 21.122 65.843 0.059 3.670\nPEV AE 16.806 1.307 18.911 15.408 20.380 68.444 0.055 3.571\nPRAG 17.179 1.213 20.634 16.232 20.956 69.226 0.049 4.008\nCV AEs 18.748 1.331 20.902 16.829 21.926 68.842 0.059 3.586\nOur\nModel\nLLM2ER 18.589 1.272 20.952 16.689 21.758 68.325 0.067 3.735\nLLM2ER-HQAR 18.642 1.433 21.113 16.774 22.495 71.398 0.055 3.734\nLLM2ER-CCR 19.983 1.382 21.532 17.981 21.782 69.993 0.044 4.178\nLLM2ER-EQR 20.040 ∗∗ 1.436∗∗ 22.196∗∗ 18.032∗∗ 22.794∗∗ 72.601∗∗ 0.045∗∗ 4.189∗∗\nImprovement 6.894% 9.892% 7.572% 7.146% 3.959% 4.875% 10.587% 4.516%\n1\nImprovement of LLM2ER-EQR over the best-performing baselines ( underlined numbers indicate best-performing baselines’ results).\n∗∗ and ∗ respectively indicate the statistical significance for p <0.01 and p <0.05 via Student’s t-test.\nTable 2: Performance comparison of all methods of generated explanations on three datasets.\nerences and item features in the generated explanations.\nLLM2ER-EQR outperform LLM2ER-CCR with average im-\nprovements of 3.713% on Distinct score, which indicates\nthe LLM2ER-EQR model’s capability to infuse informative\nterms into unpaired insubstantial explanations through adept\nalignment, thereby enriching the diversity of explanations.\nEffectiveness of Concept Selection (for Q3)\nTo study the effectiveness of involving concepts, we evaluate\nthe performance of LLM2ER-EQR with different max path\nlengths and path select numbers. Figure 3 shows the results\non the three datasets. From Figure 3 (a), (d), and (h), we can\nobserve that both the max path length and the path select\nnumber affect the number of concepts. Jointly considering\nboth Figure 3 (b) and (c) for the Amazon (Movie & TV)\ndataset, the performance reaches its peaks with a max path\nlength of 3 and a path select number of 5. A similar trend is\nobserved in the Yelp (2019) and TripAdvisor datasets. From\nthis result, we can conclude that an excessive quantity of\ncandidate concepts leads to an increase in COR, thereby di-\nminishing the degree of personalization, while an inadequate\nnumber of candidate concepts results in diminished CMR.\nTherefore, we select the max path length as 3 and the path\nselect number as 5 for our model.\nQualitative Case Study (for Q4)\nWe present some concrete cases to show the improvement\nof generated explanations of our model. For comparison,\nwe show three cases of explanations generated by baselines\nand LLM2ER-EQR in the Amazon dataset (c.f., Figure 4).\nFor each user-item pair in the three cases, we display con-\ncepts that the user and item most frequently mentioned in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9256\nmax path lengths\nmax path lengths\nmax path lengths\n(1) Amazon (Movie & TV)\n(d) Candidate Concept Number (e) COR (f) CMR\nmax path lengths\nmax path lengths\nmax path lengths\npath select numbers path select numbers path select numbers\n(2) Yelp (2019)\n(h) Candidate Concept Number (i) COR (j) CMR\nmax path lengths\nmax path lengths\nmax path lengths\npath select numbers path select numbers path select numbers\npath select numbers path select numbers path select numbers\n(a) Candidate Concept Number (b) COR (c) CMR\n(3) TripAdvisor\nFigure 3: Performance on different max path lengths and\npath select numbers on three datasets.\nthe training dataset as user preferences and item features,\nrespectively. Overall, compared with all baselines, our ex-\nplanations are more informative, similar to ground-truth ex-\nplanations, and exhibit fluency and coherence. Specifically,\nby jointly comparing the generated explanations between\ncase 1 and case 2, when recommending the same item to\ndifferent users, LLM2ER-EQR generates personalized and\ndiverse explanations, aligning effectively with users’ prefer-\nences. Moreover, our explanations contain the largest num-\nber of user preferences and item features, demonstrating that\nLLM2ER-EQR can effectively maintain the consistency of\ngenerated explanations.\nDiscussion and Conclusion\nLegacies of Language Models.It is worth noting that a\nlarge part of our framework relies on the pre-trained LLMs\n(Brown et al. 2020). This means that LLM2ER naturally in-\nherits the advantages and disadvantages of the pre-trained\nLLM. Specifically, the advantage is that LLM2ER-EQR can\nutilize the implicit knowledge existing in the pre-trained\nLLM to generate explanations, while the disadvantage is\nthat it may introduce risks, such as potentially producing of-\nfensive language, propagating social biases and stereotypes,\nand leaking private information (Weidinger et al. 2021). To\nalleviate such a problem, we have made efforts in both train-\ning and inference stages: (1) during the training stage, the\nLLM is fine-tuned by the cleaned ER datasets free of harm-\nful information to guide it to generate high-quality explana-\ntions; (2) during the inference stage, we construct prompts\nwith reasoned concepts about user preferences and item at-\nGround-\ntruth\nThere will be a spin-off movie featuring wolverine. He has a good characterand intense\nactions.\nCAML Intense movie. Wolverine's character is good. Enjoy it.\nNETE The movie is very fun and  intense, and the main  characteris very good.\nReXPlug I like a wolverine movie because he is the most popular character on screen.\nPETER Wolverine is cool andcharacters are good. A good movie for actionfans.\nPEVAE The wolverine movie is enjoyable, his actions and character are good.\nPRAG The movie excelledwith intense character, iconicwolverine, groundbreakingaction.\nCVAEs The movie is a character-driven action movie with intense wolverine scenes.\nLLM2ER The movie crafted a goodcharacter and presented intenseaction scenes.\nLLM2ER-\nEQR\nThe movie crafted an impressive wolverine characterand presented intenseaction\nscenes.\nItem: X-Men\nItem feature: screen, character, action, wolverine, scenes, intense, future\nUser preferences: screen, character, action, enjoy, wolverine, score, tear\nGround-\ntruth One can definitely tell that this director has a long future in directingaction movies.\nCAML It is an action movie directed by a good director.\nNETE The movie is a very goodaction movie and the storyis very intense.\nReXPlug The good director and wolverine‘s character makes the movie a must-see.\nPETER This intense movieis directed by a skilledaction director.\nPEVAE The director made a good action movie.\nPRAG The movie has definiteaction, deep character and futuristic scenes.\nCVAEs This movie feature intenseaction, good characters, and a skillfuldirector.\nLLM2ER The excellent director achieved a satisfying blend of scenes and character.\nLLM2ER-\nEQR The action directorhas a vision that blends futuristic scenes and captivating character.\nItem: X-Men\nItem feature: screen, character, action, wolverine, scenes, intense, future\nUser preferences : director, definite, action\nGround-\ntruth\nIt 's brilliant that George Lucas kept some of the themes from the previousfilms going \nhere.\nCAML I like the war theme of thepopular and wonderful movie.\nNETE The movie is very popular and the themeis about war.\nReXPlug George Lucas genius shines inthis movie and the theme links to previousones. \nPETER This war movie is worth watching and it is a continuation of the previoustheme.\nPEVAE It is a popular movie, maintaining themes from previousfilms.\nPRAG This movie featuresbrilliant theme, widespreadpopularity and wonderful story telling.\nCVAEs This movie keptprevious theme, being highlypopular.\nLLM2ER George Lucas astutely kept the theme of theprevious films to create this war film.\nLLM2ER-\nEQR\nGeorge Lucas kept the theme of theprevious films, creating a wonderful and popular\nfilm.\nItem: Star Wars\nItem feature: George Lucas, theme, previous,war\nUser preferences : popular, wonderful, brilliant, previous\n3\n1\n2\nFigure 4: Explanations generated by baselines and\nLLM2ER-EQR. The item features and user preferences men-\ntioned in the generated explanation are highlighted.\ntributes to guide LLM generate explanations in the direction\nwithout harmful information. Nonetheless, we must admit\nthat when actually deploying our model in a real scenario,\nit is necessary to take post-processing to further prevent the\ndisplay of harmful information to users.\nConclusions and Future Work.In this paper, we pro-\npose a novel LLM-based ER backbone LLM2ER and fine-\ntune such a backbone as LLM2ER-EQR in an RL paradigm\nwith two explainable quality reward models, to address\nthree low-quality problems, i.e., personality, consistency,\nand poor review quality in explanations. Extensive exper-\niments demonstrate the superiority of LLM2ER over the\nstate-of-the-art methods for the explainable recommenda-\ntion tasks. In the future, we plan to fully exploit LLM2ER ’s\npotential by migrating to multimodal recommended forms.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9257\nAcknowledgments\nThis work was supported in part by the National Key R&D\nProgram of China (No. 2022YFF0902704), Leading Expert\nof “Ten Thousands Talent Program” of Zhejiang Province\n(No.2021R52001), and the cooperation project of MYbank,\nAnt Group.\nReferences\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language Models are Few-shot Learners. In\nProc. of NeurIPS, 1877–1901.\nCai, Z.; and Cai, Z. 2022. PEV AE: A Hierarchical V AE\nfor Personalized Explainable Recommendation. In Proc. of\nSIGIR, 692–702.\nChen, B.; and Cherry, C. 2014. A Systematic Comparison of\nSmoothing Techniques for Sentence-Level BLEU. In Proc.\nof WMT, 362–367.\nChen, X.; Chen, H.; Xu, H.; Zhang, Y .; Cao, Y .; Qin, Z.; and\nZha, H. 2019a. Personalized fashion recommendation with\nvisual explanations based on multimodal attention network:\nTowards visually explainable recommendation. In Proc. of\nSIGIR, 765–774.\nChen, Z.; Wang, X.; Xie, X.; Wu, T.; Bu, G.; Wang, Y .; and\nChen, E. 2019b. Co-attentive multi-task learning for ex-\nplainable recommendation. In Proc. of IJCAI, 2137–2143.\nGao, J.; Wang, X.; Wang, Y .; and Xie, X. 2019. Explain-\nable recommendation through attentive multi-view learning.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 33, 3622–3629.\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple\nContrastive Learning of Sentence Embeddings. In Proc. of\nEMNLP, 6894–6910.\nGeng, S.; Liu, S.; Fu, Z.; Ge, Y .; and Zhang, Y . 2022. Rec-\nommendation as language processing (rlp): A unified pre-\ntrain, personalized prompt & predict paradigm (p5). InProc.\nof RecSys, 299–315.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial nets. In Proc. of NeurIPS, 1–\n9.\nHada, D. V .; and Shevade, S. K. 2021. Rexplug: Explainable\nrecommendation using plug-and-play language model. In\nProc. of SIGIR, 81–91.\nHu, Y .; Liu, Y .; Miao, C.; Lin, G.; and Miao, Y . 2022.\nAspect-guided syntax graph learning for explainable recom-\nmendation. IEEE Transactions on Knowledge and Data En-\ngineering.\nHu, Z.; Dong, Y .; Wang, K.; and Sun, Y . 2020. Heteroge-\nneous graph transformer. In Proc. of WWW, 2704–2710.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\nBang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of hal-\nlucination in natural language generation. ACM Computing\nSurveys, 55(12): 1–38.\nKenton, J. D. M.-W. C.; and Toutanova, L. K. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Lan-\nguage Understanding. 4171–4186.\nLi, L.; Zhang, Y .; and Chen, L. 2020. Generate neural tem-\nplate explanations for recommendation. In Proc. of CIKM,\n755–764.\nLi, L.; Zhang, Y .; and Chen, L. 2021. Personalized Trans-\nformer for Explainable Recommendation. In Proc. of ACL,\n1–11.\nLi, L.; Zhang, Y .; and Chen, L. 2023. Personalized prompt\nlearning for explainable recommendation. ACM Transac-\ntions on Information Systems, 41(4): 1–26.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74–81. Association for Computational Linguistics.\nLu, H.; Ma, W.; Wang, Y .; Zhang, M.; Wang, X.; Liu, Y .;\nChua, T.-S.; and Ma, S. 2023. User Perception of Recom-\nmendation Explanation: Are Your Explanations What Users\nNeed? ACM Transactions on Information Systems, 41(2):\n1–31.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. In Proc. of NeurIPS, 27730–27744.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 1–12.\nShuai, J.; Zhang, K.; Wu, L.; Sun, P.; Hong, R.; Wang, M.;\nand Li, Y . 2022. A review-aware graph contrastive learning\nframework for recommendation. In Proc. of SIGIR, 1283–\n1293.\nWang, L.; Cai, Z.; de Melo, G.; Cao, Z.; and He, L. 2023.\nDisentangled CV AEs with Contrastive Learning for Ex-\nplainable Recommendation. In Proc. of AAAI, 13691–\n13699.\nWang, N.; Wang, H.; Jia, Y .; and Yin, Y . 2018. Explainable\nrecommendation via multi-task learning in opinionated text\ndata. In Proc. of SIGIR, 165–174.\nWang, X.; He, X.; Cao, Y .; Liu, M.; and Chua, T.-S. 2019.\nKgat: Knowledge graph attention network for recommenda-\ntion. In Proc. of SIGKDD, 950–958.\nWeidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato, J.;\nHuang, P.-S.; Cheng, M.; Glaese, M.; et al. 2021. Ethical\nand Social Risks of Harm from Language Models. arXiv\npreprint arXiv:2112.04359, 1–64.\nXie, Z.; Singh, S.; McAuley, J.; and Majumder, B. P. 2023.\nFactual and informative review generation for explainable\nrecommendation. In Proc. of AAAI, 13816–13824.\nYan, S.; Chen, X.; Huo, R.; Zhang, X.; and Lin, L. 2020.\nLearning to build user-tag profile in recommendation sys-\ntem. In Proc. of CIKM, 2877–2884.\nZhang, J.; Chen, X.; Tang, J.; Shao, W.; Dai, Q.; Dong, Z.;\nand Zhang, R. 2023. Recommendation with Causality en-\nhanced Natural Language Explanations. In Proceedings of\nthe ACM Web Conference 2023, 876–886.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9258\nZheng, L.; Noroozi, V .; and Yu, P. S. 2017. Joint deep mod-\neling of users and items using reviews for recommendation.\nIn Proc. of WSDM, 425–434.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n9259",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5798072814941406
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5610863566398621
    },
    {
      "name": "Language model",
      "score": 0.4326232671737671
    },
    {
      "name": "Natural language processing",
      "score": 0.3595620095729828
    },
    {
      "name": "Philosophy",
      "score": 0.06715524196624756
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}