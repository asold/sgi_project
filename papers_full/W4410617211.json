{
  "title": "Large language model-based multimodal system for detecting and grading ocular surface diseases from smartphone images",
  "url": "https://openalex.org/W4410617211",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2116899721",
      "name": "Zhongwen Li",
      "affiliations": [
        "Affiliated Eye Hospital of Wenzhou Medical College",
        "Wenzhou Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A4296351757",
      "name": "Zhouqian Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4383847541",
      "name": "Liheng Xiu",
      "affiliations": [
        "West China Second University Hospital of Sichuan University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2117517573",
      "name": "Pengyao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095843257",
      "name": "Wenfang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103587906",
      "name": "Yangyang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100962162",
      "name": "Gang Chen",
      "affiliations": [
        "Aksum University"
      ]
    },
    {
      "id": "https://openalex.org/A2112050736",
      "name": "Weihua Yang",
      "affiliations": [
        "Southern Medical University Shenzhen Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1964725655",
      "name": "Wei Chen",
      "affiliations": [
        "Wenzhou Medical University",
        "Affiliated Eye Hospital of Wenzhou Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2116899721",
      "name": "Zhongwen Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296351757",
      "name": "Zhouqian Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4383847541",
      "name": "Liheng Xiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117517573",
      "name": "Pengyao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095843257",
      "name": "Wenfang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103587906",
      "name": "Yangyang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100962162",
      "name": "Gang Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112050736",
      "name": "Weihua Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1964725655",
      "name": "Wei Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2139297286",
    "https://openalex.org/W6608192604",
    "https://openalex.org/W3130938027",
    "https://openalex.org/W2768870744",
    "https://openalex.org/W203371472",
    "https://openalex.org/W2069181705",
    "https://openalex.org/W4401397810",
    "https://openalex.org/W4399290669",
    "https://openalex.org/W1985090939",
    "https://openalex.org/W3166842197",
    "https://openalex.org/W3136242128",
    "https://openalex.org/W4402289599",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4322758112",
    "https://openalex.org/W4392906075",
    "https://openalex.org/W2078351197",
    "https://openalex.org/W2057203552",
    "https://openalex.org/W2790600564",
    "https://openalex.org/W2137693004",
    "https://openalex.org/W4396833233",
    "https://openalex.org/W3193478640",
    "https://openalex.org/W2079257804",
    "https://openalex.org/W4401824351",
    "https://openalex.org/W2346062110",
    "https://openalex.org/W4385833575",
    "https://openalex.org/W4387665659",
    "https://openalex.org/W4391971084",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4321238204",
    "https://openalex.org/W4400383179",
    "https://openalex.org/W3093256930"
  ],
  "abstract": "Background The development of medical artificial intelligence (AI) models is primarily driven by the need to address healthcare resource scarcity, particularly in underserved regions. Proposing an affordable, accessible, interpretable, and automated AI system for non-clinical settings is crucial to expanding access to quality healthcare. Methods This cross-sectional study developed the Multimodal Ocular Surface Assessment and Interpretation Copilot (MOSAIC) using three multimodal large language models: gpt-4-turbo, claude-3-opus, and gemini-1.5-pro-latest, for detecting three ocular surface diseases (OSDs) and grading keratitis and pterygium. A total of 375 smartphone-captured ocular surface images collected from 290 eyes were utilized to validate MOSAIC. The performance of MOSAIC was evaluated in both zero-shot and few-shot settings, with tasks including image quality control, OSD detection, analysis of the severity of keratitis, and pterygium grading. The interpretability of the system was also evaluated. Results MOSAIC achieved 95.00% accuracy in image quality control, 86.96% in OSD detection, 88.33% in distinguishing mild from severe keratitis, and 66.67% in determining pterygium grades with five-shot settings. The performance significantly improved with the increasing learning shots (p &amp;lt; 0.01). The system attained high ROUGE-L F1 scores of 0.70–0.78, depicting its interpretable image comprehension capability. Conclusion MOSAIC exhibited exceptional few-shot learning capabilities, achieving high accuracy in OSD management with minimal training examples. This system has significant potential for smartphone integration to enhance the accessibility and effectiveness of OSD detection and grading in resource-limited settings.",
  "full_text": "TYPE Original Research\nPUBLISHED 23 May 2025\nDOI 10.3389/fcell.2025.1600202\nOPEN ACCESS\nEDITED BY\nHewa Majeed Zangana,\nUniversity of Duhok, Iraq\nREVIEWED BY\nMarwan Omar,\nIllinois Institute of T echnology, United States\nFiras Mustafa,\nDuhok Polytechnic University, Iraq\n*CORRESPONDENCE\nWei Chen,\nchenweimd@wmu.edu.cn\nWeihua Yang,\nbenben0606@139.com\nZhongwen Li,\nli.zhw@wmu.edu.cn\n†These authors have contributed equally to\nthis work and share first authorship\nRECEIVED 26 March 2025\nACCEPTED 15 May 2025\nPUBLISHED 23 May 2025\nCITATION\nLi Z, Wang Z, Xiu L, Zhang P , Wang W, Wang Y,\nChen G, Yang W and Chen W (2025) Large\nlanguage model-based multimodal system\nfor detecting and grading ocular surface\ndiseases from smartphone images.\nFront. Cell Dev. Biol. 13:1600202.\ndoi: 10.3389/fcell.2025.1600202\nCOPYRIGHT\n© 2025 Li, Wang, Xiu, Zhang, Wang, Wang,\nChen, Yang and Chen. This is an open-access\narticle distributed under the terms of the\nCreative Commons Attribution License (CC\nBY). The use, distribution or reproduction in\nother forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication\nin this journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nLarge language model-based\nmultimodal system for detecting\nand grading ocular surface\ndiseases from smartphone\nimages\nZhongwen Li 1,2†*, Zhouqian Wang 1†, Liheng Xiu 3†,\nPengyao Zhang 1, Wenfang Wang 1, Yangyang Wang 1,\nGang Chen 4, Weihua Yang 5* and Wei Chen 2*\n1Ningbo Key Laboratory of Medical Research on Blinding Eye Diseases, Ningbo Eye Institute, Ningbo\nEye Hospital, Wenzhou Medical University, Ningbo, China, 2National Clinical Research Center for\nOcular Diseases, Eye Hospital, Wenzhou Medical University, Wenzhou, China, 3Department of\nOphthalmology, West China Second University Hospital, Sichuan University, Chengdu, China, 4First\nPeople’s Hospital of Aksu, Aksu, China, 5Shenzhen Eye Hospital, Shenzhen Eye Medical Center,\nSouthern Medical University, Shenzhen, China\nBackground: The development of medical artificial intelligence (AI) models is\nprimarily driven by the need to address healthcare resource scarcity, particularly\nin underserved regions. Proposing an affordable, accessible, interpretable, and\nautomated AI system for non-clinical settings is crucial to expanding access to\nquality healthcare.\nMethods: This cross-sectional study developed the Multimodal Ocular Surface\nAssessment and Interpretation Copilot (MOSAIC) using three multimodal large\nlanguage models: gpt-4-turbo, claude-3-opus, and gemini-1.5-pro-latest, for\ndetecting three ocular surface diseases (OSDs) and grading keratitis and\npterygium. A total of 375 smartphone-captured ocular surface images collected\nfrom 290 eyes were utilized to validate MOSAIC. The performance of MOSAIC\nwas evaluated in both zero-shot and few-shot settings, with tasks including\nimage quality control, OSD detection, analysis of the severity of keratitis, and\npterygium grading. The interpretability of the system was also evaluated.\nResults: MOSAIC achieved 95.00% accuracy in image quality control, 86.96% in\nOSD detection, 88.33% in distinguishing mild from severe keratitis, and 66.67%\nin determining pterygium grades with five-shot settings. The performance\nsignificantly improved with the increasing learning shots (p < 0.01). The system\nattained high ROUGE-L F1 scores of 0.70–0.78, depicting its interpretable image\ncomprehension capability.\nConclusion: MOSAIC exhibited exceptional few-shot learning capabilities,\nachieving high accuracy in OSD management with minimal training examples.\nThis system has significant potential for smartphone integration to enhance\nFrontiers in Cell and Developmental Biology 01 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nthe accessibility and effectiveness of OSD detection and grading in resource-\nlimited settings.\nKEYWORDS\nocular surface disease, large language model, multimodal model, keratitis,\nconjunctivitis, pterygium\nIntroduction\nOcular surface diseases (OSDs) significantly contribute to\nglobal eye health challenges (Burton et al., 2021). Several OSDs\ncan lead to serious adverse consequences if not addressed timely.\nFor instance, keratitis is a leading cause of corneal blindness and\nvisual impairment worldwide ( Stapleton, 2023). Conjunctivitis,\na prevalent condition, imposes substantial economic and social\nburdens ( Azari and Barney, 2013 ). Additionally, pterygium,\none of the most common eye disorders, is associated with\naesthetic concerns, irregular astigmatism, and decreased vision\n(Rezvan et al., 2018). Early detection and appropriate treatment\nof OSDs are crucial for preventing vision loss and preserving\nocular health (Saaddine et al., 2003).\nUnfortunately, access to specialized ophthalmic care is often\nlimited, particularly in underserved regions, impeding timely\ndiagnosis of OSDs ( Resnikoff et al., 2012 ; Gupta et al., 2013 ).\nWhile portable devices have been employed in some studies\nto capture images in non-clinical settings, prompt responses\nfrom experienced experts remain indispensable ( Caffery et al.,\n2019). Recent studies have leveraged artificial intelligence (AI) to\ndevelop efficient solutions for automated disease detection and\nmanagement, aiming to mitigate the shortage of expert resources\n(Tan et al., 2023; Li et al., 2024)\nDespite the significant advancements in AI, its integration\ninto clinical practice faces several challenges. Firstly, although\nnumerous studies have developed AI systems, patients are often\nunable to access them due to the lack of public availability and\nthe persistent gap between research and product implementation\n(Closing the translation gap, 2025 ). Furthermore, most studies\napplying AI to analyze OSDs relied on anterior segment images\ncaptured by specialized devices such as the slit lamp, limiting\nthe models’ applicability in remote and underserved regions\n(Zhang et al., 2023 ; Zhongwen et al., 2025 ). These significant\nobstacles contribute to the absence of an efficient and practical\ntool for detecting and managing OSDs.\nTo make medical AI services more accessible, we developed\nMultimodal Ocular Surface Assessment Intelligent Copilot\n(MOSAIC), a large language model-based AI system with extensible\ncomponents, which included modularized agents for image quality\nAbbreviations: OSD, Ocular surface disease; AI, artificial intelligence;\nMOSAIC, Multimodal Ocular Surface Assessment Intelligent Copilot; MLLM,\nmultimodal large language model; UCSI, Union Centers Smartphone Image;\nIAP , Image Analysis Pipeline; STM, short-term memory; IQC, Image Quality\nController; DSD, Disease Detector; SVA, Severity Analyzer; JD, Jiangdong;\nGPT4V, gpt-4-turbo; CLD3O, claude-3-opus; GM15P , gemini-1.5-pro-latest;\nAPI, application program interface; ROUGE-L, Recall-Oriented Understudy\nfor Gisting Evaluation; ACC, accuracy; NB, Ningbo Eye Hospital; WZ, Eye\nHospital of Wenzhou Medical University.\ncontrol, OSD recognition, analysis of the severity of keratitis, and\npterygium grading. MOSAIC is constructed by integrating publicly\naccessible multimodal large language models (MLLMs) with the\nstrategies of prompt engineering and few-shot prompt learning.\nPrompt engineering and few-shot prompt learning have shown\npotential as effective methods in optimizing and adjusting large\nlanguage models (Wang et al., 2024; Šuster et al., 2024 ). Based\non MOSAIC, we established an automated pipeline for analyzing\nOSDs from smartphone images. To be specific, we first validated\nMOSAIC’s ability to monitor image quality, which is used to\nfilter out poor-quality images and identify the reasons for their\ninadequacy. In addition, we assessed MOSAIC’s ability to detect\nkeratitis, conjunctivitis, and pterygium using the Union Centers\nSmartphone Image (UCSI) dataset. Furthermore, we investigated\nMOSAIC’s ability to aid disease management by identifying mild-\nstagekeratitisforearlyinterventionandassessingpterygiumseverity\nto determine the optimal timing for surgery. Finally, we explored\nMOSAIC’s interpretability by assessing its image comprehension\ncapability. This study demonstrated that MOSAIC offers great\npotential for detecting and grading OSDs in general populations\nwithin non-clinical settings.\nMethods\nDesign of MOSAIC\nMOSAIC was designed as an automated and extensible system\nprocessing input images and generating output reports (Figure 1).\nMOSAIC comprises two primary modules: the Agent Allocator and\nthe Image Analysis Pipeline (IAP). The Agent Allocator functions\nas a router, assigning agents for various sub-tasks within the\nIAP. These agents are driven by prompt engineering techniques\nand short-term memory (STM) mechanisms. Prompt engineering\nhas emerged as a crucial method for adapting large language\nmodels (LLMs) to specific downstream tasks ( Liu et al., 2023 ).\nDrawing inspiration from previous studies, we composed a set of\ninstructionprompts(SupplementalNoteS1)to“anthropomorphize”\nMLLMs into distinct agents, enhancing and calibrating them for\nmultiple tasks (Kang and Kim, 2024). The STM was implemented\nusing few-shot prompt learning, a technique that enhanced\nmodel performance by providing a small number of examples to\nthe model (Brown et al., 2020).\nThe IAP of MOSAIC comprises a sequential combination of\nthree agents. First, the “Image Quality Controller” (IQC) assesses\nthe quality of the input image, determining its eligibility for\nsubsequent tasks. If the IQC deems the image “eligible”, it is\nthen forwarded to the “Disease Detector” (DSD) for disease\nidentification. Otherwise, an error message is generated, explaining\nthe reason for ineligibility and providing instructions for capturing\nFrontiers in Cell and Developmental Biology 02 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nFIGURE 1\nDesign and architecture of MOSAIC. MOSAIC was constructed with several components to keep its extendibility. The agent allocator acts as a router to\ncreate specific agents for sub-tasks in IAP . IAP is the automatic sequential workflow for analyzing keratitis, conjunctivitis, and pterygium with\nsmartphone images. IAP Image Analysis Pipeline, IQC Image Quality Controller, DSD Disease Detector, SVA Severity Analyzer.\nan eligible image. Following disease detection, the image is\ntransmitted to the “Severity Analyzer” (SVA) to evaluate the\nseverity of the identified disease. Based on the results from\neach agent in the IAP, a comprehensive final report is yielded\nto the user.\nPrompt engineering and STM\nAgents in IAP were “anthropomorphized” by a set of identities\nand memories. These identities were constructed using “instruction\nprompts” that were engineered in five dimensions: 1) Assigning\na name to the agent that the model would perform; 2) Defining\nthe intent and motivation that describe the problem the agent\nshould solve; 3) Specifying the knowledge the agent should possess;\n4) Customizing the output format for generation; 5) Establishing\nguardrails to prevent inappropriate responses (White et al., 2023).\nTo fully harness the potential of models, we employed\nSTM for agents in IAP using a few-shot prompt\nlearning paradigm ( Supplementary Figure S1). We conducted\nthree levels of few-shot prompt learning in this study: zero-\nshot, one-shot, and five-shot, to observe changes in system\nperformance and determine the optimal level for our system.\nThe images utilized for memory construction were obtained\nfrom an independent Jiangdong (JD) clinical center to avoid\nfeature leakage.\nUCSI dataset\nMOSAIC was evaluated on the UCSI dataset, which involved\nocular surface images captured by various smartphone brands from\nindependent clinical centers. The imaging settings, including zoom\nscale, exposure, and camera mode, were maintained as the default.\nFor subset A, labels were established through a consensus among\nthree experts, following criteria proposed in our previous study\n(Lietal.,2021a ).Incasesofdisagreement,apanelofOSDspecialists,\nincluding a senior specialist with 20 years of clinical experience,\nconvened to deliberate until reaching a unanimous decision. For\nsubsets B, C, and D, image labels were determined by reviewing\npatients’ medical records and associated media. The definition of\nFrontiers in Cell and Developmental Biology 03 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nmild-stage keratitis adhered to guidelines from previous studies\n(Stapleton et al., 2012; Keay et al., 2008; Li et al., 2021b). Pterygium\ngrading criteria primarily focused on surgical timing, as indicated\nby the location of the pterygium head relative to the corneal limbus\nand pupil (Liu et al., 2024; Maheshwari, 2007).\nComparison of leading MLLMs\nAs MLLMs form the backbone of MOSAIC, we conducted\ncomparative analyses of three MLLMs to identify the most suitable\nmodel for our system: gpt-4-turbo (GPT4V, OpenAI), claude-\n3-opus (CLD3O, Anthropic), and gemini-1.5-pro-latest (GM15P,\nGoogle). Models were requested through the Python library’s\nofficial application program interface (API) to prevent additional\ndata processing between the model and user, which could\noccur in chatbot web interfaces. Given the inherent stochasticity\nof transformer-based generative models, we carefully controlled\nhyperparameters to ensure the reproducibility of results. The\ndetailedsettingsareprovidedin Supplementary Table S1.Generated\nresponses were recorded and analyzed to evaluate the system’s\nperformance.\nImage quality control\nThe MOSAIC system was designed for non-professionals who\nmay lack medical imaging experience, enabling them to capture\nhigh-quality images using consumer-grade devices in non-clinical\nsettings. Toensure thereliability ofsubsequent disease-related tasks,\nwe implemented the IQC as the guardian of the IAP. The primary\nfunction of the IQC is to classify input images into four categories:\neligible, defocused, poor-field, and poor-location. Additionally, the\nIQC provides a detailed rationale for each classification decision\n(examplesareshownin Figure 2).Onlyimagesclassifiedas“eligible”\nby the IQC will be passed to the DSD. If the IQC deems an image\n“ineligible”, subsequent tasks will not be performed. Additionally,\nthe system will return a message to the user explaining the reason\nfor ineligibility and recommending effective approaches to capture\nanother image of eligible quality.\nDisease detection\nIn this study, we evaluated MOSAIC, focusing on three\ncommon OSDs: keratitis, conjunctivitis, and pterygium. After\npassing through the IQC, the input image will be conveyed\nto the DSD for disease detection. If the DSD yields “No\nkeratitis, conjunctivitis, or pterygium detected”, consequent severity\nanalysis will not be performed, and MOSAIC will generate\na report with negative results for the user. Otherwise, DSD\nprovides a diagnosis based on the image. The OSDs’ definitions\nand clinical characteristics were incorporated into the DSD\nidentity prompts and STM to promote the alignment of visual\nand natural language information. Examples of this phase are\npresented inFigure 3.\nSeverity analysis\nEarly detection of keratitis, particularly in its mild form when\nclinical features are subtle, is crucial for optimizing visual outcomes.\nTo address this, we designed a function to detect mild-stage keratitis\nfor the SVA. The definition of mild keratitis aligned with the\nestablished criteria for grading keratitis severity, which categorized\ncasesasmildifthelesionwaslocatedoutsidethecentral4 mmofthe\ncornea and had a diameter less than 2 mm. For pterygium, surgical\nintervention is the primary treatment when it encroaches upon the\ncornea and compromises visual acuity, as the restoration of corneal\ntopography is significantly related to pterygium development. To\nmonitor the pterygium progression and facilitate timely surgical\nintervention, we incorporated a function for the SVA to grade\npterygium severity, following the criteria that mainly considered\npterygium size and its relationship to the cornea. Examples of this\nphase are presented inFigure 4.\nInterpretability of MOSAIC\nIn contrast to conventional deep learning models for\nclassification tasks that only output labels, MLLMs can provide\nnot only predicted labels but also natural language explanations\nfor their decision-making. To evaluate the interpretative potential\nof MOSAIC, we calculated and visualized the ROUGE-L (Recall-\nOriented Understudy for Gisting Evaluation) F1 score metric\nbetween the image description generated by the model and the\nreference explanations used in prompt engineering. This analysis\nquantified MOSAIC’s image understanding ability and provided\ninsights into its interpretability.\nStatistical analysis\nStatistical analyses were conducted using Python 3.10.14. The\ndifferences in accuracy (ACC) were analyzed using the McNemar\ntest. All statistical tests were two-sided with a significance level of\n0.05. The interpretability assessment was conducted with the rouge\npackage(pypi.org/project/rouge)andvisualizedwithR4.4.1ggplot2\npackage (ggplot2.tidyverse.org).\nEthics statement\nThe study was approved by the Institution Review Board of\nNEH (identifier, 2020-qtky-017) and adhered to the principles of\nthe Declaration of Helsinki. Informed consent was exempted, due\nto the retrospective nature of the data acquisition and the use of\ndeidentified images.\nResults\nDataset characteristics\nThe UCSI dataset comprised a total of 375 images from\n290 eyes in four distinct subsets for various sub-tasks. Subset\nFrontiers in Cell and Developmental Biology 04 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nFIGURE 2\nTask examples of the IQC. Based on our previous study on ocular surface image quality, we employed the following definitions for image quality\ncategories. An image is classified as ineligible if it meets any of the following criteria: 1) Defocused images refer to blurry images in which the focus is\nnot on the cornea. 2) Poor-field images refer to images in which one-fifth of the cornea was covered by eyelids. 3) Poor-location images refer to\nimages in which one-fifth of the cornea was blurred because the cornea was not straight ahead. 4) An image quality is deemed eligible if it does not\nmeet any of the aforementioned criteria. IQC Image Quality Controller.\nA comprised 60 images categorized into four image qualification\ngroups: eligible, defocused, poor-field, and poor-location. Subset B\nconsisted of 140 images classified into four diagnostic categories:\nkeratitis, conjunctivitis, pterygium, and normal. Subset C included\n70 images divided into two keratitis stage categories: mild stage\nand non-mild stage. Subset D encompassed 105 images categorized\ninto three pterygium stages: observation (grade one), surgical\nconsideration (grade two), and immediate surgery (grade three).\nImages collected from Ningbo Eye Hospital (NB) and Eye Hospital\nof Wenzhou Medical University (WZ) were utilized to evaluate the\nperformance of MOSAIC, while images from JD were employed\nto construct STMs. Detailed information regarding the datasets is\npresented inTable 1.\nPerformance of the IAP components\nThe performance of the IAP components varied depending on\nthe specific models employed and the extent of few-shot prompt\nlearning implemented. Generally, we observed that as the number\nof learning examples increased, the ACCs of the IAP components\ndemonstrated improving trends (Figure 5; Table 2).\nThe IQC achieved an ACC of 95.00% in assessing the quality\nof input images utilizing the GPT4V model in the five-shot setting.\nIn the zero-shot setting, the ACC did not exceed 50.00% for\nany of the tested models. However, with only one-shot prompt\nlearning, the ACC improved to 65.00%–75.00%, indicating the\nsignificance of few-shot prompt learning. Additionally, the GM15P\nmodel achieved 85.00% ACC with the five-shot setting, while the\nCLD3O model demonstrated unsatisfactory performance in IQC,\nwith ACC ranging from 47.50% to 65.00%.\nBased on images that met quality criteria as screened by\nIQC, DSD achieved an ACC of 87.50% in detecting keratitis,\nconjunctivitis, pterygium, and normal utilizing the GM15P model\nin the five-shot setting, demonstrating superior performance\ncompared to the GPT4V model. The CLD3O model proved\nunsuitable as the backbone of DSD, with ACC ranging from\n33.33% to 40.83%.\nFrontiers in Cell and Developmental Biology 05 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nFIGURE 3\nTask examples of the DSD. The diagnostic definitions are as follows. 1) Keratitis: Keratitis is the inflammation of the cornea. 2) Pterygium: Pterygium is a\nroughly triangular tissue growth extending from the conjunctiva onto the cornea. 3) Conjunctivitis: Conjunctivitis refers to inflammation of the\noutermost layer of the white part of the eye or the inner surface of the eyelid. 4) Normal: No signs of the aforementioned conditions. DSD\nDisease Detector.\nWiththediagnosisdecisionmadebyDSD,SVAfurtheranalyzed\nthe severity of the detected disease. For recognizing mild-stage\nkeratitis, SVA achieved an ACC of 88.33% with the GPT4V model\nin the five-shot setting. It is worth mentioning that all three models\nonly attained the ACC of 50% in the zero-shot setting, classifying\nall test images as “non-mild stage of keratitis”. The GPT4V model\nnot only demonstrated the best performance among the three in\nthe five-shot setting but also attained an ACC exceeding 80.00%\n(83.33%) with only one learning example, exhibiting outstanding\nfew-shot prompt learning capability. Remarkably, few-shot prompt\nlearning seemed ineffective for the CLD3O model in this sub-task,\nconsidering all images as “non-mild stage keratitis” even when the\nlevel of learning increased to five. For grading pterygium, SVA\nachieved an ACC of 66.67% with the Google model in the five-shot\nsetting. The CLD3O model still demonstrated limited performance\nof 33.33%–37.78%, and OpenAI attained unsatisfactory results of\n50.00%–57.78%.\nBased on these results, we employed the optimal model for\neach sub-task agent with five learning shots, enabling MOSAIC to\nfunction as a flexible framework that leverages each model’s unique\nstrengths.\nInterpretability assessment\nROUGE-L F1 scores were employed in this study to\nquantify MOSAIC’s image understanding capability. The IAP\ncomponents attained the average ROUGE-L F1 score of 0.78,\n0.70, 0.72, and 0.76 for IQC, DSD, SVA (detecting mild stage\nkeratitis), and SVA (grading pterygium), respectively.Figures 6a–d\nillustrates the distribution of the ROUGE-L F1 scores for the\nFrontiers in Cell and Developmental Biology 06 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nFIGURE 4\nTask examples of the SVA. The definition of keratitis in the mild stage refers to the lesion located outside the central cornea with a diameter of less than\n2 mm. The criteria of pterygium grading mainly focus on the surgical timing indicated by the location of the pterygium head, corneal limbus, and\npupillary, which categorizes cases as grade one if the length of the limbal invasion is between 0 and 2 mm; as grade two if the invasion is between 2\nand 4 mm and as grade three if the invasion was exceeding 4 mm. SVA Severity Analyzer.\nTABLE 1 Composition of the UCSI dataset.\nSubset Evaluation task Test data Memory construction data ( ∗N-shot)\nNB WZ JD\nA Image quality control 20 20 4\nB OSDs detection 81 39 4\nC Keratitis stage analyzing 30 30 2\nD Pterygium stage analyzing 60 30 3\nThe UCSI, dataset comprises four subsets (A-D) designed for distinct tasks. The memory construction data was sourced exclusively from the JD, center to prevent feature leakage. The N-shot\nprompt learning provides models with N pairs of examples for each category during the prediction. UCSI, union centers smartphone image; NB, ningbo eye hospital; WZ, eye hospital of\nwenzhou medical university; JD, Jiangdong Eye Hospital. N number, OSD, ocular surface disease.\nFrontiers in Cell and Developmental Biology 07 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nFIGURE 5\nComparing the performance of MLLMs and few-shot levels for agents in the MOSAIC. (a–d). Confusion matrices describing the prediction results of\nthree MLLMs and three few-shot levels for agents IQC, DSD, SVA (keratitis stage), and SVA (pterygium grade) in order. (e–h). The accuracies of three\nMLLMs and three few-shot levels for agents in the same order. IQC Image Quality Controller, DSD Diseases Detector, SVA Severity Analyzer. MLLMs\nmultimodal large language model. EL eligible, DF defocused, PF poor-field, PL poor-location. KT keratitis, CJ conjunctivitis, PT pterygium, NM normal,\nMK keratitis (non-mild stage), NK keratitis (mild stage), G1 (pterygium grade one), G2 (pterygium grade two), G3 (pterygium grade three).\nTABLE 2 Differences of ACC between few-shot levels.\nSub-tasks Few-shot level GPT4V CLD3O GM15P\nIQC\n0 vs 1 <0.01 <0.01 <0.01\n0 vs 5 <0.01 <0.01 <0.01\n1 vs 5 <0.01 <0.01 <0.01\nDSD\n0 vs 1 <0.01 <0.01 <0.01\n0 vs 5 <0.01 <0.01 <0.01\n1 vs 5 <0.01 <0.01 <0.01\nSVA\nKeratitis\nStage\n0 vs 1 <0.01 0.77 <0.01\n0 vs 5 <0.01 0.84 <0.01\n1 vs 5 <0.01 0.17 <0.01\nPterygium\nGrade\n0 vs 1 <0.01 <0.01 <0.01\n0 vs 5 <0.01 <0.01 <0.01\n1 vs 5 <0.01 0.51 <0.01\nOverall, the performance of each IAP, component improved as the few-shot level increased. Exceptionally, the performance of the CLD3O model did not improve even in the five-shot setting.\nACC, accuracy; IAP image analysis pipeline; IQC, image quality controller; DSD, diseases detector; SVA, severity analyzer, GPT4V gpt-4-turbo, CLD3O claude-3-opus, GM15P\ngemini-1.5-pro-latest.\nsystem’s comprehension processes. Correctly classified test images\ndemonstrated high scores, indicating that accurate classification\ndecisions were based on proper interpretations of the input images.\nConversely, misclassified test images exhibited lower scores in the\nsystem’scomprehensionprocesses,suggestingthatinadequateimage\nunderstanding led to classification errors.\nFrontiers in Cell and Developmental Biology 08 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nFIGURE 6\nThe distribution of ROUGE-L F1 scores for MOSAIC’s interpretation of images. (a–d). The ROUGE-L F1 scores of each test image for IQC, DSD, SVA\n(keratitis stage), and SVA (pterygium grade). Higher scores are aligned with correct classification results, and lower scores are aligned with wrong\nclassification results, suggesting that the decisions made by MOSAIC agree with the reasonings. IQC Image Quality Controller, DSD Diseases Detector,\nSVA Severity Analyzer. (K) Keratitis stages, (P) pterygium grades. ROUGE-L Recall-Oriented Understudy for Gisting Evaluation.\nDiscussion\nIn this study, we developed MOSAIC, an MLLM-based AI agent\nsystem for detecting three common OSDs from smartphone images.\nWeevaluatedthesystemusingimagesfromtwoindependentclinical\ncenters within the UCSI dataset. Three MLLMs were assessed\nleveraging various levels of few-shot prompt learning. Additionally,\nwe quantified the image understanding capability of the MLLMs to\ninterpret the reasoning underlying their decision-making processes.\nWith only five-shot learning examples, MOSAIC achieved an ACC\nof 95.00% in controlling input image quality (with GPT4V model),\n87.50% in detecting three OSDs (with GM15P model), 88.33%\nin recognizing mild-stage keratitis (with GPT4V model), and\n66.67% in determining the progression stage of pterygium (with\nGM15P model).\nOSDs can lead to severe consequences if not addressed\npromptly, especially in less developed communities where\nspecialized equipment and experts are scarce. Patients typically\nseek treatment only after their visual acuity has been significantly\ncompromised (Burton, 2009). MOSAIC can enable patients to\nutilize AI models as personal healthcare copilots. Users can simply\ncapture an ocular surface image with a smartphone, upload it to\nthe system, and receive a comprehensive report. Through this\napproach,MOSAICdemonstratespromiseinempoweringhigh-risk\npopulations to proactively manage their eye health from home, for\nexample, detecting keratitis at an early stage before clinical features\nbecome apparent or monitoring the progression of pterygium to\ndetermine optimal surgical timing to reduce the risk of vision\nimpairment.\nTo identify the optimal model for each agent in MOSAIC,\nwe evaluated three MLLMs–GPT4V, CLD3O, and GM15P—across\nsubtasks within the IAP module. We found that: 1) The GPT4V\nmodel surpassed the other models in determining the input\nimage quality and detecting mild stage keratitis. 2) The GM15P\novercame the other models in detecting three OSDs from the\nnormal and grading pterygium progression (Table 3). The varying\nperformanceofdifferentmodelsacrosssubtasksmaybeattributedto\ndifferencesintheirtrainingdataandmodelarchitectures.Notably,in\nidentifyingkeratitisseverity,allthreemodelsdemonstrated100.00%\nsensitivity for “non-mild stage keratitis” in the zero-shot setting,\nsuggesting the models’ conservative approaches when faced with\npotentiallyhigh-stakestaskswithoutpriorexamples.Inthiscontext,\nFrontiers in Cell and Developmental Biology 09 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nTABLE 3 Differences of ACC between models.\nSub-tasks MLLM Zero-shot One-shot Five-shot\nIQC\nGPT4V vs CLD3O <0.01 0.44 <0.01\nGPT4V vs GM15P <0.01 <0.01 <0.01\nCLD3O vs GM15P 0.41 <0.01 <0.01\nDSD\nGPT4V vs CLD3O <0.01 <0.01 <0.01\nGPT4V vs GM15P <0.01 <0.01 <0.01\nCLD3O vs GM15P <0.01 <0.01 <0.01\nSVA\nKeratitis\nStage\nGPT4V vs CLD3O 0.12 <0.01 <0.01\nGPT4V vs GM15P 0.47 <0.01 <0.01\nCLD3O vs GM15P 0.90 <0.01 <0.01\nPterygium\nGrade\nGPT4V vs CLD3O <0.01 <0.01 <0.01\nGPT4V vs GM15P <0.01 <0.01 <0.01\nCLD3O vs GM15P <0.01 <0.01 <0.01\nIn the zero-shot setting, three MLLMs, performed comparably in detecting the keratitis stage. As the few-shot level increased, the differences in the models’ learning capabilities emerged. ACC,\naccuracy; IQC, image quality controller; DSD, diseases detector; SVA, severity analyzer, GPT4V gpt-4-turbo, CLD3O claude-3-opus, GM15P gemini-1.5-pro-latest, MLLM, multimodal large\nlanguage model.\nthe GPT4V model significantly improved its ACC to 83.33% with\njust one-shot learning, demonstrating outstanding few-shot prompt\nlearning capabilities. In contrast, the CLD3O model consistently\nunderperformed in this study, contradicting reported claims of its\nexcellency (Kaczmarczyk et al., 2024; Toufiq et al., 2023; Shojaee-\nMend et al., 2024). This discrepancy underscores the importance of\nevaluating models comprehensively across multiple modalities and\ndimensions.\nTo enhance model performance and reduce training\ndata costs, fine-tuning pre-trained foundation models for\nspecific medical downstream tasks has become a common\ndevelopment paradigm (Tajbakhsh et al., 2016). However, while\nfine-tunedmodelsdemonstrateimprovedperformanceinparticular\ndomains, their generalization capability in other domains inevitably\ndeclines. Moreover, fine-tuning requires high-powered devices,\nexperienced engineers, and domain-specific data, which are often\ninaccessibleinlessdevelopedareas.Inthisstudy,wealignedMLLMs\nwith diverse downstream tasks by constructing and injecting\ntask-specific memories into the model’s “thinking” process. This\napproach, termed few-shot prompt learning, eliminates the need\nfor extensive computational resources or large datasets. Instead, it\nrequires only a small set of images and text instructions to construct\nthe necessary memory, enabling the model to achieve impressive\nperformance across various medical tasks.\nDespite their remarkable performance capabilities,\nlarge language models remain susceptible to spontaneous\nhallucinations, which significantly compromises their reliability\nand trustworthiness. To enhance the MOSAIC’s credibility, we\ninstructed the models to generate both a predicted label (e.g.,\n“image with eligible quality”, “keratitis”, “pterygium grade two”, etc.)\nand an explanation for the decision-making process, including the\ninterpretation of the test image. Through this approach, MOSAIC\ncan provide suggestions for a patient’s ocular surface health\nconditions using eye images, effectively serving as a personal health\ncopilot. To quantify the system’s interpretability, we calculated the\nROUGE-L F1 scores of the generated explanations. The distribution\nplotofthesescoresrevealedthatcorrectlyclassifiedimagesexhibited\nhigher scores, while misclassified images demonstrated lower\nscores. This feature provides users with an additional safeguard\nto model hallucination, as the higher the score, the more reliable the\ninformation the system offers.\nRecently, several studies exploring the border of MLLMs in\nclinical scenarios have been published. Kaczmarczyk et al. evaluated\nthe accuracy and responsiveness of MLLMs in answering the\nNEJM Image Challenge dataset and found that the best model\ndemonstrated an accuracy of 58.8%–59.8% among various models\n(Kaczmarczyk et al., 2024 ). They also found that a model may\nrefuse to answer some questions, which also happened in our\npreliminary experiments and was solved by the strict engineering\nofprompts( Table 4).Zhuet al.evaluatedtheperformanceofMLLM\nininterpretingradiologicalimagesandformulatingtreatmentplans,\nfinding that it achieved 77.01% accuracy on the United States\nMedical Licensing Examination questions (Lingxuan et al., 2024).\nCompared to prior studies, our research had several significant\nfeatures. Firstly, we designed an AI agent system with extensible\ncomponents to deal with queries about OSDs in non-clinical\nenvironments automatically. Models do not just act as a “black\nbox” to handle inputs and yield outputs in our study. Instead, it\nFrontiers in Cell and Developmental Biology 10 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nTABLE 4 Responsiveness in preliminary experiments without prompt engineering.\nAgent Image label Responsiveness in the zero-shot setting Responsiveness in the five-shot setting\nIQC - 100.00% 100.00%\nDSD\nKeratitis 85.71% 96.77%\nConjunctivitis 83.33% 100.00%\nPterygium 68.18% 93.75%\nNormal 63.83% 71.43%\nSVA - 100.00% 100.00%\nIn our preliminary experiments, we observed that models occasionally refuse to respond in some cases. After careful refinement of instruction prompts, the responsiveness increased to\n100.00%, demonstrating the critical role of prompt engineering in optimizing interactions with MLLMs. IQC, image quality controller; DSD, diseases detector; SVA, severity analyzer; MLLM,\nmultimodal large language model.\nwas utilized as a backbone “engine” for each step in the whole\nsystem. The architecture of MOSAIC could be transferred to\nother similar research, and the agents allocated by the Agent\nAllocator can be extended according to the study purposes.\nSecondly, given the inevitable randomness of the transformer-based\nmodel, we controlled the hyper-parameters of involved models,\nincluding “temperature”, “max-token”, etc., to make our results\nreproducible. Last, the images in this study were not disclosed\nbefore,eliminatingthepossibilityofdatasetcontamination,wherein\nthe test images might have been inadvertently included in the\nmodels’ training datasets.\nThis study has several limitations. First, we evaluated MOSAIC,\nwhich primarily focused on OSDs and did not extend to other\ndiseases. This narrow focus, while allowing for a detailed analysis\nof OSDs, limits the applicability of our findings to a broader range\nof eye conditions. We intend to expand our evaluation to other\ndiseases in future studies. Second, the study was conducted on a\nrelatively limited dataset. While test data contained images from\ntwo individual centers, it might not fully represent the diversity\nof real-world scenarios. In future work, we plan to curate more\nextensive and diverse datasets to validate further and potentially\nenhancetherobustnessofoursystem.Third,ourstudyconcentrated\non the three leading proprietary models, excluding open-source\nalternatives from the assessment. However, our intention was to\nenhance AI accessibility globally, particularly in underdeveloped\nregions, while deploying open-source MLLMs requires substantial\ncomputational resources. Moreover, proprietary models currently\noffer greater accessibility and user-friendliness through their APIs.\nConclusion\nIn conclusion, we developed MOSAIC, an MLLM-based AI\nagent system for detecting and grading common OSDs using\nsmartphone images. Leveraging MLLMs, prompt engineering, and\nfew-shot prompt learning, MOSAIC demonstrated remarkable\nperformanceinimagequalitycontrol,diseasedetection,andseverity\nanalysis. This system shows potential for improving early detection\nand management of OSDs in non-clinical settings, particularly in\nresource-limited areas.\nData availability statement\nThe raw data supporting the conclusions of this article\nwill be made available by the authors, without undue\nreservation.\nEthics statement\nThe studies involving humans were approved by\nInstitution Review Board of NEH (identifier, 2020-qtky-\n017). The studies were conducted in accordance with the\nlocal legislation and institutional requirements. Written\ninformed consent for participation was not required from the\nparticipants or the participants’ legal guardians/next of kin\nin accordance with the national legislation and institutional\nrequirements.\nAuthor contributions\nZL: Writing – original draft, Writing – review and editing,\nConceptualization, Data curation, Formal Analysis, Funding\nacquisition, Investigation, Methodology, Project administration,\nResources, Software, Supervision, Validation, Visualization.\nZW: Writing – original draft, Writing – review and editing,\nConceptualization, Data curation, Formal Analysis, Funding\nacquisition, Investigation, Methodology, Project administration,\nResources, Software, Supervision, Validation, Visualization. LX:\nWriting – original draft, Writing – review and editing. PZ:\nConceptualization, Investigation, Software, Writing – review and\nediting. WW: Data curation, Methodology, Supervision, Writing –\nreview and editing. YW: Formal Analysis, Project administration,\nValidation, Data curation, Methodology, Supervision, Writing\n– review and editing. GC: Funding acquisition, Resources,\nVisualization,Writing–reviewandediting.WY:Conceptualization,\nInvestigation, Supervision, Writing – review and editing. WC:\nProject administration, Resources, Visualization, Writing – review\nand editing.\nFrontiers in Cell and Developmental Biology 11 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nFunding\nThe author(s) declare that financial support was received\nfor the research and/or publication of this article. This study\nreceived funding from the National Natural Science Foundation\nof China (grant no. 82201148), the Natural Science Foundation of\nZhejiang Province (grant no. LQ22H120002), the Natural Science\nFoundation of Ningbo (grant no. 2023J390), the Ningbo Top\nMedical and Health Research Program (grant no.2023030716), the\nCentralized Guided Local Science and Technology Development\nFunds Project of China (grant no. ZYYD2024CG16). The\nfunding organization played no role in the study design, data\ncollection and analysis, decision to publish, or preparation of\nthe manuscript.\nConflict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or financial relationships that could be\nconstrued as a potential conflict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their affiliated\norganizations, or those of the publisher, the editors and the\nreviewers.Anyproductthatmaybeevaluatedinthisarticle,orclaim\nthatmaybemadebyitsmanufacturer,isnotguaranteedorendorsed\nby the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/fcell.2025.\n1600202/full#supplementary-material\nReferences\nAzari, A. A., and Barney, N. P. (2013). Conjunctivitis: a systematic\nreview of diagnosis and treatment. JAMA 310 (16), 1721–1729. doi: 10.1001/\njama.2013.280318\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,\net al. (2020). “Language models are few-shot learners,”, arXiv: arXiv:2005.14165.\ndoi:10.48550/arXiv.2005.14165\nBurton, M. J. (2009). Prevention, treatment and rehabilitation. Community\nEye Health 22 (71), 33–35. Available online at: https://pmc.ncbi.nlm.nih.\ngov/articles/PMC2823104/.\nBurton, M. J., Ramke, J., Marques, A. P., Bourne, R. R. A., Congdon, N.,\nJones, I., et al. (2021). The lancet global health commission on global eye\nhealth: vision beyond 2020. Lancet Glob. Health 9 (4), e489–e551. doi: 10.1016/\nS2214-109X(20)30488-5\nCaffery, L. J., Taylor, M., Gole, G., and Smith, A. C. (2019). Models of care\nin tele-ophthalmology: a scoping review. J. T elemed. T elecare 25 (2), 106–122.\ndoi:10.1177/1357633X17742182\nClosing the translation gap (2025). Closing the translation gap: AI applications\nin digital pathology - PubMed. Available online at: https://pubmed.ncbi.nlm.nih.\ngov/33065195/(Accessed June 24, 2024).\nGupta, N., Tandon, R., Gupta, S. K., Sreenivas, V., and Vashist, P. (2013).\nBurden of corneal blindness in India. Indian J. Community Med. 38 (4), 198–206.\ndoi:10.4103/0970-0218.120153\nKaczmarczyk, R., Wilhelm, T. I., Martin, R., and Roos, J. (2024). Evaluating\nmultimodalAIinmedicaldiagnostics. npj Digit. Med.7(1),205–5.doi: 10.1038/s41746-\n024-01208-3\nKang,Y.,andKim,J.(2024).ChatMOF:anartificialintelligencesystemforpredicting\nand generating metal-organic frameworks using large language models.Nat. Commun.\n15 (1), 4705. doi:10.1038/s41467-024-48998-4\nKeay, L., Edwards, K., Dart, J., and Stapleton, F. (2008). Grading contact lens-related\nmicrobial keratitis: relevance to disease burden. Optom. Vis. Sci. 85 (7), 531–537.\ndoi:10.1097/OPX.0b013e31817dba2e\nLi, Z., Jiang, J., Chen, K., Chen, Q., Zheng, Q., Liu, X., et al. (2021b). Preventing\ncorneal blindness caused by keratitis using artificial intelligence.Nat. Commun. 12 (1),\n3738. doi:10.1038/s41467-021-24116-6\nLi, Z., Jiang, J., Chen, K., Zheng, Q., Liu, X., Weng, H., et al. (2021a). Development\nof a deep learning-based image quality control system to detect and filter out ineligible\nslit-lamp images: a multicenter study.Comput. Methods Programs Biomed.203, 106048.\ndoi:10.1016/j.cmpb.2021.106048\nLi, Z., Wang, Y., Chen, K., Qiang, W., Zong, X., Ding, K., et al. (2024). Promoting\nsmartphone-based keratitis screening using meta-learning: a multicenter study. J.\nBiomed. Inf. 157, 104722. doi:10.1016/j.jbi.2024.104722\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023).\nPre-train, prompt, and predict: a systematic survey of prompting methods in\nnatural language processing. ACM Comput. Surv. 55 (9), 195–235. doi: 10.1145/\n3560815\nLiu,Y.,Xu,C.,Wang,S.,Chen,Y.,Lin,X.,Guo,S.,etal.(2024).Accuratedetectionand\ngradingofpterygiumthroughsmartphonebyafusiontrainingmodel. Br. J. Ophthalmol.\n108 (3), 336–342. doi:10.1136/bjo-2022-322552\nLingxuan, Z., Mou, W., Lai, Y., Chen, J., Lin, S., Xu, L., et al. (2024). Step\ninto the era of large multimodal models: a pilot study on ChatGPT-4V(ision)’s\nability to interpret radiological images. Int. J. Surg. Lond. Engl. 110 (7), 4096–4102.\ndoi:10.1097/JS9.0000000000001359\nMaheshwari, S. (2007). Pterygium-induced corneal refractive changes. Indian J.\nOphthalmol. 55 (5), 383–386. doi:10.4103/0301-4738.33829\nResnikoff, S., Felch, W., Gauthier, T.-M., and Spivey, B. (2012). The number of\nophthalmologists in practice and training worldwide: a growing gap despite more than\n200,000 practitioners. Br. J. Ophthalmol. 96 (6), 783–787. doi:10.1136/bjophthalmol-\n2011-301378\nRezvan, F., Khabazkhoob, M., Hooshmand, E., Yekta, A., Saatchi, M., and\nHashemi, H. (2018). Prevalence and risk factors of pterygium: a systematic review\nand meta-analysis. Surv. Ophthalmol. 63 (5), 719–735. doi: 10.1016/j.survophthal.\n2018.03.001\nSaaddine, J. B., Narayan, K. M. V., and Vinicor, F. (2003). Vision loss: a public health\nproblem? Ophthalmology 110 (2), 253–254. doi:10.1016/s0161-6420(02)01839-0\nShojaee-Mend, H., Mohebbati, R., Amiri, M., and Atarodi, A. (2024). Evaluating\nthe strengths and weaknesses of large language models in answering neurophysiology\nquestions. Sci. Rep. 14 (1), 10785. doi:10.1038/s41598-024-60405-y\nStapleton, F. (2023). The epidemiology of infectious keratitis.Ocul. Surf. 28, 351–363.\ndoi:10.1016/j.jtos.2021.08.007\nStapleton, F., Edwards, K., Keay, L., Naduvilath, T., Dart, J. K. G., Brian,\nG., et al. (2012). Risk factors for moderate and severe microbial keratitis in\ndaily wear contact lens users. Ophthalmology 119 (8), 1516–1521. doi: 10.1016/\nj.ophtha.2012.01.052\nŠuster, S., Baldwin, T., and Verspoor, K. (2024). Zero- and few-shot prompting of\ngenerative large language models provides weak assessment of risk of bias in clinical\ntrials. Res. Synth. Methods 15 (6), 988–1000. doi:10.1002/jrsm.1749\nTajbakhsh, N., Shin, J. Y., Gurudu, S. R., Hurst, R. T., Kendall, C. B., Gotway,\nM. B., et al. (2016). Convolutional neural networks for medical image analysis:\nfull training or fine tuning? IEEE Trans. Med. Imaging 35 (5), 1299–1312.\ndoi:10.1109/TMI.2016.2535302\nTan, T. F., Thirunavukarasu, A. J., Jin, L., Lim, J., Poh, S., Teo, Z. L., et al.\n(2023). Artificial intelligence and digital health in global eye health: opportunities\nFrontiers in Cell and Developmental Biology 12 frontiersin.org\nLi et al. 10.3389/fcell.2025.1600202\nand challenges. Lancet Glob. Health 11 (9), e1432–e1443. doi: 10.1016/S2214-\n109X(23)00323-6\nToufiq, M., Rinchai, D., Bettacchioli, E., Kabeer, B. S. A., Khan, T., Subba,\nB., et al. (2023). Harnessing large language models (LLMs) for candidate\ngene prioritization and selection. J. Transl. Med. 21 (1), 728. doi: 10.1186/\ns12967-023-04576-8\nWang, L., Chen, X., Deng, X., Wen, H., You, M., Liu, W., et al. (2024).\nPrompt engineering in consistency and reliability with the evidence-\nbased guideline for LLMs. NPJ Digit. Med. 7 (1), 41. doi: 10.1038/\ns41746-024-01029-4\nWhite, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., et al. (2023).\n“A prompt pattern catalog to enhance prompt engineering with ChatGPT,”, arXiv:\narXiv:2302.11382. doi:10.48550/arXiv.2302.11382\nZhang, Z., Wang, Y., Zhang, H., Samusak, A., Rao, H., Xiao, C., et al. (2023). Artificial\nintelligence-assisted diagnosis of ocular surface diseases. Front. Cell. Dev. Biol. 11,\n1133680. doi:10.3389/fcell.2023.1133680\nZhongwen, L., He, X., Zhouqian, W., Daoyuan, L., Kuan, C., Xihang, Z., et al.\n(2025). Deep learning for multi-type infectious keratitis diagnosis: A nationwide,\ncross-sectional, multicenter study. npj Digit. Med. 7, 181–206. doi: 10.1038/\ns41746-024-01174-w\nFrontiers in Cell and Developmental Biology 13 frontiersin.org",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.632710874080658
    },
    {
      "name": "Computer science",
      "score": 0.6209011077880859
    },
    {
      "name": "Interpretability",
      "score": 0.5258769989013672
    },
    {
      "name": "Grading (engineering)",
      "score": 0.5207560062408447
    },
    {
      "name": "Machine learning",
      "score": 0.45039644837379456
    },
    {
      "name": "Medicine",
      "score": 0.3396030068397522
    },
    {
      "name": "Engineering",
      "score": 0.15149733424186707
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    }
  ]
}