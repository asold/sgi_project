{
    "title": "The Grammar-Learning Trajectories of Neural Language Models",
    "url": "https://openalex.org/W3199373335",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2797659969",
            "name": "Leshem Choshen",
            "affiliations": [
                "Hebrew University of Jerusalem"
            ]
        },
        {
            "id": "https://openalex.org/A2906794924",
            "name": "Guy Hacohen",
            "affiliations": [
                "Hebrew University of Jerusalem"
            ]
        },
        {
            "id": "https://openalex.org/A309515214",
            "name": "Daphna Weinshall",
            "affiliations": [
                "Hebrew University of Jerusalem"
            ]
        },
        {
            "id": "https://openalex.org/A343093649",
            "name": "Omri Abend",
            "affiliations": [
                "Hebrew University of Jerusalem"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2947599143",
        "https://openalex.org/W2951013084",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W3171002765",
        "https://openalex.org/W2132984949",
        "https://openalex.org/W2019055305",
        "https://openalex.org/W1584993141",
        "https://openalex.org/W3103649165",
        "https://openalex.org/W2144862731",
        "https://openalex.org/W3102226577",
        "https://openalex.org/W3116815090",
        "https://openalex.org/W2419539795",
        "https://openalex.org/W3186655327",
        "https://openalex.org/W2789115819",
        "https://openalex.org/W1599880985",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2971016963",
        "https://openalex.org/W1978924416",
        "https://openalex.org/W2128073546",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2073257493",
        "https://openalex.org/W2086426947",
        "https://openalex.org/W2989156240",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W1979773093",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2063598038",
        "https://openalex.org/W4205537036",
        "https://openalex.org/W2435103813",
        "https://openalex.org/W3161557941",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2293778248",
        "https://openalex.org/W2884554299",
        "https://openalex.org/W2951528897",
        "https://openalex.org/W2168488947",
        "https://openalex.org/W3162220931",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2791751435",
        "https://openalex.org/W2571532437",
        "https://openalex.org/W2952328691",
        "https://openalex.org/W2140661818",
        "https://openalex.org/W4221138759",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963503967",
        "https://openalex.org/W4385567023",
        "https://openalex.org/W2082137931",
        "https://openalex.org/W4232524937",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2949433733",
        "https://openalex.org/W3098458554",
        "https://openalex.org/W4288284086",
        "https://openalex.org/W3099624838",
        "https://openalex.org/W3105522431",
        "https://openalex.org/W2725114719",
        "https://openalex.org/W2118557108",
        "https://openalex.org/W2985337504",
        "https://openalex.org/W2911435132",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W2970279348",
        "https://openalex.org/W4288347751",
        "https://openalex.org/W3099095494",
        "https://openalex.org/W2951954464",
        "https://openalex.org/W2798944827",
        "https://openalex.org/W3102094970",
        "https://openalex.org/W2953369973",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W3191727383",
        "https://openalex.org/W2356084920",
        "https://openalex.org/W2960374072",
        "https://openalex.org/W4394654497",
        "https://openalex.org/W2277764111"
    ],
    "abstract": "The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models' learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8281 - 8297\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nThe Grammar-Learning Trajectories of Neural Language Models\nLeshem Choshen†, Guy Hacohen†‡, Daphna Weinshall†, Omri Abend†\nDepartment of Computer Science†\nDepartment of Brain Sciences‡\nHebrew University of Jerusalem\n{first.last}@mail.huji.ac.il\nAbstract\nThe learning trajectories of linguistic phe-\nnomena in humans provide insight into lin-\nguistic representation, beyond what can be\ngleaned from inspecting the behavior of an\nadult speaker. To apply a similar approach to\nanalyze neural language models (NLM), it is\nﬁrst necessary to establish that different mod-\nels are similar enough in the generalizations\nthey make. In this paper, we show that NLMs\nwith different initialization, architecture, and\ntraining data acquire linguistic phenomena in\na similar order, despite their different end per-\nformance. These ﬁndings suggest that there\nis some mutual inductive bias that underlies\nthese models’ learning of linguistic phenom-\nena. Taking inspiration from psycholinguis-\ntics, we argue that studying this inductive bias\nis an opportunity to study the linguistic repre-\nsentation implicit in NLMs.\nLeveraging these ﬁndings, we compare the rel-\native performance on different phenomena at\nvarying learning stages with simpler reference\nmodels. Results suggest that NLMs exhibit\nconsistent “developmental” stages. Moreover,\nwe ﬁnd the learning trajectory to be approxi-\nmately one-dimensional: given an NLM with\na certain overall performance, it is possible to\npredict what linguistic generalizations it has al-\nready acquired. Initial analysis of these stages\npresents phenomena clusters (notably morpho-\nlogical ones), whose performance progresses\nin unison, suggesting a potential link between\nthe generalizations behind them.\n1 Introduction\nChildren present remarkable consistency in their\npatterns of language acquisition. They often ac-\nquire linguistic phenomena in a similar order (Kuhl\net al., 1992; Ingram, 1989), and make similar gen-\neralizations and over-generalizations (Kuczaj II,\n1977; Pinker, 1995). This consistency provides an\nimportant starting point for linguistic study. For\nexample, arguments in favor of single or dual sys-\ntem accounts of morphological representation are\noften backed by computational models of children\nlearning trajectories (e.g., Rumelhart and McClel-\nland, 1986; Pinker and Prince, 1988; Kirov and\nCotterell, 2018). In this paper, we embrace this\nprogram for the study of computational language\nmodels, investigating learning trajectories. 1\nThe representations that language models (LM)\nacquire have been studied extensively, including\nstudying their learning dynamics to improve train-\ning (see §6). However, very little work aimed at\ndrawing connections between the training dynam-\nics and the learned representations. In this work\nwe adopt a behavioral approach, thus revealing that\nNLMs share learning trajectories and generalize\nin similar ways during training. This implies that\nstudying trajectories of NLMs is worthwhile, in the\nsense that results on one architecture or size are\nexpected to be reproducible by others.\nThese ﬁndings call for a characterization of these\ntrajectories, a new and promising territory for re-\nsearch. We take ﬁrst steps to explore these direc-\ntions, emphasizing their potential beneﬁt to a better\nfuture understanding of what models learn.\nSpeciﬁcally, we train NLMs on next-word pre-\ndiction, but evaluate and compare them by tracking\ntheir performance on grammar learning in English,\nusing the BLIMP dataset (See 2.1). BLIMP is a\ndataset that consists of 67K minimal pairs, where\neach pair includes a grammatically correct and\na grammatically erroneous sentence. NLMs are\ntested for their ability to assign higher probability\nto the correct one. See example in Table 1, and\ndetails of our experimental methodology in §2.\nWe begin (§3) by establishing that NLMs learn\ngrammatical phenomena in a consistent order. We\nevaluate NLMs at different time points along their\ntraining, showing that the performance on linguis-\n1Code is supplied inhttps://github.com/borgr/\nordert\n8281\nChallenge Correct Erroneous\nAnimate subject Galileo had talked\nto Bell.\nThis car had\ntalked to Bell.\nDrop argument The groups buy. The groups dislike.\nTable 1: BLIMP minimal pairs examples.\ntic phenomena across initializations is highly cor-\nrelated. We further ﬁnd many similarities in the set\nof examples that they correctly classify.\nStill, models of different architectures learn at a\ndifferent pace, and hence cannot be directly com-\npared at identical time points. In §3.3, we over-\ncome this by re-scaling the timeline. We then\nshow that despite architectural differences, NLMs\npresent highly correlated performance trajectories.\nIn §3.4, we further demonstrate that even the choice\nof training data has minor inﬂuence on the results.\nFinally, in §3.5 we show that the learning dynam-\nics essentially follows a single dimension. Namely,\nwhere the average performance is similar, success\non linguistic phenomena is also similar.\nWe proceed by analyzing the early stages of\nlearning in §4. We ﬁnd that, at ﬁrst, NLMs rely\nmostly on local cues and not on word order. They\nthus resemble bag-of-words models over a window\nof the preceding tokens. Later stages seem to drift\nfurther away from bag-of-words models toward\nn-gram models, and with time seem to be more\nsensitive to structural cues. We also ﬁnd evidence\nthat some latent features that the model learns may\nnot be related to linguistic phenomena.\nFinally, in §5 we take the ﬁrst steps in catego-\nrizing linguistic phenomena by their learning tra-\njectories. We identify links between their repre-\nsentations by ﬁnding phenomena that progress in\nunison. For example, we ﬁnd that morphological\nphenomena are mostly learned at similar stages.\nOf particular interest are cases where performance\ndecreases with time, which may suggest either over-\ngeneralization or biases in the BLIMP challenges.\n2 Experimental Setup\n2.1 The BLIMP Dataset\nWe use BLIMP (Warstadt et al., 2019) to assess\nthe extent to which generalizations are made by\nthe NLMs. BLIMP includes 67 grammatical chal-\nlenges categorized into 13 super-phenomena (e.g.,\nisland-related or quantiﬁers) comprising of 4 broad\nﬁelds (e.g., Syntax, Semantics). Each challenge\nconsists of 1K minimal pairs of sentences. A mini-\nmal pair contains a sentence and a near-duplicate\ndistractor that incorporates an error on a particular\nlinguistic phenomenon, i.e., only the phenomenon\nin question is changed between the sentences in a\npair (see Table 1). Each challenge includes pairs\nwith the same linguistic phenomenon.\n2.2 Training\nLM details: as training multiple GPT2 instances\n(Radford et al., 2019) is computationally demand-\ning, we train smaller NLMs. Following Turc et al.\n(2019), we trained 1 instance of GPT2small (width\n768, 12 layers, 8 attention heads) and 4 instances of\nGPT2tiny (width 512, 4 layers, 4 attention heads),\nwith different random seeds.\nSimilarly, we train a small TransformerXL (Dai\net al., 2019), XLsmall (width 512, 4 layers, 8 at-\ntention heads) and a full-sized one (width 4096,\n18 layers, 16 attention heads). We stop the full\nmodel after 600K steps, while the perplexity re-\nmained high. We use it for comparison to the\nearly stages of learning of TransformerXL. All\nmodels’ hyperparameters can be found in App. §B.\nWe also use the results of the fully trained GPT2,\nTransformerXL, LSTM and human performance\nreported in Warstadt et al. (2019).\nIn §4, we compare NLMs with simpler models.\nTo this end, we create two GPT2 tiny variations,\ndenoted BOW and Window-5. BOW replicates\nGPT2tiny, but relies only on bag of words. This is\nachieved by removing the positional weights, and\nreplacing the attention weights with a simple av-\nerage.2 Window-5 similarly ignores the positions,\nand additionally only attends to the last 5 words.\nNote that both are unidirectional LMs and consider\nonly previously predicted words at each step.\nUnless explicitly stated otherwise (as in §3.4),\nall models were trained on the WikiBooks dataset\n(Zhu et al., 2015), which contains the English\nWikipedia (2.1Bwords) and BookCorpus (854M\nwords). This dataset resembles BERT’s train-\ning data (Devlin et al., 2019), except that current\nWikipedia is used. Additionally, we trained models\non the following datasets: English openSubtitles\n(Lison and Tiedemann, 2016), newsCrawl (Barrault\net al., 2019), GigaWord (Napoles et al., 2012), and\n2Supposedly, removing the positional embeddings would\nsufﬁce. Empirically, it has little effect. Presumably, as embed-\ndings only attend to previous positions, the network manages\nto represent positions by the difference between them. This\nis in line with the ﬁnding that GPT2’s positional embeddings\nare not meaning-bearing (Wang and Chen, 2020).\n8282\na sample of openWebText (3B words; Gokaslan\nand Cohen, 2019) – a replication of GPT2 dataset.\nThroughout this paper, we report Pearson corre-\nlation. Using Spearman correlation leads to qualita-\ntively similar conclusions. When multiple models\nare correlated against each other, their average pair-\nwise correlation is reported.\n3 The Learning Order of NLMs\nIn this section, we examine various aspects of\nNLMs, generally showing that their learning trajec-\ntories are similar.\nWe evaluate network similarity by adopting a\nbehavioral approach. Accordingly, networks are\nviewed as functions, whoselatent features manifest\nthemselves only by their inﬂuence on the network’s\nbehavior. Latent features are the unobserved causes\nof the measured behavior. Consequently, parame-\nters, activation patterns and representations can be\ncompletely different among similar models. This is\nunlike the approaches employed by Williams et al.\n(2018); Saphra and Lopez (2019); Liu et al. (2021),\nwhich analyze internal representations directly.\nTo formalize the above notion, let Lt denote a\ncheckpoint, the language model Lat time t. Let\npv(Lt) denote its performance vector – the accu-\nracy obtained by Lon each BLIMP challenge p:\npv(Lt) = [acc(Lt,p)]p∈BLIMP ∈R67 (1)\nTime t is measured in training steps or perplex-\nity. The trajectory of the performance vector as a\nfunction of treﬂects L’s training dynamics.\nGiven this behavioral deﬁnition, we focus on\ncomparing the relative strength of models. Similar-\nity between models is thus measured as the corre-\nlation between their performance vectors. Hence,\nmodels are similar if they rank phenomena in the\nsame way. On the other hand, models of the same\naverage performance can be dissimilar: consider\ntwo models that agree on everything except nouns.\nOne generates only feminine nouns and the other\nplural nouns. The models’ average performance\nis similar, but due to their biases, they are correct\non different challenges. This dissimilarity suggests\nthat the models rely on different latent features.\n3.1 Consistent Order of Learning\nWe begin by showing that models produced by\ndifferent initializations learn the same phenomena,\nin the same order. In terms of our deﬁnitions above,\nthis may imply that despite converging to different\nparameter values, the learned latent features and\nthe generalization patterns made are similar.\nFigure 1: High correlation after warmup (5K steps).\nCorrelation between the performance vectors (mea-\nsured by steps) of GPT2 tiny models with different ini-\ntialization (blue) or training data (orange).\nIn order to examine the hypothesis empirically,\nwe compute the correlation between 4 random ini-\ntializations (Fig. 1). Results conﬁrm the hypothesis,\nthe correlation between GPT2tiny instances is ex-\ntremely high. It is already high after 10K steps, and\nremains high throughout training. We note that the\ncorrelation at step 0 is 0 (not shown), and that after\n10K warm-up steps the network’s ability as a LM\nis still poor. For example, perplexity is 10.9 after\n10K steps and 6.7 after 70K steps.\n3.2 Effects of Architecture\nFigure 2: Similar Accuracy despite different initial-\nizations and sizes of the GPT2 small models. Trans-\nformerXL perplexity is not computed on the same vo-\ncabulary, but still shows a (rescaled) similar trend. The\ngraph depicts trajectories on an example phenomenon\n(“existential there”). y-axis is the accuracy during train-\ning and x-axis is the model’s perplexity.\nNext, we show that different architectures also\npresent similar trajectories. As the learning pace is\nnot comparable across models, computing correla-\n8283\ntion in ﬁxed and identical intervals is not informa-\ntive. Instead, we choose tto be the perplexity on\nthe development set, comparing models at the same\nperformance level. TransformerXL is not directly\ncomparable as perplexity requires the vocabulary\nto be the same.\nFollowing this paradigm, we see that GPT2small\nand GPT2tiny are highly correlated (>0.9), present-\ning similar learning order throughout training. Ob-\nserving the trajectories per challenge qualitatively,\nwe see that they align very well (cf. Fig. 2 and App.\n§A, §C). TransformerXL also seems to share the\ngeneral tendencies of the GPT2 architectures.\nInterestingly, we see that models behave simi-\nlarly not only in terms of relative performance, but\nalso at the example level (binary decision per min-\nimal pair). We ﬁnd that GPT2small and GPT2tiny\nhave an average agreement of κ = 0.83 (Fleiss\net al., 1969). This implies strong consistency in\nthe order of learning of different examples also\nwithin phenomena. Henceforth, we focus on the\nphenomena-level as it is more interpretable, lend-\ning itself more easily to characterization. We dis-\ncuss per-example similarity further in App. §D.\n3.3 Comparison to Off-the-shelf Models\nSo far, we have observed the common trajectories\npresented by NLMs that are trained in parallel. We\nproceed to compare trajectories of one model to\nother models’ performance vectors at a single point\nof interest in their learning, i.e. a checkpoint’s per-\nformance vector. This allows us to analyze how\nsimilarities evolve, rather than whether two trajec-\ntories are synced. We compare fully trained off-the-\nshelf NLMs with the trajectory of GPT2tiny (Fig.\n3a) and GPT2small (App. §E).\nThe observed similarity to off-the-shelf models\nis high (0.6-0.8), implying that NLMs in general\nshare tendencies and biases. Moreover, similarity\nincreases until the point of same performance and\nthen (when relevant) decreases. This suggests that\nthe small NLM approaches off-the-shelf tendencies\nas it improves and stops somewhere on the same\ntrajectory of generalizations (cf. §3.5). Further-\nmore, we ﬁnd considerable correlation with the\nperformance levels of humans on the different chal-\nlenges, but still, all NLMs correlate better with our\nmodel than humans correlate with it.\nThese results present a curious order imposed on\nthe NLMs. Both GPT2 tiny and GPT2small (App.\n§E) are more similar to the LSTM model than to\nTransformerXL, and even less similar to GPT2large.\nInterestingly, our models are more similar to an\nRNN and a model with a different architecture,\nthan to a larger model with the same architecture.\nThus, it seems that the architecture type cannot\nexplain the similarities in the relative order. We\nfurther examine this issue in the next section.\n3.4 Effect of Training Data\nThis section examines the possibility that the simi-\nlarities reported in Fig. 3a can simply be explained\nby the similarity in the NLM’s training data. More\nspeciﬁcally, since the ranking by model similar-\nity reported above ﬁts the similarity between the\ntraining sets that the models were trained on, we\nview it as a potential confound and attempt to con-\ntrol for it. Our training data (WikiBooks) consists\nmostly of Wikipedia and so do the LSTM’s and\nTransformerXL’s training sets, which are trained\non earlier versions of Wikipedia and WikiMatrix\n(Schwenk et al., 2019) respectively. GPT2, on the\nother hand, is trained on openWebText, which con-\nsists of scraped web pages.\nTo tease apart the effect of training data, we\ntrained 3 additional GPT2 tiny instances over\nthe openWebText, openSubtitles and newsCrawl\ndatasets. Results (Fig. 1) show that the dataset has\nmore effect on the correlation than initialization.\nHence, the choice of training data does affect the\nlearning trajectory, but its effect decreases with\ntraining (correlation gets higher with more training\nsteps). We also recompute the correlations from\n§3.3 after training GPT2tiny on the same data as\nGPT2large (App. §F), and ﬁnd that the relative\norder between the NLMs remains the same, with\nGPT2large being the least similar.\nWe conclude that while the training data affects\nthe learned generalizations, it only very partially\nexplains the observed similarities between NLMs.\n3.5 One Dimension of Learning\nBased on the ﬁndings of the previous sub-sections,\nwe hypothesize that current NLMs all learn in a\nsimilar order, where the effect of training data and\narchitecture is secondary. In other words, training\ntime, size and efﬁciency may affect what a model\nhas learned, but not its learning order. This implies\nthat stronger models may improve performance,\nbut still follow a similar learning trajectory. If this\nhypothesis is correct, models should be most simi-\nlar to models with the same performance; similarity\nshould drop as the gap in performance widens.\n8284\n(a) Off-the-shelf and human\n (b) GPT2small checkpoint after X steps\nFigure 3: Reference models correlate the most with GPT2tiny when they have the most similar performance(or\nnear it). Correlation during GPT2 tiny training compared to off-the-shelf LMs and human performance (left) or to\nmid-training GPT2small checkpoints (right). Curves correspond to ﬁxed performance vectors. Where the X-axis\nfollows the training trajectory of gpt, each line represents similarity to a different checkpoint, either of different\nfully trained models (left) or to checkpoint during the training of a larger model (right). Numbers are the average\nperformance of the checkpoint, and are placed over the step where this average performance is the most similar to\nthat of GPT2tiny. The best score of GPT2tiny is 67.\nControlled comparison supports this hypothe-\nsis. Fig. 3b presents the correlation of GPT2 tiny\ntraining trajectory with several static checkpoints\ntaken during GPT2small training. We observe that\nat the point in which the average performance\nof GPT2tiny is closest to that of the checkpoint,\nthe correlation peaks, and then decreases again\nas GPT2tiny surpasses the checkpoint in average\nperformance. So overall correlation peaks when\naverage performance is most similar. Note that de-\nspite the different network sizes and convergence\nrates, the correlation’s maximal value is very high\n(higher than 0.9).\nFurther experiments show similar trends. Fig. 3a\npresents a similar investigation, albeit with more\nvaried architectures and training datasets. Here too\nthe maximum correlation is obtained around the\npoint of most similar performance.\n3.6 Comparison to 5-gram\nNLMs are most similar to other NLMs with the\nsame performance. However, when compared to\nnon-neural LMs, this is no longer the case.\nMore speciﬁcally, we compare GPT2 tiny to\ntwo 5-gram LMs trained on the same dataset as\nthe NLMs (WikiBooks) and another (GigaWord)\ndataset. Results are shown in Fig. 4, which is qual-\nitatively different from Fig. 3a. Here, similarity in\nperformance implies neither high correlation, nor\nthe point of highest similarity. This serves both as a\nsanity check to our methodology, and as a reminder\nof model biases: In general, models may have dif-\nferent biases and tendencies, regardless of overall\nperformance. In our case, it seems that NLMs share\nbiases between them that are not necessarily shared\nwith other LMs.\nWhile not the main purpose of the analysis, our\ncomparison reveals other noteworthy trends. For\nexample, 5-gram LMs trained on different corpora\nhave different correlations to the GPT2tiny trajec-\ntory. This is further discussed in App. §G.\nFigure 4: Correlation during training of GPT2tiny com-\npared to a 5-gram model trained on the same data\n(WikiBooks) and on GigaWord. On each curve, we\nmark the point at which the accuracy is most similar to\nGPT2tiny, and additionally indicate the corresponding\noverall average accuracy of the reference models.\n3.7 Discussion\nWe ﬁnd that the order of learning is surprisingly\nstable across architectures, model sizes and train-\ning sets. Therefore, given a new NLM, the order\n8285\nin which it will learn linguistic phenomena can be\npredicted by another model that achieves a similar\naverage accuracy. When considering non-neural\nLMs, this observation does not always hold: in-\nherently different architectures (such as 5-grams)\nhave very different trajectories. Hence, future mod-\nels with very different induced biases may present\ndifferent orders.\n4 Phases of Learning\nHaving established that different NLMs learn in\na consistent order, we investigate the emerging\nlearning trajectory by comparing it with simpler\nreference models. Our goal is to identify distinct\nlearning phases that characterize NLM’s training.\nSetup. We compare GPT2 tiny to fully trained\nLMs (same as §3.3), as well as to a variety of met-\nrics. For each metric mwe compute the average\nscore over each example for each of the 67 sets\nEpi∈p [m(pi)] ∈R67. The results are replicated\nwith GPT2small and TransformerXL and lead to\nsimilar conclusions (see App. §E).\nSentence-level Metrics. First, we consider two\nsentence-level metrics: sentence length (in tokens)\nand syntactic depth. Assuming a sentence parse\ntree, the depth is the longest path from a word to\nthe root. Sentence length is often considered to\nbe a source of challenge for infants (Brown, 1973)\nand networks (Neishi and Yoshinaga, 2019), re-\ngardless of the sentence’s complexity. Syntactic\ndepth (Yngve, 1960) is a measure used to assess\nhow cognitively complex a sentence is. We leave\nthe question of which measure of linguistic com-\nplexity (Szmrecsányi, 2004) correlates best with\nthe trajectory exhibited by NLMs to future work.\nFigure 5: Correlation between the performance vectors\nof different metrics and models against the vector of\nGPT2tiny at different stages of learning.\nOur results (Fig. 5) show that neither sentence-\nlevel metric (length and syntactic depth) can predict\nwell what is difﬁcult for the model. This is not\nsurprising, as both measures only capture sentence\ncomplexity at a general level, and are not directly\nrelated to the linguistic phenomenon that is being\ntested. We do see that the syntactic depth starts\noff as a worse predictor of the NLM performance\nand ends as a better one. We provide a different\nperspective on this initial learning phase, before\nand after that switch, later in this section.\nNext, we compare the performance vector with\ntask difﬁculty for humans, as reported in the orig-\ninal BLIMP paper. We observe that correlation is\nfairly high after a sufﬁcient number of steps. In\nfact, the network becomes more similar to humans\nas it improves: at the beginning, the network relies\non different features than humans, but with time\nmore of the hurdles are shared. However, correla-\ntion saturates at a mid-range correlation of under\n0.5. This suggests that the network (partially) relies\non features that are not used by human annotators.\nThese may be valid generalizations not tested by\nBLIMP, or erroneous ones that are still beneﬁcial\nto reduce the score on the task it was trained on (cf.\nMcCoy et al., 2019). We revisit this issue in §5.\nComparison with Limited Context and Local-\nity. Our methodology opens the door to examine\nother potential biases of LMs. We now do so, start-\ning with context and locality.\nWe consider models that take into account dif-\nferent scopes of context: unigram, and 2-5 gram\nLMs that can exploit the order of preceding words.\nWe argue that the correlation between NLMs and\nn-gram LMs may indicate that features based on\nlimited context are also employed by NLMs.\nSurprisingly, the unigram model, which doesn’t\nuse context, perfectly classiﬁes 7 phenomena,\nachieves 98.1% accuracy on 1, and completely fails\n(0% accuracy) on 8. This suggests that high accu-\nracy on some syntactic and semantic challenges\n(as deﬁned by BLIMP) can be achieved by simple\nheuristics. Note, however, that the NLMs we test\nare not trained towards any speciﬁc phenomena\nand are not ﬁne-tuned in any way. Hence, NLMs\ncan only attain heuristics or biases (generalization\nerrors) which are beneﬁcial in general, not ones\nspeciﬁc to our test challenges.\nWhile NLMs initially present a strong corre-\nlation with the unigram model, this correlation\nquickly drops (see Fig. 5). From the outset,\nGPT2tiny succeeds on 6 of the 8 phenomena that\nare classiﬁed well by unigrams, and 4 of the 8 that\n8286\nthe unigram model utterly fails on. Interestingly,\nfor 3 of the other phenomena on which the unigram\nfailed, GPT2 tiny initially achieves 0% accuracy\n(chance level is 50%), but its accuracy does climb\nduring training (e.g., see App. §A). We conclude\nthat, as expected, the NLM acquires a bias towards\npredicting frequent words early in training, but that\nthis bias is weighed in against other (contextual)\nconsiderations later on in training.\nFigure 6: Correlation between the performance vec-\ntors of GPT2tiny throughout learning with simple LMs.\nThe ﬁgure focuses on LMs found also on Fig. 5.\nComparing different scopes of context, our re-\nsults (Fig. 6 and App. §E) show that through-\nout training, the network presents high correlation\nwith n-gram models. From a certain point onward,\nthe network becomes more similar to the bi-gram\nmodel than to the other n-gram LMs. We also\nnote that similarity peaks early on, but with time\nthe correlation decreases. This may suggest that\ninitially, the NLMs acquire grammatical behavior\nthat resembles a Markov model, or even a bi-gram\nmodel. Only later does the network rely more on\nglobal features. This is in line with our earlier ﬁnd-\nings, which show an increasing correlation with\nsyntactic depth as compared to sentence length.\nAt the very beginning, NLMs often generate one\nword repetitions (e.g., \"the\" Fu et al., 2020). This\nseems to be at odds with our ﬁnding that grammar\nlearning already begins at this early stage. How-\never, while frequency may dictate the most proba-\nble predictions, comparing two options that differ\nonly slightly may prove to depend more on context,\nas our results indicate.\nLimited Context and Word Order.By compar-\ning NLMs to n-grams, we examined the effect of\ncontext within a ﬁxed window size. Now we ex-\namine the effect of word order, within a window\nand in general. To this end, we create two ablated\nGPT2tiny models. BOW is agnostic to the order be-\ntween preceding tokens, while Window-5 is similar\nbut relies only on 5 tokens (details in §2).\nOur results suggest that initially, the identity of\nthe preceding words is more important than their\norder. Both BOW and Window-5 better correlate\nwith our NLM than the n-gram models. Later on,\nthis trend reverses and the n-grams, that do ex-\nploit word order, become better correlated. Fur-\nthermore, the correlation with Window-5 is sig-\nniﬁcantly smaller than with BOW at later stages\nof learning, suggesting that the network gradually\nlearns to rely on more context (cf. Saphra and\nLopez, 2019).\n5 Classifying the Learning Trajectories\nTo understand the latent features learned by NLMs,\nwe categorize linguistic phenomena through the\nlens of their learning trajectories. We ask whether\nlinguistically similar phenomena are learned in a\nsimilar fashion, and whether what is learned simi-\nlarly is deﬁned by linguistic terms.\nWe inspect linguistic categories by comparing\nthe learning trajectories of their phenomena. In the\nMorphology ﬁeld, we ﬁnd that they display similar\ngradual curves, ultimately reaching high perfor-\nmance (median accuracy 0.85, see Fig. 7a). This\nmay indicate that some latent features learned are\nmorphological, and affect performance on almost\nall ’Morphology’ phenomena.\nSyntax-semantics phenomena also present\nunique behavior: their scores plateau near chance\nperformance (see Fig 7b), suggesting that the\nlearned features are insufﬁcient to correctly repre-\nsent phenomena in this ﬁeld. The other ﬁelds, \"se-\nmantics\" and \"syntax\" (Figs 7c,7d), do not present\nprototypical learning curves, suggesting that they\nare too broad to correspond to a single learning pat-\ntern. This, in turn, may suggest that they do not all\ncorrespond to a well-deﬁned set of latent features.\nNext, we follow the reverse direction and cluster\nthe learning curves of GPT2tiny. We use spectral\nclustering with 10 clusters and sklearn default pa-\nrameters, by projecting the learning curves into a\nnormalized Laplacian and applying k-means. Intu-\nitively, learning curves with similar values along\nthe principal directions, are clustered together.\nOther clustering methods show similar results.\nThe clusters (Fig. 8 and App. §H) reﬂect several\n8287\n(a) Morphology\n (b) Syntax-Semantics\n (c) Semantics\n (d) Syntax\nFigure 7: Morphology and Syntax-Semantics (left) characterize NLM learning well, while semantics and syntactic\nphenomena show little similarity (between lines). Learning curves of GPT2 tiny per challenge (lines), clustered\naccording to different ﬁelds (graphs) and colored by super-phenomena.\n(a)\n (b)\n (c)\n (d)\nFigure 8: Some phenomena are learned, others (c) deteriorate, implying the network (that learns language mod-\nelling, not phenomena) learns orthogonal features. Learning curves of GPT2 tiny on BLIMP challenges, obtained\nby spectral clustering and colored by ﬁelds.\nlearning proﬁles, some more expected than oth-\ners. For some, accuracy improves as learning pro-\ngresses (see Fig. 8a). Some are barely learned, and\naccuracy remains at near-chance level (see Fig. 8b).\nPerhaps more surprisingly, some clusters deterio-\nrate, and accuracy drops to nearly 0 as learning\nprogresses (see Fig. 8c). Notably, some challenges\nare quite easy – NLMs instantly reach perfect ac-\ncuracy (see Fig. 8d), while some are confusing\n– NLMs performance is worse than chance (see\nFig. 8c). In the latter cases, the NLMs presumably\nlearn unrelated, harmful generalizations.\nWhen inspecting the emerging clusters, many\n(but not all, see Fig. 8b) contain a shared promi-\nnent ﬁeld, but often varied super-phenomena (see\nFig. 8a). Thus, while the categorization in BLIMP\nreﬂects a common linguistic organization of gram-\nmatical phenomena, from the perspective of learn-\ning trajectories only few of the super-phenomena\nin BLIMP show consistent behavior. We cautiously\nconclude that there is some discrepancy between\nthe common linguistic categorization of grammati-\ncal phenomena and the categorization induced by\nthe learning trajectories of NLMs. An interesting\ndirection for future work would therefore be the\ndevelopment of a theory that can account for the\npatterns presented by NLMs’ learning trajectories.\nWe manually inspect a few phenomena with\nstrong initial performance that then deteriorates.\nWe ﬁnd that some of these challenges are solvable\nby a simple rule, easily learnable by an n-gram\nmodel. For example, in \"principle A case 1\", al-\nways preferring subjective pronouns (e.g., \"she\" or\n\"he\") over reﬂexive ones (e.g., \"himself\", \"herself\")\nis sufﬁcient to obtain a perfect score, and preferring\n\"not ever\" over \"probably/fortunately ever\" solves\n\"sentential negation NPI licensor present\". The fact\nthat NLM performance deteriorates, ﬁts our ﬁnding\nthat nascent NLMs resemble an n-gram model.\n6 Related Work\nCharacterizing what networks learn is a long-\nstanding challenge. Recently, studies suggested\nmethods to analyze trained models such as prob-\ning (Tenney et al., 2019; Slobodkin et al., 2021),\nanalyzing attention heads (V oita et al., 2019; Ab-\nnar and Zuidema, 2020) and neurons (ﬁnding also\ncorrelations across epochs; Bau et al., 2018) and\nassessing the extent to which LMs represent syntax\n(van Schijndel et al., 2019). Other works compare\noutputs, like us, to assess network generalizations\n(Choshen and Abend, 2019; Ontan’on et al., 2021),\nlook for systematic biases (Choshen and Abend,\n2018; Stanovsky et al., 2019) or evaluate character-\nistics of outputs (Gehrmann et al., 2021; Choshen\net al., 2020). McCoy et al. (2020) ﬁne-tuned BERT\nand tested generalizations on the adversarial dataset\nHANS (McCoy et al., 2019), ﬁnding models to\nmake inconsistent generalizations. Their results\ndiffer from ours, but so is their setup, which in-\n8288\nvolves ﬁne-tuning for inference.\nCharacterizing the features learned by networks\naccording to the order in which examples and phe-\nnomena are learned is a relatively new topic. Re-\ncently, Hacohen et al. (2020); Hacohen and Wein-\nshall (2021); Pliushch et al. (2021) showed that\nclassiﬁers learn to label examples in the same or-\nder. While their focus was on computer vision, it\nprovided motivation for this work. Other studies\nuse learning dynamics as a tool, rather than a topic\nof study. They choose training examples (Toneva\net al., 2018), categorize examples (Swayamdipta\net al., 2020) or characterize the loss-space (Xing\net al., 2018). Little research on NLM learning dy-\nnamics and generalization types was previously\nconducted.\nPerhaps the closest to this work is Saphra and\nLopez (2019), which compared LSTM represen-\ntations with 3 types of linguistic tagger outputs,\nﬁnding that correlation is low and that later in train-\ning, more context is used. The latter is reminiscent\nof our ﬁndings in §4.\nIn parallel work, Liu et al. (2021) probe models\nduring training. They show that, early in train-\ning, information required for linguistic classiﬁ-\ncations is found somewhere in the layers of the\nmodel. Our work supports their ﬁndings by show-\ning that grammar learning experiments conducted\nwith one model are likely to replicate on another.\nOur methodology differs from theirs in requiring\nthe information the model learnt to manifest itself\nin behavior rather than to be extractable with a\ndedicated classiﬁer.\nStudying the trajectories of language learning\nis a mostly untapped area in NLP, but is a long-\nestablished ﬁeld of research in linguistics and psy-\nchology. Such lines of research study topics such\nas acquisition of phonemes (Kuhl et al., 1992), mor-\nphology (Marcus et al., 1992), complex construc-\ntions (Gropen et al., 1991; Qing-mei, 2007) and\ninnate learning abilities (Tomasello, 2003). Con-\nsiderable computational work was also done on\nconstructing models that present similar learning\ntrajectories to those of infants (McClelland and\nRumelhart, 1981; Perfors et al., 2010; Abend et al.,\n2017, among many others).\nOur work suggests that the generalizations\nNLMs make are coupled with the bottom-line per-\nformance. This gives a new angle and opens av-\nenues of research when combined with previous\nwork about bottom-line performance. For exam-\nple, the bottom-line performance of small models\ncould predict the performance of larger models\n(Ivgi et al., 2022). In such cases, the type of gener-\nalizations made might also be predicted from the\nsmaller models.\nOur work is also closely related to ﬁelds such as\ncurriculum learning (Bengio et al., 2009; Hacohen\nand Weinshall, 2019), self-paced learning (Kumar\net al., 2010; Tullis and Benjamin, 2011), hard data\nmining (Fu and Menzies, 2017), and active learning\n(Krogh and Vedelsby, 1994; Hacohen et al., 2022;\nEin-Dor et al., 2020). In these ﬁelds, the order in\nwhich data should be presented to the learner is\ninvestigated. On the other hand, in our work, we\nstudy the order of the data in which the learner\nis learning – which may shed some light on the\nadvancement of such ﬁelds.\n7 Summary and Conclusions\nWe showed that NLMs learn English grammatical\nphenomena in a consistent order, and subsequently\ninvestigated the emerging trajectory. Our ﬁndings\nsuggest that NLMs present consistent and infor-\nmative trends. This ﬁnding suggests a path for\nstudying NLMs’ acquired behavior through their\nlearning dynamics, as a useful complementary per-\nspective to the study of ﬁnal representations.\nFuture work will consider the impact of addi-\ntional factors, architectures and learning phases\nthat appear only later in training. We hope that this\nwork will increase the afﬁnity between the knowl-\nedge and methodologies employed in developmen-\ntal studies, and those used for studying NLMs. Our\ngoal is to obtain a better understanding of what\nmakes linguistic generalization complex or simple\nto learn, for both humans and NLMs.\nAcknowledgments\nWe thank Prof. Inbal Arnon for her helpful discus-\nsions. This work was supported in part by the Israel\nScience Foundation (grant no. 2424/21), by a grant\nfrom the Israeli Ministry of Science and Technol-\nogy, and by the Gatsby Charitable Foundations.\n8289\nReferences\nOmri Abend, T. Kwiatkowski, N. Smith, S. Goldwater,\nand Mark Steedman. 2017. Bootstrapping language\nacquisition. Cognition, 164:116–143.\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention ﬂow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 4190–4197, On-\nline. Association for Computational Linguistics.\nLoïc Barrault, Ondrej Bojar, Marta R. Costa-jussà,\nC. Federmann, M. Fishel, Y . Graham, B. Haddow,\nMatthias Huck, Philipp Koehn, S. Malmasi, Christof\nMonz, M. Müller, Santanu Pal, Matt Post, and Mar-\ncos Zampieri. 2019. Findings of the 2019 confer-\nence on machine translation (wmt19). In WMT.\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2018. Iden-\ntifying and controlling important neurons in neural\nmachine translation. In International Conference on\nLearning Representations.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th annual international confer-\nence on machine learning, pages 41–48.\nRoger Brown. 1973. A ﬁrst language: The early stages.\nHarvard U. Press.\nLeshem Choshen and Omri Abend. 2018. Inherent bi-\nases in reference-based evaluation for grammatical\nerror correction. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 632–\n642, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nLeshem Choshen and Omri Abend. 2019. Automat-\nically extracting challenge sets for non-local phe-\nnomena in neural machine translation. In Proceed-\nings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL) , pages 291–303,\nHong Kong, China. Association for Computational\nLinguistics.\nLeshem Choshen, Dmitry Nikolaev, Yevgeni Berzak,\nand Omri Abend. 2020. Classifying syntactic errors\nin learner language. In CONLL.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim.\n2020. Active Learning for BERT: An Empirical\nStudy. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 7949–7962, Online. Associa-\ntion for Computational Linguistics.\nJ. Fleiss, J. Cohen, and B. Everitt. 1969. Large sample\nstandard errors of kappa and weighted kappa. Psy-\nchological Bulletin, 72:323–327.\nWei Fu and Tim Menzies. 2017. Easy over hard: A\ncase study on deep learning. In Proceedings of the\n2017 11th joint meeting on foundations of software\nengineering, pages 49–60.\nZihao Fu, Wai Lam, Anthony Man-Cho So, and Bei\nShi. 2020. A theoretical analysis of the repeti-\ntion problem in text generation. arXiv preprint\narXiv:2012.14660.\nSebastian Gehrmann, Tosin P. Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi, Aremu\nAnuoluwapo, Antoine Bosselut, Khyathi Raghavi\nChandu, Miruna Clinciu, Dipanjan Das, Kaustubh D.\nDhole, Wanyu Du, Esin Durmus, Ondrej Dusek,\nChris C. Emezue, Varun Gangal, Cristina Garbacea,\nTatsunori B. Hashimoto, Yufang Hou, Yacine Jer-\nnite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly,\nMihir Kale, Dhruv Kumar, Faisal Ladhak, Aman\nMadaan, Mounica Maddela, Khyati Mahajan, Saad\nMahamood, Bodhisattwa Prasad Majumder, Pe-\ndro Henrique Martins, Angelina McMillan-Major,\nSimon Mille, Emiel van Miltenburg, Moin Nadeem,\nShashi Narayan, Vitaly Nikolaev, Rubungo Andre\nNiyongabo, Salomey Osei, Ankur P. Parikh, Laura\nPerez-Beltrachini, Niranjan Rao, Vikas Raunak,\nJuan Diego Rodríguez, Sashank Santhanam, João\nSedoc, Thibault Sellam, Samira Shaikh, Anasta-\nsia Shimorina, Marco Antonio Sobrevilla Cabezudo,\nHendrik Strobelt, Nishant Subramani, Wei Xu, Diyi\nYang, Akhila Yerukola, and Jiawei Zhou. 2021. The\ngem benchmark: Natural language generation, its\nevaluation and metrics. ArXiv, abs/2102.01672.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki\nMaeda. 2003. English gigaword. Linguistic Data\nConsortium, Philadelphia, 4(1):34.\nJ. Gropen, S. Pinker, M. Hollander, and R. Goldberg.\n1991. Affectedness and direct objects: The role of\nlexical semantics in the acquisition of verb argument\nstructure. Cognition, 41:153–195.\nGuy Hacohen, Leshem Choshen, and D. Weinshall.\n2020. Let’s agree to agree: Neural networks share\nclassiﬁcation order on real datasets. International\nConference of Machine Learning.\n8290\nGuy Hacohen, Avihu Dekel, and Daphna Weinshall.\n2022. Active learning on a budget: Opposite strate-\ngies suit high and low budgets. arXiv preprint\narXiv:2202.02794.\nGuy Hacohen and Daphna Weinshall. 2019. On the\npower of curriculum learning in training deep net-\nworks. In International Conference on Machine\nLearning, pages 2535–2544. PMLR.\nGuy Hacohen and Daphna Weinshall. 2021. Princi-\npal components bias in deep neural networks. arXiv\npreprint arXiv:2105.05553.\nKenneth Heaﬁeld. 2011. Kenlm: Faster and smaller\nlanguage model queries. In WMT@EMNLP.\nDavid Ingram. 1989. First language acquisition:\nMethod, description and explanation . Cambridge\nuniversity press.\nMaor Ivgi, Yair Carmon, and Jonathan Berant. 2022.\nScaling laws under the microscope: Predicting trans-\nformer performance from small scale experiments.\nArXiv, abs/2202.06387.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nChristo Kirov and Ryan Cotterell. 2018. Recurrent neu-\nral networks in linguistic theory: Revisiting pinker\nand prince (1988) and the past tense debate. Trans-\nactions of the Association for Computational Lin-\nguistics, 6:651–665.\nAnders Krogh and Jesper Vedelsby. 1994. Neural net-\nwork ensembles, cross validation, and active learn-\ning. Advances in neural information processing sys-\ntems, 7.\nStan A Kuczaj II. 1977. The acquisition of regular and\nirregular past tense forms. Journal of Verbal Learn-\ning and Verbal Behavior, 16(5):589–600.\nPatricia K Kuhl, Karen A Williams, Francisco Lacerda,\nKenneth N Stevens, and Björn Lindblom. 1992. Lin-\nguistic experience alters phonetic perception in in-\nfants by 6 months of age. Science, 255(5044):606–\n608.\nM Kumar, Benjamin Packer, and Daphne Koller. 2010.\nSelf-paced learning for latent variable models. Ad-\nvances in neural information processing systems, 23.\nP. Lison and J. Tiedemann. 2016. Opensubtitles2016:\nExtracting large parallel corpora from movie and tv\nsubtitles. In LREC.\nL. Liu, Yizhong Wang, Jungo Kasai, Hannaneh Ha-\njishirzi, and Noah A. Smith. 2021. Probing across\ntime: What does roberta know and when? In\nEMNLP.\nG. Marcus, S. Pinker, M. Ullman, M. Hollander,\nT. Rosen, and F. Xu. 1992. Overregularization in\nlanguage acquisition. Monographs of the Society for\nResearch in Child Development, 57 4:1–182.\nJames L McClelland and David E Rumelhart. 1981. An\ninteractive activation model of context effects in let-\nter perception: I. an account of basic ﬁndings. Psy-\nchological review, 88(5):375.\nR. Thomas McCoy, Junghyun Min, and Tal Linzen.\n2020. Berts of a feather do not generalize to-\ngether: Large variability in generalization across\nmodels with similar test set performance. ArXiv,\nabs/1911.02969.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nCourtney Napoles, Matthew R Gormley, and Benjamin\nVan Durme. 2012. Annotated gigaword. In Pro-\nceedings of the Joint Workshop on Automatic Knowl-\nedge Base Construction and Web-scale Knowledge\nExtraction (AKBC-WEKEX), pages 95–100.\nMasato Neishi and Naoki Yoshinaga. 2019. On the\nrelation between position information and sentence\nlength in neural machine translation. In CoNLL.\nSantiago Ontan’on, Joshua Ainslie, Vaclav Cvicek,\nand Zachary Kenneth Fisher. 2021. Making\ntransformers solve compositional tasks. ArXiv,\nabs/2108.04378.\nAmy Perfors, Joshua B. Tenenbaum, and Elizabeth\nWonnacott. 2010. Variability, negative evidence,\nand the acquisition of verb argument constructions.\nJournal of Child Language, 37(3):607–642.\nSteven Pinker. 1995. Language acquisition. Language:\nAn invitation to cognitive science, 1:135–82.\nSteven Pinker and Alan Prince. 1988. On language and\nconnectionism: Analysis of a parallel distributed\nprocessing model of language acquisition. Cogni-\ntion, 28(1):73 – 193.\nIuliia Pliushch, Martin Mundt, Nicolas Lupp, and Vis-\nvanathan Ramesh. 2021. When deep classiﬁers\nagree: Analyzing correlations between learning or-\nder and image statistics. ArXiv, abs/2105.08997.\nRen Qing-mei. 2007. The cognitive and psychological\ninterpretation of construction acquisition: A survey.\nJournal of foreign languages.\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language mod-\nels are unsupervised multitask learners.\nDavid E. Rumelhart and James L. McClelland. 1986.\nOn learning the past tenses of english verbs. Paral-\nlel Distributed Processing: Explorations in the Mi-\ncrostructure of Cognition, 2:216–271.\n8291\nNaomi Saphra and Adam Lopez. 2019. Understand-\ning learning dynamics of language models with\nSVCCA. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 3257–3267, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2019. Wiki-\nmatrix: Mining 135m parallel sentences in 1620 lan-\nguage pairs from wikipedia.\nAviv Slobodkin, Leshem Choshen, and Omri Abend.\n2021. Mediators in determining what processing\nBERT performs ﬁrst. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 86–93, Online. Asso-\nciation for Computational Linguistics.\nGabriel Stanovsky, Noah A. Smith, and Luke Zettle-\nmoyer. 2019. Evaluating gender bias in machine\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1679–1684, Florence, Italy. Association for\nComputational Linguistics.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A.\nSmith, and Yejin Choi. 2020. Dataset cartography:\nMapping and diagnosing datasets with training dy-\nnamics. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 9275–9293, Online. Associa-\ntion for Computational Linguistics.\nBenedikt Szmrecsányi. 2004. On operationalizing\nsyntactic complexity. In Le poids des mots. Pro-\nceedings of the 7th international conference on tex-\ntual data statistical analysis. Louvain-la-Neuve, vol-\nume 2, pages 1032–1039.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nM. Tomasello. 2003. Constructing a language: A\nusage-based theory of language acquisition.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des\nCombes, Adam Trischler, Yoshua Bengio, and Geof-\nfrey J Gordon. 2018. An empirical study of exam-\nple forgetting during deep neural network learning.\nIn International Conference on Learning Represen-\ntations.\nJonathan G Tullis and Aaron S Benjamin. 2011. On\nthe effectiveness of self-paced learning. Journal of\nmemory and language, 64(2):109–118.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv: Computation and Language.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with\nneural language models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5835–5841.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\nYu-An Wang and Yun-Nung Chen. 2020. What do\nposition embeddings learn? an empirical study of\npre-trained language model positional encoding. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6840–6849, Online. Association for Computa-\ntional Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2019. Blimp: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the As-\nsociation for Computational Linguistics, 8:377–392.\nAdina Williams, Andrew Drozdov, and Samuel R.\nBowman. 2018. Do latent tree learning models iden-\ntify meaningful structure in sentences? Transac-\ntions of the Association for Computational Linguis-\ntics, 6:253–267.\nChen Xing, Devansh Arpit, Christos Tsirigotis, and\nYoshua Bengio. 2018. A walk with sgd. arXiv\npreprint arXiv:1802.08770.\nVictor H Yngve. 1960. A model and an hypothesis\nfor language structure. Proceedings of the American\nphilosophical society, 104(5):444–466.\nY . Zhu, Ryan Kiros, R. Zemel, R. Salakhutdinov, R. Ur-\ntasun, A. Torralba, and S. Fidler. 2015. Aligning\nbooks and movies: Towards story-like visual ex-\nplanations by watching movies and reading books.\n2015 IEEE International Conference on Computer\nVision (ICCV), pages 19–27.\n8292\nA Per challenge Graphs\nWe include behaviours of each model trained over\nthe main dataset used (Wikipedia and books) on\neach BLIMP challenge by perplexity. In general,\naccuracy is similar despite different initialization\nand size of the GPT2 models. TransformerXL\nshows a similar trend, despite the uncomparable\nPerplexity. We supply several examples here and\nleave the rest to the data accompanying this paper.\nFigure 9: The accuracy on determiner noun agreement\nduring training. Accuracy is similar despite different\ninitialization and size of the GPT2 models. Trans-\nformerXL shows a similar trend, despite the uncompa-\nrable Perplexity.\nFigure 10: The accuracy on wh vs that with gap during\ntraining.\nB Details on experimental settings\nWe include further settings to ensure reproduciblity\nof the results. Parameters shared by all the trained\nNLMs include 32K tokens in the vocabulary, 5 ·\n10−5 learning rate, max gradient norm of 1, Adam\noptimizer (Kingma and Ba, 2015), and 10Kwarm-\nup steps. TransformerXl vocabulary is kept to its\nFigure 11: The accuracy on causative during training.\ndefault. All other parameters, including GPT2small\nsize parameters, are the defaults according to the\nHuggingFace transformers library.\nOur 2-5 grams are KenLM (Heaﬁeld, 2011)\ntrained on WikiBooks. A second 5-gram model\ntrained on GigaWord corpus (Graff et al., 2003), as\nreported by BLIMP. The Uni-gram LM is deﬁned\naccording to the frequency of a word in WikiBooks.\nSentence probability is normalized by the number\nof words, which is helpful for the rare cases where\nthe minimal pairs are of different lengths.\nC Correlation during training\nWe see that tendencies during training are not only\nsimilar between instances of the same architec-\nture but also between different architectures. On\ncomparable stages of learning, the GPT2 tiny and\nGPT2small correlate well (>0.9) with respect to\ntheir performance vectors. We present the cor-\nrelations of GPT2tiny compared to GPT2small in\nFig. 12. We ﬁnd the two learn in a similar order\nthroughout their training.\nWe manually compare the results to Trans-\nformerXL. Qualitatively, observing the trajectories\nper challenge (Trajectories are found in Supp. §A\nand the supplied data) of TransformerXL, it seems\nto share the general tendencies of the GPT2 archi-\ntectures. However, reaching a lower stage of train-\ning, it never improves on some challenges (e.g.,\ndeterminer-noun agreement).\nD Models are consistent on per example\nlevel\nWe compute the binary score of every example\nby each model. We reframe the question as an\nannotator agreement problem and ask whether the\nmodels agree on the right answer for each example.\n8293\nFigure 12: Correlation between the performance vec-\ntors of GPT2 tiny and GPT2small, aligned by perplex-\nity.\nFramed this way, the methodology is clear. We\ncompute Fleiss kappa (Fleiss et al., 1969) and ﬁnd\nthe per example correlation. The full results per\nstep and challenge are added as a supplemental\nﬁle. The average overall kappa is 0.83, models not\nonly agree on the order of learning phenomena but\nalso on the order of learning examples within each\nper-phenomenon (if learnt at all). While there are\nphenomena with lower and higher agreement, there\nare only two phenomena in the range of 0.5-0.6\nagreement. Meaning even the most different ones\nhave high example correlation and there is little\nvariance between models to explain.\nOur main aim in this work is to compare models\nacquisition. However, we see the per example order\nof acquisition as less informative, unless we can\ncluster or name the examples learnt. The reason to\nchoose the phenomena was to extract such names,\nand we hence focus in our work on them.\nNote, that consistency per example was shown\nbefore in the scope of computer vision (Hacohen\net al., 2020). However, a critical difference is that\nthey deal with classiﬁcation and check whether\nwhich examples are learnt ﬁrst. We however, aim\nto ask about generalization, given that you learn\none task (language modelling), what type of gen-\neralizations do you make, tested on another. For\nexample, while learning to predict the next word,\nthe network understands after X steps that the verb\nshould be in agreement with the subject.\nE Reproducing with other models\nWe provide the GPT2small correlation with other\nmodels and with various metrics and models in Fig.\n13 and 14 respectively. We also supply the average\nBLIMP accuracies of the models we trained in Fig.\n15.\nFigure 13: Correlation between the difﬁculty of GPT2\nand of other models for each phenomena in each train-\ning step.\nFigure 14: Correlation between the difﬁculty predicted\nby BLIMP models and the difﬁculties for the model for\neach phenomena in each training step.\nFigure 15: Overall BLIMP accuracy by step.\n8294\nE.1 Results mainly replicate in\nTransformerXL\nWe replicate the same experiment over the train-\ning of the TransformerXL instance. The Trans-\nformerXL seems to reach a lower stage of learning,\nprobably due to the vast vocabulary and model.\nThe model replicates some of the general no-\ntions seen on GPT2small. It correlates most with\nsimpler models, then with humans and then with\nglobal features. At ﬁrst, sentence length makes a\nsentence more challenging than its actual structure,\n5 window BOW starts as more relevant than BOW\nover all the sentence.\nWe do see that the overall graph is quite straight.\nWith that, the increase in correlation with humans\nis quite small, the BOW models don’t drop and the\nevidence of relying on more abstract knowledge in\nlate stages is less apparent. This might be expected,\nas we know the network reached an early step on\nthe performance scale.\nFigure 16: Correlation between the difﬁculty predicted\nby metrics and the difﬁculties for the model for each\nphenomena in each time step.\nF Reproducing with other data\nAs comparison to the correlations with our main\nmodel, we provide the correlations of GPT2 tiny\ntrained on OpenWebText with the two 5-gram mod-\nels, one on WikiBooks and one on Giga word (Fig.\n17). We see that the higher resemblence to Wiki-\nBook trained model is kept despite being trained\non the same data, but the difference is lower at the\nbeginning and more stable. It might be the case\nthat over reliance on the speciﬁc data is shown at\nthose ﬁrst steps where the difference is large, but it\nwould require further evidence.\nWe also compare the model to several other\ntrained models in Fig. 18.\nFigure 17: Correlation during training of GPT2 tiny\non OpenWebText compared to 5-gram model trained\non WikiBooks and on GigaWord. Correlation is over\nBLIMP challenges. Numbers indicate the overall aver-\nage of the reference models over BLIMP and are found\nover the step with most similar accuracy on GPT2 tiny.\nGPT2tiny best score is 67.\nFigure 18: Correlation during training of GPT2 tiny\ntrained on OpenWebText data compared to Off-the-\nshelve models and XL smaller models. The correlation\nwith itself during training is shown in gray. Correlation\nis over BLIMP challenges. Numbers indicate the over-\nall average of the reference models over BLIMP and\nare found over the step with most similar accuracy on\nGPT2tiny.\n8295\nG 5-grams notes\nThe gap between the correlation with the two 5-\ngrams decreases during the ﬁrst 50K steps or so,\nand then remains constant. This suggests that\nthe choice of a dataset is more important during\nearly NLM training. Because, at the beginning\nthe network learn generalizations which are more\ncommon to counts of one (huge, general domain)\ndataset than another, and this effect diminishes.\nPossibly, this is because at this point NLMs rely\nmore on word identity, rather than on abstract gen-\neralizations, that are shared to a greater extent\nacross corpora (see §4). We observe that the 5-\ngram trained on WikiBooks correlates better with\nGPT2tiny, even when GPT2tiny is not trained on it\n(not reported). We cannot offer a simple explana-\ntion for this trend.\nH Clustering BLIMP\nWe include the learning curves of GPT2 tiny on\nBLIMP dataset, clustered according to ﬁelds\n(Fig. 19)„ super-phenomena (Fig. 20), and the spec-\ntral clustering (Fig. 21). Due to restrictions on ap-\npendix ﬁles the ﬁgures are found in corresponding\nfolders in the supplied data.\nFigure 19: Cluster of semantic phenomena, each line is\nthe trajectory of learning of a phenomenon.\nFigure 20: Anaphor agreement super phenomena tra-\njectories.\n8296\nFigure 21: Cluster of phenomebna chosen by spectral clustering. The phenomena behave similarly but do not\nfollow the same linguistic categorizations.\n8297"
}