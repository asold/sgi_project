{
  "title": "Large language models for biomedicine: foundations, opportunities, challenges, and best practices",
  "url": "https://openalex.org/W4395067526",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2112935945",
      "name": "Satya S. Sahoo",
      "affiliations": [
        "Case Western Reserve University"
      ]
    },
    {
      "id": "https://openalex.org/A2345064130",
      "name": "Joseph M. Plasek",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A687469964",
      "name": "Ozlem Uzuner",
      "affiliations": [
        "George Mason University"
      ]
    },
    {
      "id": "https://openalex.org/A2167957381",
      "name": "Trevor Cohen",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A101533768",
      "name": "Meliha Yetisgen",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2310998875",
      "name": "Hongfang Liu",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A4371576106",
      "name": "Stephane Meystre",
      "affiliations": [
        "University of Applied Sciences and Arts of Southern Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2156867267",
      "name": "YanShan Wang",
      "affiliations": [
        "University of Pittsburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6781031682",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6788811087",
    "https://openalex.org/W6762392948",
    "https://openalex.org/W3196696529",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6739585900",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W3194676777",
    "https://openalex.org/W4297834221",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4388725043",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2999905431",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2228826686",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "Abstract Objectives Generative large language models (LLMs) are a subset of transformers-based neural network architecture models. LLMs have successfully leveraged a combination of an increased number of parameters, improvements in computational efficiency, and large pre-training datasets to perform a wide spectrum of natural language processing (NLP) tasks. Using a few examples (few-shot) or no examples (zero-shot) for prompt-tuning has enabled LLMs to achieve state-of-the-art performance in a broad range of NLP applications. This article by the American Medical Informatics Association (AMIA) NLP Working Group characterizes the opportunities, challenges, and best practices for our community to leverage and advance the integration of LLMs in downstream NLP applications effectively. This can be accomplished through a variety of approaches, including augmented prompting, instruction prompt tuning, and reinforcement learning from human feedback (RLHF). Target Audience Our focus is on making LLMs accessible to the broader biomedical informatics community, including clinicians and researchers who may be unfamiliar with NLP. Additionally, NLP practitioners may gain insight from the described best practices. Scope We focus on 3 broad categories of NLP tasks, namely natural language understanding, natural language inferencing, and natural language generation. We review the emerging trends in prompt tuning, instruction fine-tuning, and evaluation metrics used for LLMs while drawing attention to several issues that impact biomedical NLP applications, including falsehoods in generated text (confabulation/hallucinations), toxicity, and dataset contamination leading to overfitting. We also review potential approaches to address some of these current challenges in LLMs, such as chain of thought prompting, and the phenomena of emergent capabilities observed in LLMs that can be leveraged to address complex NLP challenge in biomedical applications.",
  "full_text": null,
  "topic": "Biomedicine",
  "concepts": [
    {
      "name": "Biomedicine",
      "score": 0.8776457905769348
    },
    {
      "name": "Computer science",
      "score": 0.5868664979934692
    },
    {
      "name": "Data science",
      "score": 0.4870298504829407
    },
    {
      "name": "Best practice",
      "score": 0.4141278862953186
    },
    {
      "name": "Political science",
      "score": 0.15812593698501587
    },
    {
      "name": "Bioinformatics",
      "score": 0.06818199157714844
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}