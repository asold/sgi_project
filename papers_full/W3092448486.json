{
  "title": "On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers",
  "url": "https://openalex.org/W3092448486",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5084471503",
      "name": "Marius Mosbach",
      "affiliations": [
        "Saarland University"
      ]
    },
    {
      "id": "https://openalex.org/A5080508429",
      "name": "Anna Khokhlova",
      "affiliations": [
        "Saarland University"
      ]
    },
    {
      "id": "https://openalex.org/A5071306466",
      "name": "Michael A. Hedderich",
      "affiliations": [
        "Saarland University"
      ]
    },
    {
      "id": "https://openalex.org/A5008875255",
      "name": "Dietrich Klakow",
      "affiliations": [
        "Saarland University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W3023419341",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2516090925",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2963121782",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W3022717752",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2125101937",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W2951732656",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2525332836"
  ],
  "abstract": "Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.",
  "full_text": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 68–82\nOnline, November 20, 2020.c⃝2020 Association for Computational Linguistics\n68\nOn the Interplay Between Fine-tuning and Sentence-level Probing for\nLinguistic Knowledge in Pre-trained Transformers\nMarius Mosbach Anna Khokhlova Michael A. Hedderich Dietrich Klakow\nSpoken Language Systems (LSV)\nDepartment of Language Science and Technology\nSaarland Informatics Campus, Saarland University, Germany\n{mmosbach,akhokhlova,mhedderich,dklakow}@lsv.uni-saarland.de\nAbstract\nFine-tuning pre-trained contextualized embed-\nding models has become an integral part of\nthe NLP pipeline. At the same time, prob-\ning has emerged as a way to investigate the\nlinguistic knowledge captured by pre-trained\nmodels. Very little is, however, understood\nabout how ﬁne-tuning affects the represen-\ntations of pre-trained models and thereby\nthe linguistic knowledge they encode. This\npaper contributes towards closing this gap.\nWe study three different pre-trained models:\nBERT, RoBERTa, and ALBERT, and investi-\ngate through sentence-level probing how ﬁne-\ntuning affects their representations. We ﬁnd\nthat for some probing tasks ﬁne-tuning leads to\nsubstantial changes in accuracy, possibly sug-\ngesting that ﬁne-tuning introduces or even re-\nmoves linguistic knowledge from a pre-trained\nmodel. These changes, however, vary greatly\nacross different models, ﬁne-tuning and prob-\ning tasks. Our analysis reveals that while ﬁne-\ntuning indeed changes the representations of a\npre-trained model and these changes are typ-\nically larger for higher layers, only in very\nfew cases, ﬁne-tuning has a positive effect on\nprobing accuracy that is larger than just us-\ning the pre-trained model with a strong pool-\ning method. Based on our ﬁndings, we argue\nthat both positive and negative effects of ﬁne-\ntuning on probing require a careful interpreta-\ntion.\n1 Introduction\nTransformer-based contextual embeddings like\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019b) and ALBERT (Lan et al., 2020) recently be-\ncame the state-of-the-art on a variety of NLP down-\nstream tasks. These models are pre-trained on large\namounts of text and subsequently ﬁne-tuned on\ntask-speciﬁc, supervised downstream tasks. Their\nstrong empirical performance triggered questions\nconcerning the linguistic knowledge they encode\nin their representations and how it is affected by\nthe training objective and model architecture (Kim\net al., 2019; Wang et al., 2019a). One promi-\nnent technique to gain insights about the linguis-\ntic knowledge encoded in pre-trained models is\nprobing (Rogers et al., 2020). However, works on\nprobing have so far focused mostly on pre-trained\nmodels. It is still unclear how the representations\nof a pre-trained model change when ﬁne-tuning on\na downstream task. Further, little is known about\nwhether and to what extent this process adds or\nremoves linguistic knowledge from a pre-trained\nmodel. Addressing these issues, we are investigat-\ning the following questions:\n1. How and where does ﬁne-tuning affect the\nrepresentations of a pre-trained model?\n2. To which extent (if at all) can changes in prob-\ning accuracy be attributed to a change in lin-\nguistic knowledge encoded by the model?\nTo answer these questions, we investigate three\ndifferent pre-trained encoder models, BERT,\nRoBERTa, and ALBERT. We ﬁne-tune them on\nsentence-level classiﬁcation tasks from the GLUE\nbenchmark (Wang et al., 2019b) and evaluate the\nlinguistic knowledge they encode leveraging three\nsentence-level probing tasks from the SentEval\nprobing suite (Conneau et al., 2018). We focus\non sentence-level probing tasks to measure linguis-\ntic knowledge encoded by a model for two reasons:\n1) during ﬁne-tuning we explicitly train a model\nto represent sentence-level context in its represen-\ntations and 2) we are interested in the extent to\nwhich this affects existing sentence-level linguistic\nknowledge already present in a pre-trained model.\nWe ﬁnd that while, indeed, ﬁne-tuning affects\na model’s sentence-level probing accuracy and\nthese effects are typically larger for higher lay-\ners, changes in probing accuracy vary depend-\n69\ning on the encoder model, ﬁne-tuning and prob-\ning task combination. Our results also show that\nsentence-level probing accuracy is highly depen-\ndent on the pooling method being used. Only in\nvery few cases, ﬁne-tuning has a positive effect\non probing accuracy that is larger than just us-\ning the pre-trained model with a strong pooling\nmethod. Our ﬁndings suggest that changes in prob-\ning performance can not exclusively be attributed to\nan improved or deteriorated encoding of linguistic\nknowledge and should be carefully interpreted. We\npresent further evidence for this interpretation by\ninvestigating changes in the attention distribution\nand language modeling capabilities of ﬁne-tuned\nmodels which constitute alternative explanations\nfor changes in probing accuracy.\n2 Related Work\nProbing A large body of previous work focuses\non analyses of the internal representations of neural\nmodels and the linguistic knowledge they encode\n(Shi et al., 2016; Ettinger et al., 2016; Adi et al.,\n2016; Belinkov et al., 2017; Hupkes et al., 2018).\nIn a similar spirit to these ﬁrst works on probing,\nConneau et al. (2018) were the ﬁrst to compare dif-\nferent sentence embedding methods for the linguis-\ntic knowledge they encode. Krasnowska-Kiera ´s\nand Wr´oblewska (2019) extended this approach to\nstudy sentence-level probing tasks on English and\nPolish sentences.\nAlongside sentence-level probing, many recent\nworks (Peters et al., 2018; Liu et al., 2019a; Tenney\net al., 2019b; Lin et al., 2019; Hewitt and Manning,\n2019) have focused on token-level probing tasks in-\nvestigating more recent contextualized embedding\nmodels such as ELMo (Peters et al., 2018), GPT\n(Radford et al., 2019), and BERT (Devlin et al.,\n2019). Two of the most prominent works following\nthis methodology are Liu et al. (2019a) and Ten-\nney et al. (2019b). While Liu et al. (2019a) use\nlinear probing classiﬁers as we do, Tenney et al.\n(2019b) use more expressive, non-linear classiﬁers.\nHowever, in contrast to our work, most studies\nthat investigate pre-trained contextualized embed-\nding models focus on pre-trained models and not\nﬁne-tuned ones. Moreover, we aim to assess how\nprobing performance changes with ﬁne-tuning and\nhow these changes differ based on the model ar-\nchitecture, as well as probing and ﬁne-tuning task\ncombination.\nFine-tuning While ﬁne-tuning pre-trained lan-\nguage models leads to a strong empirical per-\nformance across various supervised NLP down-\nstream tasks (Wang et al., 2019b), ﬁne-tuning itself\n(Dodge et al., 2020) and its effects on the represen-\ntations learned by a pre-trained model are poorly\nunderstood. As an example, Phang et al. (2018)\nshow that downstream accuracy can beneﬁt from\nan intermediate ﬁne-tuning task, but leave the in-\nvestigation of why certain tasks beneﬁt from in-\ntermediate task training to future work. Recently,\nPruksachatkun et al. (2020) extended this approach\nusing eleven diverse intermediate ﬁne-tuning tasks.\nThey view probing task performance after ﬁne-\ntuning as an indicator of the acquisition of a par-\nticular language skill during intermediate task ﬁne-\ntuning. This is similar to our work in the sense that\nprobing accuracy is used to understand how ﬁne-\ntuning affects a pre-trained model. Talmor et al.\n(2019) try to understand whether the performance\non downstream tasks should be attributed to the\npre-trained representations or rather the ﬁne-tuning\nprocess itself. They ﬁne-tune BERT and RoBERTa\non a large set of symbolic reasoning tasks and ﬁnd\nthat while RoBERTa generally outperforms BERT\nin its reasoning abilities, the performance of both\nmodels is highly context dependent.\nMost similar to our work is the contemporane-\nous work by Merchant et al. (2020). They inves-\ntigate how ﬁne-tuning leads to changes in the rep-\nresentations of a pre-trained model. In contrast\nto our work, their focus, however, lies on edge-\nprobing (Tenney et al., 2019b) and structural prob-\ning tasks (Hewitt and Manning, 2019) and they\nstudy only a single pre-trained encoder: BERT. We\nconsider our work complementary to them since\nwe study sentence-level probing tasks, use differ-\nent analysis methods and investigate the impact of\nﬁne-tuning on three different pre-trained encoders:\nBERT, RoBERTa, and ALBERT.\n3 Methodology and Setup\nThe focus of our work is on studying how ﬁne-\ntuning affects the representations learned by a pre-\ntrained model. We assess this change through\nsentence-level probing tasks. We focus on sentence-\nlevel probing tasks since during ﬁne-tuning we ex-\nplicitly train a model to represent sentence-level\ncontext in the CLS token.\nThe ﬁne-tuning and probing tasks we study con-\ncern different linguistic levels, requiring a model\n70\nModel Task\nCoLA SST-2 RTE SQuAD\nDevlin et al. (2019)52.1 93 .5 66 .4 80 .8/88.5\nBERT 59.5 92 .4 64 .6 78 .6/86.5\nRoBERTa 60.3 93 .6 73 .6 81 .7/89.3\nALBERT 45.8 88 .5 69 .6 79 .9/87.6\nTable 1: Fine-tuning performance on the development\nset on selected down-stream tasks. For comparison\nwe also report the ﬁne-tuning accuracy of BERT-base-\ncased as reported by Devlin et al. (2019) on the test set\nof each of the tasks taken from the GLUE and SQuAD\nleaderboards. We report Matthews correlation coefﬁ-\ncient for CoLA, accuracy for SST-2 and RTE, and exact\nmatch (EM) and F1 score for SQuAD.\nto focus more on syntactic, semantic or discourse\ninformation. The extent to which knowledge of\na particular linguistic level is needed to perform\nwell differs from task to task. For instance, to\njudge if the syntactic structure of a sentence is in-\ntact, no deep discourse understanding is needed.\nOur hypothesis is that if a pre-trained model en-\ncodes certain linguistic knowledge, this acquired\nknowledge should lead to a good performance on\na probing task testing for the same linguistic phe-\nnomenon. Extending this hypothesis to ﬁne-tuning,\none might argue that if ﬁne-tuning introduces new\nor removes existing linguistic knowledge into/from\na model, this should be reﬂected by an increase\nor decrease in probing performance. 1 However,\nwe argue that encoding or forgetting linguistic\nknowledge is not necessarily the only explana-\ntion for observed changes in probing accuracy .\nHence, the goal of our work is to test the above-\nstated hypotheses assessing the interaction between\nﬁne-tuning and probing tasks across three different\nencoder models.\n3.1 Fine-tuning tasks\nWe study three ﬁne-tuning tasks taken from the\nGLUE benchmark (Wang et al., 2019b). All the\ntasks are sentence-level classiﬁcation tasks and\ncover different levels of linguistic phenomena. Ad-\nditionally, we study models ﬁne-tuned on SQuAD\n(Rajpurkar et al., 2016) a widely used question an-\nswering dataset. Statistics for each of the tasks can\n1Merchant et al. (2020) follow a similar reasoning. They\nﬁnd that ﬁne-tuning on dependency parsing task leads to an\nimprovement on the constituents probing task and attribute\nthis to the improved linguistic knowledge. Similarly, Pruk-\nsachatkun et al. (2020) view probing task performance as “an\nindicator for the acquisition of a particular language skill.”\nbe found in the Appendix.\nCoLA The Corpus of Linguistic Acceptability\n(Warstadt et al., 2018) is an acceptability task\nwhich tests a model’s knowledge of grammatical\nconcepts. We expect that ﬁne-tuning on CoLA re-\nsults in changes in accuracy on a syntactic probing\ntask.2\nSST-2 The Stanford Sentiment Treebank (Socher\net al., 2013). We use the binary version where the\ntask is to categorize movie reviews to have either\npositive or negative valence. Making sentiment\njudgments requires knowing the meanings of iso-\nlated words and combining them on the sentence\nand discourse level (e.g. in case of irony). Hence,\nwe expect to see a difference for semantic and/or\ndiscourse probing tasks when ﬁne-tuning on SST-2.\nRTE The Recognizing Textual Entailment\ndataset is a collection of sentence-pairs in either\nneutral or entailment relationship collected from\na series of annual textual entailment challenges\n(Dagan et al., 2005; Bar-Haim et al., 2006;\nGiampiccolo et al., 2007; Bentivogli et al., 2009).\nThe task requires a deeper understanding of the\nrelationship of two sentences, hence, ﬁne-tuning on\nRTE might affect the accuracy on a discourse-level\nprobing task.\nSQuAD The Stanford Questions Answering\nDataset (Rajpurkar et al., 2016) is a popular ex-\ntractive reading comprehension dataset. The task\ninvolves a broader discourse understanding as a\nmodel trained on SQuAD is required to extract\nthe answer to a question from an accompanying\nparagraph.\n3.2 Probing Tasks\nWe select three sentence-level probing tasks from\nthe SentEval probing suit (Conneau et al., 2018),\ntesting for syntactic, semantic and broader dis-\ncourse information on the sentence-level.\nbigram-shift is a syntactic binary classiﬁcation\ntask that tests a model’s sensitivity to word order.\nThe dataset consists of intact and corrupted sen-\ntences, where for corrupted sentences, two random\nadjacent words have been inverted.\n2CoLA contains sentences with syntactic, morphological\nand semantic violations. However, only about 15% of the sen-\ntences are labeled with morphological and semantic violations.\nHence, we suppose that ﬁne-tuning on CoLA should increase\na model’s sensitivity to syntactic violations to a greater extent.\n71\nsemantic-odd-man-out tests a model’s sensitiv-\nity to semantic incongruity on a collection of sen-\ntences where random verbs or nouns are replaced\nby another verb or noun.\ncoordination-inversion is a collection of sen-\ntences made out of two coordinate clauses. In half\nof the sentences, the order of the clauses is inverted.\nCoordinate-inversion tests for a model’s broader\ndiscourse understanding.\n3.3 Pre-trained Models\nIt is unclear to which extent ﬁndings on the en-\ncoding of certain linguistic phenomena generalize\nfrom one pre-trained model to another. Hence, we\nexamine three different pre-trained encoder models\nin our experiments.\nBERT (Devlin et al., 2019) is a transformer-\nbased model (Vaswani et al., 2017) jointly trained\non masked language modeling and next-sentence-\nprediction – a sentence-level binary classiﬁcation\ntask. BERT was trained on the Toronto Books cor-\npus and the English portion of Wikipedia. We focus\non the BERT-base-casedmodel which consists of\n12 hidden layers and will refer to it as BERT in the\nfollowing.\nRoBERTa (Liu et al., 2019b) is a follow-up ver-\nsion of BERT which differs from BERT in a few\ncrucial aspects, including using larger amounts of\ntraining data and longer training time. The aspect\nthat is most relevant in the context of this work is\nthat RoBERTa was pre-trained without a sentence-\nlevel objective, minimizing only the masked lan-\nguage modeling objective. As with BERT we will\nconsider the base model, RoBERTa-base, for this\nstudy and refer to it as RoBERTa.\nALBERT (Lan et al., 2020) is another recently\nproposed transformer-based pre-trained masked\nlanguage model. In contrast to both BERT and\nRoBERTa, it makes heavy use of parameter shar-\ning. That is, ALBERT ties the weight matrices\nacross all hidden layers effectively applying the\nsame non-linear transformation on every hidden\nlayer. Additionally, similar to BERT, ALBERT\nuses a sentence-level pre-training task. We will use\nthe base model ALBERT-base-v1 and refer to it\nas ALBERT throughout this work.\n3.4 Fine-tuning and Probing Setup\nFine-tuning For ﬁne-tuning, we follow the de-\nfault setup proposed by Devlin et al. (2019). A\nsingle randomly initialized task-speciﬁc classiﬁca-\ntion layer is added on top of the pre-trained en-\ncoder. As input, the classiﬁcation layer receives\nz = tanh (Wh + b), where h is the hidden rep-\nresentation of the ﬁrst token on the last hidden\nlayer and W and b are the randomly initialized\nparameters of the classiﬁer.3 During ﬁne-tuning all\nmodel parameters are updated jointly. We train for\n3 epochs on CoLA and for 1 epoch on SST-2, using\na learning rate of 2e−5. The learning rate is lin-\nearly increased for the ﬁrst 10% of steps (warmup)\nand kept constant afterwards. An overview of all\nhyper-parameters for each model and task can be\nfound in the Appendix. Fine-tuning performance\non the development set of each of the tasks can be\nfound in Table 1.\nProbing For probing, our setup largely follows\nthat of previous works (Tenney et al., 2019b; Liu\net al., 2019a; Hewitt and Liang, 2019) where a\nprobing classiﬁer is trained on top of the contex-\ntualized embeddings extracted from a pre-trained\nor – as in our case – ﬁne-tuned encoder model. No-\ntably, we train linear (logistic regression) probing\nclassiﬁers and use two different pooling methods\nto obtain sentence embeddings from the encoder\nhidden states: CLS-pooling, which simply returns\nthe hidden state corresponding to the ﬁrst token of\nthe sentence and mean-pooling which computes a\nsentence embedding as the mean over all hidden\nstates. We do this to assess the extent to which\nthe CLS token captures sentence-level context. We\nuse linear probing classiﬁers because intuitively\nwe expect that if a linguistic feature is useful for a\nﬁne-tuning task, it should be linearly separable in\nthe embeddings. For all probing tasks, we measure\nlayer-wise accuracy to investigate how the linear\nseparability of a particular linguistic phenomenon\nchanges across the model. In total, we train 390\nprobing classiﬁers on top of 12 pre-trained and\nﬁne-tuned encoder models.\nImplementation Our experiments are imple-\nmented in PyTorch (Paszke et al., 2019) and\nwe use the pre-trained models provided by the\nHuggingFace transformers library (Wolf et al.,\n2019). Code to reproduce our results and ﬁgures is\navailable online: https://github.com/uds-lsv/\nprobing-and-finetuning\n3For BERT and ALBERT h corresponds to the hidden\nstate of the [CLS] token. For RoBERTa the ﬁrst token of every\nsentence is the <s> token. We will refer to both of them as\nCLS token.\n72\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95Accuracy\nALBERT CLS\nALBERT mean\nBERT CLS\nBERT mean\nRoBERTa CLS\nRoBERTa mean\n(a) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75Accuracy\nALBERT CLS\nALBERT mean\nBERT CLS\nBERT mean\nRoBERTa CLS\nRoBERTa mean (b) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70Accuracy\n ALBERT CLS\nALBERT mean\nBERT CLS\nBERT mean\nRoBERTa CLS\nRoBERTa mean (c) odd-man-out\nFigure 1: Layer-wise probing accuracy on bigram-shift, coordination inversion, and odd-man-out for BERT,\nRoBERTa, and ALBERT. For all models mean-pooling (solid lines) consistently improves probing accuracy com-\npared to CLS-pooling (dashed-lines) highlighting the importance of sentence-level information for each of the\ntasks.\nProbing Task\nBERT-base-cased\nCLS-pooling mean-pooling\nCoLA SST-2 CoLA SST-2\n0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12\nbigram-shift 0.07 4 .73 −1.02 −4.63 0 .23 1 .45 −0.37 −3.24\ncoordinate-inversion −0.10 1 .90 −0.25 −1.15 0 .14 0 .29 −0.48 −0.85\nodd-man-out −0.20 0 .26 −0.02 −1.28 −0.34 −0.29 −0.30 −1.09\nProbing Task\nRoBERTa-base\nCLS-pooling mean-pooling\nCoLA SST-2 CoLA SST-2\n0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12\nbigram-shift 0.58 5 .35 −2.41 −7.22 0 .69 1 .74 −0.23 −4.87\ncoordinate-inversion −0.72 1 .84 −1.28 −0.63 −0.22 0 .02 −0.18 −3.83\nodd-man-out −0.66 1 .05 −1.09 −2.40 −0.08 −0.55 −0.46 −3.61\nProbing Task\nALBERT-base-v1\nCLS-pooling mean-pooling\nCoLA SST-2 CoLA SST-2\n0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12\nbigram-shift 1.55 3 .39 −1.94 −5.15 0 .26 0 .66 −0.70 −2.73\ncoordinate-inversion −0.69 −1.53 −1.07 -2.87 −0.07 −1.19 −0.35 −1.53\nodd-man-out −0.42 −1.39 −0.90 −2.75 −0.27 −1.40 −0.60 −2.82\nTable 2: Change in probing accuracy ∆ (in %) of CoLA and SST-2 ﬁne-tuned models compared to the pre-trained\nmodels when using CLS and mean-pooling. We average the difference in probing accuracy over two different\nlayers groups: layers 0 to 6 and layers 7 to 12.\n4 Experiments\n4.1 Probing Accuracy\nFigure 1 shows the layer-wise probing accuracy\nof BERT, RoBERTa, and ALBERT on each of\nthe probing tasks. These results establish base-\nlines for our comparison with ﬁne-tuned models be-\nlow. Consistent with previous work (Krasnowska-\nKiera´s and Wr ´oblewska, 2019), we observe that\nmean-pooling generally outperforms CLS-pooling\nacross all probing tasks, highlighting the impor-\ntance of sentence-level context for each of the prob-\n73\ning tasks. We also ﬁnd that for bigram-shift prob-\ning accuracy is substantially larger than that for\ncoordination-inversion and odd-man-out. Again,\nthis is consistent with ﬁndings in previous works\n(Tenney et al., 2019b; Liu et al., 2019a; Tenney\net al., 2019a) reporting better performance on syn-\ntactic than semantic probing tasks.\nWhen comparing the three encoder models, we\nobserve some noticeable differences. On odd-man-\nout, ALBERT performs signiﬁcantly worse than\nboth BERT and RoBERTa, with RoBERTa per-\nforming best across all layers. We attribute the\npoor performance of ALBERT to the fact that it\nmakes heavy use of weight-sharing, effectively\napplying the same non-linear transformation on\nall layers. We also observe that on coordination-\ninversion, RoBERTa with CLS pooling performs\nmuch worse than both BERT and ALBERT with\nCLS pooling. We attribute this to the fact that\nRoBERTa lacks a sentence-level pre-training ob-\njective and the CLS token hence fails to capture\nrelevant sentence-level information for this particu-\nlar probing task. The small differences in probing\naccuracy for BERT and ALBERT when comparing\nCLS to mean-pooling and the fact that RoBERTa\nwith mean-pooling outperforms all other models on\ncoordination-inversion is providing evidence for\nthis interpretation.\n4.2 How does Fine-tuning affect Probing\nAccuracy?\nHaving established baselines for the probing accu-\nracy of the pre-trained models, we now turn to the\nquestion of how it is affected by ﬁne-tuning. Table\n2 shows the effect of ﬁne-tuning on CoLA and SST-\n2 on the layer-wise accuracy for all three encoder\nmodels across the three probing tasks. Results\nfor RTE and SQuAD can be found in Table 5 in\nthe Appendix. For all models and tasks we ﬁnd\nthat ﬁne-tuning has mostly an effect on higher\nlayers, both positive and negative. The impact\nvaries depending on the ﬁne-tuning/probing task\ncombination and underlying encoder model.\nPositive Changes in Accuracy: Fine-tuning on\nCoLA results in a substantial improvement on the\nbigram-shift probing task for all the encoder mod-\nels; ﬁne-tuning on RTE improves the coordination-\ninversion accuracy for RoBERTa. This ﬁnding is\nin line with our expectations: bigram-shift and\nCoLA require syntactic level information, whereas\ncoordination-inversion and RTE require a deeper\ndiscourse-level understanding. However, when tak-\ning a more detailed look, this reasoning becomes\nquestionable: The improvement is only visible\nwhen using CLS-pooling and becomes negligible\nwhen probing with mean-pooling. Moreover, the\ngains are not large enough to improve signiﬁcantly\nover the mean-pooling baseline (as shown by the\nstars and the second y-axis in Figure 4). This sug-\ngests that adding new linguistic knowledge is not\nnecessarily the only driving force behind the im-\nproved probing accuracy and we provide evidence\nfor this reasoning in Section 5.1.\nNegative Changes in Accuracy: Across all\nmodels and pooling methods, ﬁne-tuning on SST-2\nhas a negative impact on probing accuracy on\nbigram-shift and odd-man-out, and the decrease\nin probing accuracy is particularly large for\nRoBERTa. Fine-tuning on SQuAD follows a\nsimilar trend: it has a negative effect on probing\naccuracy on bigram-shift and odd-man-out for both\nCLS- and mean-pooling (see Table 5), while the\nimpact on coordination-inversion is negligible. We\nargue that this strong negative impact on probing\naccuracy is the consequence of more dramatic\nchanges in the representations. We investigate this\nissue further in Section 5.2.\nChanges in probing accuracy for other ﬁne-\ntuning/probing combinations are not substantial,\nwhich suggests that representations did not change\nsigniﬁcantly with regard to the probed information.\n5 What Happens During Fine-tuning?\nIn the previous part, we saw the effects of differ-\nent ﬁne-tuning approaches on model performance.\nThis opens the question for their causes. In this\nsection, we study two hypotheses that go towards\nexplaining these effects.\n5.1 Analyzing Attention Distributions\nIf the improvement in probing accuracy with CLS-\npooling can be attributed to a better sentence rep-\nresentation in the CLS token, this can be due to a\ncorresponding change in a model’s attention distri-\nbution. The model might change the attention of\nthe CLS token to cover more tokens and with this\nbuild a better representation of the whole sentence.\nTo study this hypothesis, we ﬁne-tune RoBERTa\non CoLA using two different methods: the default\nCLS-pooling approach and mean-pooling (cf. Sec-\ntion 3.4). We compare the layer-wise attention\n74\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nMean Entropy\nbase\nﬁnetuned-cls\nﬁnetuned-meanpooling\n(a) Entropy\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nMean Earth Mover Distance\nﬁnetuned-cls\nﬁnetuned-meanpooling (b) Earth-Mover Distance\nFigure 2: Entropy and Earth mover’s distance of the attention for the CLS token for each layer with the RoBERTa\nmodel on the bigram-shift dataset. The mean over all input sequences and the mean over all attention heads of a\nlayer are taken. The Earth Mover Distance is computed between the base model and each ﬁne-tuned model.\ndistribution on bigram-shift after ﬁne-tuning to that\ndata. We expect to see more profound changes for\nCLS-pooling than for mean-pooling. To investigate\nhow the attention distribution changes, we analyze\nits entropy, i.e.\nHj =\n∑\ni\naj (xi) ·log(aj (xi)) (1)\nwhere xi is the i-th token of an input sequence\nand a(xi) the corresponding attention at position\njgiven to it by a speciﬁc attention head. Entropy\nis maximal when the attention is uniform over the\nwhole input sequence and minimal if the attention\nhead focuses on just one input token.\nFigure 2a shows the mean entropy for the CLS\ntoken (i.e. H0) before and after ﬁne-tuning. We\nobserve a large increase in entropy in the last three\nlayers when ﬁne-tuning on the CLS token (orange\nbars). This is consistent with our interpretation\nthat, during ﬁne-tuning, the CLS token learns to\ntake more sentence-level information into account,\ntherefore being required to spread its attention over\nmore tokens. For mean-pooling (green bars) this\nmight not be required as taking the mean over\nall token-states could already provide sufﬁcient\nsentence-level information during ﬁne-tuning. Ac-\ncordingly, there are only small changes in the en-\ntropy for mean-pooling, with the mean entropy\nactually decreasing in the last layer.\nEntropy alone is, however, not sufﬁcient to an-\nalyze changes in the attention distribution. Even\nwhen the amount of entropy is similar, the underly-\ning attention distribution might have changed. Fig-\nure 2b, therefore, compares the attentions of an at-\ntention head for an input sequence before and after\nﬁne-tuning using Earth mover’s distance(Rubner\net al., 1998). We ﬁnd that, similarly to the entropy\nresults, changes in attention tend to increase with\nthe layer number and again, the largest change of\nthe attention distribution is visible for the ﬁrst token\nfor layer 11 and 12 when pooling on the CLS-token,\nwhile the change is much smaller for mean-pooling.\nThis afﬁrms our hypothesis that improvements in\nthe ﬁne-tuning with CLS-pooling can be attributed\nto a change in the attention distribution which is\nless necessary for the mean-pooling.\n5.2 Analyzing MLM Perplexity\nIf ﬁne-tuning has more profound effects on the\nrepresentations of a pre-trained model potentially\nintroducing or removing linguistic knowledge, we\nexpect to see larger changes to the language mod-\neling abilities of the model when compared to the\ncase where ﬁne-tuning just changes the attention\ndistribution of the CLS token.\nFor this, we analyze how ﬁne-tuning on CoLA\nand SST-2 affect the language modeling abilities\nof a pre-trained model. A change in perplexity\nshould reveal if the representations of the model\ndid change during ﬁne-tuning and we expect this\nchange to be larger for SST-2 ﬁne-tuning where\nwe observe a large negative increase in probing\naccuracy.\nFor the ﬁrst experiment, we evaluate the pre-\ntrained masked language model heads of BERT\nand RoBERTa on the Wikitext-2 test set (Merity\net al., 2017) and compare it to the masked-language\nmodeling perplexity, hereafter perplexity, of ﬁne-\ntuned models.4 In the second experiment, we test\n4Note that perplexity results are not directly comparable\nbetween BERT and RoBERTa since both models have differ-\nent vocabularies. However, what we are interested in is rather\n75\n(a) RoBERTa-base\n (b) BERT-base-cased\n(c) RoBERTa-base\n (d) BERT-base-cased\nFigure 3: Perplexity on Wikitext-2 of models consisting of a ﬁne-tuned encoder and a pre-trained MLM-head.\nPlots (a) and (b) show how perplexity changes over the course of ﬁne-tuning with epoch 0 showing the perplexity\nof the pre-trained model. (c) and (d) show how perplexity changes when a number of last layers of the ﬁne-tuned\nencoder are replaced with corresponding layers from the pre-trained model. Note the different y-axes for RoBERTa\nand BERT.\nwhich layers contribute most to the change in per-\nplexity and replace layers of the ﬁne-tuned encoder\nby pre-trained layers, starting from the last layer.\nFor both experiments, we evaluate the perplexity of\nthe resulting model using the pre-trained masked\nlanguage modeling head. We ﬁne-tune and evaluate\neach model 5 times, and report the mean perplexity\nas well as standard deviation. Our reasoning is that\nif ﬁne-tuning leads to dramatic changes to the hid-\nden representations of a model, the effects should\nbe reﬂected in the perplexity.\nPerplexity During Fine-tuning Figure 3a and\n3b show how the perplexity of a pre-trained model\nchanges during ﬁne-tuning. Both BERT and\nRoBERTa show a similar trend where perplex-\nity increases with ﬁne-tuning. Interestingly, for\nRoBERTa the increase in perplexity after the ﬁrst\nepoch is much larger compared to BERT. Addi-\ntionally, our results show that for both models the\nincrease in perplexity is larger when ﬁne-tuning\non SST-2. This conﬁrms our hypothesis and also\nour ﬁndings from Section 4 suggesting that ﬁne-\ntuning on SST-2 has indeed more dramatic effects\nhow perplexity changes with ﬁne-tuning.\non the representations of both models compared to\nﬁne-tuning on CoLA.\nPerplexity When Replacing Fine-tuned Layers\nWhile ﬁne-tuning leads to worse language model-\ning abilities for both CoLA and SST-2, it is not\nclear from the ﬁrst experiment alone which layers\nare responsible for the increase in perplexity. Fig-\nure 3c and 3d show the perplexity results when\nreplacing ﬁne-tuned layers with pre-trained ones\nstarting from the last hidden layer. Consistent with\nour probing results in Section 4, we ﬁnd that the\nchanges that lead to an increase in perplexity\nhappen in the last layers , and this trend is the\nsame for both BERT and RoBERTa. Interestingly,\nwe observe no difference between CoLA and SST-2\nﬁne-tuning in this experiment.\n5.3 Discussion\nIn the following, we discuss the main implications\nof our experiments and analysis.\n1. We conclude that ﬁne-tuning indeed does af-\nfect the representations of a pre-trained model\nand in particular those of the last hidden lay-\ners, which is supported by our perplexity anal-\n76\nysis. However, our perplexity analysis does\nnot reveal whether these changes have a posi-\ntive or negative effect on the encoding of lin-\nguistic knowledge.\n2. Some ﬁne-tuning/probing task combinations\nresult in substantial improvements in probing\naccuracy when using CLS-pooling. Our atten-\ntion analysis supports our interpretation that\nthe improvement in probing accuracy can not\nsimply be attributed to the encoding of lin-\nguistic knowledge, but can at least partially be\nexplained by changes in the attention distribu-\ntion for the CLS token. We note that this is\nalso consistent with our ﬁndings that the im-\nprovement in probing accuracy vanishes when\ncomparing to the mean-pooling baseline.\n3. Some other task combinations have a nega-\ntive effect on the probing task performance,\nsuggesting that the linguistic knowledge our\nprobing classiﬁers are testing for is indeed\nno longer (linearly) accessible. However, it\nremains unclear whether ﬁne-tuning indeed\nremoves the linguistic knowledge our probing\nclassiﬁers are testing for from the representa-\ntions or whether it is simply no longer linearly\nseparable. We are planning to further investi-\ngate this in future work.\n6 Conclusion\nWe investigated the interplay between ﬁne-tuning\nand layer-wise sentence-level probing accuracy\nand found that ﬁne-tuning can lead to substan-\ntial changes in probing accuracy. However, these\nchanges vary greatly depending on the encoder\nmodel and ﬁne-tuning and probing task combina-\ntion. Our analysis of attention distributions after\nﬁne-tuning showed, that changes in probing accu-\nracy can not be attributed to the encoding of linguis-\ntic knowledge alone but might as well be caused\nby changes in the attention distribution. At the\nsame time, our perplexity analysis showed that ﬁne-\ntuning has profound effects on the representations\nof a pre-trained model but our probing analysis can\nnot sufﬁciently detail whether it leads to forgetting\nof the probed linguistic information. Hence we\nargue that the effects of ﬁne-tuning on pre-trained\nrepresentations should be carefully interpreted.\nAcknowledgments\nWe thank Badr Abdullah for his comments and sug-\ngestions. We would also like to thank the reviewers\nfor their useful comments and feedback, in partic-\nular R1. This work was funded by the Deutsche\nForschungsgemeinschaft (DFG, German Research\nFoundation) – project-id 232722074 – SFB 1102.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nand Danilo Giampiccolo. 2006. The second pascal\nrecognising textual entailment challenge. Proceed-\nings of the Second PASCAL Challenges Workshop\non Recognising Textual Entailment.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do neu-\nral machine translation models learn about morphol-\nogy? In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 861–872, Vancouver,\nCanada. Association for Computational Linguistics.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nﬁfth pascal recognizing textual entailment challenge.\nIn In Proc Text Analysis Conference (TAC’09).\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Proceedings of the First Inter-\nnational Conference on Machine Learning Chal-\nlenges: Evaluating Predictive Uncertainty Visual\nObject Classiﬁcation, and Recognizing Textual En-\ntailment, MLCW’05, page 177–190, Berlin, Heidel-\nberg. Springer-Verlag.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n77\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik.\n2016. Probing for semantic evidence of composition\nby means of simple classiﬁcation tasks. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space\nRepresentations for NLP , pages 134–139, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third pascal recogniz-\ning textual entailment challenge. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, RTE ’07, page 1–9, USA. Asso-\nciation for Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and’diagnostic classiﬁers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. Journal of Artiﬁcial\nIntelligence Research, 61:907–926.\nNajoung Kim, Roma Patel, Adam Poliak, Patrick Xia,\nAlex Wang, Tom McCoy, Ian Tenney, Alexis Ross,\nTal Linzen, Benjamin Van Durme, Samuel R. Bow-\nman, and Ellie Pavlick. 2019. Probing what dif-\nferent NLP tasks teach machines about function\nword comprehension. In Proceedings of the Eighth\nJoint Conference on Lexical and Computational Se-\nmantics (*SEM 2019), pages 235–249, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nKatarzyna Krasnowska-Kiera´s and Alina Wr ´oblewska.\n2019. Empirical linguistic study of sentence em-\nbeddings. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5729–5739, Florence, Italy. Association for\nComputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP , pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to bert\nembeddings during ﬁne-tuning? arXiv preprint\narXiv:2004.14448.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. ArXiv, abs/1609.07843.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8026–8037. Curran Asso-\nciates, Inc.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJason Phang, Thibault F ´evry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\n78\nYada Pruksachatkun, Jason Phang, Haokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\nPang, Clara Vania, Katharina Kann, and Samuel R\nBowman. 2020. Intermediate-task transfer learning\nwith pretrained models for natural language under-\nstanding: When and why does it work? arXiv\npreprint arXiv:2005.00628.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. arXiv preprint arXiv:2002.12327.\nY . Rubner, C. Tomasi, and L. J. Guibas. 1998. A\nmetric for distributions with applications to image\ndatabases. In Sixth International Conference on\nComputer Vision (IEEE Cat. No.98CH36271), pages\n59–66.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1526–\n1534, Austin, Texas. Association for Computational\nLinguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019. oLMpics–On what Lan-\nguage Model Pre-training Captures. arXiv preprint\narXiv:1912.13283.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pap-\npagari, R. Thomas McCoy, Roma Patel, Najoung\nKim, Ian Tenney, Yinghui Huang, Katherin Yu,\nShuning Jin, Berlin Chen, Benjamin Van Durme,\nEdouard Grave, Ellie Pavlick, and Samuel R. Bow-\nman. 2019a. Can you tell me how to get past sesame\nstreet? sentence-level pretraining beyond language\nmodeling. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4465–4476, Florence, Italy. Association for\nComputational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nA Appendices\nB Hyperparameters and Task Statistics\nTable 3 shows hyperparamters used when ﬁne-\ntuning BERT, RoBERTa, and ALBERT on CoLA,\nSST-2, RTE, and SQuAD. On SST-2 training for a\nsingle epoch was sufﬁcient and we didn’t observe\na signiﬁcant improvement when training for more\nepochs.\nTable 4 shows number of training and development\nsamples for each of the ﬁne-tuning datasets consid-\nered in our experiments. Additionally, we report\nthe metric used to evaluate performance for each\nof the tasks.\nC Additional Results\nTable 5 shows the effect of ﬁne-tuning on RTE and\nSQuAD on the layer-wise accuracy for all three\nencoder models across the three probing tasks.\nFigure 4 and Figure 5 show the change in prob-\ning accuracy ∆ (in %) across all probing tasks\n79\nHyperparameter Value\nLearning rate 2e−5\nWarmup steps 10%\nLearning rate schedule warmup-constant\nBatch size 32\nEpochs 3 (1 for SST-2)\nWeight decay 0.01\nDropout 0.1\nAttention dropout 0.1\nClassiﬁer dropout 0.1\nAdam ϵ 1e−8\nAdam β1 0.9\nAdam β2 0.99\nMax. gradient norm 1.0\nTable 3: Hyperparamters used when ﬁne-tuning.\nStatistics Task\nCoLA SST-2 RTE SQuAD\ntraining 8.6k 67k 2.5 87k\nvalidation 1,043 874 278 10k\nmetric MCC Acc. Acc. EM/ F1\nTable 4: Fine-tuning task statistics.\nwhen ﬁne-tuning on CoLA, SST-2, RTE, and\nSQuAD using CLS-pooling and mean-pooling, re-\nspectively. The second y-axis in Figure 4 shows the\nlayer-wise difference after ﬁne-tuning compared to\nthe mean-pooling baseline. Note that only in very\nfew cases this differences is larger than zero.x\n80\nProbing Task\nBERT-base-cased\nCLS-pooling mean-pooling\nRTE SQuAD RTE SQuAD\n0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12\nbigram-shift −0.21 −0.39 −0.05 −1.50 −0.07 −0.31 −0.54 −1.66\ncoordinate-inversion −0.43 −0.36 0 .04 0 .56 0 .05 0 .13 −0.03 0 .10\nodd-man-out 0.09 0 .38 −0.21 −1.89 0 .09 0 .01 −0.28 −1.73\nProbing Task\nRoBERTa-base\nCLS-pooling mean-pooling\nRTE SQuAD RTE SQuAD\n0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12\nbigram-shift −0.51 0 .44 −1.17 −4.33 −0.09 −1.32 −0.28 −3.09\ncoordinate-inversion −0.35 3 .27 0 .29 0 .50 0 .30 −0.48 0 .20 0 .05\nodd-man-out −0.11 1 .22 −0.76 −3.01 −0.04 −1.96 −0.21 −3.58\nProbing Task\nALBERT-base-v1\nCLS-pooling mean-pooling\nRTE SQuAD RTE SQuAD\n0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12 0 – 6 7 – 12\nbigram-shift 0.29 −0.43 −0.38 −3.46 −0.13 −0.82 −0.60 −3.11\ncoordinate-inversion 0.46 −0.44 0 .32 0 .92 0 .13 −0.38 0 .04 −0.27\nodd-man-out −0.03 0 .17 −0.65 −2.91 −0.17 −0.85 −0.55 −3.18\nTable 5: Change in probing accuracy∆ (in %) of RTE and SQuAD ﬁne-tuned models compared to the pre-trained\nmodels when using CLS and mean-pooling. We average the difference in probing accuracy over two different\nlayers groups: layers 0 to 6 and layers 7 to 12.\n81\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-2.00\n0.00\n2.00\n4.00\n6.00\n8.00\n10.00\n12.00Accuracy ∆%\nCoLA ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n(a) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-2.00\n0.00\n2.00\n4.00\n6.00\n8.00\n10.00\n12.00Accuracy ∆%\nCoLA ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n (b) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-2.00\n0.00\n2.00\n4.00\n6.00\n8.00\n10.00\n12.00Accuracy ∆%\nCoLA ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n (c) odd-man-out\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-12.00\n-10.00\n-8.00\n-6.00\n-4.00\n-2.00\n0.00\n2.00\n4.00\nAccuracy ∆%\nSST-2 ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n(d) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-12.00\n-10.00\n-8.00\n-6.00\n-4.00\n-2.00\n0.00\n2.00\n4.00\nAccuracy ∆%\nSST-2 ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n (e) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-12.00\n-10.00\n-8.00\n-6.00\n-4.00\n-2.00\n0.00\n2.00\n4.00\nAccuracy ∆%\nSST-2 ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n (f) odd-man-out\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-2.00\n0.00\n2.00\n4.00\n6.00Accuracy ∆%\nRTE ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n(g) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-2.00\n0.00\n2.00\n4.00\n6.00Accuracy ∆%\nRTE ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n (h) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-2.00\n0.00\n2.00\n4.00\n6.00Accuracy ∆%\nRTE ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n (i) odd-man-out\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-6.00\n-4.00\n-2.00\n0.00\n2.00Accuracy ∆%\nSQUAD v1.1 ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n(j) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-6.00\n-4.00\n-2.00\n0.00\n2.00Accuracy ∆%\nSQUAD v1.1 ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n (k) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-6.00\n-4.00\n-2.00\n0.00\n2.00Accuracy ∆%\nSQUAD v1.1 ﬁne-tuning, CLS pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n-25.00\n-20.00\n-15.00\n-10.00\n-5.00\n0.00\n5.00\nAccuracy ∆% (⋆) to mean pooling baseline\n (l) odd-man-out\nFigure 4: Difference in probing accuracy ∆ (in %) when using CLS-pooling after ﬁne-tuning on CoLA, SST-2,\nRTE, and SQuAD for all three encoder models BERT, RoBERTa, and ALBERT across all probing taks considered\nin this work. The second y-axis shows layer-wise improvement over the mean-pooling baselines (stars) on the\nrespective task.\n82\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-2.00\n-1.00\n0.00\n1.00\n2.00\n3.00\n4.00Accuracy ∆%\nCoLA ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n(a) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-2.00\n-1.00\n0.00\n1.00\n2.00\n3.00\n4.00Accuracy ∆%\nCoLA ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base (b) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-2.00\n-1.00\n0.00\n1.00\n2.00\n3.00\n4.00Accuracy ∆%\nCoLA ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base (c) odd-man-out\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-10.00\n-8.00\n-6.00\n-4.00\n-2.00\n0.00\n2.00\nAccuracy ∆%\nSST-2 ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n(d) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-10.00\n-8.00\n-6.00\n-4.00\n-2.00\n0.00\n2.00\nAccuracy ∆%\nSST-2 ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base (e) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-10.00\n-8.00\n-6.00\n-4.00\n-2.00\n0.00\n2.00\nAccuracy ∆%\nSST-2 ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base (f) odd-man-out\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-10.00\n-8.00\n-6.00\n-4.00\n-2.00\n0.00\n2.00\nAccuracy ∆%\nRTE ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n(g) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-3.00\n-2.00\n-1.00\n0.00\n1.00Accuracy ∆%\nRTE ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base (h) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-3.00\n-2.00\n-1.00\n0.00\n1.00Accuracy ∆%\nRTE ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base (i) odd-man-out\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-3.00\n-2.00\n-1.00\n0.00\n1.00\n2.00\n3.00\n4.00Accuracy ∆%\nSQUAD v1.1 ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base\n(j) bigram-shift\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-3.00\n-2.00\n-1.00\n0.00\n1.00\n2.00\n3.00\n4.00Accuracy ∆%\nSQUAD v1.1 ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base (k) coordination-inversion\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n-4.00\n-3.00\n-2.00\n-1.00\n0.00\n1.00\n2.00\n3.00\n4.00Accuracy ∆%\nSQUAD v1.1 ﬁne-tuning, mean pooling\nalbert-base-v1\nbert-base-cased\nroberta-base (l) odd-man-out\nFigure 5: Difference in probing accuracy ∆ (in %) when using mean-pooling after ﬁne-tuning on CoLA, SST-2,\nRTE, and SQuAD for all three encoder models BERT, RoBERTa, and ALBERT across all probing tasks considered\nin this work.",
  "topic": "Fine-tuning",
  "concepts": [
    {
      "name": "Fine-tuning",
      "score": 0.7485046982765198
    },
    {
      "name": "Computer science",
      "score": 0.701460063457489
    },
    {
      "name": "Transformer",
      "score": 0.6469494104385376
    },
    {
      "name": "Natural language processing",
      "score": 0.5776212215423584
    },
    {
      "name": "Sentence",
      "score": 0.5745488405227661
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5218487977981567
    },
    {
      "name": "Embedding",
      "score": 0.4842800498008728
    },
    {
      "name": "Pooling",
      "score": 0.4779898524284363
    },
    {
      "name": "Pipeline (software)",
      "score": 0.4269925355911255
    },
    {
      "name": "Linguistics",
      "score": 0.3726102113723755
    },
    {
      "name": "Physics",
      "score": 0.1026453971862793
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91712215",
      "name": "Saarland University",
      "country": "DE"
    }
  ]
}