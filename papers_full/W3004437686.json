{
  "title": "Blank Language Models",
  "url": "https://openalex.org/W3004437686",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4228007819",
      "name": "Shen, Tianxiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225740201",
      "name": "Quach, Victor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222974628",
      "name": "Barzilay, Regina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2514491119",
      "name": "Jaakkola, Tommi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2888693386",
    "https://openalex.org/W2964529779",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2289899728",
    "https://openalex.org/W2948025442",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2937808806",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2619010992",
    "https://openalex.org/W2963034998",
    "https://openalex.org/W2259512711",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2935811960",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2888173624",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2801454967",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2805041018",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035233162",
    "https://openalex.org/W2784823820",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2971167892",
    "https://openalex.org/W2020382207",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2981461746",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2948629866",
    "https://openalex.org/W2985694911",
    "https://openalex.org/W2963600562",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W3104802318",
    "https://openalex.org/W2970690146",
    "https://openalex.org/W2970619458",
    "https://openalex.org/W2996416339",
    "https://openalex.org/W2792764867"
  ],
  "abstract": "We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5186–5198,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n5186\nBlank Language Models\nTianxiao Shen∗ Victor Quach∗ Regina Barzilay Tommi Jaakkola\nMIT CSAIL\n{tianxiao, quach, regina, tommi}@csail.mit.edu\nAbstract\nWe propose Blank Language Model (BLM),\na model that generates sequences by dynami-\ncally creating and ﬁlling in blanks. The blanks\ncontrol which part of the sequence to expand,\nmaking BLM ideal for a variety of text editing\nand rewriting tasks. The model can start from\na single blank or partially completed text with\nblanks at speciﬁed locations. It iteratively de-\ntermines which word to place in a blank and\nwhether to insert new blanks, and stops gen-\nerating when no blanks are left to ﬁll. BLM\ncan be efﬁciently trained using a lower bound\nof the marginal data likelihood. On the task\nof ﬁlling missing text snippets, BLM signiﬁ-\ncantly outperforms all other baselines in terms\nof both accuracy and ﬂuency. Experiments on\nstyle transfer and damaged ancient text restora-\ntion demonstrate the potential of this frame-\nwork for a wide range of applications.1\n1 Introduction\nNeural language models have shown impressive\nperformance across many applications such as ma-\nchine translation and summarization where the text\nis generated from scratch (Bahdanau et al., 2014;\nRush et al., 2015). However, a broader set of text\ngeneration tasks — including text editing, informa-\ntion fusion, and ancient text restoration — requires\nthe model to start with partially speciﬁed text and\ngenerate the missing fragments. In the general\nsetup, the input document may have any number\nof missing spans, and each span may have an un-\nknown number of missing tokens. To perform this\ntext inﬁlling task (Zhu et al., 2019), a model should:\n(1) provide ﬁne-grained control over the generation\nlocation, (2) accommodate a variable number of\nmissing tokens, and (3) respect both the preceding\nand following context.\n∗Equal contribution\n1Our code is available at https://github.com/\nVaral7/blank_language_model\nThey also have which .\nThey also have ice cream which is really good .\nFigure 1: BLM ﬁlls in blanks of arbitrary length.\nExisting approaches focus on adapting left-to-\nright language models for text inﬁlling. Intricate\ninference algorithms leveraging dynamic program-\nming or gradient search are proposed to ﬁnd the\nﬁlling content that has a high likelihood within the\nsurrounding context (Sun et al., 2017; Liu et al.,\n2019a; Zaidi et al., 2020). These methods make\nsimpliﬁed Markov assumptions, require high de-\ncoding time complexity, and cannot adapt to vari-\nable inﬁlling length. Alternatively, Donahue et al.\n(2020) predict the concatenation of the inﬁlling\ncontent, but do not guarantee that the output will\nmatch the number of missing spans in the input.\nIn this work, we introduce the Blank Language\nModel (BLM), which uses a special “ ” symbol\nto control where tokens can be placed. The gener-\nation of BLM follows the grammar of replacing a\nblank with a word and possibly adjoining blanks.\nBy jointly modeling context and missing content,\nBLM supports the control of generation location\nand produces consistent inﬁlling of variable length.\nOur model can start from a single blank or par-\ntial text with blanks in speciﬁed locations. It maps\nthe entire input into a sequence of vector represen-\ntations, and further processes the representations in\nblank positions to determine the generation action.\nGeneration actions are performed iteratively until\nthere are no blanks. Since multiple trajectories of\nBLM actions can produce the same ﬁnal text, we\ntrain the model by maximizing a lower bound of\nthe log-likelihood marginalized over trajectories.\nAt test time, we can use simple greedy decoding or\nbeam search to ﬁll in the blanks in the input text.\nBLM shows superior performance in text inﬁll-\ning (Zhu et al., 2019), ancient text restoration (As-\n5187\nCanvas c Action a\nStep t Location b Word w (Left l, Right r)\n0. #1 #1 is Y Y\n1. #1 is #2 #1 customer N Y\n2. customer #1 is #2 #2 awesome N N\n3. customer #1 is awesome #1 service N N\n4. customer service is awesome -End-\nFigure 2: An example trajectory that generates the sentence “customer service is awesome”. Each action is a tuple\n(b,w,l,r ), indicating the blank location b selected for expansion, the word w to ﬁll in, whether to create a left\nblank l, and whether to create a right blank r.\nsael et al., 2019) and style transfer (Shen et al.,\n2017), demonstrating its ﬂexibility to generate text\nin diverse conditions. Our model achieves 92.5%\naccuracy and BLEU score of 23.1 on the Amazon\ndataset for sentiment transfer. On the task of restor-\ning ancient text that lost half of the characters, we\nreduce the error rate by 3.3 points compared to\nprevious methods.\n2 Related Work\nRecent work has explored various sequence mod-\nels for non-autoregressive machine translation (Gu\net al., 2017). The Insertion Transformer supports\ndynamic canvas with word insertion (Stern et al.,\n2019), but does not allow users to specify where to\ninsert. The model is unaware of which parts of the\ncanvas are contiguous text spans that should remain\nintact, and which (potentially scattered) parts need\nto be ﬁlled in. Directly forcing the Insertion Trans-\nformer to perform text inﬁlling can therefore lead\nto suboptimal solutions. The Levenshtein Trans-\nformer combines insertion and deletion through\ncomplex policy learning (Gu et al., 2019b). Its in-\nsertion mechanism is a two-stage process in which\nplaceholders are ﬁrst predicted and then ﬁlled-in in\na masked language model (MLM) manner. In text\ninﬁlling where the blanks/placeholders are given,\nit reduces to an MLM.\nMLMs are commonly used in representation\nlearning (Devlin et al., 2018; Joshi et al., 2020). To\nuse them in rewriting tasks, one needs to specify the\ninsertion length in advance and heuristically deter-\nmine the generation order among the masks (Fedus\net al., 2018; Wang and Cho, 2019; Ghazvininejad\net al., 2019). Similarly, XL-Net requires absolute\npositional embedding and thus does not support\nunknown-length text inﬁlling (Yang et al., 2019;\nShih et al., 2019). BLM provides a natural formula-\ntion for generative modeling that can dynamically\naccommodate insertions of various length.\nAnother line of work focuses on ﬁnding an op-\ntimal language generation order, such as syntax-\nbased generation (Dyer et al., 2016) and learning\nadaptive generation order (Gu et al., 2019a). These\napproaches are tailored to generation from scratch\nin a speciﬁc order. Our model instead is attuned for\ntext rewriting, where the missing parts can be lo-\ncated anywhere in the input text, and the algorithm\nmust ﬂexibly complete them.\n3 Blank Language Models\nA blank language model (BLM) generates se-\nquences by creating and ﬁlling in blanks. Gen-\neration starts with a single blank and ends when\nthere is no blank. In each step, the model selects a\nblank “ ”, predicts a word w, and ﬁlls the blank\nwith “w”, “ w”, “w ”, or “ w ”. This way,\na blank can be expanded to any number of words.\nWe deﬁne a canvas as a sequence of words in-\nterspersed with special “ ” tokens. The subse-\nquent action is conditioned on this intermediate\nstage of generation. Suppose the current canvas is\nc = (c1,··· ,cn) with blanks located at indices\nb1,··· ,bk (i.e. cbi = “ ”, for i = 1,...,k ).\nBLM maps this canvas to a distribution over ac-\ntions specifying how the canvas is to be revised:\np(b,w,l,r |c; θ) =BLM(c) (1)\nwhere b ∈{b1,··· ,bk}is a blank location; w is\na word in the vocabulary V; l,r ∈{0,1}denote\nwhether or not to create a blank to the left and right\nof w; and θare the model parameters. The action,\ndeﬁned as the tuple (b,w,l,r ) uniquely speciﬁes\nthe next state of canvas (see Fig. 2 for illustration).\nAlternatively, we can view the actions in BLM\nas production rules in a grammar. Each blank rep-\nresents a nonterminal symbol or the start symbol,\n5188\nalsoThey have ____ which ____\nTransformer\n.\nLinear & Softmax1) Choose a blank 2) Predict a word\n3) Create new blanks\nLinear & \nSoftmax\nreally\nMLP\nFill and repeat\nreally\nreally\nreally____\nreally ____\n____\n____\n✓\nFigure 3: Architecture of the BLM. In the ﬁrst stage, an index is chosen among all current blank positions. For that\nlocation, a word is selected in the second stage. In the ﬁnal stage, the blank representation is concatenated with the\nchosen word’s embedding and fed into an MLP to determine the creation of the following blanks.\nand the terminal symbols come from the vocabu-\nlary V. The production rules are restricted to be\nof the form “ ” →“ ?w ?” for w∈V, where\n“?” indicates that the preceding symbol is optional.\nIn contrast to context-free grammars, the probabil-\nity distribution over production rules in BLM is\nconditioned on the entire canvas generated so far.\nModel Architecture We encode the canvas c\ninto a sequence of representations (z1,··· ,zn),\nand take representations Z = (zb1 ,··· ,zbk ) where\nthe blanks are located. Let d denote the dimen-\nsion of z’s. We factorize the joint distribution\np(b,w,l,r |c; θ) into three parts (shown in Fig. 3):\n1. Choose a blank:\np(bi|c; θ) =Softmax(uTZ) (2)\nwhere u∈Rd is a parameter vector to project\nz’s into one-dimensional logits.\n2. Predict a word for the selected blank:\np(w|c,bi; θ) =Softmax(Wzbi ) (3)\nwhere W ∈R|V|×d is a parameter matrix to\nproject zbi into the vocabulary.\n3. Decide whether or not to create blanks to the\nleft and right of the predicted word:\np(l,r|c,bi,w; θ) =MLP(zbi ,vw) (4)\nwhere vw is the word vector of w, and MLP is\na multilayer perceptron with 4 output classes:\nLeft.Yes/No ×Right.Yes/No.\nLikelihood Now let us consider the probability\np(x; θ) of generating a sentence/paragraph x =\n(x1,··· ,xn) under the BLM. We call the generat-\ning process from an initial blank to complete text\na trajectory. The same ﬁnal text xcan be realized\nby multiple trajectories. However, if we specify\nthe order in which the words in xare generated,\nthe trajectory will be uniquely determined. Con-\nsider the example trajectory of a 4-word sentence\nin Fig. 2. Given the order (3,1,4,2), at step 0\nwhen we generate x3, both left and right blanks\nare created for future generations of x1 and x2,x4.\nIn step 1 of generating x1, only a right blank is\ncreated for the future x2. Subsequent steps can be\ndeduced by analogy. The correspondence between\ntrajectories and generation orders allows us to write\nthe marginal likelihood as:\np(x; θ) =\n∑\nσ∈Sn\np(x,σ; θ)\n=\n∑\nσ∈Sn\nn−1∏\nt=0\np(ax,σ\nt |cx,σ\nt ; θ) (5)\nwhere Sn is the set of alln-permutations; ax,σ\nt ,cx,σ\nt\ndenote the action and canvas at step tunder sen-\ntence xand order σ, respectively (cf. Fig. 2).\nTraining Different losses have been proposed to\ntrain generalized sequence models. For instance,\nBERT and XL-Net mask and predict 15% of tokens\nconditioned on the rest. This strategy is more suit-\nable for representation learning rather than genera-\ntion. Insertion Transformer masks different num-\nbers of tokens and weights them with uniform loss\nor binary tree loss (Stern et al., 2019; Chan et al.,\n5189\nAlgorithm 1 BLM training2\n1: Initialize model parameters θ\n2: repeat\n3: Sample a training example x= (x1,··· ,xn)\n4: Sample tfrom 0 to n−1\n5: Sample an n-permutation σ\n6: Construct canvas c that keeps tokens xσj (j =\n1,··· ,t) and collapses remaining tokens as blanks\n7: Get n−ttarget actions aj−t for ﬁlling xσj (j =\nt+ 1,··· ,n) into canvas c\n8: Compute loss ({a1,··· ,an−t},model.forward(c))\nfrom Eq. (8)\n9: Update θby gradient descent\n10: until Convergence\n2019). It aims to perform fast inference through\nparallel decoding. Here, we present a training ob-\njective from the language modeling perspective by\nestimating the log likelihood of generating x.\nDirectly computing the marginal likelihood over\nn! orders is intractable. We apply Jensen’s inequal-\nity to lower bound the log likelihood:\nlog p(x; θ) = log\n∑\nσ∈Sn\nn−1∏\nt=0\np(ax,σ\nt |cx,σ\nt ; θ)\n≥log(n!) + 1\nn!\n∑\nσ∈Sn\nn−1∑\nt=0\nlog p(ax,σ\nt |cx,σ\nt ; θ) (6)\nwhere equality holds when the posterior p(σ|x; θ)\nis uniform. By maximizing this lower bound, we\ndo not favor any particular order, but encourage the\nmodel to realize xequally well in all orders. It can\nhelp the model to complete any partial input text\nregardless of the position of blanks.\nA naive training algorithm is to directly estimate\nthe lower bound in Eq. (6): ﬁrst uniformly sample a\npermutation σfrom Sn and a step tfrom 0 to n−1,\nthen construct the canvas cx,σ\nt , and compute the\nestimated loss [−log(n!) −n·log p(ax,σ\nt |cx,σ\nt ; θ)].\nHowever, this procedure has a large variance and\ncan only compute the loss of a single action in one\npass (in contrast to left-to-right language models\nthat compute nword losses per pass).\nTo train the model more efﬁciently, we note that\nthe canvas cx,σ\nt depends only on the ﬁrst telements\nof σ. Hence we can combine into one pass the loss\ncalculations of trajectories that are the same in the\nﬁrst tsteps but different at thet+1 step. Switching\n2We implement a batch version of the algorithm.\nThey also have which .\nThey also have ice cream which is really good .\nτε εγγονον εισαι? ? ? ? ? ? ?σοϕιαι\nτε εγγονον εισαιου του σοϕιαι\nThe employees were super nice and efﬁcient !\nThe employees were rude and unprofessional !\nFigure 4: Examples of input and output for text inﬁll-\ning, ancient text restoration, and style transfer tasks.\nthe summation order of σand t, we have:\nn−1∑\nt=0\n1\nn!\n∑\nσ∈Sn\nlog p(ax,σ\nt |cx,σ\nt ; θ)\n= n·EtEσ1:t Eσt+1 Eσt+2:n [log p(ax,σ\nt |cx,σ\nt ; θ)]\n= n·EtEσ1:t Eσt+1 [log p(ax,σ\nt |cx,σ\nt ; θ)]\n= EtEσ1:t\n\n n\nn−t\n∑\nσt+1\nlog p(ax,σ\nt |cx,σ\nt ; θ)\n\n (7)\nwhich leads to our efﬁcient training algorithm: sam-\nple tfrom 0 to n−1 and partial permutation σ1:t,\nconstruct the canvas cx,σ\nt , and compute loss:\n−log(n!) − n\nn−t\n∑\nσt+1\nlog p(ax,σ\nt |cx,σ\nt ; θ) (8)\nThe whole process is illustrated in Algorithm 1.\nIn this way, we can compute in expectation n/2\naction losses per pass.\n4 Experiments\nWe test BLM’s capacity to rewrite speciﬁed por-\ntions of text on three tasks: text inﬁlling (Zhu et al.,\n2019), ancient text restoration (Assael et al., 2019)\nand style transfer (Shen et al., 2017). Fig. 4 dis-\nplays example inputs and outputs for these tasks.\nWe also measure the perplexity of BLM on lan-\nguage modeling benchmarks and compare with\ntraditional left-to-right language models.\nExperimental Details In all experiments, the se-\nquence representations in BLM are obtained us-\ning the encoder module of transformer base\n(Vaswani et al., 2017) (6 layers, 8 heads, dmodel =\n512, dff = 2048, dk = dv = 64). The MLP used\nfor blank prediction has one hidden layer of size\n1024. Weight decay, learning rate, and dropout are\ntuned based on the loss on the validation set for\neach dataset respectively. When decoding, we use\nbeam size in {1,5,10}and choose the best value as\n5190\nBLEU PPL\nMask ratio 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%\nNo inﬁll 75.2 55.0 37.4 23.6 13.0 98.4 163.0 266.3 421.0 647.9\nInsT 84.8 72.3 58.9 46.0 33.8 48.3 44.2 41.8 39.7 37.7\nMLM (oracle length) 83.7 69.3 55.5 43.2 32.2 58.4 59.8 59.8 59.0 56.8\nBERT+LM 82.8 66.3 50.3 37.4 26.2 55.1 55.2 54.9 56.5 53.6\nSeq2seq-full 86.3 72.9 59.4 46.3 34.0 51.3 46.9 41.0 31.9 20.6\nSeq2seq-ﬁll 82.8 67.5 52.9 39.9 28.6 64.6 71.0 73.4 65.6 48.7\nBLM 86.5 73.2 59.6 46.8 34.8 50.2 44.9 39.9 35.0 32.7\nTable 1: BLEU scores and perplexity of generated documents by different models for text inﬁlling. The perplexity\nis measured by a pre-trained left-to-right language model, and the original documents have perplexity 55.8.\nMask ratio 10% 20% 30% 40% 50%\nSeq2seq-full 15.0 22.4 28.7 33.3 40.6\nSeq2seq-ﬁll 31.0 28.4 34.5 42.5 47.2\nTable 2: Inﬁlling failure rate (%) of seq2seq models.\nOther methods always produce valid outputs.\nobserved on the validation set. We note that beam\nsearch in BLM does not search for the sentence\nwith the maximum marginal likelihood p(x; θ), but\ninstead for a sentence and a trajectory that have the\nmaximum joint likelihood p(x,σ; θ).\n4.1 Text Inﬁlling\nDataset We experiment on the Yahoo Answers\ndataset, which has 100K/10K/10K documents for\ntrain/valid/test respectively (Yang et al., 2017). A\ndocument has a maximum length of 200 words,\nwith an average of 78 words. Following Zhu et al.\n(2019), we automatically compile test data by delet-\ning portions of documents. For each document x,\nwe randomly mask a given ratio r of its tokens.\nContiguous masked tokens are collapsed into a sin-\ngle “ ”, resulting in a canvas cto be completed.\nMetrics We measure generation’s accuracy by\ncomputing its BLEU score against the original doc-\nument x, and ﬂuency as its perplexity evaluated by\na pre-trained (left-to-right) language model. We\nalso report the failure rate, which is the percent-\nage of invalid generations, such as missing existing\nwords or not ﬁlling in all the blanks.\nBaselines We compare BLM with ﬁve baselines:\n•Insertion Transformer (InsT): By default, InsT\ndoes not support controlling the insertion posi-\ntion. We force it to produce valid generations\nby normalizing the predictions over valid loca-\ntions, disabling the ⟨eos⟩prediction unless all\nblanks have been ﬁlled, and prioritizing slots\nthat have not been ﬁlled yet. Without these\nsteps, InsT would have a failure rate ≥88%.\n•MLM (oracle length): MLM for text inﬁlling\nrequires predicting the length of each blank.\nHere we replace blanks with the target num-\nber of ⟨mask⟩tokens, and ﬁll them autoregres-\nsively by the most-conﬁdent-ﬁrst heuristic.\n•BERT+LM: We use BERT’s representation of\neach blank as seed for a left-to-right language\nmodel that learns to generate the tokens in the\ncorresponding blank. At inference time, the\nmultiple blanks are ﬁlled in one after another,\nconditioned on previous generations.\n•Seq2seq-full (Donahue et al., 2020): We train\na seq2seq model to output the full documentx\nfrom input c. Note that it may have invalid out-\nputs that do not match the input format, such\nas missing existing tokens in cor generating\ntokens in incorrect locations.\n•Seq2seq-ﬁll (Donahue et al., 2020): We train\na seq2seq model to output only tokens to be\nplaced in the blanks, with a special ‘|’ token to\nindicate separation. For the example in Fig. 4,\nits target output will be “ice cream |is really\ngood”. Unlike seq2seq-full, seq2seq-ﬁll does\nnot have the problem of losing existing tokens\nin c. However, it may still fail to generate the\ncorrect number of ‘|’ that matches the input.\nResults As shown in Table 1, BLM achieves the\nhighest BLEU score at all mask ratios: 0.7 to 1.7\nhigher than InsT, 2.6 to 4.1 higher than MLM with\noracle length, and 3.7 to 9.4 higher than BERT+LM.\nInsT is not trained with insertion position control.\nRestricting it to generate at the speciﬁed positions\nthus bias the model towards making suboptimal\n5191\nMask-ratio 10% Mask-ratio 50%\nBlanked when time ﬂies , does it go ? the center of the\nto be recycled made into new time .\nwhen time , where ? the of universe\nto recycled made into .\nBLM when time ﬂies , where does it go ? for the center of the\nearth to be recycled and made into new time .\nwhen time was created , where did it come from ? it was the\nﬁrst part of the universe to be recycled and made into space .\nInsT when time ﬂies , where does it go ? for the center of the\nearth has to be recycled and made into new time .\nwhen time was created , where was it ? what was the name of\nthe universe to be recycled and made into space .\nMLM\n(oracle len)\nwhen time ﬂies , where does it go ? from the center of\nthe earth to be recycled converted made into new time .\nwhen time is , where is the universe ? from the creation of the\nuniverse to be recycled and made into the universe .\nBERT+LM when time ﬂies , where does it go ? to the center of the\nearth to be recycled came made into new time .\nwhen time is , where to ? i need to ﬁnd the way of the universe\nto be recycled and made into a lot .\nSeq2seq-\nfull\nwhen time ﬂies , where does it go ? at the center of the\nearth to be recycled and made into new time .\nwhen time heals , where does it go ? it ’s the end of the uni-\nverse to be recycled and made into space .\nSeq2seq-\nﬁll\nwhen time ﬂies , how does it go ? at the center of the\nearth to be recycled and made into new time .\nwhen time is time , where is time ? time is the time of time\nuniverse to the recycled be made into and . the universe\nhow |at |earth |and is time |is time |time is |time |time |the |be |and |the universe\nOriginal when time ﬂies , where does it go ? to the center of the\nuniverse to be recycled and made into new time .\nwhen time ﬂies , where does it go ? to the center of the uni-\nverse to be recycled and made into new time .\nFigure 5: Example generations of different models for text inﬁlling on Yahoo Answers. Completions are in italic.\nInvalid completions are in red. For Seq2seq-ﬁll, we present model outputs along with the merged document.\ncompletions. MLM is trained to independently\npredict masked tokens instead of jointly model-\ning them. Even with the target number of ⟨mask⟩\ntokens given, its performance is still inferior to\nBLM. BERT+LM lags behind other models. In\nBERT training, one mask corresponds to one token,\nwhereas a blank here can cover multiple tokens, and\nthe distance between words is not ﬁxed. Hence, it\nis difﬁcult for the LM to complete the sentence\nfrom BERT representations.\nSeq2seq-full has BLEU scores closest to BLM.\nHowever, its failure rate ranges from15% to 40.6%\nas the mask ratio increases. Seq2seq-ﬁll performs\nworse than Seq2seq-full, possibly because the de-\ncoder has to model segmented text while counting\nthe number of blanks.\nIn terms of ﬂuency, outputs of BLM, InsT and\nSeq2seq-full all have perplexity lower than original\ndata perplexity. This is because with beam search,\nmodels tend to generate the most typical output\nwith the highest likelihood (Holtzman et al., 2019).\nExamination of model generations conﬁrms the\nsuperiority of BLM. In Fig. 5, we showcase exam-\nple outputs by each model at different mask ratios.\nIn low mask ratio settings, models only need to\nﬁll in the blanks with a single word to produce\ngrammatical completions. Most models succeed\nin this task. With a higher mask ratio of 50%, the\nmain ideas of the document are concealed, and the\ninﬁlling task is much more challenging. Models\nneed to creatively generate sentences that ﬁt the\nimposed canvas. Although the original meaning\nof the sentence is not recovered, BLM is the only\nmodel able to produce a coherent document with\nconsistency between the question and the answer.\nOverall, BLM displays the best performance\nboth quantitatively and qualitatively. Its inherent\ntext inﬁlling ability frees it from length, order, or\ntermination heuristics used by other methods.\n4.2 Ancient Text Restoration\nAncient text restoration is a form of text inﬁlling\nwhere there are fragments in ancient documents\nthat are illegible due to time-related damages and\nneed to be recovered. Assael et al. (2019) intro-\nduces the PHI-ML dataset made of fragments of an-\ncient Greek inscriptions. Restoration is performed\nat the character-level. The number of characters\nto recover is assumed to be known and indicated\nby a corresponding number of ‘?’ symbols, as\nshown in the second row of Fig. 4. In reality, when\nepigraphists restore a deteriorated document, the\nlength of the lost fragment is unknown and needs to\nbe guessed as a ﬁrst step. While models proposed\nby Assael et al. (2019) relies on expert conjectures,\nwe note that BLM can bypass this limitation and\nﬂexibly generate completions without this addi-\ntional knowledge. However, in order to compute\nthe character error rate (CER) for each ’?’ and have\na fair comparison with previous work, we evaluate\nour model in the length-aware setting.\n5192\nSingle- Multi-slot\nMask ratio 1% 25% 40% 50%\nHuman 57.3% - - -\nPythia 32.5% - - -\nPythia-Word 29.1% 36.9% 42.3% 44.9%\nL-BLM 33.7% 37.1% 37.9% 41.6%\nTable 3: CER for ancient text restoration.\nLength-aware BLM (L-BLM) We present a\nvariant of BLM adapted to the speciﬁc features\nof this task. The vocabulary V is an alphabet of\ncharacters from the ancient Greek language. We ex-\ntend V with special “ [t] ” tokens that denote\nthe length of the fragment to recover. Speciﬁcally,\nas a preprocessing step, consecutive ‘?’ characters\nare collapsed into a single “ [t] ” token, where\ntis the number of ‘?’ symbols. For each such blank\ntoken, L-BLM is trained to predict a character to\nﬁll in and the length l∈{0,··· ,t −1}of the new\nblank to its left. The length of the new blank on the\nright is accordingly t−1 −l.\nDataset The PHI-ML dataset contains about 3\nmillion words / 18 million characters. We evaluate\nmodels in two settings: single-slot and multi-slot.\nFor the single-slot setting, we use the testing script\nof Assael et al. (2019) which samples a context of\nlength L= 1000from an inscription, then samples\na slot of length C ∈[1,10] from that context. The\ncharacters from the slot are replaced with ‘?’ and\nconstitute the target. For the multi-slot setting, we\nprogressively increase the number of slots, yielding\nmask ratios of 25%, 40% and 50% respectively.\nBaselines Assael et al. (2019) proposed two mod-\nels: Pythia, a character-level seq2seq-based ap-\nproach; and Pythia-Word, a variant of Pythia that\nuses both character and word representations as\ninput. During training, the model learns to recover\nthe missing characters of examples where a random\nslot has been masked. When testing on the multi-\nslot setting, Pythia(-Word) is applied iteratively\nwith beam size 20 for each slot.\nResults Table 3 summarizes the CER of all mod-\nels in both settings. L-BLM achieves similar CER\nas Pythia in the single-slot setting, signiﬁcantly out-\nperforming human experts. Augmented with word\nrepresentations, Pythia-Word further decreases the\nerror rate compared to character-only methods.\nIn reality, restoring damaged inscriptions re-\nquires reconstructing multiple lost fragments. As a\nlarger proportion of text is missing, Pythia-Word’s\nperformance is degraded. L-BLM is robust to the\nsetting change and outperforms Pythia-Word at the\nmask ratio of 40% and 50% by 4.4 and 3.3 points,\nrespectively. We posit that L-BLM’s advantage lies\nin its ability to maximize the joint likelihood of\nthe completions over all slots. In contrast, Pythia-\nWord’s is only aware of one slot at a time, and\nbeam search is performed locally within each slot.\n4.3 Sentiment Transfer\nThe goal of sentiment transfer is to modify the\nsentiment of a sentence while maintaining its topic\n(Shen et al., 2017). An example is described on the\nthird row of Fig. 4. Inspired by the way humans\nperform rewriting, we follow a recent line of work\nin style transfer that adopts a two-step approach (Li\net al., 2018; Xu et al., 2018; Wu et al., 2019b):\n1. Remove words and expressions of high polar-\nity from the source sentence;\n2. Complete the partial sentence with words and\nexpressions of the target sentiment.\nSpeciﬁcally, we adapt theMask-And-Inﬁll (M&I)\nframework of Wu et al. (2019b). We perform Step 1\nby training a Bi-LSTM sentiment classiﬁer and\nmasking words whose attention weight is above\naverage. We evaluate the contribution of our model\nas an inﬁlling module in Step 2 in place of their\nﬁne-tuned BERT model. To this end, we train two\ninstances of BLM on the dataset, one for each senti-\nment. At test time, the corresponding BLM is used\nto produce completions of the target sentiment.\nWu et al. (2019b) further train the inﬁlling model\nwith the classiﬁer to improve transfer accuracy.\nThey use soft words relaxation to backprop gradi-\nents from the classiﬁer to the generator. For BLM,\nhowever, we cannot pick locations or insert blanks\nas “soft” choices, making it challenging to employ\na classiﬁer at training time. Nevertheless, we can\neasily apply the classiﬁer to guide inference. We\nsample 10 outputs and keep the one with the high-\nest classiﬁer ranking. It is not slower than beam\nsearch with size 10 and can be fully parallelized.\nDatasets We test on the Yelp and Amazon re-\nview datasets (Shen et al., 2017; Li et al., 2018).\nThe Yelp dataset has 450K/4K/1K non-parallel sen-\ntences for train/valid/test respectively, and the Ama-\nzon dataset has 555K/2K/1K sentences. Each sen-\ntence is labeled as either positive or negative.\n5193\nYelp Amazon\nACC BLEU ACC BLEU\nLi et al. (2018) 88.7 8.4 48.0 22.8\nZhang et al. (2018) 96.6 22.8 84.1 33.9\nWu et al. (2019a) 91.5 29.9 40.2 41.9\nM&I with MLM 41.5 15.9 31.2 32.1\n+ classiﬁer 97.3 14.1 75.9 28.5\nM&I with BLM 79.6 21.9 52.0 24.7\n+ classiﬁer 96.5 21.5 92.5 23.1\nTable 4: Accuracy and BLEU scores for style transfer.\neveryone that i spoke with was very helpful and kind .\neveryone that i spoke with was rude and unprofessional .\neveryone that i spoke with wasn’t helpful or kind.\nthe beans were in the burro in the rice was nowhere to be found .\nthe beans were in the burro in the rice was the best i found .\nthe beans were in the burro and the rice was plentiful\nthere is deﬁnitely not enough room in that part of the venue .\nthere is always enough parking in that part of the venue .\nthere is so much room in that part of the venue\nit is n’tterrible , but it is n’tvery good either .\nit is n’tfancy , but it is still very good either .\nit is n’t perfect , but it is very good .\nFigure 6: Example generations by BLM for sentiment\ntransfer on Yelp. The ﬁrst line is the source sentence\nwith masked words in bold. The second line is BLM’s\ncompletion. The third line is a human reference.\nMetrics We use evaluation methods introduced\nby prior work (Shen et al., 2017; Li et al., 2018).\nTo assess the accuracy of generated sentences with\nrespect to the target sentiment, we use a pretrained\nCNN classiﬁer that achieves 97.7% accuracy on the\nYelp dataset and 82.2% accuracy on the Amazon\ndataset. We also measure the BLEU score between\ntransferred sentences and human references.\nResults In Table 4, we can see that directly apply-\ning BLM as the inﬁlling module is signiﬁcantly bet-\nter than MLM. The accuracy on Yelp and Amazon\ndatasets is increased by 38.1% and 20.8%, respec-\ntively. In addition to the aforementioned problem\nof MLM being trained to predict masked tokens\nindependently, it must generate the same number\nof tokens as in the source sentence, whereas our\nBLM formulation is not subject to this constraint.\nOur simple use of a classiﬁer at inference time\nfurther improves accuracy. It achieves the highest\naccuracy of 92.5% on Amazon with a small de-\ncrease in BLEU, indicating that BLM can easily\nﬁnd high-quality outputs.\nIn Fig. 6, we show examples generated by BLM\non Yelp. It can dynamically adapt to the imposed\nm 1 10 100 1000\nEstimated PPL 46.3 44.4 43.3 42.5\nTable 5: The estimated perplexity of BLM with the\nnumber of MC samples mon WikiText-103.\nPTB WT2 WT103\nLSTM (Grave et al., 2016) 82.3 99.3 48.7\nAWD-LSTM (Merity et al., 2017) 57.3 65.8 -\nTCN (Bai et al., 2018) 88.7 - 45.2\nTransformer (Dai et al., 2019) - - 30.1\nAdaptive (Baevski and Auli, 2018) - - 18.7\nTransformer-XL (Dai et al., 2019) 54.5 - 18.3\nInsT (our implementation) 77.3 91.4 39.4\nBLM 69.2 81.2 42.5\nTable 6: Perplexity on the PTB and WikiText datasets.\ncanvas and ﬁll in blanks with expressions of varied\nlengths, e.g., “nowhere to be found” →“the best i\nfound” and “deﬁnitely not” →“always”. We note\nthat failure cases arise when negative words like\n“either” are left unmasked; BLM is then unable to\nproduce satisfactory outputs from the canvas.\n4.4 Language Modeling\nLanguage modeling is a special case of text inﬁll-\ning where sequences are generated from scratch.\nTraditional left-to-right models dominate this task,\nbut are not suitable for text inﬁlling. Conversely,\nunconventional sequence models are rarely eval-\nuated on language modeling. Here, we study the\nperplexity of BLM and Insertion Transformer, and\ncompare them with left-to-right language models\nto provide additional insights.\nWe use the Monte-Carlo method to estimate the\nlikelihood in Eq. (5) with msamples. While the\nestimate is unbiased, given that per-word perplex-\nity is a convex function of per-sentence likelihood,\nsampling estimates like ours are likely yielding a\nvalue higher than the actual perplexity (see Ap-\npendix B for a proof). As mincreases, it converges\nto the actual perplexity.\nDatasets We test on three benchmark datasets:\nPenn Treebank (PTB) which has about 1M tokens\n(Mikolov et al., 2010), WikiText-2 (WT2) which\nhas 2M tokens, and WikiText-103 (WT103) which\nhas 103M tokens (Merity et al., 2016).\nResults Table 5 shows the trend of estimated\nPPL with the number of samples m. We choose\nm= 1000in our evaluation, which is close to con-\nvergence. Table 6 summarizes the perplexity of our\n5194\nmodel in comparison with previous work. The top\nresults are achieved by the Transformer-XL (Dai\net al., 2019) and the adaptive embedding method\n(Baevski and Auli, 2018). They use larger model\nsizes and supplementary techniques that can also be\ncombined with our model. BLM rivals the Insertion\nTransformer and outperforms left-to-right language\nmodels with LSTM and Temporal Convolutional\nNetwork (TCN) architecture. Language modeling\nseems to still be challenging for free-order models.\nBy reporting the perplexity of unconventional mod-\nels like BLM, we hope to stimulate future work in\nthis area to close the performance gap with tradi-\ntional left-to-right models.\n5 Conclusion\nIn this paper, we proposed the Blank Language\nModel for ﬂexible text generation. Given partially\nspeciﬁed text with one or more blanks, BLM will\nﬁll in the blanks with a variable number of tokens\nconsistent with the context. We demonstrate the\neffectiveness of our model on various text rewriting\ntasks, including text inﬁlling, ancient text restora-\ntion and style transfer.\nThe action of BLM consists of selecting a blank\nand replacing it with a word and possibly adjoin-\ning blanks. We train BLM by optimizing a lower\nbound on the marginal data likelihood that sums\nover all possible generation trajectories. In this\nway, we encourage the model to realize a sentence\nequally well in all orders, which is suitable for ﬁll-\ning arbitrary blanks. Appendix C shows examples\ngenerated by BLM along with their trajectories.\nDepending on the application, we could also train\nthe model to generate in speciﬁc orders by placing\nhigher weights on the corresponding trajectories.\nBLM has plenty of future applications, including\ntemplate ﬁlling, information fusion, assisting hu-\nman writing, etc. Moreover, we can extend our for-\nmulation to a conditional generative model. Such\nmodels can be used in machine translation to sup-\nport editing and reﬁning translation, as well as in\ndialogue systems to compose a complete sentence\nwith given elements. While we proposed BLM for\nlanguage generation, it would also be interesting to\ncompare the representations learned by BLM with\nthose produced by other pre-training methods.\nAcknowledgments\nWe thank all reviewers and the MIT NLP group for\ntheir thoughtful feedback.\nReferences\nYannis Assael, Thea Sommerschield, and Jonathan\nPrag. 2019. Restoring ancient text using deep learn-\ning: a case study on Greek epigraphy. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 6368–6375.\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv preprint arXiv:1809.10853.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.\n2018. An empirical evaluation of generic convolu-\ntional and recurrent networks for sequence modeling.\narXiv preprint arXiv:1803.01271.\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. Kermit: Gener-\native insertion-based modeling for sequences. arXiv\npreprint arXiv:1906.01604.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nChris Donahue, Mina Lee, and Percy Liang. 2020. En-\nabling language models to ﬁll in the blanks. arXiv\npreprint arXiv:2005.05339.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. arXiv preprint arXiv:1602.07776.\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: better text generation via ﬁlling in\nthe . arXiv preprint arXiv:1801.07736.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Constant-time machine\ntranslation with conditional masked language mod-\nels. arXiv preprint arXiv:1904.09324.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a con-\ntinuous cache. arXiv preprint arXiv:1612.04426.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor OK Li, and Richard Socher. 2017. Non-\nautoregressive neural machine translation. arXiv\npreprint arXiv:1711.02281.\n5195\nJiatao Gu, Qi Liu, and Kyunghyun Cho. 2019a.\nInsertion-based decoding with automatically in-\nferred generation order. Transactions of the Asso-\nciation for Computational Linguistics, 7:661–676.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019b.\nLevenshtein transformer. In Advances in Neural In-\nformation Processing Systems, pages 11179–11189.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\nDelete, retrieve, generate: a simple approach to sen-\ntiment and style transfer. In NAACL.\nDayiheng Liu, Jie Fu, Pengfei Liu, and Jiancheng Lv.\n2019a. TIGS: An inference algorithm for text in-\nﬁlling with gradient search. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4146–4156.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models. arXiv preprint arXiv:1708.02182.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Eleventh\nannual conference of the international speech com-\nmunication association.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nAlexander M Rush, Sumit Chopra, and Jason We-\nston. 2015. A neural attention model for ab-\nstractive sentence summarization. arXiv preprint\narXiv:1509.00685.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Advances in neural informa-\ntion processing systems, pages 6830–6841.\nYong-Siang Shih, Wei-Cheng Chang, and Yiming\nYang. 2019. Xl-editor: Post-editing sentences with\nxlnet. arXiv preprint arXiv:1910.10479.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible se-\nquence generation via insertion operations. arXiv\npreprint arXiv:1902.03249.\nQing Sun, Stefan Lee, and Dhruv Batra. 2017. Bidirec-\ntional beam search: Forward-backward inference in\nneural sequence models for ﬁll-in-the-blank image\ncaptioning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n6961–6969.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems.\nAlex Wang and Kyunghyun Cho. 2019. Bert has\na mouth, and it must speak: Bert as a markov\nrandom ﬁeld language model. arXiv preprint\narXiv:1902.04094.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nChen Wu, Xuancheng Ren, Fuli Luo, and Xu Sun.\n2019a. A hierarchical reinforced sequence oper-\nation method for unsupervised text style transfer.\narXiv preprint arXiv:1906.01833.\nXing Wu, Tao Zhang, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2019b. Mask and inﬁll: Applying\nmasked language model for sentiment transfer. In\nIJCAI.\nJingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu-\nancheng Ren, Houfeng Wang, and Wenjie Li. 2018.\nUnpaired sentiment-to-sentiment translation: A cy-\ncled reinforcement learning approach. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 979–988.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nZichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and\nTaylor Berg-Kirkpatrick. 2017. Improved varia-\ntional autoencoders for text modeling using dilated\nconvolutions. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning-Volume 70,\npages 3881–3890. JMLR. org.\n5196\nNajam Zaidi, Trevor Cohn, and Gholamreza Haffari.\n2020. Decoding as dynamic programming for recur-\nrent autoregressive models. In International Confer-\nence on Learning Representations.\nZhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang,\nPeng Chen, Mu Li, Ming Zhou, and Enhong Chen.\n2018. Style transfer as unsupervised machine trans-\nlation. arXiv preprint arXiv:1808.07894.\nWanrong Zhu, Zhiting Hu, and Eric Xing. 2019. Text\ninﬁlling. arXiv preprint arXiv:1901.00158.\n5197\nAppendix\nA Implementation Details for Text\nInﬁlling Baselines\nA.1 Insertion Transformer\nWe implement the Insertion Transformer in our\nown framework, using the same Transformer en-\ncoder module as for BLM and replacing the predic-\ntion layers by Insertion Transformer’s mechanism.\nThe canvas is also generated according to the train-\ning procedure of Insertion Transformer.\nA.2 Masked Language Model\nWe use theRobertaForMaskedLM architecture\nin the Transformers library for MLM (Wolf et al.,\n2019; Liu et al., 2019b).\nAt test time, the model is given an easier version\nof the text inﬁlling task where blanks are expanded\ninto sequences of⟨mask⟩tokens of the target length\n(or equivalently, the model uses an oracle to predict\nthe length of the inﬁlling).\nWe experiment with three decoding strategies:\n(1) one-shot: the model predicts all masks simul-\ntaneously (2) left-to-right: the model ﬁlls in the\nmasks from left to right (3) conﬁdent-ﬁrst: the\nmodel ﬁlls one mask at a time that has the high-\nest score. We report results for the conﬁdent-ﬁrst\nstrategy which has the best performance.\nA.3 BERT+LM\nWe use the bert-base-uncased model as\nserved by the Transformers library (Wolf et al.,\n2019; Devlin et al., 2018). The left-to-right lan-\nguage model is a Transformer decoder to predict\ntokens in a blank. Its input word embedding is con-\ncatenated with BERT’s output in the blank position\nat each time step.\nA.4 Seq2seq-full and Seq2seq-ﬁll\nFor both seq2seq baselines, we use Fairseq’s\ntransformer iwslt de en architecture (Ott\net al., 2019). To generate training data, we apply\nthe blanking procedure to the input dataset and\ngenerate kcopies of each sentence with different\nmasks. We experiment with k∈{1,10,100}and\nreport the best performance, obtained by k= 10.\nB Monte-Carlo Estimate of Perplexity\nFor a sentence xof length n, we estimate p(x; θ)\nin Eq. (5) with msamples:\nXm = n!\nm\nm∑\ni=1\np(x,σi; θ)\nwhere σi’s are randomly sampled orders.\nNote that Xm is an unbiased estimate of p(x; θ):\nE[Xm] =p(x; θ)\nThe estimated PPL is accordinly:\nYm = X−1/n\nm\nSince z−1/n is a convex function of z,\nE[Ym] =E[X−1/n\nm ] ≥E[Xm]−1/n = p(x; θ)−1/n\ni.e., the expectation of the estimated PPL ≥the\nactual PPL. As mincreases, the variance of Xm\ndecreases, and the inequality becomes tighter.\nHence, we will observe that as mincreases, the\nestimated PPL becomes smaller and converges to\nthe real PPL.\n5198\nC Generation Trajectory\nalso\nthe also\nthe also choice\nthe salsa also choice\nthe salsa was also choice\nthe salsa was also only choice\nthe salsa was also only choice .\nthe salsa was also my only choice .\n,\n, terrible\npoor , terrible\npoor , terrible ,\npoor , terrible , very\npoor selection , terrible , very\nvery poor selection , terrible , very\nvery poor selection , service terrible , very\nvery poor selection , service terrible , very !\nvery poor selection , service terrible , very slow !\nfavorite\nmy favorite\nmy favorite pittsburgh\nmy favorite pittsburgh .\nmy favorite restaurant pittsburgh .\nmy favorite restaurant in pittsburgh .\nthe\nis the\nis the .\nis the are .\nfood is the are .\nfood is the are friendly .\nfood is and the are friendly .\nfood is delicious and the are friendly .\nfood is delicious and the are very friendly .\nfood is delicious and the owners are very friendly .\nthe food is delicious and the owners are very friendly .\nFigure 7: Examples of BLM generation trajectory on the Yelp review dataset.",
  "topic": "Blank",
  "concepts": [
    {
      "name": "Blank",
      "score": 0.9570914506912231
    },
    {
      "name": "Computer science",
      "score": 0.7575562596321106
    },
    {
      "name": "Fluency",
      "score": 0.7051354646682739
    },
    {
      "name": "Word (group theory)",
      "score": 0.5948861837387085
    },
    {
      "name": "Natural language processing",
      "score": 0.5805334448814392
    },
    {
      "name": "Task (project management)",
      "score": 0.4920641779899597
    },
    {
      "name": "Sequence (biology)",
      "score": 0.484782874584198
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4803410768508911
    },
    {
      "name": "Language model",
      "score": 0.46466076374053955
    },
    {
      "name": "Artificial intelligence",
      "score": 0.389390766620636
    },
    {
      "name": "Linguistics",
      "score": 0.25352370738983154
    },
    {
      "name": "Engineering",
      "score": 0.0805307924747467
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}