{
    "title": "Q-RBSA: high-resolution 3D EBSD map generation using an efficient quaternion transformer network",
    "url": "https://openalex.org/W4391340935",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5062107979",
            "name": "Devendra K. Jangid",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A5019862057",
            "name": "Neal R. Brodnik",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A5078428285",
            "name": "McLean P. Echlin",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A5011371723",
            "name": "Chandrakanth Gudavalli",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A5037097949",
            "name": "Connor Levenson",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A5032474098",
            "name": "Tresa M. Pollock",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A5090583111",
            "name": "Samantha Daly",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A5071938464",
            "name": "B.S. Manjunath",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3015111928",
        "https://openalex.org/W3040456466",
        "https://openalex.org/W3040507155",
        "https://openalex.org/W3092568844",
        "https://openalex.org/W3039995677",
        "https://openalex.org/W3172602445",
        "https://openalex.org/W2015355231",
        "https://openalex.org/W4224285613",
        "https://openalex.org/W3213354120",
        "https://openalex.org/W1549438340",
        "https://openalex.org/W2603433416",
        "https://openalex.org/W2963578649",
        "https://openalex.org/W2963224077",
        "https://openalex.org/W3013529009",
        "https://openalex.org/W2954930822",
        "https://openalex.org/W4225672218",
        "https://openalex.org/W4312225444",
        "https://openalex.org/W2963719423",
        "https://openalex.org/W6738884980",
        "https://openalex.org/W2896208287",
        "https://openalex.org/W2497204217",
        "https://openalex.org/W2040378481",
        "https://openalex.org/W4213187341",
        "https://openalex.org/W2161469547",
        "https://openalex.org/W2894977338",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3207918547",
        "https://openalex.org/W2966878259",
        "https://openalex.org/W2963230471",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2146069630",
        "https://openalex.org/W2108812136",
        "https://openalex.org/W1558502811",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2476548250",
        "https://openalex.org/W2963372104",
        "https://openalex.org/W3096739052",
        "https://openalex.org/W3177141244",
        "https://openalex.org/W4205356152",
        "https://openalex.org/W2945932672",
        "https://openalex.org/W2946836808",
        "https://openalex.org/W2152014464"
    ],
    "abstract": "Abstract Gathering 3D material microstructural information is time-consuming, expensive, and energy-intensive. Acquisition of 3D data has been accelerated by developments in serial sectioning instrument capabilities; however, for crystallographic information, the electron backscatter diffraction (EBSD) imaging modality remains rate limiting. We propose a physics-based efficient deep learning framework to reduce the time and cost of collecting 3D EBSD maps. Our framework uses a quaternion residual block self-attention network (QRBSA) to generate high-resolution 3D EBSD maps from sparsely sectioned EBSD maps. In QRBSA, quaternion-valued convolution effectively learns local relations in orientation space, while self-attention in the quaternion domain captures long-range correlations. We apply our framework to 3D data collected from commercially relevant titanium alloys, showing both qualitatively and quantitatively that our method can predict missing samples (EBSD information between sparsely sectioned mapping points) as compared to high-resolution ground truth 3D EBSD maps.",
    "full_text": "ARTICLE OPEN\nQ-RBSA: high-resolution 3D EBSD map generation using an\nefﬁcient quaternion transformer network\nDevendra K. Jangid 1 ✉, Neal R. Brodnik 2, McLean P . Echlin3, Chandrakanth Gudavalli 1, Connor Levenson1, Tresa M. Pollock3,\nSamantha H. Daly 2 and B. S. Manjunath 1 ✉\nGathering 3D material microstructural information is time-consuming, expensive, and energy-intensive. Acquisition of 3D data has\nbeen accelerated by developments in serial sectioning instrument capabilities; however, for crystallographic information, the\nelectron backscatter diffraction (EBSD) imaging modality remains rate limiting. We propose a physics-based efﬁcient deep learning\nframework to reduce the time and cost of collecting 3D EBSD maps. Our framework uses a quaternion residual block self-attention\nnetwork (QRBSA) to generate high-resolution 3D EBSD maps from sparsely sectioned EBSD maps. In QRBSA, quaternion-valued\nconvolution effectively learns local relations in orientation space, while self-attention in the quaternion domain captures long-range\ncorrelations. We apply our framework to 3D data collected from commercially relevant titanium alloys, showing both qualitatively\nand quantitatively that our method can predict missing samples (EBSD information between sparsely sectioned mapping points) as\ncompared to high-resolution ground truth 3D EBSD maps.\nnpj Computational Materials          (2024) 10:27 ; https://doi.org/10.1038/s41524-024-01209-6\nINTRODUCTION\nIn the pursuit of materials development for extreme environ-\nments, 3D microstructural information is essential input for\nstructure-property models\n1. Many engineering materials are\npolycrystalline, meaning they are composed of many smaller\ncrystals called grains, and the arrangement of these grains impacts\ntheir thermomechanical properties. To collect crystallographic\nmicrostructure information, 3D microscopy techniques have been\ndeveloped that span lengthscales from nanoscale to mesoscale2.\nThese experiments require costly or challenging to access\nequipment, like synchrotron light sources for high X-rayﬂuxes3– 5,\nprecise automated robotic mechanical polishing and imaging6,7,\nor high-energy ion beams and/or short pulse lasers coupled to\nelectron microscopes8,9. Recent advances in 3D characterization\nhave reduced the time required for data collection, but serial\nsectioning methods (where material is progressively removed\nfrom the sample between images) are still slow processes that\nrequire expensive microscopes10,11. During serial sectioning for\nmicrostructural information, typical experimental steps might\ninclude material removal and cleanup (mechanical polishing, laser\nablation, focused ion beam milling) and imaging for orientation or\nchemical information. As such, any efforts that reduce the total\nnumber of required serial sections in a 3D experiment will\nultimately lead to substantive time and cost savings.\nOne such 3D material characterization technique is 3D electron\nbackscatter diffraction, or EBSD. EBSD is a scanning electron\nmicroscope (SEM) imaging modality that maps crystal lattice\norientation by analyzing Kikuchi diffraction patterns that are\nformed when a focused electron beam is scattered by the atomic\ncrystal structure of a material according to Bragg’s law. A grid of\nKikuchi patterns is collected by scanning the electron beam across\nthe sample surface. These patterns are then indexed to form a grid\nof orientations, which are commonly represented as images in\nRGB color space using inverse poleﬁgure (IPF) projections. EBSD\nmaps are used to determine the microstructural properties such as\ntexture, orientation gradients, phase distributions, and point-to-\npoint orientation correlations, all of which are critical for under-\nstanding material performance12. When scaling to 3D, these EBSD\nscans must be done sequentially with serial sectioning, which is a\ntime-consuming and energy-intensive process, often requiring\nhundreds of millions of EBSD patterns to be collected per sample.\nThis cost motivates methods to reduce the number of required\npoints, such as smart or sparse sampling\n13– 15, or machine learning\nsuper-resolution16. In these methods, missing information can be\ninferred using interpolation-based algorithms (bicubic, bilinear, or\nnearest neighbor) or data-based learning. Recent progress in\ncomputer vision\n16– 18 has shown that the generation of missing\nsamples/data with data-based learning outperforms traditional\ninterpolation algorithms for RGB images. However, unlike RGB\nimages, EBSD maps carry embedded crystallography, so existing\nlearning-based methods are not well suited to generate missing\nEBSD data.\nIn our previous work\n19, we developed a deep learning\nframework for 2D super-resolution that utilized an orientation-\nally-aware, physics-based loss function to generate high-resolution\n(HR) EBSD maps from experimentally gathered low-resolution (LR)\nmaps. This approach allowed for signiﬁcant gains in 2D resolution,\nbut expansion to 3D remained difﬁcult due to data availability\nlimitations (3D EBSD is expensive and time consuming to gather).\nTo address this, here we have designed a 3D deep learning\nframework based on quaternion convolution neural networks\n(QCNN) with self-attention alongside physics-based loss to super-\nresolve high resolution 3D maps using as little data as possible.\nUsing real-valued convolution for quaternion-based data has been\nshown to be inef ﬁcient and has loss in the inter-channel\nrelationship that arise from quaternion vector interdependen-\ncies\n20; which leads to longer training times and larger data\nburdens. We demonstrate that a quaternion-valued neural\nnetwork is more efﬁcient and produces better results than real-\n1Electrical and Computer Engineering, University California Santa Barbara, Santa Barbara 93106-5050 CA, USA.2Mechanical Engineering, University California Santa Barbara, Santa\nBarbara 93106-5050 CA, USA.3Materials Department, University California Santa Barbara, Santa Barbara 93106-5050 CA, USA.✉email: dkjangid@ucsb.edu; manj@ucsb.edu\nwww.nature.com/npjcompumats\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\n1234567890():,;\nvalued convolution neural networks such as those used in\nprevious work19.\nThe crystallographic information contained in EBSD maps is\ngenerally expressed in the form of crystal orientations spatially\nresolved at each pixel or voxel. These orientations, like other\nrotational data, can be expressed unambiguously using quater-\nnions. They therefore can be incorporated into network archi-\ntecture as prior information by using quaternion-valued\nconvolution for local-level correlation, rather than real-valued or\ncomplex-valued convolution. The basic component in traditional\nCNN-based architectures is real-valued convolutional layers, which\nextract high-dimensional structural information using a set of\nconvolution kernels. This approach is well-suited for uncon-\nstrained image data like RGB, but when convolution kernels fail to\naccount for strict inter-channel dependencies where present, the\nresult is greater learning complexity. Some successful efforts have\nbeen made to design lower-complexity architectures by extending\nreal-valued convolution to complex-valued convolution21,22 and\nquaternion-valued convolution 23– 25 in the ﬁeld of robotics 26,\nspeech and text processing 20, computer graphics 27,28, and\ncomputer vision25,29. Although these convolution layers are useful\nto learn local correlations, they struggle to learn long-range\ncorrelations, whereas transformer-based architectures have\nrecently shown signiﬁcant success in learning long-range correla-\ntions in natural language30 and vision tasks18,31. However, the\ncomputational complexity of transformer-based architectures\ngrows quadratically with the spatial resolution of input images\ndue to self-attention layers, so transformers alone are not well-\nsuited for restoration tasks. However, recent work by Zamir 18\nproposed self-attention across channel dimensions to reduce\ncomplexity from quadratic to linear with progressive learning for\nimage restorations and showed superior results to convolution-\nbased architecture alone.\nInspired by this idea, we propose the use of quaternion self-\nattention for EBSD super-resolution, using physics-aware\nquaternion convolution for orientation recognition, a physics-\nbased loss function that is sensitive to material crystal symmetry,\nand progressive learning to incorporate long-range material\nrelationships. Physics-aware quaternion convolution follows the\napproach of\n20,21,32, where convolution is depth-wise and uses a\nreduced number of interdependent weights whose connectivity is\nbased on the Hamiltonian. We use a loss function that accurately\nmeasures the crystal orientations in EBSD maps and also accounts\nfor the hexagonal close-packed symmetry present inα-phase Ti-\n6Al-4V and Ti-7Al, the two alloys investigated here. Finally,\nprogressive learning refers to having variable patch sizes instead\nof ﬁxed patch sizes during training, which is relevant for most\nengineered material microstructures, where important features\ncan span across length scales (and patch sizes). The titanium alloys\nstudied herein are well-known to have many different micro-\nstructural variants accessible via processing, resulting in varying\ngrain size and morphology. For the datasets that we consider\nspeciﬁcally, the Ti-6Al-4V variant has smaller equiaxed grains,\nwhile the Ti-7Al alloy has much larger grain size, so applying a\nﬁxed patch size across these two materials would be sub-optimal.\nTo enforce long-range learning among these grain features, we\nused progressive patch sizes starting from 16 to 100 during the\ntraining of the network. Training behaves in a similar fashion to\ncurriculum learning processes where the network starts with a\nsimpler task and gradually moves to learning more complex ones.\nRESULTS\nDeep learning framework\nThe objective of our framework is to generate missing sample\nplanes from experimental 3D EBSD data that is sparse along the\nz-axis. In this approach, material researchers collect sparsely\nsectioned 3D EBSD data (blue planes) as shown in Fig.1a, due to\nthe high cost associated with serial sectioning and collecting 3D\nx or y\nz\nElectron Beam EBSD\nDetector\nDeep Learning\nFramework\nGenerated High-Resolution\n3D EBSD Data\nSparsely Sectioned\n3D EBSD Data\n(a)\n(c)\n3D EBSD Data\ny\nz\nx\nQRBSA\nNetwork\n2D Planes to\n3D Volume\nGenerated High-\nResolution 3D EBSD Data\n2D EBSD SR Map\nDeep Learning Framework\nz\nx or y\nSparsely\nSectioned 2D\nEBSD Map\n519 um\n(b)\nFig. 1 Q-RBSA EBSD Resolution Enhancement Framework.In the experimental pipeline shown in (a), material researchers collect EBSD\norientation information for each (x,y) coordinate in a given sectioning plane, and then remove material using laser ablation or robotic\npolishing to reach the next plane in the z direction to build a 3D volume. In our framework (b), researchers collect EBSD information from a\nreduced set of points (blue planes), omitting some planes that would normally be gathered (gray planes). The missing information (green\nplanes) are then generated in 2D as a series of (x,z) or (y,z) planes by our quaternion-based, physics-informed deep learning framework, shown\nin (c). Here, the network takes advantage of orthogonal independence to efﬁciently generate 3D volumes using less data, as large amounts of\nEBSD are costly and the choice of serial sectioning direction has minimal impact on the resultantﬁnal volume.\nD.K. Jangid et al.\n2\nnpj Computational Materials (2024)    27 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\n1234567890():,;\nEBSD data at higher resolution. Ideally, a 3D deep learning\nframework would be designed to generate the missing planes\n(gray planes), but experimental EBSD data is costly to gather, so\navailable 3D data is extremely limited. Additionally, 3D neural\nnetworks require more learned parameters, which, with limited\navailable data, increases the likelihood of overﬁtting. Instead of a\nfull 3D architecture, a deep learning network is implemented on\n2D EBSD maps orthogonal to the sectioned planes, shown as the\nxz or yz planes in Fig.1c. Our network takes sparsely sectioned xz\nor yz EBSD maps as input to generate the missing rows normal to\nthe z-axis. The generated 2D EBSD maps are then combined into a\n3D volume. EBSD collection is a point-based scanning method\nthat is directionally independent; therefore missing z rows can be\ngenerated from xz or yz EBSD maps, and two 3D volumes can be\nformed from each sparsely sectioned dataset.\nNetwork architecture\nAlthough EBSD maps are visualized similarly to RGB images, they\nare multidimensional maps with inter-channel relationships,\nwhere crystal orientation is described using Euler angles,\nquaternions, matrices, or axis-angle pairs. Our previous work\n19\ndemonstrated that quaternion EBSD representation is well-suited\nto orientation expression for loss function design, due to its\nefﬁcient rotation simpli ﬁcation and avoidance of ambiguous\nrepresentation. However, we previously used real-valued convolu-\ntion layers to learn features, which is sub-optimal for EBSD\norientation maps where orientations are represented as unit-\nvector quaternion rotations. Generally speaking, convolution\nnetworks provide local connectivity and translation equivariance,\nwhich are desirable properties for images, but if additional feature\ncorrelations are going to be learned efﬁciently, it is critical to\nencode relevant structural modalities into the network architec-\nture and loss function. Real-valued convolution can still learn unit\nquaternion inter-channel information, but it requires extra net-\nwork complexity, and by consequence, additional data to inform\nthat complexity. Here, the use of quaternion convolution\nefﬁciently encodes prior orientation information into kernels,\nand also has the advantage of reducing the number of trainable\nparameters by 4, as explained supplementary.\nQCNN\n33 use basic quaternion convolution operation which\ncomputes the Hamilton product between the input feature maps\nand kernelﬁlters rather than just computing correlations between\nthem, as is done in real-valued convolution\n34. For instance, if we\nconsider Pinput as an input feature map of size (4K2 × H × W), andF\nas a quaternion kernel ﬁlter of size (4K2 × f × f), where K2 is the\nnumber of kernel ﬁlters in the previous layer, H and W are the\nheight and width of the input feature map (Pinput), and f is the\nspatial size of the quaternion kernelﬁlter (F), then we can split the\ninput feature map ( Pinput) into four components ( PR, PX, PY, PZ)\nalong the channel dimension, where each component has a\ndimension of (K\n2 × H × W). Similarly, the kernel ﬁlter (F) can be\ndivided into four components ( FR, FX, FY, FZ) along the channel\ndimension, where each component has a dimension of (K2 × f × f).\nThe quaternion convolution (QConv) of input feature maps (Pinput)\nwith a single kernelﬁlter (F)i sd eﬁned as follows\nP0\nquaternion ¼ F /C10 Pinput (1)\nP0\nR\nP0\nX\nP0\nY\nP0\nZ\n2\n666\n4\n3\n777\n5 ¼\nF\nR /C3 PR /C0 FX /C3 PX /C0 FY /C3 PY /C0 FZ /C3 PZ\nFX /C3 PR þ FR /C3 PX /C0 FZ /C3 PY þ FY /C3 PZ\nFY /C3 PR þ FZ /C3 PX þ FR /C3 PY /C0 FX /C3 PZ\nFZ /C3 PR /C0 FY /C3 PX þ FX /C3 PY þ FR /C3 PZ\n2\n666\n4\n3\n777\n5 (2)\nHere, ⊗ is the Hamilton product, and * represents real-valued\nconvolution operation\n34. The output quaternion feature map\n(P0\nquaternion) has a dimension of (4 ×H × W) for a single kernelﬁlter,\nwhere each component ( P0\nR; P0\nX ; P0\nY ; P0\nZ ) has a dimension of\n(1 ×H × W). H and W are the height and width of the output\nfeature maps. For a better understanding of quaternion convolu-\ntion, please refer to the Supplementary Figs. 1, 2, 3 as provided in\nthe supplementary section.\nIntroducing non-linearity through an activation function is not\nstraightforward for quaternions, as the only functions that satisfy\nthe Cauchy-Riemann-Fueter equations in the quaternion domain\nare linear or constant\n32. However, locally analytic quaternion\nactivation functions have been adapted for use in QNNs with\nstandard backpropagation algorithms\n35,36. There are two classes\nof these quaternion-valued activation functions: fully quaternion-\nvalued functions and split functions. Fully quaternion-valued\nactivation functions are an extension to the hypercomplex domain\nof real-valued functions, such as sigmoid or hyperbolic tangent\nfunctions. Despite their better performance\n37, careful training is\nneeded due to the occurrence of singularities that can affect\nperformance. To avoid this, split activation functions 37,38 have\nbeen presented as a simpler solution for QNNs. In split activation\nfunctions, a conventional real-valued function is applied\ncomponent-wise to a quaternion, alleviating singularities while\nholding true the universal approximation theorem as demon-\nstrated in\n38. We have used the split ReLU function which is deﬁned\nas follows:\nReLUðP0\nquaternionÞ¼\nReLUðP0\nRÞ\nReLUðP0\nX Þ\nReLUðP0\nY Þ\nReLUðP0\nZ Þ\n2\n666\n4\n3\n777\n5 (3)\nOur network architecture shown in Fig.2 consists of three parts:\na shallow feature extraction module, a deep feature extraction\nmodule, and an upsampling and reconstruction module.\nShallow Feature Extractor module uses a single quaternion\nconvolution layer (QConv), explained in Eq. ( 2), to reduce the\nspatial size of sparsely sectioned EBSD maps, while extracting\nshallow features.\nF\n0 ¼ HSF ðILRÞ (4)\nHere, ILR is a sparsely sectioned 2D EBSD map andHSF(. ) is a single\nquaternion convolution layer of kernelﬁlter size 3 × 3, which has 4\ninput channels and 128 output channels. The generated shallow\nfeatures (F\n0) are given to the deep feature extractor module (HDF).\nTo learn from sparsely sectioned EBSD maps, our deep feature\nextractor module uses stacked quaternion residual self-attention\n(QRSA) blocks to extract high-frequency information and long skip\nconnection to bypass low-frequency information. Residual blocks\nallow for a deeper network architecture, which provides a larger\nreceptive ﬁeld and better training stability. In our QRSA module,\nwe use both CNN and transformer ideas to combine the\neffectiveness of the locality of CNNs with the expressivity of\ntransformers that enables them to synthesize high-resolution\nEBSD maps. The CNN structure offers local connectivity and\ntranslation equivariance, allowing transformer components to\nfreely learn complex and long-range relationships. Each quater-\nnion residual self-attention (QRSA) block consists of two\nquaternion convolution layers and a piece-wise ReLU activation,\nexplained in Eq. (3), between them, and a quaternion transformer\nblock. The quaternion convolution layers with piece-wise ReLU\nactivation help in learning the local structure of extracted shallow\nfeatures, while the quaternion transformer block captures long-\nrange correlations among features. The short-skip connection is\nuseful to bypass low-frequency information during training.\nF\nDF ¼ HDF ðF0Þ (5)\nHDF ¼ QRSA1 /C14 QRSA2 /C14 :::QRSAi::: /C14 QRSA10 /C14 QConv þ I (6)\nD.K. Jangid et al.\n3\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2024)    27 \nwhere, HDF(. ) is a deep feature extractor module, andFDF is a 128\nchannels feature map which goes to the upscale and reconstruc-\ntion module.QConv is a quaternion convolution layer as explained\nin Eq. (2), QRSAi is a ith quaternion residual self-attention (QRSA)\nblock, and I is an identity feature maps.\nThe standard transformer architecture30 consists of self-attention\nlayers, feedforward networks, and layer normalization. The original\ntransformer architectures\n30,39 are not suitable for restoration tasks\ndue to the requirement of quadratic complexity of spatial size\nOðW\n2H2Þ, where W, H is the spatial size of images or EBSD maps.\nSimilar to the approach of18 and as shown schematically in Fig.3,\nwe compute attention maps across the features dimension, which\nreduces the problem to linear complexity. However instead of\ndepthwise convolution, we use quaternion convolution as\nexplained in Eq. (2), which can be considered as a combination\nof depthwise convolution and group convolution, but with four-\ndimensional quaternion constraints. We have also incorporated an\nequivalent quaternion-based gating mechanism into the feedfor-\nward network within the transformer, and the traditional convolu-\ntion used in\n18 has been replaced with quaternion convolution\nlayers as explained in Eq. (2) to account for EBSD data modalities.\nLayer normalization plays a crucial role in the stability of training in\ntransformer architectures. The quaternion layer-normalization is\nequivalent to the real-valued one by computing normalized\nfeatures across each component of the quaternion separately,\nand allows the building of deeper architectures by normalizing the\noutput at each layer. From the normalized features, the quaternion\nself-attention layerﬁrst generates query (Q), key (K) and value (V)\nprojections enriched with the local context. After reshaping query\nand key projection to reshaped query (Q\nr) and reshaped key (Kr), a\ntransposed attention map (A) is generated. The reﬁned feature\nmap, which has global statistical information, is calculated from the\ndot product of the value projection (V\nr) and the attention map (A).\nTransposed Attention Map (A)¼ Softmax Kr : Qr\nα\n/C18/C19\n(7)\nQuat-SelfAttentionðQr ; Kr ; Vr Þ¼ Vr :A (8)\nGatingðX2Þ¼ GeLU ðW1ðX2ÞÞ /C12 W2ðX2Þ (9)\nWhere ⨀ represents elementwise multiplication,α is a learnable\nscaling parameter to control the magnitude of the dot product of\nKr and Qr before applying softmax function and GeLU is Gaussian\nError Linear Units activation function 40, and Wi (i = 1,2) is a\ncombination of quaternion convolution layers with kernel size 1\nand 3, respectively.\nThe upsampling and reconstruction module has 1D pixelshufﬂe\nlayers and quaternion convolution layers of kernel size 3.\nDeep Feature\nExtractor\nShallow Feature\nExtractor Upsampling and\nReconstruction\nSparsely\nSectioned 2D\nEBSD Map HR 2D EBSD\nMap\n(a)\n(b)\n(c)\n(d)\nILR\nF0 FDF\nISR\n(4 x H x W)\n128 x H x W\n(4 x sH x W)\nDeep Feature Extractor\nQRSA1 QRSA2 QRSA10F0 FDF\n128 x H x W\nQuaternion\nTransformerBlock\nQuaternion Residual Self\nattention (QRSA)\n128 x H x W 128 x H x W\nQuaternion Transformer Block\nQuaternion\nSelf Attention\nQuaternion\nFeed Forward\n128 x H x W 128 x H x W\nQConv\n3X3\nPixelShuffle\n1D\nReLU\nQConv\n1X1\nQLayerNorm\nLegend\nAddition\ns= Scale Factor\nH =  Height of Input\nEBSD Map\nW = Width of Input EBSD\nMap\nFig. 2 Quaternion Residual Block Self-attention (QRBSA) network.A sparsely sectioned 2D EBSD map is given to the QRBSA network (a)t o\ngenerate a high-resolution 2D EBSD map. QRBSA consists of three parts: a Shallow feature extractor, a Deep feature extractor, and Upsampling\nand Reconstruction. The deep feature extractor uses a residual architecture (b) where residual self-attention blocks (c) are modiﬁed with\nquaternion convolution layers and transformer blocks (d)t oe fﬁciently handle orientation data. Quaternion convolution is used to learn local-\nlevel relationships, while quaternion transformer blocks learn the global statistics of feature maps. Pixelshuf ﬂe layer, modi ﬁed for\n1-dimensional upsampling, is used in the upsampling and reconstruction block to upsample feature maps.\nD.K. Jangid et al.\n4\nnpj Computational Materials (2024)    27 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nThe original pixelshufﬂe layer41 is designed for 2D upsampling,\nbut we have modiﬁed it for 1D upsampling in our framework that\ngenerates information in z dimension. Each block of the\nupsampling and reconstruction module upsamples deep features\nby a factor of 2, with the number of blocks depending on the\nscaling factors.\nF\n\" ¼ H\"ðFDF Þ (10)\nISR ¼ HRðF\"Þ (11)\nWhere, each upsampling block ( H↑) of the module has a\nquaternion convolution layer of kernel size (3 × 3) as explained\nin Eq. (2), and a 1D pixel-shufﬂe layer. The reconstruction block\n(HR) is a quaternion convolution layer of kernel size (3 × 3).\n2D to 3D\nThe output of the QRBSA network is a 2D high-resolution EBSD\nmap from a sparsely sectioned 2D EBSD map in the z direction.\nThe 2D high-resolution EBSD maps are then combined to make a\n3D volume. The missing z rows, as in Fig.1c, can be generated\neither from xz plane (y\nnormal) or yz plane (xnormal). Therefore, there\nare two ways to form the 3D volume. In this work, we generated\nboth 3D volumes separately, but we plan to design an algorithm\nin the future to combine the xz plane and yz plane information to\nmake a single 3D volume.\nQualitative output comparison\nThe sparsely sectioned 3D EBSD data is downsampled by scale\nfactors of 2, 4 in the z dimension by removing the xy planes\n(z\nnormal)t or eﬂect how EBSD resolution would be reduced in a\nserial sectioning experiment. Our network QRBSA is trained on 2D\northogonal planes (x\nnormal and ynormal) of paired sparsely sectioned\nEBSD maps and high-resolution EBSD maps, generating the high-\nresolution 2D maps in z dimension shown in Fig. 4. The most\nnoticeable visual defects in 2D appear as pixel noise or short\nvertical lines, particularly around small grain features and high-\naspect-ratio grains whose shortest axis is aligned with the\nz-direction. In addition to planar output analysis, we can also\ncreate 3D volumes from the sparsely sectioned xz planar (y\nnormal)\nor yz planar (xnormal) EBSD maps, and then sample the xy planes\n(znormal) from these volumes, as represented by the black arrows in\nFig. 5, to evaluate how well the QRBSA is inferring missing\nz-sample planes. Note that the planes visualized in Fig.5 are not\nimmediately adjacent to any ground truth planes. We can observe\nthat our deep learning framework is able to completely predict\nomitted xy planes, comparably to the ground truth xy plane, with\nthe exception of some shape variations around grain boundaries,\nparticularly in Ti-6Al-4V. We capture these errors in Fig.5 in the\ncolumn labeled misorientation angle map, which is contrast scaled\nsuch that all misorientation errors greater than 3\n∘ appear as white.\nLooking at this map, most of the high misorientation errors are at\ngrain boundaries with the exception of some speciﬁc small grains\nin Ti-6Al-4V. Observations of this difference map indicate that if\nthe xy plane in Fig.5 had been omitted during experimental data\nTransposed Attention\nMap (A)\nQ\nKX1\nY1\nSoftmax\nV\nR\nR\nR\nQuaternion Self-Attention\n(a)\n(b)\nQr\nKr\nVr\nR\nQConv\n3X3\nQConv\n1X1\nElementwisedot\nproduct\nR\nReshaping\nOperation\nH = Height of Input EBSD\nMap\nW = Width of Input EBSD\nMap\n128 x H x W\n128 x H x W 128 x H W\nH W x 128\n128 x H x W\n128 x H x W\n128 x 128\n128 x H W 128 x H W 128 x H  x W\n128 x H x W\n128 x H x W\nQuaternion FeedForward\nNetwork\nGeLU\nX2\nY2\nLegend\nFig. 3 Quaternion Self Attention.Self-Attention in (a) is computed using quaternion convolution across feature dimension instead of spatial\ndimension to reduce computational complexity to linear. A transposed attention map (A) is calculated from reshaped query (Qr) and reshaped\nkey (Kr). A quaternion self-attention is computed from the transposed attention map (A) and reshaped value (Vr). QUATERNION FEED\nFORWARD NETWORK: Shown in (b), performs controlled feature transformation to allow useful information to propagate further using gated\nquaternion convolution.\nD.K. Jangid et al.\n5\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2024)    27 \ncollection, our framework would have estimated it with reason-\nable accuracy.\nQuantitative output comparison\nThe pixel-wise distribution of minimum misorientations between\nnetwork output and ground truth, referred to as misorientation\nerror, is shown in Fig. 6. The x-axes of the histograms are\nthresholded and separated at 3∘, such that misorientation error of\nmagnitude less than 3∘ is shown in Fig.6a, and error greater than\n3∘ is shown in Fig.6b, c. These histograms show that the majority\nof network error is relatively unimodal and smaller in magnitude\nthan about 0.4∘, meaning that it will fall primarily within the dark\nregions of the grains in the difference maps in Fig. 5, which\ncorrespond to small intragranular misorientation errors. On the\nother hand, most of the high misorientation errors in Fig.5 are\nmuch larger than 3\n∘, which mostly correspond to errors in\npredicted grain boundary location, or small grains that were ill-\ndeﬁned in the low resolution input. While these errors are much\nlarger in magnitude, Fig. 6b shows that these represent a very\nsmall fraction of network error. A more detailed inspection of this\nerror in Fig.6c shows that this larger error is relatively random and\nuniform, with the exception of a spike around 30\n∘, which can be\nseen in all three datasets. This spike in error around 30∘ may be\nrelated to the hexagonal symmetry of the titanium materials, as\n30\n∘ is a high symmetry rotation within the 6/mmm point group,\nbut even so, these errors represent less than 2% of the total.\nThe peak signal to noise ratio (PSNR) of misorientation angle\nbetween ground truth and experimental EBSD data is shown in\nTable 1 to quantitatively evaluate the performance of the QRBSA\nnetwork for scale factors 2 and 4 for all three materials. Higher\nPSNR values represent more similarity with the ground truth. The\nPSNR of Ti-6Al-4V is lower compared to Ti-7Al datasets due to its\nhigher texture variability, wider range of orientations, and\ngenerally smaller grain features. We performed this same analysis\non four different network architectures with different computa-\ntional complexity, as shown in Table1.\nWhen considering the relationship between network com-\nplexity and performance for th is use-case, a simpler deep\nresidual architecture (EDSR)\n42, outperforms a more complex\nholistic attention network (HAN)43 on EBSD data despite having\nsigniﬁcantly lower computational complexity. The amount of\navailable EBSD data in this case is signi ﬁcantly lower than\nSparsely Sectioned\nEBSD Maps\nNetwork Output Ground Truth\nTi-6Al-4V\nTi-7Al 1%\nTi-7Al 3%\n340.6 um\n400.4 um\n462 um\n(a)\n(b)\n(c)\nFig. 4 Visual comparison of network output for example 2D EBSD maps with a scale factor of 4.The predicted EBSD maps (Network\nOutput) from the QRBSA network are similar to the ground truth EBSD maps in for both the Ti-6Al-4V dataset (a) and both Ti-7Al datasets (b)\nand (c). The black rows correspond to the missing data in the sparsely sectioned input EBSD maps. In this case, one row of EBSD data is used\nfor every three rows of missing EBSD data.\nD.K. Jangid et al.\n6\nnpj Computational Materials (2024)    27 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nopen-source RGB image datasets, so simply increasing network\ncomplexity does not improve performance, as this added\ncomplexity demands additional training information and does\nnot meaningfully consider relevant data modalities. QEDSR\nincorporates quaternion considerations in a similar architecture\nto EDSR, which greatly reduces in the number of network\nparameters, but also causes a slight drop in performance due to\noverall lack of complexity. We take advantage of this reduction\nin complexity to add in additional self-attention for better\nrecognition of long-range patterns and global statistics. This\nQRBSA network demonstrates t he best performance on EBSD\nmap restoration, while still maintaining lower complexity\nthan state-of-the-art residual architectures for single-image\nsuper-resolution tasks.\nDISCUSSION\nBoth quantitative and qualitative results demonstrate that this\nphysics-based deep learning framework can accurately estimate\nthe missing xy planes ( z\nnormal) of 3D EBSD data for multiple\nvariants of titanium alloys, both with a coarser polycrystalline\nstructure (Ti-7Al) andﬁner structure with stronger texture (Ti-6Al-\n4V). In 2D inferred EBSD planes show noise around small features,\nmostly in the form of point and line defects in the z-direction\nassociated with grains whose overall shape information was lost\ndue to omission of sample planes in low resolution. It is possible\nthat a downsampling approach incorporating anti-aliasing could\nprevent this shape information loss\n44, but this approach would not\nbe reﬂective of actual experimental downsampling in 3D EBSD.\nThis general shape loss effect, along with a larger number of small\ngrains, varying local crystallographic texture, and a wide range of\nrepresented crystal orientations, made the Ti-6Al-4V the most\ndifﬁcult dataset for inference. This is further evidenced by a larger\nnumber of grain boundary differences for Ti-6Al-4V in Fig.5,a s\nwell as a lower PSNR score in Table1. Additional noise analysis for\ngenerated xy planes is shown in the supplement, and there is\nongoing work to improve performance using 3D architectures and\ngrain shape information\n45 with adaptive multi-scale imaging in z\ndimension as more of this type of data becomes available.\nThe limiting factor when using the network approach presented\nhere on serial-sectioned 3D microstructures is the ratio of serial\nsectioning spacing in the low-resolution input relative to the size\nof the microstructural features being imaged. For example, if the\nserial section spacing is large enough to skip entire grains or\nmicrostructural features in a material, those features will never be\nresolvable with super-resolution. Therefore, an informed super-\nresolution scaling factor choice must be made prior to any\nexperiment to ensure that the low resolution input contains\nenough information for meaningful inference. Beyond this section\ndepth limitation, the approach shown here is directly applicable to\nany serial sectioning technique for gathering 3D EBSD informa-\ntion, including FIB sectioning, laser ablation, and robotic serial\nsectioning\n6,7. Further, data from other 3D grain mapping\ntechniques that rely on synchrotron X-ray sources such as\ndiffraction contrast tomography (DCT)\n5 or high energy diffraction\nmicroscopy3,4 may also be applicable for the infrastructure\npresented here. Similar approaches to this may be particularly\nuseful in lab source DCT experiments\n46,47, where the X-ray source\nconstraints limit grain mapping resolution in comparison to\nsynchrotron sources. For example, one could use dif ﬁcult\nto acquire synchrotron X-ray mapping experiments as HR data\nto train a network to inform LR X-ray mapping experiments\ncollected more routinely at the laboratory.\nIn summary, we have designed a quaternion-convolution-based\ndeep learning framework with crystallography physics-based loss\nto generate costly high-resolution 3D EBSD data from sparsely\nsectioned 3D EBSD data while accounting for the physical\nconstraints of crystal orientation and symmetry. Alongside this,\nan efﬁcient quaternion-based transformer block was developed to\nlearn long-range trends and global statistics from EBSD maps.\nMaterial Estimated XY Plane (Z Normal) Experimental XY Plane (Z Normal)\n(a) Ti-6Al-4V\n213 um262.6  um310.7  um\n(b) Ti-7Al 1%\n(c) Ti-7Al 3%\nMisorientation\nAngle Map (Z Normal)\nFig. 5 Neural network output vs. ground truth with difference map.The deep learning framework is able to estimate the missing xy planes\ndue to sparse z-sampling (gray) with data that looks similar to the ground truth for Ti-6Al-4V in (a) and Ti-7Al in (b) and (c). The misorientation\nangle map column shows the minimum possible misorientation between ground truth and estimated EBSD maps with 3∘ thresholded\nmaximum to better show low magnitude errors. This map indicates that learning grain shapes for Ti-6Al-4V is more difﬁcult than for Ti-7Al,\nlikely due to smaller grain size and more grain boundary regions.\nD.K. Jangid et al.\n7\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2024)    27 \nUsing quaternion convolution instead of regular convolution is\ncritical for crystallographic data, both in terms of output quality\nand neural network complexity, as reducing the number of\ntrainable parameters enables transformer addition without major\ncomplexity burden (see Table1). This framework can be directly\napplied to any experimental 3D EBSD approaches that rely on\nserial sectioning techniques to collect orientation information.\nMETHODS\nEBSD datasets\nEBSD maps represent crystal orientations collected at each\nphysical pixel location in crystalline materials, which are funda-\nmentally anisotropic and atomically periodic. Orientations for each\npixel within the network learning environment are expressed in\nterms of quaternions of the form q ¼ q\n0 þ^iq1 þ^jq2 þ ^kq3. The\nquaternions are suitable to design a physics-based loss function\nfor deep learning framework19. To avoid redundancy in quater-\nnion space (between q and −q), all orientations are expressed\nwith their scalar q0 as positive. For visualization according to\nestablished conventions, quaternions are reduced to the Rodri-\ngues space fundamental zone based on space group symmetry,\nconverted into Euler angles, and projected using IPF projection\nusing the open-source Dream3D software\n48, as shown in our\nprevious work 19. Ground truth 3D EBSD datasets were\n*1e6 *1e6 *1e6Ti-6Al-4V\n*1e6 *1e7 *1e7\nTi-7Al 1% Ti-7Al 3%(a)\n(b)\n(c)\nFig. 6 Histogram of Misorientation Angle for Ti-6Al-4V, Ti-7Al 1%, and Ti-7Al 3%.In (a), histograms of misorientation differences between\npredicted and ground truth are shown, where all values greater than 3∘ are clamped to 3∘. For all materials, most network error in predicted\nmisorientation is lower than 0.5∘ in magnitude. In (b), the same error histograms are displayed, but now misorientation values less than 3∘ are\nclamped to 3∘. Because larger magnitude errors occur far less frequently than smaller errors, (c) contains a zoomed inset of misorientation\nangles greater than 3∘ to better show their distribution.\nTable 1. PSNR: comparison of PSNR of misorientation angle and\ncomplexity for different networks for scale factors 2 and 4.\nNetwork Trainable\nParameters\nTi-6Al-4V x2/\nx4\nTi-7Al 1%\nx2/x4\nTi-7Al 3%\nx2/x4\nHAN 63,315,578 26.12/25.64 33.67/33.55 35.10/34.36\nEDSR 6,355,460 26.70/26.25 34.39/34.25 36.19/36.05\nQEDSR 1,593,092 26.62/26.16 34.23/34.10 35.56/35.44\nQRBSA 5,952,782 27.71/27.29 35.29/35.13 36.64/36.52\nColumns represent number of trainable parameters and PSNR for different\ntitanium datasets. A larger number is desired for both PSNR.\nBold values indicate highest PSNR for all three materials: Ti-6Al-4V, Ti-7Al\n1%, Ti-7Al 3%.\nD.K. Jangid et al.\n8\nnpj Computational Materials (2024)    27 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nexperimentally collected from titanium alloy samples: Ti-6Al-4V\nand Ti-7Al (one Ti-7Al sample deformed in tension to 1%, and one\nto 3%), using a commercially-available rapid-serial-sectioning\nelectron microscope referred to as the TriBeam8,10. More details\nof these datasets and their reconstruction are located in the\nsupplemental material. Sparsely sectioned EBSD datasets are\ncreated by removing xy (z\nnormal) planes from the high-resolution\nground truth with a downscale factor of 2 and 4 (LR= 1\n4HR or LR=\n1\n2HR). This is done to imitate the skipping of collection planes that\nwould occur in a 3D experiment with more sparsely sectioned\nEBSD data (i.e., thicker section depth), which would not inﬂuence\nthe electron beam-material interaction volume associated with\nthe EBSD mapping process. More information about dataset pre-\nprocessing are given in the supplementary material.\nNetwork implementation and output evaluation\nWe use a learning rate of 0.0002, an Adam optimizer withβ1 = 0.9,\nβ2= 0.99, ReLU activation, batch size of 4 and downscaling factor\nof 2 and 4. The patch size of HR EBSD maps is selected from\n{16, 32, 64, 100} during training of the network. The framework is\nimplemented in PyTorch and trained on NVIDIA Tesla V100 GPU\nfor 2000 epochs, which took approximately 100 h. Once training is\ncompleted, inference time for a given 2D LR EBSD map is on the\norder of less than one second for an imaging area that would\nnormally take about 10 min to gather manually.\nLoss\nThe QRBSA network is trained using a physics based loss\nfunction\n19, which uses rotational distance approximation loss\nwith enforced hexagonal crystal symmetry. Rotational distance\nloss computes the misorientation angles between the predicted\nand ground truth EBSD map in the same manner that they would\nbe measured during crystallographic analysis, with approxima-\ntions to avoid discontinuities at the edge of the fundamental zone.\nThe rotational distance θ between two unit quaternions can be\ncomputed as the following:\nq1 /C1 q2 ¼ cosðθÞ/C0 ! θ ¼ 4sin\n/C0 1 deuclid\n2\n/C18/C19\n(12)\nwhere, deuclid = ∥q1 −q2∥2. While deuclid is Lipschitz, the gradient\nof θ goes to ∞ as deuclid →2. To address this issue during neural\nnetwork training, a linear approximation was computed at\ndeuclid = 1.9, and utilized for points > 1.9.\nProgressive Learning\nIn our previous work19, we used aﬁxed patch size of dimension\n64 × 64 for training the CNN based architectures which help in\nlearning local correlations. However, self-attention is required to\nhave larger patch sizes, which aids in learning global correlations.\nInspired from the work of Zamir 18, we use progressive patch\nsamples from sizes of {16, 32, 64, 100} in the training process to\nlearn global statistics. We start from a smaller patch size in early\nepochs and increase to a larger patch sizes in the later epochs. The\nprogressive learning acts like the curriculum learning process\nwhere a network starts with a simple tasks and gradually moves to\nlearning a more complex one.\nDATA AVAILABILITY\nQRBSA inference module is publicly available through the BisQue cyberinfrastructure\nat https://bisque2.ece.ucsb.edu. Users would need to create an account at BisQue to\nuse this module. Material datasets will be available by request at the discretion of the\nauthors.\nCODE AVAILABILITY\nArchitecture code is publicly accessible through GitHub (https://github.com/UCSB-\nVRL/Q-RBSA).\nReceived: 8 May 2023; Accepted: 14 January 2024;\nREFERENCES\n1. Council, N. R.Integrated Computational Materials Engineering: A Transformational\nDiscipline for Improved Competitiveness and National Security(The National Aca-\ndemies Press, Washington, DC, 2008).\n2. Echlin, M. P., Burnett, T. L., Polonsky, A. T., Pollock, T. M. & Withers, P. J. Serial\nsectioning in the SEM for three dimensional materials science.Curr. Opin. Solid\nState Mater. Sci.24, 100817 (2020).\n3. Miller, M. P., Pagan, D. C., Beaudoin, A. J., Nygren, K. E. & Shadle, D. J. Under-\nstanding micromechanical material behavior using synchrotron x-rays and in situ\nloading. Metall. Mater. Trans. A51, 4360– 4376 (2020).\n4. Bernier, J. V., Suter, R. M., Rollett, A. D. & Almer, J. D. High-energy x-ray diffraction\nmicroscopy in materials science.Annu. Rev. Mater. Res.50, 395– 436 (2020).\n5. Reischig, P. & Ludwig, W. Three-dimensional reconstruction of intragranular strain\nand orientation in polycrystals by near-ﬁeld x-ray diffraction. Curr. Opin. Solid\nState Mater. Sci.24, 100851 (2020).\n6. Rowenhorst, D. J., Nguyen, L., Murphy-Leonard, A. D. & Fonda, R. W. Character-\nization of microstructure in additively manufactured 316l using automated serial\nsectioning. Curr. Opin. Solid State Mater. Sci.24, 100819 (2020).\n7. Chapman, M. G. et al. AFRL additive manufacturing modeling series: challenge 4,\n3d reconstruction of an IN625 high-energy diffraction microscopy sample using\nmulti-modal serial sectioning.Integr. Mater. Manuf. Innov.10, 129– 141 (2021).\n8 . E c h l i n ,M .P . ,S t r a w ,M . ,R a n d o l p h ,S . ,F i l e v i c h ,J .&P o l l o c k ,T .M .T h eT r i B e a m\nsystem: femtosecond laser ablation in situ SEM. Mater. Charact. 100,1 – 12\n(2015).\n9. Garner, A. et al. Large-scale serial sectioning of environmentally assisted cracks in\n7xxx al alloys using femtosecond laser-PFIB.Mater. Charact. 188, 111890 (2022).\n10. Echlin, M. P. et al. Recent developments in femtosecond laser-enabled TriBeam\nsystems. JOM 73, 4258– 4269 (2021).\n11. Jangid, D. K. et al. Titanium 3d microstructure for physics-based generative\nmodels: a dataset and primer. In1st Workshop on the Synergy of Scientiﬁc and\nMachine Learning Modeling@ ICML2023(2023).\n12. Schwartz, A. J., Kumar, M., Adams, B. L. & Field, D. P. (eds.)Electron Backscatter\nDiffraction in Materials Science(Springer US, 2009).https://doi.org/10.1007/978-0-\n387-88136-2.\n13. Godaliyadda, G. M. D. P. et al. A framework for dynamic image sampling based on\nsupervised learning. IEEE Trans. Comput. Imaging4,1 – 16 (2018).\n14. Zhang, Y. et al. Reduced electron exposure for energy-dispersive spectroscopy\nusing dynamic sampling.Ultramicroscopy 184,9 0– 97 (2018).\n15. Tong, V. S., Knowles, A. J., Dye, D. & Britton, T. B. Rapid electron backscatter\ndiffraction mapping: painting by numbers.Mater. Charact. 147, 271– 279 (2019).\n16. Wang, Z., Chen, J. & Hoi, S. C. Deep learning for image super-resolution: a survey.\nIEEE Trans. Pattern Anal. Machine Intell.43, 3365– 3387 (2021).\n17. Dai, T., Cai, J., Zhang, Y., Xia, S.-T. & Zhang, L. Second-order attention network for\nsingle image super-resolution. In 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 11057 – 11066 https://doi.org/10.1109/\nCVPR.2019.01132 (IEEE, 2019).\n18. Zamir, S. W. et al. Restormer: ef ﬁcient transformer for high-resolution image\nrestoration. In2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR),5 7 1 8– 5729 https://doi.org/10.1109/CVPR52688.2022.00564(IEEE, 2022).\n19. Jangid, D. K. et al. Adaptable physics-based super-resolution for electron back-\nscatter diffraction maps.npj Comput. Mater.8, 255 (2022).\n20. Parcollet, T. et al. Quaternion convolutional neural networks for end-to-end\nautomatic speech recognition.Interspeech 2018. https://api.semanticscholar.org/\nCorpusID:49325027 (2018).\n21. Trabelsi, C. et al. Deep complex networks. In 6th International Conference on\nLearning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3,\n2018, Conference Track Proceedings . https://openreview.net/forum?\nid=H1T2hmZAb (OpenReview.net, 2018).\n22. Aizenberg, I. & Gonzalez, A. Image recognition using MLMVN and frequency\ndomain features. In2018 International joint conference on neural networks (IJCNN),\n1– 8 https://doi.org/10.1109/IJCNN.2018.8489301 (IEEE, 2018).\n23. Matsui, N., Isokawa, T., Kusamichi, H., Peper, F. & Nishimura, H. Quaternion neural\nnetwork with geometrical operators.J. Intell. Fuzzy Syst.15, 149– 164 (2004).\n24. Kusamichi, H., Isokawa, T., Matsui, N., Ogawa, Y. & Maeda, K. A new scheme for\ncolor night vision by quaternion neural network. In Proceedings of the 2nd\nD.K. Jangid et al.\n9\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2024)    27 \nInternational Conference on Autonomous Robots and Agents (IEEE ICARA, 2004),\nVol. 1315 (IEEE ICARA, 2004).\n25. Isokawa, T., Matsui, N. & Nishimura, H. Quaternionic neural networks: funda-\nmental properties and applications. inComplex-valued neural networks: utilizing\nhigh-dimensional parameters, 411– 439 (IGI global, 2009).\n26. Yun, X. & Bachmann, E. R. Design, implementation, and experimental results of a\nquaternion-based kalman ﬁlter for human body motion tracking. IEEE Trans.\nRobot. 22, 1216– 1227 (2006).\n27. Shoemake, K. Animating rotation with quaternion curves. InProceedings of the\n12th Annual Conference on Computer Graphics and Interactive Techniques ,\n245– 254 https://doi.org/10.1145/325334.325242 (Association for Computing\nMachinery, New York, NY, USA, 1985).\n28. Pletinckx, D. Quaternion calculus as a basic tool in computer graphics.Visual\nComput. 5,2 – 13 (1989).\n29. Zhu, X., Xu, Y., Xu, H. & Chen, C. Quaternion convolutional neural networks. In\nComputer Vision – ECCV 2018: 15th European Conference, Munich, Germany, Sep-\ntember 8-14, 2018, Proceedings, Part VIII, 645– 661 https://doi.org/10.1007/978-3-\n030-01237-3_39 (Springer-Verlag, Berlin, Heidelberg, 2018).\n30. Vaswani, A. et al. Attention is all you need. InProceedings of the 31st International\nConference on Neural InformationProcessing Systems, Vol. 30, 6000– 6010 (Curran\nAssociates Inc., Long Beach, California, USA, 2017).\n31. Liang, J. et al. SwinIR: Image restoration using swin transformer. In2021 IEEE/CVF\nInternational Conference on Computer Vision Workshops (ICCVW ), 1833 – 1844\nhttps://doi.org/10.1109/ICCVW54120.2021.00210 (IEEE, 2021).\n32. Parcollet, T., Morchid, M. & Linarès, G. A survey of quaternion neural networks.\nArtif. Intell. Rev.53, 2957– 2982 (2020).\n33. Gaudet, C. J. & Maida, A. S. Deep quaternion networks. In2018 International Joint\nConference on Neural Networks (IJCNN),1 – 8 (IEEE, 2018).\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classiﬁcation with deep\nconvolutional neural networks.Commun. ACM 60,8 4– 90 (2017).\n35. De Leo, S. & Rotelli, P. Local hypercomplex analyticity. arXiv preprint funct-an/\n9703002 (1997).\n36. Isokawa, T., Nishimura, H. & Matsui, N. Quaternionic multilayer perceptron with\nlocal analyticity. Information 3, 756– 770 (2012).\n37. Ujang, B. C., Jahanchahi, C., Took, C. C. & Mandic, D. Quaternion valued neural\nnetworks and nonlinear adaptiveﬁlters. IEEE Trans. Neural Netw(2010).\n38. Arena, P., Fortuna, L., Occhipinti, L. & Xibilia, M. G. Neural networks for\nquaternion-valued function approximation. InProc. IEEE International Symposium\non Circuits and Systems-ISCAS’94, vol. 6, 307– 310 (IEEE, 1994).\n39. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers for image\nrecognition at scale. International Conference on Learning Representations ICLR.\nhttps://openreview.net/forum?id=YicbFdNTTy (2021).\n40. Hendrycks, D. & Gimpel, K. Gaussian error linear units (gelus). arXiv: Learning.\nhttps://arxiv.org/abs/1606.08415 (2016).\n41. Shi, W. et al. Real-time single image and video super-resolution using an efﬁcient\nsub-pixel convolutional neural network. In 2016 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 1874 – 1883 https://doi.org/10.1109/\nCVPR.2016.207 (IEEE, 2016).\n42. Lim, B., Son, S., Kim, H., Nah, S. & Lee, K. M. Enhanced deep residual networks for\nsingle image super-resolution. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition Workshops (CVPRW) , 1132 – 1140 https://doi.org/10.1109/\nCVPRW.2017.151 (IEEE, 2017).\n43. Niu, B. et al. Single image super-resolution via a holistic attention network. in\nEuropean Conference on Computer Vision, 191– 207 (Springer, 2020).\n44. Jung, J. et al. Super-resolving material microstructure image via deep learning for\nmicrostructure characterization and mechanical behavior analysis.npj Comput.\nMater. 7, 96 (2021).\n45. Jangid, D. K. et al. 3d grain shape generation in polycrystals using generative\nadversarial networks. Integr. Mater. Manuf. Innov.11,7 1– 84 (2022).\n46. Oddershede, J. et al. Non-destructive characterization of polycrystalline materials\nin 3d by laboratory diffraction contrast tomography.Integr. Mater. Manuf. Innov.\n8, 217– 225 (2019).\n47. Bachmann, F., Bale, H., Gueninchault, N., Holzner, C. & Lauridsen, E. M. 3d grain\nreconstruction from laboratory diffraction contrast tomography.J. Appl. Crystal-\nlogr. 52, 643– 651 (2019).\n48. Groeber, M. A. & Jackson, M. A. DREAM.3D: a digital representation environment for\nthe analysis of microstructure in 3D.Integr. Mater. Manuf. Innov.3,5 6– 72 (2014).\nACKNOWLEDGEMENTS\nThis research is supported in part by NSF award SI2-SSI #1664172. N.R.B. and S.H.D.\ngratefully acknowledge ﬁnancial support from NSWC Grant (N00174-22-1-0020).\nThe authors gratefully acknowledge P atrick Callahan, Toby Francis, Andrew\nPolonsky, and Joseph Wendorf for collection of the 3D Ti-6Al-4V and Ti-7Al\ndatasets. The authors acknowledge A.S.M Ifthekar, Raphael Ruschel and Satish\nKumar for their feedback on the paper. The MRL Shared Experimental Facilities\nare supported by the MRSEC Program of the NSF under Award No. DMR 2308708;\na member of the NSF-funded Materials Research Facilities Network\n(www.mrfn.org ). Use was also made of computational facilities purchased with\nfunds from the National Science Foundation (CNS-1725797) and administered by\nthe Center for Scientiﬁc Computing (CSC). The CSC issupported by the California\nNanoSystems Institute and the Materials Research Science and Engineering\nCenter (MRSEC; NSF DMR 2308708) at UC Santa Barbara. Use was made of the\ncomputational facilities purchased with funds from the National Science\nFoundation CC* Compute grant (OAC-1925717) and administered by the Center\nfor Scientiﬁc Computing (CSC). The ONR Grant N00014-19-2129 is also acknowl-\nedged for the titanium datasets. The authors of this work declare no competing\nﬁnancial or non-ﬁnancial interests.\nAUTHOR CONTRIBUTIONS\nD.K. Jangid: development of complete framework; design and coding of\ncomplete network architecture; prep rocessing of datasets for training the\nnetwork; all experiments to make our idea work; qualitative and quantitative\nevaluation of results; creating all ﬁgures; manuscript preparation. N.R. Brodnik:\ninitial preparation of training, validation, and test datasets; feedback on\ngenerated results, mentoring, concepti on of ideas, manuscript preparation.\nM.P. Echlin: preparation of datasets, conception of ideas, mentoring, feedback on\ngenerated results from network archi tecture, manuscript preparation.\nC. Gudavalli: deployment of QRBSA module on BisQue cyber infrastructure.\nC. Levenson: deployment of QRBSA module on BisQue cyber infrastructure. S.H.\nDaly (Co-PI): conception of ideas, mentor ing, funding acquisition, manuscript\npreparation. T.M. Pollock (Co-PI): conception of ideas, mentoring, funding\nacquisition, manuscript preparation. B.S. Manjunath (Co-PI): conception of ideas,\nmentoring, funding acquisition, manuscr ipt preparation. All authors contributed\nto the writing of this manuscript with writing efforts being led by D.K. Jangid, N.R.\nBrodnik and M.P. Echlin.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41524-024-01209-6.\nCorrespondence and requests for materials should be addressed to Devendra K.\nJangid or B. S. Manjunath.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2024\nD.K. Jangid et al.\n10\nnpj Computational Materials (2024)    27 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences"
}