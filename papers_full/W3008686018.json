{
  "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
  "url": "https://openalex.org/W3008686018",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2099832880",
      "name": "Adam Roberts",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001631461",
      "name": "Colin Raffel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2496873187",
      "name": "Noam Shazeer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3006439205",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2604368306",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2998901379",
    "https://openalex.org/W2989312920",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3016309009",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W2951365061",
    "https://openalex.org/W2991382858",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2612431505",
    "https://openalex.org/W2947497897",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2973113793",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2131965512",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2973123473",
    "https://openalex.org/W3015883388",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2787560479"
  ],
  "abstract": "It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models at https://goo.gle/t5-cbqa.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5418–5426,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n5418\nHow Much Knowledge Can You Pack\nInto the Parameters of a Language Model?\nAdam Roberts∗\nGoogle\nadarob@google.com\nColin Raffel∗\nGoogle\ncraffel@gmail.com\nNoam Shazeer\nGoogle\nnoam@google.com\nAbstract\nIt has recently been observed that neural lan-\nguage models trained on unstructured text can\nimplicitly store and retrieve knowledge using\nnatural language queries. In this short pa-\nper, we measure the practical utility of this\napproach by ﬁne-tuning pre-trained models to\nanswer questions without access to any exter-\nnal context or knowledge. We show that this\napproach scales with model size and performs\ncompetitively with open-domain systems that\nexplicitly retrieve answers from an external\nknowledge source when answering questions.\nTo facilitate reproducibility and future work,\nwe release our code and trained models.1\n1 Introduction\nBig, deep neural language models that have been\npre-trained on unlabeled text have proven to be\nextremely performant when ﬁne-tuned on down-\nstream Natural Language Processing (NLP) tasks\n(Devlin et al., 2018; Yang et al., 2019; Liu et al.,\n2019; Lan et al., 2019; Raffel et al., 2019). In-\nterestingly, it has also recently been observed that\nthese models can internalize a sort of implicit\n“knowledge base” after pre-training (Petroni et al.,\n2019; Jiang et al., 2019; Talmor et al., 2019).\nThis behavior is potentially useful because 1) the\nknowledge is built up by pre-training on unstruc-\ntured and unlabeled text data, which is freely avail-\nable in huge quantities on the Internet (Raffel\net al., 2019; Wenzek et al., 2019), and 2) it is pos-\nsible to retrieve information using informal natural\nlanguage queries since these pre-trained language\nmodels excel when ﬁne-tuned on natural language\nunderstanding tasks.\n∗ Equal contribution. Noam suggested trying T5 on\nopen-domain QA and coded and ran initial experiments on\nTriviaQA showing improved performance with model size.\nAdam wrote the code and ran most experiments. Colin set the\nresearch scope, wrote the paper, and ran a few experiments.\n1https://goo.gle/t5-cbqa\nPresident Franklin <M> born <M> January 1882.\nOur <M> hand-picked and sun-dried \n<M> orchard in Georgia.\nLily couldn't <M>. The waitress \nhad brought the largest <M> of \nchocolate cake <M> seen. T5\nD. Roosevelt was <M> in\nbelieve her eyes <M> \npiece <M> she had ever\npeaches are <M> at our\nWhen was Franklin D. \nRoosevelt born? T5 1882\nPresident Franklin D. \nRoosevelt was born\nin January 1882.\nPre-training\nFine-tuning\nFigure 1: T5 is pre-trained to ﬁll in dropped-out spans\nof text (denoted by <M>) from documents in a large,\nunstructured text corpus. We ﬁne-tune T5 to answer\nquestions without inputting any additional information\nor context. This forces T5 to answer questions based on\n“knowledge” that it internalized during pre-training.\nPast work investigating “language models as\nknowledge bases” has typically tried to under-\nstand the scope of the information stored in the\nmodel using synthetic tasks that are similar to the\npre-training objective (Petroni et al., 2019; Jiang\net al., 2019) and/or measure reasoning capabili-\nties (Talmor et al., 2019). In this work, we take\na different approach by evaluating the capability\nof language models on the practical task of open-\ndomain question answering – speciﬁcally, we ﬁne-\ntune the model to answer questionswithout access\nto any external knowledge or context. To do so,\nthe model must parse a natural language query and\n“look up information” stored in its parameters.\nMost past work on question answering either\nexplicitly feeds pertinent information to the model\nalongside the question (for example, an article that\ncontains the answer (Rajpurkar et al., 2016; Zhang\net al., 2018; Khashabi et al., 2018; Clark et al.,\n2019)) or allows the model to retrieve informa-\ntion from an external knowledge source (Berant\net al., 2013; Chen et al., 2017). By feeding the\nmodel the input question alone, we can determine\nhow much knowledge it has stored in its param-\n5419\neters while measuring its performance on a use-\nful real-world problem. We refer to this task as\n“closed-book question answering”.\nA separate question we address in this work\nis whether models with more parameters end up\nstoring more information. It has been shown\nthat transfer learning performance on many down-\nstream tasks tends to improve as the model size\nand amount of unsupervised pre-training increases\n(Radford et al., 2019; Liu et al., 2019; Raffel et al.,\n2019). In this work, we leverage the pre-trained\n“T5” models released by Raffel et al. (2019), the\nlargest of which has around 11 billion parameters.\nBy measuring knowledge retrieval capabilities on\nmodels of various sizes – including models that\nhave an order of magnitude more parameters than\nconsidered in past work – we can explore how well\nour approach scales.\n2 Background\nQuestion Answering The task of training a\nmodel to either select or output the correct answer\nto a given question is referred to as “question an-\nswering”. The most popular variant of this task\nfeeds the model some “context” containing the an-\nswer (for example, a paragraph from an encyclo-\npedia article) alongside the question (Rajpurkar\net al., 2016; Zhang et al., 2018; Khashabi et al.,\n2018; Clark et al., 2019). Models can be trained\neither to indicate the span of the context that con-\ntains the answer or output the text of the answer\nitself. Since this format can be seen as reading\nsome text and answering a question about it, it has\nbeen referred to as “reading comprehension”.\nA more difﬁcult variant is “open-domain ques-\ntion answering” (Prager, 2006), where the model\ncan be asked arbitrary context-independent ques-\ntions (e.g. well-known facts or historical details).\nIt is typically assumed that the model can access\nan external collection of knowledge when answer-\ning questions (e.g. a structured knowledge base\nor unstructured text corpus), but the model is not\ngiven any information about where in the collec-\ntion the answer appears. The reading comprehen-\nsion task can be considered a simpliﬁed version of\nopen-domain question answering where the model\nis provided with the oracle context to answer a\ngiven question. As an analogy, the open-domain\nquestion answering system acts as if it is taking an\nopen-book exam where it can ﬁnd and use infor-\nmation in an external source of knowledge.2\nIn this work, we consider open-domain ques-\ntion answering with the additional constraint that\nthe model is not allowed to access any external\nknowledge whatsoever when answering questions.\nInstead, the model itself must be pre-trained to\nstore knowledge in its parameters before being\nﬁne-tuned to answer questions. In one view, this\ncan be seen as an alternative way to approach\nopen-domain question answering where instead of\nlearning to access external knowledge the model\nneeds to have “memorized” it in order to answer\nquestions; in another view, this constraint creates\na third and potentially more ambitious variant of\nthe question answering task. A model that answers\nquestions in this way is metaphorically similar to\na student taking a closed-book exam, where the\nstudent must study and memorize all pertinent in-\nformation before taking the test.\nTransfer Learning with Language ModelsIn\nthe past few years, it has become increasingly\ncommon to pre-train a language model using an\nunsupervised objective on a large, unstructured\ntext corpus before ﬁne-tuning it on a downstream\ntask of interest (Dai and Le, 2015; Howard and\nRuder, 2018; Radford et al., 2018). The pop-\nularity of this form of “transfer learning” is at-\ntributable to its empirical success on many NLP\ntasks (Peters et al., 2018; Devlin et al., 2018; Yang\net al., 2019; Lan et al., 2019; Raffel et al., 2019).\nLoosely speaking, the pre-training step may pro-\nvide the model with some generally-useful aware-\nness of meaning, syntax, and “world knowledge”.\nIn question answering in particular, most state-of-\nthe-art systems use some form of transfer learning.\nCurrently, the most popular model architectures\nused in transfer learning for NLP are Transformer-\nbased (Vaswani et al., 2017) “encoder-only” mod-\nels like BERT (Devlin et al., 2018). These\nmodels can produce a single prediction for each\ninput token and have been applied to reading\ncomprehension-style question answering by pre-\ndicting which tokens of the context contain the an-\nswer. Encoder-only models are not applicable to\nclosed-book question answering because no con-\ntext is provided to extract the answer span from.\nAn alternative to encoder-only models, recently\nadvocated by Raffel et al. (2019), is to treat ev-\n2While our deﬁnition of open-book is the same as in the\nOpenBookQA dataset introduced by Mihaylov et al. (2018),\nwe do not directly address multi-hop inference in this work.\n5420\nery NLP task as a text-to-text problem using an\nencoder-decoder Transformer. When this frame-\nwork is applied to question answering, the model\nis trained to generate the literal text of the answer\nin a free-form fashion. Despite the potential dif-\nﬁculty of generating rather than extracting the an-\nswer, Raffel et al. (2019) demonstrated state-of-\nthe-art results on the SQuAD (Rajpurkar et al.,\n2016), MultiRC (Khashabi et al., 2018), BoolQ\n(Clark et al., 2019), and ReCoRD (Zhang et al.,\n2018) reading comprehension tasks.\nThe text-to-text framework is directly applica-\nble to closed-book question answering since the\nmodel can be trained to generate an answer with\nor without any additional information in its input.\nCrucially, ﬁne-tuning a text-to-text model to an-\nswer questions without any context requires that\nthe model retrieve information from its parame-\nters that it learned during pre-training. Radford\net al. (2019) considered a similar task to evalu-\nate the zero-shot question answering capabilities\nof a language model. The concurrent “RELIC”\nand “EAE” models of Ling et al. (2020) and F´evry\net al. (2020) learn representations for an explic-\nitly predeﬁned set of entities and are evaluated on\nthe same closed-book variant of TriviaQA that we\nconsider. Relatedly, Petroni et al. (2019) show\nthat it is possible to manually convert some ques-\ntions to a ﬁll-in-the-blank format amenable to an\nencoder-only model (e.g. “Who developed the the-\nory of relativity?” gets mapped to “The theory of\nrelativity was developed by ”).\n3 Experiments\nDatasets We consider the following open-\ndomain question answering datasets: Natural\nQuestions (Kwiatkowski et al., 2019), a dataset\nof questions from web queries, each accompanied\nby a Wikipedia article containing the answer; We-\nbQuestions (Berant et al., 2013), comprising ques-\ntions from web queries matched to correspond-\ning entries in FreeBase (Bollacker et al., 2008);\nand TriviaQA (Joshi et al., 2017), a collection of\nquestions from quiz league websites where each\nquestion is accompanied by pages from web and\nWikipedia searches that may contain the answer.\nIn this work, we only make use of the ques-\ntions from each dataset –we completely ignore the\nmatching documents supplied for each question.\nFor WebQuestions and TriviaQA we follow the\nstandard evaluation procedures where each pre-\ndicted answer is compared to the ground-truth\nafter both are lowercased and stripped of arti-\ncles, punctuation, and duplicate whitespace (Ra-\njpurkar et al., 2016). For Natural Questions,\nwe evaluate using both 1) the standard “open-\ndomain” version as used e.g. by (Lee et al., 2019;\nMin et al., 2019b,a; Asai et al., 2019) where the\nmodel is only required to produce a single nor-\nmalized answer and 2) the standard multi-answer\nvariant used with reading comprehension systems\n(Kwiatkowski et al., 2019). We review the details\nof Natural Questions evaluation in appendix A.\nNote that Natural Questions and TriviaQA have\nprivate test sets, so standard practice on their open-\ndomain variants is to report performance on the\ndevelopment sets. However, we also include our\nresults on the ofﬁcial TriviaQA test set by ﬁne-\ntuning on the unﬁltered training set and submitting\nour test set predictions to the leaderboard for the\nWikipedia domain. We urge future work to adopt\nthis approach to help ensure the validity of results\nand avoid potentially overﬁtting to a public set.\nTraining We leverage the pre-trained models\nprovided by Raffel et al. (2019), referred to as\nthe “Text-to-Text Transfer Transformer” (T5). The\noriginal T5 models were pre-trained on a multi-\ntask mixture including an unsupervised “span cor-\nruption” task on the C4 dataset as well as super-\nvised translation, summarization, classiﬁcation,\nand reading comprehension tasks. Note that none\nof the reading comprehension datasets used for\npre-training T5 overlap with the question answer-\ning datasets that we consider in this paper. In order\nto measure how performance scales with model\nsize, we perform experiments with the Base (220\nmillion parameters), Large (770 million), 3B (3\nbillion), and 11B (11 billion) variants of T5. Given\nthat the T5 models were pre-trained on a multitask\nmixture including question answering, we also re-\nport performance using the “T5.1.1” checkpoints,\nwhich were pre-trained on unlabeled data only.3\nFor ﬁne-tuning the T5 checkpoints, we follow\nthe procedure used in Raffel et al. (2019) with-\nout any additional hyperparameter tuning: We\nuse the AdaFactor optimizer (Shazeer and Stern,\n2018) with a constant learning rate of 0.001, 10%\ndropout rate, and a batch size of 196,608 tokens.\nWe halve the batch and double the dropout rate\nfor WebQuestions due to its small size. For the\nT5.1.1 checkpoints, we follow the same procedure\n3https://goo.gle/t5-checkpoints\n5421\nbut with a dropout rate of5% for all three datasets.\nFor evaluation, we follow the procedure used in\nLee et al. (2019): for each dataset, we hold out\n10% of the training set as a validation split, ﬁne-\ntune a model from the remaining 90% of exam-\nples, and select the best-performing checkpoint for\nﬁnal evaluation on the test set. While we chose to\ntrain for 20,000 steps, our validation accuracy typ-\nically plateaued after only a few hundred steps and\nshowed no signs of overﬁtting.\nWe decode the model’s predictions by choosing\nthe most likely token at each timestep. To map\nquestion answering tasks to the text-to-text format,\nwe simply feed the question with a task-speciﬁc\npreﬁx into the model as input and train it to predict\nthe literal answer text as output.\nSalient Span Masking Recently, Guu et al.\n(2020) found that a “salient span masking” (SSM)\npre-training objective produced substantially bet-\nter results in open-domain question answering.\nThis approach ﬁrst uses BERT (Devlin et al.,\n2018) to mine sentences that contain salient spans\n(named entities and dates) from Wikipedia. The\nquestion answering model is then pre-trained to re-\nconstruct masked-out spans from these sentences,\nwhich Guu et al. (2020) hypothesize helps the\nmodel “focus on problems that require world\nknowledge”. We experimented with using the\nsame SSM data and objective to continue pre-\ntraining the T5 checkpoints for100,000 additional\nsteps before ﬁne-tuning for question answering.\nResults Our results on the open-domain Natural\nQuestions, WebQuestions, and TriviaQA tasks are\nshown in table 1. Notably, performance on each\ndataset improves as the model size increases, with\neither T5-11B or the comparably-sized T5.1.1-\nXXL (pre-trained only on unlabeled data) per-\nforming best in every case. Further, we ﬁnd that\nusing Guu et al. (2020)’s SSM pre-training pro-\nduces a substantial boost in performance. T5.1.1-\nXXL with SSM ultimately achieves state-of-the-\nart on WebQuestions and our largest models beat\nmost other methods on Natural Questions and\nTriviaQA. Importantly, all previous methods ex-\ncept Ling et al. (2020) and F ´evry et al. (2020)\noperate in the “open-book” setting by explicitly\nretrieving and using information from an exter-\nnal knowledge source. While our largest models\nare computationally intensive, we note that most\nopen-domain question answering systems must\nTable 1: Scores achieved by ﬁne-tuning T5 on the\nopen-domain Natural Questions (NQ), WebQuestions\n(WQ), and TriviaQA (TQA) tasks.\nNQ WQ TQA\ndev test\nChen et al. (2017) – 20.7 – –\nLee et al. (2019) 33.3 36.4 47.1 –\nMin et al. (2019a) 28.1 – 50.9 –\nMin et al. (2019b) 31.8 31.6 55.4 –\nAsai et al. (2019) 32.6 – – –\nLing et al. (2020) – – 35.7 –\nGuu et al. (2020) 40.4 40.7 – –\nF´evry et al. (2020) – – 43.2 53.4\nKarpukhin et al. (2020) 41.5 42.4 57.9 –\nT5-Base 25.9 27.9 23.8 29.1\nT5-Large 28.5 30.6 28.7 35.9\nT5-3B 30.4 33.6 35.1 43.4\nT5-11B 32.6 37.2 42.3 50.1\nT5-11B + SSM 34.8 40.8 51.0 60.5\nT5.1.1-Base 25.7 28.2 24.2 30.6\nT5.1.1-Large 27.3 29.5 28.5 37.2\nT5.1.1-XL 29.5 32.4 36.0 45.1\nT5.1.1-XXL 32.8 35.6 42.9 52.5\nT5.1.1-XXL + SSM 35.2 42.8 51.9 61.6\nﬁrst do an expensive lookup step over the entire\nknowledge corpus and then attend to a long doc-\nument to extract an answer. Our approach omits\nboth of these steps, which ultimately saves a large\namount of computation and memory.\nHaving established that our approach is com-\npetitive on open-domain question answering, we\nnow evaluate it on the standard (and more difﬁ-\ncult) multi-answer variant of Natural Questions.\nVirtually all models used on this task are read-\ning comprehension systems that select the correct\nanswer from an oracle context. After ﬁne-tuning,\nT5-11B + SSM achieves a recall of 36.2 on the\nvalidation set, which lags behind the state-of-the-\nart score of 51.9 from Pan et al. (2019) 4 but out-\nperforms the best baseline published alongside the\ndataset (recall of 33.2 (Kwiatkowski et al., 2019)).\nThis shows that T5 can effectively answer ques-\ntions with multiple answers. We discuss additional\nexperiments and negative results in appendix B.\nHuman Evaluation The benchmarks we used\nand the “exact match” score assume that the model\ndirectly extracts answers from an external knowl-\nedge source. In contrast, our model generates\nanswers in a free-form fashion. We hypothesize\nthat this results in many false negatives when an-\n4Validation set recall scores from Pan et al. (2019) were\nreported in private correspondence with the authors.\n5422\nTable 2: A breakdown of the 150 hand-evaluated examples from Natural Questions where the T5 predictions were\nlabelled as incorrect by the automatic procedure. We found only 62% of these to be true positives.\nExample\nCategory Percentage Question Target(s) T5 Prediction\nTrue Negative 62.0% what does the ghost of christmas\npresent sprinkle from his torch\nlittle warmth, warmth confetti\nPhrasing Mismatch 13.3% who plays red on orange is new\nblack\nkate mulgrew katherine kiernan\nmaria mulgrew\nIncomplete Annotation 13.3% where does the us launch space\nshuttles from\nﬂorida kennedy lc39b\nUnanswerable 11.3% who is the secretary of state for\nnorthern ireland\nkaren bradley james brokenshire\nswers do not exactly match the ground-truth con-\ntext intended for each question. We therefore man-\nually inspected 150 examples from the Natural\nQuestions validation set where our model’s pre-\ndiction was counted as incorrect in hopes of iden-\ntifying “false negatives” according to the exact\nmatch metric. We found that false negatives fell\ninto three broad categories: First, answers with\nmeaning-preserving differences in phrasing (e.g.\n“April 15” vs. “April 15th”); second, questions\nthat were missing all possible correct answers (e.g.\n“where does the us launch space shuttles from”\nwas annotated with the single ground-truth an-\nswer “ﬂorida”, despite many possible correct an-\nswers such as “Kennedy Space Center”, “Merritt\nIsland”, “Cape Canaveral”, etc.); and ﬁnally, some\nquestions were unanswerable without knowing the\nexact time or article they referred to (e.g. “what\nis the latest version of microsoft ofﬁce 2010” de-\npends on when the question is being asked). We\nprovide examples of each of these false negative\ntypes in table 2. We note that open-book ques-\ntion answering systems could also be impacted to\na lesser extent by these issues (e.g. if they select a\nslightly different answer span from the annotated\none or retrieve a non-golden document that con-\ntains a different correct answer).\nOf the 150 examples inspected, we found that\n20 were marked as incorrect due to differences in\nphrasing, another 20 were not annotated with all\ncorrect answers, and 17 were unanswerable with-\nout appropriate context. Removing unanswerable\nquestions from the validation set and recomputing\nour model’s accuracy based on this false-negative\nrate produces a score of 57.8. This suggests that\nthe performance of closed-book question answer-\ning systems (in terms of how often it correctly an-\nswers questions) is substantially underestimated\nby the evaluation procedure used in these bench-\nmarks. For full transparency, we publicly release\nthe results of our human evaluation and include an\nappropriate reference when we determined that a\npredicted answer was missing from ground-truth.5\n4 Conclusion\nIn this short paper, we have shown that large lan-\nguage models pre-trained on unstructured text can\nattain competitive results on open-domain ques-\ntion answering benchmarks without any access\nto external knowledge. This suggests a funda-\nmentally different approach to designing question\nanswering systems, motivating many threads for\nfuture work: First, we obtained state-of-the-art\nresults only with the largest models which had\naround 11 billion parameters. This model size can\nbe prohibitively expensive in resource-constrained\nsettings, prompting future work on more efﬁcient\nlanguage models. Second, “open-book” models\ntypically provide some indication of what infor-\nmation they accessed when answering a question.\nThis can provide a useful form of interpretabil-\nity. In contrast, our model distributes knowledge\nin its parameters in an inexplicable way and hal-\nlucinates realistic-looking answers when it is un-\nsure. Third, the maximum-likelihood objective\nused to train our model provides no guarantees as\nto whether a model will learn a fact or not. This\nmakes it difﬁcult to ensure that the model obtains\nspeciﬁc knowledge over the course of pre-training\nand prevents us from explicitly updating or remov-\ning knowledge from a pre-trained model. Finally,\nthe tasks we used in this paper mainly measure\n“trivia”-style knowledge. We are therefore inter-\nested in measuring performance on question an-\nswering tasks that require reasoning capabilities\nsuch as DROP (Dua et al., 2019).\n5https://goo.gle/t5-cbqa-human-eval\n5423\nAcknowledgments\nWe thank Kelvin Guu, Kenton Lee, Ming-Wei\nChang, Zora Tung, and Ice Pasupat for providing\nthe open-domain question answering evaluation\nsetup and access to their salient span-annotated\ndata; Roy Frostig and Katherine Lee for comments\nand suggestions on this manuscript; Noah Con-\nstant for suggesting we try salience span masking;\nand Monica Dinculescu for building an interactive\ndemonstration of our results.6\nReferences\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2019. Learn-\ning to retrieve reasoning paths over Wikipedia\ngraph for question answering. arXiv preprint\narXiv:1911.10470.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the 2008 ACM\nSIGMOD International Conference on Management\nof Data, pages 1247–1250.\nDanqi Chen, Adam Fisch, Jason Weston, and An-\ntoine Bordes. 2017. Reading Wikipedia to an-\nswer open-domain questions. arXiv preprint\narXiv:1704.00051.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised sequence learning. In Advances in Neu-\nral Information Processing Systems.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDrop: A reading comprehension benchmark re-\nquiring discrete reasoning over paragraphs. arXiv\npreprint arXiv:1903.00161.\n6http://t5-trivia.glitch.me/\nThibault F ´evry, Livio Baldini Soares, Nicholas\nFitzGerald, Eunsol Choi, and Tom Kwiatkowski.\n2020. Entities as experts: Sparse memory ac-\ncess with entity supervision. arXiv preprint\narXiv:2004.07202.\nKelvin Guu, Kenton Lee, Zora Tung, Pasupat\nPanupong, and Ming-Wei Chang. 2020. Realm:\nRetrieval-augmented language model pre-training.\narXiv preprint arXiv:2002.08909.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2019. How can we know what language\nmodels know? arXiv preprint arXiv:1911.12543.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. arXiv preprint arXiv:1705.03551.\nVladimir Karpukhin, Barlas Ouguz, Sewon Min,\nLedell Yu Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof North American Chapter of the Association for\nComputational Linguistics (NAACL).\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics, 7.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A lite BERT for self-supervised\nlearning of language representations. arXiv preprint\narXiv:1909.11942.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised\nopen domain question answering. arXiv preprint\narXiv:1906.00300.\nJeffrey Ling, Nicholas FitzGerald, Zifei Shan,\nLivio Baldini Soares, Thibault F ´evry, David Weiss,\nand Tom Kwiatkowski. 2020. Learning cross-\ncontext entity representations from text. arXiv\npreprint arXiv:2001.03765.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\n5424\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In EMNLP.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\nproach for weakly supervised question answering.\narXiv preprint arXiv:1909.04849.\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019b. Knowledge guided text\nretrieval and reading for open domain question an-\nswering. arXiv preprint arXiv:1911.03868.\nLin Pan, Rishav Chakravarti, Anthony Ferritto,\nMichael Glass, Alﬁo Gliozzo, Salim Roukos, Radu\nFlorian, and Avirup Sil. 2019. Frustratingly\neasy natural question answering. arXiv preprint\narXiv:1909.05286.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nFabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066.\nJohn Prager. 2006. Open-domain question-answering.\nFoundations and Trends in Information Retrieval,\n1(2).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\narXiv preprint arXiv:1804.04235.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019. olmpics–on what lan-\nguage model pre-training captures. arXiv preprint\narXiv:1912.13283.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzman, Ar-\nmand Joulin, and Edouard Grave. 2019. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. arXiv preprint arXiv:1911.00359.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019. XLNet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nReCoRD: Bridging the gap between human and ma-\nchine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885.\n5425\nA Metrics for Natural Questions\nCompared to WebQuestions and TriviaQA, Nat-\nural Questions is distributed with a much richer\nset of annotations: Each question can be annotated\neither as unanswerable (given the oracle context),\nwith a short answer, or with a yes/no answer; ques-\ntions in the validation set can be annotated more\nthan once; and some questions have multiple an-\nswers (e.g. “Who are the members of the Beat-\nles?” has four answers). We consider two vari-\nants of Natural Questions. In both cases, we omit\nthe “unanswerable” label and long answers, which\nare nearly impossible to predict without the oracle\ncontext.\nThe ﬁrst variant is the standard “open-domain”\nversion as used e.g. by (Lee et al., 2019; Min et al.,\n2019b,a; Asai et al., 2019), where 1) the model is\nonly ever trained to output a single answer; 2) if\na question has multiple answers, it is only trained\nto predict the ﬁrst answer; 3) any questions with\nanswers longer than ﬁve tokens are ignored; 4)\nanswers are normalized before being compared\n(in the same manner as is typically done for We-\nbQuestions and SQuAD); and 5) a predicted an-\nswer is considered correct if it matches any of the\nanswers provided by any of the annotators (e.g.\n“Ringo Starr” would be considered a correct an-\nswer to “Who are the members of the Beatles?”).\nThe second variant closely matches the ofﬁcial\nevaluation procedure used by the Natural Ques-\ntions leaderboard, where our model is trained to\npredict all ground-truth answers and is only con-\nsidered correct if it predicts all answers for any\none of the annotators. As in the ofﬁcial evalua-\ntion, we consider questions with fewer than two\nnon-null annotations unanswerable (given the con-\ntext), but because we cannot predict unanswerabil-\nity without the context, we only report the recall\nscore. Further, because our model does not have\naccess to the oracle context, we also normalize\npredicted and ground-truth answers when compar-\ning them. The use of multiple possible answers\nalso required minor modiﬁcation of our text-to-\ntext format. In this case, we trained the model\nto output each answer delimited by the text “an-\nswer:” (for example, “answer: John Lennon an-\nswer: Ringo Starr answer: George Harrison an-\nswer: Paul McCartney”). We then split out each\nanswer from the model’s predictions as a postpro-\ncessing step before evaluating it against the set of\nanswers provided by each annotation.\nB Other Things We Tried\nIn the course of undertaking this study, we tried\nvarious ideas that ultimately did not improve per-\nformance. We brieﬂy discuss them here.\nContinued Pre-Training on WikipediaThe T5\ncheckpoints we used were primarily pre-trained on\nC4, a large and diverse dataset of unstructured web\ncontent. We were interested to see whether we\ncould improve performance by doing further pre-\ntraining on data that was better tailored to the tasks\nwe considered. Since both Natural Questions and\nTriviaQA source their answers from Wikipedia ar-\nticles, we experimented with further pre-training\non text data from English Wikipedia with the same\nunsupervised objective (“span corruption”) as was\nused by T5. We found that this additional “in-\ndomain” pre-training had virtually no effect on\nperformance. This may be because C4 already\ncontains many articles from Wikipedia and the T5\ncheckpoints were pre-trained long enough to see\nplenty of this content.\nPre-Training From Scratch On Wikipedia\nSince all of the answers to the questions in Nat-\nural Questions appeared in Wikipedia, we carried\nout an additional experiment where we pre-trained\nT5 from scratch only on data from Wikipedia. We\npre-trained on up to 1 trillion tokens (the same\namount the T5 checkpoints were pre-trained on)\nwith the span corruption objective and measured\nﬁne-tuned performance after various amounts of\npre-training. Unfortunately, this resulted in dra-\nmatically worse performance regardless of the\namount of pre-training. We suspect that this is be-\ncause Wikipedia is too small and results in detri-\nmental overﬁtting.\nSpan-Corruption Pre-Training on Wikipedia\nSentences with Salient Spans As described\npreviously, we observed signiﬁcant performance\ngains with additional pre-training using “salient\nspan masking” (SSM) on the Wikipedia sentence\ndataset from Guu et al. (2020) but not when using\nthe standard “span corruption” (SC) from Raffel\net al. (2019) on longer Wikipedia articles. While\nSC masks random spans of the input by dropping\n15% of its tokens (sampled each epoch) and re-\nplacing each consecutive span of dropped tokens\nwith a unique sentinel, SSM speciﬁcally masks out\none named entity or date in the input sentence.\nWe were interested in determining whether the\n5426\nFigure 2: Comparing additional pre-training using\neither salient span masking (SSM) or span corrup-\ntion (SC). We further pre-trained T5.1.1-XXL on the\nWikipedia sentence dataset from Guu et al. (2020) with\neach objective, ﬁne-tuning on a mixture of our three\nclosed-book QA tasks every 10,000 steps. For each\nﬁne-tuning run, we report the maximum exact match\nscore achieved on the validation set over 10,000 steps\nof ﬁne-tuning.\ngains achieved were attributable to the use of a\nmore task-speciﬁc dataset (pre-split into sentences\nthat are known to contain at least one entity) or if\nthe SSM objective itself was critical. As illustrated\nin ﬁg. 2, the SSM objective is clearly an important\ningredient in the improved performance; we saw\nno signiﬁcant improvement versus the baseline T5\nmodel when using SC.\nFine-Tuning On All Question Answering Tasks\nThe text-to-text framework used by T5 makes it\nsimple to train multitask models simply by sup-\nplying a different task-speciﬁc preﬁx for each task\nand concatenating all of the constituent datasets.\nSince all of the question answering tasks we con-\nsider in this study follow the same basic struc-\nture, we were hopeful that training on a multitask\nmixture of Natural Questions, WebQuestions, and\nTriviaQA would improve performance due to the\nadditional supervised data. While multitask train-\ning improved performance on the Natural Ques-\ntions by 0.5, it produced slightly worse results on\nthe other tasks.\nRandomly Sampling Answers For Natural\nQuestions In the open-domain variant of Natu-\nral Questions, the model is only trained to gener-\nate a single answer at a time. For the results pre-\nsented in the main text, when a question was anno-\ntated with multiple answers, we simply trained the\nmodel on the ﬁrst annotated answer. We also ex-\nperimented with sampling a random answer from\nthe set of possible answers for pre-training and\nfound that it did not affect performance.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8051602840423584
    },
    {
      "name": "Language model",
      "score": 0.6759521961212158
    },
    {
      "name": "Question answering",
      "score": 0.6357623338699341
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6134549975395203
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.6017946600914001
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5631389021873474
    },
    {
      "name": "Code (set theory)",
      "score": 0.5616586208343506
    },
    {
      "name": "Domain knowledge",
      "score": 0.538640558719635
    },
    {
      "name": "Natural language processing",
      "score": 0.5025556087493896
    },
    {
      "name": "Source code",
      "score": 0.48492127656936646
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45768606662750244
    },
    {
      "name": "Natural language",
      "score": 0.4170493483543396
    },
    {
      "name": "Machine learning",
      "score": 0.3383808732032776
    },
    {
      "name": "Data mining",
      "score": 0.198969304561615
    },
    {
      "name": "Programming language",
      "score": 0.1584390103816986
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 46
}