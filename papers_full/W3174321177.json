{
    "title": "RareBERT: Transformer Architecture for Rare Disease Patient Identification using Administrative Claims",
    "url": "https://openalex.org/W3174321177",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2891562448",
            "name": "PKS Prakash",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2646054846",
            "name": "Srinivas Chilukuri",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2224936915",
            "name": "Nikhil Ranade",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108562543",
            "name": "Shankar Viswanathan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2891562448",
            "name": "PKS Prakash",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2646054846",
            "name": "Srinivas Chilukuri",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2224936915",
            "name": "Nikhil Ranade",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108562543",
            "name": "Shankar Viswanathan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3040531546",
        "https://openalex.org/W6679187060",
        "https://openalex.org/W1825821140",
        "https://openalex.org/W6638023959",
        "https://openalex.org/W2123958887",
        "https://openalex.org/W2136213019",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2133227149",
        "https://openalex.org/W6677567047",
        "https://openalex.org/W2788085070",
        "https://openalex.org/W3017637887",
        "https://openalex.org/W2902313895",
        "https://openalex.org/W2920227719",
        "https://openalex.org/W2768348081",
        "https://openalex.org/W2972914320",
        "https://openalex.org/W2953707798",
        "https://openalex.org/W2965570621",
        "https://openalex.org/W4295725036",
        "https://openalex.org/W4294607651",
        "https://openalex.org/W2986191653",
        "https://openalex.org/W1757277845",
        "https://openalex.org/W2943998355",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W2942077940",
        "https://openalex.org/W2130652100",
        "https://openalex.org/W4250714916"
    ],
    "abstract": "A rare disease is any disease that affects a very small percentage (1 in 1,500) of population. It is estimated that there are nearly 7,000 rare disease affecting 30 million patients in the U. S. alone. Most of the patients suffering from rare diseases experience multiple misdiagnoses and may never be diagnosed correctly. This is largely driven by the low prevalence of the disease that results in a lack of awareness among healthcare providers. There have been efforts from machine learning researchers to develop predictive models to help diagnose patients using healthcare datasets such as electronic health records and administrative claims. Most recently, transformer models have been applied to predict diseases BEHRT, G-BERT and Med-BERT. However, these have been developed specifically for electronic health records (EHR) and have not been designed to address rare disease challenges such as class imbalance, partial longitudinal data capture, and noisy labels. As a result, they deliver poor performance in predicting rare diseases compared with baselines. Besides, EHR datasets are generally confined to the hospital systems using them and do not capture a wider sample of patients thus limiting the availability of sufficient rare dis-ease patients in the dataset. To address these challenges, we introduced an extension of the BERT model tailored for rare disease diagnosis called RareBERT which has been trained on administrative claims datasets. RareBERT extends Med-BERT by including context embedding and temporal reference embedding. Moreover, we introduced a novel adaptive loss function to handle the class imbal-ance. In this paper, we show our experiments on diagnosing X-Linked Hypophosphatemia (XLH), a genetic rare disease. While RareBERT performs significantly better than the baseline models (79.9% AUPRC versus 30% AUPRC for Med-BERT), owing to the transformer architecture, it also shows its robustness in partial longitudinal data capture caused by poor capture of claims with a drop in performance of only 1.35% AUPRC, compared with 12% for Med-BERT and 33.0% for LSTM and 67.4% for boosting trees based baseline.",
    "full_text": " \n \nRareBERT: Transformer Architecture for Rare Disease Patient Identifi-\ncation using Administrative Claims  \nPKS Prakash1, Srinivas Chilukuri2, Nikhil Ranade3, Shankar Viswanathan4 \nZS Associates, Safina Towers-South Block, 5th Floor, Ali Asker Road, Bengaluru 560052, Karnataka, India1,3,4 \n ZS Associates, 1560 Sherman Ave, Evanston, IL 60201, US2 \nprakash.prakash@zs.com1, srinivas.chilukuri@zs.com2, nikhil.ranade@zs.com3, shankar.viswanathan@zs.com4 \n  \n \n \nAbstract \nA rare disease is any disease that affects a very small percent-\nage (1 in 1,500) of population. It is estimated that there are \nnearly 7,000 rare disease affecting 30 million patients in the \nU. S. alone. Most of the patients suffering from rare diseases \nexperience multiple misdiagnoses and may never be diag-\nnosed correctly. This is largely driven by the low prevalence \nof t he disease that results in a lack of awareness among \nhealthcare providers. There have been efforts from machine \nlearning researchers to develop predictive models to help di-\nagnose patients using healthcare datasets such as electronic \nhealth records and admi nistrative claims. Most recently, \ntransformer models have been applied to predict diseases \nBEHRT, G -BERT and Med -BERT. However, these have \nbeen developed specifically for electronic health records \n(EHR) and have not been designed to address rare disease \nchallenges such as class imbalance, partial longitudinal data \ncapture, and noisy labels. As a result, they deliver poor per-\nformance in predict ing rare diseases compared with base-\nlines. Besides, EHR datasets are generally confined to the \nhospital systems using them and do not capture a wider sam-\nple of patients thus limiting the availability of sufficient rare \ndisease patients in the dataset. To a ddress these challenges, \nwe introduced an extension of the BERT model tailored for \nrare disease diagnosis called Rar eBERT which has been \ntrained on administrative claims datasets. RareBERT extends \nMed-BERT by including context embedding and temporal \nreference embedding. Moreover, we introduced a novel adap-\ntive loss function to handle the class imbalance. In this paper, \nwe show our experiments on diagnosing X -Linked Hypo-\nphosphatemia (XLH), a genetic rare disease. While Rare-\nBERT performs significantly better than the baseline models \n(79.9% AUPRC versus 30% AUPRC for Med-BERT), owing \nto the transformer architecture, it also shows its robustness in \npartial longitudinal data capture caused by poor capture of \nclaims with a drop in performance of only 1.35% AUPRC, \ncompared with 12% for Med -BERT and 33.0% for LSTM \nand 67.4% for boosting trees based baseline. â€  \n \n                                                 \nCopyright Â© 2021, Association for the Advancement of Artificial Intelli-\ngence (www.aaai.org). All rights reserved. \nIntroduction \nIdentifying the right patients at the right time has always \nbeen a primary goal of the life science industry, to improve \nhealthcare services. Individually rare diseases are uncom-\nmon; however, there are approximately 8,000 described rare \ndiseases mostly with onset in childhood ( Elliott et al. 2015 \nand Zurynski et al. 2017) affecting approximately 30 mil-\nlion of US population and 350  million globally worldwide \n(Colbaugh and Glass 2020) . Identifying these patients is a \nchallenging task as patients go undiagnosed or mis-diag-\nnosed for years and move across multiple physicians (Rafi, \n2016 and Zurynski Y, et al. 2017) \nIn the health care domain, with an increase in capture of \npatientsâ€™ longitudinal history via administrative claims and \nelectronic health records  (EHR) enab les healthcare indus-\ntries to understand patient progression or patient identifica-\ntion use cases such as onset of a condition, disease progres-\nsion or treatment drop-off which in-turn helps to maximize \npatient care and experience. Researchers are using machi ne \nlearning based approaches to address patient progression \nand patient identification related problem. However, owing \nto the richness of the data ; mining this high dimension se-\nquential data is a challenging task.  \nThe current research focuses on rare disease patient iden-\ntification using administrative claims data. The problem in-\nvolves many challenges related to data and modelling which \nincludes: 1) Class-imbalance; 2) Unlabeled patients (nega-\ntive class is not present) ; 3) Noisy labelled patients ; 4) Di-\nagnosis codes for rare condition are not present; and 5) Sen-\nsitivity analysis of model due to over-fitting on low positive \nclasses.  The current paper focuses on addressing the class \nimbalance and unlabeled patients challenge. Here, unlabeled \n \nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n453\npatients are also  referred as eligible patients and are ones \nwho have shown presence of comorbidities associated with \ntarget disease area such as X-linked hypophosphatemia \n(XLH) however cannot be classified either as positive or \nnegative class due to missing confirmatory t est leading to \nabsence of negative class in dataset. Additionally, impact of \nnoisy scenarios are evaluated on performance of the models.  \nThe current paper proposes a novel RareBERT architec-\nture for feature representation. The proposed architecture \nextends the Med-BERT architecture to address highly im-\nbalanced class in rare conditions and make improvements in \narchitecture by incl uding context -based embedding, tem-\nporal reference embedding and adaptive loss function for \nfaster convergence.  Feature representa tion from Rare -\nBERT is integrated with positive unlabeled (PU) learning \nbased architecture also referred as one -class classifi cation \nfor patient identification (Denis et al. 2004). The PU learn-\ning algorithm  trains a classifier on positive and unlabeled \ndata and, estimates the propensity of being a positive class \namong unlabeled dataset s using an adjusted threshold . The \nefficacy of proposed approach is validated through X-linked \nhypophosphatemia (XLH)  rare condition case study. The \nexperimentation shows significant improvement and robust-\nness of RareBERT with baseline methods.   \nRest of the paper is organized as follows: Section 2 presents \nrelated prior work; Section 3 describes RareBERT architec-\nture and positive unlabeled (PU) -learning based approach \nfor pati ent identification ; and Section 4 presents the perfor-\nmance of RareBERT using XLH rare condition as a case study, \nfollowed by conclusions  and proposed next steps in \nSection 5.  \nRelated Prior Work \nMost of the previous work has been focused on solving the \nfollowing challenges: \n1) Lack of specific diagnosis codes to identify patients \nwith the rare disease, thus making the identification \nharder (no gold standard definition to identify posi-\ntive patients) (Colbaugh, R et al. 2018) \n2) Lack of markers to identify true neg ative patients as \na lot of patients are either misdiagnosed or remain un-\ndiagnosed for a long time (Yu. K., et al. 2019, Garg \net al. 2016) \n3) Low prevalence of positive classes thus leaving a low \nsample size to learn the patterns from as well as cre-\nating a hig h data imbalance problem (Li W et al.  \n2018, Dai and Hua 2016, Hu et al. 2019).  \nMultiple approaches have been suggested to handle the \nlow positive samples, patients ranging from performing a \nrandom under sampling (Dai and Hua 2016) to using Gen-\nerative Adversarial Networks (GAN) on image representa-\ntions of patients (Li W et al. 2018). For handling the lack of \nmarkers challenge Yu. K., et al. (2019) suggests using a Se-\nquence Modeling with Generative Adversarial Networks \nthat will help identify potential patie nts from the unlabeled \n/ undiagnosed patient pool. However, here the assumption is \nthat true negatives can be identified by applying certain \nrules. In general, such rules are most likely not available for \nrare diseases specifically. \nRecently, BERT (Bidirec tional Encoder Representations \nfrom Transformers) has been proposed for language  repre-\nsentation model for obtaining text embeddings (Vaswani et \nal. 2017). The BERT architecture is proved to be very useful \nin multiclass and pairwise text classification usin g transfer \nlearning of pre-trained embeddings. Researchers in life sci-\nences are trying to utilize the BERT architecture on sequen-\ntial patient journeys to solve problem related to diagnosis \ncode prediction, medication code prediction etc. Architec-\ntures such  as BEHRT (Li Y., Rao et al.  2020), G -BERT \n(Shang et al. 2019) and Med-BERT (Rasmy et al. 2020) are \nbeing proposed. BEHRT focused on imputing medical \ncodes within Visit. G -BERT integrated graph neural net-\nwork (GNN) with BERT architecture with modified masked \nlanguage to predict medication code prediction using single \nvisit samples whi ch is a limitation. Med -BERT has further \nextended the architecture to create generalized embedding \nusing bigger vocabulary and used it to optimize two objec-\ntives disease predic tion and length of stay in hospital. Ra-\nsmy et al.  (2020) has compared different architectures of \nBEHRT, G-BERT and Med -BERT used for diagnosis and \nmedication tasks. Most of the above architectures are unable \nto generalize the embeddings when datasets becom e highly \nimbalanced. Madabushi et al. 2020 has observed imbalance \nissues in NLP application of propaganda detection and pro-\nposed Cost-Sensitive BERT architecture by increasing the \nweight of incorrect labels.    \nThe feature representation from RareBERT is used for pa-\ntient identification using the PU learning -based approach. \nThere are a few different PU approaches that have been pro-\nposed (Denis et al. (2004), Elkan et al. (2008), Plessis et al. \n(2015)). Most of these approaches involve isolating a set of \nso-called true negatives (TNs) from the unlabeled data set. \nThe proposed paper uses biased learning-based approach as \nproposed by Bekker and Davis (2018) which isolate the neg-\native classes at each iteration.  \nThe next Section presents detailed approach for pat ient \nidentification using patient administrative claim data. \nApproach \nThe current section presents  RareBERT architecture which \nhelps with feature representation. The learned features are \npassed through PU -learning based semi -supervised classi-\nfier to identi fy patients with high likelihood of having rare \ncondition. The RareBERT architecture helps to deal with \n454\nhigh dimensional sequential data with highly imbalanced \nclasses. The RareBERT architecture enhances Med -BERT \narchitecture by including: 1) adaptive loss  to balance loss \nbetween classification and mask event  prediction task; 2) \ntype embedding to capture event context such as is it diag-\nnosis, procedure, treatment; and 3) temporal reference em-\nbedding to capture event position with respect to defined in-\ndex da tes. The next sub -section provides data set -up for \nRareBERT.  \nRare-BERT \nThe proposed RareBERT set -up uses patient longitudinal \nclaims data. Patient level claims data is right aligned to an-\nchor date and mapped to a Clinical Classification Software \n(CCS) bucket to roll-up the granular data to a higher resolu-\ntion as shown in Figure 1. \nFigure 1: Illustration of patient journey \nThe patient input sequence is passed through the Rare -\nBERT architecture as shown in Fig. 2. The RareBERT de-\ntermines embeddings from se quence at multiple levels in-\ncluding token, type of event, visit information and temporal \nreference from anchor events. \nFigure 2: RareBERT architecture \nToken embedding  captures token level information or \nevent information. Type embedding captures the event type \ninformation such as diagnosis, procedure or treatment.  Visit \nembeddings capture patient visits or claims as patients could \nhave many events within a visit.  Temporal embedding cap-\ntures time reference for an event from defined ancho r date. \nAn example of RareBERT embedding encoding is presented \nin Figure 3 based on patient journey shown in Figure 1. \nFigure 3: Illustration of input data for RareBERT \nThe RareBERT utilize only [CLS] token, while [SEP] to-\nken is ignored  (Rasmy et al. 2020). RareBERT optimize s \ntwo loss function functions: (i) Masked event modelling; \nand (ii) event classification. The masked event modelling in \nsequence learning is an event fill-in-the blank task, where a \nmodel uses the event surrounding a mask token to predic t \nthe masked event which in turn helps in embedding gener-\nalization for different patient paths . The loss function \nweights for masked event modelling and event classification \nare updated adaptively to enhance the convergence. \nLet   ğ‘¢={ğ’–ğŸ,ğ’–ğŸ,ğ’–ğŸ,â€¦ğ’–ğ’âˆ’ğŸ}  represent input event to-\nkens with n number of tokens. The RareBERT mask 15% of \nevent token randomly with indices m and masked events are \nrepresented as ğ’† and non-masked event are represented as \nğ’†â€². The mask event is replaced by a special token [MA SK], \nrandom event from vocabulary, or unchanged with a proba-\nbility of 80%, 10% and 10%, respectively. Letâ€™s ğ’—=\n{ğ‘£ğŸ,ğ‘£ğŸ,ğ‘£ğŸ,â€¦ğ‘£ğ’âˆ’ğŸ}  represent predicted events and h repre-\nsents final hidden state of t he first token [CLS] from Rare -\nBERT denote embedding for the whole sequence. The pre-\ndiction for masked event modelling is optimized by mini-\nmizing negative log-likelihood (NLL) as shown below: \nğ¿ğ‘€ğ¸ğ‘€ =âˆ’ğ¸(ğ’–,ğ’—)~ğ‘»logğ‘ƒğ‘Ÿ(ğ‘’â€²|ğ’‰) â€¦    (1) \nwhere, (ğ’–,ğ’—) are pairs in the training dataset T. The ğ‘ƒ(ğ‘’â€²|ğ’‰) \nis computed as \nğ‘ƒ(ğ‘’â€²|ğ’‰)=ğ¿ğ‘œğ‘”ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘¸ğ’‰) â€¦ (2) \nwhere, Q is linear layer weight matrix . Similarly, the event \nclassification module also utilizes the negative log -likeli-\nhood loss function as shown below \nğ‘³ğ‘¬ğ‘ª =âˆ’ğ‘¬(ğ’–,ğ’—)~ğ‘»ğ¥ğ¨ğ ğ‘·ğ’“(ğ’„==ğŸ|ğ’‰) â€¦ (3) \nwhere, ğ’„âˆˆ{ğŸ,ğŸ} with 1 represent ing positive class. \nThe loss in RareBERT weighted combination of Eq. (1) and \nEq. (3) and at ith epoch is represented as  \nğ‘³ğ’Š =ğœ¶ğ’Šğ‘³ğ‘¬ğ‘ª,ğ’Š+ğ‘³ğ‘´ğ‘¬ğ‘´,ğ’Š \nwhere, ğœ¶ğ’Š is weight at ith epoch and is computed as  \nğœ¶ğ’Š =ğ’ğ’Šğ’(ğœ½,ğ‘³ğ‘´ğ‘¬ğ‘´,ğ’Šâˆ’ğŸ/(ğ‘³ğ‘¬ğ‘ª,ğ’Šâˆ’ğŸ+ğœº))   \nwhere,ğœ½ and ğœº are constant factor to control max bound \nof weights and correct for infinity scenarios.  In this paper \nrare-patient identification set-up ğœ½ is set based on class im-\nbalance and  ğœº is set to 10-6 close to handle any zero-division \nPatient Sequence\nEmbedding\nTransformers\nClassification Loss Masked event Loss\nAdaptive loss\nOutput\nUpdate\n455\nerror. Once RareBERT is trained  embedding h is extracted \nand used within the PU-learning semi-supervised set-up for \npatient identification. \nPU-Learning  \nRareBERT is u sed to learn  the feature representation h \nwhich is used within the PU learning algorithm for patient \nidentification. PU learning is a variation of the traditional set \nup where the training data consists of only po sitive and un-\nlabeled examples where unlabel ed examples include both \npositive and negative classes.  \nLet ğâŠ‚ğ“ represent training set containing only positives \npatients and UâŠ‚ğ“ represents unlabeled classes where T is a \nuniversal set with all patients. The p and u are cardinality of \nP and U, respectively. The pseudo code for proposed PU \nlearning based approac h for patient identification is shown \nin Fig. 4.  \nFigure 4: Pseudo code for PU learning \nThe PU learning is stopped based on two stopping crite-\nriaâ€™s: 1) recall threshold ğœ¸ which represents the number of \npositives patients captured by classifier; and 2) True Nega-\ntive (TN) threshold ğ‰ which captures the minimum true neg-\native identified during iteration.   \nCase Study \nTo illustrate the performance of RareBERT, X-linked hypo-\nphosphatemia (XLH) patientsâ€™ identification is performed. \nXLH, is a condition that affec ts bones, muscles, and teeth \ndue to the excessive loss of phosphate. Phosphate is lost \nthrough the urine, which causes low levels of phosphorus in \nthe blood, a condition called phosphate wasting or hypo-\nphosphatemia. The XLH condition is selected d ue to: 1) \nhigh imbalance; 2) High proportion of un-diagnose patients \n                                                 \nâ€¡Symphony Health, Integrated Dataverse (IDV)Â®, Sep. 1, 2019 â€“ Dec. 31, \n2019, unprojected de-identified patient Rx and medical claims, Jan 2020  \nwith approximate prevalence of 1 in 20K patients in US; and \n3) high disease burden (Skrinar et al. 2019).  \nData Set-up \nThe analysis is performed using patient -level claims data \nwith approximately 3.5 years of patient history from propri-\netary Symphony Healthâ€™s IDVÂ®â€¡. Patients with XLH and \nother comorbid conditions such as disorder of phosphorus \nmetabolism, rickets, muscle weakness and, bone spurs were \nconsidered for further analysis.   \nIn order to assure continuous patients' activity, standard \neligibility criteria were applied. Eligibility criteria discard \nthe patients who do not have a claim for a year. On top of it, \na lookback period of two years was examined. Lookback \nwas anchored on the first diagnosis of X -linked hypophos-\nphatemia for XLH patients an d on the last day of 2019 for \npatients with other comorbid conditions.  \nAfter applying comorbid condition, eligibility and look-\nback criteria, the final patientâ€™s cohort consists of 3,670 \nXLH patients and 263,187 unlabeled patients. The class im-\nbalance between XLH and unlabeled patients is 1.37%. \nModel Set-up and Feature Representation \nFor further modelling stratified  sampling is performed to \ncreate a train (60%), validation (15%) and test (25%)  da-\ntaset. Multiple approaches such as XGB with binary -based \nfeatures, Long Short-Term Memory (LSTM), Med-BERT \nand RareBERT are developed for benchmarking. The first \nset of models are developed with classification set -up with \nunlabeled patient is treated as negative class. The hyper-pa-\nrameter of XGB model is optimized using Bayesian optimi-\nzation (Ruben, 2014). The LSTM model is set -up based on \nKezi et al. (2019). The Kezi et al. 2019 takes LSTM embed-\nding as input to GAN for further enrichment for patient iden-\ntification. The Kezi et al. 2019 approach uses Word2Vec to \nencode event and pass to LSTM. The LSTM is set -up with \n300 embedding size using word2vec, two layers and 256 \nhidden dimensions. Based on the initial set -up, AUPRC is \ncompared across a set -up of mo dels. The test performance \nfor different models are reported in Table 1. \nS. No. Model Test AUPRC \n1 XGB (Binary-based feature) 43.0% \n2 LSTM (Kezi et al. 2019) 43.3% \n3 Med-BERT (Rasmy et al. 2020) 30.0% \n4 RareBERT (w/ adaptive loss) 79.9% \n5 RareBERT (w/o adaptive loss) 80.1% \nTable 1: Performance on sequence dataset \nConstruct P and U by assigning   = is      ğ‘  ğ‘¢  ğ‘ ğ‘’ ğ‘’  ğ‘’ğ‘¥ğ‘ğ‘š  ğ‘’\nWhile True:\nj=1  \nSubset    e    where    âŠ‚ğ‘· ğ‘     <  \nAssign   = where j âŠ‚  \nTrain classifier to determine Pr(  =1 | f (h)) treating U as negative class. \nEvaluate Pr(  =1 | f (h)) for all observations\nIdentify True Negative (TNj) where ğ‘»  âŠ‚ where   (   1      ( ))   \nUpdate U = U â€“ ğ‘»  \nj = j+1    \n# Stopping criteria\nif Recall > ğœ¸ or ğ‘»  - ğ‘»  âˆ’ <ğ‰ :\nbreak\n    represents min spies probability\n## LGBM (Ke et al. 2017) is used as classifier for training.\n456\nBased on results from Table 1, XGB (with binary fea-\ntures) and LSTM are performing at an approximately 43% \nAUPRC. Med-BERT is performing worse, compared with \nthe XGB (Binary -based feature) and  LSTM for rare event \nscenarios. The RareBERT has shown an approximately 75% \nuplift in AUPR C performance, as compared with Med -\nBERT. The RareBERT shows similar performance with and \nwithout adaptive loss. The ğœƒ parameter for RareBERT (w/ \nadaptive loss) is set to 70 based on class imbalance observed \nin positive and unlabeled class.  \nBased on the above results, the RareBERT shows a strong \nfeature representation capability to distinguish XLH and \nnon-XLH patients. Additionally, the number of iteration and \ncompute effort across models are compared and shown in \nTable 2.   Adaptive loss has helped reduce compute effort \nby 66% as compared to without (w/o) an adaptive loss set -\nup of RareBERT. \nS. No. Model \n# of Itera-\ntions \nTime \n(hr) \n1 XGB (Binary-based feature) 1700 3.25 \n2 LSTM (Kezi et al. 2019) 30 5.32 \n3 Med-BERT 56 22.32 \n4 RareBERT (w/o adaptive loss) 72 15.06 \n5 RareBERT (with adaptive loss) 29 5.15 \nTable 2: Med-BERT and RareBERT parameters \nRareBERT and LSTM compu tation is performed on 2 \nGPU machine with 16 core and 2.5 GHz processor. The \nMed-BERT is computed on 4 GPU machine with 48 cores \nand 3.5 GHz processor.  Although, Med -BERT is using \nhigher configuration, itâ€™s more expensive than Rare -BERT \nwith lower performance. The detailed set-up differences be-\ntween Med-BERT and RareBERT is shown in Table 3. \nParameter Med-BERT RareBERT \nType of input \ncode  \nICD-9 + ICD-10 event \nCCS code \nICD-9 + ICD-10 event CCS \ncode \nEmbeddings \nCode + visit + Seriali-\nzation  \nCode + code type + Visit \n+ Temporal reference \nLayer 6 2 \nAttention 6 6 \nVector 32 16 \nEmbedding size 192 96 \nclassification loss NLL NLL \nMasked event \nloss NLL NLL \nLoss  Sum Adaptive Weighted Sum \nEvent Prediction  \nFeed-forward Layer \non averaged se-\nquence  CLS Token \nTable 3: Med-BERT and RareBERT parameters \nAblation study is performed on RareBERT architecture \nfor key contribution of determine impact of individual com-\nponent on final performance . The summary of ablation \nstudy is shown in Table 4. Check (âœ“) in Table 4 repres ent \ncomponent that is considered for experimentation.  \nS. No. Temporal Type CLS Visit Token AUPRC \n \nâˆ† from \nbase \n1* âœ“ âœ“ âœ“ âœ“ âœ“ 79.9%  \n2  âœ“ âœ“ âœ“ âœ“ 70.4% 9.5% \n3 âœ“  âœ“ âœ“ âœ“ 64.4% 14.9% \n4  âœ“  âœ“ âœ“ 56.0% 23.9% \n5    âœ“ âœ“ 42.3% 37.6% \n* Base experiment \n**All experiments are performed with Adaptive loss \n***For experiment 4 & 5, CLS is replaced with feedforward layer  \nTable 4: Ablation study summary of RareBERT \nBased on Table 4, all Temporal, Type and CLS are sig-\nnificantly improving the performance of RareBERT archi-\ntecture. Also, Adaptive loss helps to improve compute by \n~66% based on Table 2 . The next sub -section focuses on \npatient identification. \nPatient Identification \nThe patients with a rare condition are identified using semi-\nsupervised learning -based approach such  as positive unla-\nbeled (PU) learning. The PU learning framework as de-\nscribed in Fig. 4 is used with light gradient boosting machine \n(LGBM) as base classifier. The recall curves obtained from \ndifferent models is shown in Fig. 5. \nBased on Fig. 4, RareBERT shows a very high recall on test \ndataset with PU further boosting the recall performance with \nimproved AUPRC to 79.8%. To further compare the perfor-\nmance of the mod el with semi -supervised learning where \nnegative class is unknown pseudo F -1 score ( Lee and Liu, \n2003) is used. The pseudo F-1 score is defined as \nğ‘ƒ ğ‘¢ğ‘’ ğ‘œ ğ¹1âˆ’ ğ‘ğ‘œğ‘Ÿğ‘’= ğ‘…ğ‘’ğ‘ğ‘  2\nPr(ğ‘==1) \nFigure 5: Recall across different models \n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nRecall\nThreshold\nMed-BERT\nXGB  + Binary feature\nLSTM\nRareBERT\nRareBERT + PU Learning\n457\nThe ğ‘ƒ ğ‘¢ğ‘’ ğ‘œ ğ¹1âˆ’ ğ‘ğ‘œğ‘Ÿğ‘’ captures the performance of the \ncurrent model with respect to the random baseline at defined \nthreshold.  Based on the above metric ğ‘ƒ ğ‘¢ğ‘’ ğ‘œ ğ¹1âˆ’ ğ‘ğ‘œğ‘Ÿğ‘’ for \nall models are reported below: \nFigure 6: Psuedo-F1 score across different models \nBased on Fig. 6, PU -BERT has shown an improvement \nof 4.4% in capturing unlabeled patients with rare conditions, \ncompared with RareBERT. The max pseudo F1 -score for a \ndifferent model is reported in Table 5. \n  S. No. Model  Max Pseudo F1 \n1 RareBERT  47.5 \n2 Med-BERT  9.5 \n3 Binary + XGB  23.2 \n4 LSTM  22.5 \n5 PU + RareBERT  49.6 \nTable 5: Max Pseudo F1-score to evaluate model lift \nTo understand the events driving the performance of \nRareBERT performance attention weights are extracted \nacross different attention heads.  Top 5 events leaning to-\nwards positive patient  based on highest attent ion weights \nacross different heads were the use of: 1) Vitamin D supple-\nment; 2) Acidifying Agents (K -phos); 3) Opiate Agonists; \n4) Anticonvulsants; 5) Beta-blocking agents. \nVitamin D and phosphate supplements are clinically rec-\nommended therapies for treati ng XLH and its symptoms \n(Dieter et al.  2019), thus correctly representing XLH pa-\ntients. Similarly, Opiate Agonist is generally used as a pain \nkiller. Skrinar et al. (2019) reported 97% of adults and 80% \nof children reported bone or joint pain/stiffness. Th us, the \nuse of pain killer drugs might also be a proxy indicating pain \nrelated diagnosis for patients.  Imel et al. (2012) report sei-\nzures as one of the symptoms for severe XLH patients which \nsuggest the use of Anticonvulsants by XLH patients.  The \nBeta-blocking agents are used in cardio conditi ons. There \nhas been some research on the association of XLH and car-\ndio vascular symptoms but not very concrete. However, \nmodel does suggest the usage of cardiovascular related \ndrugs as one of the top indicators in iden tifying XLH and \nnon-XLH patients. \nSimilarly, top events for negative patients were Chronic \nkidney disease, Deficiency and other anemia, Office visits \nfor - interview, evaluation, consultation, Laboratory - \nChemistry and Hematology procedures. Research sugg ests \nthat patients with Chronic kidney  disease and anemia also \nhave low phosphorous levels, however, post dialysis most \nof the patients get their phosphorous levels back to normal \n(Nielsen et al. 2019) â€“ this generally results in many cases \nwhere patients have got misdiagnosed as XLH or vice versa. \nHowever, RareBERT seems to have identified the differen-\ntiation between Chronic Kidney Disease and XLH patients. \nThe top events associated with XLH and non-XLH patients, \nRareBERT seems to have picked the right sig nals thus \nboosting the confidence in the predictions of the model. \nModel Sensitivity Analysis \nOne of the issues with claims dataset is noisy data coverage \ndue to missing and wrong code capture ( Tsang, 2020). \nTsang (2020) summarized the drastic implications  of noisy \nclaims data on analysis. The current paper simulates the im-\npact of missing claims through adversarial attack on the \nmodel. Simulation performed with 15% of events are ran-\ndomly removed from patient journey and pre-trained model \nperformance are re-evaluated on patient journey. The simu-\nlation is performed for 10 iteration; the average drop in \nAUPRC is reported in Figure 7. \n Figure 7: Percentage Drop in AUPRC \nBased on Figure 7, RareBERT performance is quite stable \neven after 15% missing event noise i nduced in the dataset. \nAdditionally, the impact on False Negative is more critical \nfor patient identification, where we are tagging XLH patient \nas non-XLH. Results for False Negative (FN), i.e., XLH pa-\ntient is classified as non-XLH patient is shown in Figure 8. \nFigure 8: Percentage Increase in False Negative (FNs) \n67.42%\n33.03%\n11.80%\n1.35%\nBinary Features +\nXGB\nLSTM Med-BERT RareBERT\n% Drop in AUPRC\n% Drop in AUPRC\n14.32%\n19.04%\n7.30%\n3.62%\nBinary Features +\nXGB\nLSTM Med-BERT Rare-BERT\n% Increase in FN\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nPseudo F1 score\nThreshold\nMed-BERT\nXGB  + Binary feature\nLSTM\nRareBERT\nRareBERT + PU Learning\n458\nThe above changes are evaluated with base model with \nprecision set to 10%. The default output with 10% precision \nis reported below: \nModel Base \nAUPRC \n TN FP FN TP Recall \n(%) \nXGB (Binary-\nbased feature) 43.0%  61040 4770 398 530 57.1 \nLSTM 43.3%  60725 5085 363 565 60.9 \nMed-BERT 30.0%  61931 3879 497 431 46.4 \nRareBERT \n(with adaptive loss) 79.9%  58502 7308 116 812 87.5 \nTable 6: Max Pseudo F1-score to evaluate model lift \nThe RareBERT has 87.5% reca ll with 10% precision to \nidentify patients with rare -condition. Based on Table 5 \nRareBERT reports almost double potential patients with \n87.5% recall as compared to Med-BERT. \nTo further evaluate consistency  of the proposed Ra re-\nBERT approach is also tested on Exocrine Pancreatic Insuf-\nficiency (EPI) rare condition. EPI is a condition in which the \npancreas is not able to produce and/or transport enough di-\ngestive enzymes to break down food in the intestine.  \nThere are two main challenges associated in identifyin g \nEPI patients within real wor ld claims data are: 1) There is \nno specific ICD 9 diagnosis code for EPI, thus there is no \ndirect way to identify patients who got diagnosed with EPI \nprior to Oct ober 2015 (ICD coding s ystems changed from \nICD 9 to ICD 10 post October 2015); and 2) There are a \ngood amount of patients who are misdiagnosed with a simi-\nlar condition such as Abdominal pain, Chronic pancreatitis \n(CP), etc. These criteria are more aligned with the nuances \nassociated with other rare disease conditions thus making it \na non-trivial problem to solve. \nThe patients with relevant comorbid conditions  are clas-\nsified into EPI positive, negative and unlabeled are defined \nbased on definition from Pyenson et al. 2019 as defined be-\nlow: \nâ–ª Positive class: patients are cl assified into EPI \npositive if patient have filled minimum three \nprescription of PERT \nâ–ª Negative class: Patients are classified as EPI if \nfecal elastase-1 test is observed but PERT pre-\nscription is not filled \nâ–ª Unlabeled class:  patients with any of the rele-\nvant comorbid conditions. \nAfter applying comorbid condition, eligibility and look-\nback criteria, the final patientâ€™s cohort consists of 15,045 \nEPI patients with 74,978 Negative patients. The dataset con-\nsists of 1.1 3 MM unlabeled patients  with class imbalance \nbetween EPI positive and negative cases as 16.71%. The \nperformance of models benchmarked against EPI dataset  is \nshown in Table 7. \nThe results obtained are consistent with observed in XLH \nuse case with RareBERT having 96.8% AUPRC. The XGB \nand LSTM based model  shown a test AUPRC of 79% and \n80.1%, respectively and MedBERT is unable to perform in \nhighly imbalance scenarios.  \nS. No. Model Test AUPRC \n1 XGB 79.0% \n2 LSTM (Kezi et. al. 2019) 80.1% \n3 Med-BERT (Rasmy et. al. 2020) 67.6% \n4 Rare-BERT (w/ adaptive loss) 96.8% \n5 Rare-BERT (w/o adaptive loss) 96.2% \nTable 7: Performance on sequence dataset \nConclusion and Future Work \nThe paper presents a novel RareBERT architecture for fea-\nture representation in highly imbalanced longitudinal da-\ntaset. The learned feature r epresentation is used with Posi-\ntive Unlabeled (PU) -learning based approach for patient \nidentification using patient administrative claims. The pro-\nposed PU learning algorithm utilizes positive class labels to \nidentification potential patients with rare condition. The en-\nhancement made in RareBERT helps it to learn representa-\ntion better and converge faster based on adaptive loss.  \nAdditionally, model sensitivity analysis is performed on \nthe dataset to simulate partial data capture scenarios which \nis very preva lent across industries in transactional dataset. \nThe simulations revealed that tree -based boosting is most \nsusceptible to performance drop due to partial data capture. \nThe performance drop within LSTM is also significant but \nsignificantly less as compared to tree -based bosting ap-\nproach. The transformers -based model such as Med -BERT \nand RareBERT are using adversarial loss which helped them \nto generalize the feature representation better and have \nshown significantly more robust performance.  \nThe RareBERT comes out to be most robust method. The \nrobustness can be attributed to better generalization of tem-\nporal aspect for events and [CLS] token which is utilized for \nclassification as compared to positional embedding and \nfeed-forward layer used in Med-BERT. \nThe current experiment on RareBERT did not included \nany contextual information such as physical specialty or any \npatient related demographic information. The future experi-\nments include bringing contextual information within the \nRareBERT architecture. This also includes developing a ro-\nbust approach to handle differential Rx/Mx capture or inte-\ngrating imputation mechanism to fur ther strengthen the \nmodel performance in an unseen environment.    \n459\nReferences \nBekker J. and Davis. J., 2018 Estimating the class prior in positive \nand unlabeled data through decision tree induction. In AAAI, pages \n2712â€“2719 \n Colbaugh R., and Glass K. 2020. Finding Rare Disease Patients in \nEHR Databases via Lightly -Supervised Learning. Technical Re-\nport, Volv Global, Lausanne, Switzerland \nColbaugh, R et al. 2018.  Learning to identify rare disease patients \nfrom electronic health records. AMIA Annual Symp osium, San \nFrancisco, CA USA, November 2018. \nDai D., and Hua S., 2016. Random under -sampling ensemble \nmethods for highly imbalanced rare disease classification. Interna-\ntional Conference on Data Mining, Barcelona, Spain, 12-15th Dec. \n2016 \nDenis, F., Gilleron, R., Letouzey, F. 2005. Learning from positive \nand unlabeled examples. Theoretical Computer Science , 348(1), \n70:83.  \nDu Plessis, M., Niu, G., S ugiyama, M. 2015. Convex formulation \nfor learning from positive and unlabeled data. International Con-\nference on Machine Learning, pp. 1386:1394.  \nElliott E, Zurynski Y. 2015. Rare diseases are a â€™common' problem \nfor clinicians. Aust Fam Physician. 44:630â€“3 \nElkan, C., Noto, K. 2008. Learning classifiers from only positive \nand unlabeled data. Proceedings of the 14th ACM SIGKDD inter-\nnational conference on Knowledge discovery and data mining, pp. \n213:220 \nEvans W, Rafi I. 2016. Rare diseases in general practice. British J. \nGeneral Practice, Vol. 66. \nGarg, R., S. Dong, S. Shah, and S. R. Jonnalagadda. 2016. A Boot-\nstrap Machine Learning Approach to Identify Rare Disease Pa-\ntients from Electronic Health Records. arXiv preprint \narXiv:1609.01586. \nHu Y., Chen F., Cai Y.,  and Yuan Y. 2019. A Random Under -\nSampled Deep Architecture with Medical Event Embedding: \nHighly Imbalanced Rare Disease Classification with EHR Data. \nInternational Conference on Data Science (ICDATAâ€™19), July 29 - \nAugust 1, 2019. Las Vegas, NV \nKe G., et al. 2017. LightGBM: A Highly Efficient Gradient Boost-\ning Decision Tree. 31st conference on Neural Information Pro-\ncessing Systems (NIPS 2017), Long Beach, CA, USA  \nPyeson B., et al. 2019. Applying Machine Learning Techniques to \nIdentify Undiagnosed Patients with Exocrine Pancreatic Insuffi-\nciency. Journal of Health Economics and Outcomes Research.  \nMethodology and Health Care Policy, 6(2). \nRuben M. C., 2014. BayesOpt: A Bayesian Optimiz ation Library \nfor Nonlinear Optimization, Experimenta l Design and Bandits. \nJournal of Machine Learning Research, 15(Nov):3735--3739,  \nTsang JP. 2020. Maladies of Claims Data: Manifestations, Origins, \nand Cures. PMSA Vol. 8(2). \nLi W et al. 2018. Semi-supervised Rare Disease Detection Using \nGenerative Adversar ial Network. Machine Learning for Health \n(ML4H) at NeurIPS, 2018 \nLi Y., Rao et al. 2020. BEHRT: Transformer for Electronic Health \nRecords. Scientific Reports, 7155 \nMadabushi et al.  2020, Cost-Sensitive BERT for Generalisable  \nSentence Classification with Im balanced Data. arXiv preprint  \narXiv:2003.11563 \nRasmy L., Xiang Y., Xie Z., Tao C., and Zhi D. 2020. Med-BERT: \npre-trained contextualized embeddings on large -scale structured \nelectronic he alth records for disease prediction . arXiv preprint \narXiv: 2005.12833 \nVaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez \nA., Kaiser L. 2017. Attention Is All You Need. 31st Conference on \nNeural Information Processing Systems (NIPS 2017), Long Beach, \nCA, USA \nShang J, Ma T, Xiao C, Sun J. 2019. Pre -training of g raph aug-\nmented transformers for medication recommendation. arXiv pre-\nprint arXiv:190600346. \nSkrinar A., et al. 2019. The Lifelong Impact of X -Linked Hypo-\nphosphatemia: Results From a Burden of Disease S urvey. Jour-\nnal of Endocrine Society, 3(7): 1321â€“1334 \nYu. K., et al. 2019. Rare Disease Detection by Sequence Modeling \nwith Generative Adversarial Networks. International Conference \non Machine Learning, Long Beach, California, PMLR, 2019. \n Zurynski Y, et al. 2017. Rare disease: A national survey of pedia-\ntricianâ€™s experiences and needs. BMJ Paediatrics, Vol. 1 \nLee, W.S., Liu, B. 2003. Learning with positive and unlabeled ex-\namples using weighted logistic regression. ICML: Proceedings of \nthe Twentieth International Conference on Machine Learning \nHaffner D., et al. 2019. Clinical practice recommendations for the \ndiagnosis and management of X-linked hypophosphatemia. Nature \nReviews Nephrology, 15, 435-455. \nSkrinar, Alison, Dvorak -Ewell Melita, et al. 2019.The Lifelong \nImpact of X-Linked Hypophosphatemia: Results from a Burden of \nDisease Survey . Journal of the Endocrine Society , 3(7), 1321 -\n1334. \nImel, Erik A and Econs, Michael J , 2012. Approach to the Hypo-\nphosphatemic Patient. The Journal of Clinical Endocrinology and \nMetabolism, 97(3):696. \nBeck-Nielsen, et al. 2019. FGF23 and its role in X -linked hypo-\nphosphatemia-related morbidity . Orphanet Journal of Rare Dis-\neases, 14(1):58. \n \n460"
}