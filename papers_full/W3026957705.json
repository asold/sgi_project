{
  "title": "A Further Study of Unsupervised Pre-training for Transformer Based Speech Recognition",
  "url": "https://openalex.org/W3026957705",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5110582135",
      "name": "Dongwei Jiang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050050390",
      "name": "Wubo Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5029228290",
      "name": "Ruixiong Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5010070743",
      "name": "Miao Cao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5049649860",
      "name": "Ne Luo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024873536",
      "name": "Yang Han",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5108286207",
      "name": "Wei Zou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081173423",
      "name": "Xiangang Li",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3003875258",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W2988736778",
    "https://openalex.org/W2900898015",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2963939538",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2973157397",
    "https://openalex.org/W2168961642",
    "https://openalex.org/W2963226322",
    "https://openalex.org/W2949667497",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2982413405",
    "https://openalex.org/W2922565841",
    "https://openalex.org/W3035202887",
    "https://openalex.org/W2526425061",
    "https://openalex.org/W2962742956",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2981991061",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W343636949",
    "https://openalex.org/W3005511757",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2972894903",
    "https://openalex.org/W2996383576",
    "https://openalex.org/W2113839990",
    "https://openalex.org/W2981363336",
    "https://openalex.org/W3003382064",
    "https://openalex.org/W2945824677",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W2981687423",
    "https://openalex.org/W2911291251",
    "https://openalex.org/W97072897",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, many unsupervised pre-training methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and Transformer backbone. However, many aspects of MPC have not been fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pre-training data speaking style, its extension on streaming model, and how to better transfer learned knowledge from pre-training stage to downstream tasks. Experiments reveled that pre-training data with matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided 8.46% relative error reduction on streaming model trained on HKUST. Also, the combination of target data adaption and layer-wise discriminative training helped the knowledge transfer of MPC, which achieved 3.99% relative error reduction on AISHELL over a strong baseline.",
  "full_text": "A Further Study of Unsupervised Pre-training for Transformer Based Speech\nRecognition\nDongwei Jiang, Wubo Li, Ruixiong Zhang, Miao Cao, Ne Luo, Yang Han, Wei Zou, Xiangang Li\nAI Labs, Didi Chuxing, Beijing, China\n{jiangdongwei, liwubo, zhangruixiong, caomiao, luone, hanyang, zouwei,\nlixiangang}@didiglobal.com\nAbstract\nBuilding a good speech recognition system usually requires\nlarge amounts of transcribed data, which is expensive to col-\nlect. To tackle this problem, many unsupervised pre-training\nmethods have been proposed. Among these methods, Masked\nPredictive Coding achieved signiﬁcant improvements on var-\nious speech recognition datasets with BERT-like Masked Re-\nconstruction loss and Transformer backbone. However, many\naspects of MPC have not been fully investigated. In this paper,\nwe conduct a further study on MPC and focus on three impor-\ntant aspects: the effect of pre-training data speaking style, its ex-\ntension on streaming model, and how to better transfer learned\nknowledge from pre-training stage to downstream tasks. Ex-\nperiments reveled that pre-training data with matching speaking\nstyle is more useful on downstream recognition tasks. A uniﬁed\ntraining objective with APC and MPC provided 8.46% relative\nerror reduction on streaming model trained on HKUST. Also,\nthe combination of target data adaption and layer-wise discrim-\ninative training helped the knowledge transfer of MPC, which\nachieved 3.99% relative error reduction on AISHELL over a\nstrong baseline.\nIndex Terms: unsupervised pre-training, transformer, speaking\nstyle, streaming speech recognition, knowledge transfer\n1. Introduction\nCurrent industrial end-to-end automatic speech recognition\n(ASR) systems rely heavily on large amounts of high quality\ntranscribed audio data. However, transcribed data take substan-\ntial effort to obtain in industrial applications, while at the same\ntime, a lot of un-transcribed data exist in online systems and\ncost little to collect. It is worthwhile to explore how to effec-\ntively use un-transcribed data to improve the performance of\nspeech recognition systems when labeled data are limited.\nRecently, unsupervised pre-training has shown promising\nresults in several areas, including Computer Vision (CV) [1],\nNatural Language Processing (NLP) [2] and so on. One work\nthat stands out among these methods is Bidirectional Encoder\nRepresentations from Transformers (BERT) [2], which used a\nMasked Language Model (MLM) pre-training objective and ob-\ntained new state-of-the-art results on eleven NLP benchmarks.\nIn speech area, researchers also proposed many unsuper-\nvised pre-training algorithms. Contrastive Predictive Coding\n(CPC) [3] extracts representation from data using contrastive\nloss that requires distinguishing a true future audio sample\nfrom negatives. There are multiple concurrent approaches\nthat generalize this approach [4, 5] and applied it in learn-\ning speaker representation [6], extracting speech representa-\ntion [7, 8], performing various speech-related tasks like speech\nrecognition [7, 9, 10], speech emotion recognition [11] and so\non. [12, 13, 14] proposed Autoregressive Predictive Coding\n(APC) objective that predicts unseen future frame given past\nframes and also achieved good results on phonetic classiﬁca-\ntion, speech recognition, and speech translation. Some other\nwork [15, 16, 17, 18] got motivation from NLP and applied sim-\nilar methods on speech tasks.\nAmong these methods, Masked Predictive Coding (MPC)\n[18] achieved signiﬁcant improvements on state-of-the-arts\nTransformer based speech recognition models on various\ndatasets without introducing any additional parameters for the\nspeech recognition model. However, many aspects of MPC\nhave not been fully explored:\n• Speaking style has a strong impact on performance of\nASR systems [19, 20]. Whether speaking style of pre-\ntraining data would affect the performance of down-\nstream tasks has not been fully investigated.\n• The ability to perform streaming recognition is impor-\ntant for ASR systems. MPC may not work for streaming\nrecognition because it requires bidirectional context for\npredictive coding. How MPC can contribute to stream-\ning models is worth exploring.\n• Despite abundant work [21, 22, 23] on NLP about\nknowledge transfer between pre-trained model and\ndownstream task, there are few work exploring how to\nperform better knowledge transfer in the area of speech.\nIn this paper, we investigate these aspects of MPC and discuss\nhow we can extend MPC for better speech recognition.\n2. Masked Predictive Coding\nMasked Predictive Coding (MPC) [18] uses Masked Recon-\nstruction objective to perform predictive coding on Transformer\nbased models. As depicted in Fig. 1, the model structure\nof MPC is essentially the encoder part of Transformer based\nspeech recognition model plus a single fully-connected projec-\ntion layer. During training, masks are applied on input FBANK\nfeatures before feeding into the encoder. The training objective\nis L1 loss computed between masked input FBANK features\nand projected encoder output at corresponding position. The lo-\ncal smoothness of speech makes the task of predicting adjacent\nframes too easy. So, in the current setup of MPC, we divide\ninput features into chunks of four frames and apply mask on\nchunks with a probability of 15%. Unlike BERT, MPC adopted\ndynamic masking proposed in [24] where the masking pattern\nis generated every time a sequence is fed to the model.\nOne unique characteristic of sequence-to-sequence with at-\ntention ASR models is it usually applies downsampling in the\nencoder. Previous research showed temporal pooling encour-\nages effective encoding in different temporal resolution and\nmakes alignments in the decoding easier [25]. When tempo-\nral downsampling is applied, with input feature X ∈Rt×d and\narXiv:2005.09862v2  [eess.AS]  23 Jun 2020\n!\" #$%&\n…\n!'\n(\" () ('/+\n,\" ,- ,. ,'\n…\n#$%&#$%&\n,/\n#$%&\n,0…\n…!- !. !/ !0\nTransformerEncoder\nPreNet\nProjectionLayer\nFigure 1: Masked Predictive Coding with four-fold downsample\ntemporal downsampling rate r, Transformer encoder projects\nthe output of last encoder layer to dimension X0 ∈Rt/r,d×r.\nWe then reshapes it back to same shape as input feature for MPC\nloss computation.\nThe overall training procedure of MPC consists of two\nstages, pre-training on unsupervised data and ﬁne-tuning on\nsupervised data. In the pre-training stage, MPC performs\npredictive coding directly on FBANK input and encoder out-\nput. After the unsupervised pre-training procedure, we re-\nmove the additional projection layer for predictive coding and\nadd Transformer decoder for ﬁne-tuning on downstream ASR\ntasks. All model parameters are end-to-end trainable in the\nﬁne-tuning stage. The work perhaps most similar to ours is\nMockingjay [15], which also employed Transformer encoder\nwith Masked Reconstruction loss. But their work mainly used\npre-trained model as a feature extractor while MPC works more\nlike BERT and focuses on obtaining a good parameter initial-\nization. Also, unlike [15] and other previous work on predictive\ncoding [7, 12], our setup does not introduce any additional pa-\nrameters into speech recognition model.\n3. Methods\n3.1. MPC for streaming models\nTo apply MPC in streaming models, the Transformer encoder\nneeds to be restricted to only use information that has appeared\nbefore. Though some previous work [26, 27] employed chunk-\nwise splitting for streaming models, in this paper, we simply\nchanged self-attention mask on Transformer encoder to make\nthe whole model stream-able. Speciﬁcally, we use a triangular\nmatrix for self-attention mask M in encoder, where the upper\ntriangular part is set to −∞, and the other elements to 0.\nRecent work on APC [13] got impressive results on down-\nstream tasks with Transformer decoder backbone. Inspired\nby [28], we also propose to use a uniﬁed training objective\nthat combines MPC and APC. During training, with probabil-\nity p, we apply triangular matrix on Transformer encoder and\nuse APC objective, with probability 1 - p, we use the Trans-\nformer encoder as-is with MPC objective. This parameter shar-\ning framework has the advantage of making the learned speech\nrepresentations more general because they are jointly optimized\nfor different pre-training objectives where context is utilized in\ndifferent ways.\n3.2. Knowledge transfer for MPC\nFor speech recognition task in a speciﬁc domain, its data dis-\ntribution may be a lot different from data used for MPC pre-\ntraining. Directly using MPC model in ﬁne-tuning stage might\ncause degradation in performance, even catastrophic forgetting\n[29, 30]. To deal with this problem, we followed previous work\n[21, 23] and adopted target data adaptionto ﬁne-tune MPC\nmodel on data of target task before the ﬁne-tuning stage.\nIt is well-known that different layers of neural network cap-\nture different types of information, and the transferability of dif-\nferent layers also varies a lot [31, 32]. Previous work [21] made\nuse of these ﬁndings by assigning lower learning rates to lay-\ners that are more general for downstream tasks and achieved\npromising results. To adapt their ﬁndings to MPC, we ﬁrst used\nprobing task [33, 34] on pre-trained model to ﬁnd out which lay-\ners of Transformer encoder are more useful downstream speech\nrecognition tasks. After that, layer-wise discriminative train-\ning [21] is used to assign different learning rates to each layer\nof Transformer encoder and adapt them to different extents for\nbetter knowledge transfer.\nTransfer Learning with single-step auxiliary loss [22] is\nyet another transfer learning approach that adopted multi-task\nlearning perspective via the addition of pre-training objective\nin the ﬁne-tuning stage. With the same idea, we also added\nMPC loss in the ﬁne-tuning stage. This way, the joint loss in\nthe ﬁne-tuning stage became the weighted sum of task-speciﬁc\nloss LAttn, LCTC and auxiliary MPC loss LMPC :\nL= αattn ∗LAttn + βctc ∗LCTC + γmpc ∗LMPC , (1)\nFor convenience, we name this method multi-task with MPC\nthrough out this paper. Masked input and output for MPC are\nalso added during training.\n4. Experiments\n4.1. Data\nFor Mandarin pre-training, we ﬁrst collected reading type\ndataset OpenMandarin as described in [18]. Note unlike [18],\nwe did not include HKUST and AISHELL in OpenMandarin\ndataset in this work because we found self pre-trained MPC also\nimproves performance. OpenMandarin contains about 1150\nhours of speech in total. To understand the impact of pre-\ntraining data speaking style, our internal reading type dataset\nDidi Dictation and spontaneous type dataset Didi Callcenter\nwere also included. Didi Dictation is collected from our inter-\nnal mobile dictation application while Didi Callcenter is col-\nlected from phone calls between our user and customer ser-\nvice staff. We randomly selected 5000 hours of Didi Callcenter\nand 5000 hours of Didi Dictation for MPC pre-training. For\nfair comparison with Open Mandarin, we also added another\npre-training dataset by randomly selecting 1150 hours of Didi\nCallcenter and name it Didi Callcenter - 1K. For English pre-\ntraining data, reading type dataset Librispeech [35] and sponta-\nneous type dataset Fisher [36] are used. To provide a fair com-\nparison on data size, we created a new pre-training dataset by\nrandomly selecting only 960 hours of Fisher dataset and name\nit Fisher - 1K. Detailed information of pre-training corpora is\nprovided in Table 1.\nFine-tuning experiments were conducted on HKUST,\nAISHELL-1 and Switchboard. The speaking style of HKUST\nand Switchboard is spontaneous while the speaking style of\nAISHELL is reading. Speed perturbation of 0.9 and 1.1 was\nused on the training data. Mandarin characters are used as\nTable 1: Details of pre-training corpora\nDatasets Hours Speaking Style\nOpen Mandarin 1150 Reading\nDidi Callcenter - 1K 1150 Spontaneous\nDidi Callcenter 5000 Spontaneous\nDidi Dictation 5000 Reading\nLibrispeech 960 Reading\nFisher - 1K 960 Spontaneous\nmodeling units for HKUST and AISHELL like described in\n[37], while 2000 BPE subwords [38] is used for experiments\non Switchboard.\nThe sample rate of HKUST and Switchboard is 8000 Hz,\nwhich is lower than some pre-training data. To alleviate the\npossible inﬂuence, we kept the sample rate of target dataset un-\nchanged and downsampled pre-training data with higher sample\nrate to 8000 Hz when needed.\n4.2. Experimental setups\nFor Transformer based full-sequence models, we followed\nmodel structure of previous work [39] with e = 12, d = 6,\ndmodel = 256, dff = 1280and dhead = 4. A source sequence\nX is ﬁrst fed into a prenet consisting of two-layer CNN with\n256 channels, stride size 2 and kernel size 3 and transformed to\nsubsampled sequence X0 ∈Rnsub×dattn\nbefore feeding into\nTransformer. For Transformer based streaming models, two\nrepresentative work with CTC [40] and transducer loss [41]\nare slightly modiﬁed and used in this work. We used the same\nprenet and encoder structure as Transformer based models. A\ntwo-layer Transformer decoder is used as prediction network\nfor transducer.\nIn pre-training stage, all models are trained with a total\nbatch size of 256 for about 100 epochs. We used Adam opti-\nmizer and varied learning rate with warmup schedule [42] ac-\ncording to the formula:\nlrate= k∗d0.5\nmodel ∗min(n−0.5,n ∗warmup n−1.5), (2)\nwhere nis the step number. k = 0.5 and warmup n = 5000\nwere chosen for all pre-training experiments.\nIn the ﬁne-tuning stage, all models are trained with a total\nbatch size of 128. The same warmup schedule is used except we\nchanged nto 1.0 and warmup nto 25000. For Transformer\nbased models, the Attention-CTC multi-task training objective\nis used, with weights determined on development set. Models\nfor HKUST and AISHELL is trained for 50 epochs while model\nfor Switchboard is trained for 100 epochs. Label smoothing of\n0.1 and weight decay of 1e−5 are applied for all ﬁne-tuning\nexperiments. SpecAugument [43] is applied when training for\nSwitchboard.\nIn the decoding stage, we selected 10 models with lowest\nerror rates on the validation set and averaged their parameters.\nBeam search with Attention-CTC joint decoding and RNN lan-\nguage model [44] is used for Transformer based models, with\nweights determined using grid search on development set. The\nbeam size is 10 for HKUST and AISHELL for Transformer\nbased models and 20 for Switchboard. WFST with word-level\nlanguage model is used for decoding with CTC.\nWe made our code publicly available for reproducibil-\nity at https://github.com/athena-team/athena/\ntree/mpc_improvement.\nTable 2: Character Error Rates(%) and Relative Error Rates\nReduction(%) on HKUST and AISHELL test set\nTask Pre-training Data Hours CER RERR\nHKUST\n- - 23.3 -\nHKUST 170 22.8 2.14\nOpen Mandarin (8k) 1150 22.7 2.58\nDidi Callcenter - 1K 1150 22.0 5.58\nDidi Callcenter 5000 21.5 7.73\nDidi Dictation (8k) 5000 22.1 5.15\nAISHELL\n- - 6.82 -\nAISHELL 178 6.61 3.07\nOpen Mandarin 1150 6.38 6.45\nDidi Dictation 5000 6.26 8.21\nTable 3: Word Error Rates(%) and Relative Error Rates Reduc-\ntion(%) on Switchboard and CallHome test set\nPre-training Data Hours WER RERR\nSWB CH SWB CH\n- - 8.8 17.8 - -\nSwitchboard 260 8.5 17.4 3.41 2.25\nFisher - 1K 960 8.0 16.2 9.09 8.99\nLibrispeech (8k) 960 8.4 17.2 4.55 3.37\n4.3. Effect of pre-training data speaking style\nThe results on HKUST and AISHELL with different pre-\ntraining data are listed in Table 2. Our baseline result with-\nout MPC matches the strong baseline in [39]. Comparing rel-\native error reduction of HKUST with same amounts of pre-\ntraining data, it is obvious MPC models pre-trained with match-\ning speaking style data achieved lower error rates for down-\nstream tasks.\nOpen Mandarin and Didi Dictation are both used in pre-\ntraining for HKUST and AISHELL. From Table 2, we can also\nﬁnd the relative error reduction using the same pre-training data\nis bigger with matching ﬁne-tuning data. Interestingly, the error\nreduction Didi Callcenter - 1K brings on HKUST is even bigger\nthan Didi Dictation, which suggests speaking style has a bigger\nimpact than pre-training data size in some cases.\nExperiments were also conducted on English ASR database\nSwitchboard. Our baseline results on Switchboard is also on par\nwith previous work [39]. As shown in Table 3, Switchboard got\nbetter results with spontaneous type pre-training data Fisher -\n1K than with Librispeech, which further conﬁrms our ﬁndings\non speaking style.\n4.4. MPC for streaming model\nWe ﬁrst tested the effect of directly initializing streaming model\nwith MPC. As shown in Table 4, pre-trained MPC brings con-\nsistent gains on streaming models for HKUST and AISHELL.\nMPC model pre-trained on more data obtained better results on\nstreaming models, which is also in line with the conclusion for\nfull-sequence models. Though not speciﬁcally optimized, MPC\npre-training is also useful for streaming models, which suggests\nthe parameter initialization obtained by MPC is helpful for both\nuni-directional and bidirectional models.\nTo further improve performance of MPC on streaming\nmodels, we tried to combine APC with MPC in pre-training\nTable 4: Character Error Rates(%) and Relative Error Rates\nReduction(%) for uni-directional CTC and uni-directional\nRNN-T with pre-trained MPC\nTask Model Pre-training Data CER RERR\nHKUST\nCTC\n- 29.3 -\nDidi Callcenter - 1K 28.0 4.56\nDidi Callcenter 27.6 5.80\nRNN-T\n- 28.1 -\nDidi Callcenter - 1K 26.9 4.43\nDidi Callcenter 26.6 5.20\nAISHELL\nCTC - 9.91 -\nOpen Mandarin 9.42 4.91\nRNN-T - 9.43 -\nOpen Mandarin 8.88 5.83\nTable 5: MPC + APC is the model pre-trained with APC 50% of\ntime and MPC 50% of time. Relative Error Rates Reduction(%)\nis calculated with HKUST baseline without MPC\nPre-training Data Objective CER RERR\nDidi Callcenter - 1K\nMPC 28.0 4.56\nAPC 27.8 5.12\nMPC + APC 27.2 7.32\nDidi Callcenter\nMPC 27.6 5.80\nAPC 27.9 4.72\nMPC + APC 26.8 8.46\nstage. Previous work on APC showed a future prediction step\nof 5 gave best results on Transformer decoder [13]. We im-\nplemented APC with the same future prediction step and set\nswitching probability p to 0.5. Experiments were conducted on\nHKUST dataset with Transformer CTC model and the results of\ndifferent pre-training objectives are presented in Table 5. When\nused alone, APC and MPC got similar improvements. Combin-\ning APC with MPC results in consistent gains over them, which\nechoes with the ﬁndings in [28].\n4.5. Knowledge transfer for MPC\nEach layer of pre-trained MPC encoder captures different fea-\ntures of the input speech. To get a quantitative measure of how\nimportant layer l is, for each layer, we create a new Transformer\nwith l layers of encoder and 6 layers of decoder. The encoder for\nthe new Transformer is frozen and initialized from pre-trained\nMPC model while the decoder is trainable. As shown in Fig\n2, features from the middle layers of pre-trained Transformer\nencoder are generally more helpful than features from top and\nbottom layers. In the ﬁne-tuning stage, we propose to set the\nlearning rates of encoder layers discriminatively by multiply-\ning the learning rate of layer l with λ∥l−θ∥, where λ∈(0, 1).\nTo ﬁt the accuracy curve above, we set λto be 0.95 and θto be\n5.5. This way, the parameters of middle layers are updated more\nslowly and knowledge from pre-trained MPC model is best re-\ntained. Different λand θ values have also been tested and no\nsigniﬁcant difference is observed. Using this schedule on base-\nline systems without MPC doesn’t lead to better accuracy either.\nThe results for target data adaptation and layer-wise dis-\ncriminative training are listed in Table 6. Two baseline re-\nFigure 2: CER(%) with each layer of pre-trained Transformer\nencoder. Results on HKUST is pre-trained with Didi Callcenter\nand results on AISHELL is pre-trained with Open Mandarin\nTable 6: Results on HKUST and AISHELL with different knowl-\nedge transfer methods. HKUST is pre-trained with Didi Call-\ncenter. AISHELL is pre-trained with Didi Dictation. Target\nData + Layer-wise is the combination of target data adaption\nand layer-wise discriminative training\nTask Knowledge Transfer Methods CER RERR\nHKUST\n- 21.5 -\nTarget Data Adaption 21.2 1.40\nLayer-wise Discriminative 21.3 0.93\nTarget Data + Layer-wise 20.8 3.26\nMulti-Task MPC 21.1 1.86\nAISHELL\n- 6.26 -\nTarget Data Adaption 6.23 0.48\nLayer-wise Discriminative 6.21 0.80\nTarget Data + Layer-wise 6.01 3.99\nMulti-Task MPC 6.13 2.08\nsults we used are HKUST pre-trained with Didi Callcenter and\nAISHELL pre-trained with Open Mandarin. Using target data\nadaption or layer-wise discriminative training alone doesn’t\nhelp the knowledge transfer of MPC very much. But when com-\nbined together, they provide consistent gains on downstream\ntasks.\nFor multi-task with MPC, the MPC loss should contribute\nmore to the joint loss to facilitate knowledge transfer in the ﬁrst\nfew epochs. As training proceeds, the task-speciﬁc component\nof the loss function becomes more important and γmpc should\nbe decreased. In this work, we empirically found it work well\nto set initial γmpc to 0.2 and decrease it by half every 5 epochs.\nThe result in Table 6 shows multi-task with MPC gets slight\nimprovements over baseline.\n5. Conclusion\nIn this work, we investigated three important aspects of MPC.\nPre-training data with matching speaking style was found to\nbe more useful on downstream recognition tasks. Using MPC\ndirectly on streaming models helps, but combining MPC with\nAPC brings further improvements on streaming models. Also,\nthe combination of target data adaption and layer-wise discrim-\ninative training provides consistent gains on knowledge transfer\nto downstream tasks.\n6. References\n[1] C. Doersch, A. Gupta, and A. Efros, “Unsupervised visual rep-\nresentation learning by context prediction,” in ICCV, 2015, pp.\n1422–1430.\n[2] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in NAACL-HLT (1), 2019, pp. 4171–4186.\n[3] O. A. van den, Y . Li, and V . Oriol, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748,\n2018.\n[4] A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec: Self-\nsupervised learning of discrete speech representations,” arXiv\npreprint arXiv:1910.05453, 2019.\n[5] A. Baevski, M. Auli, and A. rahman Mohamed, “Effectiveness\nof self-supervised pre-training for speech recognition,” arXiv\npreprint arXiv:1911.03912, 2019.\n[6] R. Mirco and B. Yoshua, “Learning speaker representations with\nmutual information,” Interspeech, Sep 2019.\n[7] S. Steffen, B. Alexei, C. Ronan, and A. Michael, “wav2vec: Un-\nsupervised pre-training for speech recognition,” Interspeech, Sep\n2019.\n[8] P. Santiago, R. Mirco, S. Joan, B. Antonio, and et al, “Learn-\ning problem-agnostic speech representations from multiple self-\nsupervised tasks,” Interspeech, Sep 2019.\n[9] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. van den\nOord, “Learning robust and multilingual speech representations,”\narXiv preprint arXiv:2001.11128, 2020.\n[10] M. Rivi `ere, A. Joulin, P.-E. Mazar ´e, and E. Dupoux, “Unsuper-\nvised pretraining transfers well across languages,” arXiv preprint\narXiv:2002.02848, 2020.\n[11] Z. Lian, Y . Li, J.Tao, and J. Huang, “Improving speech emo-\ntion recognition via transformer-based predictive coding through\ntransfer learning,” arXiv preprint arXiv:1811.07691, 2018.\n[12] C. Yu-An, H. Wei-Ning, T. Hao, and G. James, “An unsupervised\nautoregressive model for speech representation learning,” Inter-\nspeech, Sep 2019.\n[13] Y .-A. Chung and J. Glass, “Generative pre-training for\nspeech with autoregressive predictive coding,” arXiv preprint\narXiv:1910.12607, 2019.\n[14] C. Yu-An and G. James, “Improved speech representations with\nmulti-target autoregressive predictive coding,” arXiv preprint\narXiv:2004.05274, 2020.\n[15] A. T. Liu, S. Yang, P.-H. Chi, P.-C. Hsu, and et al, “Mockingjay:\nUnsupervised speech representation learning with deep bidirec-\ntional transformer encoders,” arXiv preprint arXiv:1910.12638,\n2019.\n[16] W. Wang, Q. Tang, and K. Livescu, “Unsupervised pre-training of\nbidirectional speech encoders via masked reconstruction,” arXiv\npreprint arXiv:2001.10603, 2020.\n[17] X. Song, G. Wang, Z. Wu, Y . Huang, and et al, “Speech-xlnet:\nUnsupervised acoustic model pretraining for self-attention net-\nworks,”arXiv preprint arXiv:1910.10387, 2019.\n[18] D. Jiang, X. Lei, W. Li, N. Luo, and et al, “Improving transformer-\nbased speech recognition using unsupervised pre-training,” arXiv\npreprint arXiv:1910.09932, 2019.\n[19] B. Mohamed, D. Renato, D. Olivier, D. Stephane, and et al, “Au-\ntomatic speech recognition and speech variability: A review,”\nSpeech communication, vol. 49, no. 10-11, pp. 763–786, 2007.\n[20] W. Mitch, T. Kelsey, H. Kate, and S. Amy, “Effect of speaking\nstyle on lvcsr performance,” in Proc. ICSLP, vol. 96, 1996.\n[21] J. Howard and S. Ruder, “Universal language model ﬁne-tuning\nfor text classiﬁcation,” in ACL, 2018.\n[22] A. Chronopoulou, C. Baziotis, and A. Potamianos, “An embar-\nrassingly simple approach for transfer learning from pretrained\nlanguage models,” Proceedings of the 2019 Conference of the\nNorth, 2019.\n[23] C. Sun, X. Qiu, Y . Xu, and X. Huang, “How to ﬁne-tune bert for\ntext classiﬁcation?” arXiv preprint arXiv:1905.05583, 2019.\n[24] Y . Liu, O. Myle, G. Naman, J. Du, and et al, “Roberta: A\nrobustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[25] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,” in ICASSP, 2016, pp. 4960–4964.\n[26] T. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kannan, and et al,\n“Improving the performance of online neural transducer models,”\nin ICASSP, 2018, pp. 5864–5868.\n[27] H. Miao, G. Cheng, C. Gao, P. Zhang, and Y . Yan, “Transformer-\nbased online ctc/attention end-to-end speech recognition architec-\nture,” arXiv preprint arXiv:2001.08290, 2020.\n[28] L. Dong, N. Yang, W. Wang, F. Wei, and et al, “Uniﬁed language\nmodel pre-training for natural language understanding and gener-\nation,” in NeurIPS, 2019.\n[29] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and et al,\n“An empirical investigation of catastrophic forgetting in gradient-\nbased neural networks,”arXiv preprint arXiv:1312.6211, 2013.\n[30] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, and et al.,\n“Overcoming catastrophic forgetting in neural networks,” Pro-\nceedings of the National Academy of Sciences, vol. 114, no. 13, p.\n35213526, Mar 2017.\n[31] J. Yosinski, J. Clune, Y . Bengio, and H. Lipson, “How transferable\nare features in deep neural networks?” in NIPS, 2014.\n[32] N. F. Liu, M. Gardner, Y . Belinkov, M. E. Peters, and et al, “Lin-\nguistic knowledge and transferability of contextual representa-\ntions,” arXiv preprint arXiv:1903.08855, 2019.\n[33] X. Shi, I. Padhi, and K. Knight, “Does string-based neural mt\nlearn source syntax?” in EMNLP, 2016.\n[34] Y . Adi, E. Kermany, Y . Belinkov, O. Lavi, and et al, “Fine-grained\nanalysis of sentence embeddings using auxiliary prediction tasks,”\narXiv preprint arXiv:1608.04207, 2017.\n[35] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: An asr corpus based on public domain audio books,”\nin ICASSP, 2015, pp. 5206–5210.\n[36] C. Cieri, D. Miller, and K. Walker, “The ﬁsher corpus: a resource\nfor the next generations of speech-to-text,” inLREC, 2004.\n[37] W. Zou, D. Jiang, S. Zhao, G. Yang, and et al, “Comparable study\nof modeling units for end-to-end mandarin speech recognition,”\nin ISCSLP, 2018, pp. 369–373.\n[38] T. Zenkel, R. Sanabria, F. Metze, and A. H. Waibel, “Subword and\ncrossword units for ctc acoustic models,” inInterspeech, 2018.\n[39] K. Shigeki, C. Nanxin, H. Tomoki, H. Takaaki, and et al, “A com-\nparative study on transformer vs rnn in speech applications,”arXiv\npreprint arXiv:1909.06317, 2019.\n[40] J. Salazar, K. Kirchhoff, and Z. Huang, “Self-attention networks\nfor connectionist temporal classiﬁcation in speech recognition,” in\nICASSP, 2019, pp. 7115–7119.\n[41] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y . Wang, and et al,\n“Transformer-transducer: End-to-end speech recognition with\nself-attention,” arXiv preprint arXiv:1910.12977, 2019.\n[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, and et al, “At-\ntention is all you need,” in NIPS, 2017, pp. 5998–6008.\n[43] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, and et al, “Specaug-\nment: A simple data augmentation method for automatic speech\nrecognition,” in Interspeech, 2019.\n[44] K. Suyoun, H. Takaaki, and W. Shinji, “Joint ctc-attention\nbased end-to-end speech recognition using multi-task learning,”\nin ICASSP, Mar 2017.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7997719645500183
    },
    {
      "name": "Transformer",
      "score": 0.7301653027534485
    },
    {
      "name": "Discriminative model",
      "score": 0.6960771679878235
    },
    {
      "name": "Predictive coding",
      "score": 0.5724450349807739
    },
    {
      "name": "Speech recognition",
      "score": 0.5447325706481934
    },
    {
      "name": "Training set",
      "score": 0.5260286927223206
    },
    {
      "name": "Machine learning",
      "score": 0.47480738162994385
    },
    {
      "name": "Transfer of learning",
      "score": 0.4722770154476166
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45744937658309937
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.37471675872802734
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36820513010025024
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}