{
  "title": "Enhancing Genetic Improvement Mutations Using Large Language Models",
  "url": "https://openalex.org/W4388184260",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4284272960",
      "name": "Brownlee, Alexander E. I.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Callan, James",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Even-Mendoza, Karine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4320534009",
      "name": "Geiger, Alina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4317848789",
      "name": "Hanna, Carol",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287134182",
      "name": "Petke, Justyna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745370841",
      "name": "Sarro, Federica",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4228113453",
      "name": "Sobania, Dominik",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2953998535",
    "https://openalex.org/W4385326687",
    "https://openalex.org/W4362598291",
    "https://openalex.org/W4367694420",
    "https://openalex.org/W4385750097",
    "https://openalex.org/W4386302275",
    "https://openalex.org/W4385302156",
    "https://openalex.org/W4387427818",
    "https://openalex.org/W4255632703",
    "https://openalex.org/W3083119198",
    "https://openalex.org/W4384157259",
    "https://openalex.org/W4386081573",
    "https://openalex.org/W2740264376",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2608579592",
    "https://openalex.org/W3151195892",
    "https://openalex.org/W4385652328"
  ],
  "abstract": "Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.",
  "full_text": "arXiv:2310.19813v1  [cs.SE]  18 Oct 2023\nEnhancing Genetic Improvement Mutations\nUsing Large Language Models\nAlexander E.I. Brownlee 1[0000− 0003− 2892− 5059], James\nCallan2[0000− 0002− 5692− 6203], Karine Even-Mendoza 3[0000− 0002− 3099− 1189], Alina\nGeiger4[0009− 0002− 3413− 283X], Carol Hanna 2[0009− 0009− 7386− 1622], Justyna\nPetke2[0000− 0002− 7833− 6044], Federica Sarro 2[0000− 0002− 9146− 442X], and\nDominik Sobania 4[0000− 0001− 8873− 7143]\n1 University of Stirling, UK\n2 University College London, UK\n3 King’s College London, UK\n4 Johannes Gutenberg University Mainz, Germany\nAbstract. Large language models (LLMs) have been successfully ap-\nplied to software engineering tasks, including program rep air. However,\ntheir application in search-based techniques such as Genet ic Improve-\nment (GI) is still largely unexplored. In this paper, we eval uate the use\nof LLMs as mutation operators for GI to improve the search pro cess. We\nexpand the Gin Java GI toolkit to call OpenAI’s API to generate edits\nfor the JCodec tool. We randomly sample the space of edits usi ng 5 dif-\nferent edit types. We ﬁnd that the number of patches passing u nit tests is\nup to 75% higher with LLM-based edits than with standard Inse rt edits.\nFurther, we observe that the patches found with LLMs are gene rally less\ndiverse compared to standard edits. We ran GI with local sear ch to ﬁnd\nruntime improvements. Although many improving patches are found by\nLLM-enhanced GI, the best improving patch was found by stand ard GI.\nKeywords: Large language models · Genetic Improvement\n1 Introduction\nAs software systems grow larger and more complex, signiﬁcant man ual eﬀort is\nrequired to maintain them [2]. To reduce developer eﬀort in softwar e mainte-\nnance and optimization tasks, automated paradigms are essential. Genetic Im-\nprovement (GI) [15] applies search-based techniques to improve n on-functional\nproperties of existing software such as execution time as well as fu nctional prop-\nerties like repairing bugs. Although GI has had success in industry [1 2,13], it\nremains limited by the set of mutation operators it employs in the sear ch [14].\nLarge Language Models (LLMs) have a wide range of applications as t hey are\nable to process textual queries without additional training for the particular task\nat hand. LLMs have been pre-trained on millions of code repositories spanning\nmany diﬀerent programming languages [5]. Their use for software en gineering\ntasks has had great success [9,6], showing promise also for program repair [17,19].\n2 A. Brownlee et al.\nKang and Yoo [10] have suggested that there is untapped potentia l in using LLMs\nto enhance GI. GI uses the same mutation operators for diﬀerent optimization\ntasks. These operators are hand-crafted prior to starting the search and thus\nresult in a limited search space. We hypothesize that augmenting LLM patch\nsuggestions as an additional mutation operator will enrich the sear ch space and\nresult in more successful variants.\nIn this paper, we conduct several experiments to explore whethe r using LLMs\nas a mutation operator in GI can improve the eﬃciency and eﬃcacy of the search.\nOur results show that the LLM-generated patches have compilatio n rates of\n51.32% and 53 .54% for random search and local search, respectively (with the\nMedium prompt category). Previously LLMs (using an LLM model as-is) wer e\nshown to produce code that compiled roughly 40% of the time [16,18]. W e ﬁnd\nthat randomly sampled LLM-based edits compiled and passed unit tes ts more\noften compared to standard GI edits. We observe that the numbe r of patches\npassing unit tests is up to 75% higher for LLM-based edits than GI Insert edits.\nHowever, we observe that patches found with LLMs are less divers e. For local\nsearch, the best improvement is achieved using standard GI Statement edits,\nfollowed by LLM-based edits. These ﬁndings demonstrate the pote ntial of LLMs\nas mutation operators and highlight the need for further researc h in this area.\n2 Experimental Setup\nTo analyze the use of LLMs as a mutation operator in GI, we used the GPT\n3.5 Turbo model by OpenAI and the GI toolbox Gin [3]. We experimented with\ntwo types of search implemented within Gin: random search and local search.\nRequests to the LLM using the OpenAI API were via the Langchain4J library,\nwith a temperature of 0 .7. The target project for improvement in our experiments\nwas the popular JCodec [7] project which is written in Java. ‘Hot’ met hods to\nbe targeted by the edits were identiﬁed using Gin’s proﬁler tool by re peating the\nproﬁling 20 times and taking the union of the resulting set.\nFor the random sampling experiments, we set up the runs with state ment-\nlevel edits (copy/delete/replace/swap from [14] and insert brea k/continue/return\nfrom [4]) and LLM edits, generating 1000 of each type at random. A t imeout of\n10000 milliseconds was used for each unit test to catch inﬁnite loops in troduced\nby edits; exceeding the timeout counts as a test failure. For local s earch, exper-\niments were set up similarly. There were 10 repeat runs (one for eac h of the top\n10 hot methods) but the runs were limited to 100 evaluations resultin g in 1000\nevaluations in total, matching the random search. In practice this w as 99 edits\nper run as the ﬁrst was used to time the original unpatched code.\nWe experimented with three diﬀerent prompts for sending request s to the\nLLM for both types of search: a simple prompt, a medium prompt, and a\ndetailed prompt. With all three prompts, our implementation requests ﬁve\ndiﬀerent variations of the code at hand. The simple prompt only requests the\ncode without any additional information. The medium prompt provides more\ninformation about the code provided and the requirements, as sho wn in Fig-\nEnhancing Genetic Improvement Mutations Using Large Langu age Models 3\nGive me 5 d i f f e r e n t Java im p lem en t at ion s o f t h i s method body :\n‘ ‘ ‘\n<code >\n‘ ‘ ‘\nThis code b elon gs to p r o j e c t <projectname >.\nWrap a l l code in c u r l y braces , i f i t i s not a l r e a d y .\nDo not i n c l u d e any method or c l a s s d e c l a r a t i o n s .\nl a b e l a l l code as jav a .\nFig. 1.The medium prompt for LLM requests, with line breaks added for readabil ity.\nure 1. Speciﬁcally, we provide the LLM with the programming language used,\nthe project that the code belongs to, as well as formatting instru ctions. The de-\ntailed prompt extends the medium prompt with an example of a useful change.\nThis example was taken from results obtained by Brownlee et al. [4]. T he patch\nis a successful instance of the insert edit applied to the jCodec project (i.e., an\nedit that compiled, passed the unit tests and oﬀered a speedup ove r the original\ncode). We use the same example for all the detailed prompt requests used\nin our experiments; this is because LLMs are capable of inductive rea soning\nwhere the user presents speciﬁc information, and the LLM can use that input\nto generate more general statements, further improved in GPT- 4 [8].\nLLM edits are applied by selecting a block statement at random in a tar get\n‘hot’ method. This block’s content is <code> in the prompt. The ﬁrst code block\nin the LLM response is identiﬁed. Gin uses JavaParser ( https://javaparser.org)\ninternally to represent target source ﬁles, so we attempt to pars e the LLM sug-\ngestion with JavaParser, and replace the original block with the LLM suggestion.\n3 Results\nThe ﬁrst experiment compares standard GI mutations, namely Insert and\nStatement edits, with LLM edits using diﬀerently detailed prompts ( Simple,\nMedium, and Detailed) using Random Sampling. Table 1 shows results for\nall patches as well as for unique patches only. We report how many p atches\nwere successfully parsed by JavaParser (named as Valid), how man y compiled,\nand how many passed all unit tests (named as Passed). We excluded patches\nsyntactically equivalent to the original software. Best results are in bold.\nWe see that although substantially more valid patches were found wit h the\nstandard Insert and Statement edits, more passing patches could be found by\nusing the LLM-generated edits. In particular, for the Medium, and Detailed\nprompts 292 and 230 patches passed the unit tests, respectively . For the Insert\nand Statement edits only 166 and 91 passed the unit tests, respectively. Anec-\ndotally, the hot methods with lowest/highest patch pass rates diﬀe red for each\noperator: understanding this variation will be interesting for futu re investigation.\nIt is also notable that LLM patches are less diverse: over 50% more u nique\npatches were found by standard mutation operators than the LL M using Medium,\n4 A. Brownlee et al.\nTable 1. Results of our Random Sampling experiment. We exclude patch es syntacti-\ncally equivalent to the original software in this table. For all and unique patches we\nreport: how many patches passed JavaParser, compiled, and p assed all unit tests.\nUnique All\nEditCategory Patches Valid Compiled Passed Patches Valid Compiled Passed\nStatement 896 819 199 80 967 869 227 91\nInsert 785 785 284 161 860 860 295 166\nSimple 193 0 0 0 1000 0 0 0\nMedium 324 260 183 154 645 463 331 292\nDetailed 332 268 126 110 606 456 250 230\nTable 2. Local Search results. We exclude all empty patches. We repor t how many\npatches compiled, passed all unit tests, and how many led to i mprovements in runtime.\nWe report best improvement found and median improvement amo ng improving patches.\nEditCategory Patches Compiled Passed ImpFound BestImp(ms) Median(ms)\nStatement 990 213 105 71 508.0 137.0\nInsert 948 414 264 136 313.0 81.0\nSimple 990 2 2 2 176.0 137.5\nMedium 990 530 520 164 395.0 75.5\nDetailed 990 379 369 196 316.0 95.0\nand Detailed prompts. With the Simple prompt, however, not a single patch\npassed the unit tests, since the suggested edits often could not b e parsed. Thus\ndetailed prompts are necessary to force LLM to generate usable o utputs.\nWe investigated further the diﬀerences between Medium and Detailed\nprompts to understand the reduction in performance with Detailed (in the\nunique patches sets) as Medium had a higher number of compiled and passed\npatches. In both prompt levels, the generated response was the same for 42\ncases (out of the total unique valid cases). However, Detailed tended to gen-\nerate longer responses with an average of 363 characters, wher eas Medium had\nan average of 304 characters. We manually examined several Detailed prompt\nresponses, in which we identiﬁed some including variables from other ﬁ les, poten-\ntially oﬀering a signiﬁcant expansion of the set of code variants GI ca n explore.\nThe second experiment expands our analysis, comparing the perfo rmance of\nthe standard and LLM edits with Local Search. Table 2 shows the re sults of\nthe Local Search experiment. We report the number of compiling an d passing\npatches as well as the number of patches were runtime improvemen ts were found.\nFurthermore, we report the median and best improvement in milliseco nds (ms).\nIn the table, we excluded all empty patches. As before, best resu lts are in bold.\nAgain, we see that more patches passing the unit tests could be fou nd with\nthe LLM using the Medium, and Detailed prompts. In addition, more improve-\nments could be found by using the LLM with these prompts. Speciﬁca lly, with\nMedium and Detailed, we found 164 and 196 improvements, respectively, while\nwe only found 136 with Insert and 71 with Statement. The best improvement\ncould be found with 508 ms with the Statement edit. The best improvement\nfound using LLMs (using the Medium prompt) was only able to improve the\nruntime by 395 ms. We also examined a series of edits in Local Search r esults to\nEnhancing Genetic Improvement Mutations Using Large Langu age Models 5\ngain insights into the distinctions between Medium and Detailed prompts due\nto the low compilation rate of Detailed prompt’s responses. In the example, a\nsequence of edits aimed to inline a call to function clip. The Detailed prompt\ntried to incorporate the call almost immediately within a few edits, likely lead-\ning to invalid code. On the other hand, the Medium prompt made less radical\nchanges, gradually reﬁning the code. It began by replacing the ter nary opera-\ntor expression with an if-then-else statement and system functio n calls before\neventually attempting to inline the clip function call.\n4 Conclusions and Future Work\nGenetic improvement of software is highly dependent on the mutatio n operators\nit utilizes in the search process. To diversify the operators and enr ich the search\nspace further, we incorporated a Large Language Model (LLM) a s an operator.\nLimitations. To generalise, future work should consider projects besides\nour target, jCodec. Our experiments used an API giving us no cont rol over the\nresponses generated by the LLM or any way of modifying or optimizin g them.\nThough we did not observe changes in behaviour during our experime nts, Ope-\nnAI may change the model at any time, so future work should consid er local\nmodels. We experimented with only three prompt types for LLM requ ests and\nwithin this limited number of prompts found a variation in the results. F inally,\nour implementation for parsing the responses from the LLMs was re latively sim-\nplistic. However, this would only mean that our reported results are pessimistic\nand an even larger improvement might be achieved by the LLM-based operator.\nSummary. We found that, although more valid and diverse patches were\nfound with standard edits using Random Sampling, more patches pas sing the\nunit tests were found with LLM-based edits. For example, with the L LM edit\nusing the Medium prompt, we found over 75% more patches passing the unit\ntests than with the classic Insert edit. In our Local Search experiment, we\nfound the best improvement with the Statement edit (508 ms). The best LLM-\nbased improvement was found with the Medium prompt (395 ms). Thus there\nis potential in exploring approaches combining both LLM and ‘classic’ G I edits.\nOur experiments revealed that the prompts used for LLM request s greatly\naﬀect the results. Thus, in future work, we hope to experiment mo re with prompt\nengineering. It might also be helpful to mix prompts: e.g., starting wit h medium\nthen switching to detailed to make larger edits that break out of local minima.\nFurther, the possibility of combining LLM edits with others such as st andard\ncopy/delete/replace/swap or PAR templates [11] could be interest ing. Finally,\nwe hope to conduct more extensive experimentation on additional t est programs.\nData Availability. The code, LLMs prompt and experimental infrastruc-\nture, data from the evaluation, and results are available as open so urce at [1].\nThe code is also under the ‘llm’ branch of github.com/gintool/gin (commit\n9fe9bdf; branched from master commit 2359f57 pending full integ ration with\nGin).\n6 A. Brownlee et al.\nAcknowledgements UKRI EPSRC EP/P023991/1 and ERC 741278.\nReferences\n1. Artifact of Enhancing Genetic Improvement Mutations Usi ng Large Language\nModels. Zenodo (Sep 2023). https://doi.org/10.5281/zenodo.8304433\n2. B¨ ohme, M., Soremekun, E.O., Chattopadhyay, S., Ugherug he, E., Zeller, A.: Where\nis the bug and how is it ﬁxed? An experiment with practitioner s. In: Proc. ACM\nSymposium on the Foundations of Software Engineering. pp. 1 17–128 (2017)\n3. Brownlee, A.E., Petke, J., Alexander, B., Barr, E.T., Wag ner, M., White, D.R.:\nGin: genetic improvement research made easy. In: GECCO. pp. 985–993 (2019)\n4. Brownlee, A.E., Petke, J., Rasburn, A.F.: Injecting shor tcuts for faster running\nJava code. In: IEEE CEC 2020. p. 1–8\n5. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., K aplan, J., Edwards,\nH., Burda, Y., Joseph, N., Brockman, G., et al.: Evaluating l arge language models\ntrained on code. arXiv preprint arXiv:2107.03374 (2021)\n6. Fan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupt a, S., Yoo, S., Zhang,\nJ.M.: Large language models for software engineering: Surv ey and open problems\n(2023)\n7. Github - jcodec/jcodec: Jcodec main repo, https://github.com/jcodec/jcodec\n8. Han, S.J., Ransom, K.J., Perfors, A., Kemp, C.: Inductive reasoning in humans\nand large language models. Cognitive Systems Research p. 10 1155 (2023)\n9. Hou, X., Liu, Y., Yang, Z., Grundy, J., Zhao, Y., Li, L., Wan g, K., Luo, X., Lo, D.,\nWang, H.: Large language models for software engineering: A systematic literature\nreview. arXiv:2308.10620 (2023)\n10. Kang, S., Yoo, S.: Towards objective-tailored genetic i mprovement through large\nlanguage models. arXiv:2304.09386 (2023)\n11. Kim, D., Nam, J., Song, J., Kim, S.: Automatic Patch Gener ation Learned from\nHuman-Written Patches (2013), http://logging.apache.org/log4j/\n12. Kirbas, S., Windels, E., Mcbello, O., Kells, K., Pagano, M., Szalanski, R., Nowack,\nV., Winter, E., Counsell, S., Bowes, D., Hall, T., Haraldsso n, S., Woodward, J.:\nOn the introduction of automatic program repair in bloomber g. IEEE Software\n38(4), 43–51 (2021)\n13. Marginean, A., Bader, J., Chandra, S., Harman, M., Jia, Y ., Mao, K., Mols, A.,\nScott, A.: Sapﬁx: Automated end-to-end repair at scale. In: ICSE-SEIP. pp. 269–\n278 (2019)\n14. Petke, J., Alexander, B., Barr, E.T., Brownlee, A.E., Wa gner, M., White, D.R.:\nProgram transformation landscapes for automated program m odiﬁcation using\nGin. Empirical Software Engineering 28(4), 1–41 (2023)\n15. Petke, J., Haraldsson, S.O., Harman, M., Langdon, W.B., White, D.R., Woodward,\nJ.R.: Genetic improvement of software: A comprehensive sur vey. IEEE Transac-\ntions on Evolutionary Computation 22, 415–432 (2018)\n16. Siddiq, M.L., Santos, J., Tanvir, R.H., Ulfat, N., Rifat , F.A., Lopes, V.C.: Exploring\nthe eﬀectiveness of large language models in generating uni t tests. arXiv preprint\narXiv:2305.00418 (2023)\n17. Sobania, D., Briesch, M., Hanna, C., Petke, J.: An analys is of the automatic bug\nﬁxing performance of chatgpt. In: 2023 IEEE/ACM Internatio nal Workshop on\nAutomated Program Repair (APR). pp. 23–30. IEEE Computer So ciety (2023)\n18. Xia, C.S., Paltenghi, M., Tian, J.L., Pradel, M., Zhang, L.: Universal fuzzing via\nlarge language models. arXiv preprint arXiv:2308.04748 (2 023)\nEnhancing Genetic Improvement Mutations Using Large Langu age Models 7\n19. Xia, C.S., Zhang, L.: Keep the conversation going: Fixin g 162 out of 337 bugs for\n$0.42 each using chatgpt. arXiv preprint arXiv:2304.00385 ( 2023)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7302603721618652
    },
    {
      "name": "Java",
      "score": 0.6282165050506592
    },
    {
      "name": "Mutation",
      "score": 0.4982297420501709
    },
    {
      "name": "Sample (material)",
      "score": 0.42056870460510254
    },
    {
      "name": "Software",
      "score": 0.41951999068260193
    },
    {
      "name": "World Wide Web",
      "score": 0.4087763726711273
    },
    {
      "name": "Programming language",
      "score": 0.30508679151535034
    },
    {
      "name": "Biology",
      "score": 0.12510743737220764
    },
    {
      "name": "Gene",
      "score": 0.08607342839241028
    },
    {
      "name": "Genetics",
      "score": 0.07853785157203674
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12093191",
      "name": "University of Stirling",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I183935753",
      "name": "King's College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I197323543",
      "name": "Johannes Gutenberg University Mainz",
      "country": "DE"
    }
  ],
  "cited_by": 2
}