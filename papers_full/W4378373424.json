{
  "title": "Swin-Conv-Dspp and Global Local Transformer for Remote Sensing Image Semantic Segmentation",
  "url": "https://openalex.org/W4378373424",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101600311",
      "name": "Youda Mo",
      "affiliations": [
        "Guangdong Polytechnic Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5100338276",
      "name": "Huihui Li",
      "affiliations": [
        "Guangdong Polytechnic Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5002529570",
      "name": "Xiangling Xiao",
      "affiliations": [
        "Guangdong Polytechnic Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5075054710",
      "name": "Huimin Zhao",
      "affiliations": [
        "Guangdong Polytechnic Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5100707683",
      "name": "Xiaoyong Liu",
      "affiliations": [
        null,
        "Guangdong Polytechnic Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5101476627",
      "name": "Jin Zhan",
      "affiliations": [
        "Guangdong Polytechnic Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739696289",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3153239544",
    "https://openalex.org/W3137415843",
    "https://openalex.org/W4294982749",
    "https://openalex.org/W4285125754",
    "https://openalex.org/W4312442876",
    "https://openalex.org/W4312960790",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W6804061671",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W4285301526",
    "https://openalex.org/W6751796012",
    "https://openalex.org/W3163966128",
    "https://openalex.org/W3012817855",
    "https://openalex.org/W3165935199",
    "https://openalex.org/W4312628443",
    "https://openalex.org/W3191020254",
    "https://openalex.org/W4312743284",
    "https://openalex.org/W4205365435",
    "https://openalex.org/W3119502086",
    "https://openalex.org/W2995766874",
    "https://openalex.org/W6803421043",
    "https://openalex.org/W4200177764",
    "https://openalex.org/W4210576848",
    "https://openalex.org/W3161825146",
    "https://openalex.org/W4286217684",
    "https://openalex.org/W4214533736",
    "https://openalex.org/W3130491581",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3003394660",
    "https://openalex.org/W4200174151",
    "https://openalex.org/W3190771647",
    "https://openalex.org/W3138096437",
    "https://openalex.org/W4213200979",
    "https://openalex.org/W4206422044",
    "https://openalex.org/W4294496168",
    "https://openalex.org/W3203535769",
    "https://openalex.org/W3144293453",
    "https://openalex.org/W3043740003",
    "https://openalex.org/W3209745495",
    "https://openalex.org/W4206356281",
    "https://openalex.org/W3023329043",
    "https://openalex.org/W3184629543",
    "https://openalex.org/W4285288041",
    "https://openalex.org/W3202923600",
    "https://openalex.org/W3168216528",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W4205138939",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3214707056",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W4296425595",
    "https://openalex.org/W4283450732",
    "https://openalex.org/W4283688235",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W3213833596",
    "https://openalex.org/W4225916372",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3215916782",
    "https://openalex.org/W2805516822"
  ],
  "abstract": "Compared with the traditional method based on hand-crafted features, deep neural network has achieved a certain degree of success on remote sensing (RS) image semantic segmentation. However, there are still serious holes, rough edge segmentation, and false detection or even missed detection due to the light and its shadow in the segmentation. Aiming at the above problems, this article proposes a RS semantic segmentation model SCG-TransNet that is a hybrid model of Swin transformer and Deeplabv3+, which includes Swin-Conv-Dspp (SCD) and global local transformer block (GLTB). First, the SCD module which can efficiently extract feature information from objects at different scales is used to mitigate the hole phenomenon, reducing the loss of detailed information. Second, we construct a GLTB with spatial pyramid pooling shuffle module to extract critical detail information from the limited visible pixels of the occluded objects, which alleviates the problem of difficult object recognition due to occlusion effectively. Finally, the experimental results show that our SCG-TransNet achieves a mean intersection over union of 70.29<inline-formula><tex-math notation=\"LaTeX\">$\\%$</tex-math></inline-formula> on the Vaihingen datasets, which is 3<inline-formula><tex-math notation=\"LaTeX\">$\\%$</tex-math></inline-formula> higher than the baseline model. It also achieved good results on POSDAM datasets. These demonstrate the effectiveness, robustness, and superiority of our proposed method compared with existing state-of-the-art methods.",
  "full_text": "5284 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nSwin-Conv-Dspp and Global Local Transformer for\nRemote Sensing Image Semantic Segmentation\nYouda Mo , Huihui Li , Xiangling Xiao , Huimin Zhao , Xiaoyong Liu , and Jin Zhan\nAbstract—Compared with the traditional method based on\nhand-crafted features, deep neural network has achieved a cer-\ntain degree of success on remote sensing (RS) image semantic\nsegmentation. However, there are still serious holes, rough edge\nsegmentation, and false detection or even missed detection due to\nthe light and its shadow in the segmentation. Aiming at the above\nproblems, this article proposes a RS semantic segmentation model\nSCG-TransNet that is a hybrid model of Swin transformer and\nDeeplabv3+, which includes Swin-Conv-Dspp (SCD) and global\nlocal transformer block (GLTB). First, the SCD module which\ncan efﬁciently extract feature information from objects at differ-\nent scales is used to mitigate the hole phenomenon, reducing the\nloss of detailed information. Second, we construct a GLTB with\nspatial pyramid pooling shufﬂe module to extract critical detail\ninformation from the limited visible pixels of the occluded objects,\nwhich alleviates the problem of difﬁcult object recognition due to\nocclusion effectively. Finally, the experimental results show that our\nSCG-TransNet achieves a mean intersection over union of 70.29 %\non the Vaihingen datasets, which is 3 % higher than the baseline\nmodel. It also achieved good results on POSDAM datasets. These\ndemonstrate the effectiveness, robustness, and superiority of our\nproposed method compared with existing state-of-the-art methods.\nIndex Terms—Global local transformer block (GLTB), remote\nsensing (RS) image, semantic segmentation, Swin transformer,\nSwin-Conv-Dspp (SCD).\nI. INTRODUCTION\nS\nEMANTIC segmentation provides pixel-level classiﬁcation\nand is applied in many real applications. In the ﬁeld of\nManuscript received 27 November 2022; revised 17 February 2023, 23\nFebruary 2023, and 24 April 2023; accepted 23 May 2023. Date of publication\n26 May 2023; date of current version 16 June 2023. This work was supported\nby the National Natural Science Foundation of China under Grant 62006049,\nGrant 62072122, and Grant 62172113, in part by The Ministry of Education\nof Humanities and Social Science Project under Grant 18JDGC012, in part by\nGuangdong Science and Technology Project under Grant KTP20210197 and\nGrant 2017A040403068, in part by Project of Education Department of Guang-\ndong Province under Grant 2022KTSCX068 and Grant 2022ZDZX1013, and in\npart by Guangdong Science and Technology Innovation Strategy Special Fund\nProject (Climbing Plan) under Grant pdjh2022b0302 and Grant pdjh2022a0290.\n(Corresponding authors: Huihui Li; Xiangling Xiao.)\nYouda Mo, Xiangling Xiao, Huimin Zhao, and Jin Zhan are with the\nSchool of Computer Science, Guangdong Polytechnic Normal University,\nGuangzhou 510665, China (e-mail: 330462897@qq.com; 979432400@qq.com;\nzhaohuimin@gpnu.edu.cn; jinerzhan@163.com).\nHuihui Li is with the School of Computer Science and Guangdong Provincial\nKey Laboratory of Intellectual Property and Big Data, Guangdong Polytechnic\nNormal University, Guangzhou 510665, China (e-mail: 29777562@qq.com).\nXiaoyong Liu is with the School of Data Science and Engineering, Guangdong\nPolytechnic Normal University, Guangzhou 510665, China, and also with the\nInstitute of GPNU, Heyuan 517002, China (e-mail: 35643506@qq.com).\nThe code will be available at https://github.com/yuwxyun275/SCG-\nTransNet.\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3280365\nremote sensing (RS), semantic segmentation is also known as\nland use and land cover type classiﬁcation[1]. In addition, RS\ntechnology can provide rich data sources for Earth observation.\nAt present, RS images have been widely used in urban plan-\nning [2], [3], [4], housing planning[5], road detection[6], and\nforest protection[7], [8].\nIn recent years, with the rapid development of deep learning\ntechnology, segmentation models based on convolutional neural\nnetworks (CNN) and full convolutional networks (FCN) [9]\nhave gradually become the most advanced image processing\ntechnology. In the process of this development, the encoder–\ndecoder [10] structure showed extremely good segmentation\nperformance, which also made it gradually become the ba-\nsic architecture of many excellent models in the future. As a\nwell-known encoder–decoder network model, UNet[11] fuses\nthe feature information of deep granularity and shallow gran-\nularity through skip connections, which effectively alleviates\nthe feature information lost by upsampling and downsampling.\nIn addition, the well-known DeeplabV3+ [12] also follows\nthe encoder–decoder structure, which is mainly improved on\nDeeplabV3 [13]. It extracts the information from different scales\nof the deep feature map in the encoder by using the hole\nspatial pyramid pooling, and fuses it with the shallow feature\ninformation in the decoder stage. Finally, it achieves very good\nperformance.\nHowever, RS images have complex imaging, redundant infor-\nmation, high similarity between classes, and are easily affected\nby the particularity of light intensity, light incident angle, and\nground objects (small scale[14], high similarity[15], and mutual\nocclusion [16]). We have summarized two main issues, as shown\nin Fig.1. From the examples, we can see that they are ﬁlled with\na large number of objects occluded by shadows, easily leading\nto rough segmentation of the target edge and serious holes in\nthe segmentation, which resulted it faces huge challenges in\nthe application process. How to effectively utilize the occluded\nobjects which have extremely small number of pixels has be-\ncome the key to RS image segmentation. In traditional CNN,\nthe encoder often uses multiple downsampling to reduce the\namount of computation while increasing the receptive ﬁeld. But\nmultiple downsampling tends to lose a lot of valuable informa-\ntion, especially for occluded objects. If such a small number\nof precious pixels is lost, the effect of identifying occluded\nobjects will become very bad. And CNN have inductive biases\nof locality and weight sharing[17], which lead to their inevitable\nconstraints in learning long-range dependencies[18] and spatial\ncorrelations [19].\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see\nhttps://creativecommons.org/licenses/by-nc-nd/4.0/\nMO et al.: SWIN-CONV-DSPP AND GLOBAL LOCAL TRANSFORMER FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION 5285\nFig. 1. Examples of the characteristics of RS images, which are taken from\nthe Vaihingen dataset. 1) Affected by the shadow of the light. Like (a) and (b),\nit is hard to recognize the “Car,” “Low vegetation,” “Impervious surface,” and\n“Tree”; 2) Interclass similarity and intraclass variability. Due to uneven lighting,\nthe “Building” in (c) are difﬁcult to identify.\nRecently, vision transformer (VIT) [20] has been brilliant\nin the application of computer vision, and various transformer\nvariants applied in the ﬁeld of computer vision are emerging\none after another. For example, the pyramid vision transformer\n(PVT) proposed by Wang et al.[21], as one of the representative\nmodels of transformer applied to the visual ﬁeld, has achieved\nexcellent results in many tasks. Although PVT reduces the\nconsumption of computing resources to a certain extent, its\ncomputational complexity has a square relationship with the\nsequence length. In order to reduce the computational cost,\nLiu et al.[22] proposed a Swin transformer based on a shifted\nwindow strategy, which limits the computation of multihead\nself-attention to nonoverlapping windows while allowing cross-\nwindow information interaction. It broke through the problem of\nvery high computational complexity of the transformer in vision\ntasks. Lin et al.[23] proposed cross attention in the transformer,\na novel cross-attention mechanism to capture local as well as\nglobal information. Shao et al.[24] proposed a local transformer\nnetwork embedded in a multiscale structure to explicitly learn\nthe correspondence between multimodal inputs. The network\ncan effectively and accurately capture the correspondence be-\ntween long and short distances. Zhang et al.[25] proposed the\nSwin-Conv module, which combined the residual convolution\nand the capabilities of the Swin transformer then inserted it into\nthe UNet[26] architecture. They also designed a practical noise\ndegradation module, which was used in image denoising. Guo\net al. [27] proposed a new visual network architecture, CNNs\nmeet transformers. By simply combining traditional convolution\nand transformer, the network performance can achieve good\nperformance, which is superior to Swin transformer and so on.\nZhu et al.[28] proposed a uniﬁed framework to segment objects\nby considering contextual information and boundary artifacts.\nAzad et al.[29] proposed a new architecture based on a pure\ntransformer named TransDeeplab, which combined transformer\nand deeplab architecture for the ﬁrst time, and achieved the effect\nof state-of-the-art( (SOTA) in medical image segmentation.\nNonetheless, the computational complexity of these VIT vari-\nants is still very high. The amount of parameters is very large,\nand the local and global context information is not sufﬁciently\ncombined. This is not conducive to solving the problems of\nrough edge segmentation and serious holes in segmentation\ncaused by shadow occlusion.\nIn order to solve the above problems, this article proposes a\nnew RS image semantic segmentation network framework SCG-\nTransNet, which combines the network structure of the Swin\ntransformer and Deeplabv3+. Deeplabv3+ is a network based on\nCNN that employs spatial pyramid pooling. The SCG-TransNet\nframework uses a Swin transformer as the encoder and decoder\nto extract features from high-resolution information. In the ﬁnal\nstage of the encoder, Swin-Conv-Dspp (SCD) is used to capture\nmultiscale feature information, and suppress the negative effects\nof high interclass similarity and intraclass difference caused by\nlight factors, so as to alleviate the hole phenomenon in seg-\nmentation. In addition, a global local transformer block (GLTB)\nmodule is added before each visual upsampling to capture local\nfeature information and global feature information to explore the\nspatial correlation between global and local features, improve\ntarget edge localization blur and alleviate segmentation blur\ncaused by target occlusion. The main contributions of this article\nare as follows.\n1) We propose an SCG-TransNet architecture that combines\nthe Swin transformer with Deeplabv3+, which is applied\nto RS image segmentation for the ﬁrst time.\n2) We propose a SCD module to alleviate the hole phe-\nnomenon generated during segmentation. SCD can be\nhelpful to extract feature information from objects at\ndifferent scales and suppress the negative effects of noise\nsuch as chromatic aberration caused by light.\n3) To extract discriminative information better, especially for\nsmall objects, we construct a GLTB with spatial pyramid\npooling shufﬂe module (SPPS), which improves the accu-\nracy of target edge localization.\nII. RELA TEDWORKS\nA. Semantic Segmentation of RS Images Based on CNN\nFCN [9], a framework proposed by Jonathan et al. in 2015\nfor images semantic segmentation, has dominated the semantic\nsegmentation tasks in RS in the subsequent years. However, the\nresults obtained from fully convolutional neural networks are not\nﬁne-grained and sensitive to detail, and lack spatial consistency\nby lacking consideration of pixel-to-pixel relationships.\nTo better address these issues, an encoder–decoder network\nDeeplabv3+ [12] based on atrous spatial pyramid pooling\n(ASPP) is proposed. ASPP mines the contextual information of\nfeatures of different resolutions through receptive ﬁeld pooling\nof different sizes, while the encoder–decoder can better capture\nthe edge information of different targets by gradually recon-\nstructing the spatial information. Subsequently, the encoder–\ndecoder architecture has been widely used in the framework\nof RS image semantic segmentation. Liu et al.[30] adopted a\ndual attention mechanism algorithm to improve the Deeplabv3+\nnetwork, which effectively enhanced the edge localization of\n5286 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nimages and the accuracy of segmentation. Baheti et al.[31]\nadopted the idea of a two-stage attention mechanism and ﬁrstly\nproposed Attention Deeplabv3+ by assigning weights to each\nchannel to capture the relationship between channels of a set\nof feature maps. Akcay et al. [32] developed an end-to-end\ntwo-stream architecture considering geospatial imagery based\non the DeepLabv3+ architecture. Wang et al.[33] proposed\na class feature attention mechanism fused with the improved\nDeeplabv3+ network CFAMNet for semantic segmentation of\ncommon features in RS images and achieved good segmen-\ntation results. Wang et al.[34] proposed a road segmentation\nmethod based on the receptive ﬁeld and improved Deeplabv3+,\ninnovatively used the initialization method to extract the layer\nbackbone network in the network structure, and better extracted\nthe characteristics of the road in the RS image. In addition, Li\net al. [35] proposed a semisupervised semantic segmentation\nstrategy for RS images, which improves the problems existing in\nthe semisupervised semantic segmentation method of confronta-\ntion network by using a consistent self-training framework.\nB. Semantic Segmentation of RS Images Based on Transformer\nIn recent years, VIT [20] has made great achievements in\nthe ﬁeld of RS image semantic segmentation. The traditional\ntransformer structure is mainly used to process word vectors in\nthe ﬁeld of natural language[36], while the VIT is compatible\nwith the transformer framework architecture into the ﬁeld of\ncomputer vision. It can still achieve very good results on RS\nimage segmentation tasks without relying on convolution. The\nconvolution operation often causes the network to focus too\nmuch on the local features of the feature map, while the attention\nmechanism in the transformer can consider the global semantic\nfeature information. Beneﬁting from the transformer’s strong\nmodeling ability for sequences, it has achieved advanced results\nin many basic vision tasks.\nWith such excellent results, many RS image researchers have\nalso applied transformer to the semantic segmentation of high-\nresolution RS images. However, most of the existing transformer\narchitecture networks for semantic segmentation still used the\nencoder–decoder architecture. For example, Li et al.[37] pro-\nposed a multistream RS spatiotemporal fusion network (MSNet)\nbased on transformer and convolution, which achieved ex-\ncellent segmentation accuracy on multiple RS datasets. Gao\net al. [38] designed an adaptive fusion module, and proposed\nSTransFuse by adopting a self-attention mechanism to adap-\ntively fuse the semantic feature information of feature maps of\ndifferent resolutions, which improved the segmentation quality\nof various RS images. Chen et al.[39] creatively proposed a\nnew algorithm based on Swin transformer and linear spectral\nmixture theory for high-resolution RS images, and achieved\nstate-of-the-art results in multiple public datasets. Li et al.[40]\ndesigned a modiﬁed transformer to capture global spatial lo-\ncation features across different scales, and demonstrated on\nobject detection in optical remote sensing images (DIOR)[41]\nand northwestern polytechnical university very high resolution\n-10 (NWPU VHR-10)[42] high-resolution RS image datasets’\nexcellent segmentation accuracy. Wang et al.[43] proposed a\nSwin transformer-based densely connected feature aggregation\nmodule by recovering resolution and generating segmentation\nmaps by designing shared spatial attention and shared channel\nattention. It enhanced the relationship between semantic features\nin space and channels, and effectively alleviated the problems\nof multiscale and confusing geospatial targets that often appear\nin high-resolution RS images. Kaselimi et al.[44] proposed a\nmultilabel visual transformer model ForestVIT, which applied\ntransformer with a self-attention mechanism to the detection\nof deforestation. Zhang et al. [45] proposed a hybrid deep\nneural network based on transformer and CNN for semantic\nsegmentation of ultrahigh-resolution RS images. Sun et al.[46]\nproposed a spectral spatial feature tokenized transformer based\non the transformer framework, which can effectively capture\nspectral spatial features and advanced semantic features so that\nthe model can better extract deep semantic features.\nC. Attention Mechanism\nIn order to improve the defect that the CNN network focuses\ntoo much on local features due to convolution and cannot capture\nglobal information well, many scholars have begun to integrate\nattention into the network. Li et al.[47] adaptively reﬁne features\nby integrating lightweight spatial and channel attention modules.\nChen et al.[48] proposed a feature map attention mechanism\nfor image super-resolution reconstruction. By using features of\ndifferent resolutions to adaptively adjust the channel features,\nwe recover more details and relieve the network from focusing\ntoo much on local areas.\nLi et al. [49] proposed a high-resolution RS image change\ndetection model with a multiscale attention mechanism. By\napplying the attention mechanism to feature maps of different\nresolution scales, feature representations of various scales are\ngenerated and then improved of the defect of over-focusing on\nlocal context. Liu et al.[50] proposed a self-attention negative\nfeedback network applied to real-time image segmentation,\nwhich reconstructed more realistic and clearer real-time images.\nHu et al.[51] proposed a dual-region learning network applied to\nhigh-resolution image reconstruction to extract continuous and\nﬁne pixel-level features through the spatial spectrum module\nwith efﬁcient feature fusion. Xia et al.[52] proposed a new\ndeformable self-attention module, which can select the positions\nof key and value pairs in self-attention according to different\ndependencies of the data. This self-attention mechanism can\nfocus on the associated regions and capture more informative\nfeatures. Zhang et al. [53] proposed a lightweight multiscale\nattention block to build attention between feature maps of differ-\nent resolutions, achieving better results. Sun et al.[54] proposed\na successive pooling attention network including a successive\npooling attention module and a feature fusion module, which\neffectively alleviates the difﬁculty of accurately segmenting\nsmall-scale objects and object boundaries in RS images.\nNonetheless, the computational complexity of the proposed\nstate-of-the-art transformer-based encoders tends to be very\nlarge, and the extraction of global contextual information is still\ninsufﬁcient. This will still lead to missed detection due to the hole\nMO et al.: SWIN-CONV-DSPP AND GLOBAL LOCAL TRANSFORMER FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION 5287\nFig. 2. (a) Architecture of our proposed SCG-TransNet. SCG-TransNet contains two important modules: SCD and GLTB with SPPS; (b) Components of the\nGLTB; (c) Components of the FPN.\nphenomenon. Therefore, in order to fully extract the global con-\ntext information, we propose a hybrid of Swin transformer and\nDeeplabv3+ as the encoder–decoder for efﬁcient segmentation\nof RS images. Speciﬁcally, for the proposed SCG-TransNet, we\navoid the problem of the loss of detail information and discontin-\nuous pixel segmentation due to high-fold direct upsampling by\nusing feature pyramid networks (FPN). In the ﬁnal stage of the\nencoder, we use SCD to efﬁciently extract feature information\nof objects of different scales while suppressing noise. In the\ndecoder, we introduce the GLTB with SPPS before each visual\nup-sampling, and ﬁnally achieve a high-precision segmentation\neffect.\nIII. METHOD\nIn this section, we detail the overall structure of the proposed\nSCG-TransNet and introduce the involved Swin transformer.\nSubsequently, two important modules in SCG-TransNet, namely\nSCD and GLTB with SPPS, are introduced.\nA. Overview\nThe overall architecture of the proposed SCG-TransNet\nis shown in Fig. 2. As a hybrid of Swin transformer and\nDeeplabv3+, our SCG-TransNet follows the encoder–decoder\nparadigm. In the encoder stage, we adopt the Swin transformer\nas the backbone network for feature extraction, and introduce\nthe SCD module in the ﬁnal stage of the encoder. In the decoder,\nFPN is used to fuse the features of different resolutions generated\nby Stage 2 and Stage 3, followed by stacking on the channel with\nthe feature map twice upsampled after SCD, which enhances the\ncommunication of multiscale features and solves the problem\nof loss of important pixel information caused by direct high-\nmultiple upsampling, and effectively enhances the continuity of\npixel information. In addition, a normalization-based attention\nmodule (NAM) attention mechanism[55] is added before SCD\nand before concat of shallow and deep features to redistribute the\nweights of multiscale feature maps for better feature extraction.\nFinally, a GLTB module is added before each visual upsampling.\nB. Swin Transformer Based Encoder and Decoder\nThe encoder is mainly composed of the Swin transformer\nbackbone network and SCD. Swin Transformer is used to extract\nhierarchical feature maps, and SCD is used to capture multiscale\ncontextual information. The decoder is mainly composed of the\nSwin transformer block, FPN, and GLTB. The FPN is used to\nfuse feature maps of different depths. The GLTB is used to\ncapture global and local semantic information of feature maps.\nThis process can be expressed as\nei = EncoderSwim−Trans (images) (1)\ndi = DecoderSwim−Trans (ei) . (2)\nThe Swin transformer block is the core of the Swin trans-\nformer backbone network. The computational complexity of\nthe traditional VIT on the global receptive ﬁeld is quadratic. In\norder to reduce the computational complexity, Liu et al. designed\nthe Swin transformer. Between successive self-attention layers,\na multihead self-attention (MSA) module in transformer is re-\nplaced by a shift-window-based module. By sequentially con-\ncatenating the window-based multihead self-attention (W-MSA)\nblock with a shifted window-based multihead self-attention\n(SW-MSA) block, the context information of the global space is\nobtained in a more efﬁcient manner. For the speciﬁc calculation\nprocess refer to[22].\nUnder this shifted window partitioning scheme, a W-MSA\nmodule and a SW-MSA module are applied in series to the\ntransformer block. The ﬁrst Swin transformer block is a W-MSA\nblock. The input featurexl−1 passes through the LayerNorm and\nW-MSA layers and establishes a residual connection to obtain\nˆxl. After that,ˆxl passes through the LayerNorm and multi-layer\nperceptron (MLP) layers and establishes a residual connection\nagain to obtainxl. The SW-MSA block has only half the window\n5288 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nsize offset in the calculation of the W-MSA layer, and the other\nstructures are almost the same as the W-MSA block. This process\ncan be expressed as\nˆxl = WMSA\n(\nLN\n(\nxl−1))\n+ xl−1. (3)\nxl =M L P\n(\nLN(ˆxl)\n)\n+ xl. (4)\nˆxl+1 = SWMSA\n(\nLN\n(ˆxl))\n+ xl. (5)\nxl+1 = MLP\n(\nLN\n(ˆxl+1))\n+ xl+1. (6)\nSpeciﬁc details of the calculations can be found in[22].\nCompared with the backbone network based on CNN, the\nSwin transformer is a sequence-to-sequence model, which\nmakes it easier to combine multimodal data. Its long-range\nmodeling capability from the attention mechanism releases the\nlimitations of traditional CNN-based models. The Swin trans-\nformer does not contain inductive biases, so it does a good job\nof capturing long-range spatial dependencies in images. Second,\ncompared with other transformer-based backbone networks, the\ncomputational complexity of the Swin transformer is lower, and\nthe speed of recognition and reasoning will be faster.\nC. Swin-Conv-Dspp\nSince atrous convolution [56] easily leads to the loss of\ncontinuous information in space, it is not conducive to capture\nobject features of different scales. To solve this problem, ASPP\nin Deeplabv3+ uses multiple parallel atrous convolutional layers\nwith different sampling rates to obtain information of different\nscales of objects. And in the case of reducing the loss of in-\nformation as much as possible, the construction of the feature\nextraction network is strengthened by increasing the receptive\nﬁeld. RS images often contain a lot of noise[57], such as light\nintensity and light incident angle. How to effectively suppress\nthe negative effects of these noises has become the key to\nsemantic segmentation of RS urban scenes. Atrous convolution\nis extracted across pixels in feature point extraction, which is a\nsparse sampling method. This will inevitably lead to the loss of\npixel information, resulting in a lack of correlation between the\nresults obtained by long-distance convolution, which is not con-\nducive to suppressing noise. This will make it difﬁcult to identify\ntargets with too high interclass similarity or too large intraclass\ndifferences due to light incident angle and light intensity, and\neventually lead to the appearance of holes.\nTherefore, we combined the characteristics of CNN and the\nSwin transformer to design a dual-space pyramid pooling layer,\nusing Swin transformer’s strong information extraction ability\nin the global context to make up for the key details lost by\nusing atrous convolution information, and strengthen the ability\nto extract global context feature information to alleviate the\ndifﬁculty of ASPP to capture the long-range dependence of\nsemantic information. The proposed SCD is shown in Fig.3.\nAtrous convolution is essentially a superposition of many high-\npass ﬁlters [58], which continuously enhances high-frequency\ninformation, so it tends to be better at extracting high-frequency\ninformation of features. The transformer is essentially a low-\npass ﬁlter [59], which continuously strengthens the underlying\nFig. 3. Structure of the SCD module, of which the top branch is the Swin\ntransformer branch and the bottom branch is the CNN branch.\nsemantic information of the image, so it is often better at ex-\ntracting low-frequency information of features. By combining\nthe advantages of the two to reduce the differences within classes\nand expand the frequency of information between classes, the\nnegative effects of various noises in RS images are effectively\nsuppressed. It improves the phenomenon that it is difﬁcult to dis-\ntinguish due to excessive intraclass differences or high interclass\nsimilarity, and alleviates the problem of hole phenomenon.\nSpeciﬁcally, SCD has Swin transformer branch and con-\nvolution branch. As shown in Fig. 3, shiftable windows of\ndifferent sizes are used to better extract semantic information\nbetween patches with different distances to capture multiscale\ninformation. Smaller window scales aim to capture local infor-\nmation, while larger windows aim to capture global contextual\ninformation. The convolution branch utilizes 1, 4, 8, and 12\natrous convolutions with different dilation rates, and it broadly\nextracts objects of different scales by expanding the receptive\nﬁeld of the convolution. Try to expand the receptive ﬁeld to\nextract feature information of different scales without reducing\nthe loss of information. By combining the strong local feature\nextraction ability of convolution and the excellent capture ability\nof the transformer in global context and long-range dependen-\ncies, SCD shows excellent antinoise performance, effectively\nalleviating the problem of holes caused by excessive similarity\nbetween classes.\nD. Global-Local Transformer Block\nThe proposed GLTB is mainly composed of the following two\nbranches: 1) the global context branch and 2) the local context\nbranch.\nIn RS images, the distribution of cars is often clustered,\nsuch as parking lots, temporary parking spaces on roads, and\nthe parking locations and distributions are often regular. If the\nglobal context information can be effectively extracted, learnt,\nand captured car parking the rules of the model, the accuracy of\nthe car class prediction will be greatly improved. The same is\ntrue for the building class, the distribution structure of the house\nis strongly related to the location. Houses are always arranged\nin a determinant layout, with buildings arranged in parallel to\nform a regular determinant. And the shape of the house is often\nMO et al.: SWIN-CONV-DSPP AND GLOBAL LOCAL TRANSFORMER FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION 5289\nrectangular, square, and rarely other shapes. Although global\ncontext information is very important in semantic segmentation\nof complex urban scenes, local information also plays a pivotal\nrole in the rich spatial details of images. Only using traditional\nconvolution to extract semantic features is not ideal because\nit is too limited to local information. Local information is very\nimportant for RS segmentation, and only using the global context\ncapture ability of transformer will result in the inability to effec-\ntively extract local information features of high-resolution RS\nimages. The advantages of both can be combined to effectively\nextract local context information and global context information.\nIn the global branch, which is mainly captured by window-\nbased multihead autonomous attention, we ﬁrst use standard\n1 ×1 convolution to expand the channel dimension of the input\n2-D feature map ∈ RB×C×H×W by a factor of 3. Next, the\n1-D sequence∈ R\n(\n3 ×B × H\nW × W\nW × h\nh\n)\n×(w ×w) × C\nh is\nconverted into Q, K, and V vectors using the window division\noperation. For details, the channel dimension is set to 64, and\nthe window size and attention head are both set to 8. Details\nof window-based multihead self-attention can be found in[60].\nAlthough self-attention based on shiftable windows can capture\nfeature information across windows, the amount of computa-\ntion is greatly increased. Therefore, we introduce the context\ninteraction module of the cross-shaped window to fuse the two\nfeature maps generated by the horizontal average pooling layer\nand the vertical average pooling layer, so as to capture the global\ncontext efﬁciently. Details of the computation of GLTB in the\nglobal branch can be found in[61].\nIn addition, in RS images, a certain category is often obscured\nby shadows or other objects. For example, houses on both sides\nof the road can easily occlude parked or moving cars on the\nroad. In this way, it is easy to cause problems such as blurred\nboundaries, false detections or even missed detections during\nsegmentation. In order to solve the problem that the target to be\nrecognized is occluded, Li et al.[62] proposed spatial pyramid\nconvolutional shufﬂe in you only look once (YOLO), hoping\nto solve the recognition of the occluded human body. However,\nit does not take into account the loss of detailed information\ncaused by the use of parallel convolution, and is still limited\nto the capture of local information by convolution, lacking\nthe extraction of global context information, which obviously\ncannot be directly applied to the ﬁeld of RS. Inspired by it,\nin the local branch, we propose an SPPS whose structure is\nshown in Fig.4. Speciﬁcally, it covers different ranges by using\nfour parallel convolution kernels of different sizes and dilation\nrate convolutions, and then stacks the results obtained by each\nbranch convolution on the channel. Since the limited visible\npixels of the occluded object are very rare, it is inevitable that\nsome pixel information will be lost after passing through the\nconvolution kernels of different sizes. For the occluded object,\nthese rare pixel information often becomes the ﬁnal occlusion\nwhether objects can be correctly identiﬁed. Therefore, we use a\nglobal average pooling layer and skip connections to compensate\nfor the loss of key details in utilizing different kernel sizes.\nThen, we use PixelShufﬂe to combine the adjacent elements.\nPixelShufﬂe will combine the information of the same position\nextracted from convolution kernals of different sizes such as\nFig. 4. Structure of the SPPS module.\nAlgorithm 1: Training Process of SCG-TransNet.\nInput: Vaihingen or Potsdam dataset D;\n1: for epoch < epochsdo\n2: Extract features by (1) with SCD module;\n3: Fusing features by (2) with GLTB module;\n4: Get segmentation maps;\n5: Update the parameters of the module;\n6: end for\nOutput: Trained SCG-TransNet;\nblue, green, yellow, and purple in the ﬁgure in an adjacent\nmanner. In the feature graph output by SPPS, the information\nof adjacent combination of features extracted from the same\nposition in the original feature graph by convolution kernel of\ndifferent sizes and expansion rate is called a cell. Each cell\ncontains information extracted by different kernel sizes at the\nsame position in the original feature map. These information\ncan provide multilevel information extracted from the same\nposition of the original feature map, and realize the extraction of\nmultireceptive ﬁeld information from the same position of the\nfeature map. SPPS can improve the ability to extract key details\nand generate distinguishable features from the limited visible\npixels of occluded objects. What is more, it further enhances the\nextraction ability of local information, and reﬁnes the global and\nlocal feature information of the feature map when upsampling\nrestores the feature map. At the same time, the module can be\nplug-and-play and can be efﬁciently migrated to other models\nfor use.\nFurthermore, we provide Algorithm1 to describe our pro-\nposed SCD-TransNet in detail.\nIV . EXPERIMENTS\nA. Datasets\n1) V aihingen Dataset:The Vaihingen dataset[63] contains\n33 remotely sensed images of different sizes, which extracted\nfrom a very large top-level orthophoto image, covering more\nthan 1.38 km2 of the city of Vaihingen. The RS image format\n5290 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nis an 8-b tag image ﬁle format (TIFF) ﬁle consisting of the\nfollowing three bands: 1) near infrared; 2) red and 3) green.\nThe digital surface model (DSM) is a single-band TIFF ﬁle with\nthe gray level (corresponding to the DSM height) encoded as a\n32-b ﬂoating point value. In our experiments, we cropped them\neach to a size of 256× 256, and the details of the experiments\nare given in[64].\n2) POTSDAM Dataset: The POTSDAM dataset[63] has 38\nremotely sensed images all of 6000× 6000 resolution in size.\nThe dataset covers 3.42 km2 of complex buildings and dense\nsettlement structures. The dataset has six categories for semantic\nsegmentation. Again, we cropped each of them to a size of\n256 × 256.\nWe ignore the category of “background” in the quantitative\nevaluation of the two datasets.\nB. Implementation Details\n1) Training Setup: Our network model is built on Pytorch’s\ndeep learning framework. For fast convergence, we use Adam as\nthe optimizer and set the propulsion to 0.9 to train the model. The\ninitial learning rate was set to 0.001 and the learning rate was\nadjusted using a step strategy. All experiments were deployed\non NVIDIA GTX 2060 and NVIDIA GTX 3090. The batch size\nwas set to 10 and the maximum epoch was 150.\n2) Loss Function: Due to the category imbalance in the\nVaihingen and POTSDAM datasets, the model training focused\non the larger categories and ignored the smaller categories. To\nimprove this problem, we used the joint loss of dice Loss[65]\nand cross entropy (CE) Loss, with the joint loss L denoted as\nL = LCE + LDice. (7)\n3) Evaluation Metrics: We use the mean cross-merge ratio\n(MIoU) and the mean F1 (Ave F1) score to evaluate model per-\nformance. These two evaluation metrics are based on confusion\nmatrices and contain the following four terms:\n1) true positive (TP);\n2) false positive (FP);\n3) true negative (TN);\n4) false negative (FN).\nIn addition, we added giga ﬂoating-point operations per\nsecond (GFLOPs) and overall accuracy (OA) in the ablation\nexperiment to evaluate the computational complexity and overall\naccuracy of the model, respectively. For each category, the\nintersection over union (IOU) is deﬁned as the intersection of\nthe predicted and true values and is calculated as follows:\nIoU = TP\nTP + FP + FN . (8)\nThe F1 score for each category is calculated as follows:\nF1=2 × precision × recall\nprecision + recall . (9)\nThe overall accuracy rate OA is calculated as\nOA = TP\nTP + FP + TN + FN (10)\nFig. 5. Comparison of segmentation results before and after using SCD and\nGLTB in the SCG-TransNet framework.\nfollows where precision = TP/(TP + FP) recall = TP/(TP +\nFN). In addition, MIoU represents the average of IoU across\nall categories, and Ave.F1 score is the average of F1 across all\ncategories.\nC. Ablation Experiments\nTo evaluate the performance of the proposed network structure\nand two important modules, we used SCG-TransNet without\nthe addition of SCD and GLTB as the baseline network and\nconducted ablation experiments on the Vaihingen dataset. The\nboldface of all tables in the text represents the maximum value\nof each column. In our baseline network, the Swin transformer\nis used for both encoder and decoder. In a large number of\nexperiments comparing different hyperparameters, we select\nthe hyperparameters with the best results to set the baseline\nnetwork. In the encoder, the ratio of mlp hidden dim to em-\nbedding dim is set to 4, the patch size is set to 4× 4s i z e\nand the patch norm is used, the stochastic depth rate is set to\n0.1, attention dropout rate is set to 0, the hidden size is set to\n96, the window size is 8, the number of layers corresponding\nto each stage is 2, 2, and 6, and the number of heads cor-\nresponding to each layer is 3, 6, and 12. In the decoder, the\nnumber of layers corresponding to each stage is 2, 2, and 2,\nand the number of heads corresponding to each layer is 3, 3,\nand 3.\n1) Effect of SCD:TableI shows that MIoU, OA, and average\nF1 improve by 1.02%,0 . 3 5%, and 0.79%, respectively, when\nSCD is considered in the SCG-TransNet framework. The car\nclass shows the largest improvement in segmentation accuracy,\nwith a 3.07% increase in IoU, followed by the building class\nwith a 0.96% increase, validating the effectiveness of SCD in\nthe network. As Fig.5 shows, in the ﬁrst row of the ablation\nexperiment, the color of the “building” is not consistent on both\nsides due to the angle of incidence of the light and the strong\nMO et al.: SWIN-CONV-DSPP AND GLOBAL LOCAL TRANSFORMER FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION 5291\nTABLE I\nABLA TIONEXPERIMENT OF THEPROPOSED MODULES ON THEVAIHINGEN DATA S E T\nillumination. The shadow from the “building” also obscures\nthe “impervious surface” above it, resulting in a high intraclass\nvariability with little interclass variability. Before the addition of\nthe SCD, the black side of the “building,” which was obscured by\nthe shadow, showed varying degrees of holes, which were well\nmitigated by the introduction of the SCD. In the second row, the\nshadow from the high vegetation obscures the low vegetation in\nits immediate vicinity, and because the obscured low vegetation\nhas very few and discontinuous pixel points, the model also\nshows varying degrees of holes in the segmentation before the\nintroduction of SCD. The introduction of SCD effectively sup-\npresses the negative effects of light and accurately separates out\nthe obscured low vegetation, while at the same time mitigating\nthe holes caused by the discontinuity of pixel points caused by\nthe obscured low vegetation. The visualization of Fig.5 shows\nthat the introduction of SCD effectively mitigates the holes in the\nsegmentation and improves the accuracy of target recognition for\nhigh interclass similarity.\n2) Effect of GLTB: As shown in Table I, when GLTB is\nconsidered in the SCG-TransNet framework, MIoU, OA, and\naverage F1 are improved by 2.23%,0 . 7 8%, and 1.65%, respec-\ntively. The car class has the most improvement in segmentation\naccuracy, with a 4.07% improvement in IoU. The IoU of the\nother four classes “impervious surface,” “building,” “low veg-\netation,” and “tree” improved by 2.43%,2 . 1 3%,2 . 2 4%, and\n0.30%, respectively. As shown in Fig.5, in the third row of the\nablation experiment, under the inﬂuence of oblique sidelight,\nthe shadow produced by “building” almost completely covers\nthe low vegetation category, which is certainly very challenging\nfor the model to identify. In the ﬁfth row, the close proximity\nof the cars leads to the problem of shadows within the class\nobscuring each other. In the third and fourth rows, it can be seen\nthat before the introduction of GLTB, the model does a very\npoor job of recognizing the obscured objects, not only incorrect\nvvcly detecting the obscured low vegetation as houses, but also\nmissing the obscured cars. With the introduction of GLTB, the\nmodel accurately segmented the obscured low vegetation and\ncars, effectively reducing the negative effects of shadows. The\nimage in the ﬁfth row was taken in a car park, with cars in close\nproximity to each other, which tested the model’s performance\nin segmenting small target edges in a densely distributed space.\nBefore the introduction of GLTB, the model was unable to reﬁne\nthe features of each car, resulting in the edge pixels of the “car”\nbeing mixed together and unable to distinguish between cars.\nWith the introduction of GLTB, the model almost perfectly\nseparates the pixels that were previously predicted to be mixed\nin the car park, and separates the different cars. Secondly, in\nrows 4–6, before the introduction of GLTB, the model showed\njagged edges for the segmentation of different categories. In\ncontrast, after the introduction of GLTB, the model segmented\nthe edges of different targets very smoothly, with almost no\njagged fuzzy edges, and the model’s performance in segmenting\nthe edges of targets improved substantially. As can be seen\nfrom the visualization of the ablation experiments in the fourth,\nﬁfth, and sixth rows, GLTB shows excellent edge segmentation\ncapability, effectively improving the situation of false detection\nor missed detection due to shadow obscuration from oblique\nsidelight.\n3) Joint Effect: Table I reﬂects that the joint effect between\nthe two modules is studied under the SCG-TransNet framework.\nWhen SCD and GLTB are introduced simultaneously, MIoU,\nOA, and average F1 are improved by 3.00%,0 . 9 3%, and 2.31%,\nrespectively. It is obvious that after adding GLTB, the IOU of\nthe “car” class is increased by nearly 9.37%, followed by the\n“low vegetation” class, the segmentation accuracy is improved\nby 3.29%, and the remaining three classes “impervious surface,”\n“building,” the IoU of “tree” has increased by 0.72%,1 . 5 9%, and\n0.05%, respectively. From Fig.5, we can clearly see that in the\nﬁrst row, the model effectively alleviates the hole phenomenon\ncaused by high intensity light, which is caused by large intraclass\ndifference and high interclass similarity. In the second, third, and\nfourth rows, the model effectively suppresses the negative effects\nof interclass and intraclass mutual occlusion caused by shadows\ngenerated by oblique side lights. In the ﬁfth row, the model\nalso achieves excellent segmentation performance in dense and\ncomplex small-object aggregation scenarios. In addition, we can\nclearly see that the SCG-TransNet combining SCD and GLTB\nhas greatly improved the edge information localization ability\nof the model, and almost smoothes all segmentation edges.\nD. Comparison With Other Methods\n1) Results on the V aihingen Dataset:Table II lists the ex-\nperimental results of different existing methods. Our proposed\nSCG-TransNet achieves 70.29% MIoU and 82.27% average F1\nsegmentation, outperforming the other methods. In traditional\nCNN, the UNet network combines high-level semantic features\nfrom decoder and low-level features from encoder correspond-\ning scales by using skip connections, Deeplabv3+ uses atrous\nconvolutions with different dilation rates to build spatial pooling\npyramids, and experimental data show that the segmentation\neffect is better than other traditional CNN methods. Compared\nto UNet, our model improves 3.94% on MIoU and 3.14% on\naverage F1, and compared to Deeplabv3+, our model improves\n3.13% on MIoU and 2.56% on average F1. UperNet and DANet\nwith pyramidal structure are not as good as our SCG-TransNet\nin extracting global contextual information. Swin-UNet uses a\npure transformer structure, which is not ideal in segmentation.\n5292 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nTABLE II\nCOMPARISON OFSEGMENTA TIONRESULTS ON THEVAIHINGEN DATA S E T\nFig. 6. Visualization results of the Vaihingen dataset.\nTransUnet uses a transformer and CNN serial in the encoding\nstage structure for feature extraction, while ST-U-shaped net-\nwork (ST-UNet) uses a parallel structure of transformer and\nCNN in the encoding phase. Compared with ST-UNet, our\nmodel improves 0.06% in MIoU and 0.12% in average F1,\ndemonstrating the superiority of our proposed SCG-TransNet.\nA visual comparison of several semantic segmentation meth-\nods used in TableII is shown in Fig.6. Compared with other\nmodels, SCG-TransNet effectively alleviates the problem of\npoor segmentation accuracy caused by high similarity between\nclasses due to differences in light intensity and light incident\nangle. As shown in the ﬁrst line, the “impervious surface” in\n“building” and “low vegetation” is very similar to the adjacent\n“low vegetation” category due to light, and other models in-\ncorrectly identify “impervious surface” for “low vegetation”,\nSCG-TransNet makes an accurate judgment. In the second row,\n“building” in the yellow box presents two colors of black and\nlight gray due to the incident angle of the light. Especially in the\nyellow box, the white car has serious reﬂections under the action\nof strong light, which is very similar to the “building” class.\nUnder such harsh environmental conditions, no other models can\nrecognize the color of “building” in black and the “building” in\nstrong reﬂection. “Car,” while SCG-TransNet effectively elimi-\nnates the interference caused by light, and accurately recognizes\nthe black “building” and the strongly reﬂective “car”. In the third\nrow, the shadows produced by high vegetation block nearby\nlow vegetation. Under the inﬂuence of shadows, other models\nmistakenly identify the occluded low vegetation as “tree,” while\nour model effectively extracts distinguishable feature informa-\ntion from the limited pixels of the occluded target, perfectly\nobstructed low vegetation is identiﬁed. In the fourth row, the\nwhite car also has reﬂected light, similar to the second row, and\nobscures the car next to it due to the difference in the height\nof the car and the angle of incidence of the light. Under the\ninﬂuence of reﬂected light, almost all other models misdetected\nthe car as “building,” but SCG-TransNet accurately identiﬁed\nand segmented it. As shown in Fig.6, SCG-TransNet effectively\nidentiﬁed the occluded target shows excellent segmentation\nperformance.\n2) Results on the POTSDAM Dataset:Table III shows the\nsegmentation results of each method on the POTSDAM dataset.\nThe proposed SCG-TransNet achieves 76.04% on MIoU and\n86.20% on average F1, outperforming the results of other meth-\nods, demonstrating the superiority of the model. Among the\ntraditional CNN models, Deeplabv3+ outperforms other tradi-\ntional CNN segmentation models. Compared with Deeplabv3+,\nSCG-TransNet improves MIoU and average F1 by 1.56% and\n1.07%, respectively. Compared with ST-UNet with 160.97 MB\nof parameters, SCG-TransNet with only 26% of ST-UNet pa-\nrameters still surpasses 0.07% and 0.12% in MIoU and average\nMO et al.: SWIN-CONV-DSPP AND GLOBAL LOCAL TRANSFORMER FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION 5293\nTABLE III\nCOMPARISON OFSEGMENTA TIONRESULTS ON THEPOTSDAM DATA S E T\nFig. 7. Visualization results of the POTSDAM dataset.\nF1, achieving SOTA results. Compared with Swin-UNet, SCG-\nTransNet improves MIoU and average F1 by 10.52% and 7.41%,\nrespectively.\nThe visualization of segmentation results for each model is\nshown in Fig.7. Looking at the ﬁrst row, the similarity between\n“low vegetation” and its adjacent “impervious surface” is very\nhigh, and both appear dark gray. Obviously, the ability of the\nmodel to localize edge information is particularly important in\nthe face of adjacent targets with such high interclass similarity.\nFrom the comparison of segmentation results of different mod-\nels, we can clearly see that our model has the best performance\nfor segmentation boundaries when the two classes are adjacent\nand the similarity between classes is so high. Looking at the\nthird row, in the yellow box above, the sides of the “impervious\nsurface” are surrounded by low vegetation. In the bottom yellow\nbox, the shadows from the two houses cause the low vegetation\nbetween them to appear black, making it difﬁcult for the model\nto be identiﬁed. Observe Fig.7, some models mistakenly detect\nit as “tree,” while our SCG-TransNet effectively suppresses\nthe adverse effects of shadows, accurately segment the low\nvegetation that is obscured, and correctly distinguishes included\nbetween “building,” “low vegetation,” and “impervious surface.”\nIn the fourth row, the “tree” in the yellow box is almost the same\ncolor as its nearby low vegetation due to the light intensity. For\nthis reason, the segmentation edges of the “tree” in this box\nare very blurred by other models and appear to have various\ndegrees of false detection. And our SCG-TransNet effectively\nimproves the phenomenon of false detection, blurred boundary,\nand boundary fault, showing strong segmentation performance.\n3) Parameter and Computation Complexity Analysis: An-\nalyzing in terms of parameters, the proposed model has only\n41.98 MB of parameters, which is far less than the 160.97 MB\nparameter of the SOTA model, and only 26% of the SOTA\nmodel. And the obtained 70.29%, 76.04% Miou, and 82.27%\non international society for photogrammetry and remote sensing\n(ISPRS)-Vaihingen dataset and ISPRS-Potsdam dataset, respec-\ntively, 86.20% average F1 surpassed SOTA’s 70.23%, 75.97%\nMiou and 82.15%, 86.13% average F1. In addition, Swin-UNet\nhas only 25.89 MB of parameters, although the parameter\namount is very small, but due to its lack of convolution operation,\nit ignores the attention to local information, which is obviously\nnot suitable for RS images with a large number of different\nscales, resulting in poor performance. The ﬁnal performance\nof FCN with only 22.70 MB of parameters is also poor due\nto the lack of attention to global information. Analyzing in\nterms of model complexity, models with transformer or Swin\ntransformer blocks usually have greater computational complex-\nity than traditional CNN semantic segmentation models. For\nexample, the GFLOPs of TransUNet and ST-UNet are 35.84 G\nand 78.69 G, respectively. Compared with the GFLOPs of the\n5294 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\ntraditional CNN semantic segmentation models DeeplabV3+\nand UpperNet, which are only 20.78 G and 26.03 G, while the\ncomputational complexity GFLOPs of DANet with multiple at-\ntention is as high as 125.78 G. The computational complexity of\nour SCG-TransNet is 52.97 G. Although SCG-TransNet does not\nhave an advantage in computational complexity when compared\nwith the traditional CNN semantic segmentation models, it still\nhas a great effect when applied to scenarios where the model\nefﬁciency requirements are not very high. At the same time, it is\nstill valuable for exploring how Swin Transformer can be better\napplied to the ﬁeld of RS with such a complex environment.\nV. CONCLUSION\nIn this work, we propose SCG-TransNet, a semantic segmen-\ntation framework combining Swin transformer and Deeplabv3+.\nCompared with models based on CNN backbone network, the\nSwin transformer does not contain inductive bias, which allows\nfor better representation of long-range dependencies. Compared\nwith other transformers, Swin transformer has lower compu-\ntational complexity, fewer parameters, and output of hierar-\nchical feature maps. The proposed SCD captures multiscale\ninformation of features by combining the excellent local feature\nextraction ability of convolution and the powerful capture ability\nof Swin transformer in global context information, so as to obtain\nmore discriminative features and effectively inhibits the noise\ncaused by shadow occlusion caused by light. In addition, the\nGLTB with SPPS can make full use of the limited pixels of\nthe occluded object to generate distinguishable representation\ninformation, effectively alleviate the situation of false detec-\ntion or even missed detection caused by the occlusion of the\ntarget, and greatly improve the model’s localization of edge\ninformation ability. In various RS image semantic segmen-\ntation, SCG-TransNet shows great potential for constructing\nlong-range dependencies and outperforms other SOTA VITs\nin our experiments. In the future, we will continue to improve\nand optimize the model, expecting that the model can be more\nlightweight while ensuring the segmentation ability, and can be\napplied to a variety of different ﬁelds.\nREFERENCES\n[1] H. Shaﬁzadeh-Moghadam et al., “Google earth engine for large-scale land\nuse and land cover mapping: An object-based classiﬁcation approach using\nspectral, textural and topographical factors,”GISci. Remote Sens., vol. 58,\nno. 6, pp. 914–928, 2021.\n[2] H. Luo, C. Chen, L. Fang, K. Khoshelham, and G. Shen, “MS-RRFSegNet:\nMultiscale regional relation feature segmentation network for semantic\nsegmentation of urban scene point clouds,”IEEE Trans. Geosci. Remote\nSens., vol. 58, no. 12, pp. 8301–8315, Dec. 2020.\n[3] J. Zhao, Y . Zhou, B. Shi, J. Yang, D. Zhang, and R. Yao, “Multistage fusion\nand multi-source attention network for multi-modal remote sensing image\nsegmentation,” ACM Trans. Intell. Syst. Technol., vol. 12, no. 6, pp. 1–20,\nDec. 2021.\n[4] L. Ding, J. Zhang, and L. Bruzzone, “Semantic segmentation of largesize\nVHR remote sensing images using a two-stage multiscale training archi-\ntecture,”IEEE Trans. Geosci. Remote Sens., vol. 58, no. 8, pp. 5367–5376,\nAug. 2020.\n[5] D. Y . Chen et al., “Building extraction and number statistics in WUI areas\nbased on UNet structure and ensemble learning,”Remote Sens., vol. 13,\nno. 6, 2021, Art. no. 1172.\n[6] C. Ayala et al., “A deep learning approach to an enhanced building footprint\nand road detection in high-resolution satellite imagery,”Remote Sens.,\nvol. 13, no. 16, 2021, Art. no. 3135.\n[7] H. A. T. Nguyen et al., “Integrating remote sensing and machine learn-\ning into environmental monitoring and assessment of land use change,”\nSustain. Prod. Consumption, vol. 27, pp. 1239–1254, 2021.\n[8] Z. Sun et al., “Use remote sensing and machine learning to study the\nchanges of broad-leaved forest biomass and their climate driving forces in\nnature reserves of northern subtropics,”Remote Sens., vol. 14, no. 5, 2022,\nArt. no. 1066.\n[9] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2015, pp. 3431–3440.\n[10] V . Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolu-\ntional encoder-decoder architecture for image segmentation,”IEEE Trans.\nPattern Anal. Mach. Intell., vol. 39, no. 12, pp. 2481–2495, Dec. 2017.\n[11] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P.-A. Heng, “H-DenseUNet:\nHybrid densely connected UNet for liver and tumor segmentation from\nCT volumes,”IEEE Trans. Med. Imag., vol. 37, no. 12, pp. 2663–2674,\nDec. 2018.\n[12] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmenta-\ntion,” inProc. Eur . Conf. Comput. Vis., 2018, pp. 801–818.\n[13] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous\nconvolution for semantic image segmentation,” 2017,arXiv:1706.05587.\n[14] J. Wu, Z. Pan, B. Lei, and Y . Hu, “FSANet: Feature-and-spatial-aligned\nnetwork for tiny object detection in remote sensing images,”IEEE Trans.\nGeosci. Remote Sens., vol. 60, 2022, Art no. 5630717.\n[15] S. Lei and Z. Shi, “Hybrid-scale self-similarity exploitation for remote\nsensing image super-resolution,” IEEE Trans. Geosci. Remote Sens. ,\nvol. 60, pp. 1–10, 2022.\n[16] R. Yang, F. Pu, Z. Xu, C. Ding, and X. Xu, “DA2Net: Distraction-\nattention-driven adversarial network for robust remote sensing image scene\nclassiﬁcation,” IEEE Geosci. Remote Sens. Lett., vol. 19, pp. 1–5, 2022.\n[17] P. W. Battaglia et al., “Relational inductive biases, deep learning, and graph\nnetworks,” 2018,arXiv:1806.01261.\n[18] Q. Liu et al., “Self-constructing graph neural networks to model long-range\npixel dependencies for semantic segmentation of remote sensing images,”\nInt. J. Remote Sens., vol. 42, no. 16, pp. 6184–6208, 2021.\n[19] A. Wiacek, E. González, and M. A. L. Bell, “CohereNet: A deep learning\narchitecture for ultrasound spatial correlation estimation and coherence-\nbased beamforming,”IEEE Trans. Ultrasonics, Ferroelectrics, Freq. Con-\ntrol, vol. 67, no. 12, pp. 2574–2583, Dec. 2020.\n[20] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Luˇci´c, and C. Schmid,\n“Vivit: A video vision transformer,” inProc. IEEE/CVF Int. Conf. Comput.\nVis., 2021, pp. 6836–6846.\n[21] W. Wang et al., “PVT V2: Improved baselines with pyramid vision\ntransformer,”Comput. Vis. Media, vol. 8, no. 3, pp. 415–424, 2022.\n[22] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 10012–10022.\n[23] H. Lin, X. Cheng, X. Wu, and D. Shen, “CAT: Cross attention in vision\ntransformer,” inProc. IEEE Int. Conf. Multimedia Expo, 2022, pp. 1–6.\n[24] R. Shao et al., “Localtrans: A multiscale local transformer network for\ncross-resolution homography estimation,” inProc. IEEE/CVF Int. Conf.\nComput. Vis., 2021, pp. 14890–14899.\n[25] K. Zhang et al., “Practical blind denoising via swin-conv-unet and data\nsynthesis,” 2022,arXiv:2203.13278.\n[26] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks\nbiomedical image segmentation,” inProc. 18th Int. Conf. Med. Image\nComput. Comput.-Assist. Interv., 2015, vol. 9351, pp. 234–241.\n[27] J. Guo et al., “CMT: Convolutional neural networks meet vision trans-\nformers,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022,\npp. 12175–12185.\n[28] F. Zhu, Y . Zhu, L. Zhang, C. Wu, Y . Fu, and M. Li, “A uniﬁed efﬁcient\npyramid transformer for semantic segmentation,” inProc. IEEE/CVF Int.\nConf. Comput. Vis., 2021, pp. 2667–2677.\n[29] R. Azad et al., “TransDeepLab: Convolution-free transformer-based\ndeeplab v3 for medical image segmentation,” in Proc. Int. Workshop\nPRedictive Intell. Med., Springer, Cham, 2022, pp. 91–102.\n[30] W. Liu et al., “Remote sensing image segmentation using dual attention\nmechanism Deeplabv3 algorithm,”Trop. Geography, vol. 40, pp. 303–313,\n2020.\n[31] B. Baheti et al., “Semantic scene segmentation in unstructured envi-\nronment with modiﬁed DeepLabV3,”Pattern Recognit. Lett., vol. 138,\npp. 223–229, 2020.\nMO et al.: SWIN-CONV-DSPP AND GLOBAL LOCAL TRANSFORMER FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION 5295\n[32] O. Akcay, A. C. Kinaci, E. O. Avsar, and U. Aydar, “Semantic segmen-\ntation of high-resolution airborne images with dual-stream DeepLabV3,”\nISPRS Int. J. Geo- Inf., vol. 11, no. 1, p. 23, 2021. [Online]. Available:\nhttps://doi.org/10.3390/ijgi11010023\n[33] Z. Wang et al., “Semantic segmentation of high-resolution remote sens-\ning images based on a class feature attention mechanism fused with\nDeeplabv3,” Comput. Geosci., vol. 158, 2022, Art. no. 104969.\n[34] Y . Wang, S. Wang, and X. B. Hong, “Road extraction using high resolution\nsatellite images based on Receptive Field and Improved Deeplabv3,”\nin Proc. J. Phys.: Conf. Ser ., IOP Publishing, 2022, vol. 2320, no. 1,\nArt. no. 012021.\n[35] J. Li, B. Sun, S. Li, and X. Kang, “Semisupervised semantic segmentation\nof remote sensing images with consistency self-training,”IEEE Trans.\nGeosci. Remote Sens., vol. 60, 2022, Art. no. 5615811.\n[36] K. Chowdhary, “Natural language processing,” inFundamentals of Artiﬁ-\ncial Intelligence, Springer, pp. 603–649, 2020.\n[37] W. Li et al., “MSNet: A multi-stream fusion network for remote sensing\nspatiotemporal fusion based on transformer and convolution,”Remote\nSens., vol. 13, no. 18, 2021, Art. no. 3724.\n[38] L. Gao et al., “STransFuse: Fusing swin transformer and convolutional\nneural network for remote sensing image semantic segmentation,”IEEE\nJ. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 10990–11003,\n2021.\n[39] G. Chen et al., “SwinSTFM: Remote sensing spatiotemporal fusion using\nswin transformer,” IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5410618.\n[40] Q. Li, Y . Chen, and Y . Zeng, “Transformer with transfer CNN for remote-\nsensing-image object detection,” Remote Sens., vol. 14, no. 4, 2022,\nArt. no. 984.\n[41] B. He et al., “UnityShip: A large-scale synthetic dataset for ship recogni-\ntion in aerial images,”Remote Sens., vol. 13, no. 24, 2021, Art. no. 4999.\n[42] W. Zhang, L. Jiao, Y . Li, Z. Huang, and H. Wang, “Laplacian feature pyra-\nmid network for object detection in VHR optical remote sensing images,”\nIEEE Trans. Geosci. Remote Sens., vol. 60, 2021, Art. no. 5604114.\n[43] L. Wang, R. Li, C. Duan, C. Zhang, X. Meng, and S. Fang, “A novel trans-\nformer based semantic segmentation scheme for ﬁne-resolution remote\nsensing images,”IEEE Geosci. Remote Sens. Lett., vol. 19, pp. 1–5, 2022.\n[44] M. Kaselimi, A. V oulodimos, I. Daskalopoulos, N. Doulamis,\nand A. Doulamis, “A vision transformer model for convolution-\nfree multilabel classiﬁcation of satellite imagery in deforesta-\ntion monitoring,” IEEE Trans. Neural Netw. Learn. Syst. , 2022,\ndoi: 10.1109/TNNLS.2022.3144791.\n[45] C. Zhang, W. Jiang, Y . Zhang, W. Wang, Q. Zhao, and C. Wang, “Trans-\nformer and CNN hybrid deep neural network for semantic segmentation\nof very-high-resolution remote sensing imagery,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, 2022, Art. no. 4408820.\n[46] L. Sun, G. Zhao, Y . Zheng, and Z. Wu, “Spectral–spatial feature tok-\nenization transformer for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., vol. 60, 2022, Art. no. 5522214.\n[47] H. Li, K. Qiu, L. Chen, X. Mei, L. Hong, and C. Tao, “SCAttNet: Semantic\nsegmentation network with spatial and channel attention mechanism for\nhigh-resolution remote sensing images,”IEEE Geosci. Remote Sens. Lett.,\nvol. 18, no. 5, pp. 905–909, May 2021.\n[48] Y . Chen et al., “Image super-resolution reconstruction based on feature\nmap attention mechanism,”Appl. Intell., vol. 51, no. 7, pp. 4367–4380,\n2021.\n[49] J. Li et al., “Change detection for high-resolution remote sensing images\nbased on a multi-scale attention siamese network,”Remote Sens., vol. 14,\nno. 14, 2022, Art. no. 3464.\n[50] X. Liu et al., “Self-attention negative feedback network for real-time image\nsuper-resolution,” J. King Saud Univ.- Comput. Inf. Sci., vol. 34, no. 8,\npp. 6179–6186, 2022.\n[51] X. Hu et al., “HDNet: High-resolution dual-domain learning for spectral\ncompressive imaging,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., 2022, pp. 17542–17551.\n[52] Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang, “Vision transformer\nwith deformable attention,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., 2022, pp. 4794–4803.\n[53] C. Zhang, H. Wan, X. Shen, and Z. Wu, “PatchFormer: An efﬁcient point\ntransformer with patch attention,” inProc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., 2022, pp. 11799–11808.\n[54] L. Sun, S. Cheng, Y . Zheng, Z. Wu, and J. Zhang, “SPANet: Successive\npooling attention network for semantic segmentation of remote sensing\nimages,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 15,\npp. 4045–4057, 2022, doi:10.1109/JSTARS.2022.3175191.\n[55] Y . Liu et al., “NAM: Normalization-based attention module,” 2021,\narXiv:2111.12419.\n[56] L. C. Chen et al., “Rethinking atrous convolution for semantic image\nsegmentation,” 2017,arXiv:1706.05587.\n[57] J. Gutter and J. Niebling, and X. X. Zhu, “Analyzing the interactions\nbetween Training Dataset Size, Label Noise and Model Performance in\nRemote Sensing Data,” inProc. IEEE IGARSS Int. Geosci. Remote Sens.\nSymp., 2022, pp 303–306.\n[58] C. Xiao et al., “Image inpainting detection based on high-pass ﬁlter\nattention network,”Comput. Syst. Sci. Eng., vol. 43, no. 3, pp. 1146–1154,\n2022.\n[59] A. I. Shaikh and S. S. Badroddin, “Noise reduction from L-band\nALOSPALSAR data set using spatial domain Gaussian low-pass ﬁlter,”\nInt. Res. J. Adv. Sci. Hub, vol. 3, pp. 87–93, 2021.\n[60] L. Wang et al., “UNetFormer: A UNet-like transformer for efﬁcient\nsemantic segmentation of remote sensing urban scene imagery,”ISPRS\nJ. Photogrammetry Remote Sensing, 190, pp. 196–214, 2022.\n[61] L. Wang et al., “UNetFormer: A UNet-like transformer for efﬁcient\nsemantic segmentation of remote sensing urban scene imagery,”ISPRS\nJ. Photogrammetry Remote Sens., vol. 190, pp. 196–214, 2022.\n[62] X. Li et al., “SPCS: A spatial pyramid convolutional shufﬂe module for\nYOLO to detect occluded object,”Complex Intell. Syst., vol. 9, no. 1,\npp. 301–315, 2022.\n[63] I. Vaihingen, 2D semantic labeling dataset. Accessed: Apr. 2018.\n[64] X. He, Y . Zhou, J. Zhao, D. Zhang, R. Yao, and Y . Xue, “Swin transformer\nembedding UNet for remote sensing image semantic segmentation,”IEEE\nTrans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 4408715.\n[65] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional\nneural networks for volumetric medical image segmentation,” inProc. 4th\nInt. Conf. 3D Vis. (3DV), Stanford, CA, USA, 2016, pp. 565–571.\n[66] T. Xiao, Y . Liu, B. Zhou, Y . Jiang, and J. Sun, “uniﬁed Perceptual Parsing\nfor Scene Understanding,” in Proc. Eur . Conf. Comput. Vis., Munich,\nGermany: Springer, 2018, vol. 11209, pp. 432–448.\n[67] J. Fu et al., “Dual attention network for scene segmentation,” inProc.\nConf. Comput. Vis. Pattern Recognit., Long Beach, CA, USA, 2019,\npp. 3146–3154.\n[68] J. Chen et al., “TransUNet: Transformers make strong encoders for medical\nimage segmentation,” 2021,arXiv:2102.04306.\n[69] H. Cao et al., “Swin-Unet: Unet-like pure transformer for medical image\nsegmentation,” in Proc. Comput. Vis.-ECCV 2022 Workshops,C h a m ,\nSpringer Nature Switzerland, 2023, pp. 205–218.\nYouda Mo was born in June 2002 in Guangdong,\nChina. He is currently a Junior Student studying data\nscience and big data technology with the College of\nGuangdong Polytechnic Normal University. He will\nreceive the B.S. degree in June, 2024.\nHis research interests include semantic segmenta-\ntion of remote sensing images, especially the segmen-\ntation of high-resolution remote sensing images in\ncomplex environments.\nHuihui Li received the Ph.D. degree in computer\nscience and engineering from the South China Uni-\nversity of Technology, Guangzhou, China, in 2019.\nShe is currently a Lecturer with the School of\nComputer Science, Guangdong Polytechnic Normal\nUniversity, Guangzhou, China. Her current research\ninterests include image processing, pattern recogni-\ntion, and emotional computing.\n5296 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nXiangling Xiao received the B.S. degree in faculty\nof intelligent manufacturing from Wuyi University,\nJiangmen, China, in 2020. She is currently work-\ning toward the master’s degree with the School of\nComputer Science, Guangdong Polytechnic Normal\nUniversity, Guangzhou, China.\nHer research interests include deep learning and\ncomputer vision.\nHuimin Zhao was born in Shanxi, China, in 1966. He\nreceived the B.Sc. and M.Sc. degrees in signal pro-\ncessing from Northwestern Polytechnical University,\nXi’an, China, in 1992 and 1997, respectively, and the\nPh.D. degree in electrical engineering from the Sun\nYat-sen University, Guangzhou, China, 2001.\nHe is currently a Professor with and the Dean of\nthe School of Computer Science, Guangdong Poly-\ntechnic Normal University, Guangzhou, China. His\nresearch interests include image, video, and informa-\ntion security technology.\nXiaoyong Liu received the Ph.D. degree in li-\nbrary science from National Science Library, Chinese\nAcademy of Sciences, Beijing, China, in 2011.\nHe is currently a Professor with the School of\nComputer Science, Guangdong Polytechnic Normal\nUniversity, Guangzhou, China. His current research\ninterests include image processing, pattern recogni-\ntion, and natural language processing.\nJin Zhan received the Ph.D. degree in Computer Ap-\nplication from Sun Yat-sen University, Guangzhou,\nChina, in 2015.\nShe is currently an Associate Professor with the\nSchool of Computer Science, Guangdong Polytech-\nnic Normal University, Guangzhou, China. Her re-\nsearch interests include image and video intelligent\nanalysis, machine learning, and computer vision.",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.6777823567390442
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5862480401992798
    },
    {
      "name": "Computer science",
      "score": 0.5827115774154663
    },
    {
      "name": "Pooling",
      "score": 0.5419396758079529
    },
    {
      "name": "Image segmentation",
      "score": 0.5017392635345459
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.48828673362731934
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4842190742492676
    },
    {
      "name": "Pixel",
      "score": 0.44763147830963135
    },
    {
      "name": "Transformer",
      "score": 0.44385913014411926
    },
    {
      "name": "Feature extraction",
      "score": 0.41268986463546753
    },
    {
      "name": "Computer vision",
      "score": 0.376108318567276
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210122543",
      "name": "Guangdong Polytechnic Normal University",
      "country": "CN"
    }
  ]
}