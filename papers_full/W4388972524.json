{
  "title": "The classification of the bladder cancer based on Vision Transformers (ViT)",
  "url": "https://openalex.org/W4388972524",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092356629",
      "name": "Ola S. Khedr",
      "affiliations": [
        "Suez Canal University"
      ]
    },
    {
      "id": "https://openalex.org/A5113712765",
      "name": "M. El-Sayed Wahed",
      "affiliations": [
        "Suez Canal University"
      ]
    },
    {
      "id": "https://openalex.org/A5051267723",
      "name": "Al-Sayed R. Al-Attar",
      "affiliations": [
        "Zagazig University"
      ]
    },
    {
      "id": "https://openalex.org/A5051184721",
      "name": "E. A. Abdel-Rehim",
      "affiliations": [
        "Suez Canal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285008112",
    "https://openalex.org/W2983476233",
    "https://openalex.org/W4309613870",
    "https://openalex.org/W4234745332",
    "https://openalex.org/W3122592341",
    "https://openalex.org/W4309701552",
    "https://openalex.org/W3136816266",
    "https://openalex.org/W3011523538",
    "https://openalex.org/W3043049467",
    "https://openalex.org/W4323665498",
    "https://openalex.org/W4205881688",
    "https://openalex.org/W3166877082",
    "https://openalex.org/W2739836671",
    "https://openalex.org/W4321497104",
    "https://openalex.org/W4303856695",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4210247145",
    "https://openalex.org/W2975497362",
    "https://openalex.org/W4207048113"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports\nThe classification of the bladder \ncancer based on Vision \nTransformers (ViT)\nOla S. Khedr 1*, Mohamed E. Wahed 2, Al‑Sayed R. Al‑Attar 3 & E. A. Abdel‑Rehim 4\nBladder cancer is a prevalent malignancy with diverse subtypes, including invasive and non‑invasive \ntissue. Accurate classification of these subtypes is crucial for personalized treatment and prognosis. \nIn this paper, we present a comprehensive study on the classification of bladder cancer into into three \nclasses, two of them are the malignant set as non invasive type and invasive type and one set is the \nnormal bladder mucosa to be used as stander measurement for computer deep learning. We utilized a \ndataset containing histopathological images of bladder tissue samples, split into a training set (70%), \na validation set (15%), and a test set (15%). Four different deep‑learning architectures were evaluated \nfor their performance in classifying bladder cancer, EfficientNetB2, InceptionResNetV2, InceptionV3, \nand ResNet50V2. Additionally, we explored the potential of Vision Transformers with two different \nconfigurations, ViT_B32 and ViT_B16, for this classification task. Our experimental results revealed \nsignificant variations in the models’ accuracies for classifying bladder cancer. The highest accuracy \nwas achieved using the InceptionResNetV2 model, with an impressive accuracy of 98.73%. Vision \nTransformers also showed promising results, with ViT_B32 achieving an accuracy of 99.49%, and \nViT_B16 achieving an accuracy of 99.23%. EfficientNetB2 and ResNet50V2 also exhibited competitive \nperformances, achieving accuracies of 95.43% and 93%, respectively. In conclusion, our study \ndemonstrates that deep learning models, particularly Vision Transformers (ViT_B32 and ViT_B16), \ncan effectively classify bladder cancer into its three classes with high accuracy. These findings have \npotential implications for aiding clinical decision‑making and improving patient outcomes in the field \nof oncology.\nBladder cancer is a major global health concern, contributing significantly to cancer-related morbidity and \nmortality. Accurate classification of its distinct histopathological subtypes is crucial for tailored treatments \nand predicting disease  progression1. Bladder cancer is a prevalent global malignancy, presenting a significant \npublic health challenge. According to the World Health Organization (WHO), it ranks among the top ten most \nfrequently diagnosed cancers, with approximately 550,000 new cases reported annually  worldwide2. The disease \nexhibits diverse histopathological subtypes, including invasive, non-invasive, and normal tissue, necessitating \ntailored therapeutic approaches. Precise and dependable classification of these subtypes is critical for determining \nappropriate treatment modalities, such as surgery, radiation therapy, or immunotherapy, while also assessing the \nlikelihood of disease progression and  recurrence3.\nBladder cancer commonly originates from the bladder’s inner surface epithelium (urothelium), with urothelial \ncarcinomas being the predominant type. However, a subset of bladder cancers (10–25% of cases) displays variant \nhistology, exhibiting distinct histomorphological phenotypes such as squamous cell carcinoma, small-cell \ncarcinoma, and adenocarcinoma. High-grade urothelial carcinomas can manifest as micropapillary, sarcomatoid, \nplasmacytoid, nested, or microcystic variants, often showing divergent differentiation into squamous and \nglandular histologies. Bladder cancers with variant histology tend to be locally aggressive, prone to metastasis, \nand exhibit poor responses to existing therapies. Nonetheless, there is ongoing debate regarding the true impact \nof histology on patient  outcomes4.\nMoreover, accurate bladder classification contributes to optimizing patient care by facilitating early detection \nof aggressive tumors and enabling personalized medicine, ultimately leading to enhanced survival rates and \nOPEN\n1Department of Mathematics -Computer Science, Faculty of Science, Suez Canal University, Ismailia 44745, \nEgypt. 2Department of Computer Science, Faculty of Computers and Informatics, Suez Canal University, \nIsmailia 44692, Egypt. 3Department of Pathology, Faculty of Vetrinary Medicine, Zagazig University, \nZagazig 11144, Egypt. 4Department of Mathematics, Faculty of Science, Suez Canal University, Ismailia 41552, \nEgypt. *email: ola_salah@science.suez.edu.eg\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nimproved quality of life for individuals afflicted by this multifaceted ailment. As the incidence of bladder \ncancer continues to rise, advancements in precise classification methodologies, including the incorporation of \nsophisticated deep learning techniques, hold the potential to revolutionize diagnostic and therapeutic approaches, \nultimately culminating in better patient outcomes and alleviating the global burden of bladder  cancer5.\nDeep learning, a specialized branch of machine learning, employs intricate neural networks to decipher \nintricate patterns in extensive datasets. In the realm of medical imaging, these deep learning algorithms have \nplayed a pivotal role in refining the precision and effectiveness of various tasks, such as segmenting images, \nrecognizing objects, and extracting essential  features6. This paper likely explores the application of deep learning \nmodels to bladder datasets, potentially employing techniques like Transfer learning model to automatically \ndiscern and extract pertinent contextual information from medical images. This process might involve the \nidentification of specific structures within the bladder, the detection of irregularities or deviations from the \nnorm, and the provision of valuable insights crucial for accurate diagnosis and meticulous treatment planning.\nThis paper presents a comprehensive study on the classification of bladder cancer into its three distinct classes, \nleveraging cutting-edge deep learning methodologies. Histopathological analysis remains the gold standard \nfor diagnosing and subclassifying bladder cancer due to its ability to assess cellular morphology and tissue \narchitecture, crucial factors in determining tumor behavior. Nevertheless, the subjectivity inherent in manual \ninterpretation has motivated the exploration of data-driven approaches aimed at enhancing diagnostic accuracy \nand reproducibility.\nOur primary objective is to evaluate the performance of state-of-the-art deep learning models in classifying \nbladder cancer subtypes. We focus on two categories of models: transfer learning and Vision Transformers (ViT), \nboth of which have demonstrated remarkable success in various image recognition tasks. Transfer learning \ncapitalizes on pre-trained knowledge from large-scale datasets to overcome data limitations and expedite model \nconvergence, while Vision Transformers, a relatively novel paradigm, enable efficient image processing through \nself-attention mechanisms, making them particularly suitable for medical image analysis. Through rigorous \nevaluation, six deep learning models were examined, including EfficientNetB2, InceptionResNetV2, InceptionV3, \nResNet50V2, and two Vision Transformer variants, ViT_B32 and ViT_B16. We aim to identify the most effective \nmodel for multi-class bladder cancer classification. To ensure robust model performance and generalizability, \nthe dataset used in this study comprises a diverse range of bladder tissue samples, meticulously annotated into \ntraining, validation, and test sets.\nOur work contributes to the growing body of literature on artificial intelligence in healthcare, emphasizing the \npivotal role of advanced technologies in shaping the future of precision medicine. The following points conclude \nthe contributions of this research:\n1. Comprehensive evaluation  We assess six advanced deep learning models, including transfer learning \narchitectures and Vision Transformers, for multi-class bladder cancer classification, providing valuable \ninsights for informed diagnostic decisions. The dataset is manually collected from the Faculty of Medicine, \nZagazig University.\n2. Advancing bladder cancer classification  Leveraging advanced deep learning, we improve bladder cancer \nclassification, enabling precise identification of invasive, non-invasive, and normal tissue subtypes for better \ntreatment planning and prognostic capabilities.\n3. Vision transformers’ potential  Exploring Vision Transformers’ success in bladder cancer classification \nhighlights their high accuracy and suitability for medical image analysis, encouraging further investigation \nin medical imaging tasks.\n4. Addressing subjectivity  Employing deep learning reduces inter-observer variability in conventional \ndiagnostics, yielding more objective and reproducible classification, and enhancing bladder cancer diagnosis \nreliability.\nRelated works\nIn recent years, the automatic feature extraction capabilities of deep learning-based models have made them \nmore and more well-liked in the field of medical image analysis. Convolutional neural networks (CNNs) stand \nout among these models as the most popular and well-regarded framework, notably for classification problems \nrequiring radiological data. For instance, researchers in Y ang et al.7, categorize muscle-invasive bladder cancer \n(MIBC) and non-muscle-invasive bladder cancer (NMIBC) using contrast-enhanced CT (CECT) images, \na comprehensive collection of nine CNN-based models was created. 369 patients who underwent radical \ncystectomy provided the 1200 CT scans that made up the dataset utilized for training and evaluation. 249 of \nthese individuals had NMIBC diagnoses, and the remaining 120 had MIBC. The CNN model was initially trained \nusing the ImageNet dataset to improve classification performance. The result of the model are AUC equal to \n99.70% and accuracy of 93.90%. Additionally, Chapman-Sung et al. 8 developed a CNN-based classification \nscheme for the two stages of bladder cancer. 84 bladder cancer CR urography (CTU) images from 76 patients \nwere included in the training dataset, with 41 CTUs containing high-stage cancer and 43 CTUs including \nlow-stage cancer. 90 bladder CTUs from 86 patients made up the test set. On the same dataset, the texture-\nbased classification method utilizing SVM attained an accuracy of 88.00%, while the CNN classifier displayed \na high-test set prediction accuracy of 91.00%. According to Chapman-Sung et al. 9, The authors contend that \nautomatically generated features using CNN-based classifiers are outperformed by feature extraction based on \ndomain expertise. The goal was to categorize two difficult early bladder cancer stages, Ta (non-invasive) and T1 \n(superficially invasive), which are tricky to distinguish histologically. 1177 bladder scans totaled in the dataset, \nof which 460 were classified as minimally invasive and 717 as superficially invasive. The accuracy of the CNN \nclassifiers, which was just 84.0%, was much lower than that of supervised machine learning classifiers developed \n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nusing manually extracted features. Also other researchers in Sarkar et al.10 planned to use the radionics-assisted \ninterpretation of CT scans to develop a diagnosis model for bladder cancer. Normal vs bladder cancer, NMIBC \nversus MIBC, and post-treatment changes (PTC) versus MIBC were the three classification tasks that were \ncarried out. The dataset had 165 areas of interest (ROIs), 100 normal samples, and 65 cancer samples. It was \nretrospective and single-center. Cross-validation was performed ten times. The accuracy for the LDA classifier on \nXceptionNet-based characteristics for determining whether a person has cancer or not was 86.07%. Researchers \nin the  study11 collected CT data from 75 bladder patients for classification and staging using a ResNet-based \nmodel, and applied super-resolution to enhance medical pictures. The model has 94.74% sensitivity rate, which \nis comparable to preoperative pathological diagnosis, was attained after retrospectively evaluating data from 76 \nindividuals with bladder cancer. The 183 patients that made up the dataset utilized in the  study12 were divided \ninto three sets: 110 for training, 73 for internal validation, and 75 for external testing. The researchers created a \nbrand-new convolutional network called FGP-Net that included DFL and Dense Blocks modules. The FGP-Net \nobtained an AUC (Area Under the Receiver Operating Characteristic Curve) of 0.861 and an accuracy of 0.795 \nwhen tested on the internal dataset. The FGP-Net achieved an AUC of 0.791 and an accuracy of 0.747 on the \nexternal dataset. These results demonstrate the effectiveness of the FGP-Net in classifying the patients and its \ngeneralizability on an external dataset.\nThese studies collectively demonstrate the promising potential of deep learning-based models in bladder \ncancer classification, offering valuable insights into the development of sophisticated approaches for improving \ndiagnostic accuracy and patient outcomes in the field of oncology.\nMaterial and methods\nThis section presents the methodology for the proposed work, as depicted in Fig.  1. Initially, the dataset is \nobtained, comprising 2629 images categorized into three classes. The dataset is then pre-processed by performing \nimage resizing and scaling. Subsequently, the pre-processed data is divided into suitable subsets for training, \nvalidation, and testing purposes. Moving forward, the methodology involves implementing both transfer learning \nFigure 1.  Block diagram for bladder cancer classification.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nand vision transformer models. These models are trained on the pre-processed dataset to acquire task-specific \nrepresentations. Following training, the models are compared based on their performance metrics, and the model \nwith the best performance is chosen for the final evaluation.\nIn alignment with applicable guidelines and regulations, all methodologies employed in this study for the \nclassification of bladder cancer using ViT were meticulously executed. The experimental protocols underwent \nrigorous approval by the ethics committee/ IRB of faculty of medicine at Zagazig University, ensuring adherence \nto established standards. Furthermore, it is imperative to note that prior to any involvement, informed consent \nwas meticulously obtained from all subjects or their legally recognized guardians, underscoring the ethical \nconsiderations upheld throughout this research endeavor.\nDataset\nThe dataset employed in this research is a proprietary dataset established by our team at Zagazig University in \nEgypt specifically for this study. This dataset has been developed and authorized under the Institutional Review \nBoard (IRP) number 11044-22-8-2023. It comprises a total of 2629 images classified into three classes , two of \nthem are the malignant set as non invasive type and invasive type and one set is the normal bladder mucosa \nto be used as stander measurement for computer deep learning. Among these, 1841 images are assigned to the \ntraining set, while 394 images are evenly distributed between the validation and testing sets. Figure 2 displays a \nsample of the dataset, representing images from all three classes.\nPreprocessing\nThe preprocessing stage involves resizing the dataset images to dimensions of 128 × 128 × 3. This step ensures \nthat the images are normalized and compatible with the pre-trained models utilized in the study. Additionally, \nscaling is a crucial aspect of the training process. This procedure involves dividing all pixel values in the images \nby 255, thereby bringing all images within the range of 0–1. The final step in the preprocessing is the data split-\nting. We split the dataset into three portions with descriptions like in Table 1.\nAs observed in Table  1, the dataset has been meticulously divided into three distinct segments: training, \nvalidation, and testing, each playing a crucial role in the model development process. In the training set, which \nconstitutes 70% of the entire dataset and encompasses 1841 samples, there are 1062 instances of the invasive class, \n461 samples of the non-invasive class, and 318 samples of the normal class. This diversity within the training \ndata allows the deep learning models to grasp the intricate patterns of each class. The validation set, comprising \n15% of the dataset with 394 samples, serves as a critical checkpoint. It includes 227 samples of the invasive class, \n99 samples of the non-invasive class, and 68 samples of the normal class, enabling the fine-tuning of the model’s \nparameters and preventing overfitting. Lastly, the testing set, mirroring the validation set with 15% of the data \nand 394 samples, provides an unbiased evaluation ground. It consists of 228 samples of the invasive class, 100 \nFigure 2.  Dataset samples for the three classes of bladder dataset.\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nsamples of the non-invasive class, and 68 samples of the normal class, allowing for a comprehensive assessment \nof the model’s performance on previously unseen data. This meticulous division strategy, totaling 2629 samples \nin the dataset, ensures a robust evaluation of the model’s accuracy and effectiveness across diverse classes, making \nit well-prepared for real-world applications..\nTransfer learning models\nTransfer learning is a method in deep-learning that utilizes knowledge obtained while solving one problem to \nenhance the performance of a similar but distinct problem. Rather than commencing the learning process anew \nfor each new task, transfer learning enables the model to build upon the knowledge gained from a previously \nmastered task as a  foundation13. We used four types of transfer learning as described below.\nEfficientNetB2\nEfficientNetB2 uses a hierarchical network architecture with multiple layers for effective feature extraction from \ninput images. Starting with convolutional layers as initial feature extractors, it detects low-level patterns like \nedges, textures, and colors. Progressing through the network, it employs deeper layers to capture higher-level \nfeatures and semantic information, enabling a better understanding of complex patterns and relationships within \nimages. The combination of depth-wise separable convolutions and squeeze-and-excitation blocks enhances \nrepresentation power, effectively capturing relevant contextual information. EfficientNetB2 benefits from com-\npound scaling to strike a balance between computational efficiency and  performance14.\nInceptionV3\nIn 2015, Google introduced InceptionV3, an advanced deep convolutional neural network designed for image \nclassification. It improves upon earlier Inception models, also known as GoogLeNet, by incorporating various \narchitectural enhancements. The central idea of InceptionV3 lies in employing multiple “Inception” modules, \nwhich are small subnetworks that capture features at different scales using 1  × 1, 3 × 3, and 5 × 5 convolutions, \nalong with max-pooling operations. This combination effectively captures both local and global information \nfrom input images. To enhance training and regularization, InceptionV3 utilizes techniques like batch normaliza-\ntion and auxiliary classifiers. These auxiliary classifiers are strategically placed at intermediate layers to address \nthe vanishing gradient problem during training. With approximately 24 million parameters, InceptionV3 is a \ndeep neural network commonly employed for image classification and feature extraction tasks in the field of \ncomputer  vision15.\nResNet50V2\nResNet and its variations, including ResNet50V2, introduced a significant advancement in deep neural networks \nby utilizing residual blocks. These blocks enable the training of extremely deep networks and address the vanish-\ning gradient issue common in conventional deep architectures. By incorporating skip connections, residual blocks \nallow the network to learn the difference between input and output, facilitating smoother gradient flow during \nbackpropagation and simplifying the training of deep models. ResNet50V2 specifically denotes a ResNet model \nwith 50 layers. It comprises a sequence of convolutional layers, batch normalization, ReLU activation functions, \nand max-pooling operations. The network is organized into multiple stages, each housing a set of residual blocks. \nBy integrating skip connections at various intermediate layers, the ResNet50V2 architecture excels in handling \ndeeper networks and exhibits enhanced training  performance16.\nInceptionResNetV2\nInceptionResNetV2 is an advanced convolutional neural network resulting from a collaboration between Google \nresearchers in 2016. It combines Inception modules from InceptionV3 and residual blocks from ResNet to achieve \nsuperior performance and accuracy. The network captures diverse and multi-scale features through integrated \nfilter sizes like 1 × 1, 3 × 3, and 5 × 5 convolutions, while skip connections address the vanishing gradient prob -\nlem for deep architectures. InceptionResNetV2 also utilizes batch normalization to enhance training speed and \nstability. With its deep and intricate design, it excels in various computer vision tasks like image classification, \nobject detection, and feature  extraction17.\nEquation (1) represents the SoftMax activation function, a widely utilized method for multi-class classification \ntasks at the end of transfer learning models. This function takes an input vector and converts it into a probability \ndistribution across various classes. In our case, it will generate probabilities for different types of bladder cancer. \nThe SoftMax formula is as  follows18:\nTable 1.  Number of samples per every portion of the data splitting process.\nDataset portion Percentage (%) # Samples # invasive class # non invasive class #normal class\nTraining 70 1841 1062 461 318\nValidation 15 394 227 99 68\nTesting 15 394 228 100 68\nTotal 100 2629 1517 660 454\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nIn this context, the symbol “z” represents the input vector, “e zi” refers to the standard exponential function \nof the input vector, “ezj” represents the standard exponential function of the output vector, and “K” denotes the \nnumber of classes in the multi-class classifier.\nIn summary, the transfer learning models were created with the specific purpose of extracting features from \nbladder images through convolutional layers and then classifying those features into different groups using the \nSoftMax activation function in the output layer. Table  2 provides a thorough overview of the crucial hyperpa-\nrameters used during the application of the trained models.\nAs seen in Table 2, The model optimizes its performance with the Adam optimizer using specific beta values \nand a learning rate of 0.001. It employs categorical cross entropy as the loss function to predict different classes. \nTraining progress is assessed to process 32 samples per batch and undergoes 30 training epochs to learn from \nthe data and enhance performance.\nVision transformers models\nVision Transformers (ViT) are advanced deep-learning models used for processing visual data, such as images. \nThey are built on the Transformer architecture, initially designed for natural language processing but later adapted \nsuccessfully for computer vision tasks. ViT excels at detecting intricate patterns in images and comprehending \nglobal contexts, making it effective for various computer vision applications 1. An overview of the model is \ndepicted in Fig. 3.\nAs seen in Fig. 3, We start by dividing the image into patches of a fixed size. Each patch is then transformed \nlinearly and augmented with position information using position embeddings. The sequence of these embedded \nvectors is then passed through a conventional Transformer encoder. To enable classification, we incorporate a \ncustomary method of introducing an extra learnable ’ classification token’ to the  sequence20.\nSoftMa x Ϭ(z) = ∑ (1)\nTable 2.  Hyperparameters for the suggested models of transfer learning.\nNo Parameter Values\n1 Optimizer Adam, beta_1 = 0.9, beta_2 = 0.999\n2 Learning rate 0.001\n3 Loss function categorical_crossentropy\n4 Metrics Accuracy, precision, confusion matrix, F1-score, sensitivity, and ROC AUC values\n5 Batch size 32\n6 Epochs 30\nFigure 3.  Vit model overview. We split the image into uniform patches, convert them into vectors, add \npositional data, and input this sequence into a standard Transformer encoder. For classification, we include a \ntrainable “classification token. ” Our encoder design is inspired by this  research19.\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nViT_B16\nViT_B16 is a specific variant of the Vision Transformer model. It has a smaller backbone size compared to the \noriginal ViT, leading to improved computational efficiency while maintaining strong performance. The funda-\nmental idea behind ViT_B16 involves dividing the input image into smaller patches and converting each patch \ninto a vector representation using linear embedding. These patch embeddings, along with positional embeddings \nthat provide spatial information, are then passed through multiple layers of the Transformer encoder. Within \nthese layers, the model employs self-attention mechanisms to weigh the significance of each patch concerning \nothers, effectively capturing long-range dependencies. The resulting transformed feature representations are then \nutilized for tasks such as image classification and other visual processing  objectives21.\nViT_B32\nViT_B32 is another variant of the Vision Transformer model distinguished by its larger backbone size compared \nto ViT_B16. This increased backbone size allows ViT_B32 to potentially capture more intricate details and \nnuanced features in images. Like ViT_B16, ViT_B32 processes images by dividing them into patches and lin-\nearly embedding each patch into a vector representation. Subsequently, these embeddings, along with positional \nembeddings, are fed through multiple layers of the Transformer encoder. The self-attention mechanisms in these \nlayers analyze the relationships between patches, enabling the model to comprehend complex image patterns \nacross larger spatial contexts. The transformed feature representations are then leveraged for tasks such as image \nclassification and other visual analysis purposes. Despite potentially requiring more computational resources due \nto its larger size, ViT_B32’s ability to capture detailed image information makes it a valuable tool for addressing \nchallenging computer vision  tasks21.\nEvaluation metrics\nWe must employ metrics for calculating the performance of these models to assess the deep learning models. \nCalculations for these metrics are shown in the Eqs. (2–5) below:\nwhere TP stands for positive and anticipated positive values, FP for projected positive but negative values, FN \nfor positive but anticipated negative values, and TN for both genuinely negative and expected negative values. \nAdditionally, the performance of prostate cancer classification using the ROC curve is provided.\nResults\nThe suggested model is evaluated on a desktop computer running Windows 11 with an Intel Core i7-11800@3.6 \nGHz processor, an NVIDIA RTX3060 graphics card, and 16 GB of RAM. For the usual back-propagation tech-\nnique, the learning rate, beta 1 and beta 2 values are 0.001 and 0.9, respectively. The learning rate is set at 0.005. \nThe weights are updated within mini-batches with a batch size of 32. Training is deemed to be finished when \nthe network’s performance does not considerably increase with more iterations. We also employ the reduce-on-\nplateau strategy for the learning rate with a 0.5-factor value to reach the lowest learning rate, min lr = 0.0000001.\nExperimental results\nTransfer learning models results\nIn this part, we present the outcomes of the transfer learning models, namely InceptionV3, ResNet50V2, Incep-\ntionResNetV2, and EfficientNetV2. Table 3 illustrates the metrics values for all the suggested transfer learning \nmodels, encompassing accuracy, precision, sensitivity, specificity, and F1-score.\n(2)Precision= TP\n(TP + FP )\n(3)Sensitiﬁty(Recall) = TP\n(TP + FN )\n(4)Accuracy= (TP + TN )\nTP + TN + FP + FN\n(5)F 1score= 2\n1\nRecall + 1\nPrecision\nTable 3.  Metrics values result for transfer learning models.\nModel Accuracy% Precision% Sensitivity% F1-score%\nEfficientNetB2 95.43 97.66 94 95.33\nInceptionV3 85.28 84.66 86.33 85\nRseNet50V2 92.64 93 88 89.33\nInceptionResNetV2 98.73 99.33 98.33 99.33\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nAs observed in Table 3, InceptionResNetV2 achieves exceptional performance, with an impressive accuracy of \n98.73% and high precision, sensitivity, and F1-score, all at 99.33%. EfficientNetB2 also shows strong performance, \nwith 95.43% accuracy and 97.66% precision, indicating its reliability in minimizing false positives. While \nInceptionV3 and RseNet50V2 perform well, their metrics values are slightly lower compared to other models. \nInceptionV3 achieves 85.28% accuracy, and RseNet50V2 achieves 92.64% accuracy with good precision and \nsensitivity. Overall, InceptionResNetV2 stands out as the most promising model for precise bladder cancer \nclassification, while EfficientNetB2 also demonstrates great potential for practical use. Figure  4 represents the \nROC AUC score for every transfer learning model.\nAs seen in Fig.  4, The AUC results of four transfer learning models, including InceptionV3, ResNet50, \nInceptionResNetV2, and EfficientNetB2, were examined for their effectiveness in classifying bladder cancer. \nResNet50 and InceptionResNetV2 demonstrated notably high AUC values, nearly reaching 1.0, for both the \n“Invasive” and “Normal” classes. Notably, InceptionResNetV2 achieved a perfect AUC of 1.0 for the “Normal” \nclass, showcasing its exceptional accuracy in identifying normal bladder samples. InceptionV3 and EfficientNetB2 \nalso displayed favorable performance, albeit with slightly lower AUC values observed for the “Non-invasive” \nand “Non-invasive” classes, respectively. The overall AUC results suggest that these transfer learning models \npossess robust discriminatory capabilities when distinguishing between various classes of bladder cancer. Given \nthe superior performance of ResNet50 and InceptionResNetV2, these models could be prioritized as preferred \noptions for accurate bladder cancer classification. Figure  5 represents the confusion matrices for the proposed \ntransfer learning models. The confusion matrix results offer valuable insights into the classification performance \nof each model for bladder cancer.\nFigure 4.  ROC curves for the proposed transfer learning models. (a) represent the ROC curve for the \nInceptionV3 model, (b) represent the ROC AUC curve for the ResNet50V2 model, (c) For InceptionResNetV2, \nand (d) the representation of EfficientNetB2.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nAs seen in Fig. 5, InceptionV3 shows a balanced performance for the “Invasive” and “Non-invasive” classes, \nwith moderately high true positive counts. However, it faces challenges in correctly identifying “Non-invasive” \ncases, as indicated by a higher false negative count. ResNet50 displays excellent performance with high true \npositive counts for both “Invasive” and “Non-invasive” cases. It has the fewest false negatives for the “Non-\ninvasive” class. InceptionResNetV2 achieves near-perfect true positive counts for all classes, demonstrating \nexceptional performance. EfficientNetB2 also performs well, with high true positive counts for all classes, \nthough it has a slightly elevated false negative count for “Non-invasive” cases compared to other models. Figure 6 \nrepresent the training accuracy and loss changes with the function epoch for the proposed transfer learning \nmodels.\nAccording to the grpahs above , it’s seen that the best gpah is for the InceptionResNetV2 model because during \nInceptionResNetV2 model training, the training loss impressively low at 0.0128 indicated accurate predictions \naligned with the training data. Achieving a remarkable 99.57% training accuracy demonstrated the model’s ability \nto classify training samples. In validation, the model showed a strong performance with 98.22% accuracy, but \na slightly higher validation loss at 2.5682 suggested room for improvement. Despite this, InceptionResNetV2 \nexhibited robustness, making it a promising choice for machine learning tasks. Fine-tuning could further enhance \nits performance on unseen data.\nVision Transformers models\nIn this section, we will present the outcomes obtained using vision transformers, specifically VIT_B16 and \nVIT_B32. The results will be showcased in terms of evaluation metrics, including ROC AUC scores and the \nFigure 5.  Confusion matrix for the transfer learning models. (a) represent the confusion matrix for the \nInceptionV3 model, (b) it’s for the ResNet50, (c) Confusion matrix for the InceptionResNetV2 and (d) it’s for \nthe confusion matrix of EfficientNetB2 model.\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nFigure 6.  Accuracy and loss progressing in function of epochs, (a) represent the training and validation \naccuracy changes for the InceptionV3 model and (b) represents the loss changes for the same model. (c) and (d) \nrepresent the accuracy and loss changes for the ResNet50 model. (e) and (f) illustrate the progress of accuracy \nand loss respectively for the InceptionResNetV2 model and finally and finally (g) and (h) represent graphs for \nthe EfficientNetB2 model.\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nconfusion matrix. Table 4 presents the metrics values for the two proposed vision transformers models, covering \naccuracy, precision, sensitivity, and F1-score.\nIn Table 4, Both models exhibit outstanding performance with high accuracy percentages of 99.24% for \nVIT_B16 and 99.49% for VIT_B32. The precision percentages are also excellent, with VIT_B16 at 99% precision \nand VIT_B32 at 99.33% precision. Impressive sensitivity levels are observed for both models, with VIT_B16 and \nVIT_B32 achieving 99.66% sensitivity each. Additionally, the F1-score, which balances precision and sensitivity, \nis remarkably high at 99% for VIT_B16 and 99.33% for VIT_B32. These results highlight the remarkable ability \nof VIT_B16 and VIT_B32 to accurately classify bladder cancer, establishing them as dependable and effective \ntools for medical image analysis in bladder cancer diagnosis and management. Figure  7 represent the ROC \nAUC score for the two ViT proposed models. They are the same value which equal to 1 for all classes in the two \nproposed vision models.\nAs seen in Fig. 7, the AUC results for vision transformers models, VIT_B16 and VIT_B32, are exceptionally \nhigh, indicating outstanding performance in bladder cancer classification across all three classes—Invasive, Non-\ninvasive, and Normal. Achieving perfect AUC values of 1.0 for all classes demonstrates accurate differentiation \nbetween positive and negative samples, ensuring optimal sensitivity and specificity.\nIn summary, both VIT_B16 and VIT_B32 demonstrate remarkable capability in bladder cancer classification, \nshowcasing their potential in revolutionizing medical image analysis. Their perfect AUC scores underscore \ntheir robustness and reliability in identifying different stages of bladder cancer, offering valuable insights for \naccurate diagnosis and treatment planning in clinical settings. Figure  8 represents the confusion matrices for \nthe proposed two Vit models.\nAs seen in Fig.  8, VIT_B16 Fig.  8(a) and VIT_B32 Fig.  8(b) show exceptional performance in classifying \nbladder cancer with 394 test samples across Invasive, Non-invasive, and Normal classes. Achieving high true \npositive counts and minimal misclassifications, these models demonstrate accurate identification of positive \ncases for Invasive and Non-invasive classes and precise discrimination of normal bladder samples. These results \nunderscore the reliability and effectiveness of VIT_B16 and VIT_B32 in bladder cancer classification, making \nthem valuable tools for medical image analysis and aiding diagnosis and treatment management.\nAs seen in Fig.  9, In the final epoch, both Vision Transformer models excelled. ViT_B16 demonstrated \nremarkable performance with a training loss of 0.5530 and a training accuracy of 99.61%. Validation results \nwere equally impressive, showcasing a validation loss of 0.5417 and a validation accuracy of 99.74%. Similarly, \nViT_B32 exhibited outstanding metrics with a training loss of 0.5387 and a training accuracy of 99.78%, along \nwith a flawless validation accuracy of 100% and a validation loss of 0.5300. These exceptional results highlight \nthe models’ accuracy in both training and validation, making them robust choices for complex machine learning \ntasks, particularly those involving intricate patterns and diverse datasets.\nComparison with related works\nIn this section, we perform a comparative analysis between our models and other relevant studies to highlight the \neffectiveness of the proposed models and stress the importance of leveraging Vision transformers like VIT_B16 \nTable 4.  Metrics results for the vision transformers models.\nModel Accuracy% Precision% Sensitivity% F1-score%\nVIT_B16 99.24 99 99.66 99\nVIT_B32 99.49 99.33 99.66 99.33\nFigure 7.  ROC AUC score for the VIT_B16 and VIT_B32 models.\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nand VIT_B32. Our goal is to showcase the superiority and efficacy of our models while emphasizing the relevance \nof incorporating Vision transformers in image classification tasks. Table 5 provides a comprehensive comparison \nbetween the proposed vision transformers models and several related studies in the realm of bladder cancer \nclassification.\nAccording to Table 5, Y ang et al. employed CNN with 1200 CT scans from bladder patients, achieving an \nimpressive AUC of 99.70% and 93.90% accuracy. Chapman-Sung et al. used CNN on 84 bladder cancer CR \nurography (CTU) images from 76 patients, obtaining an accuracy of 91.00%. Yin et al. utilized CNN on 1177 \nbladder scans, resulting in an accuracy of 84.00%. Sarkar et al. employed XceptionNet on 165 regions of inter -\nest, achieving an accuracy of 86.07%. Liu et al. utilized ResNet on CT data from 75 bladder patients, obtaining \na sensitivity of 94.74%. G. Zhang et al. implemented FGP-Net on CT data from 183 patients, achieving an AUC \nof 0.861 and an accuracy of 79.5%.\nIn contrast, the proposed Vision transformers, VIT_B16 and VIT_B32, surpassed the performance of related \nworks with remarkable accuracy levels of 99.24% and 99.49%, respectively. Moreover, both models achieved a \nperfect AUC of 1.0, highlighting their exceptional capability to accurately classify bladder cancer. Evaluated on a \ndataset comprising 2629 CT scans, the proposed vision transformers models demonstrate superior performance \nand exemplify their potential as state-of-the-art models for precise bladder cancer classification. These outcomes \nunderscore the importance of employing Vision transformers in medical image analysis and the promising \nadvancements they offer to the domain of bladder cancer diagnosis and treatment.\nDiscussion\nOur research presents a substantial leap forward in bladder cancer classification through the strategic utilization \nof transfer learning models, with a specific emphasis on the unparalleled efficacy of vision transformers. Within \nthe realm of transfer learning, models like InceptionResNetV2, EfficientNetB2, and RseNet50V2 showcased \ncommendable performances, yet it was the vision transformers, namely VIT_B16 and VIT_B32, that stood out \nprominently. This robust performance underlines the effectiveness of leveraging pre-trained networks in solving \nintricate medical image analysis problems.\nThe exceptional accuracy achieved by our proposed vision transformers is a testament to their proficiency. \nVIT_B16 and VIT_B32 exhibited astounding accuracies of 99.24% and 99.49%, respectively. These results are \nindicative of the vision transformers’ unique ability to discern intricate patterns within bladder cancer images, \nleading to highly precise classifications. At the heart of their success lies the self-attention mechanism, enabling \nthese models to focus intently on relevant regions, thereby enhancing their classification accuracy significantly. \nAdditionally, both models demonstrated perfect AUC values of 1.0, underlining not only their accuracy but also \ntheir discrimination prowess between cancerous and non-cancerous tissues.\nComparing our findings with prior studies illuminates the superiority of vision transformers in medical \nimage analysis, specifically in bladder cancer classification. When juxtaposed against existing works utilizing \nCNN approaches, including studies by Y ang et al., Chapman-Sung et al., and Yin et al., our vision transformers \nexhibited a substantial performance gap, surpassing their accuracies significantly. Moreover, when benchmarked \nagainst models like XceptionNet and ResNet used by Sarkar et al. and Liu et al., respectively, our vision trans-\nformers consistently outperformed them, indicating the unique advantages offered by vision transformers over \nthese conventional architectures.\nThe robustness and generalizability of our vision transformers further enhance their appeal in the realm of \nmedical image analysis. These models demonstrated remarkable stability across diverse datasets and imaging \nmodalities, attesting to their ability to adapt and learn meaningful representations from varied data sources. \nFigure 8.  Confusion matrices for the VIT_B16 and VIT_B32 models. (a) represent the confusion matrix for \nthe ViT_B16 and (b) for the ViT_B32 model.\n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nThis robustness ensures consistent performance, even in the face of dataset variability, making them invaluable \ntools in real-world scenarios. Moreover, the study’s findings suggest that the success of our vision transformers \nextends beyond bladder cancer classification. Their ability to discern intricate patterns and relationships in data \nsignifies their potential for application in a plethora of medical conditions, thus heralding a new era in the field \nof healthcare diagnostics and analysis.\nRegarding our choice of models, after extensive experimentation, we found that Vision Transformer models \nViT_B16 and ViT_B32 offered superior accuracy and computational efficiency for bladder classification. \nFigure 9.  Training and validation accuracy and loss progress for the ViT_B16 and ViT_B32 model (a) represent \nthe training and validation accuracy changes for the ViT_B16 and (b) represents the loss for the same model (c) \nrepresents the accuracy changes for the training and validation for the ViT_B32 model and (d) represent the loss \nchanges for the same model.\n14\nVol:.(1234567890)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\nSimultaneously, the transfer learning architectures InceptionV3, ResNet50V2, InceptionResNetV2, and \nEfficientNetB2 were selected based on their established performance in medical image analysis and their ability \nto balance accuracy with computational resources. Our decision was driven by thorough experimentation, \nensuring the optimal models were chosen for our study.\nIn conclusion, our study establishes that vision transformers, particularly VIT_B16 and VIT_B32, epitomize \nthe pinnacle of performance in bladder cancer classification. Their exceptional accuracy, perfect AUC values, and \nsuperiority over conventional CNN architectures underscore their efficacy in medical image analysis. Beyond \nbladder cancer, their adaptability and potential for generalization to diverse medical imaging tasks mark a \nsignificant advancement, holding promise for revolutionizing various aspects of healthcare diagnostics and \ntreatment planning.\nConclusion\nIn conclusion, our research showcases the impressive performance of both transfer learning models and Vision \ntransformers in bladder cancer classification. The remarkable accuracy and AUC values achieved by our pro-\nposed VIT_B16 and VIT_B32 models underscore their potential as cutting-edge tools for precise bladder cancer \ndiagnosis and treatment planning. This advancement in medical image analysis holds great promise for improv-\ning the field of bladder cancer diagnosis and contributes to the progress of computer-aided diagnosis methods. \nThe utilization of Vision transformers in this context presents a significant leap forward in accurate and reliable \nbladder cancer classification.\nData availability\nThe dataset can be obtained by contacting the corresponding author through their email address: ola_salah@\nscience.suez.edu.eg.\nReceived: 17 August 2023; Accepted: 21 November 2023\nReferences\n 1. Sarafidis, M., Lambrou, G. I., Zoumpourlis, V . & Koutsouris, D. An integrated bioinformatics analysis towards the identification \nof diagnostic, prognostic, and predictive key biomarkers for urinary bladder cancer. Cancers (Basel) https:// doi. org/ 10. 3390/ cance \nrs141 43358 (2022).\n 2. Richters, A., Aben, K. K. H. & Kiemeney, L. A. L. M. The global burden of urinary bladder cancer: An update. World J. Urol. 38, \n1895–1904. https:// doi. org/ 10. 1007/ s00345- 019- 02984-4 (2020).\n 3. Deng, S. et al. Global research trends in non-muscle invasive bladder cancer: Bibliometric and visualized analysis. Front. Oncol.  \nhttps:// doi. org/ 10. 3389/ fonc. 2022. 10448 30 (2022).\n 4. Cross, W . & Whelan, P . Bladder cancer. Surgery 28, 599–604. https:// doi. org/ 10. 1016/j. mpsur. 2010. 09. 004 (2010).\n 5. Kamoun, A. et al.  A consensus molecular classification of muscle-invasive bladder cancer. SSRN Electron. J.  https:// doi. org/ 10. \n2139/ ssrn. 33729 65 (2019).\n 6. Liu, H., Xu, Y . & Chen, F . Sketch2Photo: Synthesizing photo-realistic images from sketches via global contexts. Eng. Appl. Artif. \nIntell. https:// doi. org/ 10. 1016/j. engap pai. 2022. 105608 (2023).\n 7. Y ang, Y ., Zou, X., Wang, Y . & Ma, X. Application of deep learning as a noninvasive tool to differentiate muscle-invasive bladder \ncancer and non–muscle-invasive bladder cancer with CT. Eur. J. Radiol. https:// doi. org/ 10. 1016/j. ejrad. 2021. 109666 (2021).\n 8. Chapman-Sung, D. H. et al. Convolutional neural network-based decision support system for bladder cancer staging in CT \nurography: Decision threshold estimation and validation. Med. Imaging 2020: Comput. Aided Diagn. https:// doi. org/ 10. 1117/ 12. \n25513 09 (2020).\n 9. Yin, P . N. et al. Histopathological distinction of non-invasive and invasive bladder cancers using machine learning approaches. \nBMC Med. Inform. Decis. Mak. https:// doi. org/ 10. 1186/ s12911- 020- 01185-z (2020).\n 10. Sarkar, S. et al. Performing automatic identification and staging of urothelial carcinoma in bladder cancer patients using a hybrid \ndeep-machine learning approach. Cancers (Basel) 15, 1–15. https:// doi. org/ 10. 3390/ cance rs150 61673 (2023).\n 11. Liu, D., Wang, S. & Wang, J. The effect of CT high-resolution imaging diagnosis based on deep residual network on the pathology \nof bladder cancer classification and staging. Comput. Methods Programs Biomed. https:// doi. org/ 10. 1016/j. cmpb. 2022. 106635 \n(2022).\n 12. Zhang, G. et al. Deep learning on enhanced CT images can predict the muscular invasiveness of bladder cancer. Front. Oncol.  \nhttps:// doi. org/ 10. 3389/ fonc. 2021. 654685 (2021).\n 13. Yu, Y . et al. Deep transfer learning for modality classification of medical images. Information https:// doi. org/ 10. 3390/ info8 030091 \n(2017).\nTable 5.  Comparison of proposed vision transformers with related works.\nResearch Method Description of dataset Results\nY ang et al.7 CNN 1200 CT scans from bladder patients AUC of 99.70%, accuracy of 93.90%,\nChapman-Sung et al.8 CNN 84 bladder cancer CR urography (CTU) images from 76 patients Accuracy of 91.00%,\nYin et al.9 CNN 1177 bladder scans Accuracy of 84.00%,\nSarkar et al.10 XceptionNet 165 regions of interest Accuracy: 86.07%,\nLiu et al.11 ResNet CT data from 75 bladder patients 94.74% sensitivity\nG. Zhang et al.12 FGP-Net CT data from 183 patients AUC of 0.861, Accuracy of 79.5%\nProposed VIT_B16\nVision Transformer CT scans of 2629 images\nAccuracy 99.24%, AUC = 1\nProposed VIT_B32 Accuracy 99.49%, AUC = 1\n15\nVol.:(0123456789)Scientific Reports |        (2023) 13:20639  | https://doi.org/10.1038/s41598-023-47992-y\nwww.nature.com/scientificreports/\n 14. Tan, M. & Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine \nLearning. 10691–10700 (2019).\n 15. Lee, S. H., Y an, L. C. & Y ang, C. S. LIRNet: A lightweight inception residual convolutional network for solar panel defect \nclassification. Energies 16, 1–12. https:// doi. org/ 10. 3390/ en160 52112 (2023).\n 16. Yu, F ., Xiu, X. & Li, Y . A survey on deep transfer learning and beyond. Mathematics 10, 1–27. https:// doi. org/ 10. 3390/ math1 01936 \n19 (2022).\n 17. Szegedy, C., Ioffe, S., Vanhoucke, V . & Alemi, A. Inception-v4, inception-resnet and the impact of residual connections on learning. \nIn Proceedings of the AAAI Conference on Artificial Intelligence. 31(1), 4278–4284. https:// doi. org/ 10. 1609/ aaai. v31i1. 11231 (2017).\n 18. Siddharth, S., Simone, S. & Anidhya, A. Activation functions in neural networks. Int. J. Eng. Appl. Sci. Technol. 4, 310–316 (2020).\n 19. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 30, 5999–6009 (2017).\n 20. Dosovitskiy, A., et al., An image is worth 16X16 words: Transformers for image recognition at scale, ICLR 2021—9th Int. Conf. \nLearn. Represent. (2021).\n 21. Reedha, R., Dericquebourg, E., Canals, R. & Hafiane, A. Transformer neural network for weed and crop classification of high \nresolution UAV images. Remote Sens. https:// doi. org/ 10. 3390/ rs140 30592 (2022).\nAcknowledgements\nWe express our heartfelt gratitude to the Zagazig University Faculty of Medicine in Egypt for generously \nproviding the bladder dataset, a pivotal contribution that has greatly enriched our research. Their support has \nbeen instrumental in advancing our study in the realm of medical image analysis.\nAuthor contributions\nO.S.K. and M.E.W . wrote the main manuscript text and made the practical part, A.-S.R.A.-A. Prepared the Image \ndataset and E.A.A.-R. reviewed the manuscript.\nFunding\nOpen access funding provided by The Science, Technology & Innovation Funding Authority (STDF) in \ncooperation with The Egyptian Knowledge Bank (EKB).\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to O.S.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7017791271209717
    },
    {
      "name": "Bladder cancer",
      "score": 0.6726312637329102
    },
    {
      "name": "Computer science",
      "score": 0.5086292028427124
    },
    {
      "name": "Machine learning",
      "score": 0.5076998472213745
    },
    {
      "name": "Malignancy",
      "score": 0.4876774549484253
    },
    {
      "name": "Classifier (UML)",
      "score": 0.43060654401779175
    },
    {
      "name": "Transformer",
      "score": 0.4190003275871277
    },
    {
      "name": "Medicine",
      "score": 0.4119333028793335
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.35588812828063965
    },
    {
      "name": "Cancer",
      "score": 0.3030322194099426
    },
    {
      "name": "Pathology",
      "score": 0.25077158212661743
    },
    {
      "name": "Internal medicine",
      "score": 0.14715710282325745
    },
    {
      "name": "Engineering",
      "score": 0.08710026741027832
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114794399",
      "name": "Suez Canal University",
      "country": "EG"
    },
    {
      "id": "https://openalex.org/I192398990",
      "name": "Zagazig University",
      "country": "EG"
    }
  ],
  "cited_by": 16
}