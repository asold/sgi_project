{
  "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing",
  "url": "https://openalex.org/W4389520133",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100701473",
      "name": "Yue Guo",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5069590333",
      "name": "Zian Xu",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5005421447",
      "name": "Yi Yang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3203486519",
    "https://openalex.org/W4385570142",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W4387114267",
    "https://openalex.org/W2954278700",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2787423662",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572697",
    "https://openalex.org/W4380352301",
    "https://openalex.org/W3037252472",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "The emergence of Large Language Models (LLMs), such as ChatGPT, has revolutionized general natural language preprocessing (NLP) tasks. However, their expertise in the financial domain lacks a comprehensive evaluation. To assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval, a framework for Financial Language Model Evaluation, comprising nine datasets designed to evaluate the performance of language models. This study compares the performance of encoder-only language models and the decoder-only language models. Our findings reveal that while some decoder-only LLMs demonstrate notable performance across most financial tasks via zero-shot prompting, they generally lag behind the fine-tuned expert models, especially when dealing with proprietary datasets. We hope this study provides foundation evaluations for continuing efforts to build more advanced LLMs in the financial domain. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 815–821\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nIs ChatGPT a Financial Expert? Evaluating Language Models on Financial\nNatural Language Processing\nYue Guo Zian Xu Yi Yang\nThe Hong Kong University of Science and Technology\nyguoar@connect.ust.hk zxubz@connect.ust.hk imyiyang@ust.hk\nAbstract\nThe emergence of Large Language Models\n(LLMs), such as ChatGPT, has revolutionized\ngeneral natural language preprocessing (NLP)\ntasks. However, their expertise in the finan-\ncial domain lacks a comprehensive evalua-\ntion. To assess the ability of LLMs to solve\nfinancial NLP tasks, we present FinLMEval,\na framework for Financial Language Model\nEvaluation, comprising nine datasets designed\nto evaluate the performance of language mod-\nels. This study compares the performance of\nencoder-only language models and the decoder-\nonly language models. Our findings reveal that\nwhile some decoder-only LLMs demonstrate\nnotable performance across most financial tasks\nvia zero-shot prompting, they generally lag be-\nhind the fine-tuned expert models, especially\nwhen dealing with proprietary datasets. We\nhope this study provides foundation evaluations\nfor continuing efforts to build more advanced\nLLMs in the financial domain.\n1 Introduction\nRecent progress in natural language processing\n(NLP) demonstrates that large language models\n(LLMs), like ChatGPT, achieve impressive results\non various general domain NLP tasks. Those\nLLMs are generally trained by first conducting\nself-supervised training on the unlabeled text (Rad-\nford et al., 2019; Brown et al., 2020; Touvron\net al., 2023a) and then conducting instruction tun-\ning (Wang et al., 2023; Taori et al., 2023) or rein-\nforcement learning from human feedback (RLHF)\n(Ouyang et al., 2022) to let them perform tasks\nfollowing human instructions.\nFinancial NLP, in contrast, demands specialized\nknowledge and specific reasoning skills to tackle\ntasks within the financial domain. However, for\ngeneral language models like ChatGPT, their self-\nsupervised training is performed on the text from\nvarious domains, and the reinforcement learning\nfeedback they receive is generated by non-expert\nworkers. Therefore, how much essential knowledge\nand skills are acquired during the learning process\nremains uncertain. As a result, a comprehensive\ninvestigation is necessary to assess its performance\non financial NLP tasks.\nTo fill this research gap, we are motivated to\nevaluate language models on financial tasks com-\nprehensively. For doing so, we propose a frame-\nwork for Financial Language Model Evaluation\n(FinLMEval). We collected nine datasets on fi-\nnancial tasks, five from public datasets evaluated\nbefore. However, for those public datasets, it is\npossible that their test sets are leaked during the\ntraining process or provided by the model users\nas online feedback. To eliminate this issue, We\nused four proprietary datasets on different financial\ntasks: financial sentiment classification (FinSent),\nenvironmental, social, and corporate governance\nclassification (ESG), forward-looking statements\nclassification (FLS), and question-answering clas-\nsification (QA) for evaluation.\nIn the evaluation benchmark, we evaluate the\nencoder-only language models with supervised\nfine-tuning, with representatives of BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019), FinBERT\n(Yang et al., 2020) and FLANG (Shah et al., 2022).\nWe then compare the encoder-only models with the\ndecoder-only models, with representatives of Chat-\nGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023),\nPIXIU (Xie et al., 2023), LLAMA2-7B (Touvron\net al., 2023b) and Bloomberg-GPT (Wu et al., 2023)\nby zero-shot prompting. Besides, we evaluate the\nefficacy of in-context learning of ChatGPT with\ndifferent in-context sample selection strategies.\nExperiment results show that (1) the fine-tuned\ntask-specific encoder-only model generally per-\nforms better than decoder-only models on the fi-\nnancial tasks, even if decoder-only models have\nmuch larger model size and have gone through\nmore pre-training and instruction tuning or RLHF;\n(2) when the supervised data is insufficient, the\n815\nGet response with Prompt:\nPerform financial sentiment classification. \nClassify the following sentence into \n'neutral',' positive', or 'negative' class. \nOnly provide the label in the output. \n{placeholder}\nThe sentence: Energy-\nRelated Investments \nreported a 277% \nincrease in quarterly \nearnings. The label: \nHere are some examples: \nThe sentence: {example 1}\nThe label: {label 1} …\nThe sentence: {example n}\nThe label: {label n}\nThe sentence: Energy-\nRelated Investments \nreported a 277% increase in \nquarterly earnings.\nThe label: \nEvaluating Encoder-only Models\nExpected output: positive\nEnergy-Related Investments reported a 277% \nincrease in quarterly earnings. \npositive\nFine-tune the parameters of the \nencoder-only models with the training \nsets to output the correct label.\nVS\nEvaluating Decoder-only Models\nZero-shot Learning\nIn-context Learning\nFinancial Tasks\nSentiment Forward-looking \nstatements\nEnvironmental, social,\nand governance\nMonetary \npolicy\nNews\nHeadlines\nNER\nQA\nFinLMEval Framework\nFigure 1: The framework of financial language model evaluation (FinLMEval).\nzero-shot decoder-only models have more advan-\ntages than fine-tuned encoder-only models; (3) the\nperformance gap between fine-tuned encoder-only\nmodels and zero-shot decoder-only models is more\nsignificant on private datasets than the publicly\navailable datasets; (4) in-context learning is only\neffective under certain circumstances.\nTo summarize, we propose an evaluation frame-\nwork for financial language models. Compared to\nprevious benchmarks in the financial domain like\nFLUE (Shah et al., 2022), our evaluation includes\nfour new datasets and involves more advanced\nLLMs like ChatGPT. We show that even the most\nadvanced LLMs still fall behind the fine-tuned ex-\npert models. We hope this study contributes to the\ncontinuing efforts to build more advanced LLMs\nin the financial domain.\n2 Related Works\nThe utilization of language models in financial NLP\nis a thriving research area. While some general do-\nmain language models, like BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019), GPT (Brown\net al., 2020; OpenAI, 2023) and LLAMA (Tou-\nvron et al., 2023a,b) have been applied to finan-\ncial NLP tasks, financial domain models like Fin-\nBERT (Araci, 2019; Yang et al., 2020; Huang et al.,\n2023), FLANG (Shah et al., 2022), PIXIU (Xie\net al., 2023), InvestLM (Yang et al., 2023) and\nBloombergGPT (Wu et al., 2023) are specifically\ndesigned to contain domain expertise and generally\nperform better in financial tasks. Recent work such\nas FLUE (Shah et al., 2022) has been introduced to\nbenchmark those language models in the finance\ndomain. However, the capability of more advanced\nLLMs, like ChatGPT and GPT-4, has not been\nbenchmarked, especially on proprietary datasets.\nIn this work, in addition to the public tasks used\nin FLUE, we newly include four proprietary tasks\nin FinLMEval and conduct comprehensive evalua-\ntions for those financial language models.\n3 Methods\nWe compare two types of models in FinLMEval:\nthe Transformers encoder-only models that require\nfine-tuning on the labeled dataset, and decoder-only\nmodels that are prompted with zero-shot or few-\nshot in-context instructions. Figure 1 provides an\noutline of evaluation methods of FinLMEval.\n3.1 Encoder-only Models\nOur experiments explore the performance of vari-\nous notable encoder-only models: BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019), FinBERT\n(Yang et al., 2020) and FLANG (Shah et al., 2022).\nBERT and RoBERTa are pre-trained on general\ndomain corpora, while FinBERT and FLANG are\npre-trained on a substantial financial domain cor-\npus. We fine-tune the language models on specific\ntasks. Following the fine-tuning process, inference\ncan be performed on the fine-tuned models for spe-\ncific applications.\n816\n# train # test source description\nFinSent 8996 1000 - Financial sentiment classification dataset from analyst reports.\nFPB 2453 1000 (Malo et al., 2014) Sentiment classification dataset from financial news.\nFiQA SA 973 200 (FiQA) Aspect-based financial sentiment analysis.\nESG 3000 1000 - Environmental, social, and corporate governance classification dataset.\nFLS 2600 1000 - Forward-looking statements classification dataset from corporate reports.\nQA 868 200 - Classification on the validity of question-answering pairs.\nHeadlines 9570 1000 (Sinha and Khandait, 2020) Mulitple tasks classification dataset from news headlines.\nNER 14041 1000 (Alvarado et al., 2015) Named entity recognition on financial agreements.\nFOMC 1831 450 (Shah et al., 2023) Hawkish-dovish monetary policy classification from FOMC documents.\nTable 1: The summarization of nine datasets in FinLMEval. FPB, FiQA SA, Headlines, NER and FOMC are from\npublic datasets, and FinSent, ESG, FLS and QA are newly collected and not released before.\n3.2 Decoder-only Models\nWe also evaluate the performance of various pop-\nular decoder-only language models: ChatGPT\n(Ouyang et al., 2022), GPT-4 (OpenAI, 2023),\nPIXIU (Xie et al., 2023), LLAMA2-7B (Touvron\net al., 2023b) and Bloomberg-GPT (Wu et al.,\n2023). ChatGPT and GPT-4, developed by Ope-\nnAI, are two advanced LLMs that showcase ex-\nceptional language understanding and generation\nabilities. The models are pre-trained on a wide\narray of textual data and reinforced by human feed-\nback. PIXIU is a financial LLM based on fine-\ntuning LLAMA (Touvron et al., 2023a) with in-\nstruction data. LLAMA2 is a popular open-sourced\nLLM pre-trained on extensive online data, and\nBloombergGPT is an LLM for finance trained on\na wide range of financial data. As the model size\nof the evaluated decoder-only models is extremely\nlarge, they usually do not require fine-tuning the\nwhole model on downstream tasks. Instead, the\ndecoder-only models provide answers via zero-shot\nand few-shot in-context prompting.\nWe conduct zero-shot prompting for all decoder-\nonly models. We manually write the prompts for\nevery task. An example of prompts for the senti-\nment classification task is provided in Figure 1, and\nthe manual prompts for other tasks are provided\nin Appendix A. Furthermore, to evaluate whether\nfew-shot in-context learning can improve the model\nperformance, we also conduct in-context learning\nexperiments on ChatGPT. We use two strategies\nto select the in-context examples for few-shot in-\ncontext learning: random and similar. The former\nstrategy refers to random selection, and the lat-\nter selects the most similar sentence regarding the\nquery sentence. All in-context examples are se-\nlected from the training set, and one example is\nprovided from each label class.\n4 Datasets\nOur evaluation relies on nine datasets designed\nto evaluate the financial expertise of the models\nfrom diverse perspectives. Table 1 overviews the\nnumber of training and testing samples and the\nsource information for each dataset. Below, we\nprovide an introduction to each of the nine datasets.\nFinSent is a newly collected sentiment classifica-\ntion dataset containing 10,000 manually annotated\nsentences from analyst reports of S&P 500 firms.\nFPB Sentiment Classification (Malo et al.,\n2014) is a classic sentiment dataset of sentences\nfrom financial news. The dataset consists of 4840\nsentences divided by the agreement rate of 5-8 an-\nnotators. We use the subset of 75% agreement.\nFiQA SA (FiQA) is a aspect-based financial sen-\ntiment analysis dataset. Following the \"Sentences\nfor QA-M\" method in (Sun et al., 2019), for each\n(sentence, target, aspect) pair, we transform the\nsentence into the form \"what do you think of the\n{aspect} of {target}? {sentence}\" for classification.\nESG evaluates an organization’s considerations\non environmental, social, and corporate gover-\nnance. We collected 2,000 manually annotated\nsentences from firms’ ESG reports and annual re-\nports.\nFLS, the forward-looking statements, are beliefs\nand opinions about a firm’s future events or re-\nsults. FLS dataset, aiming to classify whether a\nsentence contains forward-looking statements, con-\ntains 3,500 manually annotated sentences from the\nManagement Discussion and Analysis section of\nannual reports of Russell 3000 firms.\nQA contains question-answering pairs extracted\nfrom earnings conference call transcripts. The goal\nof the dataset is to identify whether the answer is\nvalid to the question.\nHeadlines (Sinha and Khandait, 2020) is a\ndataset for the commodity market that analyzes\n817\nDatasets Encoder-only Models Decoder-only Models\nBERT RoBERTa FinBERT FLANG-\nBERT ChatGPT GPT-4 PIXIU LLAMA2-\n7B\nBloomberg-\nGPT\nFinSent 0.841 0.871 0.851 0.849 0.782 0.809 0.800 0.243 -\nFPB 0.914 0.934 0.912 0.881 0.869 0.905 0.965 0.339 0.511\nFiQA SA 0.750 0.875 0.805 0.695 0.898 0.920 0.930 0.480 0.751\nESG 0.931 0.956 0.958 0.925 0.477 0.626 0.509 0.209 -\nFLS 0.875 0.862 0.882 0.861 0.652 0.565 0.275 0.365 -\nQA 0.865 0.825 0.825 0.785 0.695 0.775 0.680 0.625 -\nHeadlines-PDU0.937 0.947 0.956 0.940 0.889 0.878 0.842 0.411 -\nHeadlines-PDC0.978 0.979 0.981 0.978 0.936 0.947 0.702 0.053 -\nHeadlines-PDD0.954 0.961 0.960 0.956 0.896 0.900 0.763 0.382 -\nHeadlines-PI 0.974 0.964 0.976 0.977 0.225 0.105 0.753 0.966 -\nHeadlines-AC 0.996 0.993 0.997 0.995 0.806 0.838 0.902 0.346 -\nHeadlines-FI 0.976 0.964 0.976 0.974 0.711 0.780 0.981 0.048 -\nHeadlines-PS 0.905 0.918 0.924 0.906 0.630 0.811 0.776 0.546 -\nNER 0.980 0.981 0.964 0.978 0.748 0.707 0.749 0.714 0.608\nFOMC 0.587 0.611 0.602 0.602 0.633 0.729 0.522 0.349 -\nAverage 0.897 0.909 0.905 0.907 0.723 0.753 0.739 0.405 -\nTable 2: The results of fine-tuned encoder-only models and zero-shot decoder-only models in 9 financial datasets.\nThe results, except the NER dataset, are measured in micro-F1 score. NER is measured in accuracy. Although some\nzero-shot decoder-only models can achieve considerate results in most cases, the fine-tuned encoder-only models\nusually perform better than decoder-only models.\nnews headlines across multiple dimensions. The\ntasks include the classifications of Price Direction\nUp (PDU), Price Direction Constant (PDC), Price\nDirection Down (PDD), Asset Comparison(AC),\nPast Information (PI), Future Information (FI), and\nPrice Sentiment (PS).\nNER (Alvarado et al., 2015) is a named entity\nrecognition dataset of financial agreements.\nFOMC (Shah et al., 2023) aims to classify the\nstance for the FOMC documents into the tightening\nor the easing of the monetary policy.\nAmong the datasets, FinSent, ESG, FLS, and\nQA are newly collected proprietary datasets.\n5 Experiments\nThis section introduces the experiment setups and\nreports the evaluation results.\n5.1 Model Setups\nEncoder-only models setups. We use the BERT\n(base,uncased), RoBERTa (base), FinBERT (pre-\ntrain), and FLANG-BERT from Huggingface1, and\nthe model fine-tuning is implemented via Trainer 2.\nFor all tasks, we fix the learning rate as 2 × 10−5,\nweight decay as 0.01, and the batch size as 48. We\nrandomly select 10% examples from the training\nset as the validation set for model selection and\n1https://huggingface.co/\n2https://huggingface.co/docs/transformers/\nmain_classes/trainer\nfine-tune the model for three epochs. Other hyper-\nparameters remain the default in Trainer.\nDecoder-only models setups. In the zero-shot\nsetting, for ChatGPT and GPT-4, We use the \"gpt-\n3.5-turbo\" and \"gpt-4\" model API from OpenAI,\nrespectively. We set the temperature and top_p\nas 1, and other hyperparameters default by Ope-\nnAI API. The ChatGPT results are retrieved from\nthe May 2023 version, and the GPT-4 results\nare retrieved in August 2023. For PIXIU and\nLLAMA2, we use the \"ChanceFocus/finma-7b-\nnlp\" and \"meta-llama/Llama-2-7b\" models from\nHuggingface. The model responses are generated\ngreedily. All prompts we used in the zero-shot\nsetting are shown in Appendix A. Besides, as the\nBloombergGPT (Wu et al., 2023) is not publicly\navailable, we directly adopt the results from the\noriginal paper.\nFor in-context learning, we conduct two strate-\ngies for in-context sample selection: random and\nsimilar. We select one example from each label\nwith equal probability weighting for random sam-\nple selection. For similar sample selection, we get\nthe sentence embeddings by SentenceTransformer\n(Reimers and Gurevych, 2019) \"all-MiniLM-L6-\nv2\" model3 and use cosine similarity as the measure\nof similarity. Then, we select the sentences with\nthe highest similarity with the query sentence as the\n3https://www.sbert.net/\n818\nDatasets ChatGPT\nzero ic-ran ic-sim\nFinSent 0.782 0.761 0.761\nFPB 0.869 0.832 0.844\nFiQA SA 0.898 0.891 0.891\nESG 0.477 0.726 0.800\nFLS 0.652 0.673 0.636\nQA 0.695 0.660 0.675\nHeadlines-PDU 0.889 0.839 0.765\nHeadlines-PDC 0.936 0.323 0.413\nHeadlines-PDD 0.896 0.816 0.788\nHeadlines-PI 0.225 0.768 0.844\nHeadlines-AC 0.806 0.576 0.597\nHeadlines-FI 0.711 0.606 0.592\nHeadlines-PS 0.630 0.690 0.729\nNER 0.748 0.784 0.793\nFOMC 0.633 0.672 0.650\nAverage 0.723 0.708 0.719\nTable 3: The results of ChatGPT in zero-shot and in-\ncontext few-shot learning. Zero, ic-ran, and ic-sim repre-\nsent zero-shot learning, in-context learning with random\nsample selection, and in-context learning with similar\nsample selection. The zero-shot and few-shot perfor-\nmances are comparable in most cases.\nin-context examples. The prompts for in-context\nlearning are directly extended from the correspond-\ning zero-shot prompts, with the template shown in\nFigure 1.\n5.2 Main Results\nTable 2 compares the results of the fine-tuned\nencoder-only models and zero-shot decoder-only\nmodels in 9 financial datasets. We have the follow-\ning findings:\nIn 6 out of 9 datasets, fine-tuned encoder-\nonly models can perform better than decoder-\nonly models. The decoder-only models, especially\nthose that have experienced RLHF or instruction-\ntuning, demonstrate considerable performance on\nzero-shot settings on the financial NLP tasks. How-\never, their performance generally falls behind the\nfine-tuned language models, implying that these\nlarge language models still have the potential to im-\nprove their financial expertise. On the other hand,\nfine-tuned models are less effective when the\ntraining examples are insufficient (FiQA SA) or\nimbalanced (FOMC).\nThe performance gaps between fine-tuned\nmodels and zero-shot LLMs are larger on pro-\nprietary datasets than publicly available ones.\nFor example, the FinSent, FPB, and FiQA SA\ndatasets are comparable and all about financial sen-\ntiment classification. However, zero-shot LLMs\nperform the worst on the proprietary dataset Fin-\nSent. The performance gaps between fine-tuned\nmodels and zero-shot LLMs are also more signifi-\ncant on other proprietary datasets (ESG, FLS, and\nQA) than the public dataset.\nTable 3 compares the zero-shot and in-context\nfew-shot learning of ChatGPT. In ChatGPT, the\nzero-shot and few-shot performances are com-\nparable in most cases. When zero-shot prompting\nis ineffective, adding demonstrations can improve\nChatGPT’s performance by clarifying the task, as\nthe results of ESG and Headlines-PI tasks show.\nDemonstrations are ineffective for easy and well-\ndefined tasks, such as sentiment classifications and\nHeadlines (PDU, PDC, PDD, AC, and FI), as the\nzero-shot prompts clearly instruct ChatGPT.\n6 Conclusions\nWe present FinLMEval, an evaluation framework\nfor financial language models. FinLMEval com-\nprises nine datasets from the financial domain, and\nwe conduct the evaluations on various popular lan-\nguage models. Our results show that fine-tuning\nexpert encoder-only models generally perform bet-\nter than the decoder-only LLMs on the financial\nNLP tasks, and adding in-context demonstrations\nbarely improves the results. Our findings sug-\ngest that there remains room for enhancement for\nmore advanced LLMs in the financial NLP field.\nOur study provides foundation evaluations for con-\ntinued progress in developing more sophisticated\nLLMs within the financial sector.\n7 Limitations\nThis paper has several limitations to improve in\nfuture research. First, our evaluation is limited\nto some notable language models, while other ad-\nvanced LLMs may exhibit different performances\nfrom our reported models. Also, as the LLMs keep\nevolving and improving over time, the future ver-\nsions of the evaluated models can have different\nperformance from the reported results. Second,\nFinLMEval only focuses on financial classification\ntasks, and the analysis of the generation ability of\nthe LLMs still needs to be included. Future work\ncan be done toward developing evaluation bench-\nmarks on generation tasks in the financial domain.\n819\nReferences\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timo-\nthy Baldwin. 2015. Domain adaption of named entity\nrecognition to support credit risk assessment. In Pro-\nceedings of the Australasian Language Technology\nAssociation Workshop, ALTA 2015, Parramatta, Aus-\ntralia, December 8 - 9, 2015, pages 84–90. ACL.\nDogu Araci. 2019. Finbert: Financial sentiment\nanalysis with pre-trained language models. CoRR,\nabs/1908.10063.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nFiQA. Financial question answering. https://sites.\ngoogle.com/view/fiqa.\nAllen H Huang, Hui Wang, and Yi Yang. 2023. Finbert:\nA large language model for extracting information\nfrom financial text. Contemporary Accounting Re-\nsearch, 40(2):806–841.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nPekka Malo, Ankur Sinha, Pekka J. Korhonen, Jyrki\nWallenius, and Pyry Takala. 2014. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts. J. Assoc. Inf. Sci. Technol., 65(4):782–796.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nAgam Shah, Suvan Paturi, and Sudheer Chava. 2023.\nTrillion dollar words: A new financial dataset, task &\nmarket analysis. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 6664–6679. Associa-\ntion for Computational Linguistics.\nRaj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah,\nWendi Du, Sudheer Chava, Natraj Raman, Charese\nSmiley, Jiaao Chen, and Diyi Yang. 2022. When\nFLUE meets FLANG: Benchmarks and large pre-\ntrained language model for financial domain. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2322–\n2335, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nAnkur Sinha and Tanmay Khandait. 2020. Impact of\nnews on the commodity market: Dataset and results.\nCoRR, abs/2009.04202.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Uti-\nlizing BERT for aspect-based sentiment analysis via\nconstructing auxiliary sentence. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 380–385. Associa-\ntion for Computational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\n820\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), ACL 2023, Toronto, Canada, July 9-14, 2023,\npages 13484–13508. Association for Computational\nLinguistics.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravol-\nski, Mark Dredze, Sebastian Gehrmann, Prabhanjan\nKambadur, David S. Rosenberg, and Gideon Mann.\n2023. Bloomberggpt: A large language model for\nfinance. CoRR, abs/2303.17564.\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao\nLai, Min Peng, Alejandro Lopez-Lira, and Jimin\nHuang. 2023. PIXIU: A large language model, in-\nstruction data and evaluation benchmark for finance.\nCoRR, abs/2306.05443.\nYi Yang, Yixuan Tang, and Kar Yan Tam. 2023. In-\nvestlm: A large language model for investment using\nfinancial domain instruction tuning. arXiv preprint\narXiv:2309.13064.\nYi Yang, Mark Christopher Siy Uy, and Allen Huang.\n2020. Finbert: A pretrained language model for\nfinancial communications. CoRR, abs/2006.08097.\n821",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6914721131324768
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5315867066383362
    },
    {
      "name": "Finance",
      "score": 0.47464996576309204
    },
    {
      "name": "Language model",
      "score": 0.47207188606262207
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4598349928855896
    },
    {
      "name": "Preprocessor",
      "score": 0.45380547642707825
    },
    {
      "name": "Natural language processing",
      "score": 0.4534187912940979
    },
    {
      "name": "Natural language",
      "score": 0.4350668787956238
    },
    {
      "name": "Machine learning",
      "score": 0.32057803869247437
    },
    {
      "name": "Business",
      "score": 0.09141084551811218
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}