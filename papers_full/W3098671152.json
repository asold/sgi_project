{
    "title": "Human-centric Spatio-Temporal Video Grounding With Visual Transformers",
    "url": "https://openalex.org/W3098671152",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5075863583",
            "name": "Zongheng Tang",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5002061412",
            "name": "Yue Liao",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5100330138",
            "name": "Si Liu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5042965510",
            "name": "Guanbin Li",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A5101828825",
            "name": "Xiaojie Jin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5048928060",
            "name": "Hongxu Jiang",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5101600341",
            "name": "Qian Yu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5082181536",
            "name": "Dong Xu",
            "affiliations": [
                "The University of Sydney"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1596841185",
        "https://openalex.org/W2975501350",
        "https://openalex.org/W2618799552",
        "https://openalex.org/W2251512949",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2970401629",
        "https://openalex.org/W2894280539",
        "https://openalex.org/W2991773160",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W3035590142",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2951323451",
        "https://openalex.org/W2890502146",
        "https://openalex.org/W2963109634",
        "https://openalex.org/W2798354744",
        "https://openalex.org/W2519887557",
        "https://openalex.org/W2611596598",
        "https://openalex.org/W2964345792",
        "https://openalex.org/W2970869018",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3035598501",
        "https://openalex.org/W3047922927",
        "https://openalex.org/W2964089981",
        "https://openalex.org/W2963662190",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2111078031",
        "https://openalex.org/W2897628926",
        "https://openalex.org/W2963017553",
        "https://openalex.org/W2489434015",
        "https://openalex.org/W3108748824",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2962869524",
        "https://openalex.org/W2571175805",
        "https://openalex.org/W2963843782",
        "https://openalex.org/W2969862959",
        "https://openalex.org/W2968356596",
        "https://openalex.org/W3034772468",
        "https://openalex.org/W3034325957",
        "https://openalex.org/W2948162288",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W2799058067",
        "https://openalex.org/W2904824998",
        "https://openalex.org/W2903901502"
    ],
    "abstract": "In this work, we introduce a novel task - Humancentric Spatio-Temporal Video Grounding (HC-STVG). Unlike the existing referring expression tasks in images or videos, by focusing on humans, HC-STVG aims to localize a spatiotemporal tube of the target person from an untrimmed video based on a given textural description. This task is useful, especially for healthcare and security-related applications, where the surveillance videos can be extremely long but only a specific person during a specific period of time is concerned. HC-STVG is a video grounding task that requires both spatial (where) and temporal (when) localization. Unfortunately, the existing grounding methods cannot handle this task well. We tackle this task by proposing an effective baseline method named Spatio-Temporal Grounding with Visual Transformers (STGVT), which utilizes Visual Transformers to extract cross-modal representations for video-sentence matching and temporal localization. To facilitate this task, we also contribute an HC-STVG dataset consisting of 5,660 video-sentence pairs on complex multi-person scenes. Specifically, each video lasts for 20 seconds, pairing with a natural query sentence with an average of 17.25 words. Extensive experiments are conducted on this dataset, demonstrating the newly-proposed method outperforms the existing baseline methods.",
    "full_text": "IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 1\nCopyright @ 2021 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must\nbe obtained from the IEEE by sending an email to pubs-permissions@ieee.org\nHuman-centric Spatio-Temporal Video Grounding\nWith Visual Transformers\nZongheng Tang, Yue Liao, Si Liu*, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, Dong Xu, Fellow, IEEE\nAbstract‚ÄîIn this work, we introduce a novel task ‚Äì Human-\ncentric Spatio-Temporal Video Grounding (HC-STVG). Unlike\nthe existing referring expression tasks in images or videos,\nby focusing on humans, HC-STVG aims to localize a spatio-\ntemporal tube of the target person from an untrimmed video\nbased on a given textural description. This task is useful,\nespecially for healthcare and security related applications, where\nthe surveillance videos can be extremely long but only a speciÔ¨Åc\nperson during a speciÔ¨Åc period is concerned. HC-STVG is a\nvideo grounding task that requires both spatial (where) and tem-\nporal (when) localization. Unfortunately, the existing grounding\nmethods cannot handle this task well. We tackle this task by\nproposing an effective baseline method named Spatio-Temporal\nGrounding with Visual Transformers (STGVT), which utilizes\nVisual Transformers to extract cross-modal representations for\nvideo-sentence matching and temporal localization. To facilitate\nthis task, we also contribute an HC-STVG dataset 1 consisting\nof 5,660 video-sentence pairs on complex multi-person scenes.\nSpeciÔ¨Åcally, each video lasts for 20 seconds, pairing with a\nnatural query sentence with an average of 17.25 words. Extensive\nexperiments are conducted on this dataset, demonstrating that\nthe newly-proposed method outperforms the existing baseline\nmethods.\nIndex Terms ‚ÄîSpatio-Temporal grounding, transformer,\ndataset\nI. I NTRODUCTION\nG\nIVEN natural language queries, a visual grounding task\naims to localize objects or regions in images or videos,\nwhich is an important task in the vision-language research\nÔ¨Åeld. It was originated by localizing objects in an image\nbased on short descriptions or sentences [1], [2] (Fig. 1(a)).\nWith the progress on video understanding, recent efforts have\nbeen made on referring expression in videos, such as temporal\nvideo grounding. Taking Fig. 1(b) for an example, this video\ngrounding task aims to localize a video segment corresponding\nto the given language query.\nSeveral existing tasks are relevant to the newly proposed\nHC-STVG task from the task perspective, but they have a\ndifferent focus. SpeciÔ¨Åcally, Referring Expression aims to\nspatially localize the target object in an image and outputs a\nbounding box. Temporal video grounding aims to determine\nZongheng Tang, Yue Liao, Si Liu, Qian Yu, and Hongxu Jiang are\nwith Beihang University, Beijing, China. Guanbin Li is with the School\nof Computer Science and Engineering, Sun Yat-sen University, China, and\nPazhou Lab, Guangzhou, 510330, China. Xiaojie Jin is with ByteDance AI\nLab, Beijing, China. Dong Xu is with the School of Electrical and Information\nEngineering, the University of Sydney, Sydney, Australia. The Corresponding\nauthor is Si Liu (Email: liusi@buaa.edu.cn).\n1The new dataset is available at https://github.com/tzhhhh123/HC-STVG.\nthe temporal boundaries of the target event in a video and thus\nreturns a set of consecutive frames. Compared with these two\ntasks, the HC-STVG task aims to localize the spatio-temporal\ntube of the target person described in the textual query, which\nis more challenging as it requires localization both spatially\nand temporally. For example, the target person may appear in\nthe video for a long time, but the interval of the event matching\nthe description may be short. It is necessary to utilize both the\ntextual cues (such as actions and relations with other objects)\nindicated in the query sentence and the visual cues shown in\nthe video to determine the temporal boundaries. Furthermore,\ncompared to the STVG task, we only consider humans as\nour referent. This is out of two considerations. First, humans\nare speciÔ¨Åc and always the focus of all species in the videos.\nSecond, humans produce a wealth of action information and\ninteract with other people or things. Therefore, the HC-STVG\ntask requires a model to do Ô¨Åne-grained reasoning, which is\nmore challenging but have more practical applications in the\nreal world.\nGrounding in both space and time domain is often required\nin real-world scenarios, especially for healthcare and security\nrelated applications, where the surveillance videos can be\nextremely long, but people only care about a speciÔ¨Åc person\nand his/her speciÔ¨Åc behaviors. For example, in a 24-hours\nmonitoring video, a healthcare doctor at the nursing home may\nwant to see how a particular older person was having his/her\nlunch, or a police ofÔ¨Åcer may need to search for a suspect\nfrom the crowded people and locate the process when the\nsuspect conducts a crime. In both cases, when the description\nis provided, the expected outputs are a sequence of bounding\nboxes related to the target person in the consecutive frames\ncorresponding to the described action, i.e., a spatio-temporal\ntube. Unfortunately, such a video grounding setting has not\nbeen explored yet. Hence the existing methods cannot well\nhandle this task.\nIn this work, we introduce a new task named Human-centric\nSpatio-Temporal Video Grounding (HC-STVG), which aims\nto localize a spatio-temporal tube of the target person in an\nuntrimmed video given a query description. As shown in Fig.\n1(c), based on the description ‚ÄòThe woman in a white apron\ncomes to the table and puts the tray in her hands on the\ntable‚Äô, an HC-STVG model needs to answer: which person\nis the target and which frames correspond to the described\nactions. To the best of our knowledge, it is the Ô¨Årst video\ngrounding task centered on humans. There are three key\nchallenges in our HC-STVG task: 1) It requires grounding\narXiv:2011.05049v2  [cs.CV]  2 Jun 2021\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 2\nFig. 1. Comparison of different visual grounding tasks. (a) Referring expression in images (e.g., [1], [2]) is a task of localizing objects/regions in images\nbased on a given query. (b) Temporal video grounding aims at localizing the starting and ending frame corresponding to the described action(s) (e.g., [3], [4]).\n(c) Human-centric Spatio-temporal Video Grounding is our newly proposed task, which outputs both spatial and temporal localization. The example is from\nour newly collected HC-STVG dataset. Each example includes a video clip and a corresponding description (top), spatial annotation (i.e., the green boxes),\nand temporal annotations (i.e., the blue bottom line).\nreferring expressions both spatially and temporally at the same\ntime. 2) Multi-modality information, such as visual/textural\nattributes and actions, is often required for localizing a speciÔ¨Åc\nperson, especially in a complex multi-person scene. 3) It can\nbe difÔ¨Åcult to determine the starting and ending frame of an\naction due to its dynamic nature. This task is meaningful as it\ninvolves cross-modality modeling and Ô¨Åne-grained reasoning.\nTo effectively deal with this task, we propose a new base-\nline method, termed Spatio-Temporal Grounding with Visual\nTransformers (STGVT), in which we jointly exploit multi-\nmodality information from videos and textural descriptions by\nusing a visual Transformer. SpeciÔ¨Åcally, our STGVT method\nconsists of four steps: After generating tube proposals from a\nvideo by linking bounding boxes in consecutive frames, our\nmethod learns the cross-modality interaction between the tube\nproposals and the query sentence via a Visual Transformer,\nwhich is then followed by predicting the matching tube and\ntrimming the irrelevant frames. Although there are many\nvision-language datasets for the visual grounding tasks, none\ncan support this new task. To address the issue, we contribute\nan HC-STVG dataset. Precisely, it consists of 5,660 video-\nquery pairs, where 57.2% of scenes have more than three peo-\nple. Each query is a sentence with an average of 17.25 words,\nconsisting of rich expressions related to actions and human-\nobject interaction. Based on this new benchmark dataset, we\ndemonstrate the effectiveness of our baseline method for the\nnewly proposed HC-STVG task.\nTo summary up, the contributions of this work are\nthree-fold: 1) We introduce a novel and challenging task,\ni.e., Human-centric Spatio-Temporal Video Grounding (HC-\nSTVG), which for the Ô¨Årst time, focuses on humans in spatio-\ntemporal video grounding. 2) We build the Ô¨Årst human-centric\nvideo-description dataset, which can be used as the benchmark\ndataset for the new task and inspires the subsequent research.\n3) We propose an effective baseline method for the new task\nand demonstrate the effectiveness of the proposed method\nthrough extensive experiments.\nII. RELATED WORK\nA. Referring Expression in Images/Videos\nReferring Expression aims to localize the visual object\ndescribed by any natural language expression [1], [2], [6]‚Äì\n[12]. The previous works [6] and [11] used a pretrained object\ndetection network or an unsupervised method to generate ob-\nject proposals and then match these regions with the textual de-\nscription to select the most relevant proposal. To better capture\nmulti-modality context information, MAttNet [11] proposed\nto decompose the referring expression into subject, location,\nand relation. Moreover, cross-modality correlation Ô¨Åltering is\nadopted for the referring expression task in real-time [12].\nAs for referring expression in videos, the WSSTG [13] is\nproposed to localize the spatial-temporal tube of an object\nin a trimmed video where the target objects exist through the\nwhole clip.\nB. Visual Transformer\nIn recent years, Visual Transformer methods [14]‚Äì[20] have\nachieved impressive performance in many Vision-Language\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 3\nFig. 2. Examples of our newly collected HC-STVG dataset. (a) For a video clip, we collect its text description (top), bounding boxes (green boxes), and\ntemporal annotation (bottom). (b) An example of our semi-automatic annotation strategy. For each video clip, the volunteers are asked to annotate the yellow\nbounding boxes in three selected keyframes, while the green bounding boxes in the remaining frames are tracked by the method SiamPRN [5].\ntasks, including the Referring Expression tasks. These works\nusually employed a transformer-structure model to extract\nmulti-modal feature representation from image-text or video-\ntext pairs and then predict the regions of interest(ROI) based\non the cross-modal features. The existing transformer-based\nmethods proposed for vision-language tasks can be divided\ninto two categories, the single-stream models and the two-\nstream models. The single-stream models [14]‚Äì[18] employ\na single transformer to learn cross-modal features from the\nvisual and the textual modalities. In contrast, the two-stream\nmodels [19], [20] use different encoders to encode information\nof different modalities while using a co-attention module to\nlearn cross-modal feature representation.\nC. Temporal Video Grounding\nThe goal of temporal video grounding is localizing the most\nrelevant video segment given a query sentence. The previous\nworks [3], [4], [21]‚Äì[23] used a temporal sliding-window-\nbased approach over the video frames to generate temporal\ncandidates and choose the most relevant one as the output.\nChen et al. [24] proposed aggregating the Ô¨Åne-grained frame-\nby-word interaction between videos and queries. Xu et al.\n[23] and Chen et al. [25] proposed to generate query-speciÔ¨Åc\nproposals as candidate segments by directly integrating sen-\ntence information with each Ô¨Åne-grained video clip. Ge et\nal. [26] explored activity concepts in both videos and queries\nfor temporal localization. Zhang et al. [27] iteratively adjusted\nthe structured graph to deal with the semantic misalignment\nproblem. Yuan et al. [28] utilized the Graph Convolutional\nNetwork [29] to model the relations between different video\nclips and proposed a temporal conditioned pointer network\nto screen the answer. Unlike these previous video grounding\nworks, which only deal with temporal localization, the newly\nproposed HC-STVG tackles spatial localization and temporal\nlocalization simultaneously.\nIII. HC-STVG BENCHMARK\nThe newly proposed HC-STVG task aims to localize the\ntarget person spatio-temporally in an untrimmed video. For\nthis task, we collect a new benchmark dataset with spatio-\ntemporal annotations related to the target persons in complex\nmulti-person scenes, together with full interaction and rich\naction information.\nA. Task Formulation\nIn this work, the HC-STVG task is deÔ¨Åned as follows.\nGiven an untrimmed video vœµV and a natural language de-\nscription sœµS , which describes a sequence of actions related to\na speciÔ¨Åc person in v, we aim to generate the spatio-temporal\ntube T (i.e., a sequence of bounding boxes) corresponding to\nthe target person. SpeciÔ¨Åcally, the system needs to locate the\nsegment by determining the starting and ending frames ( l, r)\nand also generates the bounding boxes of the target person in\nthe located segment.\nB. Overview of HC-STVG dataset\nThere are 5,660 video-sentence pairs in our newly collected\nHC-STVG dataset, including 4,500 training pairs and 1,160\ntesting pairs. The average temporal duration of the ground-\ntruth tubes is 5.37 seconds while each sentence has an average\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 4\nof 17.25 words. The duration of videos is further normalized\nto 20 seconds. We ensure that the test samples and the training\nsamples do not originate from the same raw video.\nThe key characteristics of our HC-STVG are summarized\nas follows: (1) human-centric. The dataset contains precise\nspatio-temporal annotations and textual description for the\nperson of interest in each video. (2) All videos are captured\nin complex multi-person scenes, along which 57.2% of the\nvideos have more than three people while the rest videos have\ntwo persons. (3) All sentences have rich descriptions related to\nthe interaction between human-human or human-objects and\n56.1% of the descriptions include both types of interaction.\nFigure 2(a) and Fig. 3 show several examples of our newly\ncollected dataset. As demonstrated in these two Ô¨Ågures, the\nvideos in the new dataset are captured in multi-person scenes.\nBesides, the descriptions have rich expressions related to\nactions and relations.\nC. Dataset Construction\nThe whole process includes Ô¨Åve modules: Raw Video Prepa-\nration, Video Span Selection, Video Description, Bounding\nbox annotation, and Video Span Extension. We explain these\nparts in detail.\nRaw Video Preparation We use raw video Ô¨Åltered by\nA V A dataset [30]. These original videos have been collected\non Youtube and have been carefully selected to ensure the\ndiversity and high-quality of the videos. These videos contain\na lot of realistic scenes, character actions, and character\ninteractions. Therefore, we can produce more suitable video-\ndescription pairs from the videos.\nVideo Span Selection Annotators watch untrimmed videos\nsequentially and follow several rules to mark suitable describ-\nable spans. To be speciÔ¨Åc, the chosen spans must be multi-\nperson scenes, thus increasing the difÔ¨Åculty of grounding in\nour dataset. Shot switching is not allowed in the annotated\nclips because shot switching splits the video content into\ndiscrete paragraphs, bringing semantic incoherence. And We\nlimit the shortest video span time to 3s to avoid annotators\nonly tagging the instantaneous actions of the characters in-\nstead of continuous action sequences. Under these restrictions,\nannotators annotate the precise temporal segment of the target\nperson‚Äôs actions in the scenario.\nVideo Description The annotator who decides the speciÔ¨Åc\nvideo span is required to write down a description of the target\nperson. To collect more diversiÔ¨Åed and natural expressions,\nwe do not provide any templates and predeÔ¨Åned verb lists\nfor annotators to follow. But we still have some guidelines\nfor annotators to make an appropriate textual annotation. (1)\nThe description can spatio-temporally locate the target person\nuniquely, which means that there can not exist another person\nthat satisÔ¨Åes the description and the actions of the target person\nare traceable. (2) We encourage annotators to describe in terms\nof explicit visual attributes, human actions, and relations, as\nour goal is to contain a variety of reasoning information.\nAnd descriptions of other persons‚Äô attributes in the sentence\nare also encouraged to increase diversity. (3) As for actions\ndescriptions, in addition to using some transitive verbs (e.g.\nwalk, turn) for target persons, workers are encouraged to write\nhuman-to-human and human-to-object interactions (e.g. hug,\nfollow) as much as possible, which urges the algorithm to pay\nclose attention to the identiÔ¨Åcation and alignment of dynamic\nvisual relation. (4) The annotators are encouraged to label\nsuccessive events that consist of the target character and related\nactions, which increases the number of verbs in the sentence\nand differs HC-STVG from the mere action localization task.\n(5) Since we only deal with the video images, the annotators\nare required to focus on the visual content rather than the\nsound content. At last, after the annotations, we manually\nreview the descriptions to ensure label quality.\nBounding Box Annotation Frame-level bounding box an-\nnotation is quite crucial for visual grounding tasks to assist\nin the judgment of location correctness. However, manually\nlabeling bounding boxes for each frame in the video may\nlead to enormous labor pressure. To trade off this problem,\nwe adopt the way of manually labeling keyframes and auto-\nmatically labeling other frames with the tracking algorithm.\nConcretely, we use SiamRPN [5] to track the target person.\nWe manually annotate three keyframes of a video span (i.e.,\nstarting, middle, ending frame) and track the other frames,\nwhich effectively outperforms using only one keyframe. We\ntrack the person in the annotated temporal clip from front to\nback and from back to front using keyframes, then we average\nthese two tracking results as an annotation. We conduct an\nadditional manual review to correct the tube for samples with\napparent differences in two direction tracking results.\nVideo Span Extension After the video spans are collected\nand the spatial-temporal annotations are complete, we extend\nthe video clips forward and backward on the temporal ground\ntruth randomly to achieve a Ô¨Åxed total length of time for Ô¨Ånal\nquery videos, and the extended time also serves as a negative\nsample for temporal localization. As localizing descriptions in\nquery videos may be ambiguous after time extension, in HC-\nSTVG, additional annotators review the total video span to\nguarantee there is no unambiguous referring.\nTABLE I\nSTATISTICS OF DIFFERENT DATASETS .\nALS STANDS FOR AVERAGE LENGTH OF SENTENCES . BA AND TA STAND\nFOR BOUNDING BOX ANNOTATION AND TEMPORAL ANNOTATION ,\nRESPECTIVELY . HC STANDS FOR HUMAN -CENTRIC .\nDataset #Queries #Videos ALS BA TA HC\nDiDeMo [4] 40,543 10,464 8.0 √ó ‚úì √ó\nCharadesSTA [3] 16,128 6,670 7.2 √ó ‚úì ‚úì\nTACoS [31] 18,818 7,206 10.5 √ó ‚úì ‚úì\nActivityNet-C [32] 71,942 12,460 14.8 √ó ‚úì ‚úì\nVID-sentence [13] 7,654 5,318 13.2 ‚úì √ó √ó\nVidSTG [33] 44,808 6,924 11.12 ‚úì ‚úì √ó\nHC-STVG Dataset 5,660 5,660 17.25 ‚úì ‚úì ‚úì\nD. Comparison with the Existing Datasets\nWe compare the existing video grounding datasets with our\nnewly collected HC-STVG dataset in Table I. The DiDeMo\ndataset [4] only provides temporal annotation for the tem-\nporal localization task. The TACoS dataset [34], ActivityNet\nCaptions dataset [32], and Charades-STA dataset [3] are all\nhuman-centric datasets as their videos capture human actions\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 5\nFig. 3. An example video-sentence pair in our newly collected dataset. In the example video clip, the target person performs a series of actions. Correspondingly,\nthese actions are described in the sentence, including ‚Äòputs his glasses on the table‚Äô,‚Äòpulls something out of his pocket‚Äô, and ‚Äòputs it on the table‚Äô.\n(e.g., cooking). Similar to DiDeMo dataset, these datasets\nare collected for the temporal localization task, and they\ndo not have annotations at the bounding box level. The\nVID-sentence dataset [13] provides the bounding-box-level\nannotations. However, it only focuses on spatial localization.\nVideo clips in this dataset are all trimmed, hence it is not\nsuitable for temporal localization. Among all datasets, the most\nrelevant dataset is the VidSTG dataset [33]. It is extended\nfrom the dataset Vidor [35], which is a dataset originally\ncollected for detecting relations in videos. The VidSTG dataset\nprovides both bounding-box-level annotations and temporal\nannotations. However, this dataset focuses on the task of\nrelation referring between subjects and objects. In contrast,\nour HC-STVG dataset focuses on localizing the target person\nspatially and temporally from multi-person scenes based on the\nquery sentences. Furthermore, on average the length of each\nsentence in our dataset is the longest among these datasets,\nand the descriptions from this dataset contain rich expressions\nrelated to actions and human-object interaction. The details of\nall these datasets are summarized in Table I.\nIV. METHODOLOGY\nThe overall framework of our STGVT method is shown in\nFig. 4. Given a video v and a query sentence s depicting a\nperson performing a series of actions, our STGVT method\naims to output a spatio-temporal tube T corresponding to\nthe description. The method consists of four steps: 1) it Ô¨Årst\ngenerates the tube proposals (Sec. IV-A). 2) Then a visual\ntransformer takes a pair of the tube proposal and the query\nsentence as the input, and extracts the cross-modal features,\nincluding the global feature and the frame features (Sec. IV-B).\n3) A matching classiÔ¨Åer is adopted to predict a matching score\nfor each tube-sentence pair. The tube proposal with the highest\nmatching score will be proceeded in the Ô¨Ånal step (Sec. IV-C).\n4) Based on the selected tube, a temporal trimming module is\nproposed to trim the irrelevant frames and produce the Ô¨Ånal\nspatio-temporal localization results. (Sec. IV-D).\nIn this section, we Ô¨Årst describe each step of the proposed\nmethod and then introduce the loss functions (Sec. IV-E) and\nthe inference process (Sec. IV-F).\nA. Tube Proposals Generation\nGiven a video, we Ô¨Årst use the region proposal network [36]\nto detect the bounding boxes Bt,i in each frame, where Bt,i\nrepresents the i-th bounding box in the t-th frame. Basically,\nwe follow the ACT method [37] to link the bounding boxes in\nconsecutive frames to form spatio-temporal tube proposals Tp.\nThis process is essential to group the bounding boxes contain-\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 6\nSeg.\nSeg.\nSeg.\nSeg.\nSeg.\n1\n 2\n 3\n 14\n 15\n[CLS]\nThe\nman\nturns\naway\nSeg.\n16\n[SEP]+ + + + + +\n+ + + + + +\nQuery:The man in the vest puts the plate on thetable and turns away\nSegmentEmbedding\nPosition Embedding\nùêµùëúùë•$\n ùêµùëúùë•%\n ùêµùëúùë•&\n ùêµùëúùë•'\n1\n 2\n 3\n 4\n+ + + +\nROIFeature\nLocationEmbedding\nPositionEmbedding\nùêµùëúùë•$(\nùêµùëúùë•$$\n10\n 11+ +\n+ + + + + +\nVisual Transformer\nGlobal Feature\nDetection&Linking\nMatchingScore\n Classification ScoreTube OffsetsClassification ScoreTube Offsets\nFrame FeatureFrame Feature\n(768D) (1024D)\n(1024D)\n(1024D)(768D)\n(768D)\n(1024D) (1024D)(1024D)\nA.\tTubeProposalsGeneration\nB.\tCross-modal\tRepresentation\nC.\tTube-description\tMatchingD.\tTube\tTrimming\nTokenEmbedding\nFig. 4. The framework of our STGVT. The framework has four modules, which correspond to four steps explained in Methodology. SpeciÔ¨Åcally, given a\nquery sentence and video, it Ô¨Årst generates tube proposals (A), and then each tube-description pair is fed into the visual transformer (B). The output feature\nof the visual transformer is used for predicting matching score, classiÔ¨Åcation score, tube offsets , which to be used for tube selection (C) and tube trimming\n(D).\ning similar visual content in adjacent frames. Different from\nthe method [37], we compute the similarity score between two\nbounding boxes (i.e., Bt,i and Bt+1,j) as follow:\nS(Bt,i,Bt+1,j) =Œªi ‚àóIoU(Bt,i,Bt+1,j)\n+Œªc ‚àóDcosine(f(Bt,i),f(Bt+1,j))\n+œÜ(Bt,i) +œÜ(Bt+1,j),\n(1)\nwhere f(¬∑) is a pretrained feature extractor, Dcosine(¬∑,¬∑) is\nthe cosine function, œÜis the conÔ¨Ådence score of the bounding\nboxes, and Œªi,Œªc are the weights of the IoU score and\nthe cosine score. In practice, we set them as 0.7 and 0.3\nrespectively.\nB. Cross-modal Representation\nThe next step is to extract cross-modal features for each\npair of the tube proposal Tp and the query description s.\nWe adopt Visual Transformer in this step due to its great\nsuccess for the cross-modal tasks [19]. The visual transformer\ntakes the visual and textual inputs separately and models\ntheir interaction through a set of transformer layers based on\nthe co-attention mechanism. SpeciÔ¨Åcally, following [38], we\nuse the position information, token information, and segment\nembedding extracted from the query description s as the\ntextual inputs; while each temporal position, visual feature,\nand spatial location from tube proposal Tp are used as the\nvisual inputs for each tube proposal Tp. The temporal position\nrefers to the frame index, the visual features are extracted from\nthe bounding box in each frame ofTp, and the bounding boxes‚Äô\ncoordinates are encoded as the location embedding.\nThe visual transformer will output two cross-modal features\nfglobal and fframe . When using the textual inputs as the\nQuery and the visual inputs as the Key and Value, fglobal\np\n(p indicates the p-th tube-sentence pair) is the element-wise\nproduct of the visual feature and textual feature, which are\nextracted from the Ô¨Årst token position. When the visual inputs\nare used as Query and the textual inputs are used as Key\nand Value, the model generates the feature fframe\np,t for each\nbounding box in each frame of Tp (p and t refer to the t-th\nframe of the p-th tube). These two features are used to predict\nthe matching scores of Tp at the tube-level and the frame-level,\nwhich to be explained below.\nC. Tube-description Matching\nIn this step, our method will verify if the input tube proposal\nTp is matching with the query description s. As mentioned\nabove, the visual transformer extracts the global feature fglobal\np\nfor each pair of Tp and s. We feed fglobal\np into a binary\nclassiÔ¨Åer to predict its matching score M‚Ä≤\np ‚àà[0,1]. Tp with\nthe highest matching score will be selected and fed into the\nlast step.\nWhen training the classiÔ¨Åer, it is worth noting that the\npositive and negative samples can be imbalanced if we only\ntreat the ground-truth tube, TGT , as the positive sample. To\ntackle this problem, we relax the criteria for the positive\nsamples by introducing two scores, soverlap and sIoU . The\ntube proposals which simultaneously satisfy the following two\nconditions, soverlap >= 0.9 and sIoU >0.5, are treated as the\npositive samples. SpeciÔ¨Åcally, (1) soverlap is deÔ¨Åned as follow:\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 7\nsoverlap = |Tp ‚à©TGT |\n|TGT | , (2)\n|TGT |refers to the total number of frames in the ground-truth\ntube. This score reÔ¨Çects the ratio of the total number of the\nintersected frames between Tp and TGT over all frames in\nTGT. 2) The average IoU score sIoU is deÔ¨Åned as follow:\nsIoU = 1\n|Tp ‚à©TGT |\n‚àë\nt‚ààTp‚à©TGT\nIoU (B‚Ä≤\nt,Bt), (3)\nwhere B‚Ä≤\nt and Bt are the detected/ground-truth bounding boxes\nof Tp and TGT at frame t, respectively. A tube is treated as a\nnegative sample when its sIoU is lower than 0.2.\nD. Tube Trimming\nGiven the dynamic characteristics of actions, the selected\ntube proposal Tp may contain redundant transition frames.\nHence it is required to trim Tp to output the Ô¨Ånal predicted\ntube T. We compute the relevance score of each frame in Tp to\nthe query s, and predict the regression offsets. Our trimming\nmodule consists of two subnets, a classiÔ¨Åcation subnet and a\nboundary regression subnet, which will be detailed below.\n1) ClassiÔ¨Åcation Subnet\nThe classiÔ¨Åcation subset is used to predict if a video\nframe should be kept in the Ô¨Ånal tube ‚Äì the spatio-temporal\nlocalization result T, or not. As introduced in Sec. IV-B,\nthe transformer outputs fframe\np,t for each frame of Tp. The\nclassiÔ¨Åer takes fframe\np,t as the input to predict a relevance score\nC‚Ä≤\np,t indicating how relevant the t-th frame is to the query\nsentence s.\n2) Boundary Regression Subnet\nBesides the relevance score C‚Ä≤\np,t, we also adopt a network\nto predict the temporal offsets, which reÔ¨Çects how far away the\ncurrent frame is from the ground-truth temporal boundary. This\nsubset‚Äôs architecture is the same as the classiÔ¨Åcation subset\nexcept that it has two outputs. SpeciÔ¨Åcally, for a positive frame\nat the t-th position, if the ground-truth tube spans from the l-\nth frame to the r-th frame (i.e., RGT = [l,r]), the regression\ntarget is Op,t = (Œ¥l,Œ¥r),\nŒ¥l = t‚àíl\nN Œ¥r = r‚àít\nN , (4)\nwhere N is the number of frames in tube Tp. Œ¥l and Œ¥r refer\nto the temporal offset from the t-th frame to the left and the\nright boundary, respectively.\nE. Loss Functions\nGiven the tube-level matching prediction score M‚Ä≤\np, the\nframe-level relevance prediction score C‚Ä≤\np,t, and boundary\nregression prediction result O‚Ä≤\np,t, and their corresponding\nground-truth labels Mp, Cp,t, Op,t, the total loss is deÔ¨Åned\nas follows,\nL=\n‚àë\np\n{Œª1 ‚àóLmatch\n(\nM‚Ä≤\np,Mp\n)\n+Œª2 ‚àóI{Mp=1}\n1\nN\n‚àë\nt\nLcls\n(\nC‚Ä≤\np,t,Cp,t\n)\n+Œª3 ‚àóI{Mp=1,Ct=1}\n1\nNpos\n‚àë\nt\nLreg\n(\nO‚Ä≤\np,t,Op,t\n)\n}\n(5)\nwhere Lmatch is the cross-entropy loss for the tube-description\nmatching module, Lcls and Lreg is a cross-entropy loss and\nIoU loss (i.e., ‚àíln(\nO‚Ä≤\np,t‚à©Op,t\nO‚Ä≤\np,t‚à™Op,t\n)) for the classiÔ¨Åcation subset\nand the boundary regression subset, respectively. N and Npos\ndenote the number of frames and the positive frames in the\ntube Tp. It is worth noting that, except for the tube proposals\ngeneration network, the rest of the proposed modules can be\ntrained in an end-to-end fashion. However, the loss Lcls and\nLreg in Eq. 5 are only computed for the positive tubes (i.e.,\nI(Mp=1)) and positive frames ( I(Mp=1,Cp,t=1)).\nF . Inference\nDuring the inference process, given a query sentence sand\na video v, our proposed method: (1) Ô¨Årst generates P tube\nproposals Tp (p = 1,2,...,P ). Each tube proposal Tp and\nthe query sentence s form a pair, which will be fed into the\nvisual transformer. (2) For each pair of Tp and s, the visual\ntransformer extracts cross-modal features, fglobal\np and fframe\np,t .\n(3) The tube-description matching module takes the global\nfeature fglobal\np as the input and predicts a matching score M‚Ä≤\np.\nThe tube with the highest matching score M‚Ä≤\np is selected and\nfurther trimmed. (4) For all frames in the selected tube Tp,\nthe classiÔ¨Åcation subnet and the regression subnet take the\nframe feature fframe\np,t as the input, and output the classiÔ¨Åcation\nscore C‚Ä≤\np,t and O‚Ä≤\np,t = (Œ¥l,Œ¥r) for each frame. We follow the\nmethod DEBUG [39] to determine the temporal boundary of\nthe selected tube. SpeciÔ¨Åcally, we start from the frame with\nthe highest C‚Ä≤\np,t. Given its predicted regression offsets O‚Ä≤\np,t =\n(Œ¥l,Œ¥r), we can produce an initial range Rinit = (t‚àíŒ¥l‚àóN,t+\nŒ¥r ‚àóN). Next, we compute Rt for the remaining N‚àí1 frames.\nIf the predicted range of the t-th frame, Rt, has overlap with\nRinit, then Rt is merged with Rinit. This process is repeated\nfor all frames whose C‚Ä≤\np,t higher than a pre-deÔ¨Åned threshold\nœµ, which will produce the Ô¨Ånal predicted tube.\nV. EXPERIMENTS\nA. Evaluation Metrics & Implementation Details\nThe main evaluation metric for HC-STVG is vIoU, and it\nis deÔ¨Åned as follow:\nvIoU = 1\n|Tunion|\n‚àë\nt‚ààTinter\nIoU (B‚Ä≤\nt,Bt), (6)\nwhere Tinter = Tp ‚à©TGT refers to the intersected frames\nbetween the predicted tube Tp and the ground-truth (GT)\ntube TGT , Tunion = Tp ‚à™TGT represents the union set of\nthe predicted frames and the GT frames, B‚Ä≤\nt and Bt are\nthe predicted bounding box and the ground truth bounding\nbox at frame t. vIoU reÔ¨Çects the accuracy of the predicted\nspatio-temporal tubes. The vIoU@perc. score stands for the\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 8\npercentage of predicted tubes whose vIoU is larger than perc.\nand mvIoU is the average of vIoUs score over the whole test\nset.\nDuring the inference process, we sample one frame from\nevery six frames of the tube before feeding the sampled frames\ninto the visual transformer in order to reduce redundancy. The\nmaximum number of bounding boxes detected in each image\nis set as 101. We use Mask R-CNN pretrained on the Visual\nGenome dataset [40] to extract the visual features from each\nbounding box. Every query sentence s is truncated or Ô¨Ålled\nto the maximum length of 40 words. The loss weights Œª1,\nŒª2, Œª3 are empirically set as 1, 1 and 2, respectively. During\nthe training process, we use Adam optimizer [41]. The initial\nlearning rate and batch size are empirically set as 2e-5 and\n32, respectively.\nTABLE II\nPERFORMANCE (%) OVER m vIoU , vIoU @0.3 AND vIoU @0.5 ON THE\nVIDSTG D ATASET OF THE DECLARATIVE SENTENCE .\nMethod Declarative Sentence\nm vIoU vIoU@0.3 vIoU@0.5\nRandom 0.69% 0.04% 0.01%\nGroundeR [42] + TALL [3] 9.78% 11.04% 4.09%\nSTPR [43] + TALL [3] 10.40% 12.38% 4.27%\nWSSTG + TALL [3] 11.36% 14.63% 5.91%\nGroundeR [42] + L-Net [44] 11.89% 15.32% 5.45%\nSTPR [43] + L-Net [44] 12.93% 16.27% 5.68%\nWSSTG [13] + L-Net [44] 14.45% 18.00% 7.89%\nSTGRN [33] 19.75% 25.77% 14.60%\nSTGVT 21.62% 29.80% 18.94%\nTABLE III\nPERFORMANCE (%) OF DIFFERENT METHODS ON THE HC-STVG\nDATASET IN TERM OF m vIoU , vIoU @0.3 AND vIoU @0.5\nMethod m vIoU vIoU @0.3 vIoU @0.5\nRandom 0.71% 0.03% 0%\nTALL [3]+WSSTG [13] 13.37% 19.95% 7.33%\n2D-TAN [45]+WSSTG [13] 15.43% 19.83% 6.81%\nSTGVT (w/o trimming) 16.93% 21.29% 6.64%\nSTGVT (w/o pretraining) 17.46% 25.09% 7.76%\nSTGVT 18.15% 26.81% 9.48%\nB. Baseline Methods\nConsidering that the existing methods cannot be directly\napplied for the human-centric spatio-temporal video ground-\ning task, we form two baselines by combining the existing\nmethods on video grounding. SpeciÔ¨Åcally, TALL [3] and 2D-\nTAN [45] are the two methods for temporal video localiza-\ntion, while WSSTG [13] method is proposed for localizing\nspatio-temporal tubes in trimmed videos. So we Ô¨Årst apply\nTALL or 2D-TAN to produce a temporary segment from each\nuntrimmed video based on each given sentence, and then\nutilize the WSSTG method to localize spatio-temporal tubes\nin the generated segment. The Random method localizes the\ntemporal segment and spatial regions randomly. It is worth\nnoting that we replaced the tube-description matching module\nwith the WSSTG method, while keeping the entire training\nprocess fully supervised. For a fair comparison, we the same\nstrategy for all these methods to generate the tube proposals,\nas introduced in Section IV-A.\nC. Experimental Results\nWe report the results of our STGVT method and the baseline\nmethods in Table III. One can observe that our STGVT method\noutperforms all baseline methods. Our method achieves the\nmvIou score of 18.15%, which outperforms baseline methods\nby a signiÔ¨Åcant margin. Our method also surpasses the base-\nlines in terms of vIoU@0.3 and vIoU@0.5.\nTo further verify the effectiveness of our method, we also\nconducted experiments on VidSTG [33] dataset. VidSTG is a\nsuitable dataset for Spatio-Temporal Video Grounding task,\nbut it is not human-centric. Thus we considered all the\ncategories in the library when generating tubes, and use the\nsame method to select the target tube and trim it. Experimental\nresults in Table II. show that our method is also effective on\ndifferent databases.\nD. Ablation Study\n1) Effectiveness of Tube-description Matching Module\nIn our proposed method, the tube-description matching\nmodule outputs a raw prediction of the spatio-temporal tube,\nwhich will further be trimmed by the tube trimming module.\nWe evaluate the quality of the raw predictions by comparing\nour method with the WSSTG [13] method in Table IV. It is\nworth noting that (1) we directly use the tube without any\ntemporal trimming as the Ô¨Ånal prediction, (2) both ours and\nWSSTG use the same method for generating tube proposals.\nTABLE IV\nTUBE MATCHING ACCURACY RESULTS ON THE HC-STVG DATASET\nMethod m vIoU vIoU @0.3 vIoU @0.5\nRandom 2.40% 1.72% 0.44%\nWSSTG [13] 12.96% 16.23% 4.35%\nSTGVT 16.93% 21.29% 6.64%\n2) Effectiveness of Tube Trimming Module\nIn this section, we demonstrate the effectiveness of the\ntube trimming module in our proposed method. As shown in\nTable III, our full method ( STGVT) signiÔ¨Åcantly outperforms\nthe simpliÔ¨Åed version without the tube trimming module\n(termed as STGVT (w/o trimming)). The performance improve-\nment of our STGVT over STGVT (w/o trimming) indicates\nthe effectiveness of the tube trimming module. However, it\nis worth noting that, even without using the tube trimming\nmodule, our simpliÔ¨Åed version STGVT (w/o trimming) method\nstill achieves better performance compared with other baseline\nmethods. Besides, we compared the results of the trimmed\npositive tube, as shown in Table V, the result of tube selection\nwe chose signiÔ¨Åcantly outperforms the baseline method.\nTABLE V\nTEMPORAL GROUNDING PERFORMANCE (%) OVER m tIoU AND\nm vIoU\nMethod m vIoU m tIoU\nRandom 0.71% 10.50%\nTALL [3] +Tubepos 31.64% 44.31%\n2D-TAN [45]+Tubepos 32.89% 46.20%\nSTGVT+Tubepos 34.71% 48.64%\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 9\nFig. 5. Representative results produced by our method and the baseline method (WSSTG+TALL). In the 2nd and the 5th examples, our method achieves\nbetter temporal localization results when compared with the baseline method. While in the 1st, the 3rd, and the 5th examples, our method achieves more\naccurate spatial localization results.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 10\n3) Effectiveness of the Pretraining Strategy\nSince we adopt the visual transformer pretrained based on\nthe Conceptual Captions dataset [46], we wonder whether the\nperformance improvement is due to the pretraining strategy.\nSo we perform another experiment by training the visual\ntransformer from scratch 2 (and our method is referred as\nSTGVT (w/o pretraining) . As shown in Table III, we observe\nthat: 1) our method STGVT (w/o pretraining) outperforms\nexisting baseline methods; 2) using the pretraining strategy\ncan improve the performance.\nE. Qualitative Analysis\nWe provide some visualization results in Fig. 5. These\nexamples show that our method outperforms the baseline\nmethod for video grounding both spatially and temporally.\nSpeciÔ¨Åcally, in most examples (except the 3rd one), there are\nmore overlap between our temporal tube prediction results\n(see the yellow line) and the ground-truth (see the green\nline) when compared with the baseline method TALL [3] +\nWSSTG [13](see the red line), which indicates our method\ncan localize the video segment more precisely. In the 3rd\nexample, both our method and the baseline method achieve\ngood performance in temporal grounding, but our method\ngenerates more accurate bounding boxes.\nVI. C ONCLUSION\nIn this work, we have introduced a novel task Human-\ncentric Spatio-Temporal Video Grounding (HC-STVG).\nFurthermore, we have contributed a new HC-STVG dataset\nwith rich spatio-temporal tube annotations for videos and the\ncorresponding descriptive sentences. We have also proposed\na baseline method named STGVT, which takes advantage\nof the Visual Transformers to tackle the HC-STVG task.\nComprehensive experiments have demonstrated that our\nmethod outperforms the existing grounding methods for the\nHC-STVG task.\nAcknowledgement: This research is supported in part by the\nNational Key Research and Development Project of China\n(No. 2018AAA0101900), the National Natural Science Foun-\ndation of China (Grant 61876177), Beijing Natural Science\nFoundation (4202034), the Guangdong Basic and Applied\nBasic Research Foundation (Grant No.2020B1515020048),\nFundamental Research Funds for the Central Universities,\nZhejiang Lab (No. 2019KD0AB04).\nREFERENCES\n[1] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, ‚ÄúModeling\ncontext in referring expressions,‚Äù in European Conference on Computer\nVision. Springer, 2016, pp. 69‚Äì85. 1, 2\n[2] S. Kazemzadeh, V . Ordonez, M. Matten, and T. Berg, ‚ÄúReferitgame:\nReferring to objects in photographs of natural scenes,‚Äù in Proceedings\nof the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), 2014, pp. 787‚Äì798. 1, 2\n[3] J. Gao, C. Sun, Z. Yang, and R. Nevatia, ‚ÄúTall: Temporal activity local-\nization via language query,‚Äù in Proceedings of the IEEE International\nConference on Computer Vision , 2017, pp. 5267‚Äì5275. 2, 3, 4, 8, 10\n2The visual transformer has two branches, one for textual feature while the\nother for visual feature. All methods except for the baseline method ‚ÄòRandom‚Äô\nuse a pretrained transformer for the textual branch.\n[4] L. Anne Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and\nB. Russell, ‚ÄúLocalizing moments in video with natural language,‚Äù in\nProceedings of the IEEE international conference on computer vision ,\n2017, pp. 5803‚Äì5812. 2, 3, 4\n[5] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, ‚ÄúHigh performance visual\ntracking with siamese region proposal network,‚Äù in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition , 2018,\npp. 8971‚Äì8980. 3, 4\n[6] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy,\n‚ÄúGeneration and comprehension of unambiguous object descriptions,‚Äù\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 11‚Äì20. 2\n[7] S. Huang, S. Liu, T. Hui, J. Han, B. Li, J. Feng, and S. Yan,\n‚ÄúOrdnet: Capturing omni-range dependencies for scene parsing,‚Äù IEEE\nTransactions on Image Processing , vol. 29, pp. 8251‚Äì8263, 2020. 2\n[8] S. Huang, T. Hui, S. Liu, G. Li, Y . Wei, J. Han, L. Liu, and B. Li, ‚ÄúRefer-\nring image segmentation via cross-modal progressive comprehension,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 10 488‚Äì10 497. 2\n[9] T. Hui, S. Liu, S. Huang, G. Li, S. Yu, F. Zhang, and J. Han, ‚ÄúLinguistic\nstructure guided context modeling for referring image segmentation,‚Äù\narXiv preprint arXiv:2010.00515 , 2020. 2\n[10] Y . Liao, S. Liu, F. Wang, Y . Chen, C. Qian, and J. Feng, ‚ÄúPpdm: Parallel\npoint detection and matching for real-time human-object interaction\ndetection,‚Äù in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , June 2020. 2\n[11] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L.\nBerg, ‚ÄúMattnet: Modular attention network for referring expression\ncomprehension,‚Äù in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2018, pp. 1307‚Äì1315. 2\n[12] Y . Liao, S. Liu, G. Li, F. Wang, Y . Chen, C. Qian, and B. Li, ‚ÄúA real-\ntime cross-modality correlation Ô¨Åltering method for referring expression\ncomprehension,‚Äù arXiv preprint arXiv:1909.07072 , 2019. 2\n[13] Z. Chen, L. Ma, W. Luo, and K.-Y . K. Wong, ‚ÄúWeakly-supervised\nspatio-temporally grounding natural sentence in video,‚Äù arXiv preprint\narXiv:1906.02549, 2019. 2, 4, 5, 8, 10\n[14] C. Alberti, J. Ling, M. Collins, and D. Reitter, ‚ÄúFusion of detected\nobjects in text for visual question answering,‚Äù arXiv: Computation and\nLanguage, 2019. 2, 3\n[15] Y . Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y . Cheng, and\nJ. Liu, ‚ÄúUniter: Learning universal image-text representations,‚Äù arXiv:\nComputer Vision and Pattern Recognition , 2019. 2, 3\n[16] G. Li, N. Duan, Y . Fang, M. Gong, D. Jiang, and M. Zhou, ‚ÄúUnicoder-\nvl: A universal encoder for vision and language by cross-modal pre-\ntraining.‚Äù in AAAI, 2020, pp. 11 336‚Äì11 344. 2, 3\n[17] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, ‚ÄúVisualbert:\nA simple and performant baseline for vision and language,‚Äù arXiv\npreprint arXiv:1908.03557, 2019. 2, 3\n[18] W. Su, X. Zhu, Y . Cao, B. Li, L. Lu, F. Wei, and J. Dai, ‚ÄúVl-bert:\nPre-training of generic visual-linguistic representations,‚Äù arXiv preprint\narXiv:1908.08530, 2019. 2, 3\n[19] J. Lu, D. Batra, D. Parikh, and S. Lee, ‚ÄúVilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language tasks,‚Äù\nin Advances in Neural Information Processing Systems , 2019, pp. 13‚Äì\n23. 2, 3, 6\n[20] H. Tan and M. Bansal, ‚ÄúLxmert: Learning cross-modality encoder\nrepresentations from transformers,‚Äù arXiv preprint arXiv:1908.07490 ,\n2019. 2, 3\n[21] M. Liu, X. Wang, L. Nie, X. He, B. Chen, and T.-S. Chua, ‚ÄúAttentive\nmoment retrieval in videos,‚Äù in The 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval, 2018,\npp. 15‚Äì24. 3\n[22] M. Liu, X. Wang, L. Nie, Q. Tian, B. Chen, and T.-S. Chua, ‚ÄúCross-\nmodal moment localization in videos,‚Äù in Proceedings of the 26th ACM\ninternational conference on Multimedia , 2018, pp. 843‚Äì851. 3\n[23] H. Xu, K. He, B. A. Plummer, L. Sigal, S. Sclaroff, and K. Saenko,\n‚ÄúMultilevel language and vision integration for text-to-clip retrieval,‚Äù in\nProceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , vol. 33,\n2019, pp. 9062‚Äì9069. 3\n[24] J. Chen, X. Chen, L. Ma, Z. Jie, and T.-S. Chua, ‚ÄúTemporally grounding\nnatural sentence in video,‚Äù in Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, 2018, pp. 162‚Äì171.\n3\n[25] S. Chen and Y .-G. Jiang, ‚ÄúSemantic proposal for activity localization in\nvideos via sentence query,‚Äù in Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence, vol. 33, 2019, pp. 8199‚Äì8206. 3\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 11\n[26] R. Ge, J. Gao, K. Chen, and R. Nevatia, ‚ÄúMac: Mining activity\nconcepts for language-based temporal localization,‚Äù in2019 IEEE Winter\nConference on Applications of Computer Vision (WACV) . IEEE, 2019,\npp. 245‚Äì253. 3\n[27] D. Zhang, X. Dai, X. Wang, Y .-F. Wang, and L. S. Davis, ‚ÄúMan: Moment\nalignment network for natural language moment retrieval via iterative\ngraph adjustment,‚Äù in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2019, pp. 1247‚Äì1257. 3\n[28] Y . Yuan, L. Ma, and W. Zhu, ‚ÄúSentence speciÔ¨Åed dynamic video\nthumbnail generation,‚Äù in Proceedings of the 27th ACM International\nConference on Multimedia , 2019, pp. 2332‚Äì2340. 3\n[29] T. N. Kipf and M. Welling, ‚ÄúSemi-supervised classiÔ¨Åcation with graph\nconvolutional networks,‚Äù arXiv preprint arXiv:1609.02907 , 2016. 3\n[30] C. Gu, C. Sun, D. A. Ross, C. V ondrick, C. Pantofaru, Y . Li, S. Vi-\njayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar, C. Schmid, and\nJ. Malik, ‚ÄúA V A: A video dataset of spatio-temporally localized atomic\nvisual actions,‚Äù in IEEE Conference on Computer Vision and Pattern\nRecognition. IEEE Computer Society, 2018. 4\n[31] A. Rohrbach, M. Rohrbach, W. Qiu, A. Friedrich, M. Pinkal, and\nB. Schiele, ‚ÄúCoherent multi-sentence video description with variable\nlevel of detail,‚Äù in German conference on pattern recognition. Springer,\n2014, pp. 184‚Äì195. 4\n[32] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles, ‚ÄúDense-\ncaptioning events in videos,‚Äù in Proceedings of the IEEE international\nconference on computer vision , 2017, pp. 706‚Äì715. 4\n[33] Z. Zhang, Z. Zhao, Y . Zhao, Q. Wang, H. Liu, and L. Gao, ‚ÄúWhere does\nit exist: Spatio-temporal video grounding for multi-form sentences,‚Äù in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2020. 4, 5, 8\n[34] M. Regneri, M. Rohrbach, D. Wetzel, S. Thater, B. Schiele, and\nM. Pinkal, ‚ÄúGrounding action descriptions in videos,‚Äù Trans. Assoc.\nComput. Linguistics, vol. 1, pp. 25‚Äì36, 2013. 4\n[35] X. Shang, D. Di, J. Xiao, Y . Cao, and T. S. Chua, ‚ÄúAnnotating objects\nand relations in user-generated videos,‚Äù in the 2019, 2019. 5\n[36] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-time\nobject detection with region proposal networks,‚Äù in Advances in neural\ninformation processing systems , 2015, pp. 91‚Äì99. 5\n[37] V . Kalogeiton, P. Weinzaepfel, V . Ferrari, and C. Schmid, ‚ÄúAction tubelet\ndetector for spatio-temporal action localization,‚Äù in Proceedings of the\nIEEE International Conference on Computer Vision , 2017, pp. 4405‚Äì\n4413. 5, 6\n[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training\nof deep bidirectional transformers for language understanding,‚Äù arXiv\npreprint arXiv:1810.04805, 2018. 6\n[39] C. Lu, L. Chen, C. Tan, X. Li, and J. Xiao, ‚ÄúDebug: A dense bottom-\nup grounding approach for natural language video localization,‚Äù in\nProceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), 2019, pp. 5147‚Äì5156.\n7\n[40] R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY . Kalantidis, L.-J. Li, D. A. Shammaet al., ‚ÄúVisual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations,‚Äù\nInternational journal of computer vision , vol. 123, no. 1, pp. 32‚Äì73,\n2017. 8\n[41] J. Duchi, E. Hazan, and Y . Singer, ‚ÄúAdaptive subgradient methods\nfor online learning and stochastic optimization.‚Äù Journal of machine\nlearning research, vol. 12, no. 7, 2011. 8\n[42] L. Yu, H. Tan, M. Bansal, and T. L. Berg, ‚ÄúA joint speaker-listener-\nreinforcer model for referring expressions,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2017, pp.\n7282‚Äì7290. 8\n[43] M. Yamaguchi, K. Saito, Y . Ushiku, and T. Harada, ‚ÄúSpatio-temporal\nperson retrieval via natural language queries,‚Äù in Proceedings of the\nIEEE International Conference on Computer Vision , 2017, pp. 1453‚Äì\n1462. 8\n[44] J. Chen, L. Ma, X. Chen, Z. Jie, and J. Luo, ‚ÄúLocalizing natural\nlanguage in videos,‚Äù in Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, vol. 33, 2019, pp. 8175‚Äì8182. 8\n[45] S. Zhang, H. Peng, J. Fu, and J. Luo, ‚ÄúLearning 2d temporal adjacent\nnetworks for moment localization with natural language,‚Äù arXiv preprint\narXiv:1912.03590, 2019. 8\n[46] P. Sharma, N. Ding, S. Goodman, and R. Soricut, ‚ÄúConceptual captions:\nA cleaned, hypernymed, image alt-text dataset for automatic image cap-\ntioning,‚Äù in Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), 2018, pp. 2556‚Äì\n2565. 10"
}