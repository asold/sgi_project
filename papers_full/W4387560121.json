{
  "title": "Integrating Graphs With Large Language Models: Methods and Prospects",
  "url": "https://openalex.org/W4387560121",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2753535486",
      "name": "Pan, Shirui",
      "affiliations": [
        "Griffith University"
      ]
    },
    {
      "id": "https://openalex.org/A4282382954",
      "name": "Zheng, Yizhen",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1994553651",
      "name": "Liu Yixin",
      "affiliations": [
        "Monash University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6855732128",
    "https://openalex.org/W6855084754",
    "https://openalex.org/W6853843227",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W6852581021",
    "https://openalex.org/W6853465110",
    "https://openalex.org/W6856286174",
    "https://openalex.org/W6855797017",
    "https://openalex.org/W4378881184",
    "https://openalex.org/W4393277515",
    "https://openalex.org/W4386044294",
    "https://openalex.org/W4385963839",
    "https://openalex.org/W4393160302",
    "https://openalex.org/W4385849236",
    "https://openalex.org/W4378465454",
    "https://openalex.org/W4377130677"
  ],
  "abstract": "Large language models (LLMs) such as Generative Pre-trained Transformer 4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications including answering queries, code generation, and more. Parallelly, graph-structured data, intrinsic data types, are pervasive in real-world scenarios. Merging the capabilities of LLMs with graph-structured data has been a topic of keen interest. This article bifurcates such integrations into two predominant categories. The first leverages LLMs for graph learning, where LLMs can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of LLMs in various complicated tasks. We also discuss and propose open questions for integrating LLMs with graph-structured data for the future direction of the field.",
  "full_text": "Integrating Graphs with Large Language\nModels: Methods and Prospects\nShirui Pan, Griffith University, Gold Coast, QLD 4215, Australia\nYizhen Zheng & Yixin Liu, Monash University, Melbourne, VIC 3800, Australia\nAbstract—Large language models (LLMs) such as GPT -4 have emerged as\nfrontrunners, showcasing unparalleled prowess in diverse applications, including\nanswering queries, code generation, and more. Parallelly, graph-structured data,\nan intrinsic data type, is pervasive in real-world scenarios. Merging the\ncapabilities of LLMs with graph-structured data has been a topic of keen interest.\nThis paper bifurcates such integrations into two predominant categories. The first\nleverages LLMs for graph learning, where LLMs can not only augment existing\ngraph algorithms but also stand as prediction models for various graph tasks.\nConversely, the second category underscores the pivotal role of graphs in\nadvancing LLMs. Mirroring human cognition, we solve complex tasks by adopting\ngraphs in either reasoning or collaboration. Integrating with such structures can\nsignificantly boost the performance of LLMs in various complicated tasks. We also\ndiscuss and propose open questions for integrating LLMs with graph-structured\ndata for the future direction of the field.\nL\narge language models (LLMs) have rapidly\ntaken centre stage due to their remarkable ca-\npabilities. They have demonstrated prowess in\nvarious tasks, including but not limited to, translation,\nquestion-answering, and code generation. Their adapt-\nability and efficiency in processing and understanding\nvast amounts of data position them as revolutionary\ntools in the age of information. Concurrently, graphs\nare a natural representation of the world, and their\nability to capture complex relationships between en-\ntities makes them a powerful tool for modelling real-\nworld scenarios. For instance, structures reminiscent\nof graphs can be observed in nature and on the\ninternet. Given the individual significance of both LLMs\nand graph structures, the exploration into how they\ncan be synergistically combined has emerged as a hot\ntopic in the AI community.\nIn the paper, we delineate two primary paradigms\nfor the synergy between LLMs and graphs. The first\nparadigm, “LLMs enhance graph learning” involves\nharnessing the capabilities of LLMs to handle various\ngraph-related tasks. This includes predicting graph\nproperties, such as the degrees and connectivity\nof nodes, as well as more intricate challenges like\nnode and graph classification. Here, LLMs can ei-\nther supplement graph algorithms or serve as main\nXXXX-XXX © 2023 IEEE\nDigital Object Identifier 10.1109/XXX.0000.0000000\npredictive/generative models. Conversely, the second\nparadigm, “Graphs advance LLMs ability”, capitalises\non the inherent structure of graphs to enhance the\nreasoning capabilities of LLMs or help LLMs to collab-\norate, facilitating them in handling multifaceted tasks.\nBy leveraging graph structures, the efficacy of LLMs\nin complex problem-solving can be significantly aug-\nmented.\nWhy integrating graphs and LLMs?\nUsing LLMs to address graph tasksenjoys two pri-\nmary advantages. First, unlike the often opaque graph\ndeep learning techniques, LLMs approach graph-\nrelated challenges primarily through reasoning, pro-\nviding clearer insight into the basis for their predic-\ntions. This transparency offers a more interpretable\nmethod for understanding complex graph analyses.\nSecondly, LLMs possess an extensive repository of\nprior knowledge spanning diverse domains. Traditional\ngraph learning models, constrained by limited train-\ning data, struggle to comprehensively assimilate this\nwealth of knowledge. Consequently, harnessing LLMs\nfor graph data processing presents an opportunity to\nleverage their scalability and the expansive reservoir\nof prior knowledge. Such knowledge can be especially\nvaluable for graph machine learning in domains such\nas finance and biology.\nUsing graphs to enhance LLMsis also a promising\nlearning paradigm. In specific, graphs can substantially\nOct Published by the IEEE Computer Society Publication Name 1\narXiv:2310.05499v1  [cs.AI]  9 Oct 2023\nTHEME/FEATURE/DEPARTMENT\nLLM\nText EnhancedAttribute\nGraph Learning Models\n(a) LLMs Augmenting Graph Algorithms\n(c) LLMs Constructing Graphs\nLLM\nGraphs\n(b) LLMs Predicting Graph Tasks\nLLM\nGraphs\n Analyse Prediction\nDirect    is predicted as A\nPredict with \nExplanation\n   's 1-hop neighbors \nis ......   Thus, it is \nmore likely A.\nText \nGenerate\nTask\n output\nLLM\nReasoning\n(d1) Overall pipeline of LLM reasoning\n(d2) Input-output  \nNo reasoning\n(d3) Chain of \nThoughts (D/M)\nStep \n1\nStep \n2\nStep \n3\n(d4) Tree of Thoughts Prompting (U/M)\n4\nStep \n1\nStep \n1\n2\n2\n2\n2\n3\n3\n3\n3\n4\n4\n4\n4\n5\n(d5) Graph of Thoughts Prompting (U/E)\nUpon Tree of Thoughts...\nPlanner\nWorker\nNode HeterogeneityAggregation & Combination \nInteraction between paths Interaction between graphs\n(e) LLMs Multi-agent Systems\nLLM\nProduct Manager\nLLM\nProgrammer\nLLM\nProject Manager\nLLM\nSystem Architect\nLLM\nQA Engineer\nLLMs\nGraphs\nLLMs EnhanceGraph Learning Graphs Enhance LLM Ability\nEncode\nFIGURE 1. The overall framework of the mutual enhancement between LLMs and Graphs. (a)-(c): three pathways for LLMs\nto enhance graph learning. (d)-(e): techniques for graph structures enhancing LLM reasoning. Brackets after technique names\nindicate graph types. D, U, M and E represent directed, undirected, homogeneous and heterogeneous graphs, respectively.\namplify the capacity of LLMs in both logical reasoning\nand collaboration within multi-agent systems 1,2,7,9. For\ninstance, using a straightforward prompt like \"Let’s\nthink step by step\", commonly referred to as the chain-\nof-thoughts, has been proven to markedly enhance the\nLLM’s proficiency in resolving mathematical problems4.\nIt’s noteworthy that such enhancements are observed\neven with the use of a chain, which represents one\nof the simplest graph structures. This gives rise to\nthe anticipation that leveraging more intricate graph\nstructures could usher in even more profound im-\nprovements. From a broader viewpoint, in multi-agent\nsystems, graphs model inter-agent relationships, facil-\nitating efficient information flow and collaboration.\nLLMs Enhance Graph Learning\nOne pivotal approach to integrating LLMs and graphs\ninvolves leveraging LLMs to bolster graph learning.\nAs illustrated in the left part of Figure 1, this en-\nhancement can materialise through three distinct path-\nways: augmenting conventional graph algorithms with\nthe prowess of LLMs; directly employing LLMs for\ndownstream graph-related tasks; and utilizing LLMs in\nthe intricate construction of graph structures. In the\nfollowing sections, we dissect each of these strategies\nin detail.\nLLMs Augmenting Graph Algorithms\nThe integration of Large Language Models (LLMs)\nwith graph algorithms primarily seeks to harness LLMs\nas attribute-enhancement mechanisms, elevating the\nintrinsic attributes of graph nodes. As depicted in\nFigure 1(a), LLMs process text information for nodes\nto produce refined attributes. These enhanced at-\ntributes can potentially improve the performance of\ngraph learning models such as graph neural networks\n(GNNs).\nA direct approach is to employ LLMs as encoders\nfor processing node text-based attributes, with the\noption to fine-tune on specific downstream tasks 3.\nAnother technique uses a proprietary LLM, GPT -3.5, to\nsimultaneously produce predictions and explanations\nfor tasks like paper classification5. Using another open-\nsource LLM, they derive node embeddings by encod-\ning both the output of LLMs and the original attributes.\nThese embeddings are combined and then integrated\ninto GNNs to boost performance.\nA more sophisticated approach uses an iterative\nmethod to harmoniously integrate both GNNs and\nLLMs capabilities 3. They are initially trained sepa-\nrately; then, via a variational EM framework, the LLM\nuses text and GNN’s pseudo labels, while the GNN\nutilizes LLM-encoded embeddings or node attributes\nand LLMs’ pseudo labels, iteratively boosting mutual\nperformance.\nLLMs Predicting Graph Tasks\nLLMs are adept at predicting graph properties, includ-\ning attributes like node degrees and connectivity, and\ncan even tackle complex challenges such as node and\ngraph classification, as illustrated in Figure 1(b).\nA straightforward application involves presenting\nLLMs with zero-shot or few-shot prompts, prompting\nthem to either directly predict an outcome or to first\nprovide an analytical rationale followed by the ulti-\nmate prediction3,6. Experiments reveal that while LLMs\ndemonstrate a foundational grasp of graph structures,\n2 Integrating Graphs with Large Language Models: Methods and Prospects Oct 2023\nTHEME/FEATURE/DEPARTMENT\ntheir performance lags behind that of graph neural net-\nwork benchmarks. They also show that performance of\nLLMs is significantly affected by the prompting strategy\nand the use of graph description language, which is a\ntextual way to describe graphs.\nA more advanced method, dubbed InstructGLM,\nhas been put forth 8. This strategy utilises a multi-\ntask, multi-prompt instructional tuning process to refine\nLLMs prior to inference on specific tasks. During fine-\ntuning, nodes are treated as new tokens—initialised\nwith inherent node features—to broaden the original\nvocabulary of LLMs. Consequently, node embeddings\ncan be refined during the training phase. Employing\nthis refined methodology, their system outperforms\ngraph neural network benchmarks across three citation\nnetworks.\nLLMs Constructing Graphs\nLLMs can help in building graphs for downstream\ntasks as shown in Figure 1(c). For instance, some\nresearchers have tried using LLMs to analyse news\nheadlines and identify companies that might be\nimpacted10. In specific, a network of companies that\nhave correlations is constructed by LLMs automati-\ncally. The generated network can be used to improve\nthe performance of predictions of stock market move-\nments.\nGraphs Enhance LLM Ability\nLeveraging graph structures can significantly boost the\nreasoning and collaborative capacities of LLMs. As\nshown in the right part of Figure 1, these improvements\nemerge via two primary mechanisms: (1) employing\ngraph structures to bolster logical reasoning in LLMs,\nand (2) utilizing graph structures to enhance LLM\ncollaboration in multi-agent systems. We delve deeper\ninto each of these approaches in the subsequent sec-\ntions.\nGraphs Improving LLMs Reasoning\nGraphs are the foundational structure of human rea-\nsoning. Through tools like mind maps and flowcharts,\nand strategies like trial and error or task decomposi-\ntion, we manifest our intrinsic graph-structured thought\nprocesses. Not surprisingly, when properly leveraged,\nthey can significantly elevate the reasoning capabilities\nof LLMs. As illustrated in Figure 1(d1), when tasked,\nLLMs follow a sequence: they process the input data,\nengage in reasoning, and then produce the final re-\nsults. Figure 1(d2) highlights the limitations of LLMs us-\ning “Input-output Prompting”; without reasoning, their\nperformance tends to suffer, especially with complex\ntasks.\nEmploying graph structures, from basic chains and\ntrees to more complex designs, can profoundly aug-\nment the reasoning capabilities of LLMs. Consider the\n“chain-of-thought prompting” (COT) method, depicted\nin Figure 1(d3) 4. In this, LLMs harness a chain, a\ntype of directed acyclic graph, for structured problem-\nsolving. Remarkably, even this basic framework triples\nLLMs’ efficacy on GSM8K, a math word problem\nbenchmark.\nIn contrast, the “Tree of Thoughts” (ToT) method,\nutilising trees—an elementary undirected acyclic\ngraph—delves deeper into reasoning. Eeach reason-\ning phase in ToT is a node 7. LLMs traverse this\ntree, eliminating non-compliant nodes and returning\nupwards as necessary, to deduce the solution. With\nthis methodology, LLMs notch up a 74% accuracy in\nthe “Game of 24” test, overshadowing the 4% from\nCOT7.\nDiving into intricate graph structures propels LLMs’\ncapabilities even further. Improving ToT, the “Graph\nof Thoughts” (GoT) paradigm has been introduced 1,2,\nas illustrated in Figure 1(d5). This advanced rea-\nsoning graph can be heterogeneous, with diverse\nnodes dedicated to specific tasks. Sophisticated mech-\nanisms, such as node aggregation and combination\n(A&C), and dynamic interactions between paths and\ngraphs, are incorporated. A&C, for instance, facilitates\nnode subdivision for task decomposition and node\namalgamation1. Path interactions offer LLMs greater\nflexibility by enabling cross-path traversals, a leap from\nToT’s isolated branch framework. Multi-graph interac-\ntions can even be orchestrated for intricate tasks 2.\nThese GoT methodologies dramatically outpace sim-\npler graph models in handling complex challenges,\nindicating that more intricate graph structures could\nusher in even more significant enhancements.\nGraphs Building LLMs Collaboration\nWhile the preceding section examined the capabilities\nof individual LLMs, complex tasks, such as software\ndevelopment, require multiple LLMs to work in tandem\nwithin a collaborative framework, i.e., multi-agent sys-\ntems, as illustrated in Figure 1(e). Graph structures can\nbe instrumental in this context. As depicted in the same\nfigure, these structures can effectively model the rela-\ntionships and information flow between collaborating\nLLMs.\nOct 2023 Integrating Graphs with Large Language Models: Methods and Prospects 3\nTHEME/FEATURE/DEPARTMENT\nOpen Questions and Directions\nThe intersection of Large Language Models (LLMs)\nwith graph structures holds promise, yet its current\ndevelopment sparks some open questions and chal-\nlenges.\nLLMs Enhancing Graph Learning\nQuestion 1. How to leverage LLMs to learn\non other types of graphs beyond Text-attributed\nGraphs (TAG)? Current LLMs for graph learning pri-\nmarily concern TAGs. However, real-world graph data,\nsuch as social networks and molecular graphs, often\nincorporate attributes from different domains. To realise\nthe potential of LLMs in graph learning, it is crucial to\nefficiently handle a wide variety of graphs as input to\nthese models.\nFuture Directions: Direction 1.Translate diverse data\ntypes into textual format: For instance, a user’s profile\non a social network might list attributes like age, ad-\ndress, gender, and hobbies. These can be articulated\nas: \"User X is a male in his 20s, residing in Melbourne,\nwith a passion for playing guitars.\" Direction 2.Lever-\naging multi-modal models for graph-text alignment:\nMulti-modal LLMs have already made notable strides\nin domains like audio and images. Identifying methods\nto synchronise graph data with text would empower\nus to tap into the capabilities of multi-modal LLMs for\ngraph-based learning.\nQuestion 2. How can we help LLMs under-\nstand graphs? Central to the success of LLMs in\ngraph learning is their ability to genuinely compre-\nhend graphs. Experimental evidence suggests that\nthe choice of graph description language can have a\nsignificant impact on LLM performance 6.\nFuture Directions: Direction 1. Expanding graph\ndescription languages: Current graph description lan-\nguages offer a somewhat restricted scope. Developing\nenhanced description methods would enable LLMs\nto grasp and process graphs more effectively. Direc-\ntion 2.Pretraining or fine-tuning LLMs on Graphs: Pre-\ntraining or fine-tuning LLMs on various graph data con-\nverted by graph description language can help LLMs\nunderstand graphs better 8. Direction 3. Foundational\ngraph models for graph learning: While foundational\nmodels have made strides in areas like language,\nand image, a gap remains in establishing large-scale\nfoundational models for graphs. Converting graphs to\ntextual format offers a unique opportunity: LLMs can\nbe trained on this data, enabling graph learning to\nleverage the prior knowledge and scalability inherent\nin LLMs.\nGraphs Enhance LLM Ability\nQuestion 3. How to elevate more sophisticated\ngraph structures to enhance LLM reasoning?Cur-\nrent explorations into LLM reasoning have touched\nupon graph structures like chains, trees, and traditional\ngraphs. However, there is vast potential in delving into\nmore intricate graph structures, such as hypergraphs,\nprobabilistic graphical models, and signed graphs.\nFuture Directions: Expanding the types of graphs\nfor LLM reasoning: Diversifying the graph types used\ncould significantly bolster LLM reasoning.\nQuestion 4. How to elevate more sophisticated\ngraph structures to enhance multi-agent systems\n(MLS)? Presently, the graph structures guiding MLS,\nlike that in MetaGPT9, are relatively rudimentary. While\nMetaGPT employs the waterfall model in software\ndevelopment—illustrated by a simple chain structure\nlinking different agents—contemporary software de-\nvelopment is far more nuanced with intricate agent\nrelationships and multifaceted processes.\nFuture Directions: Incorporating advanced graph\nstructures for team-based LLM workflows: Drawing\nfrom the utility of graph structures in reasoning, adopt-\ning varied graph forms such as trees, traditional\ngraphs, and even more intricate structures may help.\nQuestion 5. How to integrate graph structures\ninto the pipeline of LLMs?The applicability of graph\nstructures is not confined to reasoning and collabora-\ntion. There’s a compelling case to be made for their\nintegration across all stages of the LLM lifecycle: from\ntraining and fine-tuning to inference.\nFuture Directions: Utilising graph structures in train-\ning, fine-tuning, and inference. For example, graphs\ncan structure the training data, enabling more effective\nlearning.\nREFERENCES\n1. M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L.\nGianinazzi, J. Gajda, T. Lehmann, M. Podstawski, H.\nNiewiadomski, P . Nyczyk, et al., “Graph of thoughts:\nSolving elaborate problems with large language mod-\nels,” arXiv:2308.09687, 2023.\n2. B. Lei, C. Liao, C. Ding, et al., “Boosting Logical\nReasoning in Large Language Models through\na New Framework: The Graph of Thought,”\narXiv:2308.08614, 2023.\n3. Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S.\nWang, D. Yin, W. Fan, H. Liu, et al., “Exploring the\npotential of large language models (LLMs) in learning\non graphs,” arXiv:2307.03393, 2023.\n4 Integrating Graphs with Large Language Models: Methods and Prospects Oct 2023\nTHEME/FEATURE/DEPARTMENT\n4. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F . Xia,\nE. Chi, Q. V. Le, D. Zhou, et al., “Chain-of-thought\nprompting elicits reasoning in large language models,”\nAdvances in Neural Information Processing Systems ,\nvol. 35, pp. 24824–24837, 2022.\n5. X. He, X. Bresson, T. Laurent, B. Hooi, et al., “Ex-\nplanations as Features: LLM-Based Features for Text-\nAttributed Graphs,” arXiv:2305.19523, 2023.\n6. J. Guo, L. Du, H. Liu, “GPT4Graph: Can Large\nLanguage Models Understand Graph Structured\nData? An Empirical Evaluation and Benchmarking,”\narXiv:2305.15066, 2023.\n7. S. Y ao, D. Yu, J. Zhao, I. Shafran, T.L. Griffiths,\nY . Cao, K. Narasimhan, “Tree of thoughts: Delib-\nerate problem solving with large language models,”\narXiv:2305.10601, 2023.\n8. R. Y e, C. Zhang, R. Wang, S. Xu, Y . Zhang, et al., “Nat-\nural language is all a graph needs,” arXiv:2308.07134,\n2023.\n9. S. Hong, X. Zheng, J. Chen, Y . Cheng, C. Zhang,\nZ. Wang, S. K. Y au, Z. Lin, L. Zhou, C. Ran, et al.,\n“Metagpt: Meta programming for multi-agent collabo-\nrative framework,” arXiv:2308.00352, 2023.\n10. Z. Chen, L. N. Zheng, C. Lu, J. Yuan, D. Zhu, et al.,\n“ChatGPT Informed Graph Neural Network for Stock\nMovement Prediction,” in SIGKDD 2023 Workshop on\nRobust NLP for Finance , 2023.\nOct 2023 Integrating Graphs with Large Language Models: Methods and Prospects 5",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.832573652267456
    },
    {
      "name": "Natural language processing",
      "score": 0.4477543532848358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44720566272735596
    },
    {
      "name": "Programming language",
      "score": 0.3996407091617584
    },
    {
      "name": "Software engineering",
      "score": 0.3567652404308319
    },
    {
      "name": "Data science",
      "score": 0.35111746191978455
    }
  ]
}