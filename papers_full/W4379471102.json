{
  "title": "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?",
  "url": "https://openalex.org/W4379471102",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4378724465",
      "name": "Kou, Bonan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4379490928",
      "name": "Chen, Shengmai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1297027748",
      "name": "Wang Zhijie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2048381051",
      "name": "Ma Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1777018552",
      "name": "Zhang Tian-yi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4205596491",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4384345649",
    "https://openalex.org/W4206060008",
    "https://openalex.org/W3211259717",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W4308648312",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4377014236",
    "https://openalex.org/W2970575144",
    "https://openalex.org/W2945102109",
    "https://openalex.org/W1902674502",
    "https://openalex.org/W3006165421",
    "https://openalex.org/W4386566629",
    "https://openalex.org/W4210764005",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2550471858",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W4313547544",
    "https://openalex.org/W4224233602",
    "https://openalex.org/W4285490465",
    "https://openalex.org/W4308627320",
    "https://openalex.org/W4315815628",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4205371973",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W4285600327",
    "https://openalex.org/W3168488662",
    "https://openalex.org/W2908076704",
    "https://openalex.org/W2522471310",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2997451752",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W4362659486",
    "https://openalex.org/W4288025992",
    "https://openalex.org/W3036209908",
    "https://openalex.org/W2956501948",
    "https://openalex.org/W4394652731",
    "https://openalex.org/W3093033010",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W4206251287",
    "https://openalex.org/W4377009756",
    "https://openalex.org/W4386982649",
    "https://openalex.org/W4283751459",
    "https://openalex.org/W4287665430",
    "https://openalex.org/W2053154970",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4321473304",
    "https://openalex.org/W3031696893",
    "https://openalex.org/W4312565886",
    "https://openalex.org/W3161997752",
    "https://openalex.org/W2962938532",
    "https://openalex.org/W4309619902",
    "https://openalex.org/W4320409584",
    "https://openalex.org/W3109966548",
    "https://openalex.org/W2810348861",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W3211801722",
    "https://openalex.org/W4285123920",
    "https://openalex.org/W4288103164",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W4388483873",
    "https://openalex.org/W3034991278",
    "https://openalex.org/W2889883176",
    "https://openalex.org/W3112165001",
    "https://openalex.org/W3128395826",
    "https://openalex.org/W4243989635",
    "https://openalex.org/W2970487286",
    "https://openalex.org/W3084193526",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W4286750487",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W4210440021",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4224980442",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4221167729",
    "https://openalex.org/W4287022754",
    "https://openalex.org/W4287265163",
    "https://openalex.org/W3161457214",
    "https://openalex.org/W4308731473"
  ],
  "abstract": "Large Language Models (LLMs) have recently been widely used for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. We made the first attempt to bridge this knowledge gap by investigating whether LLMs attend to the same parts of a task description as human programmers during code generation. An analysis of six LLMs, including GPT-4, on two popular code generation benchmarks revealed a consistent misalignment between LLMs' and programmers' attention. We manually analyzed 211 incorrect code snippets and found five attention patterns that can be used to explain many code generation errors. Finally, a user study showed that model attention computed by a perturbation-based method is often favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.",
  "full_text": "Do Large Language Models Pay Similar Attention Like\nHuman Programmers When Generating Code?\nBONAN KOU, Purdue University, USA\nSHENGMAI CHEN‚àó, Brown University, USA\nZHIJIE WANG,University of Alberta, Canada\nLEI MA, The University of Tokyo, Japan and University of Alberta, Canada\nTIANYI ZHANG, Purdue University, USA\nLarge Language Models (LLMs) have recently been widely used for code generation. Due to the complexity\nand opacity of LLMs, little is known about how these models generate code. We made the first attempt to\nbridge this knowledge gap by investigating whether LLMs attend to the same parts of a task description\nas human programmers during code generation. An analysis of six LLMs, including GPT-4, on two popular\ncode generation benchmarks revealed a consistent misalignment between LLMs‚Äô and programmers‚Äô attention.\nWe manually analyzed 211 incorrect code snippets and found five attention patterns that can be used to\nexplain many code generation errors. Finally, a user study showed that model attention computed by a\nperturbation-based method is often favored by human programmers. Our findings highlight the need for\nhuman-aligned LLMs for better interpretability and programmer trust.\nCCS Concepts: ‚Ä¢ Software and its engineering ; ‚Ä¢ Computing methodologies ‚Üí Natural language\nprocessing;\nAdditional Key Words and Phrases: Code Generation, Large Language Models, Attention\nACM Reference Format:\nBonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2024. Do Large Language Models Pay\nSimilar Attention Like Human Programmers When Generating Code?.Proc. ACM Softw. Eng. 1, FSE, Article 100\n(July 2024), 24 pages. https://doi.org/10.1145/3660807\n1 INTRODUCTION\nLarge Language Models (LLMs) have made significant process on code generation in recent\nyears [4, 5, 26, 27, 29, 32, 36, 43, 87]. A recent study [ 14] shows that GPT-4, the state-of-the-\nart LLM with 1.7 trillion parameters, can correctly solve 84% of the Python programming tasks\nfrom the HuamnEval [27] benchmark. Despite this great progress, it remains unclear why and how\nLLMs can generate correct code from natural language descriptions.\nModel attention analysis is a common methodology to understand how a model works. It has\nbeen widely adopted in computer vision [31, 38, 42, 44, 59, 63, 80, 101] to investigate whether a\nmodel pays attention to the salient parts of an image when making a decision. In particular, recent\n‚àóThis work was done when Shengmai Chen was an undergraduate student at Purdue University.\nAuthors‚Äô addresses: Bonan Kou, Purdue University, West Lafayette, USA, koub@purdue.edu; Shengmai Chen, Brown\nUniversity, Providence, USA, shengmai_chen@brown.edu; Zhijie Wang, University of Alberta, Edmonton, Canada, zhijie.\nwang@ualberta.ca; Lei Ma, The University of Tokyo, Tokyo, Japan and University of Alberta, Edmonton, Canada, ma.lei@\nacm.org; Tianyi Zhang, Purdue University, West Lafayette, USA, tianyi@purdue.edu.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM 2994-970X/2024/7-ART100\nhttps://doi.org/10.1145/3660807\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\narXiv:2306.01220v2  [cs.SE]  23 May 2024\n100:2 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\nstudies find that aligning model attention with human attention can effectively enhance model\nperformance [8, 34, 42]. For instance, Huang et al. [42] show that the performance of Conv-4-based\nimage classification models can be increased by up to 23% when they are trained to align with\nhuman attention. Furthermore, previous studies also suggest that users have more confidence and\ntrust in human-aligned models [ 13, 40, 75, 89]. For instance, Boggust et al. [ 13] find that users\ndetermine the trustworthiness of a model by checking whether the model makes predictions based\non features they consider important.\nThese findings lead to an important scientific question for LLM-based code generation‚Äîdo LLMs\nattend to similar parts of a task description like human programmers in code generation? We choose to\ncompare model attention with human attention, since it can help us determine whether LLMs grasp\nthe deep semantics in a task description like humans or whether they just learn superficial patterns\nfrom training data, which is known as a common problem in machine learning. Furthermore, by\ncomparing human and model attention patterns, we seek to investigate whether the attention\ndifferences can be used to explain some code generation errors and inform new opportunities to\nimprove LLMs for code generation.\nTo bridge the knowledge gap, we made the first effort to unveil the code generation process of\nLLMs by analyzing which parts of human language an LLM attends to when generating code . We\npresent a large-scale study that examines the attention alignment between six LLMs and human\nprogrammers on two popular code generation benchmarks‚Äî OpenAI‚Äôs HumanEval benchmark [27]\nand Google‚Äôs MBPP benchmark [ 7]. The hypothesis is that LLMs should generate code based\non salient words from an NL description similar to human programmers, rather than generating\ncode based on trivial tokens such as prepositions and delimiters. Specifically, we investigated the\nfollowing research questions in this study:\nRQ1 To what extent is model attention aligned with human attention?\nRQ2 Can attention explain errors of code generation models?\nRQ3 What is the impact of different attention calculation methods on attention alignment?\nRQ4 Which attention calculation method is most preferred by programmers?\nSince none of the existing code generation benchmarks contain programmer attention informa-\ntion,we created the first programmer attention dataset for the programming tasks from HumanEval\nand MBPP (1,138 tasks in total). We captured programmers‚Äô attention by asking two experienced\nprogrammers to manually label words and phrases that they considered essential to solving each\nprogramming task. Their labels are validated by a third programmer. On the other hand, to capture\nmodel attention, we implemented and experimented with twelve different attention calculation\nmethods in three categories‚Äîself-attention-based, gradient-based, and perturbation-based. To en-\nsure our findings generalize across different LLMs, we analyzed the attention of six LLMs with\ndifferent sizes, including GPT-4 [2], InCoder-1.3B [32], CoderGen-2.7B [62], PolyCoder-2.7B [91],\nCodeParrot-1.5B [1], and GPT-J-6B [87].\nOur study reveals several important insights into the code generation process of LLMs. First,\nwe find a consistent attention misalignment in all six LLMs, regardless of the attention calculation\nmethods. Furthermore, we performed an in-depth analysis of the attention patterns of 211 incorrect\ncodes generated by the two best models in our study, CodeGen-2.7B, and GPT-4. We found that 27%\nof the code generation errors could be explained by five attention patterns. Finally, perturbation-\nbased methods generated attention scores that are overall more aligned with human attention than\nother methods. They are also preferred by human programmers, according to a user study of 22\nparticipants. Our findings highlight the need to develop human-aligned LLMs and provide practical\nguidelines for improving LLM-based code generation and calculating model attention.\nIn summary, this paper makes the following contributions:\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:3\n‚óè We conducted the first empirical study on the attention alignment of LLMs and human program-\nmers on code generation tasks.\n‚óè We conducted a comparative analysis of different attention calculation methods for code genera-\ntion models through both quantitative experiments and a user study.\n‚óè We made publicly available the first programmer attention dataset of 1,138 Python tasks, which\ncan be used to develop new human-aligned models and evaluate interpretability methods for code\ngeneration models. Our code and data have been made available in our GitHub repository [47].\n2 MOTIVATION AND PRELIMINARIES\nIn this section, we first explain the motivation for performing attention analysis for code generation\nmodels. Then, we introduce popular benchmarks and metrics for code generation. Finally, we define\nmodel attention and describe different kinds of attention calculation methods for LLMs.\n2.1 Motivation\nModel attention analysis is a common task in several domains, such as computer vision [ 31, 44,\n59, 63], neural machine translation [18], and autonomous driving [38, 46, 80]. Specifically, several\nstudies show that aligning model attention with human attention during the training process\ncan significantly improve the model performance [8, 34, 42]. For instance, Huang et al. [42] show\nthat by aligning the attention of Conv-4-based and ResNet-based models to human attention on\nimages, the performance of these models on image classification can be increased by up to 23% in\nthe one-shot setting and 10% in the five-shot setting. In autonomous driving, many studies have\ndemonstrated that adding attention alignment constraints can help the autonomous driving system\nto drive safer [38, 46, 80]. This motivates us to investigate the attention alignment between LLMs\nand human programmers in the domain of code generation.\nSeveral recent studies analyzed the attention of neural models for code summarization, program\nrepair, and method name prediction [8, 64, 68]. Paltenghi et al. [64] studied the attention alignment\nbetween neural model attention and programmer attention on code summarization. Bansal et al. [8]\nshowed that aligning the attention of neural code summarization models with human attention\ncan effectively improve model performance. Rabin et al. [68] found that pre-trained code models\nrely heavily on just a few syntactic features in the prompts to perform method name prediction\nand variable misuse detection. To the best of our knowledge, none of the existing studies have\ninvestigated code generation tasks or LLMs with billions of parameters. Our study fills this gap by\nanalyzing the attention patterns of six LLMs in code generation tasks.\nFinally, another motivation behind this study is the fact that there is still no consensus on how to\ncalculate the attention score of LLM-based code models. This is largely attributed to the complexity\nof the multi-head, multi-layer attention mechanism adopted by these models. For example, some\nstudies only considered model attention from the first transformer layer and stated that the first\nlayer encodes lexical-level information [10, 97], while other studies summed up the attention from\nall transformer layers to incorporate long-distance token relationships [56, 85]. To bridge this gap,\nwe conducted the first comprehensive study on 12 attention calculation methods and systemically\nevaluated them with quantitative experiments and a user study with 22 participants.\n2.2 Code Generation Benchmarks and Metrics\nIn this work, we focus on code generation tasks that generate a function from a natural language\ndescription. Since the breakthrough of OpenAI‚Äôs Codex model in 2021 [ 27], this kind of code\ngeneration task has become increasingly popular in the research community. In this task setting,\ngiven a function header and a task description in natural language, an LLM is expected to complete\nthe function according to the task description. Figure 1 shows an example.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:4 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\nFig. 1. A Python function generated by CodeGen-2.7B [62]. The generated code is highlighted in green.\nCode generation models are often evaluated on crowd-sourced programming benchmarks. Ope-\nnAI developed a programming benchmark called HumanEval and used it to evaluate the original\nCodex model and its variants [27]. HumanEval [27] includes 164 Python programming tasks and\nground-truth solutions. It is by far the most popular benchmark and has been used to evaluate most\nLLM-based code generation models. Besides, MBPP is a large benchmark [7] with 974 crowd-sourced\nprogramming tasks and solutions. Both HumanEval and MBPP include test cases to evaluate the\nfunctional correctness of generated code.\nTwo types of metrics are often used to evaluate the performance of LLM-based code generation\nmodels. First, if a code generation benchmark includes test cases, one can simply run the test\ncases to evaluate the correctness of the generated code. A typical metric in this category is Pass@k.\nPass@k was initially introduced by Kulal et al. [50]. It measures the percentage of correctly solved\nprogramming tasks where ùëò refers to the number of code samples generated by the LLM. If any\nof the ùëò samples pass all test cases in a task, the task is considered correctly solved. OpenAI then\nintroduces an unbiased version of Pass@k to reduce variances, which is widely used to evaluate\ncode generation models these days [27]. In practice, many programming tasks do not have existing\ntest cases and some code solutions do not have a well-defined function interface for testing, e.g., a\nsingle line of code without clear input and output. Thus, prior work also measures the similarity\nbetween generated code and a ground-truth solution as a proxy for correctness. BLEU [65] and\nCodeBLEU [69] are commonly adopted similarity metrics. BLEU is a metric commonly used to\nevaluate the quality of machine-generated text. It evaluates the quality of machine-generated text\nby comparing the presence of n-grams in the generated text to those in reference texts. BLEU\ncalculates a score between 0 and 1, with 1 indicating perfect similarity. CodeBLEU is designed to\nadapt BLEU specifically for evaluating generated code. While BLEU primarily considers word-level\nsimilarity, CodeBLEU considers the structure and correctness of the generated code, which are\ncrucial aspects in code generation tasks.\n2.3 Model Attention\nIn this work, we use model attention to refer to how important a token in a natural language\ntask description is considered by an LLM during code generation. It implies which parts of the\ninput the model ‚Äúattends‚Äù to during code generation. This idea resembles feature importance [41],\nsaliency map [61], and feature attribution [60] in the XAI literature. We describe three kinds of\nmodel attention calculation methods as follows.\n2.3.1 Self-attention-based Methods. The self-attention mechanism allows transformers to weigh\nthe importance of different parts of the input when making predictions. By focusing on the most\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:5\nFig. 2. Attention matrix of the first attention head of the transformer layer in CodeGen-2.7B\nrelevant tokens with the highest self-attention scores, the transformers can make better predictions\nby capturing relationships and dependencies in the input.\nAn LLM includes multiple transformer layers , each of which includes multiple attention heads .\nThese attention heads independently calculate self-attention scores between different input tokens.\nTherefore, a transformer model could have multiple sources of model attention from different\ntransformer layers and different attention heads in each layer. For example, Figure 2 shows the\nself-attention scores of the first three attention heads of the first transformer layer in CodeGen-\n2.7B when generating token ‚Äúnumbers‚Äù from input sequence ‚ÄúReturn the max between two‚Äù . The\nself-attention scores calculated by different attention heads differ for the same token. Different\nheads represent different kinds of ‚Äúfocus‚Äù from the model.\nTo calculate model attention on the input sequence, vectors of self-attention scores from different\nattention layers and different attention heads in each layer need to be aggregated into a single\nvector to represent the overall importance of each input token to the model prediction. However,\nalthough self-attention scores have been generally used as model attention [20, 33, 52, 84], there is\nstill no consensus on how to aggregate self-attention scores from different layers and attention\nheads. For example, some studies sum up the attention scores attention from all transformer layers\nand all attention heads in each layer [99] as the final attention scores. These studies argue that this\nsummation strategy can capture long-distance token relationships. Some other studies only use the\nattention scores from the first transformer layer and argue that the first layer captures lexical-level\ndependencies [10, 97].\nTo reveal how different ways to aggregate self-attention affect the alignment between model and\nhuman attention, we experimented with six self-attention-based methods in this study (detailed in\nSection 4.2.1).\n2.3.2 Gradient-based Methods. Gradient-based methods leverage the gradients of the model‚Äôs\npredictions concerning the input features to calculate the model‚Äôs attention. The calculation of\ngradient-based methods includes two different steps: (1) perform a forward pass of input of inter-\nest, and (2) calculate gradients using backpropagation through the neural network‚Äôs layers. By\nanalyzing the magnitudes of these gradients, these methods can identify which input tokens are\nmost influential in determining the output. For example, Integrated Gradients is a gradient-based\nmethod that computes the integral of the gradients of the model‚Äôs output concerning each input\nfeature [23, 76, 81]. In this study, we experimented with two gradient-based methods used in\nprevious work [76, 78] with details in Section 4.2.2.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:6 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\n2.3.3 Perturbation-based Methods. Different from the previous two categories of methods,\nperturbation-based methods [ 84, 90] are model-agnostic. In other words, they do not require\naccess to the internal information of a model. Perturbation-based methods are particularly useful\nfor calculating the attention of commercial models such as GPT-4, since these models do not reveal\ntheir self-attention layers or gradients to users.\nPerturbation-based methods first mutate the input and then calculate the model‚Äôs attention based\non the output differences. LIME [70] and SHAP [57] are two popular perturbation-based methods.\nLIME [70] generates a local explanation by approximating the specific model predictions with a\nsimpler model (e.g., a linear classifier). SHAP [57] enhances LIME by perturbing the input based on\ngame theory and uses Shapely value to estimate different tokens‚Äô importance.\nA limitation of these two methods is that they often require a large number of perturbed samples\nto ensure estimation accuracy. This is costly for calculating the attention for GPT-4, since we need\nto query GPT-4 many times. Furthermore, LIME and SHAP only mutate an input by deleting tokens,\nwhich may significantly change the meaning or the structure of an input. To address this limitation,\nmore recent perturbation-based methods choose to substitute tokens with similar or semantically\nrelated tokens in the context [54, 90]. They often use a masked language model such as BERT [25]\nto predict similar or semantically related tokens to substitute existing tokens in an input. Then,\nthey measure the influence of these substitutions on the output. In this study, we experimented\nwith SHAP [57] and the BERT masking-based method [90] (detailed in Section 4.2.3).\n3 THE CONSTRUCTION OF THE PROGRAMMER ATTENTION DATASET\nSince none of the existing code generation benchmarks contain programmer attention information\n(i.e., which words or phrases a programmer considers important when writing code), we created\nthe first programmer attention dataset based on the 1,138 programming tasks, including all 164\nprompts from HumanEval [27] and the 974 prompts from MBPP [7]. We selected these two datasets,\nsince they are widely used to evaluate code generation models and they also provide test cases for\neach programming task, which is important for calculating the correctness of model-generated\ncode.\nThe first two authors, who have more than five years of programming experience in Python,\nmanually labeled the words and phrases they considered important to solve the programming task\nin each task description. Before the labeling process, the two labelers went through programming\ntasks in HumanEval to familiarize themselves with the programming tasks and the code solutions.\nThen, they first independently labeled the first 20 task descriptions in HumanEval. This first round\nof labeling had a Cohen‚Äôs Kappa score of 0.68. The two labelers discussed the disagreements and\nsummarized four kinds of keywords that both of them considered important. The four keyword\ntypes are summarized below:\n‚óè Data types: Words or phrases that describe the types of data that the code should input or output,\nsuch as ‚Äústring‚Äù, ‚Äúnumber‚Äù, or ‚Äúlist‚Äù.\n‚óè Operators: Words or phrases that describe the operations that the code should perform on the\ndata, such as ‚Äúcompare‚Äù, ‚Äúsort‚Äù, ‚Äúfilter‚Äù, or ‚Äúsearch‚Äù.\n‚óè Conditionals: Words or phrases that specify the conditions under which the code should execute,\nsuch as phrases after ‚Äúif‚Äù and ‚Äúwhen‚Äù in a task description.\n‚óè Properties: Important properties of the manipulated data and operations, such as quantifiers\n(e.g., ‚Äúall‚Äù, ‚Äúone‚Äù), adjectives (e.g., ‚Äúfirst‚Äù, ‚Äúcloser‚Äù), and adverbs (e.g., ‚Äúevery‚Äù, ‚Äúnone‚Äù).\nAlthough there are only four types of keywords, each type is designed to be high-level and\ninclusive. For instance, the ‚Äúoperator‚Äù type refers to any kind of operation on the data, such as\n‚Äúsort a list‚Äù , ‚Äúconnect a database‚Äù , and ‚Äúplot a graph‚Äù . With this labeling standard, the two labelers\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:7\nFig. 3. Two examples of labeled prompts from our dataset.\nproceed to label the remaining 144 task descriptions in the HumanEval dataset. The Cohen‚Äôs Kappa\nscore of this round of labeling increased to 0.72, indicating a substantial agreement [58]. Then, they\ndiscussed and resolved all disagreements.\nTo verify these labels, the third author, who was not involved in the previous labeling process,\nindependently labeled the 164 task descriptions from the HumanEval dataset. Since Cohen‚Äôs Kappa\ncan only calculate the agreement level between two labelers, we used Fleiss‚Äô Kappa to measure the\nagreement between the third labeler and the initial labels from the first two labelers. The Fleiss‚Äô\nKappa score is 0.64, indicating a substantial agreement [30]. This result shows labels made by the\nfirst two labelers are reasonable and can be accepted by other programmers. Then, the first two\nlabelers continued to label 974 programming tasks in the MBPP dataset independently, which\nresulted in a Cohen‚Äôs Kappa score of 0.73. Finally, they resolved all disagreements and used the\nfinal set of labels as the programmer‚Äôs attention dataset. The entire labeling process takes 192\nperson-hours.\nOn average, each task description has 29.6 words, among which 7 words are considered important\nby both labelers. Among all four types of keywords, property keywords (45.2%) are labeled the\nmost frequently by the two labelers, followed by operators (27.2%), conditional (25%), and data types\n(2.6%). Figure 3 shows two examples of labeled task descriptions. The four types of keywords are\nlabeled in different colors.\n4 METHODOLOGY\nThis section describes the study design to answer the research questions listed in Section 1.\n4.1 Code Generation Models\nIn this study, we select six LLMs with different sizes and different model performances on code\ngeneration tasks. Table 1 shows the size, the number of self-attention layers, the number of attention\nheads in each layer, and the model performance on the combined dataset of HumanEval and MBPP\nin terms of Pass@1. We describe each model below.\n‚óè InCoder-1.3B [32] is an open-source code language model from Meta AI. It is trained on 159\nGB of permissively licensed code from GitHub, GitLab, and Stack Overflow. Compared with\nother LLMs, it adopts a new causal masking objective, which allows it to infill blocks of code\nconditioned on the arbitrary left and right contexts. We used the largest pre-trained model\nreleased by Meta, including 1.3B parameters and 24 transformer layers.\n‚óè PolyCoder-2.7B [91] is an open-source model from CMU. It is based on the GPT-2 architecture\nand is designed to be the open-source counterpart of OpenAI Codex [ 27], since Codex is not\nopen-sourced. It is trained on 249GB of code and has 2.7B parameters.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:8 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\nTable 1. Code generation models included in this study.\nModel Layer Head Pass@1\nInCoder-1.3B 24 32 15.20%\nPolyCoder-2.7B 32 32 5.59%\nCodeGen-2.7B 32 32 23.70%\nCodeParrot-1.5B 48 25 3.58%\nGPT-J-6B 28 16 11.62%\nGPT-4 - - 67%\n‚óè CodeGen-Mono-2.7B [ 62] is an open-source model from Salesforce Research. It follows a\nstandard transformer autoregressive model architecture with rotary position embedding.\n‚óè CodeParrot-1.5B [1] is another open-source effort of training a GPT-2 model for code generation.\nIt is trained on 180GB Python Code and has 1.5B parameters.\n‚óè GPT-J-6B [87] is an open-source model from EleutherAI. It adopts a transformer architecture\nsimilar to GPT-3. It is trained on 825 GB text data, which includes 95GB code from GitHub.\n‚óè GPT-4 [ 2] is the state-of-the-art language model developed by OpenAI. Since the internal\nstructure of GPT-4 is not publicly disclosed, we do not include the number of layers and heads of\nGPT-4 in Table 1. It is reported that GPT-4 has about 1.76 trillion parameters [3]. We used the\nAPI (gpt-4) provided by OpenAI to query GPT-4.\n4.2 Model Attention Calculation\nWe experimented with twelve attention calculation methods from three different categories: six\nself-attention-based methods, four gradient-based methods, and two perturbation-based methods.\n4.2.1 Self-attention-based Methods. Given that LLMs have multiple attention layers, there is\ncurrently no consensus on what is the right way to aggregate those self-attentions to explain\nLLMs. Zeng et al. [97] show that the first attention layer is indicative of which tokens the model\nattends to, while Wan et al. [85] show that deeper attention layers are better at capturing long-\ndistance dependencies and program structure. To perform a comprehensive analysis, we decide to\nexperiment with three settings: (1) only using the first attention layer (denoted as first), (2) only\nusing the last attention layer (denoted as last), and (3) using all attention layers (denoted as all).\nTo aggregate self-attentions across different attention heads in a layer, we follow the previous\nwork [99] by summing the attention values from different heads. Finally, since LLMs generate code\nin an autoregressive manner, their attention changes in each step as they read more tokens from\nthe input and as they generate more code. We are curious about which input tokens the model\nhighly attends to as they read the input and which input tokens the model highly attends to as\nthey generate code. So we consider two experiment settings: (1) summing up the attention scores\nassigned to each token during the input reading process (denoted asREADING), and (2) summing up\nthe attention scores assigned to each token during the code generation process (denoted asCODING).\nGiven the three settings in layer-wise attention aggregation and the two settings in step-wise\nattention aggregation, we have a total of six experiment settings: READING_first, CODING_first,\nREADING_last, CODING_last, READING_all, and CODING_all.\n4.2.2 Gradient-based Methods. We consider two different methods to calculate gradient-based\nmodel attention: (1) Saliency [78], and (2) Input√óGradient [76]. The saliency method calculates\nthe model‚Äôs attention by computing model gradients with respect to the input. Given a LLM ‚Ñ±,\nsuppose an input is ùëã = (Ô∏Äùë•1,ùë•2,...,ùë• ùëõ‚åãÔ∏Ä, where ùëõis the length of the input. The attention ùë†ùëñ on ùë•ùëñ is\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:9\ncalculated as ùë†ùëñ = ùúï‚Ñ±{Ô∏Éùëã}Ô∏É\nùúïùë•ùëñ\n. Different from the saliency method, Input√óGradient further multiplies\ngradients with the input‚Äôs embedding values. The attention ùë†ùëñ on ùë•ùëñ is calculated as ùë†ùëñ = ùë•ùëñ ‚ãÖ ùúï‚Ñ±{Ô∏Éùëã}Ô∏É\nùúïùë•ùëñ\n.\nSimilar to self-attentions, gradients also change constantly at each generative step. Thus, we also\nexperimented with the two step-wise attention aggregation settings as in the self-attention methods.\nThe combination of the two gradient calculation methods with the two step-wise aggregation\nsettings results in four gradient-based methods‚ÄîInput√óGradient_reading, Input√óGradient_coding,\nSaliency_reading, and Saliency_coding.\n4.2.3 Perturbation-based Methods. We consider two different perturbation-based methods: (1)\nSHAP [57], which masks the input through deleting some tokens, and (2)BERT Masking [90], which\nmasks the input through substituting a token with its masked language modeling prediction result\nfrom BERT.\n‚óè SHAP. We use the official SHAP library with a perturbation count equal to 50 to calculate the\nmodel‚Äôs attention (SHAP scores) on different tokens.1 We aggregate SHAP scores on predicting\ndifferent tokens by summing them up.\n‚óè BERT Masking . Since the code from the original BERT Masking paper [90] is not publicly available,\nwe re-implemented this method based on the description in the paper. Specifically, given each\ntoken in an input prompt, this method masks it and uses a pre-trained BERT model from Hug-\ngingFace to predict the most likely token in the masked position. Then, it substitutes the original\ntoken with the predicted token and prompts the LLM to regenerate the code solution. This\nmethod then calculates the model‚Äôs attention on this token by calculating the BLEU score [65]\nbetween the original code solution and the new solution. We iterate through all tokens in the\ninput prompts to obtain model‚Äôs attention on different tokens.\n4.3 Attention Alignment Measurement\nWe measured the attention alignment between models and human programmers using two robust\nmetrics‚ÄîSan Martino‚Äôs token overlapping metrics [ 22] and Krippendorff‚Äôs alpha [ 49]. We also\nexperimented with two simple metrics, Cohen‚Äôs kappa and keyword coverage rate, and obtained\nsimilar results. Due to the page limit, we only reported the results of the first two metrics in this\npaper. The results of the other two metrics are included in the GitHub repository [47].\n4.3.1 San Martino‚Äôs Token Overlapping Metrics [ 22] calculate the overlap between two sets of\ntokens in terms of precision, recall, and F-1 score. It was initially designed for sequence labeling\ntasks in NLP [ 22] and has been recently adopted in SE studies as a robust metric for toxicity\ndetection in code review comments [72]. Specifically, it assigns partial credit for partial overlaps\nbetween two sets of tokens, which makes it a suitable choice for our task. Thus, we follow [22] to\ncompare salient words selected by humans and models. For human attention, a token is considered\nhighly attended if it is labeled as an important word by human programmers, as described in\nSection 3. For model attention, since the attention calculation methods in Section 4.2 compute a\ncontinuous score for each token, it is hard to determine a universal threshold to decide which token\nis highly attended by a model. Thus, we rank the tokens in a programming task description based\non their attention scores and select the top ùêæ tokens as the highly attended tokens by a model.\nSince the average of human-labeled important words is 7 per task description, we experiment with\nùêæ = 5,10,20 in our study. Given a set of highly attended tokens by human programmers and a set\nof highly attended tokens by a model, we follow the equations in [72] to compute precision, recall,\nand F-1 score. We report all three scores in Table 2.\n1https://shap.readthedocs.io/en/latest/\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:10 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\nFig. 4. Mapping NL words to LLM sub-tokens\n4.3.2 Krippendorff‚Äôs Alpha [ 48] is a robust statistical measure for inter-labeler agreement, where\nùõº = 1 indicates perfect agreement, ùõº = 0 indicates no agreement, and ùõº = ‚àí1 indicates complete\ndisagreement. It can be used to measure the agreement level between human programmers and\nLLMs on important words in a programming task description. Compared to other inter-rater\nagreement metrics such as Cohen‚Äôs kappa [21], Krippendorff‚Äôs alpha is more robust to the number\nof coders, missing data, and sample size. To calculate Krippendorff‚Äôs alpha, human and model\nlabels must be stored in vectors of the same length. This is hard because LLMs perform subword\ntokenization to handle out-of-vocabulary words with a manageable vocabulary size. For example,\nin Figure 4, the word ‚Äúseparate‚Äù is tokenized into two tokens: ‚Äúsepar‚Äù and ‚Äúate‚Äù through byte pair\nencoding (BPE). During inference, an LLM will compute attention scores separately for these two\nsubtokens, though they are from the same English word.\nTo address this challenge, we map NL words back to LLM tokens. If the model tokenizes a natural\nlanguage word into multiple tokens, all sub-tokens will be considered selected by human labelers.\nUsing the same example from Figure 4, if the natural language word ‚Äúseparate‚Äù is selected by the\nhuman labelers, both sub-tokens ‚Äúsepar‚Äù and ‚Äúate‚Äù will be considered selected and represented by 1\nin the vectors used to calculate Krippendorff‚Äôs alpha.\n4.4 User Study Design\nTo answer RQ4, we conducted a user study to evaluate the different attention calculation methods.2\nWe recruited 22 students (18 males and 4 females) through the department mailing lists in a\nCS department. According to our user study, all participants have an average of 5.62 years of\nprogramming experience. All participants have some basic understanding of model attention in\nmachine learning.\nWe randomly selected 8 task descriptions from our dataset. For each task, we leveraged CodeGen-\n2.7B, the best-performing open-source model on HumanEval in our experiment, to calculate its\nmodel attention. We did not consider GPT-4 in this user study, since it is close-sourced and we cannot\ncompute its attention using self-attention-based methods and gradient-based methods. We selected\none attention calculation method from each category (self-attention-based, perturbation-based, and\ngradient-based): ùê∂ùëÇùê∑ùêºùëÅùê∫ _ùëôùëéùë†ùë°, ùêºùëõùëùùë¢ùë° √óùê∫ùëüùëéùëëùëñùëíùëõùë°_ùëêùëúùëëùëñùëõùëî, and SHAP.\nIn each user study, participants first read the task description to understand the programming task\nand then read the code generated by CodeGen. For each attention method, we render a highlighted\nversion of the task description similar to Figure 3, where the top 10 attended tokens are highlighted\nbased on the attention scores computed by this method. We chose to render the top 10 important\nkeywords since it is close to the average number of important words (7) labeled in our dataset.\n2Our user study questionnaire and participants‚Äô responses are available in our GitHub repository:\nhttps://github.com/BonanKou/Attention-Alignment-Empirical-Study.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:11\nTable 2. Human-model attention alignment calculated by different methods in terms of San Martino‚Äôs\nPrecision (P), Recall (R), F1 score (F1), and Krippendorff‚Äôs alpha score (KA). Among the 12 different attention\ncalculation methods, the one that produces the most aligned result for each model under each ùêæ setting in\neach metric is highlighted in yellow . Note that we only experimented with perturbation-based methods on\nGPT-4, since GPT-4 does not provide access to its self-attention layers and gradients.\n(a) Perturbation-based methods.\nBERT_masking SHAP\nTop 5 Top 10 Top 20 Top 5 Top 10 Top 20Method\nP R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA\nIncoder 48.8%40.6%44.3%0.25 40.8% 67.4% 50.8% 0.2536.6%90.3% 52.1% 0.1233.9% 28.4% 30.9% 0.1133.4% 55.9% 41.8% 0.1832.8% 86.7% 47.6% 0.15\nCodeGen 51.8%43.2%47.1%0.29 41.8%68.9%52.0% 0.2736.8%90.7%52.4% 0.1333.0% 27.5% 30.0% 0.1032.9% 54.9% 41.1% 0.1733.0% 87.1% 47.9% 0.16\nCodeParrot51.5% 43.0% 46.9% 0.2942.1% 69.2% 52.3% 0.2836.6%90.2% 52.0% 0.1233.2% 27.7% 30.2% 0.1032.7% 54.7% 40.9% 0.1733.3% 87.6% 48.3% 0.17\nGPT-J-6B 49.9%41.3%45.2%0.26 41.0%67.4%51.0% 0.2636.6%90.3%52.1% 0.1233.3% 27.8% 30.3% 0.1033.3% 55.7% 41.7% 0.1833.1% 87.1% 47.9% 0.16\nPolyCoder49.3%41.3%45.0%0.26 40.9%67.7% 51.0% 0.2636.6%90.4%52.1% 0.1234.4% 28.9% 31.4% 0.1233.5% 56.2% 42.0% 0.1933.2% 87.2% 48.1% 0.16\nGPT-4 32.7% 27.6% 30.0% 0.0436.7%61.7%46.0% 0.1736.3%89.4%51.7% 0.1134.7%29.2%31.7%0.12 34.2% 57.4% 42.9%0.20 33.1% 87.0% 47.9%0.16\n(b) Gradient-based methods\nInput√óGradient_reading Input√óGradient_coding\nTop 5 Top 10 Top 20 Top 5 Top 10 Top 20Method\nP R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA\nIncoder 33.7% 25.9% 29.3% 0.1336.9% 46.8% 41.2% 0.3235.6% 52.7% 42.5% 0.2834.6% 27.5% 30.7% 0.1437.2% 47.1% 41.6% 0.3335.6% 52.8% 42.5% 0.28\nCodeGen 44.8% 34.2% 38.8% 0.2239.4% 59.8% 47.5% 0.2835.2% 87.3% 50.1% 0.2246.0% 34.7% 39.5% 0.2341.0% 61.5% 49.2%0.31 36.2% 88.5% 51.4%0.25\nCodeParrot55.0% 43.2% 48.4% 0.3345.8% 71.2% 55.8% 0.4035.1% 87.5% 50.1% 0.2257.9% 44.9% 50.6% 0.3746.8% 71.8% 56.7% 0.4236.0% 88.7% 51.2% 0.24\nGPT-J-6B 40.9% 30.7% 35.1% 0.1737.4% 56.7% 45.1% 0.2434.5% 85.8% 49.2% 0.2042.7% 32.5% 36.9% 0.1939.4% 59.6% 47.4%0.28 35.5% 87.2% 50.5%0.22\nPolyCoder43.3% 33.8% 38.0% 0.2038.3% 59.9% 46.7% 0.2634.4% 86.2% 49.1% 0.2038.1% 30.1% 33.6% 0.1436.4% 57.3% 44.5% 0.2335.2% 87.4% 50.2% 0.22\nSaliency_reading Saliency_coding\nTop 5 Top 10 Top 20 Top 5 Top 10 Top 20Method\nP R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA\nIncoder 36.6% 29.3% 32.6% 0.0641.9% 67.3% 51.6% 0.2941.4% 88.6% 56.4% 0.3039.4% 31.5% 35.0% 0.0942.8% 67.3% 52.3% 0.2941.3% 88.5% 56.4% 0.30\nCodeGen 46.1% 34.9% 39.8% 0.2339.3% 59.6% 47.4% 0.2835.3% 87.4% 50.3% 0.2347.3% 35.4% 40.5% 0.2440.7% 60.8% 48.8% 0.3036.2% 88.5% 51.4% 0.25\nCodeParrot53.8% 42.1% 47.2% 0.3144.6% 69.3% 54.3% 0.3834.8% 86.8% 49.7% 0.2156.3% 43.7% 49.2% 0.3545.3% 69.6% 54.9% 0.3935.7% 87.9% 50.8% 0.23\nGPT-J-6B 41.2% 30.6% 35.1% 0.1735.5% 53.1% 42.6% 0.2034.2% 85.1% 48.8% 0.1941.2% 30.5% 35.0% 0.1737.5% 56.3% 45.0% 0.2435.5% 87.2% 50.5% 0.22\nPolyCoder41.6% 32.5% 36.5% 0.1836.5% 56.9% 44.5% 0.2334.2% 85.9% 49.0% 0.2036.7% 29.0% 32.4% 0.1335.3% 55.6% 43.2% 0.2135.0% 87.0% 50.0% 0.22\n(c) Self-attention-based methods.\nREADING_first CODING_first\nTop 5 Top 10 Top 20 Top 5 Top 10 Top 20Method\nP R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA\nIncoder 44.9% 38.5% 41.5% 0.1746.7%75.9%57.8% 0.3941.7% 90.2%57.0%0.3145.3% 38.0% 41.4% 0.1845.1% 73.0% 55.7% 0.3541.4% 89.5% 56.7% 0.30\nCodeGen 9.8% 8.3% 9.0% -0.17 26.0% 41.6% 32.0% 0.0534.0% 84.1% 48.4% 0.199.5% 8.5% 9.0% -0.17 25.6% 40.4% 31.3% 0.0433.8% 83.8% 48.2% 0.19\nCodeParrot33.8% 27.5% 30.3% 0.1037.9% 59.9% 46.4% 0.2634.5% 86.2% 49.3% 0.2044.0% 35.7% 39.4% 0.2242.2% 67.2% 51.9% 0.3534.6% 86.6% 49.4% 0.21\nGPT-J-6B 6.5% 5.5% 6.0% -0.21 23.0% 33.2% 27.2% -0.0333.0% 81.3% 46.9% 0.167.4% 6.2% 6.7% -0.20 22.2% 33.0% 26.5% -0.0433.2% 81.9% 47.3% 0.17\nPolyCoder41.4% 32.5% 36.4% 0.1941.7% 63.7% 50.4% 0.3335.8% 87.9% 50.8%0.2343.8% 33.8% 38.1% 0.2042.5% 64.8%51.3% 0.3435.8% 87.8% 50.9% 0.23\nREADING_last CODING_last\nTop 5 Top 10 Top 20 Top 5 Top 10 Top 20Method\nP R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA\nIncoder 40.2% 31.9% 35.6% 0.1241.2% 64.8% 50.4% 0.2741.0% 88.8% 56.1% 0.2941.9% 33.9% 37.5% 0.1343.2% 69.6% 53.3% 0.3141.4% 89.3% 56.6% 0.30\nCodeGen 33.8% 24.8% 28.6% 0.0839.5% 59.1% 47.4% 0.2934.7% 85.2% 49.3% 0.2131.0% 24.4% 27.3% 0.0737.4% 57.1% 45.2% 0.2535.1% 86.4% 49.9% 0.22\nCodeParrot60.1%45.4%51.7% 0.3850.4%76.4%60.8% 0.4837.0% 90.2%52.4%0.2656.4% 43.2% 48.9% 0.3548.5% 73.6% 58.4% 0.4536.7% 89.7% 52.1% 0.26\nGPT-J-6B 8.8% 7.0% 7.8% -0.19 26.6% 42.7% 32.8% 0.0533.4% 82.7% 47.5% 0.1713.8% 10.6% 12.0% -0.1427.4% 41.8% 33.1% 0.0633.6% 83.2% 47.9% 0.18\nPolyCoder23.1% 18.1% 20.3% -0.0334.3% 53.8% 41.9% 0.1934.7% 86.4% 49.5% 0.2132.8% 25.7% 28.9% 0.0937.9% 58.8% 46.1% 0.2635.4% 87.0% 50.3% 0.22\nREADING_all CODING_all\nTop 5 Top 10 Top 20 Top 5 Top 10 Top 20Method\nP R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA\nIncoder 34.8% 32.5% 33.6% 0.0839.0% 66.7% 49.2% 0.2540.4% 88.2% 55.4% 0.2840.6% 39.2% 39.9% 0.1642.0% 71.8% 53.0% 0.3141.2% 89.8% 56.5% 0.30\nCodeGen 19.6% 14.7% 16.8% -0.0924.9% 37.0% 29.7% -0.0034.0% 83.9% 48.4% 0.1928.4% 23.1% 25.4% 0.0332.1% 50.7% 39.3% 0.1534.5% 85.8% 49.2% 0.21\nCodeParrot41.8% 31.8% 36.1% 0.1639.6% 61.3% 48.1% 0.2835.1% 87.1% 50.1% 0.2250.1% 39.7% 44.3% 0.2843.4% 68.0% 53.0% 0.3635.2% 87.3% 50.1% 0.22\nGPT-J-6B 12.6% 10.2% 11.3% -0.1522.9% 35.0% 27.7% -0.0433.8% 83.6% 48.1% 0.1830.6% 24.4% 27.2% 0.0532.7% 51.2% 39.9% 0.1534.6% 85.6% 49.2% 0.21\nPolyCoder28.7% 21.3% 24.4% 0.0136.3% 56.3% 44.1% 0.2134.8% 86.5% 49.6% 0.2133.4% 25.7% 29.0% 0.0838.4% 59.7% 46.8% 0.2635.1% 86.9% 50.0% 0.22\nParticipants were then asked to rate each attention method by indicating their agreement with the\nfollowing three statements on a 7-point Likert scale (1‚Äîcompletely disagree, 7‚Äîcompletely agree).\nQ1 The model-attended keywords align with my attention when reading the natural language task\ndescription.\nQ2 This attention explains why the model succeeded in or failed at generating the correct code.\nQ3 I want to see this attention when working with code generation models in real life.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:12 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\nThe order of different attention calculation methods is randomized to mitigate the learning effect.\nWe also do not reveal the names of the attention calculation methods to reduce bias. At the end of\nthe user study, participants answer three open-ended questions about different attention calculation\nmethods and the user study design. We ask these open-ended questions to study the correlation\nbetween model explainability and user trust. These questions include:\nQ4 Are you interested to know how the LLM generates the code?\nQ5 What do you want to find out about the internal code generation process in LLM?\nQ6 Do you trust this LLM? What do you need to know to improve the trust of the LLM?\n5 RESULTS\n5.1 RQ1: To What Extent Is Model Attention Aligned With Human Attention?\nTo answer this question, we collect the top ùêæ keywords that the six LLMs attend to and compare\nthem with the keywords labeled as important in the programmer attention dataset. As described in\nSection 4.3, we use San Martino‚Äôs token overlapping metrics [22] and Krippendorff‚Äôs alpha [49] to\nmeasure the attention alignment between LLMs and human programmers. When running models\nwhere the max output length can be set, we set the token limit to the number of tokens of the\nground truth plus 20 to tolerate moderate redundancy.\nTable 2a, Table 2b, and Table 2c present the attention alignment results when computing atten-\ntion scores with perturbation-based, gradient-based, and self-attention-based methods, respectively.\nBecause GPT-4 does not reveal its internal states during runtime, only perturbation-based methods\nare applicable. Therefore, this section discusses the results of the best perturbation-based method,\nBERT_masking. Section 5.3 compares different attention calculation methods in detail.\nWith BERT_masking method, from ùêæ = 5 to ùêæ = 10, the F1 scores of all models increase because\nas more and more tokens are selected by the models, we observe very high (around 90%) recall,\nwhich compensates for the declining precision. However, from ùêæ = 10 to ùêæ = 20, the F1 scores\nremain stable due to rapid declines in precision scores. On the other hand, the Krippendorff‚Äôs alpha\nscores remain stable from ùêæ = 5 to ùêæ = 10 (except for GPT-4) but rapidly decrease from ùêæ = 10 to\nùêæ = 20.\nOverall, for all models and all ùêæ values, the Krippendorff‚Äôs alpha does not change much and\nremains below 0.3, and the F1 scores remain below 0.6, indicating little agreement between model\nattention and human attention [ 58]. Among these models, GPT-4 showed the lowest attention\noverlap with human attention regarding both metrics. One possible explanation is that ultra-large\nmodels such as GPT-4 have developed a reasoning strategy different from that of human program-\nmers. These results suggest a consistent attention misalignment between LLMs and programmers\nwhen generating code.\nFinding 1\nThere is a consistent misalignment between LLM attention and programmer attention in all\nsettings, indicating that LLMs do not reason programming tasks like human programmers.\n5.2 RQ2: Can Attention Explain Errors of Code Generation Models?\nTo answer this question, the first two authors manually analyzed code generation errors made by\nthe best two models in our study‚ÄîGPT-4 and CodeGen-2.7B. In total, these two models generated\n920 incorrect code solutions on the two benchmarks. We randomly sampled 211 incorrect solutions,\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:13\nincluding 172 incorrect solutions from CodeGen-2.7B and 39 incorrect solutions from GPT-4. The\nsample size is statistically significant with a 90% confidence level and 5% margin of error.\nThe first two authors started with the first 50 code generation errors and independently checked\nwhether the attention pattern of each code calculated by the BERT_masking, which gives the most\naligned results in the quantitative experiments, could explain the error in it. For simple tasks,\nit takes about five minutes to check each one. For complicated tasks (e.g., tasks that require an\nunderstanding of specific math concepts), it takes around 10 to 15 minutes, since the authors need\nto manually debug the code and inspect its runtime values to understand the error first. After\nanalyzing 50 errors, they discussed these errors with the other authors and summarized six common\nattention patterns that can be used to explain code generation errors:\n‚óè Missing attention to critical conditions. The task description mentions certain conditions or corner\ncases to handle. However, the model misses or incorrectly handles one or more such conditions\nsince it does not attend to the words or phrases that describe the corresponding conditions.\n‚óè Missing attention to important descriptive words of an operation or a data object. The task description\nmentions an important property of an operation or a data object, such as ‚Äúlargest element‚Äù and\n‚Äúascending order‚Äù. However, the model does not attend to these descriptive words and thus\ngenerates a code solution with incorrect logic.\n‚óè Missing attention to operation descriptions. The task description mentions an operation or action,\nsuch as open a file and sort a list . However, the model fails to attend to the verb words or phrases.\nTherefore, the generated code performs the wrong action or does not perform the action.\n‚óè Missing attention to data types. Programming tasks often explicitly mention the expected type\nof inputs and outputs. Some task descriptions also mention the data type of some intermediate\nresults. However, the model does not attend to some data type descriptions. This can lead to\ndifferent types of errors, e.g., returning the wrong type of data, calling a method on the wrong\ntype of object, etc.\n‚óè Incorrect mapping between NL words and code elements. In some cases, we observe the model\nattends to an important word or phrase correctly but the model maps it to a wrong method call,\nvariable, parameter, value, or logic, potentially due to some conceptual misunderstanding of the\nsemantics meaning of the NL words.\nThen, they independently labeled the remaining errors, discussed their labeling with each other,\nand resolved the conflicts. In total, we found that errors in 57 of the 211 incorrect solutions (27%)\ncan be explained by one of the five attention patterns mentioned above. Specifically, 54 of the 172\nincorrect solutions from CodeGen-2.7B (31%) are explainable and 3 of the 39 incorrect solutions from\nGPT-4 (8%) are explainable. This finding suggests that neural attention analysis can be potentially\napplied to locate and repair a non-trivial portion of errors in LLM-generated code. Given that only\n3 errors made by GPT-4 can be explained by attention misalignment patterns, this implies that\nweaker models such as CodeGen-2.7B are more likely to be affected by attention misalignment and\nthus their generation errors are more explainable. This is an interesting observation since GPT-4\nalso suffers from attention misalignment, as shown in Table 2, but its generation errors cannot be\neasily explained by attention analysis. This indicates that as the language models become large\nenough, they may have developed a different way of interpreting input prompts and generating\ncontent. This calls for new research to understand why ultra-large language models such as GPT-4\ngenerate incorrect code. Besides, we acknowledge that many errors cannot be easily explained by\nmodel attention. Such errors include syntax errors, undefined variable names, incorrect API usage,\nincorrect array index, infinite loop, etc. This result indicates that further analysis is required to\nunderstand the root causes of these errors.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:14 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\nWe show the distribution and examples for each type of error below. In these examples, we\nhighlight keywords with high attention scores from the model in blue.\n5.2.1 Missing Attention to Critical Conditions. This prompt asks for a function that matches a\nstring with a character ‚Äúa‚Äù followed by zero or more character ‚Äúb‚Äùs. However, CodeGen-2.7B\ngenerates a function that only matches words with one ‚Äúa‚Äù followed by one or more ‚Äúb‚Äùs. Our\nattention analysis shows conditional phrase ‚Äúfollowed by zero or more‚Äù do not contain any of the\nmost attended tokens, which explains why the model ignores the condition where the character‚Äúa‚Äù\ncan be followed with no ‚Äúb‚Äùs or more than one ‚Äúb‚Äù. 6 incorrect solutions fall into this category.\n1 # Write a function that matches a string that has an ``a'' followed by zero or more ``b'' s .\n2 def function(string):\n3 return string.count('ab') >= 1\n5.2.2 Missing Attention to Important Descriptive Words of an Operation or a Data Object. The\nprompt below asks for a function that calculates the ùëõùë°‚Ñé power of all elements in a list. However,\nthe code generated by CodeGen-2.7B calculates the ùëõùë°‚Ñé power of a specific element without using\nthe lambda function. Our attention analysis shows the property phrase ‚Äúindividual items‚Äù contains\nnone of the most attended words in the prompt. This explains why the model fails to perform\nthe requested operation on all items instead of a specific item. 13 incorrect solutions fall into this\ncategory.\n1 # Write a function to find the n-th power of individual elements in a list using lambda function.\n2 def function(list, n):\n3 return list[n-1] ** n\n5.2.3 Missing Attention to Operation Descriptions. The prompt below asks for a function that\ncounts how many pairs of elements at the same index in two given arrays are equal. However,\nCodeGen-2.7B generates a function that checks whether the two strings are equal. Our attention\nanalysis shows the model the operator keyword ‚Äúcount‚Äù is not one of the most attended words in\nthe prompt, which explains why the generated code does not perform counting as requested. 6\nincorrect solutions fall into this category.\n1 # Write a function to count the same pair in two given lists using map function.\n2 def function(x,y):\n3 return x == y\n5.2.4 Missing Attention to Data Types. This prompt asks for a function that matches a word that\ncontains ‚Äúz‚Äù but does not have ‚Äúz‚Äù at the start or end. However, CodeGen-2.7B generates a function\nthat checks whether the string retrieved from removing the first character from the given word\nequals the string retrieved from removing the last character from the given word. Our attention\nanalysis shows that the data type keyword ‚Äúz‚Äù is not one of the most attended words in the prompt.\nThis explains why the generated code fails to check for the existence of a specific character in the\ngiven word. 7 errors belong to this category.\n1 # Write a function that matches a word containing 'z', not at the start or end of the word.\n2 def function(word):\n3 return word[1:] == word[:-1]\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:15\n5.2.5 Incorrect Mapping Between NL Words and Code Elements. This prompt asks for a function\nthat calculates the ùëõùë°‚Ñé bell number. However, instead of invoking the correct formula for bell\nnumber, CodeGen-2.7B calculates the factorial of ùëõ. Our attention analysis shows the keyword\n‚Äúbell‚Äù is one of the most attended words in the prompt. However, it still generates incorrectly for\nthis prompt because it does not understand the semantic meaning of ‚Äúbell number‚Äù . 25 incorrect\nsolutions belong to this category.\n1 # Write a python function to find nth bell number .\n2 def function(n):\n3 if n == 1:\n4 return 1\n5 else:\n6 return n * function(n - 1)\nFinding 2\nAttention analysis can explain 27% of generation errors in a benchmark of 211 errors. These errors\nprovide insights into the limitations of LLMs.\n5.3 RQ3: What Is the Impact of Attention Calculation Methods on the Alignment?\nTo compare different attention calculation methods, we calculated the average San Martino‚Äôs F-1\nscore and Krippendorff‚Äôs alpha of all five models (except for GPT-4) for all ùêæ (Table 3). Table 3\nshows that BERT_masking gives the highest alignment in both metrics for ùêæ = 5,10. However, the\nother perturbation-based method, SHAP, does not outperform other methods as BERT_masking\ndid. On the other hand, gradient-based methods are generally better than self-attention-based\nmethods, especially for ùêæ = 5 in terms of both metrics. Surprisingly, attention scores computed by\nself-attention-based methods (e.g., CODING_first) are least aligned with human attention, especially\nfor smaller ùêæ values. One possible reason is that other computation units in the transformer\narchitecture, such as the feed-forward layers, also play an important role in the code generation\nprocess. For instance, Geva et al. find that feed-forward layers influence model predictions by\npromoting concepts in the vocabulary space [35]. Thus, only considering self-attention layers does\nnot fully capture the influence of each input token on model predictions.\nTable 3. The average San Martino‚Äôs F-1 score and\nKrippendorff‚Äôs Alpha over all five models (excluding\nGPT-4) in various ùêæ settings. The highest score in\neach column is highlighted in yellow .\nTop 5 Top 10 Top 20Method F1 KA F1 KA F1 KABERT_masking 45.7%0.2751.4% 0.2652.1% 0.12SHAP 30.6% 0.1141.5% 0.1848.0% 0.16Input√óGradient_reading37.9% 0.2147.3% 0.3048.2% 0.22Input√óGradient_coding38.3% 0.2247.9%0.3149.2%0.24Saliency_reading37.5% 0.2046.0% 0.2848.0% 0.22Saliency_coding 37.5% 0.2146.7% 0.2949.0%0.24READING_first 24.6% 0.0142.7% 0.2050.5% 0.22READING_last 28.8% 0.0746.6% 0.2651.0% 0.23READING_all 24.4% 0.0039.8% 0.1450.3% 0.22CODING_first 26.9% 0.0543.4% 0.2150.5% 0.22CODING_last 30.9% 0.1047.2% 0.2751.4%0.24CODING_all 33.2% 0.1246.4% 0.2551.0% 0.23\nFurthermore, within gradient-based methods,\nwe observe that calculating attention distribution\nduring the coding stage (e.g., Saliency_coding)\ngives better alignment than the attention\ndistribution during the reading stage (e.g.,\nSaliency_reading). This pattern can be observed\nfor all ùêæ values (i.e., ùêæ = 5,10). However, the dif-\nferences between Input√óGradient and Saliency\nare trivial.\nFinally, as discussed in Section 2.3.1, there is\nno consensus on how to aggregate self-attention\nscores [10, 97, 99]. We experimented with six dif-\nferent self-attention aggregation methods used\nin previous work to find the best self-attention\naggregation method. First, among the three\noptions‚Äî(1) only summing attention scores in\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:16 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\nFig. 5. Participants‚Äô choices over different attention calculation methods in three dimensions.\nthe first layer, (2) only summing attention scores in the last layer, and (3) summing attention scores\nfrom all layers‚Äîonly summing attention scores in the last layer produces the attention distributions\nthat are most aligned with human attention distributions in both reading and coding stages, except\nwhen K is set to 5 in the coding stage. This suggests that future research should consider the last\nlayer when leveraging self-attentions to interpret code language models. Similar to the finding in\ngradient-based methods, for self-attention-based methods, attention distributions at the coding\nstage (e.g., CODING_first) are consistently more aligned with human attention than those at the\nreading stage (e.g., READING_first).\nFinding 3\nBERT_masking produces the best attention alignment to human programmers among all methods.\nGradient-based methods are generally better than self-attention-based methods. For gradient-\nbased and self-attention-based methods, attention distributions in the coding stage produce higher\nalignment. For self-attention-based methods, attention distributions in the last layer produce the\nhighest alignment.\n5.4 RQ4: Which Attention Calculation Method Is the Most Preferred?\nFigure 5 shows participants‚Äô assessment about SHAP, Input√óGradient_coding, and CODING_all\nin terms of the alignment with their own attention, the explainability of the computed attention,\nand their own preference (Q1-Q3 in Section 4.4). Overall, the perturbation-based method, SHAP,\nis favored over the other two methods in all three aspects. The average rating on the attention\nalignment of SHAP is 5.13, while the average for InputXGradient_coding and CODING_all are 4.59\nand 3.62, respectively.\nFurthermore, participants suggested that SHAP has better explainability than InputXGradi-\nent_coding (mean difference: 0.59, Welch‚Äôs ùë°-test, ùëù= 0.02) and CODING_all (mean difference: 1.26,\nWelch‚Äôs ùë°-test, ùëù = 0.00001). However, according to participants‚Äô responses about their trust in\nLLMs (Q6 in Section 4.4), 14 out of 22 participants expressed a lack of confidence and trust, even\nafter seeing the attention-based explanations. For example, P3 wrote, ‚ÄúI want to know whether\nthe LLM model can provide the reasons why they generate the code. For instance, the reference code. ‚Äù\nParticipants also asked for more fine-grained attention analysis. Specifically, they wished to see\nwhich parts of the input are responsible for generating which parts of the output. For instance, P10\nsaid, ‚Äú[I want to know] how the LLM determines the input and output, what are the discriminative\nwords that drive different generations. ‚Äù Participants also showed more preference for SHAP over the\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:17\nother two methods. The mean preference differences between SHAP and InputXGradient_coding\nand between SHAP and CODING_all is 5.04 vs. 4.56 (Welch‚Äôs ùë°-test, ùëù = 0.05) and 5.04 vs. 3.48\n(Welch‚Äôs ùë°-test, ùëù= 0.00009).\nFinding 4\nOverall, participants preferred the perturbation-based method over the gradient-based and the\nself-attention-based methods. However, participants still felt a lack of trust in LLMs after seeing\nthe attention-based explanations and wished to see richer explanations such as reference code\nand fine-grained attention mapping between text and code.\n6 IMPLICATIONS AND OPPORTUNITIES\nOur study has several significant implications that benefit the development of more reliable and\nmore accurate LLMs for code generation in the future.\nIn RQ1, we discovered a consistent misalignment between human and model attention in all\nsix models. While it is possible that LLMs use an entirely different method to reason about task\ndescriptions compared with human programmers, it is arguable whether such a method is indeed\ngood for programmers due to concerns about interpretability, robustness, and trust. Many studies\nhave shown that human-aligned models are perceived as more trustworthy by humans [8, 34, 38,\n42, 46, 80]. Furthermore, in practice, LLMs today can reliably solve a limited number of simple\nprogramming tasks and struggle to handle more sophisticated or custom tasks, which indicates\na huge space for improvement. Thus, we believe investigating the attention patterns of LLMs is\na worthwhile effort to help us understand how LLMs generate code and why LLMs make some\nmistakes and also inform new opportunities to improve LLMs.\nIn RQ2, we manually analyzed the attention of 211 incorrect generations of the best two mod-\nels in our paper (GPT-4 and CodeGen-2.7B) and found 27% of the errors can be explained by\nincorrect attention alignment. Our finding suggests the potential of fixing these generation errors\nby adjusting the attention alignment. Similarly, previous work in Computer Vision has shown\nthat the performance of neural models can be improved if we force their attention to align with\nhumans [31, 44, 59, 63]. One potential solution is to extend the loss function of LLMs with a penalty\nterm that measures the KL divergence between model attention and human attention. During\ntraining, the loss will increase when model attention deviates from human attention. As a result,\nthe LLM will be trained to distribute attention in a way similar to human programmers. We have\nopen-sourced our human attention dataset to facilitate future work on attention alignment.\nFurthermore, the model attention patterns in RQ3 can help to improve the robustness of code\ngeneration models by developing new attention-based adversarial training methods. Most adver-\nsarial training methods only apply small perturbations to random tokens in a prompt, without\nconsidering the importance of a token to the model [ 11, 79, 92, 98]. Our results show that it is\nworthwhile to prioritize important tokens during perturbation. Perturbing these important words\nand asking the model to generate code for these perturbed prompts may reveal robustness issues\nmore efficiently.\nIn RQ3, we compared 12 attention calculation methods with quantitative experiments (Table 2).\nTherefore, our study provides practical guidelines for choosing the attention calculation method for\nfuture research on LLM-based code generation models. Our experiment results indicate developers\nwho want to measure the attention of code generation models should first consider BERT_masking\nsince it consistently achieves the best alignment with human attention in all except for two settings\n(Table 3). Between self-attention-based and gradient-based methods, researchers should prioritize\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:18 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\ngradient-based methods that generally give better and more stable alignment in most settings\n(Table 3). For gradient-based and self-attention-based methods, calculating the attention distribution\nof the coding stage gives better alignment (Table 3). Finally, when leveraging self-attention to\ninterpret transformer-based code generation models, researchers should consider using the self-\nattention scores computed from the last layer, which is demonstrated to be more aligned with\nhuman attention (Table 3).\nIn RQ4, we conducted a user study to compare developers‚Äô perceptions on different explanation\nmethods. Most participants considered SHAP as the best XAI method for LLM code generation\nmodels. This finding suggests that future researchers may want to use the perturbation-based\nmethod as the default XAI method for LLM-based code generation. Furthermore, in the post-study\nsurvey, participants also asked for more fine-grained attention analysis, such as revealing the\nassociation between individual input and output tokens. This highlights the need for new XAI\nmethods to interpret LLM-based code generation models.\nIn the future, we would also like to explore how to teach humans how LLMs interpret code.\nUnderstanding how LLMs interpret code can inspire human programmers to craft prompts that are\nmore understandable by LLMs and thus guide LLMs to generate better code.\n7 THREAT TO VALIDITY\nInternal validity. One potential threat lies in the manual labeling process. Two programmers\nmanually labeled the important words to understand a task description and implement the correct\nfunction. Since this criterion of keywords is highly subjective, the words selected by the two labelers\nmay not represent the choice of a large pool of programmers. To mitigate this threat, the authors\nestablished a labeling standard through discussion and achieved substantial agreement on the\nlabeling. Furthermore, we invite a third labeler to validate our annotated dataset by independently\nlabeling the 164 prompts from the HumanEval dataset. We calculated the Fleiss‚Äô Kappa score among\nthe labels of three labelers and the result (0.64) indicates a significant agreement.\nExternal validity. One potential threat to external validity is that we have only experimented\nwith one programming language, Python. We cannot guarantee that our findings generalize to\nanother language.\nConstruct validity. One potential threat to construct validity lies in the survey design. As we know,\nprogramming problems are mentally demanding to solve and programmers may have different\nviews on which part of the prompt should the model attend to. However, in the current design,\nonly 22 participants were involved. The small sample size may result in findings that are not\ngeneralizable. To combat this threat, we only invited participants who are experienced in Python\nprogramming and have used code generation LLMs at least once to control the quality of the user\nstudy.\n8 RELATED WORK\n8.1 Code Generation From Natural Language\nSince CodeBERT [28], there has been a large body of literature where Large Language Models\n(LLMs) are used in code generation [5, 27, 29, 32, 37, 62, 66, 74, 82, 86, 88, 91, 96]. Both Guo et al. [37]\nand Zeng et al. [ 96] used a pre-trained BERT [ 25] model to encode NL questions and database\nschemas for text-to-SQL generation. CodeBERT adopts the same model architecture as BERT but\nis trained with a hybrid objective on code and text data from GitHub repositories [ 28]. Codex,\nCodeGPT, and GraphCodeBERT improve CodeBERT by leveraging data flow in the pre-training\nstage [36]. Recently, modern LLMs such as GPT-4 and Google Bard excel in code generation tasks\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:19\non various benchmarks [14, 24]. The highly accurate and contextually rich code snippets generated\nby these models enable the automation of various programming tasks.\nIn addition to developing new LLMs for code, prior work has also presented new methods to\nenhance LLMs for more accurate and robust code generation [15, 16, 51, 66, 74, 94, 100]. Instead of\ndirectly generating code from text, REDCODER [66] first retrieves similar code snippets from a\ncorpus and then passes them along with the text description to an LLM for code generation. Shen\net al. [74] proposed to leverage domain knowledge from documentation and code comments to\nassist code generation. Chakraborty et al. proposed a new pre-training task called naturalizing of\nsource code (i.e., translate an artificially created code to a human-written form) to help an LLM\nlearn how to generate natural code [ 15]. Chen et al. proposed to use an LLM to automatically\ngenerate test cases to examine the code generated by the same LLM [16]. Zan et al. proposed to first\ndecompose a LLM to two LLMs, one for generating program sketches and the other for filling the\nsketches [94]. SkCoder [51] is designed to mimic developers‚Äô code reuse behavior by first retrieving\na code example related to a given task, extracting a sketch from it, and editing the sketch based on\nthe task description.\n8.2 Empirical Studies on Code Generation\nRecently, many studies have evaluated LLM-based code generation models in different aspects,\nincluding performance [27, 39, 55, 71, 97], robustness [56, 103], security [6, 67, 93], code smells [77],\nusability [9, 12, 83, 91], and licensing [19]. The most related to us are those that investigate the\nexplainability of LLMs for code [45, 53, 85, 99]. Karmakar et al. studied what pre-trained BERT-based\nmodels such as CodeBERT and GraphCodeBERT have learned about code using probing tasks [45].\nA probing task is essentially a prediction task on a specific code property, such as code length and\ncyclomatic complexity, to test whether the model is sensitive to the property. Compared with our\nwork, they did not analyze the code generation process, e.g., why and how certain code is generated\nbased on a text description. Liguori et al. proposed a perturbation-based method to evaluate NMT\nmodels for code generation [53]. In addition, Zhang et al. investigated the code generation process\nby analyzing the self-attention layers in CodeBERT and GraphCodeBERT [99]. Wan et al. did a\nsimilar self-attention analysis and also designed a new probe task on code structures to analyze\nwhether LLMs have learned information about code structures [85]. Compared with these studies,\nour work differs by analyzing the consistency between LLMs‚Äô attention and programmers‚Äô attention\non the NL description of a programming task.\n8.3 Model Attention Analysis in Other Domains\nMany attention calculation methods have been developed to explain models in other domains.\nFor example, in CV domain, Selvaraju et al. [73] proposed to use the gradients of a convolutional\nneural network (CNN) to indicate the importance of each pixel in an image for a specific prediction.\nSimilarly, Zhou et al. [102] used the global average pooling mechanism to calculate the importance\nof each region for CNN to classify an image correctly. Autonomous driving is another domain\nwhere the need for explainability is strong. For example, Zeiler et al. [95] use deconvolution layers\nto understand how autonomous vehicles capture real-time image segments using CNNs. In another\nwork, Chen et al. [17] proposed a data-efficient policy learning approach called Semantic Predictive\nControl (SPC) that explains how perceived environmental states are mapped to actions.\n9 CONCLUSION\nThis paper presents an empirical study on attention alignment between LLM-based code generation\nmodels and human programmers. Our results reveal that there is a consistent misalignment between\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:20 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\nLLMs‚Äô and programmers‚Äô attention. Among the 12 attention calculation methods, perturbation-\nbased methods produced attention scores that were better aligned with human attention and were\nalso more preferred by user study participants. Based on our study results, we further discuss\nseveral implications and future research opportunities for better interpretation and performance\nimprovement of LLM-based code generation models.\n10 DATA AVAILABILITY\nOur code and data are available on a GitHub repository [47].\nReferences\n[1] 2022. CodeParrot. https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot.\n[2] 2023. ChatGPT. http://chat.openai.com.\n[3] 2023. GPT-4 Parameters: Unlimited guide NLP‚Äôs Game-Changer. https://medium.com/@mlubbad/the-ultimate-guide-\nto-gpt-4-parameters-everything-you-need-to-know-about-nlps-game-changer-109b8767855a.\n[4] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified Pre-training for Program\nUnderstanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies . 2655‚Äì2668.\n[5] Alex Andonian, Quentin Anthony, et al. 2021. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch .\nhttps://doi.org/10.5281/zenodo.5879544\n[6] Owura Asare, Meiyappan Nagappan, and N Asokan. 2022. Is github‚Äôs copilot as bad as humans at introducing\nvulnerabilities in code? arXiv preprint arXiv:2204.04741 (2022).\n[7] Jacob Austin, Augustus Odena, et al . 2021. Program Synthesis with Large Language Models. arXiv preprint\narXiv:2108.07732 (2021).\n[8] Aakash Bansal, Bonita Sharif, and Collin McMillan. 2023. Towards Modeling Human Attention from Eye Movements\nfor Neural Source Code Summarization. Proceedings of the ACM on Human-Computer Interaction 7, ETRA (2023),\n1‚Äì19.\n[9] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with\ncode-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\n[10] Joshua Bensemann et al. 2022. Eye gaze and self-attention: How humans and transformers attend words in sentences.\nIn Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics . 75‚Äì87.\n[11] Pavol Bielik and Martin Vechev. 2020. Adversarial robustness for code. In International Conference on Machine\nLearning. PMLR, 896‚Äì907.\n[12] Christian Bird et al . 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-\nprogramming tools. Queue 20, 6 (2022), 35‚Äì57.\n[13] Angie Boggust, Benjamin Hoover, Arvind Satyanarayan, and Hendrik Strobelt. 2022. Shared interest: Measuring\nhuman-ai alignment to identify recurring patterns in model behavior. In Proceedings of the 2022 CHI Conference on\nHuman Factors in Computing Systems . 1‚Äì17.\n[14] S√©bastien Bubeck et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712 (2023).\n[15] Saikat Chakraborty et al. 2022. NatGen: generative pre-training by ‚Äúnaturalizing‚Äù source code. In Proceedings of the\n30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering .\n18‚Äì30.\n[16] Bei Chen et al. 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).\n[17] Jianyu Chen, Shengbo Eben Li, and Masayoshi Tomizuka. 2021. Interpretable end-to-end urban autonomous driving\nwith latent deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems 23, 6 (2021), 5068‚Äì\n5078.\n[18] Wenhu Chen, Evgeny Matusov, Shahram Khadivi, and Jan-Thorsten Peter. 2016. Guided alignment training for\ntopic-aware neural machine translation. arXiv preprint arXiv:1607.01628 (2016).\n[19] Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota. 2022. To what extent do deep learning-based code recom-\nmenders generate predictions by cloning code from the training set?. InProceedings of the 19th International Conference\non Mining Software Repositories . 167‚Äì178.\n[20] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What Does BERT Look at? An\nAnalysis of BERT‚Äôs Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP . 276‚Äì286.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:21\n[21] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement 20, 1\n(1960), 37‚Äì46.\n[22] Giovanni Da San Martino et al. 2019. Fine-grained analysis of propaganda in news article. In Proceedings of the 2019\nconference on empirical methods in natural language processing and the 9th international joint conference on natural\nlanguage processing (EMNLP-IJCNLP) . Association for Computational Linguistics, 5636‚Äì5646.\n[23] Misha Denil, Alban Demiraj, and Nando De Freitas. 2014. Extraction of salient sentences from labelled documents.\narXiv preprint arXiv:1412.6815 (2014).\n[24] Giuseppe Destefanis, Silvia Bartolucci, and Marco Ortu. 2023. A Preliminary Analysis on the Code Generation\nCapabilities of GPT-3.5 and Bard AI Models for Java Functions. arXiv preprint arXiv:2305.09402 (2023).\n[25] Jacob Devlin et al . 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics .\n4171‚Äì4186.\n[26] Ahmed Elnaggar et al. 2021. CodeTrans: Towards Cracking the Language of Silicon‚Äôs Code Through Self-Supervised\nDeep Learning and High Performance Computing. arXiv preprint arXiv:2104.02443 (2021).\n[27] Mark Chen et al. 2021. Evaluating Large Language Models Trained on Code. (2021). arXiv:2107.03374 [cs.LG]\n[28] Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri. 2017. Component-based synthesis of\ntable consolidation and transformation tasks from examples. ACM SIGPLAN Notices 52, 6 (2017), 422‚Äì436.\n[29] Zhangyin Feng et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Language Processing. In\nProceedings of the 28th ACM International Conference on Information and Knowledge Management . 307‚Äì316.\n[30] Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin 76, 5 (1971), 378.\n[31] Ruth C Fong, Walter J Scheirer, and David D Cox. 2018. Using human brain activity to guide machine learning.\nScientific reports 8, 1 (2018), 5397.\n[32] Daniel Fried et al. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh International\nConference on Learning Representations . https://openreview.net/forum?id=hQwb-lbM6EL\n[33] Andrea Galassi, Marco Lippi, and Paolo Torroni. 2020. Attention in natural language processing. IEEE transactions on\nneural networks and learning systems 32, 10 (2020), 4291‚Äì4308.\n[34] Yuyang Gao et al. 2022. Aligning eyes between humans and deep neural network through interactive attention\nalignment. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2 (2022), 1‚Äì28.\n[35] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions\nby promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680 (2022).\n[36] Daya Guo et al. 2020. Graphcodebert: Pre-training code representations with data flow.arXiv preprint arXiv:2009.08366\n(2020).\n[37] Jiaqi Guo et al. 2019. Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics . 4524‚Äì4535.\n[38] Christopher Hazard et al. 2022. Importance is in your attention: agent importance prediction for autonomous driving.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2532‚Äì2535.\n[39] Dan Hendrycks et al. 2021. Measuring Coding Challenge Competence With APPS. NeurIPS (2021).\n[40] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020.\nAligning AI with Shared Human Values. arXiv preprint arXiv:2008.02275 (2020).\n[41] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2018. Evaluating feature importance estimates.\n(2018).\n[42] Siteng Huang, Min Zhang, Yachen Kang, and Donglin Wang. 2021. Attributes-guided and pure-visual attention\nalignment for few-shot recognition. InProceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 7840‚Äì7847.\n[43] Paras Jain and Ajay Jain. 2021. Contrastive Code Representation Learning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing .\n[44] Shaohua Jia et al. 2018. Biometric recognition through eye movements using a recurrent neural network. In 2018\nIEEE International Conference on Big Knowledge (ICBK) . IEEE, 57‚Äì64.\n[45] Anjan Karmakar and Romain Robbes. 2021. What do pre-trained code models know about code?. In 2021 36th\nIEEE/ACM International Conference on Automated Software Engineering (ASE) . IEEE, 1332‚Äì1336.\n[46] Iuliia Kotseruba, Amir Rasouli, and John K Tsotsos. 2016. Joint attention in autonomous driving (JAAD). arXiv\npreprint arXiv:1609.04741 (2016).\n[47] Bonan Kou. 2024. Attention-Alignment-Empirical-Study. https://github.com/BonanKou/Attention-Alignment-\nEmpirical-Study.\n[48] Klaus Krippendorff. 2004. Reliability in content analysis: Some common misconceptions and recommendations.\nHuman communication research 30, 3 (2004), 411‚Äì433.\n[49] Klaus Krippendorff. 2018. Content analysis: An introduction to its methodology . Sage publications.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:22 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\n[50] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang. 2019. Spoc:\nSearch-based pseudocode to code. Advances in Neural Information Processing Systems 32 (2019).\n[51] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. 2023. SkCoder: A Sketch-based Approach for Automatic\nCode Generation. arXiv preprint arXiv:2302.06144 (2023).\n[52] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding neural networks through representation erasure. arXiv\npreprint arXiv:1612.08220 (2016).\n[53] Pietro Liguori et al. 2022. Can NMT understand me? towards perturbation-based evaluation of NMT models for code\ngeneration. In 2022 IEEE/ACM 1st International Workshop on Natural Language-Based Software Engineering (NLBSE) .\nIEEE, 59‚Äì66.\n[54] Shusen Liu et al. 2018. Nlize: A perturbation-driven visual interrogation tool for analyzing and interpreting natural\nlanguage inference models. IEEE transactions on visualization and computer graphics 25, 1 (2018), 651‚Äì660.\n[55] Xiaodong Liu, Ying Xia, and David Lo. 2020. An Empirical Study on the Usage of Transformer Models for Code\nCompletion. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME) . IEEE, 408‚Äì418.\n[56] Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, and Li Li. 2023. On the Reliability and Explainability of Automated\nCode Generation Approaches. arXiv preprint arXiv:2302.09587 (2023).\n[57] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. Advances in neural\ninformation processing systems 30 (2017).\n[58] Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica 22, 3 (2012), 276‚Äì282.\n[59] Cristina Mel√≠cio et al. 2018. Object detection and localization with artificial foveal visual attention. In 2018 Joint IEEE\n8th international conference on development and learning and epigenetic robotics (ICDL-EpiRob) . IEEE, 101‚Äì106.\n[60] Christoph Molnar. 2020. Interpretable machine learning . Lulu. com.\n[61] Ernst Niebur. 2007. Saliency map. Scholarpedia 2, 8 (2007), 2675.\n[62] Erik Nijkamp, Bo Pang, et al. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program\nSynthesis. In The Eleventh International Conference on Learning Representations .\n[63] Afonso Nunes, Rui Figueiredo, and Plinio Moreno. 2020. Learning to search for objects in images from human gaze\nsequences. In Image Analysis and Recognition: 17th International Conference . Springer, 280‚Äì292.\n[64] Matteo Paltenghi and Michael Pradel. 2021. Thinking like a developer? comparing the attention of humans with\nneural models of code. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) .\nIEEE, 867‚Äì879.\n[65] Kishore Papineni et al. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computational Linguistics . 311‚Äì318.\n[66] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented\nCode Generation and Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021 .\n2719‚Äì2734.\n[67] Hammond Pearce et al. 2022. Asleep at the keyboard? assessing the security of github copilot‚Äôs code contributions. In\n2022 IEEE Symposium on Security and Privacy (SP) . IEEE, 754‚Äì768.\n[68] Md Rafiqul Islam Rabin, Vincent J Hellendoorn, and Mohammad Amin Alipour. 2021. Understanding neural code\nintelligence through program simplification. In Proceedings of the 29th ACM Joint Meeting on European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering . 441‚Äì452.\n[69] Shuo Ren et al. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297\n(2020).\n[70] Marco Tulio Ribeiro et al. 2016. \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings\nof the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 1135‚Äì1144.\n[71] Rafael R Rodrigues et al. 2021. Studying the usage of text-to-text transfer transformer to support code-related tasks.\nIn 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) .\nIEEE, 327‚Äì336.\n[72] Jaydeb Sarker, Sayma Sultana, Steven R Wilson, and Amiangshu Bosu. 2023. ToxiSpanSE: An Explainable Toxicity\nDetection in Code Review Comments. In 2023 ACM/IEEE International Symposium on Empirical Software Engineering\nand Measurement (ESEM) . IEEE, 1‚Äì12.\n[73] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.\n2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE\ninternational conference on computer vision . 618‚Äì626.\n[74] Sijie Shen, Xiang Zhu, Yihong Dong, Qizhi Guo, Yankun Zhen, and Ge Li. 2022. Incorporating domain knowledge\nthrough task augmentation for front-end JavaScript code generation. In Proceedings of the 30th ACM Joint European\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering . 1533‚Äì1543.\n[75] Donghee Shin. 2021. The effects of explainability and causability on perception, trust, and acceptance: Implications\nfor explainable AI. International Journal of Human-Computer Studies 146 (2021), 102551.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\nDo Large Language Models Pay Similar Attention Like Human Programmers When Generating Code? 100:23\n[76] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating\nactivation differences. In International conference on machine learning . PMLR, 3145‚Äì3153.\n[77] Mohammed Latif Siddiq et al . 2022. An Empirical Study of Code Smells in Transformer-based Code Generation\nTechniques. In 2022 IEEE 22nd International Working Conference on Source Code Analysis and Manipulation (SCAM) .\nIEEE, 71‚Äì82.\n[78] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising\nimage classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013).\n[79] Shashank Srikant et al. 2020. Generating Adversarial Computer Programs using Optimized Obfuscations. In Interna-\ntional Conference on Learning Representations .\n[80] Andrea Stocco et al. 2022. Thirdeye: Attention maps for safe autonomous driving systems. In Proceedings of the 37th\nIEEE/ACM International Conference on Automated Software Engineering . 1‚Äì12.\n[81] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International\nconference on machine learning . PMLR, 3319‚Äì3328.\n[82] Lewis Tunstall, Leandro von Werra, and Thomas Wolf. 2022.Natural Language Processing with Transformers: Building\nLanguage Applications with Hugging Face . O‚ÄôReilly Media, Incorporated.\n[83] Priyan Vaithilingam et al. 2022. Expectation vs. experience: Evaluating the usability of code generation tools powered\nby large language models. In CHI conference on human factors in computing systems extended abstracts . 1‚Äì7.\n[84] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention interpretability\nacross nlp tasks. arXiv preprint arXiv:1909.11218 (2019).\n[85] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. 2022. What do they capture? a structural\nanalysis of pre-trained language models for source code. InProceedings of the 44th International Conference on Software\nEngineering. 2377‚Äì2388.\n[86] Bailin Wang et al . 2020. RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics . 7567‚Äì7578.\n[87] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:\n//github.com/kingoflolz/mesh-transformer-jax.\n[88] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-\nDecoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing . 8696‚Äì8708.\n[89] Katharina Weitz et al. 2019. \" Do you trust me?\" Increasing user-trust by integrating virtual agents in explainable AI\ninteraction design. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents . 7‚Äì9.\n[90] Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020. Perturbed Masking: Parameter-free Probing for Analyzing\nand Interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics .\n4166‚Äì4176.\n[91] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large\nlanguage models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming .\n1‚Äì10.\n[92] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial examples for models of code. Proceedings of the ACM on\nProgramming Languages 4, OOPSLA (2020), 1‚Äì30.\n[93] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot‚Äôs code generation. In\nProceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering . 62‚Äì71.\n[94] Daoguang Zan, Bei Chen, et al. 2022. CERT: Continual Pre-training on Sketches for Library-oriented Code Generation.\nIn Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria,\n23-29 July 2022 , Luc De Raedt (Ed.). ijcai.org, 2369‚Äì2375. https://doi.org/10.24963/ijcai.2022/329\n[95] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Computer\nVision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 . Springer,\n818‚Äì833.\n[96] Yu Zeng et al . 2020. RECPARSER: A Recursive Semantic Parsing Framework for Text-to-SQL Task.. In IJCAI.\n3644‚Äì3650.\n[97] Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang, and Lingming Zhang. 2022. An extensive\nstudy on pre-trained models for program understanding and generation. In Proceedings of the 31st ACM SIGSOFT\nInternational Symposium on Software Testing and Analysis . 39‚Äì51.\n[98] Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. 2020. Generating adversarial examples for holding\nrobustness of source code processing models. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34.\n1169‚Äì1176.\n[99] Kechi Zhang, Ge Li, and Zhi Jin. 2022. What does Transformer learn about source code?arXiv preprint arXiv:2207.08466\n(2022).\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.\n100:24 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang\n[100] Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. 2022. Diet code is healthy: Simplifying programs\nfor pre-trained models of code. In Proceedings of the 30th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering . 1073‚Äì1084.\n[101] Haiying Zhao, Wei Zhou, Xiaogang Hou, and Hui Zhu. 2020. Double attention for multi-label image classification.\nIEEE Access 8 (2020), 225539‚Äì225550.\n[102] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning deep features for\ndiscriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2921‚Äì2929.\n[103] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan-Fang Li. 2023.\nOn Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on\nCodex. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics .\n1090‚Äì1102.\nReceived 2023-09-28; accepted 2024-04-16\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.624918520450592
    },
    {
      "name": "Programmer",
      "score": 0.6085618138313293
    },
    {
      "name": "Computer science",
      "score": 0.5485103130340576
    },
    {
      "name": "Code (set theory)",
      "score": 0.5123220682144165
    },
    {
      "name": "Sophistication",
      "score": 0.4544151723384857
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2421039640903473
    },
    {
      "name": "Programming language",
      "score": 0.21747753024101257
    },
    {
      "name": "Sociology",
      "score": 0.12508466839790344
    },
    {
      "name": "Social science",
      "score": 0.10613864660263062
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154425047",
      "name": "University of Alberta",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I219193219",
      "name": "Purdue University West Lafayette",
      "country": "US"
    }
  ],
  "cited_by": 3
}