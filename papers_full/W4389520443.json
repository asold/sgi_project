{
  "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
  "url": "https://openalex.org/W4389520443",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2396430639",
      "name": "Shijie Geng",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2180288621",
      "name": "Juntao Tan",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2101069935",
      "name": "Shuchang Liu",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2946305277",
      "name": "Zuohui Fu",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2112127489",
      "name": "Yongfeng Zhang",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2767724106",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4224315096",
    "https://openalex.org/W3212501936",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2951645301",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W4226408727",
    "https://openalex.org/W3100260481",
    "https://openalex.org/W4378942580",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4364384540",
    "https://openalex.org/W2140310134",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4376312036",
    "https://openalex.org/W4285171841",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W2963655167",
    "https://openalex.org/W4387848745",
    "https://openalex.org/W3192113933",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4323570427",
    "https://openalex.org/W2739992143",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4303648971",
    "https://openalex.org/W3092995403",
    "https://openalex.org/W3175536494",
    "https://openalex.org/W4312884055",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4360612299",
    "https://openalex.org/W3199245537",
    "https://openalex.org/W2965398302",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4280586754",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W4286527741",
    "https://openalex.org/W4386542364",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3100452485",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W2740167620",
    "https://openalex.org/W4377866382",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4385849236",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226182379",
    "https://openalex.org/W4386561866",
    "https://openalex.org/W4381573034",
    "https://openalex.org/W3093028137",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under a shared format. Additionally, we propose a parameter-efficient training method for foundation models, which involves freezing the P5 backbone and fine-tuning lightweight adapters, resulting in improved recommendation performance and increased efficiency in terms of training time and memory usage. Code and data of VIP5 are available at https://github.com/jeykigung/VIP5.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9606–9620\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nVIP5: Towards Multimodal Foundation Models for Recommendation\nShijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang\nDepartment of Computer Science, Rutgers University, NJ 08854, US\n{sg1309, juntao.tan, shuchang.syt.liu, zuohui.fu, yongfeng.zhang}@rutgers.edu\nAbstract\nComputer Vision (CV), Natural Language Pro-\ncessing (NLP), and Recommender Systems\n(RecSys) are three prominent AI applications\nthat have traditionally developed independently,\nresulting in disparate modeling and engineering\nmethodologies. This has impeded the ability\nfor these fields to directly benefit from each\nother’s advancements. With the recent devel-\nopment of foundation models, large language\nmodels have emerged as a potential general-\npurpose interface for unifying different modali-\nties and problem formulations. In light of this,\nwe propose the development of a multimodal\nfoundation model (MFM) considering visual,\ntextual, and personalization modalities under\nthe P5 recommendation paradigm, thus named\nVIP5 (Visual P5), to unify various modalities\nand recommendation tasks. This will enable the\nprocessing of multiple modalities in a shared ar-\nchitecture for improved recommendations. To\nachieve this, we introduce multimodal personal-\nized prompts to accommodate multiple modal-\nities under a shared format. Additionally, we\npropose a parameter-efficient training method\nfor foundation models, which involves freezing\nthe P5 backbone and fine-tuning lightweight\nadapters, resulting in improved recommenda-\ntion performance and increased efficiency in\nterms of training time and memory usage. Code\nand data of VIP5 are available at https://\ngithub.com/jeykigung/VIP5.\n1 Introduction\nWith rapid growth, recommender systems have\ngradually become an indispensable element in peo-\nple’s daily lives. With more time spent on the Web,\npeople reveal their interests through richer modali-\nties than before. In response to the trend, current\nrecommendation systems (Meng et al., 2020; Hou\net al., 2019; Zhang et al., 2021a, 2017) consider\nmore diverse contents when making recommenda-\ntion decisions to users.\nHistorically, the technical developments for pro-\ncessing different types of information (such as\npersonalization, visual, and textual) are mostly\nspread across different research communities. For-\ntunately, recent advances in Foundation Models\n(FMs) such as Large Language Models (LLMs)\nunfold a promising route for building general-\npurpose models and unifying diverse modalities,\nso that one single architecture can handle visual,\ntextual and personalized information at the same\ntime, enabling the possible approach towards Arti-\nficial General Intelligence (AGI) (Ge et al., 2023)\nand Artificial General Recommender (AGR) (Lin\nand Zhang, 2023). As a pioneering work, GPT-3\n(Brown et al., 2020) can perform in-context learn-\ning, enabling it to solve brand-new problems given\nfew-shot demonstration examples as prompts. Sim-\nilarly, CLIP (Radford et al., 2021; Geng et al.,\n2022d) maintains superior zero-shot generalization\nability when shifting to an out-of-distribution vi-\nsual domain if provided with appropriate prompt.\nWith more and more emergent abilities (Wei et al.,\n2022b) revealed in foundation models, they be-\ncome not only a popular backbone to finetune\ndownstream tasks (Alayrac et al., 2022; Sanh et al.,\n2022; Wei et al., 2022a) but also an effective train-\ning scheme for unifying multiple modalities in a\nshared interface (Wang et al., 2022; Chen et al.,\n2022; Cho et al., 2021; Jiang et al., 2022). Follow-\ning the trend in language and vision domains, P5\n(Geng et al., 2022c) and M6-Rec (Cui et al., 2022)\nput forward the concept of personalized foundation\nmodels for recommendation and propose to pre-\ntrain LLMs on instructional prompts to accommo-\ndate various recommendation tasks under a shared\nmodel and training objective.\nWhile there are large models for language (Raf-\nfel et al., 2020; Brown et al., 2020), vision (Yu\net al., 2022; Radford et al., 2021), graphs (Ye et al.,\n2023; Geng et al., 2022b) and recommendation\n(Geng et al., 2022c; Cui et al., 2022) domains\n9606\nI ﬁnd the purchase history list of user_1035 :\nVIP5\n4406\nPick the most suitable item from the following list and recommend to user_251 :\n2317\nSequential Recommendation\nDirect Recommendation\nExplanation Generation\nBased on the feature word exercises , generate an explanation for user_45 about this product : Black Mountain Products \nResistance Band Set with Door Anchor, Ankle Strap, Exercise Chart, and Resistance Band Carrying Case\nGood for those small \nexercises that one can't \ndo with freeweights\n, 4541\n , 11615\n7162\n , 10964\n, 5709 , 326\n , 9910\n , 2317…\n4011 -> 3531 \n -> 5632 \n -> 5603 \n-> 5634  I wonder what is the next item to recommend to the user . Can you help me decide ?  -> 5633 \nFigure 1: An example task scope of VIP5 covering three popular recommendation tasks. Based on multimodal personalized\nprompts (left) that interleave language and visual tokens, VIP5 is able to transfer all task and all modalities into a unified sequence\nformat, and generates target outputs (right) according to certain task descriptions. VIP5 treats large language models as a fixed\ngeneral-purpose interface and finetunes extra visual and language processing layers to achieve the ability for handling various\nrecommendation tasks.\nseparately, in this work, we take one step further\nand aim to unify the above foundation models to\njointly process multi-modality information sources\nfor personalization and recommendation. To this\nend, we follow the “Pretrain, Personalized Prompt\nand Predict Paradigm (P5)” for recommendation\n(Geng et al., 2022c; Xu et al., 2023) and propose\na Multimodal Foundation Model (MFM) named\nVIP5 (Visual P5), which provides the following\nadvantages for recommender systems: 1) VIP5\nprovides multimodal personalized prompts for sup-\nporting all modalities’ connections to the recom-\nmendation foundation model. Specifically, to con-\nstruct multimodal personalized prompts, VIP5 em-\nploys a mapping network to transfer features from\nother modalities into the corresponding tokens. By\nthis step, multimodal features are projected to the\nsame manifold space of the backbone foundation\nmodel. 2) The VIP5 framework provides the abil-\nity of Parameter-efficient tuning rather than Pre-\ntraining in existing recommendation foundation\nmodels such as P5 (Geng et al., 2022c). Different\nfrom the pre-training step of P5 that updates all\nthe parameters in the backbone foundation model\n– which is impractical when the size of foundation\nmodel grows explosively – VIP5 only finetunes a\nsmall proportion of extra lightweight adapter mod-\nules during training while maintaining the large\nlanguage model backbone fixed. 3) With the ability\nof multi-modality learning and parameter-efficient\ntuning, VIP5 further improves the performance of\nrecommendation foundation models with both less\ntraining time and less memory usage, making it\neasier to train and deploy foundation models for\nrecommendation. Overall, our key contributions\nare outlined as follows:\n• We propose VIP5 framework to unify CV , NLP,\nand RecSys foundation models and facilitate rec-\nommendation with multimodal information.\n• We introduce multimodal personalized prompts\nto adapt multi-modality information into a shared\ntokenized space with textual, visual and person-\nalization inputs.\n• We develop adapter-based parameter-efficient\ntuning for VIP5 to achieve a better recommenda-\ntion performance and training efficiency.\n• Based on the experimental results, VIP5 beats\nstrong baselines on three task groups while sav-\ning substantial training time and memory usage.\n2 Related Work\nPrompt Learning. Prompt learning (Liu et al.,\n2021) gradually emerges as a popular paradigm\nto control the behavior of large language models\nsince it can effectively adapt a pretrained model to\ndownstream tasks in either zero-shot or few-shot\nstyle. The success of GPT series (Radford et al.,\n2019; Brown et al., 2020) attracts the first wave\nof interests on the topic. The in-context learning\ncapability of GPT-3 (Brown et al., 2020) inspires\nmany efforts on automatic prompt search or genera-\ntion (Gao et al., 2021; Jiang et al., 2020; Shin et al.,\n2020; Zhang et al., 2022) to achieve higher-quality\ndiscrete prompts. However, it is naturally hard to\noptimize these approaches in a discrete space. To\nsolve this issue, soft prompt based approaches such\nas Prefix-Tuning (Li and Liang, 2021), Prompt-\nTuning (Lester et al., 2021), CoOp (Zhou et al.,\n2022), and Visual-Prompt Tuning (Jia et al., 2022)\n9607\nare proposed to leverage additional trainable con-\ntinuous embeddings as prefix to conduct finetuning\non downstream tasks. While achieving better scal-\nability and generalization ability, the learned soft\nprompts are more difficult to interpret than discrete\nprompts. To accommodate all above merits, in-\nstruction prompts that directly describe different\ntasks via natural language instructions are adopted\nby a lot of methods (Weller et al., 2020; Wei et al.,\n2022a; Sanh et al., 2022; Aribandi et al., 2022;\nMishra et al., 2022), highlighting significant im-\nprovements on unseen tasks.\nLarge Recommendation Models. Motivated by\nthe success of Large Language Models (LLMs), the\nRecSys community started to pay more attention\nto recommendation model’s generalization ability\nand transferability (Li et al., 2023b). For example,\ninspired by the prompt learning paradigm, PETER\n(Li et al., 2021) and PEPLER (Li et al., 2022) pro-\nposes to learn personalized continuous prompts to\nrepresent user and item IDs and generates natural\nlanguage explanations to justify recommendations.\nIn contrast, M6-Rec (Cui et al., 2022) converts all\nuser behavior information to plain text sequences\nand feeds them into a Transformer encoder, and\nthen designs a task-specific training loss for down-\nstream tasks and finetuning. Apart from previous\nefforts, P5 (Geng et al., 2022c) and OpenP5 (Xu\net al., 2023) leverage not only instruction-based\nfinetuning on LLMs to represent personalized fields\nfor users and items but also unifies various tasks\nvia natural language instructions. Hence, P5 is\nable to unify various recommendation tasks into\na shared encoder-decoder architecture and a joint\ntraining objective. P5-ID (Hua et al., 2023b) fur-\nther explores different item ID creation methods for\nLLM-based recommendation models, such as se-\nquential indexing, collaborative indexing, semantic\nindexing, and hybrid indexing.\nMultimodal Recommendation. Current ap-\nproaches to multimodal recommendation can be\ndivided into three categories. The most common\nusage is to leverage multimodal content as side\ninformation to assist recommendation decisions.\nFor example, VBPR (He and McAuley, 2016) pro-\nposes using visual features to supplement user feed-\nback and improve matching-based recommenda-\ntions. PiNet (Meng et al., 2020) proposes to cover\nmore personalized visual preferences about users.\nIt simultaneously learns heterogeneous visual fea-\ntures with semantic and collaborative information\nand then fuses different visual information through\na dual-gating module. JRL (Zhang et al., 2017) pro-\nposes Joint Representation Learning over multiple\nmodalities for improved recommendation. Another\nstream of approaches focus on providing recom-\nmendations along with correlated visual explana-\ntions. These methods usually work in domains\nwhere visual information is important to user be-\nhavior patterns, such as fashion (Hou et al., 2019;\nVerma et al., 2020; Chen et al., 2019), travel (Geng\net al., 2022a), and food (Meng et al., 2020). Fur-\nthermore, several recent approaches have been pro-\nposed to discover the rich intra-item and inter-item\nsemantic structures from multimodal contents to\nfacilitate better item representations and thus en-\nhance recommendation performances (Zhang et al.,\n2021b,a; Deldjoo et al., 2022).\n3 VIP5 Paradigm with Multimodal\nPersonalized Prompts\nWe introduce the proposed VIP5 paradigm in this\nsection. In Section 3.1, we incorporate multimodal\nsignals into personalized prompts. In Section 3.2,\nwe elaborate how to conduct parameter-efficient\ntuning with adapters based on multimodal person-\nalized prompts.\n3.1 Multimodal Personalized Prompts\nA personalized prompt includes personalized fields\nfor users and items (Geng et al., 2022c; Li et al.,\n2022, 2023a), with formats ranging from ID num-\nbers to detailed descriptions. In our work, we de-\nvelop foundation models as a general-purpose in-\nterface to connect available modalities that could\nbe helpful for eliciting user preferences. To facili-\ntate this end, we propose “multimodal personalized\nprompts”. Technically, we consider textual, visual,\nand personalization information as three example\nmodalities in our multimodal personalized prompts\n(Figure 2).\nGiven an item image I ∈ RH×W×3, where\nH and W are the image height and width, we\nfirst adopt a visual encoder such as CLIP image\nbranch (Radford et al., 2021) to extract its feature\nx ∈Rdv , where dv represents the visual feature\ndimension. To connect the image feature to other\ntext-based tokens in a personalized prompt, as illus-\ntrated in Figure 2(c), we design a mapping network\nf with two linear layers to transfer the original im-\nage feature to kimage tokens: p1,...,p k = f(x).\nThen we append the image tokens to their corre-\n9608\nMultimodal Personalized Prompt (b)\n Pick the most suitable item for user_251 :\n , 4541 , 2317\n7162 CLIP Image \nEncoder\n<i1> <i2>\nMapping\nNetwork\n!\n❄\n(c)\nVisual \nTokens\n<i3> <i4>\n<i5> <i6>\nT oken\nPosition\nWhole-word\npick the most suitable item for _\n<p1> <p2> <p3> <p4> <p5> <p6> <p7> <p8> <p9> <p10> <p11> <p12> <p13> <p14> <p15>\n<w1>\nCategory <c—text>\n<i1>\n<c-visual>\n<i2>\n(a)\n❄\nP5 Encoder\n!\nAdapter\n ✕ L\n23<s>\n1723\n!\nAdapter\n❄\nP5 Decoder\n ✕ L\nuser 25 1 : 71 62 , 45 41 <i5> <i6>23 17\n<p16> <p17> <p18>\n<i3> <i4>\n<p19> <p20>\n,\n<p21> <p22> <p23> <p24> <p25>\n<w3> <w4> <w5> <w6> <w7> <w8> <w9> <w10> <w11> <w12> <w13> <w14> <w15> <w16> <w17> <w18> <w19><w2>\n<c—text> <c-visual> <c—text> <c-visual>\n+ + + + + + + + + + + + + + + + + + + + + + + + +\n+ + + + + + + + + + + + + + + + + + + + + + + + +\n+ + + + + + + + + + + + + + + + + + + + + + + + +\nVIP5 Model\nArchitecture\nFigure 2: An illustration of the VIP5 framework. VIP5 is built on an encoder–decoder Transformer model that takes in textual\ninputs as well as image inputs to produce responses or make recommendation decisions. In the figure, a fire symbol represents\ntraining with parameter update while a snow flake symbol stands for the frozen parameters.\nsponding item tokens to construct a multimodal\npersonalized field M:\nM: w1 ···wm \nitem tokens\n,p1,...,p k  \nimage tokens\n. (1)\nWe create a collection of 29 multimodal personal-\nized prompts covering three important task families\n– sequential recommendation, direct recommenda-\ntion, and explanation. The full list of prompts is\nprovided in Figure 8, 9, and 10. Based on the collec-\ntion of multimodal personalized prompts, we use\nthe multimodal personalized field Mas in Eq.(1)\nto substitute the item field in the prompt. It is worth\nnoting that the prompts for sequential and direct\nrecommendation usually contain more than one\nmultimodal personalized fields.\n3.2 Parameter-efficient Tuning with Adapters\nOur VIP5 framework is shown in Figure 2(a). For\ntokenized multimodal sequence S, we first apply\nposition encoding Pand whole-word embedding\nWon S to help the model better recognize the\nabsolute positions of input tokens and important\nuser/item fields (e.g., “user_251” is split into 4 sep-\narate tokens [“user”, “_”, “25”, “1”], but they share\nthe same whole-word embedding “⟨w7⟩”). Besides,\nwe adopt an additional category embedding Cto\nidentify whether a token is textual or visual. Af-\nterwards, we feed the resulting sequence into the\nL-layered text encoder Eand decoder Dmodules.\nBesides multimodal personalized prompts, we\npropose parameter-efficient tuning using adapters\nfor computation- and memory-efficient training.\nBy inserting adapters into the foundation model\nbackbone, freezing its parameters, and updating\nlightweight adapter modules, this strategy largely\nreduces trainable parameters, decreasing training\ntime and memory usage. Tuning few additional\nparameters addresses efficiency concerns when in-\ncorporating visual tokens into text-based person-\nalized prompts. More importantly, fine-tuning the\nentire backbone may cause over-fitting for easier\ntasks, whereas parameter-efficient tuning can lever-\nage both training efficiency and the power of large\nfoundation models.\nFormally, if we denote the input sequence for the\ni-th layer of text encoder as Si = [s1,··· ,sn], in\ntraditional Transformer,Siwill go through one self-\nattention block and a feed-forward network. While\nin VIP5, we insert adapters (Houlsby et al., 2019;\nSung et al., 2022) in both the self-attention block\nand the feed-forward network, the exact position is\nafter each module and before the LayerNorm (Ba\net al., 2016). The whole process can be written as:\nSi+1 = A2\n(\nFFN\n(\nA1\n(\nAttention\n(\nSiWQ, SiWK, SiWV\n))))\n,\n(2)\nwhere WQ,WK,WV ∈Rd×dh are weight matrices\nfor projecting query, key, and value, respectively,\ndh = d/his the dimensionality for each head. The\nAttention function is defined as:\nAttention(Q,K,V) = softmax\n(QK⊤\n√dh\n)\nV.\n(3)\nBesides, FFN is a feed-forward module consisting\nof two fully-connected layers. A1 and A2 are the\nfeature adapters after the self-attention and feed-\nforward network. They are both bottleneck fully-\nconnected layers with an activation function in be-\ntween. We can represent these adapters as:\nA= fup (σ(fdown(Si))) +Si, (4)\n9609\nwhere fdown and fup are the down-sampling and\nup-sampling layers of an adapter, and σ is the\nGELU activation function (Hendrycks and Gim-\npel, 2016). Similar to text encoder, we also adopt\nadapters between the cross-attention block and its\nLayerNorm layer inside text decoder.\nVIP5 utilizes the conditional token generation\nloss for all three recommendation tasks. After en-\ncoding the input multimodal personalized prompts\ninto a contextualized latent sequence with E, the\ntext decoder Dautoregressively predict next tokens\nconditioned on the already generated tokens y<j\nand the input text t. In summary, VIP5 adopts the\nfollowing training objective to perform parameter-\nefficient tuning with adapters:\nLθ = −\n|y|∑\nj=1\nlog Pθ(yj |y<j,t) . (5)\nAfter training, we perform inference with VIP5\nbased on given multimodal personalized prompts.\nFor sequential and direct recommendation task\ngroups, we create a list of candidate items for rec-\nommendation via beam search. For explanation\ntask group, we simply apply greedy decoding for\ntext generation.\n4 Experiments\nIn this section, we provide the performance com-\nparison between the VIP5 framework and repre-\nsentative approaches for different task groups. We\nconduct extensive experiments and ablation studies\nto explore the following research questions:\n• RQ1: Does the proposed parameter-efficient\nVIP5 framework perform well when compared\nwith baseline methods across the three task\ngroups?\n• RQ2: When conducting parameter-efficient tun-\ning, which parts should we insert adapters and\nperform finetuning? In addition, will different\nadapter reduction rates affect the performance\nand efficiency of VIP5?\n• RQ3: Does visual information play an important\nrole in multimodal personalized prompts? What\nif we change the number of image tokens and the\ntype of visual encoder?\n4.1 Experimental Setups\nDatasets. We employ four real-world datasets\ncollected from Amazon platform for experiments\nDataset Clothing Sports Beauty Toys\n#Users 39,387 35,598 22,363 19,412\n#Items 23,033 18,357 12,101 11,924\n#Reviews 278,677 296,337 198,502 167,597\n#Photos 22,299 17,943 12,023 11,895\n#Sparsity (%) 0.0307 0.0453 0.0734 0.0724\nTable 1: Detailed statistics of the datasets used in our paper.\nand ablation studies, namely Clothing, Shoes &\nJewelry, Sports & Outdoors, Beauty, and Toys &\nGames. These datasets 1 contain user purchase\nrecords, reviews, item descriptions, and images.\nTable 1 offers detailed dataset statistics.\nTasks and Metrics. In this paper, we cover three\nrecommendation task groups: A) sequential rec-\nommendation, B) direct recommendation, and C)\nexplanation generation. We follow the same pre-\nprocessing steps and train/validation/test splits as\nin (Geng et al., 2022c). For sequential recommen-\ndation, the last and second last items in each user’s\ninteraction history are adopted as test and valida-\ntion ground-truths, with remaining items as training\ndata. For direct recommendation, we use sequen-\ntial recommendation’s train/validation/test splits\nto generate 100 candidate lists as in (Zhao et al.,\n2022). For explanation generation, we adopt an\n8:1:1 random split and extract rating explanations\nusing the Sentires library (Zhang et al., 2014).\nWe evaluate sequential and direct recommenda-\ntion task groups using Hit Ratio (HR@k) and Nor-\nmalized Discounted Cumulative Gain (NDCG@k),\nwhile explanation generation tasks are assessed us-\ning text generation metrics like BLEU and ROUGE.\nIn all tables, bold numbers indicate the best ap-\nproach for each metric.\nImplementation Details. We utilize the pre-\ntrained P5-small checkpoint as VIP5’s backbone,\nas it often outperforms P5-base (Geng et al., 2022c).\nVIP5’s encoder and decoder consist of 6 Trans-\nformer blocks, a 512-dimension embedding size,\nand 8 attention heads. To process visual informa-\ntion, we use CLIP’s (Radford et al., 2021) image\nbranch as VIP5’s visual encoder and pre-extract\nimage features. Similar to P5, we employ the Sen-\ntencePiece (Sennrich et al., 2016) tokenizer with a\n32,100 vocabulary size to generate sub-word input\ntokens. By default, the mapping network serves\nas the image tokenizer in VIP5 and the number of\nimage tokens is set to 2, while the adapters have a\nreduction factor of 8 for the bottleneck dimension.\nFor each task group, all multimodal personal-\n1http://jmcauley.ucsd.edu/data/amazon/links.html\n9610\nMethods Sports Beauty\nHR@5 NDCG@5 HR@10 NDCG@10 HR@5 NDCG@5 HR@10 NDCG@10\nHGN 0.0189 0.0120 0.0313 0.0159 0.0325 0.0206 0.0512 0.0266\nSASRec 0.0233 0.0154 0.0350 0.0192 0.0387 0.0249 0.0605 0.0318\nS3-Rec 0.0251 0.0161 0.0385 0.0204 0.0387 0.0244 0.0647 0.0327\nP5(A-3) 0.0272 0.0169 0.0361 0.0198 0.0503 0.0370 0.0659 0.0421\nVIP5(A-3) 0.0412 0.0345 0.0475 0.0365 0.0556 0.0427 0.0677 0.0467\nP5(A-9) 0.0258 0.0159 0.0346 0.0188 0.0490 0.0358 0.0646 0.0409\nVIP5(A-9) 0.0392 0.0327 0.0456 0.0347 0.0529 0.0413 0.0655 0.0454\nMethods Clothing Toys\nHR@5 NDCG@5 HR@10 NDCG@10 HR@5 NDCG@5 HR@10 NDCG@10\nHGN 0.0107 0.0071 0.0175 0.0092 0.0321 0.0221 0.0497 0.0277\nSASRec 0.0107 0.0066 0.0194 0.0095 0.0463 0.0306 0.0675 0.0374\nS3-Rec 0.0076 0.0045 0.0135 0.0063 0.0443 0.0294 0.0700 0.0376\nP5(A-3) 0.0478 0.0376 0.0554 0.0401 0.0655 0.0570 0.0726 0.0593\nVIP5(A-3) 0.0603 0.0564 0.0632 0.0573 0.0662 0.0577 0.0749 0.0604\nP5(A-9) 0.0455 0.0359 0.0534 0.0385 0.0631 0.0547 0.0701 0.0569\nVIP5(A-9) 0.0569 0.0531 0.0597 0.0540 0.0641 0.0556 0.0716 0.0580\nTable 2: Performance comparison on sequential recommendation.\nMethods Sports Beauty\nHR@1 HR@5 NDCG@5 HR@10 NDCG@10 HR@1 HR@5 NDCG@5 HR@10 NDCG@10\nBPR-MF 0.0314 0.1404 0.0848 0.2563 0.1220 0.0311 0.1426 0.0857 0.2573 0.1224\nBPR-MLP 0.0351 0.1520 0.0927 0.2671 0.1296 0.0317 0.1392 0.0848 0.2542 0.1215\nVBPR 0.0262 0.1138 0.0691 0.2060 0.0986 0.0380 0.1472 0.0925 0.2468 0.1245\nP5(B-5) 0.0574 0.1503 0.1050 0.2207 0.1276 0.0601 0.1611 0.1117 0.2370 0.1360\nVIP5(B-5)0.0606 0.1743 0.1185 0.2539 0.1441 0.0580 0.1598 0.1099 0.2306 0.1327\nP5(B-8) 0.0567 0.1514 0.1049 0.2196 0.1269 0.0571 0.1566 0.1078 0.2317 0.1318\nVIP5(B-8)0.0699 0.1882 0.1304 0.2717 0.1572 0.0615 0.1655 0.1147 0.2407 0.1388\nMethods Clothing Toys\nHR@1 HR@5 NDCG@5 HR@10 NDCG@10 HR@1 HR@5 NDCG@5 HR@10 NDCG@10\nBPR-MF 0.0296 0.1280 0.0779 0.2319 0.1112 0.0233 0.1066 0.0641 0.2003 0.0940\nBPR-MLP 0.0342 0.1384 0.0858 0.2327 0.1161 0.0252 0.1142 0.0688 0.2077 0.0988\nVBPR 0.0352 0.1410 0.0877 0.2420 0.1201 0.0337 0.1294 0.0808 0.2199 0.1098\nP5(B-5) 0.0320 0.0986 0.0652 0.1659 0.0867 0.0418 0.1219 0.0824 0.1942 0.1056\nVIP5(B-5)0.0481 0.1287 0.0890 0.1992 0.1116 0.0428 0.1225 0.0833 0.1906 0.1051\nP5(B-8) 0.0355 0.1019 0.0688 0.1722 0.0912 0.0422 0.1286 0.0858 0.2041 0.1099\nVIP5(B-8)0.0552 0.1544 0.1058 0.2291 0.1297 0.0433 0.1301 0.0875 0.2037 0.1110\nTable 3: Performance comparison on direct recommendation.\nized prompts except the last are used to train VIP5.\nPrompts A-3/A-9, B-5/B-8, and C-3/C-12 are used\nfor evaluation purpose, with A-3, B-5, C-3 testing\nmodel performance under seen prompts and A-9,\nB-8, C-12 under zero-shot unseen prompts. VIP5\nis trained for 10 epochs with a batch size of 36 on\nfour NVIDIA A100 GPUs, using a learning rate\nof 1 ×10−3 and AdamW (Loshchilov and Hut-\nter, 2018) optimizer. As multimodal personalized\nprompts contain more image tokens, we set input\ntoken’s maximum length to 1024. During infer-\nence, beam size B is set to 20 for sequential and\ndirect recommendation tasks that require generat-\ning a list of candidate items.\nComparison Baselines. To make performance\ncomparisons, we consider a collection of baselines\nfor each task group. For all three task groups, we\ninclude P5 (Geng et al., 2022c) as a baseline to com-\npare with existing foundation models for recom-\nmendation. P5 pre-trains all tasks with predefined\ntext-based personalized prompts via autoregressive\nlanguage modeling loss and performs inference\nusing greedy decoding or beam search to gener-\nate outputs. Additionally, we compare with task-\nspecific approaches. For sequential recommenda-\ntion, baseline methods include HGN (Ma et al.,\n2019), SASRec (Kang and McAuley, 2018), and\nS3-Rec (Zhou et al., 2020). For direct recommen-\ndation, we compare with BPR-MF (Rendle et al.,\n2009), BPR-MLP, and VBPR (He and McAuley,\n2016). For explanation generation, we inherent\nthe baselines of P5: Attn2Seq (Dong et al., 2017),\nNRT (Li et al., 2017), and PETER (Li et al., 2021).\nWhen providing a hint feature word as input, PE-\nTER becomes its variant PETER+, which we also\nuse as an explanation generation baseline.\n4.2 Performance on Task Groups (RQ1)\nIn this section, we conduct parameter-efficient tun-\ning for VIP5 on multimodal personalized prompts\n9611\nMethods Sports Beauty\nBLUE4 ROUGE1 ROUGE2 ROUGEL BLUE4 ROUGE1 ROUGE2 ROUGEL\nAttn2Seq 0.5305 12.2800 1.2107 9.1312 0.7889 12.6590 1.6820 9.7481\nNRT 0.4793 11.0723 1.1304 7.6674 0.8295 12.7815 1.8543 9.9477\nPETER 0.7112 12.8944 1.3283 9.8635 1.1541 14.8497 2.1413 11.4143\nP5(C-3) 0.6212 11.8539 2.0707 9.0189 1.0230 14.3242 2.0761 10.9085\nVIP5(C-3) 1.0639 14.8628 2.1012 11.1059 1.2850 17.7492 2.3482 12.9170\nPETER+ 2.4627 24.1181 5.1937 18.4105 3.2606 25.5541 5.9668 19.7168\nP5(C-12) 1.3144 22.9182 4.9976 17.1976 1.6313 24.6267 4.9623 18.6423\nVIP5(C-12)2.3003 24.4887 5.5057 18.6610 2.8390 26.0513 6.0159 20.4387\nMethods Clothing Toys\nBLUE4 ROUGE1 ROUGE2 ROUGEL BLUE4 ROUGE1 ROUGE2 ROUGEL\nAttn2Seq 0.6296 11.4588 1.2558 9.0429 1.6238 13.2245 2.9942 10.7398\nNRT 0.4599 10.1480 0.9720 8.2434 1.9084 13.5231 3.6708 11.1867\nPETER 0.7204 12.1836 1.3912 9.7084 1.9861 14.2716 3.6718 11.7010\nP5(C-3) 0.7569 12.2833 1.8116 9.6023 1.4522 12.6100 3.8144 10.1450\nVIP5(C-3) 1.1904 14.1685 2.0308 10.8488 2.3241 15.30063.6590 12.0421\nPETER+ 3.6204 28.4342 7.7994 22.4059 4.7919 28.3083 9.4520 22.7017\nP5(C-12) 1.8811 27.7922 7.3203 21.5462 2.6216 27.8984 9.0076 21.6136\nVIP5(C-12)3.2581 28.9059 8.5168 22.8807 3.9293 28.9225 9.5441 23.3148\nTable 4: Performance comparison on explanation generation (numbers are in percentage %).\nA-9\nNDCG@5\n0.00\n0.02\n0.04\n0.05\n0.07\nClothing Toys Beauty Sports\nText Multimodal\nB-8\nNDCG@5\n0.00\n0.03\n0.05\n0.08\n0.10\nClothing Toys Beauty Sports\nText Multimodal\nC-12\nBLUE4\n0.00\n0.50\n1.00\n1.50\n2.00\nClothing Toys Beauty Sports\nText Multimodal\n1\nA-9\nNDCG@5\n0.00\n0.02\n0.04\n0.05\n0.07\nClothing Toys Beauty Sports\nText Multimodal\nText Multimodal\nClothing 0.0529 0.0533\nToys 0.0575 0.0564\nBeauty 0.0301 0.0307\nSports 0.0242 0.0240\nB-8\nNDCG@5\n0.00\n0.03\n0.05\n0.08\n0.10\nClothing Toys Beauty Sports\nText Multimodal\nText Multimodal\nClothing 0.0621 0.0706\nToys 0.0569 0.0661\nBeauty 0.0700 0.0787\nSports 0.0719 0.0835\nC-12\nBLUE4\n0.00\n0.50\n1.00\n1.50\n2.00\nClothing Toys Beauty Sports\nText Multimodal\nText Multimodal\nClothing 1.1135 1.2577\nToys 1.3959 1.6810\nBeauty 0.7701 0.8737\nSports 0.6498 0.7669\n4\nA-9\nNDCG@5\n0.00\n0.02\n0.04\n0.05\n0.07\nClothing Toys Beauty Sports\nText Multimodal\nB-8\nNDCG@5\n0.00\n0.03\n0.05\n0.08\n0.10\nClothing Toys Beauty Sports\nText Multimodal\nC-12\nBLUE4\n0.00\n0.50\n1.00\n1.50\n2.00\nClothing Toys Beauty Sports\nText Multimodal\n1\nFigure 3: Performance comparison between text-based prompt and multimodal prompt.\nfrom all the three task groups. For each task group,\nwe select one seen and one unseen prompt for eval-\nuation. Performance comparisons with baselines\nare presented in Table 2, 3, and 4.\nSequential Recommendation. As shown in Ta-\nble 2, we adopt Prompt A-3 and Prompt A-9 to\nevaluate the performances of different approaches.\nFrom the table, we can see that VIP5 is able to\nachieve better performances than all sequential rec-\nommendation baselines on all the four experiment\ndatasets, among which a relatively large gap can be\nobserved on Sports and Clothing datasets. The re-\nsults show that our parameter-efficient tuning strat-\negy works effectively on the sequential recommen-\ndation task group.\nDirect Recommendation. For the direction rec-\nommendation task group, we evaluate different\nmethods using Prompt B-5 and Prompt B-8 as input\nmultimodal personalized prompts. Table 3 presents\nthe performance comparison, showing VIP5 out-\nperforming all baselines on Sports. While VIP5\nachieves marginally lower HR@10 onToys, Beauty,\nand Clothing datasets, it still surpasses all baselines\non other metrics.\nExplanation Generation. Table 4 illustrates the\nperformance comparison for explanation genera-\ntion task group. In the table, Prompt C-12 are ap-\nplied to evaluate all methods under hint feature\nword setup, while Prompt C-3 targets at direct\nexplanation generation with only the given user–\nitem pair. The experimental results indicate that\nVIP5 outperforms other baselines when equipped\nwith the multimodal personalized Prompt C-3. For\nPrompt C-12, VIP5 achieves superior performances\nthan P5 across all datasets in terms of all metrics\nand has the highest ROUGE1, ROUGE2, ROUGEL\nscores of the four experimental datasets.\n4.3 Parameter-efficient Tuning (RQ2)\nIn this section, we discuss how to conduct\nparameter-efficient tuning with adapters to show\nthe impact of different tuning choices.\nHow to conduct parameter-efficient tuning. We\nfirst try three fine-tuning approaches: 1) inserting\nadapters in Transformer’s self-attention blocks and\nonly fine-tuning them, 2) fine-tuning adapters in\nboth self-attention and cross-attention blocks, 3)\nfully fine-tuning all parameters. For this ablation,\nwe conduct experiments on Toys with ResNet-101\nvisual features, a reduction rate of 8, and a sin-\ngle image token in multimodal prompt. Figure 4\ndemonstrates that fine-tuning adapters in all atten-\ntion blocks is necessary to achieve better (Prompt\nC-12) or comparable (Prompt A-9 & B-8) results\n9612\nNDCG@5\n0.01\n0.02\n0.03\n0.05\n0.06\nA-9\nSelf-Attn Self- & Cross-Attn Full\nNDCG@5\n0.03\n0.04\n0.05\n0.06\n0.07\nB-8\nBLUE4\n1.40\n1.48\n1.55\n1.63\n1.70\nC-12\n1\nFigure 4: Performance comparison among only activat-\ning adapters in self-attention blocks, both self-attention\nand cross-attention blocks, and full finetuning.\nwith full fine-tuning. Moreover, Table 5 shows\nthat the former saves 21.2% time and 18.1% mem-\nory usage during training compared to the latter,\nhighlighting VIP5’s effectiveness and efficiency.\nOn adapter reduction rate. The reduction rate is\nan important hyper-parameter for adapters. When\ndecreasing the reduction rate, the hidden dimension\nof bottleneck layers will increase correspondingly,\nresulting in a higher percentage of trainable param-\neters. We select five different values of reduction\nrates and perform all experiments with ResNet-101\nvisual features and a single image token in multi-\nmodal prompt. In Figure 5, we can see that 4 and 8\nare suitable reduction rates for all the 3 task groups.\n4.4 Ablation on Visual Components (RQ3)\nIn this section, we aim to explore whether visual\ninformation matters for different task groups. We\nalso estimate the influence of the number of image\ntokens and the visual encoder type.\nText-based vs. multimodal personalized\nprompts. To compare text-based and multimodal\npersonalized prompts, we set the number of image\ntokens to 0 and 1, respectively, and conduct experi-\nments on all four datasets with a reduction rate of\n8 and ResNet-101 visual features. Figure 3 shows\nthat introducing visual signals into personalized\nprompts improves all datasets for direct recommen-\ndation task group (Prompt B-8). This is in line with\nour expectation that an item’s visual appearance\nsignificantly influences people’s choices. For se-\nquential recommendation, visual information does\nnot bring obvious performance improvements, indi-\ncating that the purchase sequence itself is more sig-\nnificant for predicting next items. For explanation\ngeneration, visual information positively impacts\nall datasets, especially for the Toys dataset.\nOn the number of image tokens. To examine the\ninfluence of the number of image tokens, we select\nfour different numbers (1, 2, 3, 5) and conduct ad-\nditional experiments on Toys with a reduction rate\nof 8 and ResNet-101 visual features. According to\nNDCG@5\n0.02\n0.03\n0.04\n0.05\n0.06\nA-9\nDown 32 Down 16 Down 8 Down 4 Down 2\nDown 32 Down 16 Down 8 Down 4 Down 2\nA-9 0.0438 0.0555 0.0564 0.0594 0.0562\nNDCG@5\n0.04\n0.05\n0.05\n0.06\n0.07\nB-8\nDown 32 Down 16 Down 8 Down 4 Down 2\nB-8 0.0599 0.0650 0.0661 0.0648 0.0605\nBLUE4\n0.00\n0.50\n1.00\n1.50\n2.00\nC-12\nDown 32 Down 16 Down 8 Down 4 Down 2\nC-12 1.1765 1.4626 1.6810 1.8898 1.1040\n2\nFigure 5: Ablation on the downsample reduction rate.\nNDCG@5\n0.05\n0.05\n0.06\n0.06\nA-9\n1 Img Token 2 Img Tokens 3 Img Tokens 5 Img Tokens\n1 Img Token 2 Img Tokens 3 Img Tokens 5 Img Tokens\nA-9 0.0564 0.0563 0.0562 0.0578\nNDCG@5\n0.06\n0.06\n0.07\n0.07\n0.07\nB-8\n1 Img Token 2 Img Tokens 3 Img Tokens 5 Img Tokens\nB-8 0.0661 0.0671 0.0649 0.0687\nBLUE4\n0.00\n0.50\n1.00\n1.50\n2.00\nC-12\n1 Img Token 2 Img Tokens 3 Img Tokens 5 Img Tokens\nC-12 1.6810 1.8421 1.6104 1.4537\n4\nFigure 6: Ablation on the image token number.\nNDCG@5\n0.03\n0.04\n0.05\n0.05\n0.06\nA-9\nRN50 RN101 ViTB/32 ViTB/16 ViTL/14\nRN50 RN101 ViTB/32 ViTB/16 ViTL/14\nA-9 0.0569 0.0564 0.0582 0.0559 0.0561\nNDCG@5\n0.03\n0.04\n0.05\n0.07\n0.08\nB-8\nRN50 RN101 ViTB/32 ViTB/16 ViTL/14\nB-8 0.0647 0.0661 0.0677 0.0745 0.0761\nBLUE4\n0.00\n0.45\n0.90\n1.35\n1.80\nC-12\nRN50 RN101 ViTB/32 ViTB/16 ViTL/14\nC-12 1.5510 1.6810 1.5947 1.4680 1.2945\n2\nFigure 7: Ablation on the visual encoder type.\nFigure 6, enabling 5 image tokens in multimodal\npersonalized prompt achieves the best performance\non Prompt A-9 and Prompt B-8, while 2 image\ntokens perform the best for Prompt C-12. However,\nlonger visual prompt results in more training time\n(e.g., 5 image tokens take 60.8% more time than\n2 image tokens). Therefore, we choose 2 image\ntokens as default setting considering the trade-off.\nOn visual encoder type. The visual encoder type\nis another factor influencing multimodal person-\nalized prompt representation. We explore vari-\nous CLIP visual branch architectures: ResNet50,\nResNet101, ViT-B/32, ViT-B/16, ViT-L/14 (in an\nascending order of visual encoder ability according\nto CLIP (Radford et al., 2021)). All experiments\nare performed on Toys with a reduction rate of 8\nand a single image token. The results are reported\nin Figure 7. Similar to our previous conclusions, vi-\nsual information matters most for direct recommen-\ndation, with continuous performance gains when\nusing better visual encoders. However, for sequen-\ntial recommendation and explanation generation,\nbetter visual encoders do not always improve per-\nformance. This is most likely because the purchase\nsequence is more crucial than visual information\nfor predicting the next item in sequential recom-\nmendation, leading to similar performances under\ndifferent visual encoders. For explanation genera-\n9613\nMethods/MetricsTime/Epoch Trainable Param. Memory Usage\nSelf-Attn 10.55 2.97 27.4Self- & Cross-Attn 11.10 3.58 29.0Full (P5) 14.08 100 35.6\nTable 5: Comparison of different training strategies in terms\nof trainable parameter (%), training time (min), and memory\nusage (GB) on the Toys dataset.\ntion, hint words significantly influence generated\nsentences, and the compatibility between hint word\nand visual embedding varies across different visual\nencoders. However, VIP5 is still better than the\nbest baseline under most visual encoders.\n5 Conclusions\nThis paper presents VIP5, a parameter-efficient\nmultimodal foundation recommendation model uni-\nfying vision, language, and personalization in-\nformation. We design multimodal personalized\nprompts to integrate visual signals with text and\npersonalization information, enhancing recommen-\ndation across diverse modalities. Our parameter-\nefficient tuning strategy updates a small propor-\ntion of adapters, achieving a better trade-off be-\ntween recommendation performance, training ef-\nficiency, and memory usage. Through extensive\nexperiments, we show the effectiveness of our VIP5\nframework and show that multimodality informa-\ntion is helpful for various recommendation tasks.\nFuture work includes further scaling up the back-\nbone model, incorporating even more modalities,\nand exploring improved prompt strategies, such as\nchain-of-thought prompting.\nLimitations and Future Work\nDespite the promising results and advantages\noffered by our Multimodal Foundation Model\n(MFM), there are several limitations that need to\nbe addressed – 1) Bias and fairness: VIP5 relies on\nthe quality and diversity of training data. Existing\nbiases may lead to biased and unfair recommen-\ndations. Future work could explore methods to\nmitigate biases, improve data representativeness,\nand promote fairness of LLMs for recommendation\n(Li and Zhang, 2023; Hua et al., 2023a). 2) Model\ntransparency and interpretability: VIP5 lacks in-\nherent transparency, which can hinder users’ trust\nin recommendations. Future work will aim to en-\nhance transparency and explainability for VIP5-\ngenerated recommendations. 3) Scalability to other\nmodalities: Extending the VIP5 framework to other\nmodalities, such as audio or video, remains a chal-\nlenge. Incorporating these modalities efficiently\nis an important aspect for further investigation. 4)\nEfficiency of LLM: Efficiency is an important fac-\ntor for LLMs to gain practical applications in real-\nworld systems, because the latency should be lim-\nited to a small amount of time when delivering ser-\nvices to users. In this work, we have made an initial\nattempt to improve LLM efficiency by proposing\nthe parameter-efficient tuning approach. In the fu-\nture, it is important to investigate the efficiency of\nLLMs on various stages of the pipeline, such as\nthe effiency of pre-training, fine-tuning and prompt-\nbased inference (Li et al., 2023a). In conclusion,\naddressing these limitations can pave the way for\nimproved multimodal foundation models and more\neffective recommendations across various applica-\ntions and domains.\nAcknowledgement\nThis work was supported in part by NSF 1910154,\n2007907, 2046457 and 2127918. Any opinions,\nfindings, conclusions, or recommendations ex-\npressed in this material are those of the authors\nand do not necessarily reflect those of the sponsors.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-\nglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,\nJai Gupta, Kai Hui, Sebastian Ruder, and Donald\nMetzler. 2022. Ext5: Towards extreme multi-task\nscaling for transfer learning. In International Confer-\nence on Learning Representations.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In NeurIPS.\nTing Chen, Saurabh Saxena, Lala Li, David J. Fleet, and\nGeoffrey Hinton. 2022. Pix2seq: A language model-\n9614\ning framework for object detection. In International\nConference on Learning Representations.\nXu Chen, Hanxiong Chen, Hongteng Xu, Yongfeng\nZhang, Yixin Cao, Zheng Qin, and Hongyuan Zha.\n2019. Personalized fashion recommendation with\nvisual explanations based on multimodal attention\nnetwork: Towards visually explainable recommenda-\ntion. In Proceedings of the 42nd International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, pages 765–774.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In Proceedings of the 38th International Con-\nference on Machine Learning, volume 139 of Pro-\nceedings of Machine Learning Research, pages 1931–\n1942. PMLR.\nZeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang. 2022. M6-rec: Generative pretrained\nlanguage models are open-ended recommender sys-\ntems. arXiv preprint arXiv:2205.08084.\nYashar Deldjoo, Tommaso Di Noia, Daniele Malitesta,\nand Felice Antonio Merra. 2022. Leveraging content-\nstyle item representation for visual recommendation.\nIn European Conference on Information Retrieval ,\npages 84–92. Springer.\nLi Dong, Shaohan Huang, Furu Wei, Mirella Lapata,\nMing Zhou, and Ke Xu. 2017. Learning to generate\nproduct reviews from attributes. In EACL.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In ACL-IJCNLP.\nYingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji,\nJuntao Tan, Shuyuan Xu, Zelong Li, and Yongfeng\nZhang. 2023. OpenAGI: When LLM Meets Domain\nExperts. In Proceedings of the Thirty-Seventh An-\nnual Conference on Neural Information Processing\nSystems (NeurIPS).\nShijie Geng, Zuohui Fu, Yingqiang Ge, Lei Li, Gerard\nde Melo, and Yongfeng Zhang. 2022a. Improving\npersonalized explanation generation through visual-\nization. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 244–255.\nShijie Geng, Zuohui Fu, Juntao Tan, Yingqiang Ge, Ger-\nard De Melo, and Yongfeng Zhang. 2022b. Path\nlanguage modeling over knowledge graphsfor ex-\nplainable recommendation. In Proceedings of the\nACM Web Conference 2022, pages 946–955.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge,\nand Yongfeng Zhang. 2022c. Recommendation as\nlanguage processing (rlp): A unified pretrain, per-\nsonalized prompt & predict paradigm (p5). In Pro-\nceedings of the Sixteenth ACM Conference on Rec-\nommender Systems.\nShijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen,\nand Yongfeng Zhang. 2022d. Hiclip: Contrastive\nlanguage-image pretraining with hierarchy-aware at-\ntention. In The Eleventh International Conference on\nLearning Representations.\nRuining He and Julian McAuley. 2016. Vbpr: visual\nbayesian personalized ranking from implicit feed-\nback. In Proceedings of the AAAI conference on\nartificial intelligence, volume 30.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nMin Hou, Le Wu, Enhong Chen, Zhi Li, Vincent W\nZheng, and Qi Liu. 2019. Explainable fashion rec-\nommendation: A semantic attribute region guided\napproach. arXiv preprint arXiv:1905.12862.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nWenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji,\nand Yongfeng Zhang. 2023a. UP5: Unbiased Foun-\ndation Model for Fairness-aware Recommendation.\narXiv:2305.12090.\nWenyue Hua, Shuyuan Xu, Yingqiang Ge, and\nYongfeng Zhang. 2023b. How to Index Item IDs for\nRecommendation Foundation Models. In Proceed-\nings of 1st International ACM SIGIR Conference on\nInformation Retrieval in the Asia Pacific (SIGIR-AP).\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire\nCardie, Serge Belongie, Bharath Hariharan, and Ser-\nNam Lim. 2022. Visual prompt tuning. arXiv\npreprint arXiv:2203.12119.\nYunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi\nWang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, An-\nima Anandkumar, Yuke Zhu, and Linxi Fan. 2022.\nVima: General robot manipulation with multimodal\nprompts. arXiv preprint arXiv:2210.03094.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nWang-Cheng Kang and Julian McAuley. 2018. Self-\nattentive sequential recommendation. In 2018 IEEE\nInternational Conference on Data Mining (ICDM),\npages 197–206. IEEE.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In EMNLP.\nLei Li, Yongfeng Zhang, and Li Chen. 2021. Person-\nalized transformer for explainable recommendation.\nIn Proceedings of the 59th Annual Meeting of the\n9615\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n4947–4957.\nLei Li, Yongfeng Zhang, and Li Chen. 2022. Person-\nalized prompt learning for explainable recommenda-\ntion. ACM Transactions on Information Systems.\nLei Li, Yongfeng Zhang, and Li Chen. 2023a. Prompt\nDistillation for Efficient LLM-based Recommenda-\ntion. In Proceedings of the 32nd ACM International\nConference on Information and Knowledge Manage-\nment.\nLei Li, Yongfeng Zhang, Dugang Liu, and Li Chen.\n2023b. Large language models for generative rec-\nommendation: A survey and visionary discussions.\narXiv:2309.01157.\nPiji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, and\nWai Lam. 2017. Neural rating regression with ab-\nstractive tips generation for recommendation. In Pro-\nceedings of the 40th International ACM SIGIR con-\nference on Research and Development in Information\nRetrieval, pages 345–354.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nACL.\nYunqi Li and Yongfeng Zhang. 2023. Fairness of Chat-\nGPT. arXiv:2305.18569.\nGuo Lin and Yongfeng Zhang. 2023. Sparks of Arti-\nficial General Recommender (AGR): Experiments\nwith ChatGPT. Algorithms, 16(9).\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nChen Ma, Peng Kang, and Xue Liu. 2019. Hierarchical\ngating networks for sequential recommendation. In\nProceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining,\npages 825–833.\nLei Meng, Fuli Feng, Xiangnan He, Xiaoyan Gao, and\nTat-Seng Chua. 2020. Heterogeneous fusion of se-\nmantic and collaborative information for visually-\naware food recommendation. In Proceedings of the\n28th ACM International Conference on Multimedia,\npages 3460–3468.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2022. Reframing\ninstructional prompts to gptk’s language. Findings of\nthe Association for Computational Linguistics: ACL\n2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner,\nand Lars Schmidt-Thieme. 2009. Bpr: Bayesian\npersonalized ranking from implicit feedback. In\nProceedings of the Twenty-Fifth Conference on Un-\ncertainty in Artificial Intelligence , UAI ’09, page\n452–461, Arlington, Virginia, USA. AUAI Press.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In EMNLP.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\nVl-adapter: Parameter-efficient transfer learning for\nvision-and-language tasks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 5227–5237.\nDhruv Verma, Kshitij Gulati, Vasu Goel, and Rajiv Ratn\nShah. 2020. Fashionist: Personalising outfit recom-\nmendation for cold-start scenarios. In Proceedings\nof the 28th ACM International Conference on Multi-\nmedia, pages 4527–4529.\n9616\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022. OFA: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Pro-\nceedings of the 39th International Conference on\nMachine Learning, volume 162 of Proceedings of\nMachine Learning Research , pages 23318–23340.\nPMLR.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022a. Finetuned language\nmodels are zero-shot learners. In International Con-\nference on Learning Representations.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022b. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research.\nOrion Weller, Nicholas Lourie, Matt Gardner, and\nMatthew E Peters. 2020. Learning from task de-\nscriptions. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1361–1375.\nShuyuan Xu, Wenyue Hua, and Yongfeng Zhang. 2023.\nOpenP5: Benchmarking Foundation Models for Rec-\nommendation. arXiv:2306.11134.\nRuosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu,\nand Yongfeng Zhang. 2023. Natural Language is All\na Graph Needs. arXiv:2308.07134.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nJinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu,\nShuhui Wang, and Liang Wang. 2021a. Mining latent\nstructures for multimedia recommendation. In Pro-\nceedings of the 29th ACM International Conference\non Multimedia, pages 3872–3880.\nJinghao Zhang, Yanqiao Zhu, Qiang Liu, Mengqi\nZhang, Shu Wu, and Liang Wang. 2021b. Latent\nstructures mining with contrastive modality fusion\nfor multimedia recommendation. arXiv preprint\narXiv:2111.00678.\nYongfeng Zhang, Qingyao Ai, Xu Chen, and W Bruce\nCroft. 2017. Joint representation learning for top-\nn recommendation with heterogeneous information\nsources. In Proceedings of the 2017 ACM on Con-\nference on Information and Knowledge Management,\npages 1449–1458.\nYongfeng Zhang, Haochen Zhang, Min Zhang, Yiqun\nLiu, and Shaoping Ma. 2014. Do users rate or re-\nview? boost phrase-level sentiment labeling with\nreview-level sentiment classification. In SIGIR.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models. arXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Zihan Lin, Zhichao Feng, Pengfei\nWang, and Ji-Rong Wen. 2022. A revisiting study of\nappropriate offline evaluation for top-n recommenda-\ntion algorithms. ACM Transactions on Information\nSystems, 41(2):1–41.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022. Learning to prompt for vision-\nlanguage models. International Journal of Computer\nVision, pages 1–12.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu,\nSirui Wang, Fuzheng Zhang, Zhongyuan Wang, and\nJi-Rong Wen. 2020. S3-rec: Self-supervised learning\nfor sequential recommendation with mutual informa-\ntion maximization. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowl-\nedge Management, pages 1893–1902.\n9617\nAppendix\nFrom Figure 8 to Figure 10, we provide a detailed list of 29 multimodal personalized prompts used in our\npaper that covers three recommendation tasks.\nPrompt A-1\nInput template: Given the following purchase \nhistory of user_{{user_id}}:\n{{purchase_history}}\npredict next possible item to be purchased by \nthe user?\nTarget template: {{next_item}}\nPrompt A-2\nInput template: I find the purchase history \nlist of user_{{user_id}}:\n{{purchase_history}}\nI wonder which is the next item to recommend \nto the user. Can you help me decide?\nTarget template: {{next_item}}\nPrompt A-4\nInput template: Given the following purchase \nhistory of {{user_desc}}:\n{{purchase_history}}\npredict next possible item for the user\nTarget template: {{next_item}}\nPrompt A-5\nInput template: Based on the purchase history \nof {{user_desc}}:\n{{purchase_history}}\nCan you decide the next item likely to be \npurchased by the user?\nTarget template: {{next_item}}\nPrompt A-7\nInput template: User_{{user_id}} has the \nfollowing purchase history:\n{{purchase_history}}\nDoes the user likely to buy {{item_id}} \n{{item_photo}} next?\nTarget template: {{answer_choices[label]}} \n(yes/no)\nPrompt A-8\nInput template: According to {{user_desc}}'s \npurchase history list:\n{{purchase_history}}\nPredict whether the user will purchase \n{{item_id}} {{item_photo}} next?\nTarget template: {{answer_choices[label]}} \n(yes/no)\nPrompt A-3\nInput template: Here is the purchase history \nlist of user_{{user_id}}:\n{{purchase_history}}\ntry to recommend next item to the user\nTarget template: {{next_item}}\nPrompt A-6\nInput template: Here is the purchase history \nof {{user_desc}}:\n{{purchase_history}}\nWhat to recommend next for the user?\nTarget template: {{next_item}}\nPrompt A-9\nInput template: According to the purchase \nhistory of {{user_desc}}:\n{{purchase_history}}\nCan you recommend the next possible item to \nthe user ?\nTarget template: {{next_item}}\nFigure 8: Multimodal personalized prompts for Task Group A: Sequential Recommendation.\n9618\nPrompt B-1\nInput template: Will user_{{user_id}} likely \nto interact with item_{{item_id}} \n{{item_photo}}?\nTarget template: {{answer_choices[label]}} \n(yes/no)\nPrompt B-2\nInput template: Shall we recommend \nitem_{{item_id}} {{item_photo}} to \nuser_{{user_id}}?\nTarget template: {{answer_choices[label]}} \n(yes/no)\nPrompt B-3\nInput template: For {{user_desc}}, do you \nthink it is good to recommend {{item_title}} \n{{item_photo}}?\nTarget template: {{answer_choices[label]}} \n(yes/no)\nPrompt B-5\nInput template: Which item of the following \nto recommend for {{user_desc}}?\n{{candidate_items}}\nTarget template: {{target_item}}\nPrompt B-6\nInput template: Choose the best item from the \ncandidates to recommend for {{user_desc}}?\n{{candidate_items}}\nTarget template: {{target_item}}\nPrompt B-7\nInput template: Pick the most suitable item \nfrom the following list and recommend to \nuser_{{user_id}}:\n{{candidate_items}}\nTarget template: {{target_item}}\nPrompt B-8\nInput template: We want to make recommendation \nfor user_{{user_id}}. Select the best item \nfrom these candidates: \n{{candidate_items}}\nTarget template: {{target_item}}\nPrompt B-4\nInput template: I would like to recommend \nsome items for user_{{user_id}}. Is the \nfollowing item a good choice?\n{{item_title}} {{item_photo}}\nTarget template: {{answer_choices[label]}} \n(yes/no)\nFigure 9: Multimodal personalized prompts for Task Group B: Direct Recommendation.\n9619\nPrompt C-1\nInput template: Generate an explanation for \nuser_{{user_id}} about this product: \n{{item_title}} {{item_photo}}\nTarget template: {{explanation}}\nPrompt C-2\nInput template: Given the following review \nheadline {{review_headline}}\ncan you help generate an explanation of \nuser_{{user_id}} for item_{{item_id}} \n{{item_photo}}?\nTarget template: {{explanation}}\nPrompt C-3\nInput template: Help user_{{user_id}} \ngenerate a {{star_rating}}-star explanation \nabout this product:\n{{item_title}} {{item_photo}}\nTarget template: {{explanation}}\nPrompt C-4\nInput template: Generate an explanation for \n{{user_desc}} about this product:\n{{item_title}} {{item_photo}}\nTarget template: {{explanation}}\nPrompt C-5\nInput template: Based on the following review \nheadline: {{review_headline}}\nGenerate {{user_desc}}'s purchase explanation \nabout {{item_title}} {{item_photo}}\nTarget template: {{explanation}}\nPrompt C-6\nInput template: Help {{user_desc}} generate a \n{{star_rating}}-star explanation for \nitem_{{item_id}} {{item_photo}}\nTarget template: {{explanation}}\nPrompt C-7\nInput template: Predict the star rating , then \nuse {{feature_word}} as feature word to \ngenerate user_{{user_id}}'s purchase \nexplanation for item_{{item_id}} \n{{item_photo}}\nTarget template: {{star_rating}},\n{{explanation}}\nPrompt C-8\nInput template: What score will {{user_desc}} \nrate item_{{item_id}} {{item_photo}}? Then \ngive an explanation for the rating score. (1 \nbeing lowest and 5 being highest)\nTarget template: {{star_rating}},\n{{explanation}}\nPrompt C-9\nInput template: Based on the feature word \n{{feature_word}}, generate an explanation for \nuser_{{user_id}} about this product: \n{{item_title}} {{item_photo}}\nTarget template: {{explanation}}\nPrompt C-10\nInput template: Given the word \n{{feature_word}}, can you help generate an \nexplanation for {{user_desc}} about the \nproduct: \n{{item_title}} {{item_photo}}\nTarget template: {{explanation}}\nPrompt C-11\nInput template: Using the word \n{{feature_word}}, write a {{star_rating}}-star \nexplanation for user_{{user_id}} about \nitem_{{item_id}} {{item_photo}}\nTarget template: {{explanation}}\nPrompt C-12\nInput template: According to the feature word \n{{feature_word}}, generate a {{star_rating}}-\nstar explanation for {{user_desc}} about \nitem_{{item_id}} {{item_photo}}\nTarget template: {{explanation}}\nFigure 10: Multimodal personalized prompts for Task Group C: Explanation Generation.\n9620",
  "topic": "Modalities",
  "concepts": [
    {
      "name": "Modalities",
      "score": 0.8863567113876343
    },
    {
      "name": "Computer science",
      "score": 0.8301466703414917
    },
    {
      "name": "Personalization",
      "score": 0.7177668809890747
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.6042484045028687
    },
    {
      "name": "Recommender system",
      "score": 0.5907213091850281
    },
    {
      "name": "Code (set theory)",
      "score": 0.4801173210144043
    },
    {
      "name": "Architecture",
      "score": 0.47025999426841736
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4601789712905884
    },
    {
      "name": "Human–computer interaction",
      "score": 0.45122236013412476
    },
    {
      "name": "Exploit",
      "score": 0.4421805739402771
    },
    {
      "name": "Machine learning",
      "score": 0.3368719816207886
    },
    {
      "name": "Multimedia",
      "score": 0.3214510679244995
    },
    {
      "name": "World Wide Web",
      "score": 0.2283737063407898
    },
    {
      "name": "Programming language",
      "score": 0.1728821098804474
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102322142",
      "name": "Rutgers, The State University of New Jersey",
      "country": "US"
    }
  ],
  "cited_by": 41
}