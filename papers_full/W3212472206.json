{
  "title": "NB-MLM: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis",
  "url": "https://openalex.org/W3212472206",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5047490968",
      "name": "Nikolay Arefyev",
      "affiliations": [
        null,
        "Lomonosov Moscow State University",
        "National Research University Higher School of Economics",
        "Samsung (Russia)"
      ]
    },
    {
      "id": "https://openalex.org/A5009397647",
      "name": "Dmitrii Kharchev",
      "affiliations": [
        null,
        "Lomonosov Moscow State University",
        "National Research University Higher School of Economics",
        "Samsung (Russia)"
      ]
    },
    {
      "id": "https://openalex.org/A5071397402",
      "name": "Artem Shelmanov",
      "affiliations": [
        null,
        "Lomonosov Moscow State University",
        "National Research University Higher School of Economics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2507974895",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2031998113",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W3121606938",
    "https://openalex.org/W2154359981",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1777150271",
    "https://openalex.org/W2153579005"
  ],
  "abstract": "While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for the final task fine-tuning. We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words, which are not necessarily related to a final task. We propose a technique for more efficient adaptation that focuses on predicting words with large weights of the Naive Bayes classifier trained for the task at hand, which are likely more relevant than the most frequent words. The proposed method provides faster adaptation and better final performance for sentiment analysis compared to the standard approach.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9114–9124\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n9114\nNB-MLM: Efﬁcient Domain Adaptation of Masked Language Models for\nSentiment Analysis\nNikolay Arefyev♦,∇,△ Dmitrii Kharchev♦,∇ Artem Shelmanov⃝,□,∇\n♦Samsung Research Center Russia / Moscow, Russia\n∇Lomonosov Moscow State University / Moscow, Russia\n△National Research University Higher School of Economics / Moscow, Russia\n⃝Artiﬁcial Intelligence Research Institute / Moscow, Russia\n□Sber AI Lab / Moscow, Russia\n{nick.arefyev,dimitriy.kharchev}@gmail.com shelmanov@airi.net\nAbstract\nWhile Masked Language Models (MLM) are\npre-trained on massive datasets, the additional\ntraining with the MLM objective on domain\nor task-speciﬁc data before ﬁne-tuning for the\nﬁnal task is known to improve the ﬁnal perfor-\nmance. This is usually referred to as the do-\nmain or task adaptation step. However, unlike\nthe initial pre-training, this step is performed\nfor each domain or task individually and is still\nrather slow, requiring several GPU days com-\npared to several GPU hours required for the\nﬁnal task ﬁne-tuning.\nWe argue that the standard MLM objective\nleads to inefﬁciency when it is used for the\nadaptation step because it mostly learns to pre-\ndict the most frequent words, which are not\nnecessarily related to a ﬁnal task. We pro-\npose a technique for more efﬁcient adaptation\nthat focuses on predicting words with large\nweights of the Naive Bayes classiﬁer trained\nfor the task at hand, which are likely more rel-\nevant than the most frequent words. The pro-\nposed method provides faster adaptation and\nbetter ﬁnal performance for sentiment analysis\ncompared to the standard approach.\n1 Introduction\nPre-training of neural networks with a language\nmodel (LM) or masked language model (MLM)\nobjective on large amounts of non-domain-speciﬁc\ntexts has given a signiﬁcant boost of performance in\nalmost all natural language processing tasks. While\n16GB of texts were shown to BERT (Devlin et al.,\n2019) and ten times more to RoBERTa (Liu et al.,\n2019) during pre-training, the further training of\nthese models with the MLM objective on domain-\nspeciﬁc texts before ﬁne-tuning to the target task\nwas shown to further improve the ﬁnal results (Sun\net al., 2019; Gururangan et al., 2020). This tech-\nnique is called the domain or task adaptation, de-\npending on the degree of similarity of the data for\nadaptation to the target dataset. While initial pre-\ntraining is extremely expensive, it does not depend\non the ﬁnal task and can be performed only once.\nHowever, domain or task adaptation is done for\neach domain or task individually and is still quite\nresource-demanding, requiring hundreds of thou-\nsands of training steps or several GPU days, unlike\nﬁnal ﬁne-tuning, which can often be done in a few\nGPU hours (Sun et al., 2019).\nIn this work, we propose a method for more ef-\nﬁcient MLM adaptation. We have noticed that the\nstandard MLM spends most of the training time on\nlearning to restore the most frequent words like de-\nterminers or auxiliary verbs hidden (masked) from\nits input. While such training examples may be\nuseful for learning English grammar, their dom-\nination during the adaptation phase seems to be\nwasteful for many ﬁnal tasks. Since the ﬁnal task\nand the dataset are already known in this phase, we\npropose to undersample such examples in favor of\nexamples with targets related to the ﬁnal task. This\nrelatedness is estimated using a Naive Bayes clas-\nsiﬁer. Hence, we call our modiﬁed objective Naive\nBayes Masked Language Model (NB-MLM). We\nhypothesize that hiding from the model and asking\nit to restore mostly features that are important for\nthe ﬁnal task will likely result in faster adaptation.\nAdditionally, the absence of simple features and the\nrequirement to restore them may teach the model\nto exploit more sophisticated and implicit features\nrelevant to the ﬁnal task.\nWe evaluate the proposed method on two\ndatasets for sentiment analysis. It is one of the\nmost popular tasks in natural language process-\ning (Feldman, 2013) and an excellent playground\nfor the comparison of adaptation methods due to\nthe large amount of labeled and unlabeled user re-\nviews of different products available. In particular,\nwe consider the task of classifying the binary senti-\nment polarity of a given review. Our experiments\n9115\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5\nPercent of the target tokens\n the\n,\n.\n a\n and\n of\n to\n is\nbr\n in\n it\n I\n that\n this\n's\n was\n movie\n />\n><\n /\nTokens\nUniform\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5\nPercent of the target tokens\n bad\n?\n great\n love\n best\n acting\n plot\n well\n nothing\n no\n thing\n script\n minutes\n worst\n always\n performance\n 2\n horror\n't\n money\nNB-MLM\nFigure 1: Target tokens that the model is asked to predict most often for Uniform MLM and NB-MLM.\nshow that the NB-MLM objective can signiﬁcantly\nreduce adaptation time while achieving the same\nﬁnal performance or help to improve performance\ngiven the same amount of time for adaptation. 1\n2 Related Work\nPre-training Transformer networks with the MLM\nobjective is proposed in (Devlin et al., 2019) for\nthe BERT model and is shown to outperform the\nmore traditional LM objective, though the similar\ntask of predicting a word from its left and right\ncontext was used with different architectures ear-\nlier (Mikolov et al., 2013; Melamud et al., 2016).\nRoBERTa enhances BERT by pre-training longer\non ten times larger corpora, getting rid of the next\nsentence prediction (NSP) task during pre-training,\nand selecting different target words to be masked\nand predicted in each epoch (dynamic masking).\nVarious approaches to further pre-training of\nBERT on domain or task-speciﬁc data are com-\npared in (Sun et al., 2019), while Gururangan\net al. (2020) carry out a similar investigation\nwith RoBERTa. They try various options of data\nsources for adaptation: texts only from the tar-\nget dataset (called task adaptation or within-task\npre-training), larger datasets from the same do-\nmain (called domain adaptation or in-domain pre-\ntraining), and datasets from different domains\n(called cross-domain pre-training). They ﬁnd the\ntask adaptation, which is a computationally cheap-\nest option, to be a surprisingly good solution. In\ntheir experiments, it often outperforms the domain\nadaptation and is only marginally worse than com-\n1The repository for this paper: https://github.\ncom/nvanva/nb-mlm\nbining both methods. However, due to the large\namount of data used in domain adaptation, Guru-\nrangan et al. (2020) train the MLM only for one or\nvery few epochs. We ﬁnd that our method leverag-\ning large data more efﬁciently makes the domain\nadaptation comparable to the task adaptation, and\ntheir combination is signiﬁcantly better than each\nof them.\nOur idea of employing Naive Bayes weights is in-\nspired by the NB-SVM model (Wang and Manning,\n2012; Mesnil et al., 2014), which scales bag-of-\nngrams vectors with Naive Bayes classiﬁer weights\nand then trains linear SVM or logistic regression\nclassiﬁers on them. It proved to be a very strong\nbaseline, often outperforming both linear and more\nsophisticated models from that time.\n3 MLM Objectives for Adaptation\nUniform MLM. For each input example, the\nstandard MLM objective, as proposed by Devlin\net al. (2019), samples 15% of the input positions\n(subwords) for calculating the loss. The positions\nare sampled from the uniform distribution without\nreplacement: P(pos) ∝1. Then 80% of the tokens\non sampled positions are masked (replaced with a\n[MASK] token), 10% are replaced with some ran-\ndom tokens from the uniform distribution over the\nvocabulary, and 10% are left intact.\nNB-MLM. As an alternative, we propose sam-\npling 15% of positions from a non-uniform distri-\nbution that gives higher probabilities to positions\nthat contain subwords with high feature importance\nfi(w): P(pos) ∝ exp(fi(wpos)/T), where the\ntemperature T is the hyperparameter allowing to\nbalance between uniform sampling and determin-\n9116\n20 40 60 80 100\nepoches\n95.4\n95.6\n95.8\n96.0\n96.2\n96.4test accuracy\nNB-MLM\nUniform\n2500 5000 7500 10000 12500 15000 17500 20000\nadaptation steps\nNB-MLM\nUniform\n5 10 15 20 25\nepoches\n98.20\n98.25\n98.30\n98.35\n98.40\n98.45test accuracy\nNB-MLM\nUniform\n2500 5000 7500 10000 12500 15000 17500\nadaptation steps\nNB-MLM\nUniform\nFigure 2: Task (left) and domain (right) adaptation with the standard Uniform MLM and the proposed NB-MLM\nobjectives. Test accuracy on IMDB (top) and Yelp (bottom) for the classiﬁers ﬁne-tuned from different MLM\ncheckpoints saved during adaptation. Means and standard deviations over 6 runs are plotted for each model, except\nfor DAPT on IMDB, where there were 15 runs. Corresponding dev accuracies are in Appendix B.\nistic selection of positions that contain the most\nimportant features. For binary classiﬁcation, the\nfeature importance is estimated using the Naive\nBayes classiﬁer weights as follows:\nfi(w) =|logP (w|1) −logP (w|0)|.\nThus, those features that are much more proba-\nble in one class than in another receive the highest\nscores. Similar to the method proposed by Wang\nand Manning (2012), the probabilities are esti-\nmated by the multinomial Naive Bayes model with\nadditive smoothing (alpha = 0.1). Additionally,\nthe scores are set to zero for those features that\noccurred in less than m examples to avoid the over-\nrepresentation of unreliable features. As an exam-\nple, Figure 1 shows the words that the model is\nmost frequently asked to predict during the task\nadaptation on the IMDB movie reviews dataset\n(T = 0.1, m = 5for NB-MLM). Evidently, NB-\nMLM learns to predict words relevant to sentiment\nanalysis more often than the standard MLM.\nAlong with the uniform and NB-based distribu-\ntions, during the preliminary experiments, we tried\nother options, which are described and compared\nin Appendix D. However, only NB-MLM outper-\nformed the uniform baseline.\n4 Experiments and Results\nDuring the preliminary experiments described in\nAppendix A, we found that our method helps for\nboth BERT and RoBERTa models. However, the\nlatter model achieved signiﬁcantly better perfor-\nmance. Therefore, we describe the results for\nRoBERTa in the rest of the paper.\nFor domain adaptation (denoted as DAPT), we\nemployed the Amazon Reviews dataset (McAuley\net al., 2015) with duplicates removed. We removed\nreviews shorter than 500 characters and split the\nrest into the training and validation sets of 21M\nand 10K reviews correspondingly. The valida-\n9117\nModel IMDB Yelp P.\nERR macro-F1 ERR macro-F1\nOur experiments with task and domain adaptation for RoBERTa-base\nUniform short DAPT 4.42** 95.58** 1.78 98.22\nNB-MLM short DAPT 4.14** 95.86** 1.77 98.23\nUniform DAPT 4.19** 95.81** 1.73 98.27\nNB-MLM DAPT 3.85** 96.15** 1.71 98.29\nUniform short all-TAPT 3.92 96.08 1.70 98.30\nNB-MLM short all-TAPT 3.82 96.18 1.69 98.31\nUniform all-TAPT 3.74 96.26 1.59 98.41\nNB-MLM all-TAPT 3.66 96.34 1.58 98.42\nUniform short DAPT+all-TAPT 3.96* 96.04* 1.69 98.31\nNB-MLM short DAPT+all-TAPT 3.73* 96.27* 1.66 98.34\nUniform DAPT+all-TAPT 3.62 96.38 1.55 98.45\nNB-MLM DAPT+all-TAPT 3.54 96.46 1.51 98.49\nPreviously published results of the task and domain adaptation for BERT-base and RoBERTa-base\nBERT-base+ITPT (Sun et al., 2019) 4.37 - 1.92 -\nBERT-base+IDPT (Sun et al., 2019) 4.88 - 1.87 -\nRoBERTa-base+DAPT (Gururangan et al., 2020) - 95.4 - -\nRoBERTa-base+TAPT (Gururangan et al., 2020) - 95.5 - -\nRoBERTa-base+DAPT+TAPT (Gururangan et al., 2020) - 95.6 - -\nRoBERTa-base+Curated-TAPT (Gururangan et al., 2020) - 95.7 - -\nRoBERTa-base+DAPT+Curated-TAPT (Gururangan et al., 2020) - 95.8 - -\nLarge SOTA models (not directly comparable to our models)\nBERT-large+ITPT (Sun et al., 2019) 4.21 - 1.81 -\nXLNET-large (Yang et al., 2019) 3.20 - 1.37 -\nTable 1: Comparison of NB-MLM to the standard Uniform MLM and to the previously published results. From\nruns of each model with different random seeds, medians are taken. Adaptation scenarios where the difference\nbetween NB-MLM and Uniform MLM is statistically signiﬁcant according to the McNemar’s test are marked with\n* (p-value < 0.05) or ** (p-value < 0.01). ITPT (within-task pre-training), TAPT (task-adaptive pre-training), and\nCurated-TAPT (TAPT with extra unlabeled data from IMDB) denote further MLM pre-training on the target dataset\nonly, which is similar to our all-TAPT. IDPT (in-domain pre-training) and DAPT (domain-adaptive pre-training)\ncorrespond to our DAPT. The best results for base and large models separately are in bold.\ntion set was used to calculate perplexity during\nMLM training. For task adaptation (denoted as\nall-TAPT), we used all texts (without labels) from\nthe target dataset, i.e. IMDB (Maas et al., 2011)\nor Yelp (Zhang et al., 2015) 2. For IMDB, we em-\nployed the split of Gururangan et al. (2020) to make\nthe results of our experiments directly comparable\nwith their results. We used the binary classiﬁcation\nversion of Yelp (Zhang et al., 2015). For validation,\nwe randomly selected 5K positive and 5K negative\nexamples.\nFor domain and task adaptation, we used the\nbatch size of 1024, while classiﬁers were ﬁne-tuned\nwith the batch size of 32. Based on our preliminary\nexperiments, we set the learning rate of 2e-4 for\nthe domain adaptation, 1e-4 for the task adaptation,\nand 1e-5 for ﬁnal ﬁne-tuning. Following Gururan-\ngan et al. (2020), we performed domain adapta-\ntion for one epoch on the Amazon dataset (20K\nsteps, 38h on one V100 GPU) and task adapta-\n2Using the whole target dataset for task adaptation has\nshown the best results for both Uniform MLM and NB-MLM,\nsee Appendix C. This setup, when test examples (without\nlabels) are exploited during training, is known as transductive\nlearning.\ntion for 100 epochs on IMDB (18h) and 24 epochs\non Yelp (14h). To show that NB-MLM can ob-\ntain results similar to Uniform MLM in a much\nshorter time, we also report the results of short\nadaptation with the duration reduced to 4K steps\non Amazon, 20 epochs on IMDB, and 6 epochs on\nYelp. To estimate the variance of the results due to\nthe randomness in the order of training examples\nand positions selected for masking and prediction,\nwe have trained each model with different random\nseeds. For both Uniform MLM and NB-MLM,\nwe aggregated metrics from 15 runs for DAPT on\nIMDB, 3 runs for DAPT+all-TAPT on both IMDB\nand Yelp, and 6 runs for all other scenarios. The\nclassiﬁers were ﬁne-tuned for 4 epochs on IMDB\nand 2 epochs on Yelp 3. For task adaptation with\nNB-MLM, we set T = 0.4, m = 50 based on\npreliminary experiments (see Appendix A). For do-\nmain adaptation with NB-MLM, we set T = 0.1,\nm = 10 on IMDB and T = 0.1, m = 50 af-\nter grid search from T = [0.05, 0.1, 0.2, 0.4, 0.8],\nm = [10, 50]. Generally, for task adaptation with\n3Longer ﬁne-tuning resulted in a higher variance of metrics\nand worse ﬁnal performance due to strong over-ﬁtting.\n9118\nmany epochs of training on smaller datasets, larger\ntemperatures are required to avoid over-ﬁtting due\nto the same words masked in each example at each\nepoch. For domain adaptation, only one epoch of\ntraining is done on a large dataset, hence, smaller\ntemperatures perform better.\nFigure 2 shows how the ﬁnal classiﬁcation ac-\ncuracy improves during the task and domain adap-\ntation. Our NB-MLM model signiﬁcantly helps\nfor domain adaptation on IMDB. For task adapta-\ntion, the difference is much smaller and ﬁts into\ntwo standard deviations. Still, on average, the NB-\nMLM seems to provide a consistent improvement\nthroughout the adaptation. For Yelp, the improve-\nments from NB-MLM are also small but consistent.\nTable 1 compares our models and the previously\npublished results on the test sets. In order to apply\nMcNemar’s test for statistical signiﬁcance, instead\nof averaging across all runs of each model with\ndifferent random seeds, we have to take predictions\nof a particular run. Thus, for each of our models,\nwe selected the run with the median performance\n(for the even number of runs, the one just above the\nmedian) and reported its performance in the table.\nFor IMDB, the domain adaptation with NB-\nMLM obtains results similar to the Uniform MLM\nin 5x fewer training steps and data (only 20% of\nthe data is seen during the ﬁrst 4K steps). When\ntrained for one epoch, it improves the results by\nmore than 0.3%, which is also statistically signif-\nicant. For task adaptation, the NB-MLM gives a\nmuch smaller improvement. Similarly to the results\nof Gururangan et al. (2020), in our experiments,\nthe task adaptation with the Uniform MLM outper-\nforms the domain adaptation that employs much\nmore data by almost 0.5%. We suppose that this\nis due to the small proportion of relevant exam-\nples sampled by the Uniform MLM, which require\nmany repetitions to learn from. Probably, training\ndomain adaptation for hundreds of epochs, simi-\nlarly to task adaptation, can ﬁx this problem, but\nthis is not feasible for large datasets and moder-\nate computation resources. More efﬁcient domain\nadaptation with NB-MLM, which focuses on tar-\ngets that are likely relevant for the ﬁnal task, re-\nduces this difference to 0.2%. Finally, using the\ndomain adaptation followed by the task adapta-\ntion results in the best ﬁnal performance. In this\nscenario, NB-MLM gives 0.2% improvement for\nshort adaptation and 0.1% for long adaptation. For\nYelp, the metrics are higher, and the differences are\nsmaller but still consistent.\n5 Conclusion\nWe proposed a technique for the more efﬁcient do-\nmain and task adaptation of MLMs. It is especially\nhelpful for leveraging large data efﬁciently during\nthe domain adaptation, resulting in signiﬁcantly\nshorter adaptation time or better performance.\nAcknowledgements\nWe are very grateful to our anonymous reviewers\nfor insightful comments. The contribution of Niko-\nlay Arefyev to the paper was partially made within\nthe framework of the HSE University Basic Re-\nsearch Program. This research was supported in\npart through computational resources of HPC facil-\nities at HSE University (Kostenetskiy et al., 2021).\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRonen Feldman. 2013. Techniques and applications for\nsentiment analysis. Commun. ACM, 56:82–89.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nP. S. Kostenetskiy, R. A. Chulkevich, and V . I. Kozyrev.\n2021. HPC resources of the higher school of eco-\nnomics. Journal of Physics: Conference Series,\n1740:012050.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv, abs/1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\n9119\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nJulian McAuley, Christopher Targett, Qinfeng Shi, and\nAnton van den Hengel. 2015. Image-based recom-\nmendations on styles and substitutes. In Proceed-\nings of the 38th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, SIGIR ’15, page 43–52, New York, NY ,\nUSA. Association for Computing Machinery.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In Proceedings\nof The 20th SIGNLL Conference on Computational\nNatural Language Learning, pages 51–61, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nGrégoire Mesnil, Tomas Mikolov, Marc’Aurelio Ran-\nzato, and Yoshua Bengio. 2014. Ensemble of gen-\nerative and discriminative techniques for sentiment\nanalysis of movie reviews. ArXiv abs/1412.5335.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their composition-\nality. Advances in Neural Information Processing\nSystems, 26:3111–3119.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune BERT for text classiﬁcation?\nIn Chinese Computational Linguistics - 18th China\nNational Conference, CCL 2019, Kunming, China,\nOctober 18-20, 2019, Proceedings, volume 11856 of\nLecture Notes in Computer Science, pages 194–206.\nSpringer.\nSida Wang and Christopher Manning. 2012. Baselines\nand bigrams: Simple, good sentiment and topic clas-\nsiﬁcation. In Proceedings of the 50th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 90–94, Jeju Island,\nKorea. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. Advances in Neural Infor-\nmation Processing Systems, 32:5753–5763.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Proceedings of the 28th International\nConference on Neural Information Processing Sys-\ntems - Volume 1, NIPS’15, page 649–657, Cam-\nbridge, MA, USA. MIT Press.\n9120\nA Preliminary Experiments with BERT and RoBERTa\n20k 40k 60k 80k 100k 120k 140k\nNumber of MLM updates\n5.6\n5.8\n6.0\n6.2\n6.4Error rate\nBERT\n0 10k 20k 30k 40k 50k 60k 70k 80k\nNumber of MLM updates\n4.4\n4.6\n4.8\n5.0\n5.2\n5.4Error rate\nRoBERTa\nTemperature\n0.2\n0.4\n0.6\nPosition sampling\nNB-MLM\nMLM\nFigure 3: BERT (m = 100) and RoBERTa (m = 50) best error rates on the IMDB dev set from our split.\nTo verify our hypothesis, in the preliminary\nexperiments, we tried improving the results of\nthe ITPT (withIn-Task Pre-Training) method (Sun\net al., 2019). Since no code for this paper was\navailable at that time, we implemented this method\nusing the Transformers library (Wolf et al., 2020)4,\nwhich closely followed the details and hyperpa-\nrameters speciﬁed in the paper but adopted recom-\nmendations from more recent models by not using\nNSP prediction and exploiting dynamic masking.\nSince no ofﬁcial development set is available for\nthe IMDB dataset (Maas et al., 2011) and the split\nis not speciﬁed in the paper, for early stopping\nduring classiﬁer ﬁne-tuning and NB-MLM hyper-\nparameters selection, we employed our own split 5.\nNote that this split was used only for preliminary\nexperiments; later, we switched to the split of Gu-\nrurangan et al. (2020). For adaptation, we used the\nwhole dataset, excluding half of the development\nset to measure the validation perplexity.\nFigure 3 (left) shows the ﬁnal classiﬁcation error\nrate depending on the number of adaptation steps.\nThe best error rate on the development set across\n10 epochs of the classiﬁer ﬁne-tuning is shown.\nEvidently, NB-MLM outperforms MLM on aver-\nage. Despite the variance of their difference being\nrather high, we can see that after 60K adaptation\nsteps, NB-MLM with the best temperature robustly\nshows equal or better results than the best result\nof MLM across 150K adaptation steps, which is\nalmost 2.5x speedup. For comparison, Figure 3\n(right) shows the results for RoBERTa using the\nsame split. Evidently, RoBERTa with NB-MLM\nadaptation robustly outperforms MLM. For small\ntemperature T = 0.2 after 20K steps of adapta-\ntion, we receive better results than MLM trained\nmore than 3 times longer. However, later the per-\nformance drops signiﬁcantly for the smallest tem-\nperature. Inspecting perplexity during adaptation,\nwe found that the model begins to strongly overﬁt\nafter 20K steps, which is likely related to the same\npositions for masking and prediction sampled at\neach epoch. Larger temperature T = 0.4 provides\nsmaller beneﬁts in the short run but gives more ro-\nbust improvements and better ﬁnal results. Overall,\nafter 20K steps, it gives the same performance as\nthe MLM trained for 75K steps, which is almost\n4x speedup.\n4https://github.com/huggingface/transformers\n5https://github.com/nvanva/filimdb_evaluation/blob/master/FILIMDB.tar.gz\n9121\nB Results on the Development Sets\nModel IMDB Yelp P.\nERR macro-F1 ERR macro-F1\nOur experiments with task and domain adaptation for RoBERTa-base\nUniform short DAPT 4.3 95.7 1.6 98.4\nNB-MLM short DAPT 3.7 96.3 1.5 98.5\nUniform DAPT 4.0 96.0 1.6 98.4\nNB-MLM DAPT 3.5 96.5 1.6 98.4\nUniform short all-TAPT 3.6 96.4 1.5 98.5\nNB-MLM short all-TAPT 3.7 96.3 1.5 98.5\nUniform all-TAPT 3.8 96.2 1.5 98.5\nNB-MLM all-TAPT 3.5 96.5 1.4 98.6\nUniform short DAPT+all-TAPT 3.8 96.2 1.6 98.4\nNB-MLM short DAPT+all-TAPT 3.5 96.5 1.6 98.4\nUniform DAPT+all-TAPT 3.4 96.6 1.5 98.5\nNB-MLM DAPT+all-TAPT 3.4 96.6 1.5 98.5\nTable 2: Validation metrics corresponding to the test metrics from Table 1 and used for early stopping. The results\nare rounded to one decimal place due to logging issues.\n20 40 60 80 100\nepoches\n95.4\n95.6\n95.8\n96.0\n96.2\n96.4\n96.6valid accuracy\nNB-MLM\nUniform\n2500 5000 7500 10000 12500 15000 17500 20000\nadaptation steps\nNB-MLM\nUniform\n5 10 15 20 25\nepoches\n98.30\n98.35\n98.40\n98.45\n98.50\n98.55\n98.60valid accuracy\nNB-MLM\nUniform\n2500 5000 7500 10000 12500 15000 17500\nadaptation steps\nNB-MLM\nUniform\nFigure 4: Task (left) and domain (right) adaptation with the standard Uniform MLM and the proposed NB-MLM\nobjectives. Means and standard deviations over best dev accuracies on IMDB (top) and Yelp (bottom) correspond-\ning to the test accuracies from Figure 2 and used for early stopping are shown. There were 15 runs for DAPT on\nIMDB and 6 runs for other models.\nIn this section, we show the results on the de-\nvelopment sets corresponding to the results on the\ntest sets provided in the main text. Since these re-\nsults were used to select hyperparameters and also\nfor early stopping during ﬁne-tuning of the clas-\nsiﬁers, they are less reliable to draw conclusions\nabout ﬁnal classiﬁcation performance and shall be\nconsidered only together with the results on the\ntest sets. While general trends are the same, we\nnotice that for domain adaptation, the gap between\nNB-MLM and Uniform MLM on the IMDB dev\nset (ﬁgure 4, top right) is twice as large as on the\ntest set. This may be due to the large variance of\nclassiﬁcation accuracy during ﬁne-tuning and using\nearly-stopping on the development set.\n9122\nC Comparison of Various Subsets of IMDB for Task Adaptation\n20 40 60 80 100\nepoches\n95.8\n96.0\n96.2\n96.4\n96.6valid accuracy\nall\ntrain+dev+test\ncurated\ntrain\n20 40 60 80 100\nepoches\n95.7\n95.8\n95.9\n96.0\n96.1\n96.2\n96.3test accuracy\nall\ntrain+dev+test\ncurated\ntrain\n20 40 60 80 100\nepoches\n95.8\n96.0\n96.2\n96.4\n96.6valid accuracy\nall\ntrain+dev+test\ncurated\ntrain\n20 40 60 80 100\nepoches\n95.7\n95.8\n95.9\n96.0\n96.1\n96.2\n96.3\n96.4test accuracy\nall\ntrain+dev+test\ncurated\ntrain\nFigure 5: Task adaptation of Uniform MLM (top) and NB-MLM (bottom) on different subsets of IMDB. Accuracy\non the IMDB dev (left) and test (right) sets. Means and standard deviations over 6 runs are shown for each model.\nThe all-TAPT method from our experiments em-\nploys examples (without labels) from all subsets\nof the target dataset for MLM training during task\nadaptation. For IMDB, this includes examples from\nthe train (20K), unlabeled (50K), dev (5K), and test\n(25K) sets. We excluded only 1K examples from\nthe unlabeled set in order to calculate the validation\nMLM loss on them. The scenario when test exam-\nples (without labels) are shown along with other\nexamples during training is known as transductive\nlearning. Alternatively, Gururangan et al. (2020)\nperforms task adaptation on the train set alone or\nthe concatenation of the train and the unlabeled\nsets. They denote the latter as Curated-TAPT.\nFigure 5 compares these alternatives for Uni-\nform MLM and NB-MLM. For NB-MLM, we\nselected optimal hyperparameters on the devel-\nopment set individually for each alternative, re-\nsulting in T = 0.4, m= 50 for all-TAPT and\ntrain+dev+test, and T = 0.8, m= 50 for other\nalternatives. In line with Gururangan et al. (2020),\nwe see that additional examples from the unlabeled\nset signiﬁcantly help for both models compared to\nadaptation on the train set only. Adding dev and\ntest examples further improves their performance.\nTo understand if this improvement comes from\nsimply adding more examples or adding exactly\ntest examples, we additionally plot charts for task\nadaptation on train+dev+test subsets. Adaptation\non train+dev+test (50K examples) is on par with\nCurated-TAPT (70k examples from the train and\nunlabeled subsets) while trained on approximately\n1.5x fewer examples. However, adding the dev and\ntest sets robustly improves Curated-TAPT while in-\ncreasing the number of examples by the same factor.\nThis probably indicates that using test examples for\nadaptation provides more beneﬁts for performance\nthan simply adding a comparable amount of other\nexamples. Still, this question deserves more exper-\niments and is out of the scope of this work.\n9123\nD Alternatives for the Naive Bayes Weights\n20 40 60 80 100\nepoches\n95.9\n96.0\n96.1\n96.2\n96.3\n96.4\n96.5\n96.6\n96.7valid accuracy\nNB-MLM\nFreq baseline\nUniform\n20 40 60 80 100\nepoches\n95.6\n95.7\n95.8\n95.9\n96.0\n96.1\n96.2\n96.3\n96.4test accuracy\nNB-MLM\nFreq baseline\nUniform\n5000 10000 15000 20000\nadaptation steps\n95.4\n95.6\n95.8\n96.0\n96.2\n96.4\n96.6valid accuracy\nNB-MLM\nFreq baseline\nUniform\n5000 10000 15000 20000\nadaptation steps\n95.5\n95.6\n95.7\n95.8\n95.9\n96.0\n96.1\n96.2test accuracy\nNB-MLM\nFreq baseline\nUniform\nFigure 6: Comparison with the frequency-based baseline on the IMDB dev (left) and test (right) sets for all-TAPT\n(top) and DAPT (bottom).\nThe uniform distribution over positions is tradi-\ntionally used to sample target subwords that are\nmasked and predicted during MLM pre-training\nand adaptation. However, as Figure 1 shows, it\nmakes the model learning to predict mostly fre-\nquent functional words such as articles, preposi-\ntions, pronouns, etc. While it may teach the model\nto extract some grammar-related features perfectly,\nit may also prevent the model from learning more\nspeciﬁc features required for the ﬁnal task due to\nrare necessity in such features during MLM train-\ning and limited model capacity. To solve this prob-\nlem, we may simply lower the probability of sam-\npling positions containing frequent words. Figure 6\ncompares the standard Uniform MLM and the pro-\nposed NB-MLM to a frequency-based baseline. In\nthis baseline, we perform domain or task adaptation\nsimilarly to NB-MLM, but sample positions from\nP(pos) ∝( 1\nfreq (wpos) )\n1\nn , where n plays the same\nrole as the temperature in NB-MLM, allowing to\nbalance between sampling positions from the uni-\nform distribution and selecting positions contain-\ning the most infrequent words. Word frequencies\nfreq (w) are estimated from the training subset of\nthe IMDB dataset. We selected optimal values of\nn on the IMDB development set for all-TAPT and\nDAPT separately, resulting inn = 3.5 and n = 2.5\ncorrespondingly. Evidently, the frequency-based\nbaseline is on par with the uniform baseline. There\nare occasional improvements in the best validation\naccuracy, but they do not convert into improve-\nments on the test set.\nNext, we introduce another alternative, which is\nbased on the conditional pointwise mutual informa-\n9124\ntion between tokens and classes given context:\nPMI (w, c|ctx) = log\n(P(w|c, ctx)\nP(w|ctx)\n)\n.\nConceptually, it prefers to select tokens that are eas-\nier to predict based on the nearby context and class\nof the example than from the context alone. We\nsupposed that learning to predict such tokens will\nmake the model extract class-related features from\nthe whole example rather than use only nearby con-\ntext. We deﬁne the nearby context as one preceding\ntoken and one succeeding token and minimize PMI\nover them while maximizing it over classes. This\nmeans that we prefer selecting tokens, which are\nnot easily predicted from either preceding or suc-\nceeding tokens but are much better predicted, at\nleast for examples of one of the classes if that class\nis known.\nfi(wi) = max\nc\nmin\nctx∈{wi−1,wi+1}\nPMI (wi, c|ctx)\nSimilarly to NB-MLM, we estimated these weights\nfrom the IMDB training set and set them to zero\nfor those tokens that appear in less than m exam-\nples. Then we apply temperature softmax to con-\nvert these weights into a probability distribution\nover positions. We selected the hyperparameters on\nthe development set, resulting in T = 0.1, m= 10.\nFigure 7 shows that for all-TAPT on IMDB, the\nweights based on conditional MI do not help to\nimprove the results of the Uniform MLM, unlike\nNB weights. Based on these results, we did not\nexperiment with them for DAPT.\n20 40 60 80 100\nepoches\n95.9\n96.0\n96.1\n96.2\n96.3\n96.4\n96.5\n96.6\n96.7valid accuracy\nNB-MLM\nCondMI-MLM\nUniform\n20 40 60 80 100\nepoches\n95.6\n95.7\n95.8\n95.9\n96.0\n96.1\n96.2\n96.3\n96.4test accuracy\nNB-MLM\nCondMI-MLM\nUniform\nFigure 7: Comparison with the weights based on conditional mutual information for all-TAPT on the IMDB dev\n(left) and test (right) sets. Means and standard deviations over 6 runs are shown for each model.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8572299480438232
    },
    {
      "name": "Domain adaptation",
      "score": 0.8444982767105103
    },
    {
      "name": "Task (project management)",
      "score": 0.7048206925392151
    },
    {
      "name": "Inefficiency",
      "score": 0.7006206512451172
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.6993337869644165
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5871258974075317
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5564388632774353
    },
    {
      "name": "Language model",
      "score": 0.5531297922134399
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5067532658576965
    },
    {
      "name": "Sentiment analysis",
      "score": 0.491789847612381
    },
    {
      "name": "Machine learning",
      "score": 0.4808952808380127
    },
    {
      "name": "Naive Bayes classifier",
      "score": 0.4282112121582031
    },
    {
      "name": "Natural language processing",
      "score": 0.3734856843948364
    },
    {
      "name": "Support vector machine",
      "score": 0.08288758993148804
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}