{
  "title": "PyMT5: multi-mode translation of natural language and Python code with transformers",
  "url": "https://openalex.org/W3091798252",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2104015161",
      "name": "Colin Clément",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3086233478",
      "name": "Dawn Drain",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3092423288",
      "name": "Jonathan Timcheck",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2627672354",
      "name": "Alexey Svyatkovskiy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1981173961",
      "name": "Neel Sundaresan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1860267373"
  ],
  "abstract": "Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9052–9065,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n9052\nPYMT5: multi-mode translation of natural language and\nPYTHON code with transformers\nColin B. Clement∗\nMicrosoft Cloud and AI\ncoclemen@microsoft.com\nDawn Drain\nMicrosoft Cloud and AI\ndadrain@microsoft.com\nJonathan Timcheck†\nStanford University\ntimcheck@stanford.edu\nAlexey Svyatkovskiy\nMicrosoft Cloud and AI\nalsvyatk@microsoft.com\nNeel Sundaresan\nMicrosoft Cloud and AI\nneels@microsoft.com\nAbstract\nSimultaneously modeling source code\nand natural language has many excit-\ning applications in automated software\ndevelopment and understanding. Pur-\nsuant to achieving such technology, we\nintroduce P YMT5, the P YTHON method\ntext-to-text transfer transformer, which is\ntrained to translate between all pairs of\nPYTHON method feature combinations: a\nsingle model that can both predict whole\nmethods from natural language documen-\ntation strings (docstrings) and summarize\ncode into docstrings of any common style.\nWe present an analysis and modeling ef-\nfort of a large-scale parallel corpus of 26\nmillion P YTHON methods and 7.7 mil-\nlion method-docstring pairs, demonstrat-\ning that for docstring and method gen-\neration, P YMT5 outperforms similarly-\nsized auto-regressive language models\n(GPT2) which were English pre-trained\nor randomly initialized. On the C ODE -\nSEARCH NET test set, our best model pre-\ndicts 92.1% syntactically correct method\nbodies, achieved a BLEU score of 8.59 for\nmethod generation and 16.3 for docstring\n∗Corresponding author\n†Work done during a Microsoft internship\ngeneration (summarization), and achieved\na ROUGE-L F-score of 24.8 for method\ngeneration and 36.7 for docstring genera-\ntion.\n1 Introduction\nSoftware is a keystone of modern society,\ntouching billions of people through services\nand devices daily. Writing and documenting\nthe source code of this software are challeng-\ning and labor-intensive tasks; software devel-\nopers need to repeatedly refer to online doc-\numentation resources in order to understand\nexisting code bases to make progress. Devel-\noper productivity can be improved by the pres-\nence of source code documentation and a de-\nvelopment environment featuring intelligent,\nmachine-learning-based code completion and\nanalysis tools.\nRecent progress in natural language process-\ning (NLP), especially encoder/decoder-based\ntransformer models (Vaswani et al., 2017)\nand pre-training (Radford et al., 2018; Lewis\net al., 2019), has led to state-of-the-art per-\nformance on language modeling, classiﬁca-\ntion (Devlin et al., 2018), translation (Raffel\net al., 2019), summarization (Liu and Lap-\n9053\nata, 2019), grammar correction (Bryant et al.,\n2017), entity recognition, dialogue genera-\ntion (Budzianowski and Vuli ´c, 2019), and\nmore. Along with these quantitative advances\nhave come deeper understanding of the learned\nhidden representations which power transform-\ners (Kovaleva et al., 2019; V oita et al., 2019;\nClark et al., 2019; Ethayarajh, 2019). While\nthey are arguably not ‘natural,’ programming\nlanguages are increasingly becoming model-\ning playgrounds for NLP modeling. Since\nthese languages by deﬁnition have a gram-\nmar, syntax, and known relationships between\nentities, they offer enticing opportunities for\nan even deeper probing of NLP models and\ntasks. Beyond theoretical importance, many\nNLP tasks have practical utility in software\ndevelopment environments: language model-\ning or generation can be used for code com-\npletion (Raychev et al., 2014; Bruch et al.,\n2009; Svyatkovskiy et al., 2019, 2020), transla-\ntion/summarization to generate documentation\nor natural language summaries (Moreno et al.,\n2013; Scalabrino et al., 2017; Wan et al., 2018;\nAlon et al., 2018) or even summarize a set of\ncode changes (Moreno et al., 2014), transla-\ntion and grammar error correction to patch and\ndetect bugs (Zhai et al., 2019), and joint em-\nbedding of code and natural language for code\nsearch (Husain et al., 2019; Gu et al., 2018).\nIn this work we focus on jointly modeling\nboth source code (PYTHON ) and concomitant\nnatural language documentation (docstrings)\nwith transformers, through the study of dual\ntasks: generating method code bodies from\nsignatures and docstrings, and generating doc-\nstrings from signatures and method code bod-\nies. While previous work (Allamanis et al.,\n2015; Yin and Neubig, 2017) has leveraged the\ngrammar of code to extract features like the Ab-\nstract Syntax Tree for modeling (treating code\nand natural language as separate modalities),\nwe follow examples like Barone and Sennrich\n(2017) and treat PYTHON and its docstrings\nas fundamentally no different than other ‘natu-\nral’ languages, representing both source code\nand natural language docstrings as sequences\nof tokens sharing the same vocabulary. Here\nwe present a multi-mode translation method\nresulting in PYMT5 , the PYTHON method\ntext-to-text transfer transformer (inspired by\nthe text-to-text transfer transformer T5 (Raffel\net al., 2019)). Our single model can both learn\ncode/language generation and understand the\nrelationships between them.\nThe paper is organized as follows: we\nbegin in sec. 2 by presenting examples of\nthe performance of our novel multi-mode\nPYMT5 —the PYTHON method text-to-text\ntransfer transformer model—which we trained\nto translate between all pairs of combinations\nof method signatures, docstrings, and bod-\nies which do not have the same feature in\nboth the source and target. In sec. 2.1 we de-\nscribe our training data and the pre-processing\nsteps for source code and natural language\nwe followed, and compared it to existing par-\nallel docstring-method corpora like CODE -\nSEARCH NET (CSN)(Husain et al., 2019) and\nthat presented by Barone et al (Barone and Sen-\nnrich, 2017). In sec.2.2 we explain our BART-\nlike (Lewis et al., 2019) pre-training scheme,\ndemonstrating a 25× speed-up in training time\nfor docstring generation. Next, in sec. 2.3 we\nanalyze and classify PYTHON docstrings, en-\nabling style-conditioned docstring generation\nin PYMT5 . In sections 3 and 4, we discuss\nPYMT5 results on method generation and doc-\nstring generation respectively and compare it\nto two GPT2 models randomly initialized and\npre-trained on English.\n2 Multi-mode training\nFigure 1 shows examples of inputs and outputs\nof our model PYMT5 for 3 example tasks:\n(top, blue) predicting a body from a method\n9054\nFigure 1: Real examples of P YMT5 performing method generation using combinations of signatures\nand docstrings. A leading comment in the input sequence instructs the model to output a particular\ncombination of features, e.g. ‘ # target signature and body’ instructs P YMT5 to predict\nboth a signature and body.\nPYMT5\nFigure 2: PYMT5 performing docstring generation on an example method, showing the output when the\ntarget preﬁx indicates one line (top, blue) and Numpydoc docstring (bottom, red) styles.\nsignature, (middle, red) predicting a whole\nmethod from a natural language docstring,\nand (bottom, green) predicting a body from\na signature and docstring. Note that the com-\nment ‘# target <specification>’ in-\nstructs the model to choose a particular form\nof output. Further note that PYMT5 correctly\nlearns to interpret natural language: it inter-\nprets ‘even’ as being related to ‘(example\n%2) == 0’, and ‘greater than 1000’\nas ‘number > 1000’. The model also pro-\nduces syntactically correct code (as we will\ndiscuss later, we never show the model syntac-\ntically incorrect code), and correctly infers the\ntypes of ‘lst’ and ‘numbers’ to be iterables\ncontaining numbers.\nPYMT5 can also be prompted with source\ncode to produce a docstring summary in\nvarious styles. Figure 2 shows the model\nprompted with one of the methods generated\nby PYMT5 in Fig. 1 (top, blue), in both a\n‘one line’ (top, blue) style and a ‘Numpydoc’\n(bottom, red) style. It infers the intent from the\nsignature name and code, and even infers that\ntype of the argument is alist and return type\nint. It produces the same terse one sentence\nsummary of the function in both cases.\nIn order to teach PYMT5 to maximally re-\nlate the separate method features (signatures,\ndocstrings, bodies), we trained it to translate\n9055\nbetween all pairs of feature combinations in\nwhich the same feature does not appear in both\nthe source and target. This scheme is also ad-\nvantageous as our corpus is unbalanced, with\nonly 1/5 methods featuring docstrings, and so\nthe model can learn to leverage all the features\nwhether they are present or not. Additionally, it\nhas been shown that code is more ‘predictable’\nthan natural language (Hindle et al., 2012). If\nthe method and argument names are a domi-\nnating signal due to their relatively rigid struc-\nture, the model may learn to ignore the content\nof docstrings. This multi-mode method over-\ncomes that by training the model to generate\nmethod bodies from docstrings alone. See the\nappendix for a more detailed description of the\nmulti-mode training scheme.\n2.1 Dataset\nOur data consists of 118k GITHUB reposito-\nries, which includes all public repositories la-\nbelled as containing primarily PYTHON source\ncode, featuring at least 10 stars, and which\nhave had a commit in the past 5 years. We\nsuccessfully cloned 112k of these repositories,\nextracting 5.3 million PYTHON ﬁles from the\ndefault HEAD state of each repository. We then\nremoved literal duplicate ﬁles, resulting in 2.3\nmillion unique ﬁles, but did not remove ﬁner-\ngrained clones. After removing license from\nthe ﬁles, the literal contents were used in the\npre-training step, comprising about 27GB of\nraw text.\nIn order to extract method-level informa-\ntion for ﬁne-tuning, we used the python3.7\nstandard library ast to produce the ﬁle-\nlevel Abstract Syntax Tree (AST) for each\nPYTHON ﬁle, extracting every individual and\nclass method. For each ﬁle which failed to\nparse, we used 2to3 and autopep8 to over-\ncome the issue of different styles and white\nspace or tab conventions, successfully parsing\n97.3% of the 2.3 million unique PYTHON ﬁles.\nWe used the PYTHON module astunparse\nto take the AST for each method and unparse\nthem back into source code, so that our ﬁne-\ntuned model was never trained on syntactically\nincorrect code. The statistics of our method-\ndocstring corpus are summarized in Table. 1.\nOur parallel method-docstring corpus is twice\nas large as the next largest irrespective of lan-\nguage and over 15× as large as the next largest\nPYTHON parallel corpus, both in CSN.\nFor each method, we ignored comments as\nthey generally represent trivia and are not part\nof the normal language syntax. We cleaned the\ndocstrings by removing non-ASCII characters,\nnormalizing Unicode, and replacing commit\nhashes, ﬁle paths, and URLs with placeholder\ntokens. In all studies here, we randomly split\nthe ﬁles at the repository level (to prevent data\nleakage) with 90% for training, 5% for valida-\ntion, and 5% for a test set.\n2.2 Pre-training\nThe majority of our PYTHON methods—over\n20 million methods— do not possess doc-\nstrings. This imbalance is, in fact, an oppor-\ntunity in light of the recent trend for NLP:\nunsupervised pre-training of language mod-\nels on vast amounts of raw text (Devlin et al.,\n2018). Using these pre-trained models as start-\ning points for downstream tasks—like classi-\nﬁcation, translation, summarization, and ques-\ntion answering—consistently yields state-of-\nthe-art results (Lewis et al., 2019; Raffel et al.,\n2019).\nFollowing this trend, we use a similar span-\nmasking objective used by the recent text-to-\ntext transfer transformer (T5) (Raffel et al.,\n2019). As shown in Figure 3, after tokeniz-\ning the inputs, we sample a random subset of\nthe token spans up to length 3 to be replaced\nwith, e.g. a [MASK0] token, and then teach\nthe sequence-to-sequence model to replace the\nmissing tokens. The training target is com-\n9056\nDataset Methods w/ docstring Languages\nPYMT5 2.6 × 107 7.7 × 106 PYTHON\nCSN (Husain et al., 2019) 6.4 × 106 2.3 × 106 PYTHON , et al.\nCiurumelea et al. (2020) 1.6 × 105 1.6 × 105 PYTHON\nBarone and Sennrich (2017) 1.6 × 105 1.5 × 105 PYTHON\nTable 1: Summary statistics of our PYTHON parallel corpus compared to others presented in the literature.\nCSN contains 500k PYTHON methods with docstrings, among 6 other languages. Our parallel corpus is\n3× as large as the next largest, and over 15× the size of the next largest PYTHON parallel corpus.\nFigure 3: Denoising auto-encoder pre-training for sequence-to-sequence tasks, based on the span-\nmasking objective used by the T5 (Raffel et al., 2019). P YTHON ﬁles are ﬁrst tokenized with spaces\nreplaced by the character ˙G, which is 256 in ordinal above the space character (similarly for newlines,\ntabs, etc.). Note that indentation is a token of multiple ˙G’s. We replace random sub-sequences of tokens\nwith numbered masks, and train the model to return each mask followed by the tokens it replaced.\nprised of numbered mask tokens followed by\nthe tokens that mask represents.\nThe architecture of PYMT5 is an encode-\ndecoder transformer with a vocabulary of\n50181 (byte-pair BPE encoder trained on raw\npython ﬁles), 6 self-attention encoder/decoder\nlayers in each encoder layers, and a hidden di-\nmension of 1472, totaling 374 million parame-\nters. All the experiments in this paper, includ-\ning GPT2 were done using this same extended\nGPT tokenizer. We pre-trained PYMT5 on\n27GB of raw source code in total, for 3 weeks\non sixteen 32 GB Tesla V100 GPU s, or 73\nepochs total. When training on docstring gen-\neration alone, we observed 25× faster conver-\ngence to a lower loss when starting with this\npre-trained model as compared to a random ini-\ntialization. See the appendix for details. In all\nexperiments PYMT5 is trained starting with\nthis pre-trained model.\n2.3 Docstring analysis\nWhen examining docstring samples from our\ncorpus, one of the most salient features is\nthe different styles of documentation. The\nPYTHON community has no prescribed or de\nfacto style for docstrings, but PYTHON en-\nhancement protocol 257 (Goodger and van\nRossum, 2001) does describe one-line and\nmulti-line docstrings, and mandates indenta-\ntion as well. Most modern large-scale projects\nutilize docstring styles which are parseable, al-\nlowing the automatic creation and synchroniza-\ntion of source code and documentation web-\nsites, see, e.g. sphinx. Therefore, a number\nof standard styles have evolved in the commu-\nnity.\nThe currently dominant parseable docstring\n9057\nstyles (and the ones supported by sphinx)\nare RESTRUCTURED TEXT (reST) (Jones,\n2013), the ofﬁcial GOOGLE style (Google,\n2020), NUMPY style (also technically satis-\nﬁes reST) (Maintainers, 2020), and JAVADOC\nstyle (jav, 2011). The difference be-\ntween each style is mainly in the syntax\nof denoting sections (if they exist) and\nthe name/type/description annotation of the\nmethod arguments and returned/yielded quan-\ntities (if they exist). We deﬁned, in addi-\ntion to these styles, one-line (containing only\none line), one-paragraph (containing no empty\nlines), and ‘other’ to label any docstring not\ndescribed so far, which includes informal user\ndocstring styles and a few project-speciﬁc\nstyles like the SAGE mathematics toolkit li-\nbrary.\nTable 2 shows the breakdown of the fraction\nof each of these styles in our corpus. The plu-\nrality of docstrings (44%) are one-line. The\nnext most common style is one-paragraph at\n14%. The next four most-common styles are\nthe machine parseable styles discussed above,\ncomprising 26.2% of the total number of doc-\nstrings. The appendix contains detailed dis-\ntributions of method signature, docstring, and\nmethod body character and line lengths.\nStyle Fraction of methods\nOne line 44%\nOne paragraph 14%\nREST 13%\nGOOGLE 7.3%\nNUMPY 4.8%\nJAVADOC 1.6%\nOther 15%\nTable 2: Docstring style statistics from 7.7 million\nPYTHON docstrings.\nTo visualize the space of these styles, we\nused FAST TEXT vector embeddings of the doc-\nstrings, obtaining 100-dimension continuous\nvector representations of each. We then used\nPCA to reduce the dimensionality to 50 and ap-\nplied the t-distributed stochastic neighbor em-\nbedding (T-SNE ) to obtain a two-dimensional\nvisualization. Figure 4 shows 1/10th of our\ncorpus (700k docstrings) embedded, colored\nby docstring style as deﬁned above. We can\nsee clear clustering of styles, indicating that\nsimilar docstrings use the same style (for the\nparseable styles). There is also a natural di-\nchotomy between parseable and non-parseable\nstyles: the left side is dominated by ‘one line,’\n‘one paragraph,’ and ‘other’ styles, and the four\nparseable styles are largely on the right side.\nThis observation can be used to generate docu-\nmentation consistent with the style of a given\nproject, or it could be used to translate meth-\nods into more informal descriptions useful for\nsearch indices.\nFigure 4: Visualization of continuous embed-\ndings of 1/10th of our docstring corpus (770k doc-\nstrings), colored by docstring style. Embeddings\nwere obtained using F AST TEXT , and the two-\ndimensional embedding was obtained via PCA\n(for dimensionality reduction and initialization)\nand t-SNE.\n9058\nModel Ppl BLEU Syntax Stat. R1 R2 RL\nGPT2-med 2.36 5.60 85% Prec. 25.8 12.3 26.8\nrandom Rec. 26.7 12.1 25.9\nF1 21.8 10.6 22.5\nGPT2-med 2.09 5.63 86% Prec. 25.4 12.1 26.3\nenglish Rec. 26.9 12.2 26.1\nF1 21.7 10.6 22.5\nPYMT5 2.36 10.6 93.6% Prec. 33.8 21.5 33.6\nRec. 44.1 25.0 43.8\nF1 35.1 21.5 32.2\nCSN test:\nGPT2-med – 2.8 77.2% Prec. 32.3 11.8 33.7\nrandom Rec. 19.6 7.0 19.3\nF1 20.9 7.6 21.9\nPYMT5 – 8.59 92.1% Prec. 25.6 12.5 25.3\nRec. 40.2 18.3 39.6\nF1 28.4 13.5 24.8\nBarone and Sennrich (2017) test:\nPYMT5 – 20.2 91.1% Prec. 41.3 28.5 40.7\nRec. 52.2 34.7 51.3\nF1 43.2 29.8 39.7\nBarone et al. – 10.9 – – – –\nTable 3: Comparing 3 models–GPT2 with a random weight initialization, GPT2 pre-trained on English,\nand P YMT5–on the task of method generation from a signature and natural language docstring. The\nﬁrst three rows use our test set consisting of 1,285,794 methods. The fourth and ﬁfth rows compare\nthe performance of P YMT5 and GPT2-medium on the CodeSearchNet P YTHON test set. The ﬁnal\nrows compare the performance of PYMT5 on the parallel corpus test set of Barone and Sennrich (2017).\nSyntax is the fraction of predicted methods which had correct syntax using the PYTHON 3.7 grammar.\n3 Method generation\nNow we turn our attention to method gener-\nation: predicting a whole method code body\nfrom either a method signature, a natural lan-\nguage docstring, or both. We ﬁrst discuss a\nbenchmark of this task using a GPT2-medium\nmodel (345 million parameters, see the ap-\npendix for details), training from scratch and\nstarting with the publicly releasedOPEN AI En-\nglish pre-trained checkpoint with weights from\nHuggingFace(Wolf et al., 2019). In all experi-\nments we used an extended GPT2 tokenizer—\nincluding white-space (one tab, two tabs, etc.)\ntokens—for a total vocabulary size of 50337,\nand we used beam decoding with a beam width\nof 5.\nThe third row of tab. 3 shows PYMT5 has\nmore than double the BLEU score, overall\nbetter recall, and signiﬁcantly better ROUGE-\n2 and ROUGE-L F-scores than our GPT2\nbaselines. Further, 93.6% of the methods\ngenerated by PYMT5 were syntactically\ncorrect PYTHON 3.7, whereas only 86% of\nGPT2 methods were syntactically correct.\nPYMT5 was trained on 16 Tesla V100 16GB\nGPUs for 62 epochs, or 5 weeks training time\n(see the appendix for its hyper-parameters) and\nthe GPT2 baselines were trained on the same\nhardware for 1 week training time (achieving\nthe same or better validation loss/perplexity as\nPYMT5).\nThe English pre-trained initialization of\nGPT2 only slightly beats the random initial-\nization of GPT2, which could indicate that the\nlearned biases of English are not particularly\nbeneﬁcial for writing PYTHON code; the met-\nrics are almost all within our margin of error.\nNote that Barone and Sennrich (2017) also\nmodeled methods from docstrings, obtaining\na similar BLEU score of 10.9 on their own\nPYTHON parallel corpus. On the Barone et al.\ntest set, PYMT5 obtains nearly double these\nscores at 20.2; such a large discrepancy could\nbe explained by data leaking from their test set\n9059\nModel Ppl BLEU R1 R2 RL\nGPT2-med 2.36 19.4 P 32.6 19.3 33.6\nrandom R 36.2 19.4 34.7\nF1 31.0 18.2 31.6\nGPT2-med 2.15 19.6 P 33.1 19.4 33.9\nEnglish R 36.4 19.5 34.8\nF1 31.4 18.3 31.8\nPYMT5 3.74 25.2 P 42.1 23.7 41.3\nR 50.4 27.0 49.3\nF1 43.3 24.4 39.8\nCSN test:\nGPT2-med – 9.5 P 30.6 13.3 31.4\nrandom R 31.1 12.9 29.8\nF1 26.3 11.5 27.2\nPYMT5 – 16.3 P 38.0 19.2 36.8\nR 52.7 24.5 51.0\nF1 41.3 20.4 36.7\nBarone test:\nPYMT5 – 17.4 P 39.6 26.0 38.7\nR 53.6 33.7 52.1\nF1 43.1 27.8 39.1\nBarone et al. – 13.84 – – – –\nTable 4: Comparing 3 models–GPT2 with a ran-\ndom weight initialization, GPT2 pre-trained on\nEnglish, and P YMT5–on the task of natural lan-\nguage docstring generation from a signature and\nmethod body. The ﬁrst three rows are evaluated\non our test set of 383695 methods. The fourth\nand ﬁfth rows shows performance of PYMT5 and\nGPT2-medium on the CSN P YTHON test set, and\nthe last two rows compare our model to Barone et\nal. on their test set.\ninto our training set. Barone’s test set is also\n200× smaller than ours and may not be a rep-\nresentative sample of the wholePYTHON code\ndomain.\nThe third and fourth rows of tab. 3 show the\nperformance of PYMT5 using the publicly\navailable CSN PYTHON test set, from which\nwe ﬁnd notably worse results than on our own\ntest set. CSN curated their whole set by remov-\ning any methods with ‘test’ in the name and any\nmethods with fewer than 3 lines of code. We\ncalculated the performance of PYMT5 only\non a subset of our test set curated the same\nway as CSN, observing F-scores for R1, R2,\nand R-L on our test set of 29.7, 17.2, and 26.1,\nwhich is lower than our nominal test set perfor-\nmance of 35.1, 21.5, and 32.2 and closer to the\nCSN performance of 28.4, 13.5, and 24.8. We\nbelieve this curating choice explains the differ-\nence between our test set and the CSN test set.\nWe also conclude that tests and short methods\nare ‘easier’ to complete, which is plausible,\nand bodes well for automatic code completion\napplications.\n4 Docstring Generation\nWe now examine results from the docstring\ngeneration task, which for evaluation pur-\nposes were conditioned on both signatures and\nmethod bodies. As in method generation, we\nset a GPT2 benchmark with random initial-\nization and pre-trained English initialization\nas well as the same hyperparameters. Table 4\nshows that the ROUGE scores of the GPT2\nbaselines are within the margin of error; a\nsomewhat surprising result given the English\ndomain of docstrings. The third row shows\nPYMT5 to be superior to GPT2-medium in\nterms of BLEU and all of the ROUGE metrics.\nWe again present the results from the pub-\nlicly available CSN test set. Similar to the\nmethod generation task, PYMT5 performs\nworse on the CSN data than our own, likely\nfor the same reasons we discussed in sec. 3.\nWe also evaluated PYMT5 on the Barone et\nal. parallel test set, as shown in the second to\nlast row of tab. 4, and ﬁnd PYMT5 performs\nnotably worse on Barone’s test set than our\nown test set, contradicting the hypothesis that\nour doubling of the method generation BLEU\nscore is due to data leakage. PYMT5 has a\nmuch higher BLEU score than that reported by\nBarone et al, perhaps indicating real progress\nin the code summarization ﬁeld.\nDocstring generation is similar to code sum-\nmarization, though the domains are different as\ndocstrings also contain structured annotations\nof arguments, return values, raised exceptions,\nand even in-line unit tests (doctest). TranS3\nby Wang et al. (Wang et al., 2020) reports a\nbest ROUGE-L of 51.27 on the same test set\nfor code summarization, but does not specify\n9060\nwhich statistic they are reporting, so we can-\nnot make strong conclusions about the perfor-\nmance of PYMT5 compared to the state of the\nart.\n5 Conclusion\nIn this work, we presented a novel multi-mode\nPYTHON method text-to-text transfer trans-\nformer model PYMT5 as well as the largest\nparallel corpus of PYTHON source code and\ndocstrings reported in the literature to date. We\nhave trained PYMT5 to translate between\nall pairs of combinations of method signa-\ntures, docstrings, and method bodies which\ndo not have the same feature in both the source\nand target. Further, we introduced control\ntoken preﬁxes for docstring generation to fa-\ncilitate docstring generation of various styles.\nFocusing on two modeling tasks – predict-\ning PYTHON methods from docstrings and\nsummarizing PYTHON source code methods\ninto docstrings of various commonly occur-\nring styles – we have compared this new ap-\nproach to the auto-regressive GPT2 baselines\ntrained on individual docstring or method gen-\neration tasks. On the CODE SEARCH NET test\nset PYMT5 achieves a BLEU score of 8.59\nfor method generation and 16.3 for docstring\ngeneration, and a ROUGE-L F-score of 24.8\nfor method generation and 36.7 for docstring\ngeneration. We have demonstrated the ef-\nfectiveness of dynamic masked pre-training,\nreducing docstring generation training time\nby 25×. Looking forward, we plan to lever-\nage PYMT5 for various downstream auto-\nmated software engineering tasks—including\ncode documentation and method generation\nfrom natural language statements—and de-\nvelop more model evaluation criteria to lever-\nage the unique properties of source codes.\nAcknowledgements\nWe would like to thank the Microsoft Cloud\nand AI SmartML engineering team for help\nin preparing the data, Shao Kun Deng for the\ndevelopment of compelling user experiences\nleveraging PYMT5, and Christian Bird for use-\nful discussions.\nReferences\n2011. Java doc. Technical report.\nMiltiadis Allamanis, Daniel Tarlow, Andrew D.\nGordon, and Yi Wei. 2015. Bimodal modelling\nof source code and natural language. In Pro-\nceedings of the 32nd International Conference\non International Conference on Machine Learn-\ning - Volume 37, ICML’15, page 2123–2132.\nJMLR.org.\nUri Alon, Shaked Brody, Omer Levy, and Eran\nYahav. 2018. code2seq: Generating sequences\nfrom structured representations of code. arXiv\npreprint arXiv:1808.01400.\nAntonio Valerio Miceli Barone and Rico Sennrich.\n2017. A parallel corpus of python functions and\ndocumentation strings for automated code doc-\numentation and code generation. arXiv preprint\narXiv:1707.02275.\nMarcel Bruch, Martin Monperrus, and Mira\nMezini. 2009. Learning from examples to im-\nprove code completion systems. In Proceed-\nings of the 7th joint meeting of the European\nsoftware engineering conference and the ACM\nSIGSOFT symposium on the foundations of soft-\nware engineering, pages 213–222.\nChristopher Bryant, Mariano Felice, and Edward\nBriscoe. 2017. Automatic annotation and eval-\nuation of error types for grammatical error cor-\nrection. Association for Computational Lin-\nguistics.\nPaweł Budzianowski and Ivan Vuli´c. 2019. Hello,\nit’s gpt-2–how can i help you? towards the\nuse of pretrained language models for task-\noriented dialogue systems. arXiv preprint\narXiv:1907.05774.\n9061\nAdelina Ciurumelea, Sebastian Proksch, and Har-\nald Gall. 2020. Suggesting comment comple-\ntions for python using neural language models.\nIn 27th edition of the IEEE International Con-\nference on Software Analysis, Evolution and\nReengineering (SANER). IEEE.\nKevin Clark, Urvashi Khandelwal, Omer Levy,\nand Christopher D Manning. 2019. What does\nbert look at? an analysis of bert’s attention.\narXiv preprint arXiv:1906.04341.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805.\nKawin Ethayarajh. 2019. How contextual are con-\ntextualized word representations? comparing\nthe geometry of bert, elmo, and gpt-2 embed-\ndings. arXiv preprint arXiv:1909.00512.\nDavid Goodger and Guido van Rossum. 2001.\nDocstring conventions. PEP 257.\nGoogle. 2020. Google python style guide. Techni-\ncal report.\nXiaodong Gu, Hongyu Zhang, and Sunghun Kim.\n2018. Deep code search. In Proceedings of the\n40th International Conference on Software En-\ngineering, ICSE ’18, page 933–944, New York,\nNY , USA. Association for Computing Machin-\nery.\nAbram Hindle, Earl T Barr, Zhendong Su, Mark\nGabel, and Premkumar Devanbu. 2012. On the\nnaturalness of software. In 2012 34th Inter-\nnational Conference on Software Engineering\n(ICSE), pages 837–847. IEEE.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit,\nMiltiadis Allamanis, and Marc Brockschmidt.\n2019. Codesearchnet challenge: Evaluating the\nstate of semantic code search. arXiv preprint\narXiv:1909.09436.\nRichard Jones. 2013. A restructuredtext primer.\ndocutils. sourceforge. net, March.\nOlga Kovaleva, Alexey Romanov, Anna Rogers,\nand Anna Rumshisky. 2019. Revealing\nthe dark secrets of bert. arXiv preprint\narXiv:1908.08593.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\n2019. Bart: Denoising sequence-to-sequence\npre-training for natural language generation,\ntranslation, and comprehension. arXiv preprint\narXiv:1910.13461.\nYang Liu and Mirella Lapata. 2019. Text sum-\nmarization with pretrained encoders. arXiv\npreprint arXiv:1908.08345.\nNumpydoc Maintainers. 2020. Numpydoc doc-\nstring guide. Technical report.\nLaura Moreno, Jairo Aponte, Giriprasad Sridhara,\nAndrian Marcus, Lori Pollock, and K Vijay-\nShanker. 2013. Automatic generation of nat-\nural language summaries for java classes. In\n2013 21st International Conference on Pro-\ngram Comprehension (ICPC) , pages 23–32.\nIEEE.\nLaura Moreno, Gabriele Bavota, Massimiliano\nDi Penta, Rocco Oliveto, Andrian Marcus, and\nGerardo Canfora. 2014. Automatic genera-\ntion of release notes. In Proceedings of the\n22nd ACM SIGSOFT International Symposium\non Foundations of Software Engineering, pages\n484–495.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu.\n2019. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\nVeselin Raychev, Martin Vechev, and Eran Ya-\nhav. 2014. Code completion with statistical\nlanguage models. In Proceedings of the 35th\nACM SIGPLAN Conference on Programming\nLanguage Design and Implementation, pages\n419–428.\nSimone Scalabrino, Gabriele Bavota, Christo-\npher Vendome, Mario Linares-V´asquez, Denys\n9062\nPoshyvanyk, and Rocco Oliveto. 2017. Au-\ntomatically assessing code understandability:\nHow far are we? In 2017 32nd IEEE/ACM In-\nternational Conference on Automated Software\nEngineering (ASE), pages 417–427. IEEE.\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu\nFu, and Neel Sundaresan. 2020. Intellicode\ncompose: Code generation using transformer.\narXiv preprint arXiv:2005.08025.\nAlexey Svyatkovskiy, Ying Zhao, Shengyu Fu,\nand Neel Sundaresan. 2019. Pythia: Ai-assisted\ncode completion system. In Proceedings of the\n25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, pages\n2727–2735.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Advances in neural\ninformation processing systems, pages 5998–\n6008.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019.\nThe bottom-up evolution of representations in\nthe transformer: A study with machine transla-\ntion and language modeling objectives. arXiv\npreprint arXiv:1909.01380.\nYao Wan, Zhou Zhao, Min Yang, Guandong Xu,\nHaochao Ying, Jian Wu, and Philip S Yu. 2018.\nImproving automatic source code summariza-\ntion via deep reinforcement learning. In Pro-\nceedings of the 33rd ACM/IEEE International\nConference on Automated Software Engineer-\ning, pages 397–407.\nWenhua Wang, Yuqun Zhang, Zhengran Zeng, and\nGuandong Xu. 2020. Transˆ 3: A transformer-\nbased framework for unifying code summa-\nrization and code search. arXiv preprint\narXiv:2003.03238.\nThomas Wolf, Lysandre Debut, Victor Sanh,\nJulien Chaumond, Clement Delangue, An-\nthony Moi, Pierric Cistac, Tim Rault, R’emi\nLouf, Morgan Funtowicz, and Jamie Brew.\n2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. ArXiv,\nabs/1910.03771.\nPengcheng Yin and Graham Neubig. 2017. A syn-\ntactic neural model for general-purpose code\ngeneration. In Proceedings of the 55th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages\n440–450, Vancouver, Canada. Association for\nComputational Linguistics.\nJuan Zhai, Xiangzhe Xu, Yu Shi, Minxue Pan,\nShiqing Ma, Lei Xu, Weifeng Zhang, Lin Tan,\nand Xiangyu Zhang. 2019. Cpc: automatically\nclassifying and propagating natural language\ncomments via program analysis.\nA Appendix\nA.1 Docstring statistics\nFigure 5 shows the distributions of various fea-\ntures of docstrings in our corpus. The top row\nis the distribution of total character-level length\nof the method signatures (left), docstrings (cen-\nter), and code bodies. The blue lines are for\nmethods possessing a docstring, and we can\nsee that the vast majority of these methods\nhave docstrings with more than 10 characters.\nThe bottom row shows the distribution of line\nlengths of the concomitant features from the\ntop row. While the most common line length\nof docstrings is 1 (comprising 41%), the vast\nmajority of docstrings have multiple lines.\nA.2 Pre-training details\nFigure 7 is the complete training script,\nusing the Facebook AI Research Se-\nquence ( FAIR SEQ) modeling library, with\nwhich we pre-trained PYMT5 . The data\nwas pre-noised and processed using the\nfairseq-preprocess command, and\nplaced in the directory indicated by $DIR.\nThe architecture and training hyper-parameters\nare set in this script. PYMT5 was trained\nwith the same hyperparameters, but with data\ndescribed in sec.A.4.\nFigure 7 shows learning curves of a sin-\ngle seq2seq model of the same architecture as\nPYMT5 trained only on docstrings, starting\nfrom random initializations, and starting from\nour pre-trained model. As the ﬁgure shows, the\n9063\nFigure 5: Histogram of the number of characters (top row) in the P YTHON signatures (left), docstrings\n(middle), and method body (right). The blue lines are for methods with docstrings, the yellow lines are for\nmethods without docstrings. The vast majority of docstrings have more than 10 characters. The bottom\nrow shows histograms of the number of lines for the same features described in the top row.\npre-trained initialization converged to a better\nvalidation loss 25× faster than the randomly\ninitialized model.\nA.3 GPT2 training details\nOur GPT2 experiments also used the FAIR SEQ\nlibrary, with the OpenAI English checkpoint\nsupplied by the HuggingFace library. Fig-\nure 8 shows the complete training script, where\nfor the English pre-trained initialization a pre-\ntrained checkpoint was provided. Each models\nwas trained on 4 Tesla V100 GPUs with 16GB\nof memory each, for 7 days.\nA.4 Multi-mode training details\nIn order to better teach PYMT5 to under-\nstand the relationships between all the differ-\nent features of code (signatures, docstrings,\nand bodies) we taught it to translate between\nFigure 6: Learning curves for training a sequence-\nto-sequence transformer, translating from python\nmethod deﬁnitions to their docstrings. Blue curves\nrepresent the training and validation loss, and show\nthat convergence (validation loss stops decreasing)\noccurs after 3.97 × 105 steps or 183 epochs. The\noptimization of the pre-trained model with identi-\ncal hyperparameters reaches and beats the best val-\nidation loss at 1.5 × 104 steps or 7 epochs.\n9064\nFigure 7: The fairseq-train script used to\npre-train P YMT5, setting all the relevant hyper-\nparameters.\nFigure 8: The fairseq-train script we used\nto train our GPT model baselines\nall pairs of combinations of these features\nwhich do not contain the same feature in\nboth the source and target. In this way, the\nmodel can learn to produce method bodies us-\ning both signatures and docstrings, or one or\nthe other. Table 5 spells out exactly which\ncombinations were provided to the model\nas a source and target. For each source\nexample the comment string ‘ # target\n<feature> (<style>)’ was added, in-\nstructing the model which feature combination\n(e.g. signature and body). Only if a docstring\nwas in the target, a style imperative was added,\nwhere the styles are deﬁned and discussed in\nthe main text.\nFigure 9 shows the training curves for\nPYMT5, where the solid black line is the train-\ning loss, and all the other curves are the valida-\ntion loss for each of the tasks indicated in tab. 5.\nThe dashed lines indicate tasks where doc-\nstrings are present in the target, showing that\nthese are generally less predictable than code-\nonly targets (as the validation loss is larger).\nPYMT5 was trained on 16 Tesla V100 16GB\nGPUs for 62 epochs, or 5 weeks training time.\nSourcesSignature\nDosctring\nBody\nSig + doc\nSig + body\nDoc + body\nSignature \u0013 \u0013 \u0013\nDocstring \u0013 \u0013 \u0013\nBody \u0013 \u0013 \u0013\nSig + doc \u0013\nTargets\nSig + body \u0013\nDoc + body \u0013\nTable 5: A table of all possible translation possibil-\nities between the 3 features of a function: the sig-\nnature (sig), docstring (doc), and body. We train\nour model to translate between sources and targets\nindicated with a \u0013, which were chosen as all pairs\nof feature combinations which do not contain the\nsame feature in both the source and target. The sys-\ntem is then instructed to target code bodies when\nperforming function completion.\n9065\nFigure 9: Learning curve for the multi-mode training, where the black line is the training loss, and the\nother lines are the validation loss for each mode of translation. Dashed lines indicate the docstrings are\nin the target, solid lines have only code in the target.",
  "topic": "Python (programming language)",
  "concepts": [
    {
      "name": "Python (programming language)",
      "score": 0.9068695306777954
    },
    {
      "name": "Computer science",
      "score": 0.7998619079589844
    },
    {
      "name": "Automatic summarization",
      "score": 0.6145071983337402
    },
    {
      "name": "Transformer",
      "score": 0.6131119132041931
    },
    {
      "name": "Machine translation",
      "score": 0.5953489542007446
    },
    {
      "name": "Programming language",
      "score": 0.5900827050209045
    },
    {
      "name": "Natural language processing",
      "score": 0.5601462125778198
    },
    {
      "name": "Artificial intelligence",
      "score": 0.52491694688797
    },
    {
      "name": "Code generation",
      "score": 0.5077215433120728
    },
    {
      "name": "Natural language",
      "score": 0.4931182563304901
    },
    {
      "name": "Source code",
      "score": 0.4763079881668091
    },
    {
      "name": "Operating system",
      "score": 0.08498191833496094
    },
    {
      "name": "Engineering",
      "score": 0.0769931972026825
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}