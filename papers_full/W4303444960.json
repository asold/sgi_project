{
  "title": "Hybrid Transformer Network for Deepfake Detection",
  "url": "https://openalex.org/W4303444960",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2476675712",
      "name": "Sohail Ahmed Khan",
      "affiliations": [
        "University of Bergen"
      ]
    },
    {
      "id": "https://openalex.org/A2760104377",
      "name": "Duc-Tien Dang-Nguyen",
      "affiliations": [
        "University of Bergen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2412509443",
    "https://openalex.org/W2009130368",
    "https://openalex.org/W3206817819",
    "https://openalex.org/W3092997524",
    "https://openalex.org/W2981803355",
    "https://openalex.org/W4365806366",
    "https://openalex.org/W2909336075",
    "https://openalex.org/W3083246145"
  ],
  "abstract": "Deepfake media is becoming widespread nowadays because of the easily available tools and mobile apps which can generate realistic looking deepfake videos/images without requiring any technical knowledge. With further advances in this field of technology in the near future, the quantity and quality of deepfake media is also expected to flourish, while making deepfake media a likely new practical tool to spread mis/disinformation. Because of these concerns, the deepfake media detection tools are becoming a necessity. In this study, we propose a novel hybrid transformer network utilizing early feature fusion strategy for deepfake video detection. Our model employs two different CNN networks, i.e., (1) XceptionNet and (2) EfficientNet-B4 as feature extractors. We train both feature extractors along with the transformer in an end-to-end manner on FaceForensics++, DFDC benchmarks. Our model, while having relatively straightforward architecture, achieves comparable results to other more advanced state-of-the-art approaches when evaluated on FaceForensics++ and DFDC benchmarks. Besides this, we also propose novel face cut-out augmentations, as well as random cut-out augmentations. We show that the proposed augmentations improve the detection performance of our model and reduce overfitting. In addition to that, we show that our model is capable of learning from considerably small amount of data.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8634101152420044
    },
    {
      "name": "Overfitting",
      "score": 0.8254836797714233
    },
    {
      "name": "Transformer",
      "score": 0.5596343874931335
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5480603575706482
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4643993675708771
    },
    {
      "name": "Deep learning",
      "score": 0.46141135692596436
    },
    {
      "name": "Machine learning",
      "score": 0.4603867530822754
    },
    {
      "name": "Data science",
      "score": 0.3365422785282135
    },
    {
      "name": "Artificial neural network",
      "score": 0.25302213430404663
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}