{
    "title": "A Hierarchical Transformer with Speaker Modeling for Emotion Recognition in Conversation",
    "url": "https://openalex.org/W3113800548",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2148827383",
            "name": "Li Jiang-nan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2009932017",
            "name": "Lin Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114657071",
            "name": "Fu Peng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221918251",
            "name": "Si, Qingyi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1926383388",
            "name": "Wang Wei-ping",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2998672049",
        "https://openalex.org/W2146334809",
        "https://openalex.org/W2604314403",
        "https://openalex.org/W2805662932",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2891359673",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3006478952",
        "https://openalex.org/W2972965453",
        "https://openalex.org/W2970431814",
        "https://openalex.org/W2964300796",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2740550900",
        "https://openalex.org/W2996849360",
        "https://openalex.org/W2965453734",
        "https://openalex.org/W3035413677",
        "https://openalex.org/W3205498744",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963873807"
    ],
    "abstract": "Emotion Recognition in Conversation (ERC) is a more challenging task than conventional text emotion recognition. It can be regarded as a personalized and interactive emotion recognition task, which is supposed to consider not only the semantic information of text but also the influences from speakers. The current method models speakers' interactions by building a relation between every two speakers. However, this fine-grained but complicated modeling is computationally expensive, hard to extend, and can only consider local context. To address this problem, we simplify the complicated modeling to a binary version: Intra-Speaker and Inter-Speaker dependencies, without identifying every unique speaker for the targeted speaker. To better achieve the simplified interaction modeling of speakers in Transformer, which shows excellent ability to settle long-distance dependency, we design three types of masks and respectively utilize them in three independent Transformer blocks. The designed masks respectively model the conventional context modeling, Intra-Speaker dependency, and Inter-Speaker dependency. Furthermore, different speaker-aware information extracted by Transformer blocks diversely contributes to the prediction, and therefore we utilize the attention mechanism to automatically weight them. Experiments on two ERC datasets indicate that our model is efficacious to achieve better performance.",
    "full_text": "A Hierarchical Transformer with Speaker Modeling for Emotion Recognition in\nConversation\nJiangnan Li 1,2, Zheng Lin 1, Peng Fu 1, Qingyi Si 1,2, Weiping Wang1\n1Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\n2School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China\n{lijiangnan, linzheng, fupeng, siqingyi, wangweiping}@iie.ac.cn\nAbstract\nEmotion Recognition in Conversation (ERC) is a more chal-\nlenging task than conventional text emotion recognition. It\ncan be regarded as a personalized and interactive emotion\nrecognition task, which is supposed to consider not only the\nsemantic information of text but also the inÔ¨Çuences from\nspeakers. The current method models speakers‚Äô interactions\nby building a relation between every two speakers. How-\never, this Ô¨Åne-grained but complicated modeling is com-\nputationally expensive, hard to extend, and can only con-\nsider local context. To address this problem, we simplify\nthe complicated modeling to a binary version: Intra-Speaker\nand Inter-Speaker dependencies, without identifying every\nunique speaker for the targeted speaker. To better achieve the\nsimpliÔ¨Åed interaction modeling of speakers in Transformer,\nwhich shows excellent ability to settle long-distance depen-\ndency, we design three types of masks and respectively utilize\nthem in three independent Transformer blocks. The designed\nmasks respectively model the conventional context modeling,\nIntra-Speaker dependency, and Inter-Speaker dependency.\nFurthermore, different speaker-aware information extracted\nby Transformer blocks diversely contributes to the prediction,\nand therefore we utilize the attention mechanism to automati-\ncally weight them. Experiments on two ERC datasets indicate\nthat our model is efÔ¨Åcacious to achieve better performance.\nIntroduction\nNowadays, intelligent machines to precisely capture speak-\ners‚Äô emotions in conversations are gaining popularity, thus\ndriving the development of Emotion Recognition in Conver-\nsation (ERC). ERC is a task to predict the emotion of the\ncurrent utterance expressed by a speciÔ¨Åc speaker according\nto the context (Poria et al. 2019b), which is more challenging\nthan the conventional emotion recognition only considering\nsemantic information of an independent utterance.\nTo precisely predict the emotion of a targeted utterance,\nboth the semantic information of the utterance and the in-\nformation provided by utterances in the context are criti-\ncal. Nowadays, a number of works (Hazarika et al. 2018a,b;\nMajumder et al. 2019; Ghosal et al. 2019) demonstrate\nthat the interactions between speakers can facilitate extract-\ning information from contextual utterances. We denote this\nkind of information with modeling speakers‚Äô interactions as\nCopyright ¬© 2021, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nTimeline\nPhoebe\nHey!\njoy\nu1\nJoey\nHey-Hey-Hey!\njoy\nu2\nChandler\nWhat are you doing?\nneutral\nu3\nPhoebe\nWe‚Äôre just celebrating that \nJoey got his health \ninsurance back.\njoy\nu4\nChandler\nOh, all right. \nneutral\nu5\n(b). Relational graph for u3, u4 and u5 in\nSelf  and  Inter-speaker  dependencies.  7 \nrelations are involved. \n(c). Relational graph for u3, u4 and u5  in \nsimplified dependencies. Only 2 relations  \nare involved. \nSimplified \ndependencies\nSelf and Inter-speaker\ndependencies\nIntra-Speaker\nInter-Speaker\nPho‚ÜíPho\nCha‚ÜíPho\nJoe‚ÜíPho\nhistory\nfuture\nCha‚ÜíCha\nPho‚ÜíCha\n2\n3 1 7\n5\n6\n4\n1\n1\nu3\nu4 u5\n2 2\n2\n2\n1 1\n1\n1\n1\nu3\nu4 u5\n(a). Illustration of speakers‚Äô interactions. \nFigure 1: (a) illustrates a conversation clip of 3 speakers and\nthe utterance in yellow frame is selected as a targeted utter-\nance; (b) and (c) illustrate the relation graphs of u3, u4, u5\nin different dependencies.\nspeaker-aware contextual information. To capture speaker-\naware contextual information, the state-of-the-art model Di-\nalogueGCN (Ghosal et al. 2019) introduces Self and Inter-\nSpeaker dependencies, which capture the inÔ¨Çuences from\ndifferent speakers. As illustrated in Fig. 1 (a), Self and Inter-\nSpeaker dependencies establish a speciÔ¨Åc relation between\nevery two speakers and construct a fully connected relational\ngraph. And then a Relational Graph Convolutional Network\n(RGCN) (Schlichtkrull et al. 2018) is applied to process such\na graph.\nAlthough DialogueGCN can achieve excellent perfor-\nmance with Self and Inter-Speaker dependencies, this\nspeaker modeling is easy to be complicated with the number\nof speakers increasing. As shown in Fig. 1 (b), for a con-\nversation clip with two speakers, the considered relations\nreach to 7. The number can drastically increase with more\nspeakers involved. Thus this complicated speaker modeling\nis hard to deal with the condition that the number of speak-\ners dynamically changes, and not Ô¨Çexible to be deployed in\nother models. In addition, RGCN processing the fully con-\nnected graph with multiple relations requires tremendous\narXiv:2012.14781v1  [cs.CL]  29 Dec 2020\nconsumption of computation. This limitation leads to Dia-\nlogueGCN only considering the local context in a conversa-\ntion (Ghosal et al. 2019). Therefore, it is appealing to intro-\nduce a simple and general speaker modeling, which is easy\nto extend in all scenes and realize in other models so that\nlong-distance context can be available.\nTo address the above problem, we propose a TRans-\nforMer with Speaker Modeling (TRMSM). First, we sim-\nplify the Self and Inter-speaker dependencies to a binary\nversion, which only contains two relations: Intra-Speaker\ndependency and Inter-Speaker dependency. As illustrated\nin Fig. 1, for the speaker of the targeted utterance, Intra-\nSpeaker dependency focuses on the inÔ¨Çuence from the same\nspeaker, and Inter-Speaker dependency treats other speakers\nas a whole group instead of building a relation between ev-\nery two speakers. In this way, our simpliÔ¨Åed modeling can\nbe easy to extend in other models and deal with the scene\nwith the dynamical number of speakers without introducing\nnew relations between speakers.\nFurthermore, with the ability to settle long-distance de-\npendency, Transformer (Vaswani et al. 2017) achieves ex-\ncellent performance among a great number of Natural Lan-\nguage Processing (NLP) problems. To better model the long-\ndistance contextual utterances in a conversation, we uti-\nlize a hierarchical Transformer with two levels: sentence\nlevel and dialogue level. In the sentence level, BERT (De-\nvlin et al. 2019) encodes the semantic representation for a\ntargeted utterance, and in the dialogue level, Transformer\nis used to capture the information from contextual utter-\nances. To better model our simpliÔ¨Åed dependencies in the\ndialogue-level Transformer, we design three masks: Con-\nventional Mask for conventional context modeling, Intra-\nSpeaker Mask for Intra-Speaker dependency, and Inter-\nSpeaker Mask for Inter-Speaker dependency. To realize the\nfunctions of masks, we deploy three independent Trans-\nformer blocks in dialogue level and the designed masks are\nrespectively used in these Transformer blocks. With differ-\nent speaker-aware contextual information extracted by these\nTransformer blocks, whose contributions to the Ô¨Ånal predic-\ntion are diverse, we utilize the attention mechanism to au-\ntomatically weight and fuse them. Besides, we also apply\ntwo other simple fusing methods: Add and Concatenation to\ndemonstrate the advancement of the attention.\nSpeciÔ¨Åcally, our contributions are concluded as follows:\n‚Ä¢ We simplify Self and Inter-speaker dependencies to a bi-\nnary version, so that the speaker interaction modeling can\nbe extended in hierarchical Transformer and the long-\ndistance context can be considered.\n‚Ä¢ We design three types of masks to achieve speakers‚Äô in-\nteractions modeling in Transformer and utilize the atten-\ntion mechanism to automatically pick up the important\nspeaker-aware contextual information.\n‚Ä¢ We conduct experiments on two ERC datasets: IEMO-\nCAP and MELD. Our method achieves state-of-the-art\nperformance on both datasets on average.\nRelated Work\nTwo aspects are strongly related to our work: Emotion\nrecognition in conversation and Utilization of mask in Trans-\nformer.\nEmotion recognition in conversation Hierarchical\nstructure based on RNN (Jiao, Lyu, and King 2020; Jiao\net al. 2019) or Transformer (Zhong, Wang, and Miao 2019;\nLi et al. 2020) is leveraged in ERC to capture contextual\ninformation. Except contextual information, speaker infor-\nmation is proven to be important to ERC. Speakers can be\nregarded as objects related to utterances or additional infor-\nmation for utterances. As objects, speakers are involved in\nthe graph of conversation as nodes to interact with utterances\n(Zhang et al. 2019). As additional information, speaker in-\nformation is modeled via utterances. SpeciÔ¨Åcally, Hazarika\net al. (2018b,a) employ GRUs and Memory Network (Mem-\nnet) (Sukhbaatar et al. 2015) to model speakers‚Äô interactions\nin the dyadic conversation, which is difÔ¨Åcult to extend to\nmulti-speaker conditions. Therefore, Majumder et al. (2019)\ngeneralize speakers as parties, track them by GRU, and uti-\nlize attention mechanism to gather interactive information in\nmulti-speaker conversations. Even so, Ghosal et al. (2019)\nargue that Majumder et al. (2019) ignored the inÔ¨Çuences\nfrom other speakers and propose Self and Inter-Speaker de-\npendencies to formalize interactions within and between\nspeakers. However, the complicated modeling of speakers‚Äô\ninteractions is difÔ¨Åcult to apply in other models, thus requir-\ning a simpliÔ¨Åed version.\nUtilization of mask in Transformer Masks in Trans-\nformer are utilized to mask the unattended elements in self-\nattention. Recently, masks are well-designed and leveraged\nin language modeling (Dong et al. 2019; Devlin et al. 2019;\nRadford et al. 2018) and conversation structure modeling\n(Zhu et al. 2020). Masks are Ô¨Çexible and convenient to be\nimplemented and we choose them to model the interactions\nof speakers in Transformer.\nMethodology\nIn this section, we will elaborate on the task deÔ¨Ånition and\nthe structure of TRMSM which is illustrated in Fig. 2. Our\nmodel contains 4 parts: Sentence-Level Encoder, Dialogue-\nLevel Encoder, Fusing Method, and ClassiÔ¨Åer.\nTask DeÔ¨Ånition\nERC task includes K emotions, whose set is E =\n{emo1,emo2,...,emo K}. Given a conversation C =\n[u1,u2,...,u N ] containing N textual utterances, each ut-\nterance un = [ w1,w2,...,w Ln ] within is sequentially\nformed by Ln words. Particularly, M speakers, whose set is\nSPK = {spk1,spk2,...,spk M }, participate in the conver-\nsation. For each utterance un, a emotion label en ‚ààE and\na speaker annotation pn ‚àà SPK are assigned. ERC task\naims to predict the emotion of every utterance in Cwith the\ninformation provided above.\nSentence-Level Encoder\nTo encode a more informative and context-aware represen-\ntation of a single utterance based on Transformer, we utilize\n...\nBERT Encoder\nMaxpooling+Linear\nw1w2wùë≥ùíè\n‚Ä¶\nAdd & Nrom\nAdd & Nrom\nFeed Forward\nMasked Multi-\nHead Attention\nAdd & Nrom\nAdd & Nrom\nFeed Forward\nMasked Multi-\nHead Attention\nAdd & Nrom\nAdd & Nrom\nFeed Forward\nMasked Multi-\nHead Attention\nClassifier\ne...\n‚Ä¶ ‚Ä¶\n(c) Inter-Speaker Mask\nP1\nP2\nP1\nP2\nP1 P2 P1 P2\n(a) Conventional Mask\nP1\nP2\nP1\nP2\nP1 P2 P1 P2\n(b) Intra-Speaker Mask\nP1\nP2\nP1\nP2\nP1 P2 P1 P2\n+ + =\nConcat =\nFusing Method\nInter-Speaker Blocks\nIntra-Speaker Blocks\nConventional Blocks\nun\nu1\nuN\n(i) Add (ii) Concatenation\n1\n-inf\nDialogue-Level EncoderSentence-Level Encoder Classifier\ne\ne\nUtterance\nxN\nC\nOC\nC\nOC\nRA\nOC\nER\nùëÖ\nAttention \nFigure 2: The structure of our proposed model, which is based on Transformer structure. Our proposed masks are utilized\nin the Multi-Head Attention of Dialogue-Level Encoder and are illustrated for 3 types: (a)conventional, (b)Intra-Speaker and\n(c)Inter-Speaker masks. The fusing methods include Attention, (i)Add, and (ii)Concatenation.\na BERT encoder. Limited by the max length of a sequence\nsupported by BERT, we cannot input the concatenated se-\nquence of all utterances in a conversation, whose length fre-\nquently exceeds 768 in cases of long conversations, to cap-\nture the global contextual information. Therefore, BERT is\nsolely used to encode the sentence-level context in a single\nutterance. An utterance un = [w1,w2,...,w Ln ] is fed into\nBERT to obtain the contextualized representation of words:\nW = BERT(w1,w2,...,w Ln ) (1)\nwhere W ‚ààRLn√ódw is the output of the top layer of BERT\nand dw is the dimension of the word representation. To ob-\ntain an utterance representation for un, a max-pooling oper-\nation followed by a projection is deployed:\nun = Linear(Maxpooling(W)) (2)\nwhere un ‚ààRdu represents the utterance and du is the di-\nmension of utterance representation. By processing every ut-\nterance in a conversation, we Ô¨Ånally obtain the representa-\ntion matrix C ‚ààRN√ódu .\nDialogue-Level Encoder\nIn the dialogue level, we utilize three transformer blocks:\nConventional Blocks for conventional context modeling,\nIntra-Speaker Blocks for Intra-Speaker dependency, and\nInter-Speaker Blocks for Inter-Speaker dependency. Due to\nthe same structures of all Transformer blocks, we simply in-\ntroduce the general process of the Ô¨Årst layer of Transformer\nblocks.\nGiven the conversation matrix C processed by the\nsentence-level encoder, to avoid the absence of positional in-\nformation in C, an Absolute Positional Embedding is added\nto every representation in C:\nC = C+ PE(0 :N) (3)\nwhere PE(0 :N) is in the same dimension as C.\nSelf-attention intuitively provides an interactive pattern\nfor contextual modeling of conversations. Taking advantage\nof the mechanism of self-attention, the targeted utterances\ncan be parallelly processed. Therefore, the targeted utter-\nances are regarded as a query matrix, and the contextual ut-\nterances act as a key matrix, so that every utterance simul-\ntaneously assesses how much information shall be obtained\nfrom every contextual utterance. In this way, C is projected\nto query matrix Q ‚ààRN√óda , key matrix K ‚ààRN√óda , and\nvalue matrix V ‚ààRN√óda by linear projections without bias:\n[Q; K; V] =Linear([C; C; C]) where [] is the concatenat-\ning operation. Self-attention is calculated by:\nA(Q,K,V,M ) =softmax((QKT ) ‚àóM‚àöda\n)V (4)\nwhere ‚àódenotes element-wise multiplication; M ‚ààRN√óN\nis the utilized mask which is a square matrix whose non-\ninÔ¨Ånite elements equal 1. We will introduce different masks\nused by diverse blocks later. Transformer hires multiple self-\nattention (Multi-Head Attention, MHA) to model different\naspects of information. And then the outputs of all heads\nare concatenated and projected to O with the same size\nof C. After the Attention module, a Position-wise Feed-\nForward Network (FFN) module is deployed to produce out-\nput F ‚ààRN√ódu . MHA and FFN are both residually con-\nnected. Therefore the output O1\nC of the Ô¨Årst layer of Trans-\nformer is:\nA\n‚Ä≤\n= LayerNorm(O+ C), (5)\nF = max(0,A\n‚Ä≤\nW1 + b1)W2 + b2, (6)\nO1\nC = LayerNorm(F + A\n‚Ä≤\n). (7)\nO1\nC acts as the input of the second transformer layer, and\nby this analogy, we obtain the Ô¨Ånal output OC ‚ààRN√ódu\nafter multiple layers. Therefore, the outputs of our 3 blocks\ncan be denoted as: OC\nC for Conventional Block, ORA\nC for\nIntra-Speaker Block, andOER\nC for Inter-Speaker Block. Due\nto the limited space in this paper, more details about Trans-\nformer can be reviewed in Vaswani et al. (2017).\nMasks can prompt Transformer blocks to realize their\ndifferent functions, and we introduce how to form these 3\nmasks:\nConventional Mask sets all the elements of itself to 1,\nwhich means that every targeted utterance can get access to\nall the contextual utterances. Conventional Mask is applied\nin the multi-head attention of Conventional Blocks and is\nillustrated in Fig. 2 (a). We annotate Conventional Mask as\nMC.\nIntra-Speaker Mask only considers those contextual ut-\nterances tagged with pn, which is the speaker tag of the\ntargeted utterance. Therefore, based on MC, Intra-Speaker\nMask MRA sets positions representing other speakers to -\nINF . Intra-Speaker Mask is illustrated in Fig. 2 (b).\nInter-Speaker Mask regards other speakers different\nfrom the one of the targeted utterance as one unit due to\nour simpliÔ¨Åcation. Therefore, based on MC, Inter-Speaker\nMask MER sets positions whose speaker is the same as the\nspeaker tag of the targeted utterance to -INF. Inter-Speaker\nMask is illustrated in Fig. 2 (c).\nFusing Method\nAs blocks produce different outputs that carry various\nspeaker-aware contextual information, we utilize 3 simple\nmethods to fuse the information.\nAdd As illustrated in Fig. 2 (i), Add equally regards the\ncontributions of all outputs of blocks. Therefore, the fusing\nrepresentation is:\nR= OC\nC + ORA\nC + OER\nC (8)\nConcatenation Concatenation (illustrated in Fig. 2 (ii))\nis also a simple but effective method to combine different\ninformation. Different from Add operation, Concatenation\ncan implicitly choose the information which is important for\nthe Ô¨Ånal prediction due to the following linear projection of\nclassiÔ¨Åer. Therefore, the fusing representation R‚ààRN√ó3du\nis:\nR= Concat(OC\nC ,ORA\nC ,OER\nC ,dim = 1) (9)\nAttention As the contributions of different speaker par-\nties are diversely weighted, it is feasible that the model au-\ntomatically chooses the more important information. There-\nfore, we utilize the widely used attention (Lian et al. 2019)\nDataset Num. ofdialogues Num. ofutterances Avg. lengthof dialoguetrain dev test train dev test train/devtest\nIEMOCAP 120 31 5810 1623 48 52MELD 1039 114 280 9989 1109 2610 10 9\nTable 1: Statistics about IEMOCAP and MELD.\nto achieve this goal. Attention mechanism takes 3 block out-\nputting representations as inputs and produces an attention\nscore for each representation. For simplicity, we take rep-\nresentations OC\nC i ‚ààR1√ódu , ORA\nC i ‚ààR1√ódu , and OER\nC i ‚àà\nR1√ódu of utterance ias an example. Therefore, the attention\nscore and fusing representation are computed as:\nOi = Concat(OC\nC i,ORA\nC i,OER\nC i,dim = 0), (10)\nŒ±= softmax(wF Oi\nT ), (11)\nRi = Œ±Oi. (12)\nwhere Oi ‚àà R3√ódu is the concatenated representations,\nŒ±‚ààR1√ó3 is the attention score, wF ‚ààR1√ódu is a trainable\nparameter, and Ri ‚ààR1√ódu is the fusing representation. Fi-\nnally, all fusing representations of utterances are concate-\nnated as R‚ààRN√ódu .\nClassiÔ¨Åer\nWith the sentence-level and dialogue-level contextual infor-\nmation fully modeled by encoders, the dialogue-level output\nis fed to a classiÔ¨Åer which predicts the Ô¨Ånal emotion distri-\nbutions:\nÀÜY = softmax(RWclf + bclf ) (13)\nwhere Wclf ‚ààRdu√óK(Wclf ‚ààR3du√óK for Concatenation),\nbclf ‚ààRK and ÀÜY is the matrix of emotion distributions of\nall utterances in conversation C. The model is trained by a\ncross-entropy loss function, which is calculated as:\nL= ‚àí 1‚àëT\nl=1 Nl\nT‚àë\nl=1\nNl‚àë\ni=1\nK‚àë\ne=1\nye\ni log( ÀÜYe\ni ) (14)\nwhere yi is the one-hot vector denoting the emotion label\nof utterance iin a conversation, edenotes the dimension of\neach emotion, Nl denotes the length of l-th conversation,\nand T denotes the number of conversations in a dataset.\nExperimental Setup\nDatasets\nWe evaluate our models on two datasets: IEMOCAP (Busso\net al. 2008), MELD (Poria et al. 2019a), and both of them\nare multi-modal datasets that contain three modalities. We\nsolely consider the textual modality following Ghosal et al.\n(2019). Statistics about the datasets are shown in Tab. 1.\n‚Ä¢ IEMOCAP This dataset contains a series of dyadic\nconversation between 10 unique speakers. 6 categories\nof emotions are considered in our experiments: neutral,\nhappy, sad, angry, excited, and frustrated. Following Ma-\njumder et al. (2019), the training set is split into a new\ntraining set and a validation set by the ratio of 80: 20.\nMethods\nIEMOCAP\nHappy Sad Neutral Angry Excited Frustrated Average\nAcc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. wF1\nMemnet(Sukhbaatar et al. 2015)‚Ä†25.72 33.53 55.53 61.77 58.12 52.84 59.32 55.39 51.50 58.30 67.20 59.00 55.72 55.10\nCMN(Hazarika et al. 2018b)‚Ä† 25.00 30.38 55.92 62.41 52.86 52.39 61.76 59.83 55.52 60.25 71.13 60.69 56.56 56.13\nDialogueRNN(Majumder et al. 2019)25.69 33.18 75.10 78.80 58.59 59.21 64.71 65.28 80.27 71.86 61.15 58.91 63.40 62.75\nKET(Zhong, Wang, and Miao 2019)- - - - - - - - - - - - - 59.56\nAGHMN(Jiao, Lyu, and King 2020)48.30 52.10 68.30 73.30 61.60 58.40 57.50 61.90 68.10 69.70 67.10 62.30 63.50 63.50\nDialogueGCN(Ghosal et al. 2019)40.62 42.75 89.14 84.54 61.92 63.54 67.53 64.19 65.46 63.08 64.18 66.99 65.25 64.18\nDialogueGCN(80:20) 52.83 42.47 79.43 77.26 60.93 58.48 61.89 57.82 66.85 74.91 56.28 56.82 63.23 62.46\nBERT(Devlin et al. 2019) 42.19 39.05 60.45 59.91 49.11 52.24 55.14 54.82 64.22 55.97 54.26 55.88 54.06 54.01\nTRM 43.08 42.42 77.27 74.54 58.24 60.88 65.00 60.22 68.37 66.98 61.48 59.84 62.25 62.11\nTRMSM-Add 43.53 48.53 76.15 76.74 66.06 64.37 56.24 60.6 75.72 68.49 63.23 61.18 64.21 64.45\nTRMSM-Cat 48.71 49.46 76.84 77.02 64.44 63.00 57.03 60.72 76.07 70.33 60.74 62.09 64.72 64.82\nTRMSM-Att 43.36 50.22 81.23 75.82 66.11 64.15 60.39 60.97 77.46 72.70 62.16 63.45 65.34 65.74\nTable 2: The results of our models on IEMOCAP. Weighted-F1 score (wF1) is used as the metric. ‚Ä† means referring from\nGhosal et al. (2019).\n‚Ä¢ MELD This dataset contains over 1400 multi-speakers\nconversations collected from the TV seriesFriends. Emo-\ntions in this dataset are annotated into 7 categories: neu-\ntral, joy, surprise, anger, disgust, sadness and fear.\nCompared Methods\nTo distinguish our models with different fusing methods,\nwe construct 3 model variants: TRMSM-Add, TRMSM-Cat,\nand TRMSM-Att. To show the importance of the speaker-\nrelated information, we construct our model without Intra-\nSpeaker Blocks and Inter-Speaker Blocks, which we denote\nit as TRM. Besides, our models are compared with the base-\nlines below:\n‚Ä¢ CMN (Hazarika et al. 2018b) CMN is proposed to\nmodel dyadic conversations using two sets of RNNs and\nMemnets to respectively track different speakers.\n‚Ä¢ DialogueRNN (Majumder et al. 2019) DialogueRNN\nis the Ô¨Årst state-of-the-art model to address ERC of multi-\nspeakers. RNNs are deployed to track speakers‚Äô states and\nglobal state during conversations.\n‚Ä¢ AGHMN (Jiao, Lyu, and King 2020) AGHMN is the\nstate-of-the-art unidirectional model in real-time ERC.\nTo retain the positional information from its hierarchical\nstructure, a GRU is constructed for attention mechanism.\n‚Ä¢ DialogueGCN (Ghosal et al. 2019) To fully model the\ninteractive information between speakers, DialogueGCN\nmodels detailed dependencies between speakers using a\nRelational GCN.\n‚Ä¢ KET (Zhong, Wang, and Miao 2019) KET introduces\nthe Transformer structure to model context in conversa-\ntions. It also proposes an effective graph attention to ex-\ntract information from commonsense knowledge bases.\n‚Ä¢ BERT (Devlin et al. 2019) A vanilla BERT followed by\na classiÔ¨Åer is Ô¨Åne-tuned to show the importance of con-\ntext.\n‚Ä¢ Other baselines Both based on CNN to extract se-\nmantic information, scLSTM (Poria et al. 2017) utilizes\nLSTM (Hochreiter and Schmidhuber 1997) and Mem-\nnet (Sukhbaatar et al. 2015) utilizes memory network to\nmodel conversational context.\nImplementation\nFor BERT and sentence-level encoder, an uncased BERT-\nbase1 model is adopted. For the dialogue-level encoder, the\ndimension of dialogue-level representation is set to 300 for\nIEMOCAP and 200 for MELD; the number of transformer\nlayers is set to 6 for IEMOCAP and 1 for MELD; the\nnumber of heads is set 6 for IEMOCAP and 4 for MELD;\ndropout rate is set to 0.1. Additionally, models are trained\nusing AdamW (Kingma and Ba 2015; Loshchilov and Hut-\nter 2019) for 10000 steps with 1000 steps for warming up,\nand the learning rate linearly decaying after the warm-up is\nset to 1e-5 for IEMOCAP and 8e-6 for MELD. Due to the\nparallel prediction of utterances in one conversation, batch\nsize is set to 1 following Jiao, Lyu, and King (2020). Be-\nsides, DialogueGCN is trained in the setting of 90:10 data\nsplit on IEMOCAP, and for a fair comparison, we re-run\nDialogueGCN with 80:20 data split using the open-source\ncode2. All of our results reported are the average values of 5\nruns.\nResults and Discussions\nOverall Results\nFor IEMOCAP, weighted-F1 (wF1) score is used as the met-\nric. However, the data proportion of MELD is in a severely\nimbalanced condition. Therefore, the weighted-F1 score is\nnot that proper and enough for MELD. To balance the contri-\nbutions of large classes and small classes, we follow Zhang\net al. (2020) and also use the average value of macro F1\nscore and micro F1 score as one metric, which is calculated\nby mF1 = (F1macro + F1micro)/2.\nFor IEMOCAP, as shown in Tab. 2, BERT attains wF1\nof 54.01 which is substantially worse than our models and\nmost state-of-the-art models considering the dialogue-level\ncontext. This result may indicate that IEMOCAP contains\nconsiderable utterances that cannot be predicted only de-\npending on the semantic information, which is out of the\n1https://github.com/huggingface/transformers\n2https://github.com/declare-lab/conv-\nemotion/tree/master/DialogueGCN\nMethods MELDNeutral Surprise Fear Sadness Joy Disgust Anger AverageAcc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. wF1 Acc. wF1 mF1\nscLSTM(Poria et al. 2017)‚ô¶ 78.40 73.8046.80 47.703.80 5.40 22.40 25.1051.60 51.304.30 5.20 36.70 38.4057.50 55.90 46.40DialogueRNN(Majumder et al. 2019)‚ô¶ 72.10 73.5 54.40 49.401.60 1.20 23.90 23.8052.00 50.701.50 1.70 41.90 41.5056.10 55.90 45.30AGHMN(Jiao, Lyu, and King 2020)83.40 76.4049.10 49.709.20 11.50 21.60 27.0052.40 52.4012.20 14.0034.90 39.4060.30 58.10 49.45KET(Zhong, Wang, and Miao 2019)- - - - - - - - - - - - - - - 58.18 -DialogueGCN(Ghosal et al. 2019)- - - - - - - - - - - - - - - 58.10 -BERT(Devlin et al. 2019) 74.85 76.5753.26 56.2521.94 21.8935.33 33.0153.82 57.0236.64 27.6750.70 42.4261.82 61.07 53.40TRM 76.57 76.6255.53 55.6723.9 23.8 36.43 32.3452.07 57.4629.46 25.1350.68 44.6461.80 61.30 53.45TRMSM-Add 75.8877.7153.40 56.4924.67 21.1436.39 31.5653.97 57.8335.07 22.6252.36 45.9562.93 62.06 53.96TRMSM-Cat 75.69 77.2652.64 56.3325.66 22.7338.4134.3757.81 58.0735.61 22.5748.34 45.9062.76 62.01 54.04TRMSM-Att 75.48 77.5655.9057.2525.91 20.3836.82 32.9 55.5558.6638.3128.6352.1145.9563.2362.36 54.57\nTable 3: The Results of our models on MELD. MELD uses weighted-F1 (wF1) score, and the average value (mF1) of Macro-F1\nand Micro-F1, as the metrics. ‚ô¶means referring from Jiao, Lyu, and King (2020).\n64.01\n64.45\n62.11\n64.6 64.82\n64.18\n65.74\n61.5\n62.5\n63.5\n64.5\n65.5\nTRMSM W/O \nSM\nTRMSM W/O \nCM\nTRMSM\nW-F1\nIEMOCAP\n53.75\n53.96\n53.45\n53.83\n54.0454.34\n54.57\n52.5\n53.5\n54.5\n55.5\nTRMSM W/O \nSM\nTRMSM W/O \nCM\nTRMSM\nM-F1\nMELD\n(a) (b)\nTRMSM+Add\nTRMSM+Cat\nTRMSM+Att\nTRMSM+Add\nTRMSM+Cat\nTRMSM+Att\nFigure 3: The results of models with different blocks. wF1\nfor IEMOCAP; mF1 for MELD.\nconversational context. Furthermore, TRMSM-Att outper-\nforms AGHMN by 2.24 wF1 and DialogueGCN (80: 20)\nby 3.28 wF1, which beneÔ¨Åts from the powerful Transformer\nand our speaker modeling with long-distance information\nconsidered. For emotions, TRMSM-Att achieves the best\nF1 on Frustrated, and TRMSM-Add achieves the best F1\non Neutral. Besides, our models can attain second or third\nhigher results among other emotions. This demonstrates that\nour models are competitive to achieve comprehensive per-\nformance.\nFor MELD, as shown in Tab. 3, BERT outperforms other\nstate-of-the-art models by a great margin, which indicates\nthe importance of external knowledge brought by BERT.\nCompared with BERT, TRM attains marginally better re-\nsults, which may be attributed to the limited conversational\ncontextual information in MELD. To conÔ¨Årm this, compar-\ning the results of CNN 3 and scLSTM (based on CNN), we\ncan notice that the improvement is also limited. Although\nMELD provides limited contextual information in conversa-\ntions, TRMSM-Att still outperforms BERT by 1.29 wF1 and\n1.17 mF1, which indicates the effectiveness of our model to\ncapture such information. For emotions, BERT beats other\nstate-of-the-art models by a great margin in such an imbal-\nanced circumstance, and TRMSM-Att attains the best F1 on\n4 emotions including large classes Joy, Anger, Surprise, and\nthe small class Disgust. This demonstrates that BERT can\nalleviate data imbalance and our model can take advantage\nof such a feature.\nFor both datasets, all TRMSM variants outperform\nTRM to show the importance of speaker-aware contex-\n3CNN achieves 55.02 wF1 on MELD by Poria et al. (2019b).\n54.21\n59.87\n62.76 64.68 64.8 65.74\n54\n58\n62\n66\n(-0, 0) (-2, 2) (-4, 4) (-8, 8) (-10, 10) all\nWeighted-F1\nWindow size\n55.8 59.41\n62.48\n64.18\nTRMSM-Att\nDialogueGCN\nFigure 4: wF1 of TRMSM-Att and DialogueGCN with dif-\nferent ranges of context. all means using global context.\ntual information. TRMSM-Att and TRMSM-Cat outperform\nTRMSM-Add, which indicates the importance of different\naspects of speaker information requiring to be treated dif-\nferently. TRMSM-Att outperforming TRMSM-Cat demon-\nstrates that automatically and explicitly picking up speaker-\nrelated information is better than the implicit way.\nModel Analysis\nAblation Study To better understand the inÔ¨Çuences of\nmasks on our models, we report the results of the models\nremoving the Transformer blocks with different masks on\nIEMOCAP and MELD. In this part, we denote Convention\nMask as CM and Intra-Speaker, Inter-Speaker Masks as SM.\nAccordingly, TRMSM w/o SM is equivalent to TRM.\nAs seen in Fig. 3, on both datasets, TRMSM w/o CM\n(solely applying SM) can achieve better performance than\nTRMSM w/o SM (solely applying CM). We attribute it to\nthat speaker modeling does not drop the contextual informa-\ntion from conversations, and on the contrary, speaker mod-\neling can guide the model to extract more effective informa-\ntion to the Ô¨Ånal prediction. Furthermore, TRMSM outper-\nforms both TRMSM w/o CM and TRMSM w/o SM, which\ndemonstrates that all of our designed masks are critical to\nachieving better performance.\nEffect of Range of Context To Ô¨Ånd out the inÔ¨Çuence\nof the range of context on our model, we train TRMSM-Att\nwith different ranges of available context on IEMOCAP and\nrefer the results from Ghosal et al. (2019) for DialogueGCN.\nWe utilize different windows (‚àíx,y) to limit the context,\nwhere x, yis respectively the number of utterances in prior\ncontext and post context. As illustrated in Fig. 4, with the\nwindow widened, the performance increases as shown in\nboth models. For DialogueGCN, (-10, 10) is the max win-\ndow of context and therefore it cannot get access to the long-\n0 0 0 0 0 0 00 000.13 0.11 0.12 0.1 0.11 0.110.095 0.12 0.11\nF F F F F F F F F FM M M M M M M M M\n0.94 0.061\nOh, thank God... on the phone for half an \nhour ... talk to a freaking human being.\nI got it in the email\nFrustrated\nFrustrated\nTRMSM: Fru.\nTRM: Ang.\nTRMSM: Fru.\nTRM: Neu.\nyeah\nFrustrated\nTRMSM: Fru.\nTRM: Neu.\nIntra-s. Inter-s.\nIntra-speaker Block Attention\n:Speaker\n:Attention score\nAll right, all right, I‚Äôll ... my \ngirlfriend. But I'm just doing it for \nyou guys.\nNeutral TRMSM: Neu.\nTRM: Sad.\n0 0 00 00.13 0.11 0.160.140.16 0.15 0.17\nYeah, you should, really.\nNeutral\nTRMSM: Neu.\nTRM: Neu.\nSo big deal, so Joey's had a \nlot of girlfriends, it doesn‚Äôt ...\nNeutral TRMSM: Neu.\nTRM: Neu.\n0.38 0.62\nCh Ch Ch Ch ChRo RoMo Mo RaRa Mo\nIntra-s. Inter-s.Attention from fusing \nmethod\nInter-speaker Block Attention\n:Speaker\n:Attention score\n(a) (b)\nM: Male F: Female Ch: Chandler Ro: Ross\nMo: Monica Ra: Rachel\nAttention from fusing \nmethod\nFigure 5: Heatmaps of attention from fusing method and self-attention of Intra-, Inter-Speaker Blocks for the targeted utterances\n(whose speakers are marked in yellow). Labels of utterances are tagged below the utterances. Predictions of TRMSM and TRM\nare marked in green for correctness and red for mistake.\n35\n45\n55\n65\n75\nhappy\nsad\nneural\nangry\nexcite\nd\nfrustra\nted\n35\n45\n55\n65\n75\nhappy\nsad\nneural\nangry\nexcite\nd\nfrustr\nated\n35\n45\n55\n65\n75\nhappy\nsad\nneural\nangry\nexcite\nd\nfrustra\nted\n0\n20\n40\n60\n80\nneur\nal\nsurpr\nise\nfear\nsadn\nessjoy\ndisg\nust\nange\nr\n0\n20\n40\n60\n80\nneur\nal\nsurpr\nise\nfear\nsadn\nessjoy\ndisg\nust\nange\nr\n0\n20\n40\n60\n80\nneur\nal\nsurpr\nise\nfear\nsadn\nessjoy\ndisg\nust\nange\nr\n(a) 1 Layer (b) 3 Layers (c) 6 Layers\n(d) 1 Layer (e) 3 Layers (f) 6 Layers\nTRM\nTRMSM-Att\nFigure 6: The F1 score on every emotion class by TRMSM-\nAtt and TRM. (a)-(c) for IEMOCAP; (d)-(f) for MELD.\ndistance context. On the contrary, the performance is further\nimproved by TRMSM-Att with all context available. This\nindicates that the local contextual information is critical for\nthe prediction and the long-distance information is also im-\nportant for contextual modeling to further improve the per-\nformance.\nEffect of Number of Layers We study the effect of\nthe number of layers to our model on different datasets. Fig.\n6 illustrates the radar graphs for the F1 scores of emotions\nin IEMOCAP and MELD by TRM and TRMSM-Att. As\nthe number of layers increasing, the F1 scores of emotions\nin IEMOCAP normally expand. While in MELD, increas-\ning the number of layers gradually hurts the performance to\nbe 0 of F1 on emotions Fear and Disgust which are classes\nwith the fewest data. We think the reason may be that MELD\nsuffers from data imbalance and increasing the number of\nlayers leads to severer overÔ¨Åtting on small classes. For data\nimbalance, methods like re-balance can be applied to allevi-\nate it. Re-balance is out of the scope of this paper and our\nfuture work will study data imbalance of ERC.\nCase Study\nTo better understand how our model captures Intra-Speaker\nand Inter-Speaker dependencies, we illustrate two conver-\nsation clips ending with the targeted utterances so that\nthe targeted utterances can only refer to the prior context.\nWe choose TRMSM-Att without Conventional Blocks so\nthat only speaker information related blocks are considered.\nSpeciÔ¨Åcally, we illustrate heatmaps of attention from fusing\nmethod and self-attention4 in Transformer blocks. For sim-\nplicity, we denote attention from fusing method as FAtt.\nIn the scene of Fig. 5 (a), the speaker M keeps in frus-\ntration through the conversation and F in a neutral state has\nfew inÔ¨Çuences on M. Therefore, FAtt pays more attention to\nIntra-Speaker dependency so that Intra-Speaker Blocks can\nextract information from M himself. We can see from the\nheatmap that the targeted utterance yeah grades the highest\nscore to the farthest contextual utterance whose emotion is\nalso frustration, which is out of the range of context that Dia-\nlogueGCN can refer. In a sense, this indicates the importance\nof long-distance information.\nAs the condition in Fig. 5 (b), speakers in this conver-\nsation basically keep in a neutral state except that Chan-\ndler shows other emotions like anger and surprise before\nthe targeted utterance. Although the targeted utterance with\nspeaker Chandler shows slight sadness from the semantic\nview, it is supposed to be predicted as neutral according to\nthe conversational context. SpeciÔ¨Åcally, FAtt grades Inter-\nSpeaker Blocks with higher score and self-attention in Inter-\nSpeaker Blocks extracts information from the neutral utter-\nances of other speakers. This case indicates the effectiveness\nof our model to extract inter-speaker information.\nConclusion\nIn this work, we simplify the Self and Inter-Speaker de-\npendencies to a binary version. To achieve the simpliÔ¨Åed\nmodeling of speakers‚Äô interactions, we design three masks:\nConventional Mask, Intra-Speaker Mask, and Inter-Speaker\nMask. These masks are utilized in the self-attention mod-\nules of the second-level Transformer blocks of a hierarchical\nTransformer. As the speaker-aware information extracted by\ndifferent masks diversely contributes to the prediction, atten-\ntion mechanism is utilized to weight and fuse them. Finally,\nour model achieves state-of-the-art results on 2 ERC datasets\nand further analysis shows that our model is efÔ¨Åcacious for\nERC.\n4We average the attention scores of all self-attention heads in\nthe top layer of Transformer\nReferences\nBusso, C.; Bulut, M.; Lee, C.; Kazemzadeh, A.; Mower,\nE.; Kim, S.; Chang, J. N.; Lee, S.; and Narayanan, S. S.\n2008. IEMOCAP: interactive emotional dyadic motion cap-\nture database. Lang. Resour. Evaluation.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proc. of NAACL-HLT, 4171‚Äì\n4186.\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .;\nGao, J.; Zhou, M.; and Hon, H. 2019. UniÔ¨Åed Language\nModel Pre-training for Natural Language Understanding\nand Generation. In Proc. of NeurIPS, 13042‚Äì13054.\nGhosal, D.; Majumder, N.; Poria, S.; Chhaya, N.; and Gel-\nbukh, A. F. 2019. DialogueGCN: A Graph Convolutional\nNeural Network for Emotion Recognition in Conversation.\nIn Proc. of EMNLP-IJCNLP, 154‚Äì164.\nHazarika, D.; Poria, S.; Mihalcea, R.; Cambria, E.; and\nZimmermann, R. 2018a. ICON: Interactive Conversational\nMemory Network for Multimodal Emotion Detection. In\nProc. of EMNLP, 2594‚Äì2604.\nHazarika, D.; Poria, S.; Zadeh, A.; Cambria, E.; Morency,\nL.; and Zimmermann, R. 2018b. Conversational Mem-\nory Network for Emotion Recognition in Dyadic Dialogue\nVideos. In Proc. of NAACL-HLT, 2122‚Äì2132.\nHochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term\nMemory. Neural Computation9(8): 1735‚Äì1780.\nJiao, W.; Lyu, M. R.; and King, I. 2020. Real-Time Emo-\ntion Recognition via Attention Gated Hierarchical Memory\nNetwork. In Proc. of AAAI.\nJiao, W.; Yang, H.; King, I.; and Lyu, M. R. 2019. Hi-\nGRU: Hierarchical Gated Recurrent Units for Utterance-\nLevel Emotion Recognition. In Proc. of NAACL-HLT, 397‚Äì\n406.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for\nStochastic Optimization. In Proc. of ICLR.\nLi, Q.; Wu, C.; Zheng, K.; and Wang, Z. 2020. Hierarchical\nTransformer Network for Utterance-level Emotion Recogni-\ntion. arXiv preprint arXiv:2002.07551.\nLian, Z.; Tao, J.; Liu, B.; and Huang, J. 2019. Conversational\nEmotion Analysis via Attention Mechanisms. In Proc. of\nInterspeech, 1936‚Äì1940.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In Proc. of ICLR.\nMajumder, N.; Poria, S.; Hazarika, D.; Mihalcea, R.; Gel-\nbukh, A. F.; and Cambria, E. 2019. DialogueRNN: An At-\ntentive RNN for Emotion Detection in Conversations. In\nProc. of AAAI, 6818‚Äì6825.\nPoria, S.; Cambria, E.; Hazarika, D.; Majumder, N.; Zadeh,\nA.; and Morency, L. 2017. Context-Dependent Sentiment\nAnalysis in User-Generated Videos. In Proc. of ACL, 873‚Äì\n883.\nPoria, S.; Hazarika, D.; Majumder, N.; Naik, G.; Cambria,\nE.; and Mihalcea, R. 2019a. MELD: A Multimodal Multi-\nParty Dataset for Emotion Recognition in Conversations. In\nProc. of ACL, 527‚Äì536.\nPoria, S.; Majumder, N.; Mihalcea, R.; and Hovy, E. H.\n2019b. Emotion Recognition in Conversation: Research\nChallenges, Datasets, and Recent Advances. IEEE Access\n7: 100943‚Äì100953.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nSchlichtkrull, M. S.; Kipf, T. N.; Bloem, P.; van den Berg,\nR.; Titov, I.; and Welling, M. 2018. Modeling Relational\nData with Graph Convolutional Networks. In Proc. of\nESWC, 593‚Äì607.\nSukhbaatar, S.; Szlam, A.; Weston, J.; and Fergus, R. 2015.\nEnd-To-End Memory Networks. In Proc. of NeuIPS, 2440‚Äì\n2448.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Proc. of NeuIPS, 5998‚Äì6008.\nZhang, B.; Yang, M.; Li, X.; Ye, Y .; Xu, X.; and Dai, K.\n2020. Enhancing Cross-target Stance Detection with Trans-\nferable Semantic-Emotion Knowledge. In Proc. of ACL,\n3188‚Äì3197.\nZhang, D.; Wu, L.; Sun, C.; Li, S.; Zhu, Q.; and Zhou, G.\n2019. Modeling both Context- and Speaker-Sensitive De-\npendence for Emotion Detection in Multi-speaker Conver-\nsations. In Proc. of IJCAI, 5415‚Äì5421.\nZhong, P.; Wang, D.; and Miao, C. 2019. Knowledge-\nEnriched Transformer for Emotion Detection in Textual\nConversations. In Proc. of EMNLP-IJCNLP, 165‚Äì176.\nZhu, H.; Nan, F.; Wang, Z.; Nallapati, R.; and Xiang, B.\n2020. Who did They Respond to? Conversation Structure\nModeling using Masked Hierarchical Transformer. In Proc.\nof AAAI."
}