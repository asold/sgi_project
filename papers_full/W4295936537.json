{
    "title": "BabyNet: Residual Transformer Module for Birth Weight Prediction on\\n Fetal Ultrasound Video",
    "url": "https://openalex.org/W4295936537",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5064752399",
            "name": "Szymon Płotka",
            "affiliations": [
                null,
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A5053661052",
            "name": "Michal K. Grzeszczyk",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5002367112",
            "name": "Robert Brawura-Biskupski-Samaha",
            "affiliations": [
                "Postgraduate School of Molecular Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A5049714725",
            "name": "Paweł Gutaj",
            "affiliations": [
                "Poznan University of Medical Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5053205813",
            "name": "Michał Lipa",
            "affiliations": [
                "Medical University of Warsaw"
            ]
        },
        {
            "id": "https://openalex.org/A5070627781",
            "name": "T. P. Trzcinski",
            "affiliations": [
                "Warsaw University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5080126237",
            "name": "Arkadiusz Sitek",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3181236787",
        "https://openalex.org/W3137278571",
        "https://openalex.org/W2986936637",
        "https://openalex.org/W2026421762",
        "https://openalex.org/W2966709403",
        "https://openalex.org/W4205908798",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W3203701986",
        "https://openalex.org/W1973936103",
        "https://openalex.org/W2789413961",
        "https://openalex.org/W3204334240",
        "https://openalex.org/W1968532354",
        "https://openalex.org/W4214612132",
        "https://openalex.org/W2954296085",
        "https://openalex.org/W3202688103",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W3126144365"
    ],
    "abstract": "Predicting fetal weight at birth is an important aspect of perinatal care,\\nparticularly in the context of antenatal management, which includes the planned\\ntiming and the mode of delivery. Accurate prediction of weight using prenatal\\nultrasound is challenging as it requires images of specific fetal body parts\\nduring advanced pregnancy which is difficult to capture due to poor quality of\\nimages caused by the lack of amniotic fluid. As a consequence, predictions\\nwhich rely on standard methods often suffer from significant errors. In this\\npaper we propose the Residual Transformer Module which extends a 3D\\nResNet-based network for analysis of 2D+t spatio-temporal ultrasound video\\nscans. Our end-to-end method, called BabyNet, automatically predicts fetal\\nbirth weight based on fetal ultrasound video scans. We evaluate BabyNet using a\\ndedicated clinical set comprising 225 2D fetal ultrasound videos of pregnancies\\nfrom 75 patients performed one day prior to delivery. Experimental results show\\nthat BabyNet outperforms several state-of-the-art methods and estimates the\\nweight at birth with accuracy comparable to human experts. Furthermore,\\ncombining estimates provided by human experts with those computed by BabyNet\\nyields the best results, outperforming either of other methods by a significant\\nmargin. The source code of BabyNet is available at\\nhttps://github.com/SanoScience/BabyNet.\\n",
    "full_text": "BabyNet: Residual Transformer Module for\nBirth Weight Prediction on Fetal Ultrasound\nVideo\nSzymon P lotka1,2, Michal K. Grzeszczyk 1, Robert\nBrawura-Biskupski-Samaha3, Pawe l Gutaj4, Micha l Lipa5, Tomasz Trzci´ nski6,\nand Arkadiusz Sitek1\n1 Sano Centre for Computational Medicine, Cracow, Poland\ns.plotka@sanoscience.org\n2 Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands\n3 The Medical Centre of Postgraduate Education, Warsaw, Poland\n4 Poznan University of Medical Sciences, Poznan, Poland\n5 Medical University of Warsaw, Warsaw, Poland\n6 Warsaw University of Technology, Warsaw, Poland\nAbstract. Predicting fetal weight at birth is an important aspect of\nperinatal care, particularly in the context of antenatal management,\nwhich includes the planned timing and the mode of delivery. Accurate\nprediction of weight using prenatal ultrasound is challenging as it requires\nimages of speciﬁc fetal body parts during advanced pregnancy which is\ndiﬃcult to capture due to poor quality of images caused by the lack\nof amniotic ﬂuid. As a consequence, predictions which rely on standard\nmethods often suﬀer from signiﬁcant errors. In this paper we propose\nthe Residual Transformer Module which extends a 3D ResNet-based net-\nwork for analysis of 2D + t spatio-temporal ultrasound video scans. Our\nend-to-end method, called BabyNet, automatically predicts fetal birth\nweight based on fetal ultrasound video scans. We evaluate BabyNet us-\ning a dedicated clinical set comprising 225 2D fetal ultrasound videos of\npregnancies from 75 patients performed one day prior to delivery. Exper-\nimental results show that BabyNet outperforms several state-of-the-art\nmethods and estimates the weight at birth with accuracy comparable to\nhuman experts. Furthermore, combining estimates provided by human\nexperts with those computed by BabyNet yields the best results, outper-\nforming either of other methods by a signiﬁcant margin. The source code\nof BabyNet is available at https://github.com/SanoScience/BabyNet.\nKeywords: Deep learning · Fetal birth weight · Transformer\n1 Introduction\nFetal birth weight (FBW) is a signiﬁcant indicator of perinatal health prognosis.\nAccurate prediction of FBW, as well as gestational age, complications in preg-\nS. P lotka and M. K. Grzeszczyk – Authors contributed equally.\narXiv:2205.09382v2  [eess.IV]  6 Jun 2022\n2 S. P lotka et al.\nnancy, and maternal physical parameters are critical in determining the best\nmethod of delivery (natural or Cesarean). These factors are widely used as a\npart of the hospital admission procedure in the World [14]. However, FBW pre-\ndiction is a challenging task, requiring highly visible fetal body standard planes,\nwhich can only be identiﬁed by experienced sonographers. Unfortunately, weight\npredictions provided by experienced sonographers are often imprecise, with up\nto 10% mean absolute percentage errors. Currently, FBW is estimated on the\nbasis of fetal biometric measurements of body organs – head circumference (HC),\nbiparietal diameter (BPD), abdominal circumference (AC), femur length (FL),\nwhich are used as the input to heuristic formulae [6], [11].\nIn recent years, machine learning-based methods have been proposed as a\npossible means of automating FBW prediction. Lu et al. [9], [10] presents a\nsolution based on an ensemble model consisting of Random Forest, XGBoost and\nLightGBM algorithms. Tao et al. [20] use a hybrid-LSTM network model [24]\nfor temporal data analysis. Convolutional neural network (CNN)-based models\nare also proposed to estimate fetal weight based on ultrasound images [2], [5] or\nvideos [13], [12]. However, such methods do not rely on the true FBW as the\nground truth, but instead predict it through heuristic formulae using estimated\nfetal body-part biometrics, which is prone to errors.\nRecently, Transformers [22] have been proposed as an alternative architecture\nto CNNs, and have achieved competitive performance for many computer vision\ntasks e.g. Vision Transformer (ViT) for image classiﬁcation [3] or Video Vision\nTransformer (ViViT) for video recognition [1]. Transformers utilize the Multi-\nHead Self-Attention (MHSA) mechanism to learn the global context between\ninput sequence elements. Unfortunately, due to their high computational com-\nplexity, Transformers require a large amount of training data and long training\ntimes. Many methods have been developed to bridge the gap between sample-\neﬃcient learning with a high inductive bias of CNNs and performance but data-\nineﬃcient Transformers. Hybrid models utilizing CNN layers and Transformer\nblocks have also been introduced [4], [8], [15].\nIn this paper we utilize Transformers for direct estimations of fetal weights\nfrom US videos. We implement this solution as an extension of a 3D ResNet-\nbased network [21] with a Residual Transformer Module (RTM) called BabyNet.\nThe RTM allows local and global feature representation through residual con-\nnections and utilization of convolutional layers. This representation is reﬁned\nthrough the global self-attention mechanism included inside RTM. BabyNet is\na hybrid neural network that eﬃciently bridges CNNs and Transformers for\n2D+ t spatio-temporal ultrasound video scans analysis to directly predict fetal\nbirth weight. The main contribution of our work is as follows: (1) We provide an\nend-to-end method for birth weight estimation based directly on fetal ultrasound\nvideo scans, (2) We introduce a novel Residual Transformer Module by adding\ntemporal position encoding to 3D MHSA in 3D ResNet-based neural network,\n(3) To the best of our knowledge, BabyNet is the ﬁrst framework to automate fe-\ntal birth weight prediction on fetal ultrasound video scans trained and validated\nwith data acquired one day prior to delivery.\nBabyNet: Birth Weight Prediction from Fetal US 3\nconv1\nT x H x W x DT x H x W x DT x H x W x DT x H x W x DT x d x H x W T x d x H x W\nW Q W K \nT x d x H x W T x d x H x W\nW V \nT x H*W x H*WT x H*W x H*WT x H*W x H*W\n1 x d x H x 1\n1 x 1 x W x D1 x 1 x W x D1 x d x 1 x W\nRw\nT x d x 1 x 1\nR t \nx T x d x H x W\nT x H x W x DT x H x W x DT x d x H x W\nT x H*W x H*W\nT x H*W x H*W\nsoftmax \nT x d x H x Wz \nbirth\nweight\nprediction\nRM \nRM \nconv2\nRMD \nRM \nconv3\nRMD \nRM \nconv4\n3D Convolution\nBatch normalization\nReLU\nMHSA\nFrames (T0 x 1 x H0 x W0)\nRelative Position Encodings\nFully Connected Layer\nRM RMD \n=\n=\nGlobal Average Pooling\nRwR h R w \nT x H*W x H*W\nR t \nconv5\nFig. 1.The overview of our proposed BabyNet method for birth weight estimation\ndirectly from fetal US video scans. In BabyNet, we replace two Residual Modules of 3D\nResNet-18 with two Residual Transformer Modules (RTM) containing 3D Multi-Head\nSelf-Attention (MHSA) with Relative Positional Encoding (RPE). RPE is calculated as\nthe sum of height (Rh), width (Rw) and temporal (Rt) position encodings. For clarity,\nonly one attention head is presented in the image. The network takes 16 consecutive\nframes as the input to make a single-segment prediction. All frames for a given patient\nare divided into non-overlapping 16-frame segments and a patient-level prediction is\nobtained by averaging all segment predictions.\n2 Method\nThe overview of our method for end-to-end FBW prediction is presented in Fig.\n1. We use 3D ResNet-18 for high-level US feature extraction. The RTM is de-\nsigned to learn local and global feature representation with 3D Multi-Head Self-\nAttention mechanism and convolutional layers. We replace the last two residual\nmodules of ResNet with RTMs.\n2.1 Feature Extraction\nWe employ 3D ResNet-18 [21] as the base network to extract high-level 2 D+ t\nspatio-temporal US feature representations. The initial input to the network is\nUS video sequence SUS ∈RT0×1×H0×W0 of height H0, width W0 and frame num-\nber T0. It is transformed via convolutional residual modules to a low-resolution\nfeature map sequence S\n′\nUS ∈ RT1×D1×H1×W1 , where T1 = T0/4, D1 = 512,\n4 S. P lotka et al.\nH1 = H0/8, and W1 = W0/8. Multi-channel, low-resolution feature map se-\nquences are fed to the RTM.\n2.2 Residual Transformer Module\nResidual modules are constructed from a layer followed by a rectiﬁed linear\nunit (ReLU) and Batch Normalization. This structure is repeated two or three\ntimes with a skip connection of the input added to the output of the previous\nlayers [7]. To include global low-resolution feature map context processing via a\nself-attention mechanism we design RTM in a similar manner to BoT [19]. Our\nRTM extends BoT to 3D space by adding temporal position encoding [17] to 3D\nMulti-Head Self-Attention. BoT utilizes MHSA instead of 3×3 convolution in the\nresidual bottleneck module, created to decrease the computational complexity in\ndeeper ResNet architectures. 3D ResNets are often shallower and do not contain\nBottleneck blocks. Thus, to utilize the self-attention mechanism in shallower\nResNets we replace the last convolutional layer in the residual module with\nMHSA, and deﬁne RTM as:\ny= BN (MHSA(σ(BN(Conv(x)) + x (1)\nwhere xand y are input and output of the RTM respectively, Conv denotes the\nconvolutional layer, BN is Batch Normalization and σ stands for ReLU.\n2.3 3D Multi-Head Self-Attention\nTo learn multiple attention representations at diﬀerent positions, instead of per-\nforming a single attention, many self-attention heads (Multi-Head Self-Attention)\nare jointly trained with their outputs concatenated [22]. Since such operation is\npermutation-invariant, positional encoding r needs to be added to include po-\nsitional information. Depending on the application, absolute (e.g. sinusoidal) or\nrelative positional encodings (RPE) [17], recently identiﬁed as a better ﬁt for\nvision tasks [23], can be used. To process 2 D+ tUS videos with MHSA we add\ntemporal positional encoding to the 2D RPE and compute positional encoding\nr as the sum of Rh ∈ R1×D×H×1, Rw ∈ R1×D×1×W and Rt ∈ RT×D×1×1,\nthe height, width and temporal positional encodings respectively. Finally, we\ncompute the 3D MHSA output of S\n′′\nUS ∈RT×D×H×W input as:\nMHSA\n(\nS\n′′\nUS\n)\n= concat\n[\nsoftmax\n(Qi(Ki + r)T\n√\nd\n)\nVi\n]\n(2)\nwhere T = T1\n2 , D = D1, H = H1\n2 , W = W1\n2 , Qi, Ki, Vi are queries, keys\nand values for the ith attention head calculated from WQ(S\n′′\nUS ), WK(S\n′′\nUS ) and\nWV (S\n′′\nUS ) 1×1×1 3D convolutions performed over inputS\n′′\nUS and dis Ddivided\nby the number of heads.\nBabyNet: Birth Weight Prediction from Fetal US 5\nFig. 2.Sample US frames extracted from fetal US videos. The frames show the fetal\nbody part standard planes of the head, abdomen and femur respectively, going from\nleft to right. Images obtained several hours before delivery are of lower quality than at\nearlier stages of pregnancy due to the lack of amniotic ﬂuid.\n3 Experiments\nIn this section, we describe our dataset and present architectural details of\nBabyNet. We compare BabyNet’s performance with other 2D+tspatio-temporal\nvideo analysis methods and with results obtained from clinicians. We show,\nthrough an ablation study, the importance of BabyNet components that have\nbeen added or replaced in 3D ResNet-18.\nDataset and Pre-processing.Ethical Committee approval was obtained for\nall subjects enrolled in the study. The dataset consists of 225 2D fetal ultrasound\nvideo scans in standard plane view of fetal head, abdomen, and femur. The multi-\ncentre dataset was obtained from 75 pregnant women aged 21 to 42 and acquired\nthrough routine US examinations less than 24 hours prior to delivery. The data\nwas acquired by three experienced sonographers using GE Voluson E6 and S10\ndevices. Each US video scan is stored in DICOM ﬁle format, captured in two\nresolutions: 960 ×720 and 852 ×1136 pixels. The number of frames is between\n463 and 1448, with a mean of 852. The US videos were obtained in sector scan\nsweep mode with frame per second (FPS) between 24 and 37. For each video,\nwe resample pixel spacing to 0.2 ×0.2 mm. As the ground truth, we use the true\nfetal weight measured at birth. The ground truth values were between 2085 and\n4995, with a mean of 3454 grams [g].\nImplementation Details. We adopt 3D ResNet-18 [21] as our base neural\nnetwork. Table 1 presents the architectural details of BabyNet, as compared to\n3D ResNet-18. BabyNet comprises a 3D convolutional stem followed by conv\nstages: three with two residual modules each, and one ﬁnal stage implemented\nwith two RTMs. The output of the ﬁnal RTM is global average pooled (GAP)\nand fed to the fully-connected (FC) layer with one neuron (512 input weights) for\nfetal birth weight prediction. We implement our model with PyTorch and train\nit using an NVIDIA RTX 2080 Ti 24GB GPU with a mini-batch size of 2 and\nan initial learning rate of 1 ×10−4 with a step decay by a factor of g= 0.1 every\n160th epochs until convergence over 200 epochs. To minimize the Mean Squared\n6 S. P lotka et al.\nTable 1. Comparison of ResNet3D-18 and BabyNet architectures. We replace the\nlast two residual modules of 3D ResNet-18 with two Residual Transformer Modules\ncontaining a 3D MHSA instead of the second 3 ×3 3D convolution.\nStage name Output size 3D ResNet-18 BabyNet\nconv1 T0 ×H0\n2 ×W0\n2 3 ×7 ×7, 64, stride 1 ×2 ×2\nconv2 T0 ×H0\n2 ×W0\n2\n[\n3 ×3 ×3, 64\n3 ×3 ×3, 64\n]\n×2\n[\n3 ×3 ×3, 64\n3 ×3 ×3, 64\n]\n×2\nconv3 T0\n2 ×H0\n4 ×W0\n4\n[\n3 ×3 ×3, 128\n3 ×3 ×3, 128\n]\n×2\n[\n3 ×3 ×3, 128\n3 ×3 ×3, 128\n]\n×2\nconv4 T0\n4 ×H0\n8 ×W0\n8\n[\n3 ×3 ×3, 256\n3 ×3 ×3, 256\n]\n×2\n[\n3 ×3 ×3, 256\n3 ×3 ×3, 256\n]\n×2\nconv5 T0\n8 ×H0\n16 ×W0\n16\n[\n3 ×3 ×3, 512\n3 ×3 ×3, 512\n]\n×2\n[\nRTM\n  \n3 ×3 ×3, 512\nMHSA\n]\n×2\n1 ×1 ×1 Global Avg Pooling, FC layer\nError (MSE) loss function, we employ an ADAM optimizer with 1×10−4 weight\ndecay. During training, we apply data augmentation including rotate ( ±25◦),\nrandom brightness and contrast, horizontal ﬂip, image compression and blur for\neach mini-batch. We retain height and width ratio and resize video frames to\n64×64 (H0 ×W0) with padding. The number of attention heads is empirically set\nto 4, while the temporal sequence length T0 is 16. Thus, BabyNet transforms US\ninput sequence SUS ∈R16×1×64×64 to the output OSUS ∈R1 of predicted fetal\nbirth weights. We perform 5-fold cross-validation (CV) to compare and verify\nthe robustness of the regression algorithm. We ensure that data from a single\npatient appears only in a single fold.\nEvaluation Metrics. As measurement metrics, we use Root Mean Square\nError (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage\nError (MAPE) to evaluate the regression performance.\nComparison with Clinicians and State-of-the-art Algorithms.We com-\npare BabyNet with several 2 D+ t spatio-temporal video analysis methods. In\nparticular, we compare it with results obtained by clinicians in [18] as well as\nresults obtained by clinicians for the dataset used in this work. We also present\nresults for Video Vision Transformer (ViViT) [1] and test the hybrid approach of\n2D ResNet-50 as a convolutional feature extractor (without GAP and FC layers)\nBabyNet: Birth Weight Prediction from Fetal US 7\nTable 2.Five-fold cross-validation results and comparison of state-of-the-art methods.\nThe mean of Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean\nAbsolute Percentage Error (MAPE) across all folds are reported.\nMethod mMAE [g] mRMSE [g] mMAPE [%]\nClinicians (from [18]) - - 7.9 ±6.8\nClinicians (this work) 213 ±155 264 ±158 6.3 ±4.8\nViViT [1] 361 ±244 444 ±230 10.6 ±7.3\n2D ResNet + ViViT 344 ±241 426 ±226 10.3 ±7.2\n3D ResNet-18 [21] 328 ±234 421 ±225 10.1 ±7.1\nBabyNet 254 ±230 341 ±215 7.5 ±6.6\nClinicians (this work) & BabyNet180 ±156 237 ±145 5.2 ±4.6\nto ViViT network. Finally, we utilize a vanilla 3D ResNet-18 [21]. We train all\nmodels in the same fashion as BabyNet.\nTable 2 presents a comparison of 5-fold CV results for all tested methods.\nResults for machine learning methods are out-of-fold predictions. Combination\nof estimations performed by clinicians with estimations provided by BabyNet is\nthe most accurate, with MAE of 180 ±156 (max p-value < 0.001), RMSE of\n237±145 (max p-value <0.001), and MAPE of 5.2±4.6 (max p-value <0.001).\nMax p-value is the maximum paired dual sided p-value computed for results of\nthe ”Clinicians (this work) & BabyNet” method and other methods listed in\nTable 2.\nWe did not detect a statistically signiﬁcant diﬀerence between the perfor-\nmance of clinicians measured in [18] and our algorithm (p-value = 0.6). Estima-\ntions provided by clinicians in our study seem to be better than those provided\nby clinicians in [18] (p-value = 0.04) and BabyNet (p-value = 0.07). Out of all\nneural networks investigated in this work, the hybrid approach of utilizing 3D\nconvolutions and 3D MHSA within RTM as a part of 3D ResNet-18 outperforms\nother methods based on plain CNNs, plain Transformer or CNN+Transformer\nnetworks.\nWe noted that the best results were obtained by averaging estimations pro-\nvided by clinicians and by BabyNet. The performance of the ensemble of clin-\nicians & BabyNet was better by 18% compared to clinicians alone in terms of\nmMAPE, which is a clear indication of added value and potential clinical beneﬁts\nof BabyNet.\nAblation study. We conducted an ablation study to show the eﬀectiveness of\nnovel components within BabyNet. In this experiment, we employ 3D ResNet-18\nas the base neural network for 2 D+ t spatio-temporal US video scan analysis.\n8 S. P lotka et al.\nTable 3.Ablation study.\nMethod mMAE [g] ↓mRMSE [g] ↓mMAPE [%] ↓\n3D ResNet-18 (base) 328 ±234 421 ±225 10.1 ±7.1\n+ RTM 277 ±228 374 ±221 8.1 ±7.0\n+ RTM + TPE (ours)254 ±230 341 ±215 7.5 ±6.6\nTo learn multiple relationships and enable capture of richer interpretations of\nthe US video sequence, we integrate CNN and Transformer by swapping the last\nconvolutional layer in the residual module for MHSA. To further enhance 2D+t\nspatio-temporal feature representation in space and time, we add temporal posi-\ntion encoding (TPE). Table 3 demonstrates that the combination of CNN with\na Transformer-based module, MHSA and temporal position encoding improves\nperformance of the weight-estimation task directly from US video scan.\n4 Discussion\nIn this work we were not able to match the performance of the clinicians in\nestimating fetal weight (mMAPE 7.5% vs. 6.3%); however, clinicians who worked\nwith us and provided measurements are top experts in performing biometric\nmeasurements. On the other hand, we were able to match the performance of\nclinicians reported in [18] (7.5% vs 7.9% p-value=0.6). The training data set\nwas relatively small and we expect to signiﬁcantly improve the performance of\nBabyNet by using more data in future work.\nThe method presented here can be characterized as end-to-end. Due to 2D+t\nspatio-temporal feature processing it does not require standard plane detection\nwhich substantially reduces the workload involved in performing the estimation\nof FBW. In clinical practice, BabyNet can be used as an aid for clinicians in their\ndecision-making process regarding the type of delivery. According to literature\n[16], [18] the heavier the child, the greater the likelihood of Cesarean delivery.\nSerious complications may arise when a heavy child’s FBW is misjudged. Under\nthese circumstances, if natural delivery is decided upon severe complications for\nboth mother and child may arise.\nThis work has certain limitations. A relatively small number of patients was\nused in the study, which can aﬀect the accuracy and generalization of results.\nA related issue is that the patient population is limited and we do not know if\nBabyNet would work on a diﬀerent population (e.g. diﬀerent race). The algo-\nrithm is trained and evaluated on short clips of US videos recorded by clinicians.\nTo operate in a clinical setting, further eﬀort would be needed to create a system\nthat extracts appropriate clips for BabyNet analysis.\nBabyNet: Birth Weight Prediction from Fetal US 9\n5 Conclusions\nIn this paper we presented an extension of the 3D ResNet-based network with\na Residual Transformer Module (RTM), named BabyNet, for 2 D + t spatio-\ntemporal fetal ultrasound video scan analysis. The proposed framework is an\nend-to-end method that automatically performs fetal birth weight prediction.\nThis is done without the need for ﬁnding standard planes in ultrasound video\nscans, which are required in the classical method of estimating fetal weight.\nCombining classical and BabyNet estimations provides the best results, signiﬁ-\ncantly outperforming top expert clinicians who use available commercial tools.\nOur method has the potential to help clinicians select – on the basis of US ex-\namination – the type of delivery which is safest for the mother and the child.\nFuture work includes testing BabyNet on external datasets which are preferably\nacquired using diﬀerent devices and by operators with diﬀerent levels of experi-\nence. Moreover, we plan to use multimodal data – combine the fetal US video\nand clinical data to improve the performance and robustness of the model.\nAcknowledgements\nThis work is supported by the European Union’s Horizon 2020 research and inno-\nvation programme under grant agreement Sano No 857533 and the International\nResearch Agendas programme of the Foundation for Polish Science, co-ﬁnanced\nby the European Union under the European Regional Development Fund. We\nwould like to thank Piotr Nowakowski for his assistance with proofreading the\nmanuscript.\nReferences\n1. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇ ci´ c, M., Schmid, C.: Vivit: A\nvideo vision transformer. In: Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV). pp. 6836–6846 (October 2021)\n2. Bano, S., Dromey, B., Vasconcelos, F., Napolitano, R., David, A.L., Peebles, D.M.,\nStoyanov, D.: Autofb: Automating fetal biometry estimation from standard ul-\ntrasound planes. In: International Conference on Medical Image Computing and\nComputer-Assisted Intervention. pp. 228–238. Springer (2021)\n3. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nInternational Conference on Learning Representations (2021)\n4. d’Ascoli, S., Touvron, H., Leavitt, M.L., Morcos, A.S., Biroli, G., Sagun, L.: Con-\nvit: Improving vision transformers with soft convolutional inductive biases. In:\nInternational Conference on Machine Learning. pp. 2286–2296. PMLR (2021)\n5. Feng, M., Wan, L., Li, Z., Qing, L., Qi, X.: Fetal weight estimation via ultrasound\nusing machine learning. IEEE Access 7, 87783–87791 (2019)\n6. Hadlock, F.P., Harrist, R., Sharman, R.S., Deter, R.L., Park, S.K.: Estimation of\nfetal weight with the use of head, body, and femur measurements—a prospective\nstudy. American journal of obstetrics and gynecology 151(3), 333–337 (1985)\n10 S. P lotka et al.\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016)\n8. Liu, Y., Sun, G., Qiu, Y., Zhang, L., Chhatkuli, A., Van Gool, L.: Transformer in\nconvolutional neural networks. arXiv preprint arXiv:2106.03180 (2021)\n9. Lu, Y., Fu, X., Chen, F., Wong, K.K.: Prediction of fetal weight at varying ges-\ntational age in the absence of ultrasound examination using ensemble learning.\nArtiﬁcial intelligence in medicine 102, 101748 (2020)\n10. Lu, Y., Zhang, X., Fu, X., Chen, F., Wong, K.K.: Ensemble machine learning for\nestimating fetal weight at varying gestational age. In: Proceedings of the AAAI\nconference on artiﬁcial intelligence. vol. 33, pp. 9522–9527 (2019)\n11. Milner, J., Arezina, J.: The accuracy of ultrasound estimation of fetal weight in\ncomparison to birth weight: A systematic review. Ultrasound 26(1), 32–41 (2018)\n12. P lotka, S., Klasa, A., Lisowska, A., Seliga-Siwecka, J., Lipa, M., Trzcinski, T.,\nSitek, A.: Deep learning fetal ultrasound video model match human observers in\nbiometric measurements. Physics in Medicine & Biology (2022)\n13. P lotka, S., W lodarczyk, T., Klasa, A., Lipa, M., Sitek, A., Trzci´ nski, T.: Fetal-\nnet: Multi-task deep learning framework for fetal ultrasound biometric measure-\nments. In: International Conference on Neural Information Processing. pp. 257–\n265. Springer (2021)\n14. Pressman, E.K., Bienstock, J.L., Blakemore, K.J., Martin, S.A., Callan, N.A.: Pre-\ndiction of birth weight by ultrasound in the third trimester. Obstetrics & Gyne-\ncology 95(4), 502–506 (2000)\n15. Reynaud, H., Vlontzos, A., Hou, B., Beqiri, A., Leeson, P., Kainz, B.: Ultrasound\nvideo transformers for cardiac ejection fraction estimation. In: International Con-\nference on Medical Image Computing and Computer-Assisted Intervention. pp.\n495–505. Springer (2021)\n16. Scioscia, M., Vimercati, A., Ceci, O., Vicino, M., Selvaggi, L.E.: Estimation of birth\nweight by two-dimensional ultrasonography: a critical appraisal of its accuracy.\nObstetrics & Gynecology 111(1), 57–65 (2008)\n17. Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155 (2018)\n18. Sherman, D.J., Arieli, S., Tovbin, J., Siegel, G., Caspi, E., Bukovsky, I.: A compar-\nison of clinical and ultrasonic estimation of fetal weight. Obstetrics & Gynecology\n91(2), 212–217 (1998)\n19. Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A.: Bottleneck\ntransformers for visual recognition. In: Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition. pp. 16519–16529 (2021)\n20. Tao, J., Yuan, Z., Sun, L., Yu, K., Zhang, Z.: Fetal birthweight prediction with\nmeasured data by a temporal machine learning method. BMC Medical Informatics\nand Decision Making 21(1), 1–10 (2021)\n21. Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look\nat spatiotemporal convolutions for action recognition. In: Proceedings of the IEEE\nconference on Computer Vision and Pattern Recognition. pp. 6450–6459 (2018)\n22. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. pp. 5998–6008 (2017)\n23. Wu, K., Peng, H., Chen, M., Fu, J., Chao, H.: Rethinking and improving rela-\ntive position encoding for vision transformer. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. pp. 10033–10041 (2021)\nBabyNet: Birth Weight Prediction from Fetal US 11\n24. Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convo-\nlutional lstm network: A machine learning approach for precipitation nowcasting.\nIn: Advances in neural information processing systems. pp. 802–810 (2015)"
}