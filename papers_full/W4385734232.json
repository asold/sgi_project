{
    "title": "ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",
    "url": "https://openalex.org/W4385734232",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3030076732",
            "name": "Sophie Jentzsch",
            "affiliations": [
                "Deutsches Zentrum für Luft- und Raumfahrt e. V. (DLR)"
            ]
        },
        {
            "id": "https://openalex.org/A2252032993",
            "name": "Kristian Kersting",
            "affiliations": [
                "Technical University of Darmstadt",
                "Hess (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2970252517",
        "https://openalex.org/W2958608582",
        "https://openalex.org/W3196552276",
        "https://openalex.org/W4287887504",
        "https://openalex.org/W2013983253",
        "https://openalex.org/W3178261001",
        "https://openalex.org/W4319662457",
        "https://openalex.org/W3158658773",
        "https://openalex.org/W2736601468",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W3176724088",
        "https://openalex.org/W3119298692",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3183822815",
        "https://openalex.org/W1597745626",
        "https://openalex.org/W4220993274",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4225418320"
    ],
    "abstract": "Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny?We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional explanations for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the classification of jokes. ChatGPT has not solved computational humor yet but it can be a big leap toward \"funny\" machines.",
    "full_text": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, pages 325–340\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nChatGPT is fun, but it is not funny!\nHumor is still challenging Large Language Models\nSophie Jentzsch1 and Kristian Kersting2,3,4\n1Institute for Software Technology, German Aerospace Center (DLR), Cologne, Germany\n2Computer Science Department, Technical University Darmstadt, Darmstadt, Germany\n3Centre for Cognitive Science, TU Darmstadt\n4Hessian Center for AI (hessian.AI)\nsophie.jentzsch@dlr.de, kersting@cs.tu-darmstadt.de\nAbstract\nHumor is a central aspect of human commu-\nnication that has not been solved for artificial\nagents so far. Large language models (LLMs)\nare increasingly able to capture implicit and\ncontextual information. Especially, OpenAI’s\nChatGPT recently gained immense public at-\ntention. The GPT3-based model almost seems\nto communicate on a human level and can even\ntell jokes. Humor is an essential component of\nhuman communication. But is ChatGPT really\nfunny?\nWe put ChatGPT’s sense of humor to the test.\nIn a series of exploratory experiments around\njokes, i.e., generation, explanation, and detec-\ntion, we seek to understand ChatGPT’s capabil-\nity to grasp and reproduce human humor. Since\nthe model itself is not accessible, we applied\nprompt-based experiments.\nOur empirical evidence indicates that jokes are\nnot hard-coded but mostly also not newly gen-\nerated by the model. Over 90% of 1008 gener-\nated jokes were the same 25 Jokes. The system\naccurately explains valid jokes but also comes\nup with fictional explanations for invalid jokes.\nJoke-typical characteristics can mislead Chat-\nGPT in the classification of jokes. ChatGPT\nhas not solved computational humor yet but it\ncan be a big leap toward “funny” machines.\n1 Introduction\nFor humans, humor plays a central role in forming\nrelationships and can enhance performance and mo-\ntivation [16]. It is a powerful instrument to affect\nemotion and guide attention [14]. Thus, a compu-\ntational sense of humor holds the potential to mas-\nsively boost human-computer interaction (HCI).\nUnfortunately, although computational humor is a\nlongstanding research domain [26], the developed\nmachines are far from \"funny.\" This problem is\neven considered to be AI-complete [22].\nContinuous advances and recent breakthroughs\nin machine learning (ML), however, lead to an ever-\nincreasing panoply of applications [e.g., 15, 3] and\nWhy don't scientists trust atoms?\nBecause they make up everything!\nSure, here's a joke for you:\nTell me a joke, please! \nFigure 1: Exemplary illustration of a conversation be-\ntween a human user and an artificial chatbot. The joke\nis a true response to the presented prompt by ChatGPT.\nlikewise open new opportunities for natural lan-\nguage processing (NLP). Transformer-based large\nlanguage models (LLMs) increasingly capture and\nreflect implicit information, such as stereotypes [7],\nmoral [6], and humor [5, 25]. Humor is often im-\nplicit and carried by subtle details. Thus these\nnovel qualities of LLMs give reason to hope for\nnew advances in artificial humor.\nMost recently, OpenAI’s ChatGPT gained im-\nmense attention for its unprecedented capabilities.\nUsers can interact with the model via public chat\nAPI in a conversation-like course. The system is\nable to answer a huge variety of questions while\ntaking the previous contextual conversation into\nconsideration. In fact, it can even tell jokes, as\ndisplayed in Fig. 1. ChatGPT is fun and almost ap-\npears to interact on a human-like level. Yet, when\ninteracting with the model, users may quickly get\na glimpse of its limitations. Although ChatGPT\ngenerates text in almost error-free English, gram-\nmatical glitches and content-related mistakes occur.\nIn the preceding exploration, we noticed that Chat-\nGPT is likely to repeat the exact same jokes fre-\nquently. Moreover, the provided jokes were strik-\ningly correct and sophisticated. These observations\nled to the hypothesis that output jokes are not origi-\nnally generated by the model. Instead, they seem\n325\nto be reproduced from the training data or maybe\neven hard coded in a predefined list.\nAs the system’s inner workings are not shared,\nwe conducted a series of structured prompt-based\nexperiments to learn about the system’s behavior\nand allow for inference regarding the generation\nprocess of ChatGPT’s output. Specifically, we aim\nto understand to what extent ChatGPT is able to\ncapture human humor by conducting a systematic\nprompt-based analysis.\nThe remainder of this paper is structured as fol-\nlows: First, Sec. 2 discusses related research. The\nmain contribution assembles three experimental\nconditions: Joke generation, joke explanation, and\njoke detection. While the overall approach is out-\nlined in Sec. 3, the detailed conduction is explained\ntogether with the results of each experiment in\nSec. 4.1, Sec. 4.2, and Sec. 4.3, respectively. We\nclose by discussing implications and further re-\nsearch in Sec. 5 and conclude our work in Sec. 6.\nTerminology Disclaimer.AI-related terminol-\nogy tends to make use of analogies to human char-\nacteristics, such as neural networks, or the term ar-\ntificial intelligence itself. Likewise, we use human-\nrelated terms in the discussion around conversa-\ntional agents, which are supposed to mimic human\nbehavior as accurately as possible, e.g., ChatGPT\n\"understands\" or \"explains.\" Although we believe\nthat these analogies describe the system’s behav-\nior and its inner workings very well, they might\nbe misleading. We would like to emphasize that\nthe discussed AI models are not on a human level\nand, at best, constitute a simulation of the human\nmind. Whether AI can potentially ever think or\nunderstand in a conscious way is a philosophical\nquestion that is out of the scope of this investiga-\ntion.\n2 Related Work\nOur work considers the intersection of two lines of\nresearch, namely LLMs and computational humor.\nLarge Language Models. NLP witnessed sev-\neral leaps in the recent past. Only a few years ago,\nthe introduction of transformer architectures in neu-\nral networks [21] enabled the development of con-\ntextualized models, such as BERT [9] or GPT [2].\nThese so-called large language models (LLMs) cap-\nture relations in the natural data and even reflect\nimplicitly-included information, which can be both\na risk [23] and a chance [17]. Either way, however,\nit is a prerequisite to solving computational humor.\nOpenAI1 recently released ChatGPT, a GPT3.5-\nbased LLM that is able to interact with users con-\nversationally through a chat-like API. It is fine-\ntuned with Reinforcement Learning from Human\nFeedback (RLHF) [12] in three essential steps, in-\ncluding large-scale manual labeling and proximal\npolicy optimization [18]. The result of this process\nis a model with unprecedented capabilities: It inter-\nacts in a conversational manner, i.e., it refers to the\nusers’ prompt while also taking information from\nthe preceding conversation into account. It is able\nto summarize information, generate new texts of all\nshapes, and even write program code. Nevertheless,\nthere are still glitches and limitations, e.g., factual\nwrong information presented as valid.\nComputational Humor. Computational humor\nis a small but active research area of NLP with\nthe main streams Humor Detection or Classifica-\ntion [e.g., 8, 4], and Humor Generation [e.g., 19].\nAlthough advances in ML and NLP transfer to com-\nputational humor, researchers highlight the great\ncomplexity of automated humor and the limitations\nof current endeavors [ 26, 24]. Humor is one of\nthe few capabilities that seemed to be reserved for\nhuman individuals thus far, which is why it is even\nconsidered AI-complete [14, 22].\nWhile decades of research in linguistics and psy-\nchology offer quite a few logic-based humor theo-\nries [e.g., 13, 27], the work in the context of artifi-\ncial agents is predominantly data-driven. In these\napproaches, pretrained language models, such as\nChatGPT, play a central role [ 10]. With their in-\nnovative capabilities, GPT-based models have the\npotential to open a new chapter of computational\nresearch.\n3 Method\nThe presented experiments are grouped into three\nindividual tasks, which are introduced in Sec. 3.1,\nSec. 3.2, and Sec. 3.3. Implementation details and\nextended results are made available at GitHub 2.\nIn all experiments, each prompt was conducted\nin a new empty conversation to avoid unwanted\ninfluence. To conduct a large number of prompts\nwith OpenAI’s ChatGPT3 web service, there were\ncertain obstacles to overcome. Since there was no\nofficial API available at the time, prompts were\n1OpenAI, https://openai.com/\n2Project repository:\nhttps://github.com/DLR-SC/JokeGPT-WASSA23\n3ChatGPT user API at chat.openai.com/, Accessed:\nJanuary-March 2023 (detailed dates in the Appendix)\n326\nWhy did the scientist\nwin an award? Because he\nwas outstanding in his ﬁeld.\nB-topic5% 53%42%\nWhy did the scarecrow\nwin an award? Because\nhe did very good work. \n100%0% 0%\nA\n0% 0% 100%\nThe scarecrow won\nan award because he\ndid good work.\n-wordplay36% 16% 40%\nThe scarecrow won an\naward because he was\noutstanding in his ﬁeld.\nDC-structure\n100% 0% 0%\nWhy did the scarecrow\nwin an award? Because he\nwas outstanding in his ﬁeld.\n-wordplay\nOriginal Joke:\nFigure 2: Modification of top jokes to create joke detection conditions. Below each condition, the percentages of\nsamples are stated that were classified as joke (green), potentially funny (yellow), and not as a joke (red). In condition\n(A) Minus Wordplay, the comic element, and, therefore, the pun itself, was removed. For condition (B) Minus\nTopic, the joke-specific topic was additionally eliminated, e.g., by removing personifications. Condition (C) Minus\nStructure keeps the validity of the joke intact but changes the typical q-a-structure to a single-sentence-sample.\nFrom that, the comic element was additionally removed to create condition (D) Minus Wordplay.\nentered with the help of a wrapper. The number\nof permitted prompts per hour was limited to 74.\nMoreover, ChatGPT was unavailable for longer\nperiods due to exceeded capacity.\nIn this work, we differentiate between originally\ngenerated output, i.e., text composed by the model,\nand replicated output, i.e., text that is memorized\nfrom training data and played back by the system\nin the exact wording. Modified output is a mix of\nboth, i.e., replicated text that is slightly altered, e.g.,\nmixing the first half of one joke with the second\nhalf of another. We classify a joke as valid if it is\nfunny from a human perspective. Accordingly, an\ninvalid joke might be grammatically correct and\neven contain joke-like elements but fails to deliver\na punch line. Naturally, as humor is subjective,\nthese categories are always debatable. That being\nsaid, the distinction is comparably evident for the\npresent examples, as we expound in the following\nchapters.\n3.1 Joke Generation\nTo test if there is a limited number of reoccurring\njokes, we analyze the deviation of output jokes. To\nthis end, we asked ChatGPT to provide a joke a\nthousand times. We applied a predefined list of\nten differently worded prompts, such as \"Can you\ntell me a joke, please?\" or \"I would love to hear\na joke.\" The resulting observations are described\nin Sec. 4.1. We identified 25 repeating top jokes,\nwhich form the basis for the two subsequent tasks.\n3.2 Joke Explanation\nIn the joke generation task, it was tested whether\nChatGPT is able to generate valid jokes. How-\never, this task does not necessarily reflect the sys-\ntem’s ability to understand humor, i.e., why the\njoke might be funny from a human perspective. To\nsee to what extent the model captures these com-\nplex inner workings of jokes, we asked ChatGPT\nto explain each of the generated top 25 jokes. The\nprompt was always \"Can you explain why this joke\nis funny: [joke].\" The results from this second task\nare presented in Sec. 4.2.\n3.3 Joke Detection\nIn the first two tasks, we identified certain criteria\nthat (almost) all output joke samples had in com-\nmon, i.e., structure, topic, and wordplay. These\ncriteria seemed to be central joke characteristics\nfor ChatGPT. To examine how close these cues\nare connected to ChatGPT’s conception of humor,\nwe manually modified the top 25 jokes to elimi-\nnate one or multiple of the three criteria, resulting\nin four additional conditions for these jokes. We\nasked the system to classify each sample with the\nprompt \"What kind of sentence is that: [sample].\"\nChatGPT’s response would then either include a\ncategorization as a joke or not, as explained in\nSec. 4.3. The three defined joke characteristics\nwere defined as follows:\nStructure: The generated jokes from Sec. 4.1\nwere in noteworthy similar semantic structure.\nDespite one sample, all 1008 generated jokes\nwere in the same question-answer format.\n327\nComic element: In jokes, there is usually a stylis-\ntic element that creates comic momentum.\nChatGPT’s generated jokes exclusively con-\ntained wordplay puns, e.g., the double mean-\ning of one word.\nTopic Joke scenarios tend to be bizarre and not\nclose to reality. Not always, but often they\ncontain personifications of objects, i.e., pro-\ntagonists can be computers or bananas.\nTo determine the impact of these characteristics\non the classification result, we compared the orig-\ninal top 25 jokes to samples with one or multiple\nof these characteristics removed. The considered\njokes were modified manually as described in Fig. 2\nto create alternative samples that were semantically\nas similar as possible. The comprehensive sets of\nsamples and their classification can be found in\nthe Appendix in Sec D. The sets were created as\nfollows.\nIn the first modification A, the wordplay was re-\nmoved from the joke (minus wordplay). To achieve\nthat, the term(s) that form(s) the center of the pun\nwere replaced by a wording with comparable pri-\nmary meaning but without double meaning. As a\nside effect, this step removes the comic element\nand therefore destroys the pun. The joke would not\nbe perceived as funny from a human perspective.\nIf the jokes contained a joke-like topic, e.g., an\naward-winning scarecrow, this was removed in a\nsecond step (minus topic) by replacing it with an\neveryday entity, e.g., a scientist, to achieve modifi-\ncation B. In case the original sample did not contain\nan unrealistic joke-specific topic, such as \"Why did\nthe man put his money in the freezer?\", it was in-\ncluded in set B and not A. Thus, samples of the\nset A contained a joke topic but no comic element\n(N = 19), and samples of the set B included none\nof both (N = 25).\nEliminating the question-answer format for mod-\nification C, i.e., minus structure, was straightfor-\nwardly possible for all 25 original jokes (N = 25)\nby rewriting it in the format \"[sentence one] be-\ncause [sentence two].\" In this case, the pun re-\nmained intact, and the joke was similarly funny.\nThe original topic remained unchanged. Then, the\ncomic element, i.e., the wordplay, was again re-\nmoved to form set D (N = 25).\n4 Results\nWith this design at hand, let us now turn to our\nempirical evidence gathered on joke generation,\nexplanation, and detection.\n4.1 Joke Generation\nTo test how rich the variety of ChatGPT’s jokes is,\nwe asked it to tell a joke a thousand times. All\nresponses were grammatically correct. Almost\nall outputs contained exactly one joke. Only the\nprompt do you know any good jokes? provoked\nmultiple jokes, leading to 1008 responded jokes in\ntotal. Besides that, the variation of prompts did\nhave any noticeable effect.\nTo extract the deviation of jokes in the set of re-\nsponses, similar samples were grouped. Removing\ndirect duplicates reduced the number of individ-\nual samples to 348. Then, we removed opening\nsentences, such as \"How about this one\" in the ex-\nample in Fig. 1, and minor formatting differences,\nsuch as extra line breaks. This resulted in a list of\n128 individual responses. Finally, some samples\ncould again be grouped together, such as in Ex.1.\nExample 1. The following samples are no di-\nrect duplicates, as the wording is slightly different.\nHowever, they represent the same pun and are there-\nfore grouped together.\n(1.1) Why did the bicycle fall over?\nBecause itwas two-tired.\n(1.2) Why didn’t the bicycle stand up by itself?\nBecause it was two tired.\nThese steps resulted in a final list of 25 top fre-\nquent jokes.\nTop 25 Jokes. The final list of the top 25 jokes\ncovered 917 of 1008 samples and can be found\nin the Appendix in Sec. B. Jokes are presented\ntogether with their number of occurrences. These\nare the five most frequent jokes:\nT1. Why did the scarecrow win an award? Be-\ncause he was outstanding in his field. (140)\nT2. Why did the tomato turn red?\nBecause it saw the salad dressing. (122)\nT3. Why was the math book sad?\nBecause it had too many problems. (121)\nT4. Why don’t scientists trust atoms?\nBecause they make up everything. (119)\n328\nT5. Why did the cookie go to the doctor?\nBecause it was feeling crumbly. (79)\nThe number of occurrences among these examples\nvaries largely. While the top four jokes occurred\nover a hundred times each, the jokes T13 to T25\noccurred less than 20 times. All 25 puns together\nsum up to about 90% of the gathered puns, but the\ntop four examples alone make more than50%. This\nobservation contradicts our initial hypothesis: In\nthe case of randomly picking from a predefined list,\nwe would expect the occurrence of samples to be\nmore equally distributed. Nevertheless, the small\nnumber of repeating samples indicates a limited\nversatility in ChatGPT’s response pattern.\nAll of the top 25 samples are existing jokes.\nThey are included in many different text sources,\ne.g., they can immediately be found in the exact\nsame wording in an ordinary internet search. There-\nfore, these examples cannot be considered original\ncreations of ChatGPT.\nOf 1008 samples, 909 were identical to one\nof the top 25 jokes. The remaining 99 samples,\nhowever, did not necessarily contain new content.\nAbout half of them were again modifications of\nthe top jokes, as illustrated by the examples Ex. 2,\nEx. 3, and Ex. 4. While some of the modified\npuns still made sense and mostly just replaced parts\nof the original joke with semantically similar ele-\nments, others lost their conclusiveness. Thus, al-\nthough the top 25 joke samples rather appear to be\nreplicated than originally generated, there seems to\nbe original content in the remaining samples.\nExample 2. Item 2.1 is the famous chicken joke -\na so-called anti-joke: It creates expectations with\nits joke-typical build-up but omits the reliving\npunch line. Besides that original joke, many vari-\nations exist with the chicken in another situation\nbut a similar-sounding anti-pun. Item 2.2 is such\na variation and is even more represented in the\nset of generated samples than in the original joke.\nItems 2.3, 2.4, and 2.5 are not covered by the top\n25 jokes and can be considered modifications by\nChatGPT, e.g., by replacing \"chicken\" in Item 2.2\nwith a semantically similar concept, i.e., \"duck,\" to\ncreate Item 2.5.\n(2.1) Why did the chicken cross the road?\nTo get to the other side. (7)\n(2.2) Why did the chicken cross the playground?\nTo get to the other slide. (33)\n(2.3) Why did the duck cross the road?\nTo get to the other pond. (2)\n(2.4) Why did the chicken wear a tuxedo?\nBecause it was a formal occasion. (1)\n(2.5) Why did the duck cross the playground?\nTo get to the other slide. (1)\nFor anti-jokes, it is especially hard to tell whether\na sample is valid, as they do not compute in the\nclassical sense. Yet, it is safe to say that the first\ntwo items are already existing jokes, and the latter\nones are rather rare or even generated by ChatGPT.\nExample 3. While it is debatable whether we\nobserve that behavior in Ex. 1, Ex 2. clearly il-\nlustrates how ChatGPT mixes up elements from\ndifferent valid jokes and, by that means, creates\nnew samples. Item 3.1 is one of the top 25 jokes.\nThe Items 3.2 and 3.3 were in the remaining 99\nnot-grouped jokes.\n(3.1) Why did the man put his money in the\nfreezer? He wanted cold hard cash. (13)\n(3.2) Why did the man put his watch in the\nblender? He wanted to make time fly. (2)\n(3.3) Why did the man put his money in the\nblender? He wanted to make time fly. (2)\nItem 3.1 and Item 3.2 are valid replicated jokes,\nand Item 3.3 is an invalid combination of those.\nExample 4. One of the most frequent jokes in\nthe output list is the math book pun, Item 4.1.\nThere were numerous valid modifications of that\njoke with minor grammatical variations, such as\nItem 4.2. It remains unclear whether all variations\nare replicated from training data sources or only\nitem 4.1 and the others were modifications by Chat-\nGPT. In Item 4.3, the \"math book\" is replaced by\na \"physics book.\" This change is unnecessary and\ndulls the punch line, but the joke is still valid, as\nalso physics books are concerned with mathemati-\ncal problems. Item 4.4, in contrast, rather appears\nas an insult than a joke. There is no wordplay in-\ncluded. ChatGPT exceeds the joke template and\ngenerates an invalid output.\n(4.1) Why was the math book sad?\nBecause it had too many problems. (92)\n(4.2) Why did the math book look so mad?\nBecause it had too many problems. (9)\n(4.3) Why was the physics book sad?\nBecause it had so many problems. (1)\n329\n(4.4) Why was the physics book sad? Because it\nhad so many formulas to memorize and so\nfew friends to share them with.(3)\nThe examples Ex. 1 to Ex. 4 illustrate how Chat-\nGPT starts from replicated joke examples and mod-\nifies them to original but rather invalid responses. If\na user openly asks for a pun, most responses consist\nof replicated puns, and comparably few examples\nare modified jokes or original content. However,\nthe modification of existing jokes can be forced\nto happen, e.g., by specifying the prompt. The\nfollowing examples, Ex. 5, Ex. 6, and Ex. 7, addi-\ntionally illustrate how ChatGPT is able to deploy\nthe learned joke structure to create original content.\nThese examples were not part of the initial 1008\nsamples but were created manually to illustrate that\nbehavior.\nExample 5. ChatGPT is able to fit different sce-\nnarios in the question-answer joke template.\nP: Can you tell me a joke about cats?\n(5.1) Why did the cat put his paw in the air?\nBecause he wanted to catch a snowflake!\nThe scenario of a cat catching a snowflake is a\nrealistic picture. The joke structure is similar to the\nreproduced puns. Nevertheless, the joke is missing\nits comic element and would most likely not work\nas a joke in a real conversation.\nExample 6. In some cases, ChatGPT’s modifica-\ntions can work quite well.\nP: Tell me a joke about physics students!\n(6.1) Why did the physics student break up with\ntheir lab partner? There was no chemistry!\nChatGPT’s output is most likely based on another\nexisting joke that comes with the question: \"Why\ndid the physics teacher break up with the biology\nteacher?\". Although the punch line in Item 6.1 is\nnot as sharp as in the original joke, it still holds the\nwordplay and is partly valid as a joke.\nExample 7. ChatGPT can adapt to a wide vari-\nety of specific contexts, such as the field of ML.\nHowever, the further away the content is from the\nknown jokes, the more likely it seems to lack any\ncomic element.\nP: Tell me a joke about machine learning!\n(7.1) Why did the machine learning algorithm\ncross the road? To get to the other side of\nthe training data set!\nIn Item 7.1, we observe how highly specific vocab-\nulary is integrated into the previously seen chicken\npun format. Repeating the same prompt yielded\ndifferent outputs, which are all in the same textual\nstructure, but with different content.\n(7.2) Why was the machine learning algorithm\nalways so pessimistic? Because it had a\nnegative bias!\n(7.3) Why did the neural network go to therapy?\nBecause it was suffering from overfitting!\nThe samples are quite reasonable in terms of con-\ntent, as, for instance, biases in ML systems. On top\nof that, they even contain basic implementations of\nwordplays, e.g., the double meaning of suffering in\nthe context of disorders and in the context of ma-\nchine learning overfitting. Nevertheless, the puns\nlack the last nuances to constitute an actually valid\njoke.\n4.2 Joke Explanation\nWe asked ChatGPT to explain each of the 25 top\njokes, which mostly resulted in a valid explanation.\nWordplays and double meanings were identified\nand pointed out correctly, such as in Ex. 8.\nExample 8. ChatGPT correctly names the dou-\nble meaning of the term \"problems\" as the comic\nelement of the pun and explains both meanings.\nP: Can you explain why this joke is funny:\nWhy was the math book sad? Because it had\ntoo many problems.\n(8.1) This joke is funny because it’s a play on\nwords. The word \"problems\" is being used\nin two different ways: as a mathematical\nterm (referring to mathematical exercises)\nand as an idiomatic expression (referring to\ndifficulties or challenges in life). [...]\nFor 23 of 25 valid jokes, the explanations were\naccurate and reasonable. ChatGPT did not only\nexplain direct but also acoustic double meanings\ncorrectly, such as \"too tired\" and \"two-tired\" or\n\"peeling\" and \"feeling.\" A special case is the ex-\nplanation of the chicken-anti-joke and its modifica-\ntions that were already discussed in Ex. 2.\n330\nExample 9. The chicken joke is a famous anti-\njoke. This common knowledge is reflected in the\nmodel and the joke, therefore, explained correctly.\nP: Can you explain why this joke is funny:\nWhy did the chicken cross the road? To get\nto the other side.\n(9.1) The humor in this joke lies in the unexpected\nand straightforward punchline. When some-\none hears the setup of the joke, they are likely\nexpecting a clever or surprising answer to\nwhy the chicken crossed the road. However,\nthe punchline is just a simple and obvious\nexplanation - the chicken crossed the road\nto get to the other side.\nThe joke is an example of an \"anti-joke,\" a\ntype of joke that intentionally avoids a typ-\nical punchline and instead delivers a mun-\ndane or literal answer. In this case, the an-\nswer is so obvious that it’s unexpected and\ntherefore amusing to many people.\nExample 10. However, for the similarly popular\nvariations of the chicken joke, ChatGPT fails to\nrecognize this connection. It only focuses on the\ncontent of the given sample and creates an abstruse\nexplanation:\nP: Can you explain why this joke is funny:\nWhy did the chicken cross the playground?\nTo get to the other slide.\n(10.1) The joke is a play on words, using the dou-\nble meaning of the word \"slide\". In a play-\nground, there are often two types of slides\n- a playground slide that children can slide\ndown and a baseball slide, which is a sliding\ntechnique used in baseball. [...]\nThis example illustrates how ChatGPT tries to ap-\nply an approved scheme of reasoning to a situation\nwhere it is not suitable.\nExample 11. ChatGPT does not only come up\nwith a convincing explanation for valid jokes but\nas well for invalid ones. This sample was one of\nthe few outputs among the generated 1008 jokes\nfrom Sec. 4.1 that were considered invalid. It only\noccurred once.\nP Can you explain why this joke is funny:\nWhy did the cookie go to the gym?\nTo get a-cookie-dized.\n(11.1) The joke is a play on words. \"To get \"a-\ncookie-dized\" is a pun on the phrase \"to\nget categorized\", with \"cookie\" being substi-\ntuted for \"categorized.\" The joke is that the\ncookie is going to the gym to become more\nfit and healthy, as if it were a person, but this\nis also implying that the cookie is trying to\nfit into a certain category or mold.\nThe joke itself is hard to understand, and the expla-\nnation even harder. ChatGPT’s explanation does\nnot reveal any humorous elements. The highlighted\nwordplay makes no sense from a human perspec-\ntive. The system seems unable to contradict that\nthis is not a funny joke or to admit that it cannot\nidentify any pun. Instead, it creates a fictional but\nconvincing-sounding explanation. Further exam-\nples can be found in the Appendix in Sec. C.\n4.3 Joke Detection\nThe results of Sec. 4.2 indicate that ChatGPT will\nmost likely not object if a presented sample is not\nvalid, leading to the question of whether the system\nis even able to differentiate jokes from not-funny\nbut joke-like samples. Therefore, we built five\ndifferent conditions (see Fig. 2) of each joke, in-\ncluding the valid jokes themselves, and asked the\nsystem to classify each sample.\nChatGPT’s responses were diverse and con-\ntained specifics regarding sentence structure or\nstylistic elements. In terms of joke classification,\nhowever, three categories of responses stood out:\n• [P] Joke: The sample is immediately and ex-\nplicitly recognized as a joke or a pun.\n• [H] Potentially funny: The humorous poten-\ntial of the sentence is mentioned secondary.\n• [N] No Joke: The sample is neither classified\nas a joke or pun, nor is its comic potential\nmentioned in the response.\nFig. 2 illustrates the modification steps and the\noverall results of this task. A more detailed de-\nscription of the categories, as well as all considered\nmodifications and their classification, are given in\nthe Appendix in Sec. D.\nAll original 25 jokes were clearly classified as a\njoke. This is not much surprising since each of the\npresented samples was output by ChatGPT as an ex-\nemplary joke in an earlier task. However, it serves\nas an affirmation of the validity of this task and of\nChatGPT’s confidence in reasoning. Two of the\n331\nmodification sets, namely modification A and mod-\nification C, show mixed classifications. These are\nthe two conditions where one of the three charac-\nteristics was removed, and the other two remained\nunchanged. In both cases, the classifications of\njokes are relatively equally divided into jokes and\nno jokes, with a slight tendency to the latter. Only\na few samples were categorized as potentially hu-\nmorous. For the remaining modification sets, i.e.,\nset B and set D, each with two characteristics re-\nmoved, all included samples were classified as no\njoke. None of the 25 samples per set was classified\nas joke or potentially humorous.\nThus, one single joke characteristic alone is not\nsufficient for ChatGPT to classify a sample as a\njoke. This applies to both a joke-typical struc-\nture and a joke-typical topic. In the case of two\njoke characteristics, the classification results were\nmixed, and all samples with three joke characteris-\ntics were classified as a joke.\n5 Discussion\nWe aimed to understand ChatGPT’s ability to cap-\nture and reflect humor. The results from three\nprompt-based tasks show implications regarding\nthe system’s inner workings.\nJoke Generation. More than 90% of the gen-\nerated samples were the same 25 jokes. This re-\ncurrence supports the initial impression that jokes\nare not originally generated. Presumably, the most\nfrequent instances are explicitly learned and mem-\norized from the model training, e.g., in the RLHF\nstep that substantially contributes to ChatGPT’s\nrevolutionary capabilities. If and to what extent a\ngenerated output is reproduced from training data\nis a non-trivial question. If we get the opportunity\nto access further training details, we will test that\nsubsequent hypothesis.\nNevertheless, we also observed jokes that were\nmodified or generated by ChatGPT. This and the un-\neven distribution of output samples do not support\nthe initial hypothesis of hard-coded jokes. Chat-\nGPT generalizes characteristics of the learned top\njokes, e.g., semantic format and wordplay puns,\nand can squeeze new topics into the known pat-\ntern. Although these are valid joke characteristics,\nit is quite a one-sided conception of jokes and even\nmore of humor in general. Thus, ChatGPT under-\nstands this specific kind of joke quite well but fails\nto reflect a larger spectrum of humor.\nJoke Explanation. The model is able to grasp\nand explain stylistic elements, such as personifica-\ntions and wordplays, impressively well. Yet, there\nare obvious limitations: ChatGPT struggles to ex-\nplain sequences that do not fit into the learned pat-\nterns. Further, it will not indicate when something\nis not funny or that it lacks a valid explanation.\nInstead, it comes up with a fictional but convincing-\nsounding explanation, which is a known issue with\nChatGPT.\nJoke Detection. We identified three main\ncharacteristics that generated jokes had in com-\nmon, i.e., structure, wordplay, and topic. The\npresence of a single joke-characteristic, e.g., the\nquestion-answer template, is not sufficient for a\nsample to be wrongly classified as a joke. The\nfact that ChatGPT was not misled by such surface\ncharacteristics shows that there is indeed a certain\nunderstanding of humorous elements of jokes.\nWith more joke characteristics, a sample is more\nlikely to be classified as a joke.\nAlthough ChatGPT’s jokes are not newly gener-\nated, this does not necessarily take away from the\nsystem’s capabilities. Even we humans do not in-\nvent new jokes on the fly but mostly tell previously\nheard and memorized puns. However, whether an\nartificial agent is able to understand what it learned\nis an exceptionally tough question and partly rather\nphilosophical than technical.\nIn the present experiments, all prompts were\nposted in an empty, refreshed chat to avoid un-\ncontrolled priming. But, clearly, context plays an\nimportant role in the perception of humor. Chat-\nGPT is able to capture contextual information and\nadjust its responses accordingly to the preceding\ncourse of conversation. This is an intriguing ca-\npacity, which we would like to include in future\ninvestigations.\n6 Conclusion\nWe test ChatGPT’s ability to recognize and reflect\nhuman humor. The model is able to correctly iden-\ntify, reproduce, and explain puns that fit into the\nlearned pattern, but it fails to meet puns of other\nkinds, resulting in a limited reflection of humor.\nAlso, it cannot yet confidently create intentionally\nfunny original content.\nThe observations of this study illustrate how\nChatGPT rather learned a specific joke pattern in-\nstead of being able to be actually funny. Never-\n332\ntheless, in the generation, the explanation, and the\nidentification of jokes, ChatGPT’s focus bears on\ncontent and meaning and not so much on superficial\ncharacteristics. These qualities can be exploited to\nboost computational humor applications. In com-\nparison to previous LLMs, this can be considered\na huge leap toward a general understanding of hu-\nmor.\nWe plan to conduct similar tasks on newly re-\nleased GPT4 models [11] and on equivalent open\nsource models, such as LLaMa [ 20] and GPT-\nNeoX [1], to compare their capabilities regarding\njoke generation and understanding.\nLimitations\nThe present study comes with two major limita-\ntions. First, humor is highly subjective, and a\nvalid and reliable evaluation is hard. Things can\nbe perceived as funny for very different reasons -\neven for being particularly not funny, such as anti-\njokes. Thus, when ChatGPT generates an odd joke\nabout ML, one could even argue that ChatGPT has\na sense of humor that is just different from ours.\nAlso, humor is diverse in reality. The present in-\nvestigation focuses on one specific form of humor,\nnamely standalone jokes. There are more manifes-\ntations to consider, which would require a much\nmore complex experimental setup.\nSecond, we cannot confidently trace back the\noutcome of the system or map it to specific input\ndata. This is challenging for large data-driven mod-\nels in general, but especially in this case, where\nwe neither have access to the model itself nor to\nany training data or to the exemplary samples from\nRLHF. This prompt-based investigation creates a\ngood intuition for the opportunities and limitations\nof ChatGPT. However, our observations and con-\nclusions are solely based on system outputs. Fur-\nther insights are needed to truly understand those\nrelations.\nEthics Statement\nChatGPT has achieved massive public attention\nand societal impact, as people use the tool for all\ndifferent kinds of tasks. This impact comes with a\nhuge responsibility and risks, such as discriminat-\ning biases or spreading misinformation.\nHowever, the system fails to meet the require-\nments of open science, as data, training details, and\nmodel characteristics are kept private. We, there-\nfore, consider our work an important contribution\nto understanding ChatGPT’s capabilities and objec-\ntively highlight its potential and limitations.\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers for the helpful comments. This work\nbenefited from the Hessian Ministry of Higher Ed-\nucation, and the Research and the Arts (HMWK)\ncluster projects “The Adaptive Mind” and “The\nThird Wave of AI”.\nReferences\n[1] Sid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregres-\nsive language model. Challenges & Perspectives in\nCreating Large Language Models, page 95.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\n[3] Giancarlo Frosio. 2023. The artificial creatives: The\nrise of combinatorial creativity from dall-e to gpt-3.\nHandbook of Artificial Intelligence at Work: Inter-\nconnections and Policy Implications (Edward Elgar,\nForthcoming).\n[4] Xu Guo, Han Yu, Boyang Li, Hao Wang, Pengwei\nXing, Siwei Feng, Zaiqing Nie, and Chunyan Miao.\n2022. Federated learning for personalized humor\nrecognition. ACM Transactions on Intelligent Sys-\ntems and Technology (TIST), 13(4):1–18.\n[5] Md Kamrul Hasan, Sangwu Lee, Wasifur Rah-\nman, Amir Zadeh, Rada Mihalcea, Louis-Philippe\nMorency, and Ehsan Hoque. 2021. Humor knowl-\nedge enriched transformer for understanding mul-\ntimodal humor. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 35, pages\n12972–12980.\n[6] Sophie Jentzsch, Patrick Schramowski, Constantin\nRothkopf, and Kristian Kersting. 2019. Semantics\nderived automatically from language corpora con-\ntain human-like moral choices. In Proceedings of\nthe 2019 AAAI/ACM Conference on AI, Ethics, and\nSociety, pages 37–44.\n[7] Sophie Jentzsch and Cigdem Turan. 2022. Gender\nbias in bert-measuring and analysing biases through\nsentiment rating in a realistic downstream classifica-\ntion task. In Proceedings of the 4th Workshop on Gen-\nder Bias in Natural Language Processing (GeBNLP),\npages 184–199.\n333\n[8] Yuta Kayatani, Zekun Yang, Mayu Otani, Noa Gar-\ncia, Chenhui Chu, Yuta Nakashima, and Haruo Take-\nmura. 2021. The laughing machine: Predicting hu-\nmor in video. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision,\npages 2073–2082.\n[9] Jacob Devlin Ming-Wei Chang Kenton and\nLee Kristina Toutanova. 2019. Bert: Pre-training\nof deep bidirectional transformers for language un-\nderstanding. In Proceedings of NAACL-HLT, pages\n4171–4186.\n[10] JA Meaney, Steven Wilson, Luis Chiruzzo, Adam\nLopez, and Walid Magdy. 2021. Semeval 2021 task 7:\nHahackathon, detecting and rating humor and offense.\nIn Proceedings of the 15th International Workshop\non Semantic Evaluation (SemEval-2021), pages 105–\n119.\n[11] OpenAI. 2023. Gpt-4 technical report.\n[12] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo\nAlmeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama,\nAlex Gray, et al. Training language models to follow\ninstructions with human feedback. In Advances in\nNeural Information Processing Systems.\n[13] Victor Raskin. 1979. Semantic mechanisms of hu-\nmor. In Annual Meeting of the Berkeley Linguistics\nSociety, volume 5, pages 325–335.\n[14] G Ritchie, R Manurung, H Pain, and D O’Mara.\n2006. Computational humor. IEEE intelligent sys-\ntems, 21(2):59–69.\n[15] Robin Rombach, Andreas Blattmann, Dominik\nLorenz, Patrick Esser, and Björn Ommer. 2021. High-\nresolution image synthesis with latent diffusion mod-\nels.\n[16] Brandon M Savage, Heidi L Lujan, Raghavendar R\nThipparthi, and Stephen E DiCarlo. 2017. Humor,\nlaughter, learning, and health! a brief review. Ad-\nvances in physiology education.\n[17] Patrick Schramowski, Cigdem Turan, Nico Ander-\nsen, Constantin A Rothkopf, and Kristian Kersting.\n2022. Large pre-trained language models contain\nhuman-like biases of what is right and wrong to do.\nNature Machine Intelligence, 4(3):258–268.\n[18] John Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proximal\npolicy optimization algorithms. arXiv e-prints, pages\narXiv–1707.\n[19] Oliviero Stock and Carlo Strapparava. 2005. Ha-\nhacronym: A computational humor system. In Pro-\nceedings of the ACL Interactive Poster and Demon-\nstration Sessions, pages 113–116.\n[20] Hugo Touvron, Thibaut Lavril, Gautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al. 2023. Llama: Open and\nefficient foundation language models. arXiv preprint\narXiv:2302.13971.\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\n[22] Tony Veale. 2021.Your Wit is My Command: Build-\ning AIs with a Sense of Humor. Mit Press.\n[23] Jonas Wagner and Sina Zarrieß. 2022. Do gender\nneutral affixes naturally reduce gender bias in static\nword embeddings? In Proceedings of the 18th Con-\nference on Natural Language Processing (KONVENS\n2022), pages 88–97.\n[24] Xingbo Wang, Yao Ming, Tongshuang Wu, Haipeng\nZeng, Yong Wang, and Huamin Qu. 2021. Dehu-\nmor: Visual analytics for decomposing humor. IEEE\nTransactions on Visualization and Computer Graph-\nics, 28(12):4609–4623.\n[25] Orion Weller and Kevin Seppi. 2019. Humor de-\ntection: A transformer gets the last laugh. pages\n3621–3625.\n[26] Thomas Winters. 2021. Computers learning humor\nis no joke. Harvard Data Science Review, 3(2).\n[27] Dolf Zillmann and Jennings Bryant. 1980. Misat-\ntribution theory of tendentious humor. Journal of\nExperimental Social Psychology, 16(2):146–160.\n334\nA Access Dates\nModels such as ChatGPT are constantly approved\nand changed. Thus, observations made on one day\nare not necessarily similarly valid on another day.\nTherefore, we list the dates of experimental access\nas precisely as possible in the following. All dates\nare in 2023.\nJoke Generation: 22. - 31. January\nJoke Explanation: 03. - 13. February\nJoke Detection: 23. February - 01. March\nB Joke Generation - Top 25 Jokes\nThe majority of generated samples were the same\n25 puns, which are presented in the following as\nT1 - T25 together with each number of occurrence:\nT1. Why did the scarecrow win an award?\nBecause he was outstanding in his field. (140)\nT2. Why did the tomato turn red?\nBecause it saw the salad dressing. (122)\nT3. Why was the math book sad?\nBecause it had too many problems. (121)\nT4. Why don’t scientists trust atoms?\nBecause they make up everything. (119)\nT5. Why did the cookie go to the doctor?\nBecause it was feeling crumbly. (79)\nT6. Why couldn’t the bicycle stand up by itself?\nBecause it was two-tired. (52)\nT7. Why did the frog call his insurance company?\nHe had a jump in his car. (36)\nT8. Why did the chicken cross the playground?\nTo get to the other slide. (33)\nT9. Why was the computer cold?\nBecause it left its Windows open. (23)\nT10. Why did the hipster burn his tongue?\nHe drank his coffee before it was cool. (21)\nT11. Why don’t oysters give to charity?\nBecause they’re shellfish. (21)\nT12. Why did the computer go to the doctor?\nBecause it had a virus. (20)\nT13. Why did the banana go to the doctor?\nBecause it wasn’t peeling well. (19)\nT14. Why did the coffee file a police report?\nBecause it got mugged. (18)\nT15. Why did the golfer bring two pairs of pants?\nIn case he got a hole in one. (13)\nT16. Why did the man put his money in the freezer?\nHe wanted cold hard cash. (13)\nT17. Why don’t seagulls fly over the bay?\nBecause then they’d be bagels. (13)\nT18. Why did the chicken go to the seance?\nTo talk to the other side. (11)\nT19. Why was the belt sent to jail?\nBecause it held up a pair of pants. (11)\nT20. Why did the chicken cross the road?\nTo get to the other side. (7)\nT21. Why did the computer go to the doctor?\nBecause it had a byte. (6)\nT22. Why did the cow go to outer space?\nTo see the moooon. (6)\nT23. Why did the man put his money in the blender?\nHe wanted to make liquid assets. (6)\nT24. Why don’t skeletons fight each other?\nThey don’t have the guts. (5)\nT25. What do you call an alligator in a vest?\nAn investigator. (5)\nC Joke Explanation\nIn this task, ChatGPT was asked to explain each of\nthe top 25 generated jokes. The prompt for each\njoke is \" Can you explain why this joke is funny:\n[joke].\" This prompt is not noted for each joke in\nthe following, as it is redundant. ChatGPT’s re-\nsponses are complete. Only the formatting of texts\nmight be changed in favor of the paper structure.\nThe majority, i.e., 23 of 25 generated explana-\ntions, were valid. Only the explanations E8, which\nis discussed in Ex. 9.1 in Sec. 4.2, and E16 are not\naccurate. A selection of exemplary explanations is\npresented in this section. The explanation numbers\nmatch the number in the top joke list in Sec. B. The\nExplanations E3, E8, and E20 are discussed in the\nmain paper. The comprehensive list of explanations\nis provided in the project repository4.\nE2. Why did the tomato turn red? Because it\nsaw the salad dressing. The joke is based on a\npun or wordplay. When a tomato ripens, it turns\nfrom green to red. The question \"Why did the\ntomato turn red?\" sets up an expectation that there\nwill be a logical reason, such as exposure to sun-\nlight or reaching maturity. However, the punchline\n\"Because it saw the salad dressing\" is unexpected\n4will be added after the anonymity period ends.\n335\nand humorous because it is a play on words. The\nword \"dressing\" has a double meaning here; it can\nrefer to the salad dressing (a condiment typically\nused on salads) or the act of dressing up or looking\nnice. So the joke is implying that the tomato saw\nthe dressing (meaning the nice-looking salad) and\ngot embarrassed, blushing and turning red.\nOverall, the joke is a lighthearted play on words\nthat relies on the listener’s understanding of the\nmultiple meanings of the word \"dressing\" to create\na humorous surprise.\nE3. Why was the math book sad? Because it had\ntoo many problems. This joke is a play on words,\nspecifically on the word \"problems.\" In mathemat-\nics, a \"problem\" typically refers to a question or\nexercise that requires a solution, whereas in every-\nday language, \"problems\" can refer to difficulties\nor challenges. In the joke, the math book is said to\nbe \"sad\" because it has too many \"problems.\"\nWhile the word \"problems\" in this context could\nrefer to the mathematical exercises in the book, the\njoke also suggests that the book is sad because\nit has too many difficulties or challenges to deal\nwith. The double meaning of \"problems\" is what\nmakes the joke humorous. Additionally, the idea of\na book having emotions like sadness is personifi-\ncation, which is another common element of jokes.\nThe unexpected pairing of a book and emotions can\nalso contribute to the humor of the joke.\nE6. Why couldn’t the bicycle stand up by itself?\nBecause it was two-tired. The joke is a play on\nwords, specifically on the word \"tired.\" In the con-\ntext of the joke, \"two-tired\" sounds like \"too tired,\"\nwhich means being exhausted or lacking energy.\nHowever, the actual meaning of \"two-tired\" in the\njoke is that the bicycle has two tires instead of a\nstand or kickstand to support it.\nSo, the punchline is unexpected because it uses\na homophone (a word that sounds the same but has\na different meaning) to create a double meaning\nin the sentence. This incongruity between what we\nexpect to hear and what is actually said is what\nmakes the joke humorous.\nE8. Why did the chicken cross the playground?\nTo get to the other slide. The joke is a play\non words, using the double meaning of the word\n\"slide\". In a playground, there are often two types\nof slides - a playground slide that children can\nslide down and a baseball slide, which is a sliding\ntechnique used in baseball. The joke sets up the\nexpectation that the chicken is crossing the play-\nground to get to the other side, which is a common\nsetup for a joke. However, the punchline surprises\nthe listener by using the word \"slide\" in a differ-\nent way, revealing that the chicken actually wants\nto use the playground slide. The unexpected twist\non a familiar joke structure, along with the clever\nwordplay, makes the joke humorous.\nE16. Why did the man put his money in the\nfreezer? He wanted cold hard cash. This joke\nis funny because it plays with the multiple meanings\nof the phrase \"cold hard cash.\"\nLiterally, \"cold hard cash\" refers to physical\ncurrency that is cold to the touch and difficult to\nbend or manipulate, as opposed to checks or digital\npayments. However, in this joke, the phrase takes\non a figurative meaning as well, suggesting that the\nman wanted his money to be \"cold\" or safe from\ntheft, and \"hard\" or difficult for anyone to access.\nBy placing his money in the freezer, the man\nachieves both meanings of \"cold hard cash\" in a\nhumorous way. The pun on the phrase is unex-\npected and clever, making the joke amusing.\nE20. Why did the chicken cross the road? To\nget to the other side. The humor in this joke lies\nin the unexpected and straightforward punchline.\nWhen someone hears the setup of the joke, they are\nlikely expecting a clever or surprising answer to\nwhy the chicken crossed the road. However, the\npunchline is just a simple and obvious explanation\n- the chicken crossed the road to get to the other\nside.\nThe joke is an example of an \"anti-joke,\" a type\nof joke that intentionally avoids a typical punchline\nand instead delivers a mundane or literal answer.\nIn this case, the answer is so obvious that it’s unex-\npected and therefore amusing to many people.\nE22. Why did the cow go to outer space? To see\nthe moooon. This joke is a play on words and\nrelies on the similarity in sound between \"moo\"\n(the sound a cow makes) and \"moon\" (the natural\nsatellite that orbits the Earth). The punchline, \"to\nsee the moooon,\" is humorous because it combines\nthe idea of a cow going to outer space with a pun\nthat emphasizes the word \"moon\" in a way that\nreferences the cow’s vocalization.\nAdditionally, the joke is also amusing because it\npresents a nonsensical scenario of a cow traveling\nto outer space, which is unexpected and somewhat\nabsurd. The humor comes from the surprise and\n336\nplayfulness of the wordplay and the unexpectedness\nof the punchline.\nD Joke Detection\nIn the joke detection task, ChatGPT was openly\nasked to categorize different samples. The sam-\nples were in one of five conditions: The original\njoke, (A) with removed wordplay, (B) with a re-\nmoved topic (which entails removed wordplay), (C)\nremoved structure, and (D) removed structure &\nwordplay. In set C, the joke is still valid, as only the\nsentence structure but no component of the comic\nelement is modified. All other modifications result\nin \"unfunny\" variations of the original joke. Thus,\nfrom a human perspective, the original jokes, as\nwell as samples from set C, would probably be cat-\negorized as jokes, and samples from sets A, B, and\nD not.\nAll modifications that are considered in this in-\nvestigation are listed in the remainder of this chap-\nter, together with their classification. The original\njokes are classified as a joke without exception and\ncan be found in Sec. B. An overview of sample sets\nand results is displayed in Tab. 1.\nThe input prompt for each sample was \" What\nkind of sentence is that: [sample].\" ChatGPT’s re-\nsponses were diverse and could contain individual\nexplanations of sentence structure or stylistic el-\nements. In terms of joke classification, however,\nresponses could be grouped into three categories:\nJoke or pun, potentially humorous, and no joke.\nThese classes are defined as follows.\n[P] Joke/ Pun ChatGPT immediately classifies\nthe sample as a joke or pun with a statement like\nThe sentence \"[...]\" is a joke or a play on words. It\nis a type of humor known as a \"pun.\". The response\nmight contain information about the semantic struc-\nture, like It is a question-and-answer format, where\nthe question sets up the joke and the answer deliv-\ners the punchline. In some cases, an explanation\nof the comic workings is also delivered, as in The\nsentence is a joke or a pun, which is a form of\nwordplay that exploits multiple meanings of a term,\nor of similar-sounding words, for humorous effect.\nIn this case, the word \"[...]\" is used in two ways:\nfirst, [...]; and second, [...], which creates a play\non words. Either way, the response clearly states\nthe sample to be a joke or a pun, or both.\n[H] Potentially Humorous In some responses,\nChatGPT only mentions the humorous potential of\nthe input statement secondary. It does not immedi-\nately label the sample to be a joke or pun, e.g., The\nsentence \"[...]\" is a declarative sentence. It makes\na statement or expresses a fact. In this case, the\nsentence describes an action taken by a chicken,\nwhich is unusual and may be intended to be hu-\nmorous or absurd. In some cases, the focus is\non stylistic elements, such as in The sentence is a\npersonification, where an inanimate object is given\nhuman-like qualities and actions. [...], which are\nactions typically associated with humans.\n[N] No Joke ChatGPT does neither classify the\nsample as a joke or pun nor mention the comic\npotential of the input sentence. In this context, \"no\njoke\" does not mean that the sample is explicitly\nstated to be not a joke, but no similar quality is\nmentioned at all. Instead, the response focuses\non grammatical structures or similar. Exemplary\nresponses for this category are The sentence \"...\"\nis a declarative sentence. It presents a statement\nor a fact and ends with a period or The sentence\n\"[...]\" is a dialogue or a conversation between two\npeople where one person is asking a question and\nthe other person is answering it.\nD.1 Modification A - Minus Wordplay\nTo create these samples, the wordplay was removed\nfrom each original joke by minimally altering the\nwording. This modification also eliminates the\ncomic element of jokes, which is they would most\nlikely not be perceived as funny by humans.\n[N] Why did the scarecrow win an award?\nBecause he did very good work.\n[N] Why did the tomato turn red?\nBecause it had a lot sun recently.\n[P] Why was the math book sad?\nBecause it was a rainy day.\n[P] Why don’t scientists trust atoms?\nBecause they tend to lie.\n[P] Why did the cookie go to the doctor?\nBecause it was feeling unwell.\n[P] Why couldn’t the bicycle stand up by itself?\nBecause it didn’t have racks.\n[H] Why did the frog call his insurance company?\nHe had a scratch in his car.\n[N] Why was the computer cold?\nBecause the heater was broken.\n337\nIncluded Characteristics Classification\nCondition N wordplay structure topic [P] [H] [N]\nOriginal 25 ✓ ✓ ✓ 25 0 0\nMod A 19 ✗ ✓ ✓ 8 1 10\nMod B 25 ✗ ✓ ✗ 0 0 25\nMod C 25 ✓ ✗ ✓ 9 4 12\nMod D 25 ✗ ✗ ✓ 0 0 25\nTable 1: Classification results of the joke detection task:Condition is either the original sample, i.e., one of the top 25\njokes, or one of four modifications. Included characteristics illustrates which of the three identified characteristics\nare included. N is the number of samples per condition. The classification is either [P] joke, [H] potentially funny,\nor [N] no joke.\n[N] Why did the hipster burn his tongue?\nHe drank hot coffee.\n[N] Why don’t oysters give to charity?\nBecause they have no money.\n[N] Why did the computer go to the doctor?\nBecause it was sick.\n[P] Why did the banana go to the doctor?\nBecause it was sick.\n[P] Why did the coffee file a police report?\nBecause it got robbed.\n[N] Why did the golfer bring two pairs of pants?\nIn case one gets damaged.\n[N] Why don’t seagulls fly over the bay?\nBecause they are mostly living in ports.\n[N] Why was the belt sent to jail?\nBecause it stole in a store.\n[P] Why did the computer go to the doctor?\nBecause it had a headache.\n[P] Why did the chicken go to outer space?\nTo see the moon.\n[N] Why don’t skeletons fight each other?\nThey are dead already.\nD.2 Modification B - Minus Topic\nTo create the samples of this condition, the joke-\ntypical and potentially humorous topic was re-\nmoved from the jokes. For instance, in the case\nof personification, i.e., if the protagonist of the\noriginal joke was an object, such as a tomato or\na computer, it was replaced by a natural, human\nprotagonist, such as a teacher or a scientist. The typ-\nical question-answer format remained unchanged.\nHowever, just as in Modification A, the actual pun\ngets lost.\n[N] Why did the scientist win an award?\nBecause he did very good work.\n[N] Why did the man turn red?\nBecause he saw the neighbour dressing.\n[N] Why was the child sad?\nBecause it had many problems.\n[N] Why don’t scientists trust journalists?\nBecause they make up everything.\n[N] Why did the woman go to the doctor? Because\nshe was feeling crumbly.\n[N] Why couldn’t the man stand up by himself?\nBecause he was drunk.\n[N] Why did the driver call his insurance com-\npany?\nHe had a jump in his car.\n[N] Why did the kid cross the playground?\nTo get to the other slide.\n[N] Why was the student cold?\nBecause it was winter.\n[N] Why did the coworker burn his tongue?\nHe drank his coffee too hot.\n[N] Why don’t millionaires give to charity?\nBecause they’re selfish.\n[N] Why did the man go to the doctor?\nBecause he had a virus.\n[N] Why did the teacher go to the doctor?\nBecause he wasn’t feeling well\n[N] Why did the driver file a police report?\nBecause it got robbed.\n[N] Why did the athlete bring two pairs of pants?\nIn case one gets dirty.\n[N] Why did the man put his money in the freezer?\nTo hide it from intruders.\n338\n[N] Why don’t pigeon fly over the bay?\nBecause they mostly live in cities.\n[N] Why did the daughter go to the seance?\nTo talk to her mother.\n[N] Why was the cashier sent to jail?\nBecause she held up a dress.\n[N] Why did the man cross the road?\nTo get to the other side.\n[N] Why did the man go to the doctor?\nBecause it had a bite.\n[N] Why did the astronaut go to outer space?\nTo see the moon.\n[N] Why did the man put bananas in the blender?\nHe wanted to make a smoothie.\n[N] Why don’t schoolboys fight each other?\nThey don’t have the guts.\n[N] What do you call a man in a vest? A vest\nwearer.\nD.3 Modification C - Minus Structure\nIn these samples, only the sentence structure of\nthe original jokes was altered in order to eliminate\nthe prominent question-answer structure. Thus, the\nactual pun remained intact, and the joke topic was\nunchanged.\n[P] The scarecrow won an award because he was\noutstanding in his field.\n[N] The tomato turned red because it saw the\nsalad dressing.\n[N] The math book was sad because it had too\nmany problems.\n[P] Scientists don’t trust atoms because they make\nup everything.\n[N] The cookie went to the doctor because it was\nfeeling crumbly.\n[P] The bicycle couldn’t stand up by itself because\nit was two-tired.\n[P] The frog called his insurance company be-\ncause he had a jump in his car.\n[N] The chicken crossed the playground to get to\nthe other slide.\n[N] The computer was cold because it left its Win-\ndows open.\n[N] The hipster burned his tongue because he\ndrank his coffee before it was cool.\n[P] Oysters don’t give to charity because they’re\nshellfish.\n[N] The computer went to the doctor because it\nhad a virus.\n[N] The banana went to the doctor because it\nwasn’t peeling well.\n[H] The coffee filed a police report because it got\nmugged.\n[N] The golfer brings two pairs of pants in case\nhe got a hole in one.\n[N] The man put his money in the freezer because\nhe wanted cold hard cash.\n[H] Seagulls don’t fly over the bay because then\nthey’d be bagels.\n[H] The chicken went to the seance to talk to the\nother side.\n[H] The belt was sent to jail because it held up a\npair of pants.\n[N] The chicken crossed the road to get to the\nother side.\n[P] The computer went to the doctor because it\nhad a byte.\n[N] The cow went to outer space to see the\nmoooon.\n[P] The man put his money in the blender because\nhe wanted to make liquid assets.\n[P] Skeletons don’t fight each other because they\ndon’t have the guts.\n[P] An alligator in a vest is called an investigator.\nD.4 Modification D - Minus Wordplay 2\nSamples of this condition are again modifications\nfrom sample set D. Additionally to altering the sen-\ntence structure, the wordplay was removed, such\nas in Set A. This resulted in potentially unfunny,\none-line statements.\n[N] The scientist won an award because she did\ngood work.\n[N] The man turned red because he saw his neigh-\nbour dressing.\n[N] The child was sad because it was raining out-\nside.\n[N] Scientists don’t trust journalists because they\ntend to lie.\n339\n[N] The teacher went to the doctor because he was\nill.\n[N] The man couldn’t stand up by himself because\nhe was drunk.\n[N] The driver called his insurance company be-\ncause he had a scratch in his car.\n[N] The child crossed the playground to get to the\nother slide.\n[N] The student was cold because the heater was\nbroken.\n[N] The coworker burned his tongue because he\ndrank hot coffee.\n[N] Millionaires don’t give to charity because they\nare selfish.\n[N] The woman went to the doctor because she\nwas sick.\n[N] The chef went to the doctor because he was\nsick.\n[N] The driver filed a police report because she\ngot robbed.\n[N] The athlete brings two pairs of pants in case\none gets damaged.\n[N] The man put his money in the freezer to hide\nit from thieves.\n[N] Pigeons don’t fly over the bay because they\nare mostly living in cities.\n[N] The daughter went to the seance to talk to the\nother side.\n[N] The cashier was sent to jail because she held\nup a dress.\n[N] The man crossed the road to get to the other\nside.\n[N] The man went to the doctor because he had a\nbite.\n[N] The astronaut went to outer space to see the\nmoon.\n[N] The man put his bananas in the blender be-\ncause he wanted to make a smoothie.\n[N] Schoolboys don’t fight each other because\nthey don’t have the guts.\n[N] A man in a vest is called a vest wearer.\n340"
}