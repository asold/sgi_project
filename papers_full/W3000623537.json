{
  "title": "SIFRank: A New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model",
  "url": "https://openalex.org/W3000623537",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2106036969",
      "name": "Yi Sun",
      "affiliations": [
        "PLA Army Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2148487581",
      "name": "Hangping Qiu",
      "affiliations": [
        "PLA Army Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2079030413",
      "name": "Yu Zheng",
      "affiliations": [
        "Ministry of Industry and Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104999120",
      "name": "Zhongwei Wang",
      "affiliations": [
        "PLA Army Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2127809753",
      "name": "Chao-ran Zhang",
      "affiliations": [
        "PLA Army Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2106036969",
      "name": "Yi Sun",
      "affiliations": [
        "Engineering (Italy)"
      ]
    },
    {
      "id": "https://openalex.org/A2148487581",
      "name": "Hangping Qiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2079030413",
      "name": "Yu Zheng",
      "affiliations": [
        "Engineering (Italy)"
      ]
    },
    {
      "id": "https://openalex.org/A2104999120",
      "name": "Zhongwei Wang",
      "affiliations": [
        "Engineering (Italy)"
      ]
    },
    {
      "id": "https://openalex.org/A2127809753",
      "name": "Chao-ran Zhang",
      "affiliations": [
        "Engineering (Italy)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2167329753",
    "https://openalex.org/W2050715692",
    "https://openalex.org/W6631501603",
    "https://openalex.org/W2974528752",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6743384090",
    "https://openalex.org/W6639055396",
    "https://openalex.org/W6601310731",
    "https://openalex.org/W6691640376",
    "https://openalex.org/W2792059528",
    "https://openalex.org/W2604853468",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W6679775712",
    "https://openalex.org/W6629028937",
    "https://openalex.org/W2605035112",
    "https://openalex.org/W6742080785",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6732014743",
    "https://openalex.org/W2963345057",
    "https://openalex.org/W2890179025",
    "https://openalex.org/W2915128308",
    "https://openalex.org/W2064418625",
    "https://openalex.org/W2962903510",
    "https://openalex.org/W2790109590",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963245897",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1854214752"
  ],
  "abstract": "In the age of social media, faced with a huge amount of knowledge and information, accurate and effective keyphrase extraction methods are needed to be applied in information retrieval and natural language processing. It is difficult for traditional keyphrase extraction models to contain a large amount of external knowledge information, but with the rise of pre-trained language models, there is a new way to solve this problem. Based on the above background, we propose a new baseline for unsupervised keyphrase extraction based on pre-trained language model called SIFRank. SIFRank combines sentence embedding model SIF and autoregressive pre-trained language model ELMo, and it has the best performance in keyphrase extraction for short documents. We speed up SIFRank while maintaining its accuracy by document segmentation and contextual word embeddings alignment. For long documents, we upgrade SIFRank to SIFRank+ by position-biased weight, greatly improve its performance on long documents. Compared to other baseline models, our model achieves state-of-the-art level on three widely used datasets.",
  "full_text": "Received December 20, 2019, accepted January 4, 2020, date of publication January 9, 2020, date of current version January 17, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.2965087\nSIFRank: A New Baseline for Unsupervised\nKeyphrase Extraction Based on Pre-Trained\nLanguage Model\nYI SUN\n 1, HANGPING QIU\n 1, YU ZHENG\n 2, ZHONGWEI WANG\n 1,\nAND CHAORAN ZHANG\n 1\n1Command & Control Engineering College, Army Engineering University of PLA, Nanjing 210001, China\n2Ceprei (Nanjing) Laboratory, The Fifth Research Institute of MIIT, Nanjing 211800, China\nCorresponding author: Hangping Qiu (qiuhp_zy@163.com)\nThis work was supported by the Equipment Development Fund Project under Grant 6141B08010101.\nABSTRACT In the age of social media, faced with a huge amount of knowledge and information, accurate\nand effective keyphrase extraction methods are needed to be applied in information retrieval and natural\nlanguage processing. It is difﬁcult for traditional keyphrase extraction models to contain a large amount of\nexternal knowledge information, but with the rise of pre-trained language models, there is a new way to\nsolve this problem. Based on the above background, we propose a new baseline for unsupervised keyphrase\nextraction based on pre-trained language model called SIFRank. SIFRank combines sentence embedding\nmodel SIF and autoregressive pre-trained language model ELMo, and it has the best performance in\nkeyphrase extraction for short documents. We speed up SIFRank while maintaining its accuracy by document\nsegmentation and contextual word embeddings alignment. For long documents, we upgrade SIFRank to\nSIFRank+by position-biased weight, greatly improve its performance on long documents. Compared to\nother baseline models, our model achieves state-of-the-art level on three widely used datasets.\nINDEX TERMS Keyphrase extraction, pre-trained language model, sentence embeddings, position-biased\nweight, SIFRank.\nI. INTRODUCTION\nKeyphrase extraction is the task of selecting a set of\nwords or phrases from a document that could summarize\nthe main topics discussed in the document [1]. Keyphrase\nextraction can greatly accelerate the speed of information\nretrieval, help people get the ﬁrst-hand information from a\nlong text quickly and accurately.\nA. MOTIVATION\nKeyphrase Extraction can be divided into two main kinds of\napproaches: supervised and unsupervised. Supervised meth-\nods perform better on speciﬁc domain tasks, but it takes a lot\nof labor to annotate the corpus, and the model after training\nmay overﬁt and do not work well on other datasets. The\nmain traditional unsupervised methods are mainly divided\ninto the models based on statistics and the models based on\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Shuai Han\n .\ngraph. Statistical models usually use different information\nfeatures such as word frequency, n-gram feature, location\nand document grammar, but this kind of information can\nhardly reﬂect the complex relationships between words in the\ndocument. Graph-based models treat human language as a\ncomplex network [2], use graph to model the relationships\nbetween words or phrases in a document. The most typical\nmodel is TextRank, the later models optimize TextRank [3]\nwith different algorithms or external information.\nAlthough graph-based models are effective, the effect of\nkeyphrase extraction can be improved better by introduc-\ning external knowledge or additional features. Using pre-\ntrained language model is one of the ways that can provide\na large amount of external knowledge. According to the\nreview paper of Papagiannopoulou and Tsoumaka [4], this\nwas summarized as embeddings-based models. With the rise\nof pre-trained language models, lots of supervised tasks in\nnatural language processing are well solved by ELMo [5],\nBert [6], XLNet [7] and other deep neural network models.\n10896 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/VOLUME 8, 2020\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nThe features of the word are no longer static word embed-\ndings like Word2Vec, but dynamic, real-time and contextual\nword embeddings such as ELMo. The pre-trained language\nmodels are pre-trained on large-scale unlabeled corpus, and\nthe representation of text can be dynamically adjusted accord-\ning to different contexts. Based on the above advantages,\nusing pre-trained language models in keyphrase extraction\ncan combine the beneﬁts of statistical models and the graph-\nbased models.\nSentence embeddings are the representation of a sen-\ntence or document. There are many ways to get the sentence\nembeddings, these methods can be well applied to differ-\nent downstream supervised tasks because of the attention\nmodel. Attention model can train the weights of embeddings\nwith supervision. However, in the unsupervised keyphrase\nextraction task, the relationship between word embeddings,\nsentence embeddings and the topic of the document need to\nbe explained by an appropriate model.\nSometimes it is not enough to use the original sentence\nembeddings for keyphrase extraction. Take the bag-of-word\nsentence embedding model for example, this kind of model\ndoes not contain position information of the words or phrases\nof the document. However, it is well known that the position\ninformation plays an important role in keyphrase extraction,\nespecially for long documents.\nB. CONTRIBUTIONS\nThe main contributions of this paper are summarized as fol-\nlows:\ni) We introduce sentence embedding model SIF [8] to\nexplain the relationship between sentence embeddings and\nthe topic of the document. Then we combine the autoregres-\nsive pre-trained language model ELMo with SIF to compute\nphrase embeddings and document embeddings. The cosine\nsimilarity is used to calculate the distance between the candi-\ndate phrases and the topic. Our model is called SIFRank.\nSIFRank computes representations of the text dynamically\nand real-time and optimized with domain data information.\nSIFRank achieves the state-of-the-art effect on two short doc-\nument datasets (Inspec and DUC2001). Besides, our model is\nmore robust than the previous SOTA model EmbedRank.\nii)We propose a method called document segmentation to\nspeed up the process of computing the word embeddings in\nlong documents. However, as the document is divided into\nsmaller parts, the effect of keyphrase extraction will decrease.\nWe take the average of the contextual embeddings of a same\nword in different position and context as the embedding\nanchor, then replace the contextual word embeddings with\nembedding anchor to calculate the result, the performance of\nthe model rebounds obviously.\niii) In order to improve the keyphrase extraction ability\nof the model for the long document dataset, we propose the\nposition-biased weight. We use the inverse of the phrases’\nﬁrst occurrence offset position as the position-biased weight.\nThen the softmax is used to make the position-biased weight\nuniform and smooth. By adding the position-biased weight\nto the cosine similarity in SIFRank, the performance of the\nmodel on long document dataset DUC2001 has been sig-\nniﬁcantly improved, and the performance on short docu-\nment dataset is not affected too much. This model is call\nSIFRank+, and it achieves the state-of-the-art result on\ndataset DUC2001.\nII. RELATED WORK\nIn this section, the related work is mainly divided into the\nfollowing 3 parts: unsupervised keyphrase extraction, pre-\ntrained language models and embedding-based keyphrase\nextraction.\nA. UNSUPERVISED KEYPHRASE EXTRACTION\nThe unsupervised keyphrase extraction methods mainly\nuse different features of the document, such as word fre-\nquency feature, position feature, linguistic features, topic\nfeature, length feature, relationship between words, external\nknowledge-based information, etc.\nGraph-based keyphrase extraction is one of the most\neffective and widely used unsupervised keyphrase extraction\nmethods. Inspired by PageRank [9], Mihalcea and Tarau\nproposed TextRank [3], this model abstracts the document\ninto a graph, where words or phrases are nodes in the\ngraph and relationships between words are edges. After\nthis, various methods were proposed to expand the infor-\nmation of the document graph. Wan and Xiao proposed\nExpandRank [10], this method uses a small number of nearest\nneighbor documents to provide more knowledge to improve\nsingle document keyphrase extraction. Bougouin et al. pre-\nsented TopicRank [11], this model is applied to assign a\nsigniﬁcance score to each topic by candidate keyphrases\nclustering. Topics are scored using the TextRank ranking\nmodel and keyphrases are extracted by selecting the most\nrepresentative candidate from each of the top-ranked top-\nics. Boudin proposed Multipartite [12] that encodes topi-\ncal information within a multipartite graph structure, this\nmodel exploits keyphrases’ mutually reinforcing relation-\nship to improve candidate ranking. Florescu and Caragea\nproposed PositionRank [13], this model use the position\ninformation of a word’s occurrences into a biased TextRank,\nand signiﬁcantly improves the effect of TextRank on long\ndocument. The position-biased weight is the sum of a word’s\ninverse position in the document, p(wi) =∑\nk 1/pk (wi,d).\nB. PRE-TRAINED LANGUAGE MODELS\nPre-trained language model is generally the kind of model\nthat trained on large-scale unlabeled corpus through neural\nnetwork structure, and then applied to downstream tasks by\nextracting network features or sharing network parameters\n(two main paradigms: feature extraction and ﬁne-tuning,\naccording to Peters et al. [14]). The development of pre-\ntrained language model has gone through roughly three\nstages: static text embeddings models, contextual text embed-\ndings models and ﬁne-tuning models.\nVOLUME 8, 2020 10897\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nThe static text embedding models’ weights are frozen, and\nthe representations of text are ﬁxed. The classic word embed-\nding models like Word2Vec [15] and GloVe [16]. Joulin et al.\nproposed FastText [17], this model adds a character-based\nn-gram model to Word2Vec, which makes it possible to calcu-\nlate the embeddings of words out of vocabulary (OOV). Sen-\ntence embeddings are higher granularity text representations.\nLe and Mikolov proposed Doc2Vec [18] based on Word2Vec.\nKiros et al. proposed the skip-thoughts [19], which trains\nan encoder-decoder model to reconstruct the surrounding\nsentences of an encoded passage. Arora et al. propose SIF [8]\nin which sentences are represented as the weighted average of\nthe word embeddings. Pagliardini et al. proposed Sent2Vec\n[20], which uses n-gram features of words to generate sen-\ntence embedding.\nContextual text embedding models can calculate the text\nembeddings dynamically based on the context. The CoVe\nproposed by McCann et al.[21] inputs the static word embed-\ndings GloVe into the supervised neural machine translation\ntask to get the context-based embeddings. The ELMo model\nproposed by Peters et al. [5] is a method of deep contextual-\nized representation. The word embeddings are learned func-\ntions of the internal states of a deep bidirectional language\nmodel (biLM), which is pre-trained on a large corpus.\nThe ﬁne-tuning type pre-trained language models’ pre-\ntrained parameters are unfrozen and can be ﬁne-tuned on\na new task. This type of model no longer extracts the\npresentation of text. Devlin et al. proposed an autoencod-\ning pre-trained language model BERT [6], a deep bidirec-\ntional Transformers model, which has introduced two tasks:\nmask language model (MLM) and next sentence prediction.\nIn the ﬁne-tuning step, different tasks differ only in the\ninput and output layers. Yang et al. proposed a generalized\nautoregressive pre-trained language model XLNet integrating\nTransformer-XL [7], which can learn bidirectional contexts\nby maximizing the expected likelihood over all permutations\nof the factorization order.\nFurthermore, according to the method of pre-training, the\npre-trained language model can be divided into two cat-\negories, one is autoregressive (AR), and the other one is\nautoencoding (AE). The AR language models like ELMo\nand XLNet seek to estimate the probability distribution of\na text corpus with an autoregressive model. Different from\nthe former, AE language models like BERT and its variants\nsuch as RoBERTa [22] do not perform explicit density esti-\nmation but instead aims to reconstruct the original data from\ncorrupted input. A big problem with the BERT-like models\nis that they use the artiﬁcial symbols like [MASK] during\npretraining, but they don’t exist in the text of the downstream\ntask. The pretrain-ﬁnetune discrepancy has a certain impact\non our keyphrase extraction model, which will be discussed\nin the analysis of the experiment afterwards.\nC. EMBEDDING-BASED KEYPHRASE EXTRACTION\nPre-trained language models provide a new research direc-\ntion for keyphrase extraction. The pre-trained model contains\na lot of information and can well represent the relation-\nship between words or phrases. Therefore, in recent years,\nembedding-based keyphrase extraction has achieved good\nperformance.\nWang et al. [23] proposed to use a Deep Belief Net-\nwork to model the hierarchical relationship with keyphrase\nembeddings. This method can clearly distinguish the tar-\nget document from others. Papagiannopoulou and Tsoumaka\nproposed RV A [24], a local word vectors guiding keyphrase\nextraction model, which uses the average of all the candidate\nphrases’ embeddings trained on individual ﬁles with GloVe\nas the reference vector, and then the similarity between the\nembeddings of candidate keyphrase and the reference vector\nis calculated and used as the score to rank. Bennani-Smires\net al. proposed EmbedRank [25], which uses the cosine simi-\nlarity between the embeddings of candidate keyphrase and the\nsentence embeddings of the document. In EmbedRank, two\nsentence embedding models Doc2Vec [18] and Sent2Vec [20]\nare used to get the representation of document. In addition,\nthey also increase use maximal marginal relevance (MMR)\nto increase coverage and diversity of the keyphrase.\nIII. MODEL OVERVIEW\nIn this section, we ﬁrstly describe the overall structure of\nSIFRank, then explain the relationship between word embed-\ndings, sentence embeddings and the topic of document rea-\nsonably through sentence embedding model SIF. Finally,\nwe brieﬂy introduce ELMo and its main characteristics and\nusage.\nA. OVERALL STRUCTURE\nThe framework of the SIFRank model is presented in\nFIGURE 1. In this model, we followed the general process\nof keyphrase extraction. The main steps are as follows:\nStep 1: The document is tokenized and part-of-speech\ntagged to sequence of tokens with part-of-speech tags.\nStep 2: Extract the noun phrases (NPs) from the sequence\naccording to the part-of-speech tags using NP-chunker (pat-\ntern wrote by regular expression). The NPs extracted from the\ndocument are the candidate keyphrases.\nStep 3: Put the sequence of tokens into the pre-trained\nlanguage model, extract the representation of each token.\nIn this case, the representation may be multi-layers word\nembeddings with different characteristics.\nStep 4: Through the sentence embedding model, turn the\nembeddings of the NPs and document to NP embeddings\nand document embeddings. At this point, they have the same\nnumber of layers and dimensions.\nStep 5: Calculate the cosine distance between NP\nembeddings and document embeddings. We regard this\ndistance as the similarity between candidate keyphrases\nand the topic of document. Choose Top-N of most\nsimilar candidate keyphrases as the ﬁnal keyphrases.\nAnd it’s the most important factor for ranking candidate\nkeyphrases.\n10898 VOLUME 8, 2020\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nFIGURE 1. The framework of the SIFRank model.\nB. SENTENCE EMBEDDING MODEL SIF\nIn this paper, we choose sentence embedding model SIF [8]\nto get the embeddings of NPs and document. It is not only\nbecause it works well with most pre-trained language models,\nbut also because the sentence embeddings obtained by this\nmodel can reﬂect the topic of document well.\nAccording to the SIF, for document d ∈D, the generation\nof sentence s is a dynamic random walk process. The kth\nword wk is generated at step k. Assume that the topic of the\ndocument does not change much during this process. That is\nto say, the generation of all words is determined by a single\ntopic cd ∈R. So, for a given sentence s, the sentence embed-\ndings are the max likelihood estimate of the topic embeddings\nthat determines the whole document. Therefore, calculating\nthe distance between candidate keyphrase embeddings and\ndocument embeddings is to calculate the similarity between\ncandidate keyphrases and document topic.\nArora et al. [8] proposed two ‘‘smooth’’ assumptions in\ntheir paper. One is that it’s assumed that some words don’t\nappear due to context. Another assumption is that the pres-\nence of high-frequency words (such as ‘‘the’’, ‘‘and’’) are\nirrelevant to the topic of the sentence. Based on these assump-\ntions, the generation probability of the sentence s with cd as\nthe topic is:\nPr [s|cd ] =\n∏\nw∈s\nPr(w|cd ) =\n∏\nw∈s\n[αfw\n+(1 −α)exp(<vw,˜cd >)\nZ˜cd\n]\n(1)\nwhere Z˜cd =∑\nw∈V exp(<˜cd ,vw >), and ˜cd =βc0 +(1 −\nβ)cd , c0⊥cd . The fw is the statistical probability of a word\nappearing on a large corpus.\nFinally, the sentence vector (the maximum likelihood esti-\nmation of the topic) can be expressed as:\nvs = 1\n|s|\n∑\nw∈s\na\na +fw\nvw = 1\n|s|\n∑\nw∈s\nWeight(w)vw (2)\nAccording to experience, the hyper-parameter a is proba-\nbly suitable in [10 −3, 10−4].\nC. PRE-TRAINED LANGUAGE MODEL EMLO\nThe word embeddings generated by ELMo [5] have 3 layers,\nwhich are represented by L0, L1 and L2 in this paper, and\nevery layer has 1024 dimensions. L0 is the Char Encode\nLayer. The static embeddings of token are generated by this\nConvolutional Neural Network layer. L1 and L2 are con-\ntextual word embeddings generated by biLM (biLSTM in\ntheir paper). According to the description in the paper of\nPeters et al., L1 is better to capture grammar information and\nL2 is better to capture context-dependent semantic informa-\ntion. The embeddings of these three layers can be weighted\nthen input into different downstream tasks.\nIV. SIFRANK AND SIFRANK+\nIn this section, we will introduce the model SIFRank and\nSIFRank+in more details, including model domain adap-\ntation, document segmentation and embeddings alignment,\ntopic similarity of candidate keyphrases and position-biased\nweight for long documents.\nA. SIFRANK\nFor a given document d, the embeddings of d is vd . The\nembeddings of the candidate keyphrase NP is vNP. SIFRank\nis deﬁned as the similarity or correlation score between vd\nand vNP:\nSIFRank(vNPi ,vd ) =Sim(vNPi ,vd ) (3)\nThe similarity can be generally calculated by cosine dis-\ntance:\nSim(vNPi ,vd ) =cos(vNPi ,vd ) = ⃗vNPi ·⃗vd\n∥⃗vNPi∥∥⃗vd ∥ (4)\nWhen the Euclidean distance is used to calculate the simi-\nlarity, the weight of the embeddings should be normalized.\nThe value of SIFRank is between 0 and 1, The closer it is\nto 1, the more relevant the candidate keyphrase is to the topic\nof the document. Conversely, the closer the value is to 0, the\nmore irrelevant the phrase is to the topic.\nVOLUME 8, 2020 10899\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nB. MODEL DOMAIN ADAPTATION\nFor text in different domains, the probability distribution of\nwords may be different. In some speciﬁc domain, words that\nare usually rare may be common.\nIn order to better adapt the model to the task of different\ndomains, we change the weight function of word in the\nprocess of sentence embeddings calculation. This weight is\nthe weighted sum in common corpus and domain corpus:\nWeight(w) =λWeightcom(w) +(1 −λ)Weightdom(w)\n=λ a\na +fw\n+(1 −λ) a′\na′+f ′w\n(5)\nwhere λ∈[0,1], Weightcom(w) is counted on large-scale cor-\npus Wikipedia, and Weight com(w) is counted on the domain\ncorpus of the task.\nIn case the frequency of the word is not found in the\nstatistics, the method of maximizing the weight of the word\nis adopted:\nWeight(wi) =min{1,maxwj∈s,i̸=j Weight(wj)} (6)\nOnce the frequency of a word cannot be found, the weight\nis set to the minimum 1 and the maximum weight of the other\nwords in the sentence.\nC. DOCUMENT SEGMENTATION AND EMBEDDINGS\nALIGNMENT\nThe same word has different word embeddings in different\ncontexts or in different position. Therefore, the word embed-\ndings are deﬁned as v\nsj\nwp\ni\n, where wi stands for word, sj is the\nsentence where the word is in, p is the position of the word in\nthe sentence.\n1) DOCUMENT SEGMENTATION\nWhen the whole document is put into ELMo, it will take a\nlong time to calculate the embeddings. When the document\nis segmented to several parts as one batch. They can be\ncomputed independently and in parallel.\nLet MSL be the minimum sequence length. Segment the\ndocument into instances no shorter than MSL. This means\nthat each instance consists of several complete sentences, and\nthe length of each instance is just longer than or equal to MSL.\nThe details of document segmentation (DS)are shown in\nAlgorithm 1.\n2) EMBEDDINGS ALIGNMENT\nBut with the segmentation of the document, the model loses\nthe complete context of the document. The performance of\nthe model will be affected. Therefore, we use the method\ncalled embeddings alignment (EA)to maintain model per-\nformance.\nAccording to Schuster et al.[26], for a non-homophones\nword, the embedding anchor is roughly at the center of\nthe point cloud of all contextual embeddings. Deﬁne the\nembedding anchor for the word wi to be ¯vwi . Therefore,\nAlgorithm 1Document Segmentation\nInput: Document d, sentence s ∈d, MSL\nOutput: The batch after document segmentation\n1. batch ←empty list\n2. instance ←empty list\n3. for alls in d do\n4. if len(instance) >MSL then\n5. add instance to batch\n6. instance ←empty list\n7. else\n8. add s to instance\n9. end if\n10. end for\n11. add instance to batch\nthe embedding anchor is deﬁned as shown in (7),\n¯vwi =1\nn\n∑\nsj,p\nv\nsj\nwp\ni\n(7)\nThat is, the embedding anchor of a word is the average\nof all contextual embeddings in different sentences and posi-\ntions. After calculating all contextual embeddings from the\nsegmented document, embedding anchors are used to replace\nall the embeddings to align the embeddings in the document.\nD. POSITION-BIASED WEIGHT FOR LONG DOCUMENTS\nFor most long documents, the author tends to write the main\ntopic of the document, which means that the most important\nkeyphrases often occur at the beginning of the document.\nAs SIFRank is a kind of bag-of-words model, it is neces-\nsary to consider the position information into the importance\nof candidate keyphrases in the keyphrase extraction for long\ndocument (especially those with multiple paragraphs).\nIn the study of Florescu and Caragea [13], the position-\nbiased weight is the sum of a word’s inverse position in the\ndocument. The word appearing at 2 th, 5 th and 10 th, has a\nweight p (wi)=1/2 +1/5 +1/10 =0.8.\nAs the word frequency information has been calculated by\nembeddings superposition in SIF. To prevent double count-\ning, we only consider where the candidate keyphrase ﬁrst\nappears. The position-biased weight is the inverse of the\nphrases’ ﬁrst occurrence offset:\np(NPi) = 1\np1 +µ (8)\nwhere p1 is the relative position NPi ﬁrst time occurrences\n(the order of all the candidate keyphrase), p1 ∈ N∗. µ is\na hyper-parameter to optimize position-biased weight of the\ncandidate keyphrases at the beginning, especially the ﬁrst\nphrase, µ∈R∗.\nTo further narrow the gap of position-bias weight of adja-\ncent candidate keyphrases, the softmax function is used to\nnormalize it:\n˜p(NPi) =soft max(p(NPi)) = exp(p(NPi))\n∑N\nk=1 exp(p(NPk ))\n(9)\n10900 VOLUME 8, 2020\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nTABLE 1. Analysis of the three datasets (the missing of keyphrases is calculated after stemming).Missing in docmeans that the annotated keyphrases do\nnot appear in the document.Missing in candidatesmeans that the annotated keyphrases are not in candidate keyphrases.\nIn conclusion, for long document, SIFRank will be\nchanged into the form shown in (10), we call it SIFRank+.\nSIFRank+(NPi,d) =˜p(NPi) ·Sim(vNPi ,vd ) (10)\nV. EVALUATION\nIn this section, we make a comprehensive evaluation of\nSIFRank and SIFRank+on three public keyphrase extraction\ndatasets.\nA. DATASETS\nIn this paper, three public datasets, Inspec, DUC2001 and\nSemEval2017, are used to evaluate our model. The statis-\ntics of the three datasets are shown in table 1. The Inspec\ndataset has the shortest average length of documents, and the\nDUC2001 is the longest. It is worth noting that not all the gold\nkeyphrases appear in the original text, and not all keyphrases\ncan be identiﬁed as candidate keyphrases. Therefore, it is the-\noretically impossible to achieve 100% keyphrase extraction.\nThe Inspec dataset [27] consists of 2000 short documents\nselected from scientiﬁc journal abstracts. There are 1000\ndocuments for training, 500 for validation and 500 for test.\nWe choose the test part to validate our model in this paper.\nThe SemEval2017 dataset [28] is the Task 10 in SemEval\n2017 competition. It contains 493 paragraphs selected from\nScienceDirect journal, covering computer science, materi-\nals science and physics. Each document is annotated with\nkeyphrases by an undergraduate and an expert.\nThe DUC2001 dataset [10] consists of 308 newspaper\narticles from the TREC-9. The articles came from several\nnewspapers and are divided into 30 topics.\nIt can be found that in table 2, documents of each topic\nin DUC2001 have similar keyphrases. About 18.11% (325 in\n1795) keyphrases appear more than once, and the frequency\nsum of these keyphrases accounts for 40.75% (1011 in\n2481) of the total frequency of all keyphrases. Therefore,\nkeyphrases in DUC2001 appear more frequently in the corpus\nof the whole dataset, which means the frequency information\ncounted from this dataset may not work.\nB. COMPARE WITH OTHER BASELINES\nWe compare our model with 3 types of unsupervised\nkeyphrase extraction methods: statistical model, graph-based\nmodel and embedding-based model. The statistical models\nTABLE 2. Analysis of the keyphrase repetition.Total means the number\nnonredundant words.M-T-O means the number of the keyphrase which\nappears more than once.Proportion means the proportion of the M-T-O\nkeyphrases’ number. Frequency proportion means the proportion of the\nM-T-O keyphrases’ frequency.\nare TFIDF and YAKE 1 [29]. The graph-based models 2 are\nTextRank [3], SingleRank [10], TopicRank [11], Position-\nRank [13] and Multipartite [12]. The embedding-based mod-\nels are RV A [24] and EmbedRank [25].\nFor TFIDF, n-gram window length is set to 3. For YAKE\nthe window size is 1, deduplication threshold is 0.9, the\nn-gram length is 3. TextRank and SingleRank have window\nsizes of 2 and 10 respectively. The minimum clustering sim-\nilarity threshold of TopicRank is set to 0.74. PositionRank\nhas a window size of 10. The hyper-parameter controlling\nthe weight adjustment strength in the Multipartite is set to\n1.1. The word embeddings of RV A are generated on each\nsingle document separately by GloVe, and the reference\nvector is computed on the full-text (not only the title and\nthe abstract), the dimension is 100. The EmbedRank 3 uses\nDBOW’s Doc2Vec model trained on wikipedia corpus. 4\nIn the SIFRank and SIFRank +proposed in this paper,\nwe use the original ELMo model pre-trained by AllenNLP. 5\nAnd we use ELMo’s layer L0 when the document is shorter\nthan 128, and ELMo’s layer L1 when the document is larger\nthan 128.\nAll the models use the same tools for tokenizing, part-of-\nspeech tagging and noun phrase chunking under the same\nenvironment. We use the Stanford CoreNLP 6 to tokenize and\npart-of-speech tag.\n1https://github.com/LIAAD/yake\n2https://github.com/boudinﬂ/pke\n3https://github.com/swisscom/ai-research-keyphrase-extraction\n4https://github.com/jhlau/doc2vec\n5https://allennlp.org/elmo\n6https://stanfordnlp.github.io/CoreNLP/\nVOLUME 8, 2020 10901\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nTABLE 3. Comparison of our model SIFRank and SIFRank+ with other baselines. N is the number extract from a single document by the models.\nRegular expression { <NN.∗|JJ>∗<NN.∗>} is used to\nextract noun phrases as the candidate keyphrases.\nAs the number of keyphrases annotated in different docu-\nments is different, the number of keyphrases extracted N is set\nto 5, 10 and 15. In this paper, macro Precision (P), Recall (R)\nand F1 value (F1) are adopted to evaluate each model. When\nthe extracted results are compared with the annotated results,\nall the words are treated with lowercase and stem processing.\nAs the result in table 3, the embedding-based models\nhave obvious advantages over the graph-based models in\n10902 VOLUME 8, 2020\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nTABLE 4. Precision of ELMo’s different layers. The number of keyphrases extracted N is set to 5.\nTABLE 5. The Precision of GloVe and BERT in SIFRank. The number of keyphrases extracted N is set to 5.\nshort document datasets Inspec and SemEval2017. But for\nthe long document dataset DUC2001, PositionRank works\nbetter than the simple embedding-based models. Our model\nSIFRank has the state-of-the-art result on short document\nInspec and SemEval2017. Our model SIFRank +with the\nposition-biased weight gets the state-of-the-art result on long\ndocument dataset DUC2001.\nC. PERFORMANCE OF ELMO’S DIFFERENT LAYERS\nAs shown in table 4, we compare the performance of different\nlayers of ELMo. ELMo-lstm-Lx stands for using the layer Lx\nonly. ELMo-lstm-L0L1L2 stands for using the average of 2\nlayers L0, L1 and L2. ELMo-Transformerstands for changing\nthe ELMo from 2 layers biLSTM to 6 layers biTransformer,\nand get the average of the 6 layers [30].\nIt shows that the contextual layer L1 performs best on the\nshort dataset Inspec. The static layer L0 performs best on\nlonger datasets SemEval2017 and DUC2001. It means that\nthe contextual embeddings (L1 and L2) can extract the feature\nof word better from short text. But when run on long text,\nthe model’ performance with contextual embeddings (L1 or\nL2) will greatly reduce.\nIt can also indicate that Transformer structures are weaker\nfor extracting word representations than LSTM on unsuper-\nvised tasks. Transformer may be more suitable for supervised\ntasks ﬁne-tuning type task.\nD. COMPARE WITH OTHER PRE-TRAINED LANGUAGE\nMODELS\nWe compare the effect of replacing ELMo with word embed-\ndings of GloVe and BERT, as shown in table 5.\nGloVe word embeddings are trained on Wikipedia\n2014 and Gigaword 5 corpus, which are 50, 100, 200 and\n300 dimensions respectively. 7\nThe BERT word embeddings use of the bert-as-service 8\nand extracts the word embeddings generated from the second-\nto-last layer (-2 layer) of BERT BASE (12 layers Transformer,\n768 dimensions) and BERT LARGE (24 layers Transformer,\n1024 dimensions) respectively. 9\nThe RoBERTa is used in the same way as BERT, there are\nalso two versions in different sizes, RoBERTa BASE (12 layers\nTransformer, 768 dimensions) and RoBERTa LARGE (24 lay-\ners Transformer, 1024 dimensions). 10\nWe use the ﬁrst layer of XLNet, which shows the best\nperformance. XLNet also has two version XLNet-Base\nand XLNet-Large, the difference is that they both use\nTransformer-XL [31] instead of Transformer. 11\nExperimental results show that for most pre-trained mod-\nels’ word embeddings, the higher the dimension is, the better\nthe effect of keyphrases extraction is.\nThe word embeddings extracted from AE language mod-\nels BERT and RoBERTa with Transformer structure work\nwell on the short Inspec dataset, while it works poorly on\nthe long document dataset DUC2001. We also try to do\nthe incremental pretraining on BERT and RoBERTa using the\ndataset corpus, but it doesn’t bring a stable improvement to\nthe model.\n7https://nlp.stanford.edu/projects/glove/\n8https://github.com/hanxiao/bert-as-service\n9https://github.com/google-research/bert\n10https://github.com/pytorch/fairseq/tree/master/examples/roberta\n11https://github.com/zihangdai/xlnet\nVOLUME 8, 2020 10903\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nTABLE 6. Comparison of robustness with EmbedRank. The number of\nkeyphrases extracted N is set to 5.\nThe AR language model XLNet has little difference in\nperformance from BERT and RoBERTa on supervised dataset\nsuch as GLUE and SQuAD (RoBERTa even performs better\nthan XLNet), but XLNet greatly improves the performance\non keyphrase extraction task. We think this is because that\nAE models use the artiﬁcial symbols like [MASK] during\npretraining, which will cause pretrain-ﬁnetune discrepancy.\nMeanwhile, the keyphrase extraction algorithm proposed in\nour paper is an unsupervised model, it is very sensitive to this\ndiscrepancy.\nEven though XLNet uses Transformer-XL which can\nlearn longer-term dependency better than RNNs and Trans-\nformer, it still performs poorly on the long document dataset\nDUC2001 compared to ELMo and even GloVe. Therefore,\nwe infer that the deeper the hidden layers, the worse the\nability to extract word representations in our task (maybe\nthe more complicated the word representation, but this is not\nsuitable for our task).\nE. ABLATION EXPERIMENT\nThe EmbedRank model removes the irrelevant words, remain\nonly nouns and adjectives when calculate sentence embed-\ndings, which can improve the model’s effect. But the perfor-\nmance will decrease greatly when it removes this part. Cause\nthe three evaluation indicators (P, R and F) are positively\ncorrelated when the number (N) of extracted keyphrases is\ndetermined, we selected one of them (P) to present to save\nthe space of paper, and the following tables are the same.\nIn table 6, we compare the changes in Precision of SIFRank\nand EmbedRank with or without removing irrelevant words.\nRemove oocmeans to remove the out-of-consideration words.\nOur SIF model performs well even without removing the\nout-of-consideration words, can better avoid the inﬂuence of\nirrelevant words, showing good robustness.\nF. PARAMETER SENSITIVE EXPERIMENT\nWe verify the inﬂuence of hyper-parameter λon the Precision\nof keyphrase extraction of the domain adaptation method on\nthese three datasets. The number of keyphrases extracted N\nis 5.\nThe results are shown in FIGURE 2 and table 7. The best\nvalues of λfor Inspec, SemEval2017 and DUC2001 are 0.5,\n0.6 and 1.0 respectively. It is observed that appropriate λcan\nimprove the performance of the algorithm above Inspec and\nFIGURE 2. The relationship between accuracy and parameter\nhyper-parameter λ.\nTABLE 7. The influence of hyper-parameterλ on the Precision of\nkeyphrase extraction. The number of keyphrases extracted N is set to 5.\nSemEval2017 to some extent. However, due to the particular-\nity of DUC2001 dataset, domain words frequency statistics of\nthis dataset does not help improve the performance.\nG. PERFORMANCE OF DOCUMENT SEGMENTATION AND\nEMBEDDINGS ALIGNMENT\nAs shown in FIGURE 3, without document segmentation,\nthe time costed by ELMo to calculate word embeddings\nincreases rapidly as the length of the document grows. With\n10904 VOLUME 8, 2020\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\nTABLE 8. The effect of document segmentation (DS) and embeddings\nalignment (EA) on SIFRank.\nFIGURE 3. Operation times of SIFRank under the full document and with\ndocument segmentation.\ndocument segmentation, and setting MSL to 16 can greatly\nspeed up the model.\nBut the subsequent, through contextual word embed-\ndings (ELMo L1 and L2 layer) decreases in the Preci-\nsion of extracting keyphrases. Therefore, by the method of\nembeddings alignment, model’s performance can be able to\nrecover. As shown in table 8, respectively is SIFRank under\nfull document calculation results, the result after document\nsegmentation (DS), and the result after the document segmen-\ntation and embeddings alignment (EA).\nH. CASE STUDY\nTo observe the difference between SIFRank and EmbedRank,\nwe select a document in the Inspec randomly. The\nkeyphrases’ correlation scores calculated by the two models\nis presented by heat maps.\nAs shown in FIGURE 4, the phrases with bold italics\nand underline in the text are the annotated keyphrases. Dif-\nferent colors represent the correlation scores of candidate\nkeyphrases by the two model. The heat map bar on the right\nshows the correlation distribution of candidate keyphrases.\nIt is observed that the correlation of candidate keyphrases\ncalculated by SIFRank in this paper has a good degree of\ndifferentiation, which can distinguish irrelevant candidate\nkeyphrases well. On the contrary, the difference between the\ncorrelation scores of different candidate keyphrases calcu-\nlated by EmbedRank is not obvious.\nFIGURE 4. Correlation scores heat maps of candidate keyphrases by\nEmbedRank and SIFRank.\nVI. CONCLUSION\nIn this paper, we propose a new baseline for unsupervised\nkeyphrase extraction based on pre-trained language model\ncalled SIFRank. We introduce sentence embedding model\nSIF and the autoregressive pre-trained language model ELMo\ninto SIFRank, which achieves state-of-the art result on short\ndocument datasets. We also speed up SIFRank while main-\ntaining its accuracy by document segmentation and embed-\ndings alignment. SIFRank is upgraded to SIFRank +with\nposition-biased weight, and its performance on long docu-\nments dataset is greatly improved.\nIn the future, there are still some questions that need fur-\nther study. Firstly, even though different pre-trained mod-\nels or layers have different characteristics, it is difﬁcult to\nbuild an efﬁcient ensemble unsupervised keyphrase extrac-\ntion model with multiple different sub-models. In our experi-\nments, the performance of any ensemble model is not as good\nas its best sub-models. Secondly, the use of position bias\nweight has greatly improved the model on long document\ndataset. Is there any other information that can be used to\nimprove the model and how to integrate them? Whether the\nnormalization method of weight must have the same distribu-\ntion as the sentence embedding model?\nREFERENCES\n[1] K. S. Hasan and V . Ng, ‘‘Automatic keyphrase extraction: A survey of the\nstate of the art,’’ in Proc. 52nd ACL, 2014, pp. 1262–1273.\n[2] R. F. I. Cancho and R. V . Sole, ‘‘The small world of human language,’’\nProc. Roy. Soc. B, Biol. Sci., vol. 268, no. 1482, pp. 2261–2265, 2001.\n[3] R. Mihalcea and P. Tarau, ‘‘TextRank: Bringing order into text,’’ in Proc.\nEMNLP, 2004, pp. 404–411.\n[4] E. Papagiannopoulou and G. Tsoumakas, ‘‘A review of keyphrase extrac-\ntion,’’Wiley Interdiscipl. Rev., Data Mining Knowl. Discovery, to be pub-\nlished, doi: 10.1002/widm.1339.\n[5] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ in Proc.\nNAACL-HLT, 2018, pp. 2227–2237.\nVOLUME 8, 2020 10905\nY. Sunet al.: SIFRank: New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model\n[6] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nNAACL-HLT, 2019, pp. 4171–4186.\n[7] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n‘‘XLNet: Generalized autoregressive pretraining for language understand-\ning,’’ 2019, arXiv:1906.08237. [Online]. Available: https://arxiv.org/a\nbs/1906.08237\n[8] S. Arora, Y . Liang, and T. Ma, ‘‘A simple but tough-to-beat baseline for\nsentence embeddings,’’ in Proc. ICLR, 2017.\n[9] L. Page, S. Brin, R. Motwani, and T. Winograd, ‘‘The PageRank cita-\ntion ranking: Bringing order to the Web,’’ Stanford InfoLab, Palo alto,\nCA, USA, Tech. Rep. SIDL-WP-1999-0120, 1999. [Online]. Available:\nhttp://ilpubs.stanford.edu:8090/422/\n[10] X. Wan and J. Xiao, ‘‘Single document keyphrase extraction using neigh-\nborhood knowledge,’’ in Proc. AAAI, 2008, pp. 855–860.\n[11] A. Bougouin, F. Boudin, and B. Daille, ‘‘TopicRank: Graph-based topic\nranking for keyphrase extraction,’’ in Proc. IJCNLP, 2013, pp. 543–551.\n[12] F. Boudin, ‘‘Unsupervised keyphrase extraction with multipartite graphs,’’\nin Proc. NAACL-HLT, vol. 2, 2018, pp. 667–672.\n[13] C. Florescu and C. Caragea, ‘‘A position-biased PageRank algorithm for\nkeyphrase extraction,’’ in Proc. AAAI, 2017, pp. 4923–4924.\n[14] M. Peters, S. Ruder, and N. A. Smith, ‘‘To tune or not to tune? Adapt-\ning pretrained representations to diverse tasks,’’ 2019, arXiv:1903.05987.\n[Online]. Available: https://arxiv.org/abs/1903.05987\n[15] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efﬁcient estimation of\nword representations in vector space,’’ 2013, arXiv:1301.3781. [Online].\nAvailable: https://arxiv.org/abs/1301.3781\n[16] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. EMNLP, 2014, pp. 1532–1543.\n[17] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, ‘‘Bag of tricks for\nefﬁcient text classiﬁcation,’’ in Proc. EACL, vol. 2, 2017, pp. 427–431.\n[18] Q. Le and T. Mikolov, ‘‘Distributed representations of sentences and\ndocuments,’’ in Proc. ICML, 2014, pp. 1188–1196.\n[19] R. Kiros, Y . Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun,\nand S. Fidler, ‘‘Skip-thought vectors,’’ in Proc. NIPS, vol. 2, 2015,\npp. 3294–3302.\n[20] M. Pagliardini, P. Gupta, and M. Jaggi, ‘‘Unsupervised learning of sentence\nembeddings using compositional N-gram features,’’ in Proc. NAACL-HLT,\n2018, pp. 528–540.\n[21] B. McCann, J. Bradbury, C. Xiong, and R. Socher, ‘‘Learned in translation:\nContextualized word vectors,’’ in Proc. NIPS, 2017, pp. 6297–6308.\n[22] Y . Liu, M. Ott, and N. Goyal, ‘‘RoBERTa: A robustly optimized bert\npretraining approach,’’ 2019, arXiv:1907.11692. [Online]. Available:\nhttps://arxiv.org/abs/1907.11692\n[23] Y . Wang, Y . Jin, X. Zhu, and C. Goutte, ‘‘Extracting discriminative\nkeyphrases with learned semantic hierarchies,’’ in Proc. COLING, 2016,\npp. 932–942.\n[24] E. Papagiannopoulou and G. Tsoumakas, ‘‘Local word vectors guiding\nkeyphrase extraction,’’ Inf. Process. Manage., vol. 54, no. 6, pp. 888–902,\n2018.\n[25] K. Bennani-Smires, C. Musat, A. Hossmann, M. Baeriswyl, and M. Jaggi,\n‘‘Simple unsupervised keyphrase extraction using sentence embeddings,’’\nin Proc. CoNLL, 2018, pp. 221–229.\n[26] T. Schuster, O. Ram, R. Barzilay, and A. Globerson, ‘‘Cross-lingual\nalignment of contextual word embeddings, with applications to zero-shot\ndependency parsing,’’ in Proc. NAACL-HLT, vol. 1, 2019, pp. 1599–1613.\n[27] A. Hulth, ‘‘Improved automatic keyword extraction given more linguistic\nknowledge,’’ in Proc. EMNLP, Jul. 2003, pp. 216–223.\n[28] I. Augenstein, M. Das, S. Riedel, L. Vikraman, and A. McCallum,\n‘‘SemEval 2017 task 10: ScienceIE-extracting keyphrases and relations\nfrom scientiﬁc publications,’’ in Proc. SemEval, 2017, pp. 546–555.\n[29] R. Campos, V . Mangaravite, A. Pasquali, A. M. Jorge, C. Nunes, and\nA. Jatowt, ‘‘YAKE! Collection-independent automatic keyword extractor,’’\nin Proc. ECIR, 2018, pp. 806–810.\n[30] M. Peters, M. Neumann, L. Zettlemoyer, and W. T. Yih, ‘‘Dissecting\ncontextual word embeddings: Architecture and representation,’’ in Proc.\nEMNLP, 2018, pp. 1499–1509.\n[31] Z. Dai, Z. Yang, and Y . Yang, ‘‘Transformer-XL: Attentive language\nmodels beyond a ﬁxed-length contex,’’ 2019, arXiv:1901.02860. [Online].\nAvailable: https://arxiv.org/abs/1901.02860\nYI SUN received the M.S. degree in informa-\ntion engineering from the Army Engineering Uni-\nversity of PLA, in 2018, where he is currently\npursuing the Ph.D. degree. His research interests\ninclude natural language processing and network\nuser analysis. He received the silver medal in the\nACM International Collegiate Programming Con-\ntest, in 2016.\nHANGPING QIU received the M.S. degree in\ncomputer application from the Institute of Com-\nmunication Engineering, in 1990, and the Ph.D.\ndegree in information engineering from the PLA\nUniversity of Science and Technology, in 2009.\nShe is currently a Professor with the Army Engi-\nneering University of PLA. Her research interests\ninclude data analysis, information management,\nand system integration.\nYU ZHENG received the B.S. degree from Hebei\nUniversity, in 2016, and the M.S. degree in\ncomputer application from the Army Engineering\nUniversity of PLA, in 2019. She is currently a\nSoftware Engineer with the Ceprei (Nanjing) Lab-\noratory. Her research interests include software\ntesting and machine learning.\nZHONGWEI WANG received the B.S. degree\nfrom the Army Engineering University of PLA\nin 2017, where he is currently pursuing the M.S.\ndegree. His research interests include information\nrecommendation and network user analysis.\nCHAORAN ZHANG received the B.S. degree\nin communication engineering from Dalian\nMinzu University, in 2018. He is currently pur-\nsuing the M.S. degree in computer applica-\ntion with the Army Engineering University of\nPLA. His research interests include natural lan-\nguage processing, deep learning, and optical\ncommunication.\n10906 VOLUME 8, 2020",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8850206136703491
    },
    {
      "name": "Baseline (sea)",
      "score": 0.7922183275222778
    },
    {
      "name": "Language model",
      "score": 0.6503069996833801
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6404134035110474
    },
    {
      "name": "Natural language processing",
      "score": 0.5925093293190002
    },
    {
      "name": "Natural language",
      "score": 0.5334689021110535
    },
    {
      "name": "Sentence",
      "score": 0.490596204996109
    },
    {
      "name": "Word embedding",
      "score": 0.4813878834247589
    },
    {
      "name": "Automatic summarization",
      "score": 0.4636712074279785
    },
    {
      "name": "Word (group theory)",
      "score": 0.4499106705188751
    },
    {
      "name": "Embedding",
      "score": 0.40723785758018494
    },
    {
      "name": "Linguistics",
      "score": 0.09238913655281067
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210163363",
      "name": "PLA Army Engineering University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I890469752",
      "name": "Ministry of Industry and Information Technology",
      "country": "CN"
    }
  ],
  "cited_by": 132
}