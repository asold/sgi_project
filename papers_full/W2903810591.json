{
  "title": "Tied Transformers: Neural Machine Translation with Shared Encoder and Decoder",
  "url": "https://openalex.org/W2903810591",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2110275676",
      "name": "Yingce Xia",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2127696565",
      "name": "Tianyu He",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1983219134",
      "name": "Xu Tan",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2089111133",
      "name": "Fei Tian",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2109309577",
      "name": "Di He",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1719067107",
      "name": "Tao Qin",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2110275676",
      "name": "Yingce Xia",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A1983219134",
      "name": "Xu Tan",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2089111133",
      "name": "Fei Tian",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2109309577",
      "name": "Di He",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1719067107",
      "name": "Tao Qin",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2251743902",
    "https://openalex.org/W2770553825",
    "https://openalex.org/W2443536229",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W6730267373",
    "https://openalex.org/W6729383884",
    "https://openalex.org/W2798931235",
    "https://openalex.org/W2765961751",
    "https://openalex.org/W6685145238",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W6600089797",
    "https://openalex.org/W6684207430",
    "https://openalex.org/W2284660317",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2785093437",
    "https://openalex.org/W6740590407",
    "https://openalex.org/W6751388411",
    "https://openalex.org/W6689098867",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2540093921",
    "https://openalex.org/W2963774520",
    "https://openalex.org/W2963247703",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2964034111",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W4300687381",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964007535",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W2794365787",
    "https://openalex.org/W2555745756",
    "https://openalex.org/W2952576443",
    "https://openalex.org/W2803569830",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2742079690",
    "https://openalex.org/W2963602293",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2779385623",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4298393544",
    "https://openalex.org/W1944672",
    "https://openalex.org/W2963206679",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2546938941",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964190861",
    "https://openalex.org/W1902237438"
  ],
  "abstract": "Sharing source and target side vocabularies and word embeddings has been a popular practice in neural machine translation (briefly, NMT) for similar languages (e.g., English to French or German translation). The success of such wordlevel sharing motivates us to move one step further: we consider model-level sharing and tie the whole parts of the encoder and decoder of an NMT model. We share the encoder and decoder of Transformer (Vaswani et al. 2017), the state-of-the-art NMT model, and obtain a compact model named Tied Transformer. Experimental results demonstrate that such a simple method works well for both similar and dissimilar language pairs. We empirically verify our framework for both supervised NMT and unsupervised NMT: we achieve a 35.52 BLEU score on IWSLT 2014 German to English translation, 28.98/29.89 BLEU scores on WMT 2014 English to German translation without/with monolingual data, and a 22.05 BLEU score on WMT 2016 unsupervised German to English translation.",
  "full_text": "The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)\nTied Transformers: Neural Machine\nTranslation with Shared Encoder and Decoder\nYingce Xia,1 Tianyu He,2 Xu Tan,1 Fei Tian,1 Di He,3 Tao Qin1\n1Microsoft Research, 2University of Science and Technology of China\n3Key Laboratory of Machine Perception, MOE, School of EECS, Peking University\n{yinxia,xuta,fetia,taoqin}@microsoft.com, hetianyu@mail.ustc.edu.cn, di he@pku.edu.cn\nAbstract\nSharing source and target side vocabularies and word embed-\ndings has been a popular practice in neural machine trans-\nlation (brieﬂy, NMT) for similar languages (e.g., English to\nFrench or German translation). The success of such word-\nlevel sharing motivates us to move one step further: we con-\nsider model-level sharing and tie the whole parts of the en-\ncoder and decoder of an NMT model. We share the encoder\nand decoder of Transformer (Vaswani et al. 2017), the state-\nof-the-art NMT model, and obtain a compact model named\nTied Transformer. Experimental results demonstrate that such\na simple method works well for both similar and dissimilar\nlanguage pairs. We empirically verify our framework for both\nsupervised NMT and unsupervised NMT: we achieve a35.52\nBLEU score on IWSLT 2014 German to English translation,\n28.98/29.89 BLEU scores on WMT 2014 English to Ger-\nman translation without/with monolingual data, and a 22.05\nBLEU score on WMT 2016 unsupervised German to English\ntranslation.\n1 Introduction\nNeural machine translation (brieﬂy, NMT), based on the\nencoder-decoder framework with an attention module, has\nmade signiﬁcant progress in recent years (Vaswani et al.\n2017; Hassan et al. 2018; He et al. 2016; Xia et al. 2017b).\nA common practice for training an NMT system is to share\nsource and target side vocabularies, especially when the\ntwo languages are similar or in the same language fam-\nily, like English ↔French translation (Gehring et al. 2017;\nVaswani et al. 2017). Such a shared vocabulary is built\nupon subword units like BPE (Sennrich, Haddow, and Birch\n2016b) or word pieces (Wu et al. 2016), and both source\nand target words are split into smaller segmentations in this\nshared vocabulary. Acting in this way, the words in source\nsentences and target sentences are mapped to the same em-\nbedding space, which strengthens the semantic correlation\nbetween the two languages, and regularizes a neural network\nmodel with high capacity.\nParameter sharing has a long history in machine learn-\ning and has been applied in different learning settings such\nas multi-task learning (Ruder 2017; Zhang and Yang 2017),\ntransfer learning (Pan and Yang 2010), meta-learning (Pham\nCopyright c⃝ 2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\net al. 2018), etc. Parameter sharing means that multiple mod-\nels are bridged by tying parts of or all the model parameters.\nIt has multiple potential beneﬁts, such as reducing the num-\nber of model parameters so as to control model complex-\nity (Firat et al. 2016), introducing prior knowledge to regu-\nlarize models (Xia et al. 2018), and saving the storage space\nor memory size (Johnson et al. 2017). Parameter sharing\nis also adopted within network structure, including spatial\nsharing for the convolutional kernels in convolutional neural\nnetworks and temporal sharing for the transition functions in\nrecurrent neural networks (Goodfellow et al. 2016). Inspired\nby the success of sharing vocabularies (and so word embed-\ndings) of source and target languages, a question naturally\ncomes out: Can we go one step forward to further share the\nwhole parts of the encoder and decoder of an NMT model?\nWe make an initial attempt to answer this question and\nthen propose tied transformer. We cast the typical encoder-\ndecoder based sequence-to-sequence model into a more\ncompact one in that there is only one copy of parameter set,\nwhich is applicable to both the encoder and decoder. In that\nway we force the sharing among the weights of the encoder\nand the decoder, rather than only among the source-side and\ntarget-side word embeddings.\nWe conduct extensive experiments to test the effective-\nness of our proposed model. (1) For the translation between\nsimilar languages (i.e., languages within the similar lan-\nguage family), like {German, Spanish, Romanian}-from/to-\nEnglish, our model achieves promising even state-of-the-art\nresults: We achieve 35.52 for IWSLT German to English\ntranslation (see Figure 2), 28.98/29.89 for WMT 2014 En-\nglish to German translation without/with monolingual data\n(see Table 4), and 34.67 for WMT 2016 English to Ro-\nmanian translation (see Table 5). (2) For the translation of\ndissimilar languages (e.g., languages in different language\nfamilies that cannot share vocabularies such as {Russian,\nHebrew, Chinese}-from/to-English), our model also obtains\ngood improvements (See Table 7&8). (3) For unsupervised\nNMT where no bilingual data is available, on WMT 2016\nGerman→English translation, we improve the BLEU score\nfrom 21.0 to 22.05 (see Table 9).\n2 Background\nIn this section, we introduce the background of a general\nNMT model and the basic structure of Transformer.\n5466\n2.1 NMT Architecture\nThe task of NMT is to translate a sentence x ∈ Xin the\nsource language to the one y ∈ Yin the target language,\nwhere Xand Yare two language spaces. An NMT model\nconsists of an encoder fE, a decoder fD and a source-to-\ntarget attention model fA. A source sentence x is ﬁrst en-\ncoded into a series of hidden representations by using fE,\nthat is, (h1,h2,··· ,hTx ) = fE(x), where Tx is the length\nof x, hi is the i’th hidden representation ∀i ∈[Tx]. Then,\nwhen translating the j’th word yj in the target side, the at-\ntention model fA is used to calculate the context vector of\nthe encoder’s outputs, that is,\ncj = ∑Tx\ni=1αi,jhi, αi,j = fA(hi,sj−1), (1)\nwhere αi,j’s have to satisfy ∑Tx\ni=1αi,j = 1 and αi,j ≥\n0. Next, based on the context vector cj, we calculate\nthe j’s hidden representation at target side by sj =\nfD(sj−1,yj−1,cj), where yj−1 is the (j−1)’th target word.\nFinally, sj is mapped to yj ∈ Yby a simple network.\nThe fE, fD can be specialized by LSTM (Wu et al. 2016),\nGRU (Bahdanau, Cho, and Bengio 2015), CNN (Gehring\net al. 2017), Transformer (Vaswani et al. 2017) and so on.\nfA can be implemented by a bilinear function like αi,j ∝\nexp(hiWAsj−1) ∀i ∈ [Tx], a sum of two afﬁne transfor-\nmations like αi,j ∝exp(WA,hhi + WA,ssj−1) ∀i ∈[Tx]\nwhere the W’s are the parameters to be learned, and so on.\nLuong, Pham, and Manning (2015) summarize different at-\ntention models.\n2.2 Parameter Sharing in NMT\nSeveral parameter sharing mechanisms have been explored\nfor multilingual NMT. Dong et al. (2015) leveraged a net-\nwork with one encoder and multiple decoders for one-to-\nmany language translations. Luong et al. (2016) proposed a\nstructure with multiple encoders and one decoder for many-\nto-one language translation. Zoph and Knight (2016) tar-\ngeted at a multi-source translation problem, where the de-\ncoder is shared. Firat, Cho, and Bengio (2016) designed\na network with multiple encoders and decoders plus a\nshared attention mechanism across different language pairs\nfor many-to-many language translation. (Ha, Niehues, and\nWaibel 2016; Johnson et al. 2017) used a single encoder-\ndecoder model to work on many-to-many languages trans-\nlation. While there exist quite a few papers studying param-\neter sharing for multiple language translation, there are few\nwork studying parameter sharing in one task and we attack\nthis problem.\n2.3 Brief Introduction of Transformer\nTransformer (Vaswani et al. 2017) is a general framework\nbuilt upon self-attention that achieves state-of-the-art results\non many translation tasks. Transformer is a stacked network\nwith several blocks. Each block consists of two or three basic\nmodules:\n(1) A self-attention module ϕS, used to output a weighted\nversion of its inputs. That is, given an m-element set Z =\n{z1,z2,··· ,zm}, for any i ∈ [m], zS\ni = ϕS(zi,Z) =\n∑m\nj=1αi,jzj, where αi,j is calculated by the multi-head at-\ntention as that proposed in Vaswani et al. (2017) withzi and\nZ as inputs. Z can be a collection of either source side hid-\nden states or target side hidden states. This layer is asso-\nciated with a residual connection and layer normalization\nLN (Ba, Kiros, and Hinton 2016), whose eventual output is\nLN(zi + zS\ni ) for any i∈[m].\n(2) An optional cross-lingual (i.e., source-to-target) atten-\ntion model ϕC that will appear in the decoder only. Sim-\nilar to ϕS, set H = {h1,h2,··· ,HTx }, Tx is the length\nof source sentence x, H is the output of the last layer\nin the encoder, si is the i’th hidden state in the decoder,\nsC\ni = ϕC(si,H), where ϕC(si,H) is executed in a simi-\nlar way as the ϕS in step (1) with si and H as inputs. This\nlayer will eventually outputs LN(si + sC\ni ). Both ϕS and ϕC\nare implemented as multi-head attention models and details\ncan be found in Vaswani et al. (2017).\n(3) A feed-forward network ϕF(z) = W2 max(W1z +\nb1,0) + b2, where the W’s andb’s are the parameters to be\nlearned. Again, this layer is followed with a residual connec-\ntion and layer normalization.\n3 Our Model\nIn this section, we introduce the architecture of our proposed\ntied transformer. We describe the model architecture in Sec-\ntion 3.1 and give a theoretical discussion in Section 3.2.\nFigure 1: The architecture of tied transformer\n3.1 Model Architecture\nThe architecture of tied transformer is shown in Figure 1.\nWe follow the notations deﬁned in Section 2.3 to mathe-\nmatically describe our model. Tied transformer is a stacked\nmodel with Lblocks, where the l’th block consists of a self-\nattention module ϕl\nS, a cross-lingual attentionϕl\nC and a non-\nlinear function ϕl\nF, where superscript lrepresents the layer\nid. We show how the encoder and decoder are shared.\n(1) Encoding For any input x, we initialize H0 =\n{h0\n1,h0\n2,··· ,h0\nTX }, where h0\ni is the word embedding of the\ni’th word. The encoding process can be formulated as fol-\n5467\nlows: For any l∈[L],\nhl\ni = ϕl\nF\n(\nϕl\nS(hl−1\ni ,Hl−1)\n)\n;\nHl = {hl\n1,hl\n2,··· ,hl\nTX }.\n(2)\nEventually, we obtainHL, a set containing all the outputs of\nthe last layer in the encoder.\n(2) Decoding At the (j + 1) ’th decoding step, denote\nthe generated hidden states at the target side as Sl\nj =\n{sl\n1,sl\n2,··· ,sl\nj}, l∈{0,1,··· ,L}where S0\nj represents the\ncollection of word embeddings at target side. We ﬁrst set\ns0\nj+1 = yj and initialize S0\nj+1 = S0\nj ∪{yj}, where yj is\nthe word embedding of the predictedj’th wordyj. Then, for\nany l∈[L],\nsl\nj+1 = ϕl\nF\n(\nϕl\nC\n(\nϕl\nS(sl−1\nj+1,Sl−1\nj+1),HL))\n,\nSl\nj+1 = Sl\nj ∪{sl\nj+1}.\n(3)\nOnce we obtain sL\nj+1, we could use a simple network with\na softmax layer to predict the corresponding word yj+1. We\nrepeat decoding until meeting the end-of-sentence token.\nNote that given any l ∈[L], the ϕl\nS and ϕl\nF in Eqn.(2)\nand Eqn.(3) are shared; however, in a standard encoder-\ndecoder based network, such two components are not tied.\nThis shows that our proposed model is strongly regularized\nduring training.\nNote that such a parameter sharing scheme could also be\napplied to LSTM based encoder-decoder frameworks. An\nempirical analysis is illustrated in Section 4.4.\n3.2 Theoretical Discussion\nWe provide a brief theoretical discussion about the proposed\ntied transformer. Denote the parameters of the encoder, de-\ncoder and source-to-target attention modules asθe, θdand θa\nrespectively. For a tied transformer,θe= θd. In an NMT task,\nwhat we learn is a mapping f : X ↦→Y, that can translate\nsentences from source language space Xto target language\nspace Y. Our objective is to minimize the (expected) risk of\na model f, which is deﬁned as follows:\nR(f) = E(x,y)∼P[ℓ(f(x),y)] . (4)\nwhere Prepresents the underlying distribution of a source\nand target language sentence pair over X ×Y, ℓ is the\nloss function that can achieve the mapping X ×Y ↦→\nR. Denote Θe, Θd and Θa as the parameter spaces of\nthe encoder, decoder and source-to-target attention model.\nFor the conventional transformer, f ∈ Fc where Fc =\n{f(x; θe,θd,θa)|θe ∈ Θe,θd ∈ Θd,θa ∈ Θa}. For the\ntied transformer, f ∈Ft where Ft = {f(x; θe,θd,θa)|θe ∈\nΘe,θd ∈Θd,θa ∈Θa,θe = θd}. Obviously, Ft ⊆Fc.\nWhen training an NMT model, we minimize the empirical\nrisk on the ntraining samples {(xi,yi)}n\ni=1, xi ∈X , yi ∈\nY, which is deﬁned as follows: for any f ∈Fc or f ∈Ft,\nRn(f) = 1\nn\nn∑\ni=1\nℓ(f(xi),yi).\nWe introduce Rademacher complexity (Mohri et al., 2012)\nfor our proposed method, a measure for the complexity of\nthe function space.\nDeﬁnition 1 Denote the Rademacher complexity of the\nstandard transformer and the tied transformer as Rc\nn and\nRt\nn respectively. We have\nRc\nn = E\nz,σ\n[\nsup\nf∈Fc\n1\nn\nn∑\ni=1\nσiℓ(f(xi),yi)\n]\n,\nRt\nn = E\nz,σ\n[\nsup\nf∈Ft\n1\nn\nn∑\ni=1\nσiℓ(f(xi),yi)\n]\n,\n(5)\nwhere z = {z1,z2,··· ,zn}∼P n, zi = (xi,yi) in which\nxi ∈X and yi ∈Y, σ = {σ1,··· ,σm}are i.i.d sampled\nwith P(σi = 1) = P(σi = −1) = 0.5.\nThe following theorem holds for our proposed method:\nTheorem 1 (Theorem 3.1, (Mohri et al., 2012)) Let\nℓ(f(x),y) be a mapping from X ×Yto [0,1]. Then, for\nany δ ∈(0,1), with probability at least 1 −δ, the following\ninequalities holds:\nR(f) ≤Rn(f) + 2Rc\nn +\n√\n1\n2nln(1\nδ), ∀f ∈Fc,\nR(f) ≤Rn(f) + 2Rt\nn +\n√\n1\n2nln(1\nδ), ∀f ∈Ft.\nSince Ft ⊆Fc, we have that Rt\nn ≤Rc\nn, which shows that\ntied transformers have better generalization abilities than\nstandard transformer.\n4 Experiments on Similar Languages\nIn this section, we verify our proposed method on languages\nin similar language families. We work on various amounts of\ntraining data, including IWSLT translation tasks (less than\n200k sentences pairs), WMT translation tasks (more than\n2M sentence pairs) and the usage of monolingual data (ad-\nditional 8M unlabeled sentences).\n4.1 Settings\nWe choose IWSLT 2014 {German, Spanish, Romanian }-\nfrom/to-English translation and WMT 2014 English-to-\n{German, Romanian}to verify our proposed algorithm. We\nbrieﬂy denote German, Romanian, Spanish and English as\n“De”, “Ro”, “Es” and “En” respectively.\nDatasets For IWSLT 2014 De ↔En, Es ↔En and Ro ↔En\ntranslation tasks, there are respectively 153k, 181k, 182k\nsentence pairs in each datasets 1. For De →En translation,\nwe follow the common practice to lowercase all words; and\nwe use the same validation and test sets as those used in\n(Edunov et al. 2018), which consists of 7kand 7ksentences\nrespectively. For the other translation tasks, we do not low-\nercase the words; we use IWSLT14.TED.tst2013 as the val-\nidation sets and IWSLT14.TED.tst2014 as test sets.\nFor WMT 2014 En →De and WMT 2016 En →Ro trans-\nlation, there are 4.5M and 2.8M bilingual sentence pairs\nrespectively. For En→De translation, we concatenate new-\nstest2012 and newstest2013 as the validation set and use\n1All IWSLT 2014 training data can be found at https://wit3.fbk.\neu/archive/2014-01/texts\n5468\nnewstest2014 as the test set. For En →Ro, we use news-\ndev2016 as the validation set and use newstest2016 as the\ntest. For both IWSLT and WMT translation tasks, all the\ndatasets are preprocessed into workpieces following (Wu et\nal. 2016).\nModel Conﬁgurations For the IWSLT 2014 translation\ntasks, we choose the transformer small setting with 8\nblocks, where each block contains a self-attention layer,\nan optional encoder-to-decoder attention layer and a feed-\nforward layer. The word embedding dimension, hidden state\ndimension, non-linear layer dimension and the number of\nhead are 256, 256 and 1024 and 4 respectively. For WMT\n2014 En→De translation and WMT 2016 En →Ro task, we\nchoose the transformer big setting, where the four numbers\nare 1024, 1024, 4096 and 16 respectively. The dropout rate\nfor the two settings are 0.1 and 0.2. For the three IWSLT\n2014 translation tasks, we train each model on two V100\nGPUs for up to three days until convergence and the mini-\nbatch size is ﬁxed as 4096 tokens per GPU, including both\nthe source to target and target to source data. For the two\nWMT translation tasks, we train each model on four P40\nGPUs for ten days until convergence. We try to use the\nlargest possible minibatch sizes for the WMT settings to ful-\nﬁll GPU memory limitation.\nEvaluation. Following (Wu et al. 2016; Vaswani et\nal. 2017), the eventually output is picked up by\nargmaxy′∈Clog P(y′|x)/|y′|α, where x denotes the source\nsentence, Crepresents the candidates carried out by beam\nsearch, |y′|denotes the number of words in y′and αis the\nlength penalty tuned by the validation set. For IWSLT re-\nlated tasks, we use beam search with beam width 6 and\nα = 1 .1 to generate candidates. For WMT related tasks,\nthe beam size is changed to 4 and the α is chosen as 0.6\nfor a fair comparison with (Vaswani et al. 2017). We use the\nBLEU scores as the evaluation metrics, which is the geo-\nmetric mean of four n-gram precisions between the trans-\nlation results and reference sentences, n = 1,2,3,4. To be\nmore speciﬁc, (1) For IWSLT De→En translation and WMT\nEn→De translations, we calculate the tokenized BLEU by\nmulti-bleu.perl2 for fair comparisons with previous\nmethods. (2) For other IWSLT translation tasks, we use\nsacreBLEU3. (3) For En→Ro, we use detokenized BLEU\nfor fair comparison with previous methods4.\n4.2 Results on IWSLT Related Tasks\nThe experimental results of IWSLT 2014 translation tasks\nare shown in Table 1. The three rows in Table 1 represent the\nbasic Transformer algorithm, our proposed algorithm and\nthe improvements brought by our algorithm. We also list the\nIWSLT 2014 De→En results reported by previous literature.\nThe tied transformer signiﬁcantly outperforms the stan-\ndard transformer. The improvements are two-folded: ﬁrst,\n2https://github.com/moses-smt/mosesdecoder/blob/master/\nscripts/generic/multi-bleu.perl\n3https://github.com/awslabs/sockeye/tree/master/contrib/\nsacrebleu\n4https://github.com/moses-smt/mosesdecoder/blob/master/\nscripts/tokenizer/detokenizer.perl\nDe→En Es →En Ro →En\nTransformer 33.27 38.67 29.64\nTied Transformer 35.10 40.51 30.99\nImprovements 1.83 1.84 1.35\nExisting Results on IWSLT De→En\nGRU + Dual Transfer Learning (Wang et al. 2018)32.35\nCNN + Reinforcement learning (Edunov et al. 2018)32.85\nLSTM + Variational Attention (Deng et al. 2018)33.10\nTransformer + Model-level dual (Xia et al. 2018)34.71\nTable 1: Results of IWSLT 2014 {De, Es, Ro}-to-En trans-\nlation tasks.\nwe could see Transformer outperforms non-transformer sys-\ntems, which is helpful to get a great score; second, on top of\nsuch a strong baseline, we are still capable to further im-\nprove the performances, which demonstrate the effective-\nness of our method. On the three tasks De→En, Es→En and\nRo→En, our proposed method could outperform the Trans-\nformer baseline by 1.83, 1.84 and 1.35 points, which indeed\nveriﬁes our motivation introduced in Section 1 that such a\nframework is helpful to improve the translations within a\nsame language family. As far as we could survey, we achieve\nthe best result on IWSLT 2014 De →En, whose previously\nbest result in 33.81 provided by (Elbayad et al. 2018).\nEn→De En →Es En →Ro\nTransformer 27.72 37.51 23.80\nTied Transformer 29.07 38.81 24.31\nImprovements 1.35 1.30 0.51\nTable 2: Results of IWSLT 2014 En-to-{De, Es, Ro}trans-\nlation tasks.\nWe also check the ability of translating from English to\nother languages. The results are shown in Table 2. Again,\ntying the weights of encoder and decoder outperforms the\nbaseline. The results in Table 1 and Table 2 demonstrate that\nour proposed method works pretty good on the translation\ntasks within the same language family again.\nThen, we compare our proposed tied transformer with\ntwo well-known baselines, which are both based on 8-block\ntransformers:\n1. Cross-tasking sharing (Johnson et al. 2017; Ha, Niehues,\nand Waibel 2016), brieﬂy denoted as CTS, which means\nto use one encoder-decoder based network to solve two\ntasks together. In this framework, the encoder is used to\nencode two languages and the decoder is used to decode\ntwo languages.\n2. Dual supervised learning (Xia et al. 2017a), brieﬂy\ndenoted as DSL, in which the training objective\nof De →En and En →De are jointly constrained by\nP(x)P(y|x; θxy) = P(y)P(x|y; θyx), where x, y, θxy,\nθyx represent a German sentence, an English sentence,\nthe parameters of De →En translation model and the pa-\nrameters of En →De translation model respectively. We\n5469\nlist it as a baseline here considering we report the results\nof two dual tasks and they could be considered together.\nThe results are shown in Table 3. We have several observa-\ntions: (1) Our method outperforms both the standard base-\nline and DSL, which shows that parameter sharing is more\npowerful in these two tasks. (2) Compared with the cross-\ntask sharing, our method outperforms this baseline by 0.77\nand 0.74 point, which shows that sharing parameter like ours\nis more useful than the others. Besides, it demonstrates that\nparameter sharing is indeed a useful technique in NMT for\nboth cross-task sharing and our proposed sharing scheme,\nwhich can be seen as a kind of intra-task sharing, especially\nwhen the training data is limited.\nBaseline DSL CTS Ours\nDe→En 33.27 33.60 34.33 35.10\nEn→De 27.72 27.88 28.33 29.07\nTable 3: Comparison with several other baselines on IWSLT\n2014 De↔En translation tasks.\nNext, we explore how the test performances of our pro-\nposed framework change with different number of blocks\nand different hidden dimensions.\nIn Figure 2(a), we ﬁx the hidden dimension as 256 and\nincrease the block number from6 to 14. We have the follow-\ning observations: (1) our proposed algorithm always outper-\nforms the baseline with different number of blocks, demon-\nstrating our algorithm has better generalization ability. (2)\nBoth our algorithm and the baseline achieve the best re-\nsults at the 8-block setting. As we increase more blocks, the\nperformances will drop, indicating that deeper models need\nstronger regularization.\n(a) Different block numbers\n (b) Different hidden dimensions\nFigure 2: IWSLT 2014 De→En BLEU with different model\nparameters.\nIn Figure 2(b), we ﬁx the block number as 8 and vary the\nhidden dimensions. For the baseline. the BLEU starts to drop\nwhen the hidden dimension is larger than 256. In compari-\nson, the BLEU of our algorithm starts to drop when the di-\nmension is larger than 384, which shows that our algorithm\nhas better generalization ability. Another observation is that\nour shared model cannot be too small: when the hidden di-\nmension is 128, the BLEU score of our algorithm is 0.82\npoint lower than the baseline, which shows that the model\ndoes not have enough capacity to handle the task. Finally, by\nsetting the dimension as 384, we improve the BLEU score\nfrom 35.10 to 35.52, setting a new record on this task.\n4.3 Results on WMT Related Tasks\nTo verify the performances of our algorithm on larger\ndatasets, we move to WMT 2014 En →De and WMT 2016\nEn→Ro translation tasks.\nIn Table 4, we show the BLEU scores of En→De transla-\ntion. In this case, we can see that a 6-block tied transformer\n(with BLEU 28.35) can almost catch up with the standard6-\nblock transformer (with BLEU 28.4) but using only around\nhalf of the parameters. This shows that even though the train-\ning corpus is large, our method is able to catch up with the\nbaselines.\n8-layer LSTM + RL (Wu et al. 2016) 24.60\n15-layer CNN network (Gehring et al. 2017) 25.16\n6-block transformer (Vaswani et al. 2017) 28.40\n6-block tied transformer 28.35\n8-block tied transformer 28.88\n12-block tied transformer 28.98\n8-block tied transformer + 8M unlabeled data 29.89\nTable 4: WMT 2014 En→De BLEU.\nThen we increase the number of blocks. When the net-\nwork has 8 blocks, the BLEU score is 28.88, 0.48 point\nahead of the baseline. When we increase it to 12 blocks, the\nBLEU score is 28.98, which achieves 0.58 point improve-\nment over the baseline. Note that our model with 12 blocks\nhas the approximate number of parameter with the baseline\nwith 6 blocks.\nWe also check the ability of our model with the sup-\nport of monolingual data. We use the back-translation tech-\nnique to leverage monolingual data (Sennrich, Haddow, and\nBirch 2016a). To achieve that, we ﬁrst use the 4.5M bilin-\ngual data to train an initial De →En translation model with\n32.12 BLEU score. Then, we choose 8M monolingual Ger-\nman data from newscrawl 20135. Next, we use the obtained\nDe→En model to translate the 8M sentences with beam-\nsize 4 and α 1.0. Finally, we merge the original bilingual\ndataset and the generated English-German pairs from the\nmonolingual data and train another En→De model based on\ntied transformer. In this way, we obtain 29.89 BLEU score,\nwhich shows that our proposed tied model can handle large\namount of training data.\nThe En →Ro translation results are summarized in Ta-\nble 5. We implement the standard transformer as baseline\nand get a 33.05 BLEU score. Our method with 6 blocks can\nachieve 34.67 BLEU, which is a state-of-the-art result. We\nalso tried the 8-block network but obtained a lower score.\nThis is because the En →Ro task does not have as much\ntraining data as En →De and thus could not apply too large\nmodels.\n5http://www.statmt.org/wmt14/\ntraining-monolingual-news-crawl/news.2013.de.shufﬂed.gz\n5470\n20-layer CNN network (Gehring et al. 2017) 30.02\n6-block transformer (Vaswani et al. 2017) 33.05\n6-block tied transformer 34.67\n8-block tied transformer 34.55\nTable 5: WMT 2016 En→Ro BLEU.\n4.4 Tied LSTM Models for NMT\nTo verify the generality of sharing the encoder and decoder,\nwe adapt our framework into the LSTM and obtained a tied\nLSTM model. We follow (Wu et al. 2016) to build a deep\nLSTM NMT model but sharing the parameters of the en-\ncoder and the decoder (excluding the attention model).\nWe work on IWSLT 2014 De →En translation to ver-\nify our idea. For the baseline, we choose a 4-layer single-\ndirection LSTM with vocabulary size 24k, word embed-\nding dimension 256 and hidden dimension 512. For the tied\nLSTM, we also ﬁx the word embedding dimension as 256\nand the number of layers as4. We set the hidden dimensions\nas 512 and 1024, corresponding to almost 50% and 100%\nparameters of the baseline LSTM. We share the source em-\nbeddings, target embeddings and the softmax matrix. We use\nAdadelta (Zeiler 2012) to optimize the network. The results\nare shown in Table 6.\nBaseline hidden 512 hidden 1024\n29.41 29 .57 30 .61\nTable 6: IWSLT 2014 De→En with tied LSTMs\nWe can see the baseline achieves 29.41 BLEU score. Af-\nter sharing the encoder and decoder, we obtain 0.16 gain.\nThe improvement is not very signiﬁcant due to the limited\nmodel capacity. When we increase the hidden dimension\nto 1024, we can achieve 30.61 BLEU score, which is 1.2\npoint improvement over the baseline. This shows that our\nproposed method can be generalized to more structures like\nLSTM. We leave how to apply our framework to more com-\nplex LSTMs as future work.\n5 Experiments of Dissimilar Languages\nIn Section 4, we have veriﬁed the effectiveness of our pro-\nposed method for translating between languages within the\nsame family. In this section, we analyze how our method\nperforms to translating between languages in different fam-\nilies.\nSettings. We choose IWSLT 2014 {Russian, Hebrew,\nChinese}-from/to-English translation tasks to verify our\nproposed algorithm, where there are 160k, 151k, 182ksen-\ntence pairs in each datasets. Russian, Hebrew and Chi-\nnese are brieﬂy denoted as “Ru, He, Zh” respectively.\nWe use IWSLT14.TED.tst2013 as the validation sets and\nIWSLT14.TED.tst2014 as test sets. Again, all sentences are\nsplit into wordpieces by (Wu et al. 2016). We use the same\nmodel structure and the evaluation metrics as that used in\nSection 4 for IWSLT 2014 tasks.\nResults. The experimental results are shown in Table 7. Our\nproposed method outperforms the Transformer baseline by\n1.16, 0.97 and 0.87 points on the three tasks.\nRu→En He →En Zh →En\nTransformer 17.89 31.96 16.45\nTied Transformer 19.05 32.93 17.32\nImprovements 1.16 0.97 0.87\nTable 7: Experimental results on IWSLT 2014{Ru, He, Zh}-\nto-English translation tasks.\nThe English to Russian, Hebrew and Chinese translation\nresults are shown in Table 8.We could still make improve-\nments on these tasks.\nEn→Ru En →He En →Zh\nTransformer 14.59 20.84 11.88\nTied Transformer 14.90 21.47 12.38\nImprovements 0.31 0.63 0.50\nTable 8: Experimental results on IWSLT 2014 English-to-\n{Ru, He, Zh}translation tasks.\nAs shown in Figure 3, an interesting observation is that,\nno matter for similar languages or different languages, over-\nall, the improvement of X →En translation quality brought\nby our method is larger than the corresponding En →X,\nwhere X is the language we used in the IWSLT datasets.\nWe conjecture that English is an easier language to decode,\nwhich results in the aforementioned observation.\nFigure 3: Improvements of our method compared with the\nTransformer baseline.\nWe also ﬁnd that the improvements of translating within\nsame language family are larger than that within differ-\nent language families, no matter for X-to-English transla-\ntion or English-to-X translation. Our explanation is that the\nlanguages within the same family are easily mapped into\nthe same language space, considering their vocabulary are\nshared. Working on the same semantic space will result in\nbetter improvements than those in different spaces.\n6 Experiments on Unsupervised NMT\nUnsupervised NMT targets at making translation between\ntwo languages possibly without bilingual data. Several lit-\nerature leverages parameter sharing and dual learning to\n5471\nsolve this problem (Lample, Denoyer, and Ranzato 2018;\nLample et al. 2018), which motivates us to solve unsuper-\nvised NMT with a tied transformer.\n6.1 Model Adaption\nBackground We brieﬂy introduce (Lample et al. 2018),\na recent state-of-the-art unsupervised NMT algorithm and\nthen adapt it with our tied transformer. Like the standard\nNMT system, an unsupervised NMT model also consists of\nan encoder, a source-to-target attention module, and a de-\ncoder as well as a shared embedding for similar languages\nlike German and English. The source sentences and target\nsentences are mapped into a same vocabulary using BPE\ntechniques. The shared embedding is pretrained with fast-\nText (Bojanowski et al. 2017) to get good initial values. The\nproposed model will handle both source-to-target translation\nand target-to-source translation like Johnson et al. (2017).\nThe reason to use one model for two translation tasks is that\na single model could better map two different languages into\nthe same representation space, which is easier for decoding.\nThe training loss of an unsupervised NMT model usually\nconsists of two parts, a language model loss and a back-\ntranslation loss. LetPX→Y denote the translation from space\nXto space Y, and so for the other similar notations.\n(1) Language model loss, implemented by a denoising au-\ntoencoder. Mathematically,\nLlm =Ex∼X[−log PX→X(x|σ(x))]+\nEy∼Y[−log PY→Y(y|σ(y))], (6)\nwhere σ(·) is a noise model with randomly dropping several\nwords, swapping words, etc.\n(2) Back translation loss, implemented by back-translating\nthe monolingual data and feeding into the reversed models.\nMathematically,\nLback =Ex∼X[−log PY→X(x|ˆy(x))]+\nEy∼Y[−log PX→Y(y|ˆx(y))], (7)\nin which ˆy(x) = arg max u∈YPX→Y(u|x) and ˆx(y) =\narg maxw∈XPY→X(w|y). Note that the four P···’s are im-\nplemented in a single encoder-decoder based model, where\neach translation task has a different “task embedding”, i.e.,\na learnable vector indicating the translating directions.\nAdaption To achieve unsupervised NMT, the aforemen-\ntioned four models P···’s in Eqn. (6) and Eqn. (7) are imple-\nmented in a single tied transformer. The model architecture\nis the same as that introduced in Section 3, where each task\nhas a task embedding as that used in Lample et al. (2018).\nWe also use Llm + Lback as the training loss.\n6.2 Settings\nWe work on German↔English translation. Following Lam-\nple et al. (2018), we collect 50M WMT monolingual data\nfrom newscrawl 2014 to newscrawl 2017. The BPE (Sen-\nnrich, Haddow, and Birch 2016b) with60kmerge operations\nis applied to pre-process the data. Newstest2014 and new-\nstest2016 German↔English translation datasets are used as\nthe validation set and test set respectively.\nWe follow the transformer base conﬁguration (Vaswani\net al. 2017) to set the hyper-parameter of our model. The\nword embedding dimension, hidden state dimension, non-\nlinear layer dimension and the number of heads of the multi-\nhead attention are 512, 512 and 2048 and 8 respectively. We\ntry different settings:4 blocks and 8 blocks for the tied trans-\nformer. In the training phase, we ﬁx the dropout rate as 0.2,\napply Adam optimizer with learning rate0.0002 to optimize\nthe network. The model is implemented in TensorFlow and\ntrained on 8 M40 GPUs. In the inference phase, we use beam\nsearch with beam width 4 and set length penalty αas 0.6.\n6.3 Results\nThe results of unsupervised NMT are shown in Table 9. To\nspeed up the training, we ﬁrst train an initial unsupervsied\nNMT model, back translate the monolingual data, train a\nwarm-start tied transformer and then keep tuning the ob-\ntained model by online sampling. We can see that our pro-\nposed model performs best among all NMT based transla-\ntion systems.\nBLEU\nLSTM model (Lample, Denoyer, and Ranzato 2018) 13.3\n4-block Transformer(Lample et al. 2018) 21.0\nOur method with4 blocks 21.61\nOur method with8 blocks 22.05\nTable 9: Experimental results of unsupervised neural ma-\nchine translation on WMT 2016 De→En.\nSpeciﬁcally, when using the 4-block model, which ac-\ncounts only half of the parameters used in (Lample et al.\n2018), on De→En translation, we can achieve 21.61 BLEU\nscore compared to the 21.0 obtained by the baseline algo-\nrithm. When we increase the number of blocks from 4 to 8,\nthe BLEU score increases to 22.05. These results demon-\nstrate the effectiveness of our method.\nAs for En →De translation, we do not observe signiﬁ-\ncant improvement. Specially, we improve the En→De from\n17.2 (Lample et al. 2018) to 17.31/17.42 on the 4/8-block\nmodel. Such a phenomenon is consistent with the results\nshown in Figure 3. We will leave a more detailed study in\nthe future work.\n7 Conclusion and Future Work\nIn this paper, we studied a new parameter sharing mecha-\nnism in NMT by tying the encoder and decoder and pro-\nposed the tied transformer. Experimental results on several\ndifferent tasks demonstrated the effectiveness of our propose\nmodel. For future work, there are several interesting direc-\ntions to explore in the future: First, we studied hard param-\neter sharing in this work. How to constrain the parameters\nin a soft way like regularizing the distances of parameters is\nan interesting topic. Second, how to apply this idea to multi-\nlingual translation is worthy of investigation. Third, we will\nstudy whether this idea works for other applications such\ntext summarization and question answering.\n5472\nReferences\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv preprint arXiv:1607.06450.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural ma-\nchine translation by jointly learning to align and translate. In\nICLR.\nBojanowski, P.; Grave, E.; Joulin, A.; and Mikolov, T. 2017.\nEnriching word vectors with subword information. TACL\n135–146.\nDeng, Y .; Kim, Y .; Chiu, J.; Guo, D.; and Rush, A. M. 2018.\nLatent alignment and variational attention. arXiv preprint\narXiv:1807.03756.\nDong, D.; Wu, H.; He, W.; Yu, D.; and Wang, H. 2015.\nMulti-task learning for multiple language translation. In\nACL, volume 1, 1723–1732.\nEdunov, S.; Ott, M.; Auli, M.; Grangier, D.; and Ranzato, M.\n2018. Classical structured prediction losses for sequence to\nsequence learning. In NAACL.\nElbayad, M.; Besacier, L.; and Verbeek, J. 2018. Pervasive\nattention: 2d convolutional neural networks for sequence-to-\nsequence prediction. In The SIGNLL Conference on Com-\nputational Natural Language Learning.\nFirat, O.; Sankaran, B.; Al-Onaizan, Y .; Vural, F. T. Y .; and\nCho, K. 2016. Zero-resource translation with multi-lingual\nneural machine translation. EMNLP.\nFirat, O.; Cho, K.; and Bengio, Y . 2016. Multi-way, mul-\ntilingual neural machine translation with a shared attention\nmechanism. arXiv preprint arXiv:1601.01073.\nGehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin,\nY . N. 2017. Convolutional sequence to sequence learning.\nIn ICML.\nGoodfellow, I.; Bengio, Y .; and Courville, A. 2016. Deep\nLearning. MIT Press.\nHa, T.-L.; Niehues, J.; and Waibel, A. 2016. Toward mul-\ntilingual neural machine translation with universal encoder\nand decoder. arXiv preprint arXiv:1611.04798.\nHassan, H.; Aue, A.; Chen, C.; Chowdhary, V .; Clark, J.;\nFedermann, C.; Huang, X.; Junczys-Dowmunt, M.; Lewis,\nW.; Li, M.; et al. 2018. Achieving human parity on au-\ntomatic chinese to english news translation. arXiv preprint\narXiv:1803.05567.\nHe, D.; Xia, Y .; Qin, T.; Wang, L.; Yu, N.; Liu, T.; and Ma,\nW.-Y . 2016. Dual learning for machine translation. In Ad-\nvances in Neural Information Processing Systems, 820–828.\nJohnson, M.; Schuster, M.; Le, Q. V .; Krikun, M.; Wu, Y .;\nChen, Z.; Thorat, N.; Vi ´egas, F.; Wattenberg, M.; Corrado,\nG.; et al. 2017. Google’s multilingual neural machine trans-\nlation system: enabling zero-shot translation. TACL.\nLample, G.; Ott, M.; Conneau, A.; Denoyer, L.; and Ran-\nzato, M. 2018. Phrase-based & neural unsupervised ma-\nchine translation. In EMNLP.\nLample, G.; Denoyer, L.; and Ranzato, M. 2018. Unsuper-\nvised machine translation using monolingual corpora only.\nIn ICLR.\nLuong, M.-T.; Le, Q. V .; Sutskever, I.; Vinyals, O.; and\nKaiser, L. 2016. Multi-task sequence to sequence learning.\nICLR.\nLuong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effec-\ntive approaches to attention-based neural machine transla-\ntion. In EMNLP.\nMohri, M.; Rostamizadeh, A.; and Talwalkar, A. 2012.\nFoundations of machine learning. MIT press.\nPan, S. J., and Yang, Q. 2010. A survey on transfer learn-\ning. IEEE Transactions on knowledge and data engineering\n22(10):1345–1359.\nPham, H.; Guan, M. Y .; Zoph, B.; Le, Q. V .; and Dean,\nJ. 2018. Efﬁcient neural architecture search via parameter\nsharing. arXiv preprint arXiv:1802.03268.\nRuder, S. 2017. An overview of multi-task learning in deep\nneural networks. arXiv preprint arXiv:1706.05098.\nSennrich, R.; Haddow, B.; and Birch, A. 2016a. Improving\nneural machine translation models with monolingual data.\nIn ACL.\nSennrich, R.; Haddow, B.; and Birch, A. 2016b. Neural ma-\nchine translation of rare words with subword units. In Pro-\nceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , vol-\nume 1, 1715–1725.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems, 6000–6010.\nWang, Y .; Xia, Y .; Zhao, L.; Bian, J.; Qin, T.; Liu, G.; and\nLiu, T. 2018. Dual transfer learning for neural machine\ntranslation with marginal distribution regularization. InPro-\nceedings of the Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence.\nWu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Google’s neural machine translation system:\nBridging the gap between human and machine translation.\nCoRR abs/1609.08144.\nXia, Y .; Qin, T.; Chen, W.; Bian, J.; Yu, N.; and Liu, T.-Y .\n2017a. Dual supervised learning. In ICML2017.\nXia, Y .; Tian, F.; Qin, T.; Yu, N.; and Liu, T.-Y . 2017b. Se-\nquence generation with target attention. In ECMLPKDD,\n816–831. Springer.\nXia, Y .; Tan, X.; Tian, F.; Qin, T.; Yu, N.; and Liu, T.-Y .\n2018. Model-level dual learning. In International Confer-\nence on Machine Learning, 5379–5388.\nZeiler, M. D. 2012. Adadelta: an adaptive learning rate\nmethod. arXiv preprint arXiv:1212.5701.\nZhang, Y ., and Yang, Q. 2017. A survey on multi-task learn-\ning. arXiv preprint arXiv:1707.08114.\nZoph, B., and Knight, K. 2016. Multi-source neural transla-\ntion. NAACL.\n5473",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8966119289398193
    },
    {
      "name": "Computer science",
      "score": 0.843602180480957
    },
    {
      "name": "Transformer",
      "score": 0.840400218963623
    },
    {
      "name": "BLEU",
      "score": 0.8023170232772827
    },
    {
      "name": "Encoder",
      "score": 0.7044409513473511
    },
    {
      "name": "German",
      "score": 0.6955660581588745
    },
    {
      "name": "Natural language processing",
      "score": 0.6210551261901855
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6106137633323669
    },
    {
      "name": "Translation (biology)",
      "score": 0.5073887705802917
    },
    {
      "name": "Language model",
      "score": 0.42131325602531433
    },
    {
      "name": "Evaluation of machine translation",
      "score": 0.4174365997314453
    },
    {
      "name": "Example-based machine translation",
      "score": 0.40235450863838196
    },
    {
      "name": "Speech recognition",
      "score": 0.3834809958934784
    },
    {
      "name": "Machine translation software usability",
      "score": 0.14884382486343384
    },
    {
      "name": "Linguistics",
      "score": 0.11514908075332642
    },
    {
      "name": "Voltage",
      "score": 0.09394261240959167
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 65
}