{
  "title": "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer",
  "url": "https://openalex.org/W4206861281",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2902235306",
      "name": "Buyu Li",
      "affiliations": [
        "EA Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2143124792",
      "name": "Yongchi Zhao",
      "affiliations": [
        "EA Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4209278308",
      "name": "Shi Zhelun",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2101699347",
      "name": "Lu Sheng",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2902235306",
      "name": "Buyu Li",
      "affiliations": [
        "Hue University"
      ]
    },
    {
      "id": "https://openalex.org/A2143124792",
      "name": "Yongchi Zhao",
      "affiliations": [
        "Hue University"
      ]
    },
    {
      "id": "https://openalex.org/A4209278308",
      "name": "Shi Zhelun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101699347",
      "name": "Lu Sheng",
      "affiliations": [
        "Beihang University",
        "Software (Spain)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2136625416",
    "https://openalex.org/W6715350866",
    "https://openalex.org/W6663412969",
    "https://openalex.org/W377899342",
    "https://openalex.org/W2892040300",
    "https://openalex.org/W2057203440",
    "https://openalex.org/W2121378366",
    "https://openalex.org/W6770208262",
    "https://openalex.org/W3189935790",
    "https://openalex.org/W6854993605",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6760869523",
    "https://openalex.org/W6677316912",
    "https://openalex.org/W3010931949",
    "https://openalex.org/W2315431645",
    "https://openalex.org/W6631518110",
    "https://openalex.org/W2998680134",
    "https://openalex.org/W2983796203",
    "https://openalex.org/W3204221554",
    "https://openalex.org/W15066456",
    "https://openalex.org/W1527575096",
    "https://openalex.org/W2982573856",
    "https://openalex.org/W2999276620",
    "https://openalex.org/W4232236500",
    "https://openalex.org/W2412428697",
    "https://openalex.org/W3005171070",
    "https://openalex.org/W4385489997",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W3068510429",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2052872069",
    "https://openalex.org/W2899129842",
    "https://openalex.org/W2928521819"
  ],
  "abstract": "Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics. Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements. In this paper, we reformulate it by a two-stage process, i.e., a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements. We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively. Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture. This dataset also encodes dances as key poses and parametric motion curves apart from pose sequences, thus benefiting the training of our DanceFormer. Extensive experiments demonstrate that the proposed method, even trained by existing datasets, can generate fluent, performative, and music-matched 3D dances that surpass previous works quantitatively and qualitatively. Moreover, the proposed DanceFormer, together with the PhantomDance dataset, are seamlessly compatible with industrial animation software, thus facilitating the adaptation for various downstream applications.",
  "full_text": "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion\nTransformer\nBuyu Li,1 Yongchi Zhao,1 Zhelun Shi,2 Lu Sheng2*\n1 Huiye Technology, 2 College of Software, Beihang University\n{libuyu, zhaoyongchi}@huiye.tech, {18373044, lsheng}@buaa.edu.cn\nAbstract\nGenerating 3D dances from music is an emerged research\ntask that beneﬁts a lot of applications in vision and graphics.\nPrevious works treat this task as sequence generation, how-\never, it is challenging to render a music-aligned long-term se-\nquence with high kinematic complexity and coherent move-\nments. In this paper, we reformulate it by a two-stage process,\ni.e., a key pose generation and then an in-between parametric\nmotion curve prediction, where the key poses are easier to be\nsynchronized with the music beats and the parametric curves\ncan be efﬁciently regressed to render ﬂuent rhythm-aligned\nmovements. We named the proposed method as Dance-\nFormer, which includes two cascading kinematics-enhanced\ntransformer-guided networks (called DanTrans) that tackle\neach stage, respectively. Furthermore, we propose a large-\nscale music conditioned 3D dance dataset, called Phantom-\nDance, that is accurately labeled by experienced animators\nrather than reconstruction or motion capture. This dataset\nalso encodes dances as key poses and parametric motion\ncurves apart from pose sequences, thus beneﬁting the train-\ning of our DanceFormer. Extensive experiments demonstrate\nthat the proposed method, even trained by existing datasets,\ncan generate ﬂuent, performative, and music-matched 3D\ndances that surpass previous works quantitatively and quali-\ntatively. Moreover, the proposed DanceFormer, together with\nthe PhantomDance dataset, are seamlessly compatible with\nindustrial animation software, thus facilitating the adaptation\nfor various downstream applications.\nIntroduction\nAutomatically generating music conditioned 3D dances is an\nappealing but challenging task that emerged in the research\ncommunity of vision and graphics, which can signiﬁcantly\nbeneﬁt various downstream applications in AR/VR, games,\nﬁlms, and even the social networks. Beyond human actions\nlike walking, jumping, and sitting with atomic functionali-\nties, dancing is a type of artistic performance that focuses\nmore on its choreography, i.e., sequential steps and move-\nments with high kinematic complexity that are synchronized\nwith the beats and rhythms of the music. It is challenging\nfor humans with professional training to generate expressive\n*Lu Sheng is the corresponding author.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nchoreographies, no matter how hard a machine is to generate\nvisually plausible dances that accompanied by the music.\nMost of the prior works (Lee et al. 2019; Li et al. 2021;\nLee, Kim, and Lee 2018; Tang, Jia, and Mao 2018; Li et al.\n2020; Zhuang et al. 2020; Alemi, Franc ¸oise, and Pasquier\n2017) formulated the music conditioned 3D dance genera-\ntion as a sequence generation problem, where each frame\nin the sequence describes the human pose by the joint-level\nrotations and translations. However, rendering a long-term\npose sequence with high kinematic complexity and coherent\nmovements is still an open question.\nIn this paper, we would like to exploit a popular anima-\ntion strategy (Lasseter 1987; Thomas, Johnston, and Thomas\n1995) in the ﬁeld of computer graphics, where the motion\nof characters is efﬁciently rendered by interpolating poses\nin keyframes through parametric curves. This coarse-to-ﬁne\nstrategy is especially beneﬁcial in our task since it both en-\nsures kinematic complexity and motion coherency in the\ngenerated dances, where the poses in the keyframes can be\ngenerated and synchronized with the music beats due to their\nco-occurrence (Lee et al. 2019; Li et al. 2021), and the para-\nmetric curves can be efﬁciently regressed so as to produce\ndiverse motion patterns that are consistent with the rhythms\nof the music. It also allows user-controlled temporal reso-\nlutions, and generates smooth sequences with fewer tempo-\nral ﬂickers (Williams 2012). Moreover, formulating dances\nas a sequence of key poses and parametric motion curves\nwould be seamlessly compatible with industrial animation\npipelines. Therefore, we would like to tackle the music con-\nditioned 3D dance generation from this new perspective, and\ndecompose this task into two easier sub-tasks, namely, the\nkey pose generation and the subsequent parametric motion\ncurve regression.\nTo this end, we propose DanceFormer, a two-stage music\nconditioned 3D dance generation framework that consists of\ntwo cascaded Transformer-like (Vaswani et al. 2017) net-\nworks (called DanTrans), where the ﬁrst one is to generate\nkey poses and the second one is to regress the parameters\nof the motion curves between adjacent key poses. At ﬁrst,\nwe employ an off-the-shelf beat track algorithm (Ellis 2007)\nto detect beats from the input music (e.g., around 30 sec-\nonds). The DanTrans for the key pose generation takes a se-\nquence of music spectrum features centered at each beat as\nthe input, and then generates a sequence of key poses that\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n1272\nsynchronizes with the sequence of the music beats. And the\nDanTrans for the parametric motion curve regression em-\nploys a different sequence of music spectrum features that\nare extracted between adjacent beats and feeds the generated\nkey poses as well to regress the sequence of the parameters\nfor each motion curve. In our implementation, we employ\nthe Kochanek-Bartels splines (Kochanek and Bartels 1984)\nto model motion curves, which beneﬁts rendering various\nmovements from a small set of control parameters.\nEach DanTrans contains a Wave Encoder as a stack of\ntransformer encoders to extract audio features, and a Mo-\ntion Decoderconstructed by a stack of transformer decoders\nto auto-regressively predict key poses or parameters of the\nmotion curves. To be speciﬁc, the Motion Decoder contains\na new Kinematic Propagation Module (KPM) and a Struc-\ntured Multi-head Attention Module (SMAM) to enhance the\nspatial correlation for the dance generation process based on\nthe Kinematic topological relations of the human body. This\nmotion decoder increases the physical signiﬁcance and thus\nalleviates unreasonable predictions seldom appeared in real\ndances. These DanTrans networks are adversarially trained\nin accompany with `2 reconstruction losses.\nIn order to train the proposed DanceFormer, we also\npropose a new PhantomDance dataset that is produced by\nprofessional animators rather than reconstruction from 2D\nvideos (Lee et al. 2019; Li et al. 2021; Lee, Kim, and Lee\n2018) or by motion capture (Zhuang et al. 2020; Tang, Jia,\nand Mao 2018). The reconstructed data can not ensure the\naccuracy due to the limitation of recent 3D reconstruction\nalgorithms. The motion capture methods are more accurate\nbut still suffer from noise, such as limb ﬂickers and joint\nmismatches. In contrast, our PhantomDance dataset is pro-\nduced by a team of experienced animators instructed by\nprofessional dancers. The animated dances are encoded by\nposes from keyframes and motion curve parameters, which\nthus can produce more ﬂuent and expressive human pose se-\nquences that match the input music.\nExtensive experiments are conducted on PhantomDance\nand AIST++ (Li et al. 2021), demonstrating that Dance-\nFormer attains state-of-the-art results and signiﬁcantly sur-\npasses other works both qualitatively and quantitatively.\nAbove all, the contributions are summarized as follows:\n(1) A new perspective to model the music conditioned 3D\ndance generation as key pose generation and parametric mo-\ntion curve regression, which follows the animation princi-\nples and simultaneously ensures kinematic complexity and\nmotion coherency.\n(2) A novel framework named DanceFormer that consists\nof two Transformer-like networks called DanTrans, where\nthe ﬁrst network generates key poses and the second one\nregresses the parameters of the motion curves. More impor-\ntantly, the DanTrans networks calculate attentions in adap-\ntive temporal ranges and explicitly enhance the kinematic\ncorrelation among the outputs.\n(3) The PhantomDance dataset, as the ﬁrst dance dataset\ncrafted by professional animators instead of 3D reconstruc-\ntion or motion capture, provides more smooth and expres-\nsive dances that are synchronized with music, and directly\ncompatible with industrial animation pipelines.\nRelated Works\n3D Dance Synthesis 3D dance synthesis has been ad-\ndressed in many ways. The traditional methods were based\non retrieval from the dance dataset (Takano, Yamane, and\nNakamura 2010; Chao et al. 2004). Recently, deep learn-\ning models have been widely employed in this task, attain-\ning visually pleasing results, such as those using convolu-\ntional neural networks (Lee, Kim, and Lee 2018; Zhuang\net al. 2020), recurrent neural networks (Tang, Jia, and Mao\n2018), or using variational auto-encoders (Lee et al. 2019) to\nenhance the diversity of the generated dances. CSGN (Yan\net al. 2019) generates the dance sequences by enhancing\nthe skeleton-level relations with a graph neural network\n(GNN), but the proposed Kinematic Propagation Module\n(KPM) used in our DanceFormer adopts forward and inverse\nkinematic message passing to better involve the kinematic\ncorrelations. ChoreoMaster (Chen et al. 2021) introduces\na choreography-oriented choreomusical embedding frame-\nwork, and uses the embedding to retrieve dance snippets in\nthe motion graph for the input music. Because it doesn’t\ngenerate new dances, we don’t compare our method with\nit. As the Transformer (Vaswani et al. 2017) achieves great\nsuccess in sequence-to-sequence generation tasks in NLPs,\nsome of the most recent works on the dance generation also\nbring it into use. Li et al. (2020) presented a two-stream\nmotion transformer generative model. AI Choreographer (Li\net al. 2021) presents a cross-model transformer with future-\nN supervision for auto-regressive motion prediction. These\nworks directly generate the sequence of human poses, while\nthe proposed DanceFormer generates a series of key poses\nand accompanied parameters of the motion curves, which\nensures necessary kinematic complexity, motion coherency,\nadaptive temporal resolutions in the generated dances, and\ncompatible with the industrial animation software, such as\nMaya, Unity and etc.\n3D Dance Dataset The widely used motion datasets, such\nas Human3.6M (Ionescu et al. 2013) and AMASS (Mah-\nmood et al. 2019), collected common actions like walking,\nrunning, jumping and sitting. However, these datasets are\nhard to adapt for the dance generation task due to the huge\ndistribution gap between daily motions and the dance move-\nments, as well as the absence of carefully aligned music-\ndance pairs. In fact, high-quality 3D dance movements with\nsynchronized music are extremely hard to collect. Many ex-\nisting datasets are limited by the quantity or quality of data.\nSome of them just use sequences of 2D keypoints to rep-\nresent dances (Lee et al. 2019; Lee, Kim, and Lee 2018).\nAIST++ (Li et al. 2021), as the largest dataset up to date,\npresented a 5-hours 3D dance set. But it gathered dances by\nreconstructing 3D poses from 2D multi-view videos, thus\nthe accuracy of pose parameters may not be guaranteed.\nThe other works use motion capture to build dataset (Alemi,\nFranc ¸oise, and Pasquier 2017; Tang, Jia, and Mao 2018;\nZhuang et al. 2020), in which the pose reliability is better but\nmisalignment between the dance-music pair is inevitable.\nOn the contrary, the proposed PhantomDance is the largest\npublic 3D dance dataset up to date and its data quality is\nmuch higher than existing datasets thanks to careful labels\n1273\nfrom experienced animators with the help of professional\ndancers. The stored key poses and parametric motion curves\nalso beneﬁt animations by industrial software.\nThe PhantomDance Dataset\nData Collection\nWe collect260 popular dance videos from more than13 gen-\nres from over 100 different subjects (dancers) on YouTube,\nNicoNico and Bilibili, which have 9:6 hours in total. Then\na team of experienced animators produced the 3D dance an-\nimations, as sequences of key poses and parametric motion\ncurves. An expert dancer provided professional instructions\nto teach the animators, thus improving the holistic expres-\nsiveness and detail richness of the animated dances, as well\nas the synchronization with beats and rhythms of the accom-\npanied music. This dataset is named PhantomDance, and we\nwill make it publicly available to facilitate future research.\nData Formatting\nIn our PhantomDance dataset, the dances can be ani-\nmated in industrial animation software, such as Maya (De-\nrakhshani 2012), on a standard SMPL (Loper et al. 2015)\ncharacter model with 24 body joints. We followed Hu-\nman3.6M (Ionescu et al. 2013) and provided a subject-\ninvariant 3D human skeleton representation for all the 260\nmotion ﬁles, with uniﬁed limb lengths. The motion is param-\neterized as the parametric curves of the root (the hip joint)\nposition and the rotations of the 24 skeleton joints. For each\nsequence, we take the root position on the ﬁrst frame as the\norigin of the 3D coordinate. And the rotations are expressed\nwith quaternions. That is, there are 3 + 4×24 = 99curves\nin each motion ﬁle. Each parametric motion curve is rep-\nresented as the Kochanek-Bartels Spline interpolated from\nlabeled key poses synchronized with extracted music beats,\nwhere the curve segment between adjacent two key poses\ncan be controlled by a ﬁxed set of parameters.\nWe use the parametric motion curves to represent dance\nmovements because (1) its formulation is analytical and\ncan be densely sampled with various temporal resolutions,\nwhich is very useful for real-time rendering; (2) its shape is\nin nature continuous and thus consistent with the aesthetic\nprinciples when evaluating dances.\nIn Comparison to Previous Datasets\nTable 1 shows the comparison between our PhantomDance\ndataset with the other public music-conditioned 3D Dance\nDatasets. The collected dances in PhantomDance mainly\ncover 13 genres, ranging from Urban Dance to Chinese Clas-\nsical Dance, with extra genres from animes or idol groups\nthat are hard to classify. The beats per minute (BPM) of\nthese dances range from 75 to 178. Apart from labeling the\ndetailed 3D positions and rotations of each pose, our dataset\nalso provides the ground-truth music beats and parameters\nof each motion curve, thus directly facilitating animation or\nmodel learning that is compatible with industrial software.\nBesides the aspect of dataset scale, the PhantomDance\ndataset also has high quality. Motion data in the other\nDataset Pos/Rot Genres Music Seconds\nGrooveNet X/\u0002 1 3 1380\nEA-MUD X/\u0002 6 23 1849\nDance w/ Melody X/\u0002 4 61 5640\nAIST++ X/X 10 60 18694\nPhantomDance X/X 13+ 260 34667\nTable 1: Our PhantomDance dataset v.s. the other datasets,\nnamely GrooveNet (Alemi, Franc ¸oise, and Pasquier 2017),\nEA-MUD (Sun et al. 2020), Dance with Melody (Tang, Jia,\nand Mao 2018) and AIST++ (Li et al. 2021).\ndatasets are collected from optical motion capture sys-\ntem (Alemi, Franc ¸oise, and Pasquier 2017; Tang, Jia, and\nMao 2018; Sun et al. 2020) or 3D reconstruction from multi-\nview videos (Li et al. 2021). Motion capture systems can not\ntotally avoid data noise like limb ﬂicker and joint mismatch.\nAnd video-based 3D reconstruction suffers from more seri-\nous noise like foot slippery and limb twists due to the limita-\ntion of reconstruction algorithms. Thus professional anima-\ntors are needed for data correction. In contrast, motion data\nin PhantomDance are produced by expert animators from\nscratch with the guidance of a professional dancer. And we\nhave a strict quality checking process so that it costs about\n15 months to ﬁnish the dance labeling. More details of our\nPhantomDance dataset and the qualitative comparison with\nthe other datasets can be seen on our project page1.\nDanceFormer\nAs illustrated in Figure 1, we propose the DanceFormer, a\ntwo-stage music conditioned 3D dance generation frame-\nwork that consists of two cascaded transformer-like net-\nworks. Each network is named DanTrans, where the ﬁrst one\ngenerates key poses, and the second one predicts the param-\neters of the motion curves between adjacent key poses. Each\nDanTrans network contains a wave encoder that is similar\nto the standard transformer encoder (Vaswani et al. 2017),\nand a motion decoder with a Kinematic Propagation Mod-\nule (KPM) and a Structured Multi-head Attention Module\n(SMAM), which enhances the kinematic correlation for the\ndance generation process based on the Kinematic topologi-\ncal relations of the human body. Adversarial learning is ap-\nplied during the training in both stages.\nStage 1: Key Pose Generation\nFollowing the rhythm (beats) is a basic principle in dance\ntheory (Goodridge 1999), and previous studies also show\nthat music beats and kinematic inﬂections have strong con-\nsistency in time (Lee et al. 2019; Li et al. 2021). Thus we\ndeﬁne the key poses directly as the poses on the beats, which\nare represented as the position and rotation (in quaternion)\nof each skeleton joint, with the dimension of (3 + 4)×\n24 = 168. We ﬁrst leverage the dynamic programming algo-\nrithm (Ellis 2007) to track the time of each beat in the music.\nOur target in stage 1 is to generate a sequence of key poses\nto match these extracted beats.\n1https://huiye-tech.github.io/post/danceformer/\n1274\nx N\noutputs\nKPM\nAtt.\nKPM\nAtt.\noutputs\nencoder\nfeature\n+ pos encode\nWave\nEncoder\nMotion\nDecoder\nDanTrans\ninputs outputs\n(shifted right)\noutputsDiscriminator\nAdv. Loss Pred. Loss\nmotion\ncurves\ndance animation\nDanTrans\n2\nDanTrans\n1\nmusic\nbeats\nkey \nposes\nembedding\nFigure 1: Overview of the workﬂow of DanceFormer. The networks in stage 1 take as input the music wave features in the\nneighborhood of each beat and output a sequence of key poses. Then the networks in stage 2 utilize the generated key poses\nand the music wave features between every two key poses to predict the motion curve in between. Each network adopts the\nproposed transformer-like network, called DanTrans. The DanTrans has a new transformer decoder, which uses the proposed\nKinematic Propagation Module (KPM). Note that the attention module (Att. in the ﬁgure) is a structured multi-head attention.\nmusic\nmotion\nparameter\ncurves\nbeat\nknots\nFigure 2: Motion Curves. The ﬁgure shows an example of\n4-knot TCB spline parameterization for motion curves.\nThe input sequential music features are music spectrum\nfeatures between a window centered at each beat. We use a\nwindow of size 0:8 seconds in our implementation since the\nlowest beat per-minus (BPM) is 75 in common dance music\n(which is validated in our PhantomDance dataset, and also\nvalid in other datasets). We choose the Hamming window\nto tackle the window overlap in music with higher BPMs.\nIn each window, we calculate the 40-dimensional form of\nMFCC (Logan et al. 2000) that has 13 coefﬁcients of Mel\nﬁlters cepstrum coefﬁcients with the ﬁrst and second order\ndifferences (13 ×2 dimensions) and the energy sum (1 di-\nmension). We further append a 12-dimension chroma fea-\nture to the feature of every beat and ﬁnally attain the inputs\nto the wave encoder. And the motion decoder combines the\nencoded features with the right-shifted outputs and sequen-\ntially predict the key poses, as indicated by the sequence-\nto-sequence prediction strategy based on the standard trans-\nformer (Vaswani et al. 2017).\nStage 2: Parametric Motion Curve Prediction\nBased on the generated key poses, the DanTrans in stage 2\nis targeted to generate the motion curves in between.\nThe motion curves are deﬁned as the values of trans-\nformation parameters with regard to time. The transfor-\nmation parameters are translation (x;y;z ) and rotation\n(rx;ry;rz;rw) in quaternion of each joint. We use a\nmulti-knots Kochanek-Bartels splines (Kochanek and Bar-\ntels 1984) to model each curve. It is a sort of cubic Her-\nmite spline that is used in animation editing software. The\nKochanek-Bartels spline is also named TCB spline because\nit has 3 parameters: tfor tension, bfor bias and cfor con-\ntinuity. While for a multi-knot TCB spline, we should also\ndetermine the intermediate knots and their tangents. Figure 2\nshows a example of motion curve parameterization.\nIn our implementation, we use one 4-knot TCB spline to\nﬁt one motion curve between two adjacent key poses, as an\noptimal tradeoff between ﬁtting accuracy and representation\ncompactness. Since the endpoints in the curve are just on the\nkey poses, we have 7 control parameters to predict, namely\nthe t, c, band the position of the two intermediate knots.\nSimilar to the DanTrans in stage 1, we extract the se-\nquence of the MFCC features from the music between every\ntwo adjacent beats as the input for the wave encoder, and\nthe motion decoder auto-regressively outputs the 7 control\nparameters that are aligned with each MFCC feature. Note\nthat the motion decoder requires the right-shifted output se-\nquences as its input, which should be additionally concate-\nnated with the two key poses at the beginning and the end of\nthe curve, which are generated in stage 1.\nNetwork Structure of DanTrans\nDanTrans has a similar Network structure to the standard\ntransformer (Vaswani et al. 2017) as illustrated on the right\nof Figure 1, except that the motion decoder employs the pro-\nposed Kinematic Propagation Module (KPM) and the Struc-\ntured Multi-head Attention Module (SMAM) in each trans-\nformer decoder layer. We employ a stack of N = 6 trans-\nformer decoders in the motion decoder of each DanTrans.\nNote that an additional KPM is applied at the beginning of\nthe motion decoder for output embedding.\nKinematic Propagation Module The Kinematic Propa-\ngation Module (KPM) is constructed on the basis of human\nbody structure, i.e., the 24 skeleton joint nodes with a tree\ntopology. The networks consist of a stack of so-called FK-\n1275\nFC\nFC\nFC\nFC\nFC\nFC\nForward Kinematic \nFeature Passing\nInverse Kinematic \nFeature Passing\nJoint \nNode \nFeatures\nFC FC\nFC\nQ Q\nQ\nQ Q\nAtt\nAttAtt\nK K\nK\nK K\nV V\nV\nV V\nAttAtt\nFC\nFC\nFC\nFC\nFCFC\nFCFC\nFC\nFC\nFCFC\nFigure 3: The Kninematic Propagation Module (KPM) and\nthe Structured Multi-head Attention. Features are embedded\nand passed along the kinematic chains forward and inversely\nwith fusion operations. Note that the endpoints of feature\npassing are root and leaf nodes, and we just show a part of\nthem for clearer view.\nIK blocks as illustrated in Figure 3. Note that the fully con-\nnected layer (FC) represents the complete BN-Linear-ReLU\nmodule, while we use FC to represent it for simplicity.\nThe FK-IK block has two steps, i.e., the forward kine-\nmatic (FK) feature passing and then the inverse kinematic\n(IK) feature passing, respectively. The forward kinematic\nfeature passing step ﬁrstly performs a feature embedding\non the root node and passes it to the neighbor child nodes\nalong the kinematic chain. After receiving the features from\nthe parent node, the child node performs feature fusion and\nthen conducts feature embedding by linear projection. The\nFC layers are utilized for feature embedding and the fusion\nadopts the addition operation. The backward kinematic fea-\nture passing step has a similar feature embedding and the\npassing process is performed from the leaf nodes to the root.\nThese procedures are similar to the operations of forward\nkinematics (FK) and inverse kinematics (IK) in computer\ngraphics and robotics. The FK-IK block attempts to encode\nfeatures into the motion controlling parameter space and to\nbring physics constraints into the model.\nFor the output embedding process at the beginning of the\nmotion decoder, we use a KPM with one FK-IK block. And\nfor feed-forward process in the intermediate layers, we adopt\na stack of 2 FK-IK blocks for each KPM. All the KPMs have\nthe same output feature size of 64.\nStructured Multi-Head Attention The features out-\nputted by KPM are arranged as the joint nodes, they nat-\nurally act as heads in the multi-head attention mechanism\nused in common transformers (Vaswani et al. 2017). In the\nmulti-head cross-attention, the encoded features from the in-\nput music are processed by two sets of 24 paralleled FC lay-\ners with the output size of 64, which then act as the values\nand keys. Note that no concatenation is needed after atten-\ntion calculation since the features should maintain the body\nstructure, and directly fed into the succeeding KPM module.\nThus this process saves computation compared to the typi-\ncal multi-head attention that concatenates features from each\nhead and uses a big linear layer for feature embedding. The\nstructured multi-head attention module, also shown in the\nright of Figure 3, deﬁnes the head as the node in the body\nstructure, which has explicit physical signiﬁcance. To some\ndegree this design is more reasonable than the multi-heads\nin the original transformer.\nTraining Objective\nWe adopt an adversarial training strategy that regards each\nDanTrans network as an individual generator, and learns two\ndiscriminators to judge whether distributions of the gener-\nated sequences and the groundtruth ones are aligned or not.\nEach discriminator consists of two modules to process the\nmusic feature and the output/groundtruth key pose sequence\n(in stage 1) or the parameters of the motion curves (in stage\n2), and the processed features are combined and fed into\nthe binary classiﬁer to predict the authenticity of the out-\nputs conditioned on the input music. All modules in the dis-\ncriminator are 3-layer MLPs. Moreover, when updating the\ngenerator, we also include an `2 reconstruction loss to avoid\nunwanted solutions that are far away from real data.\nExperiments\nImplementation Details\nSince most music in PhantomDance has the verse-chorus\nform, we divide the 260 dance animations into 1000 se-\nquences. Among them 900 pieces of music-dance pairs are\nused for model training, and the other 100 are split into\nthe test set. We carefully pick the test set to ensure it cov-\ners 13 genres and the BPM range from 80 to 180. We fol-\nlow the ofﬁcial training/testing splits in AIST++ (Li et al.\n2021). To gather the ground-truth labels for the training of\nthe DanceFormer, we use the beat track algorithm (Ellis\n2007) to extract beats and ﬁt the parameter motion curves\nfrom the provided pose sequences. The DanceFormer is end-\nto-end trained using 4 TITAN Xp GPUs with a batch size\nof 8 on each GPU. We use the Adam optimizer with be-\ntas {0:5;0:999}and a learning rate of 0:0002. The learning\nrate drops to 2e−5, 2e−6 after 100k, 200k steps. The model\nis trained with 300k steps for AIST++ and 400k steps for\nPhantomDance. The dimension of features in DanceFormer\nis 256 unless otherwise speciﬁed.\nEvaluation Metrics\nNormalized Power Spectrum Simularity (NPSS)It is an\nevaluation metric for long-term motion synthesis compared\nto Mean Square Error (MSE). We just follow its ofﬁcial\nimplementation (Gopalakrishnan et al. 2019) and compute\nNPSS in the joint motion space RT\u0002N\u00027 (4 for joint rota-\ntion represented as quaternion and 3 for joint position).\nFrechet Distance (FD) It is proposed by AIST++ (Li et al.\n2021) that has two metrics, namely PFD for position and\nVFD for velocity. We also employ it to calculate the distri-\nbution distance of joints.\nPosition Variance (PVar) It evaluates the diversity of the\ngenerated dance. In AIST++, a piece of music corresponds\nto more than one dance, but only one in PhantomDance. So\nwe make a modiﬁcation to the metric PVar in the experi-\nments on PhantomDance where we compute it along differ-\nent music pieces which have identical length.\nBeat Consistency Score (BC) It is a metric for motion-\nmusic correlation. We follow (Li et al. 2021) to deﬁne kine-\nmatic beats as the local minima of the kinetic velocity. BC\n1276\nComponent NPSS PFD VFD PVar BC\nDanceFormer 8.03 114.03 0.55 0.912 0.785\nCurve to Frames 14.55 1132.4 1.06 0.233 0.317\nKPM to Linear 10.54 162.44 0.79 0.592 0.637\nKPM to GNN 10.34 146.43 0.69 0.647 0.692\nTable 2: Ablation study about our DanceFormer on the\nPhantomDance dataset.\nAttention NPSS PFD VFD PVar BC\nGlobal 9.73 151.53 0.72 0.649 0.737\nLocal 9.01 125.59 0.68 0.744 0.763\nGaussian Loc. 8.93 117.14 0.64 0.753 0.764\nLearned Loc. 8.03 114.03 0.55 0.912 0.785\nTable 3: The comparison of different Attention mechanisms.\ncomputes the average distance between every music beat and\nits nearest kinematic beat with the following equation:\nBC = 1\n|Bx|\njBxjX\ni=1\nexp\n \n−\nmin8tx\nj 2Bx ||tx\nj −ty\ni ||2\n2\u001b2\n!\n(1)\nwhere Bx = {tx\nj };By = {ty\ni }are kinematic beats and mu-\nsic beats respectively and \u001b= 3. Note that BC has a similar\nform to Beat Alignment Score (BA) proposed in (Li et al.\n2021), but they are different in essence. BA forces every\nkinematic beat to match a music beat, but a dance usually\nhas many small kinematic beats that occur between mu-\nsic beats. Moreover, a music synchronized dance just needs\nto ensure the most salient music beats are accompanied by\nthe action emphasis (kinematic beats). So our proposed BC,\nwhich ﬁnds matched kinematic beat for each music beat, is\nmore appropriate in this case.\nAblation Study\nTo validate the effectiveness of our DanceFormer, we con-\nduct the ablation study on our PhantomDance dataset.\nEffectiveness of Curve Prediction To evaluate the effec-\ntiveness of our curve prediction, we sample 60-FPS poses\nfrom the data and use a standard transformer to directly pre-\ndict the sequence. The input sequences are extracted from a\n0:5s sliding window with a step of1=60 second on the music\nwaves. Its result is the second line of Table 2 as “Curve to\nFrame”. The experimental result demonstrates the advantage\nof our pose-to-curve two stage generation framework.\nEffectiveness of KPM To study the impact of the KPM\ncomponent, we ﬁrst replace it with the standard feed-\nforward (linear layers) modules in vanilla transformer\n(Vaswani et al. 2017). And the multi-head attention also\ncomes back to the common form. The third line in Table 2\nshows that the KPM has signiﬁcant improvement in mo-\ntion generation quality. To further study the FK-IK process-\ning, we introduce a comparison architecture which substi-\ntutes the FK-IK block with a 2-layer graph neural network\n(GNN) (Scarselli et al. 2008). This GNN fuses the joint fea-\ntures according to the adjacency matrix of undirected graph\ndeﬁned by the joint structure. The experimental result shows\nthat GNN has better performances compared to the base-\nline (“KPM to Linear”), which proves that introducing spa-\ntial correlations beneﬁts the representation of the learning\nmodel. While the proposed KPM surpasses the trivial GNN\nby a large margin, validating that involving spatial correla-\ntion and physics constraints into the network brings in sig-\nniﬁcant improvements in the generation quality that is eval-\nuated by NPSS, PFD and VFD.\nVariants of Attention In our implementation, we ﬁnd\nthat local attention (Luong, Pham, and Manning 2015) is\nmore proper for the motion sequence generation task than\nthe global attention in standard transformers. It can be ex-\nplained by the temporal locality of motions. That is, the\npose several seconds ago has no direct inﬂuence on the cur-\nrent pose. Moreover relationship between motion states is\nstronger with as time goes closer. So we further compare\nseveral attention algorithms and the results are shown in Ta-\nble 3. We use a sequence length of 17 in the simple local\nattention experiment shown on the second line. And we add\na Gaussian mask on the attention results before softmax\nwith a standard variance of 4. The result is on the third line\nand it has a slight improvement. Finally we use a learned\nmask to be optimized with the networks and obtain the best\nresult. The trained mask has a triangle shape on the whole\nwith some humps. Our DanceFormer employs this learnable\nlocal attention in our implementation.\nComparison with Other Methods\nWe mainly compare our method with AI Choreographer (Li\net al. 2021), which to our knowledge obtains the state-of-\nthe-art results for music-conditioned dance generation. The\nother two most related works, namely Li et al. (Li et al.\n2020) and Music2Dance (Zhuang et al. 2020) are also com-\npared. The experimental comparisons are performed on both\nthe AIST++ and our PhantomDance datasets.\nQuantitative Comparisons The results are viewed in Ta-\nble 4. Our method outperforms Li et al. (Li et al. 2020) and\nMusic2Dance (Zhuang et al. 2020) by a considerably large\nmargin. And it also surpasses AI Choreographer (Li et al.\n2021) signiﬁcantly in the quality related metrics (a30% gain\nof NPSS, a 28% gain of PFD and a 27% gain of VFD), di-\nversity metrics (a 46% gain of PVar) and beat consistency\nmetrics (a 93% gain of BC). The promising results on the\ndance quality metrics are mainly owing to the network struc-\nture of KPM that enhances the kinematic correlations. The\ngeneration diversity is ensured by the adversarial learning\nscheme. Note that the upper bound of the metric BC is 1,\nwhich means there exactly exists a kinematic beat at the time\nof each music beat. So our results have relatively high beat\nconsistency score, which is due to the proposed two-stage\nframework of our DanceFormer. Since no metric emphasizes\nthe ﬂuency of the dance performance, the advantages of mo-\ntion curve formulation can only be well revealed in qualita-\ntive results, as shown in Figure 4.\nQualitative Results and User Study Figure 4 provides\na sequence of frames about the generated dances by our\nDanceFormer. These results show that the proposed method\ncan provide diverse movements with high kinematic com-\nplexity. Video results accompanied with music, including\n1277\nFigure 4: Visualization of generated results of DanceFormer.\nMethod AIST++ PhantomDance\nNPSS # PFD # VFD # PVar \" BC \" NPSS # PFD # VFD # PVar \" BC \"\nLi et al. (Li et al. 2020) 16.31 5595.91 3.40 0.019 0.359 18.34 7944.78 5.84 0.014 0.175\nM2D (Zhuang et al. 2020) 14.74 2367.26 1.13 0.215 0.378 15.94 3147.89 3.74 0.267 0.223\nAI Chore. (Li et al. 2021) 8.29 113.56 0.45 0.509 0.452 10.62 164.33 0.73 0.624 0.388\nDanceFormer 6.01 84.32 0.34 0.734 0.782 8.03 114.03 0.55 0.912 0.785\nTable 4: Dance generation evaluation on the AIST++ and the PhantomDance datasets. M2D and AI Chore. represent Mu-\nsic2Dance and AI Choreographer respectively. Our method outperforms other baselines in terms of quality, diversity and beat\nconsistency. Especially, due to our two-stage prediction schema, our model has superior performance in term of BC indicating\nthat our model can generate dances which better match the given music.↓means that lower results indicate better methods, and\n↑vice versa.\n0.03\n0.22\n0.39\n0.86\n0.97\n0.78\n0.61\n0.14\nPerformance Quality\n0.02 0.12\n0.33\n0.82\n0.98 0.88\n0.67\n0.18\nMatching the Music\nMusic2Dance AI Choreographer OursGround-truthLi et al.\nFigure 5: Results of the user study. We conduct a user study\nto ask participants to choose the better dances from pairwise\ncomparisons. The criteria includes the performance quality\nand matching the music. The number denotes the percentage\nof preference on the comparison pairs.\nthe comparisons with the other works can be found on the\nproject page2.\nWe also conducted a user study to evaluate the quality\nof the generated music-conditioned dances. All the 100 se-\nquences of the validation set of PhantomDance were used\nfor the study. And then we collect the generated dances\nby our method and the compared baselines (Li et al. 2021,\n2020; Zhuang et al. 2020). In addition, the ground-truth\ndances are also included. The user study was conducted us-\ning a pairwise comparison scheme. For each of the 100 mu-\nsic, we provide 4 pairs in which our results occur with the\nresults from the baseline methods or the ground truth. Thus\n400 pairs were provided to the participants, and they were\n2https://huiye-tech.github.io/post/danceformer/\nasked to make two choices for each pair: “Which dance is\na better performance (more ﬂuent, graceful and pleasing)?”\nand “Which dance matches the music better?”. There are\n100 participants in the user study. Figure 5 shows the user\nstudy results, where our DanceFormer outperforms the other\nmethods on both criteria. Most of the participants rated that\nour method generates better dances in performance quality\ncompared with other works, and even more participants held\nthe opinion that the dances generated by our model better\nmatch the music.\nConclusion\nIn this work, we propose a new perspective to model the\nmusic-conditioned 3D dance generation task. Different from\nprevious works that deﬁne the outputs as sequences of poses,\nwe formulate them as key poses and in-between motion\ncurves. The curve representation makes the generated re-\nsults more ﬂuent and graceful. Based on this formulation, we\npropose the transformer-based DanceFormer with the novel\nDanTrans architecture consisting of the KPM module for\nbetter modeling kinematic correlations. DanceFormer thus\nyields high-quality results in the experimental comparisons.\nMoreover, we propose the PhantomDance Dataset, the ﬁrst\nmusic-conditioned 3D dance dataset that uses curves to rep-\nresent body motion, and it is the largest 3D dance dataset\nwith the best visual quality up to date.\nAcknowledgements\nThis work was partially supported by the National Nat-\nural Science Foundation of China (No. 61906012, No.\n62132001).\n1278\nReferences\nAlemi, O.; Franc ¸oise, J.; and Pasquier, P. 2017. Groovenet:\nReal-time music-driven dance movement generation using\nartiﬁcial neural networks. networks, 8(17): 26.\nChao, S.-P.; Chiu, C.-Y .; Chao, J.-H.; Yang, S.-N.; and Lin,\nT.-K. 2004. Motion retrieval and its application to motion\nsynthesis. In 24th International Conference on Distributed\nComputing Systems Workshops, 2004. Proceedings., 254–\n259. IEEE.\nChen, K.; Tan, Z.; Lei, J.; Zhang, S.-H.; Guo, Y .-C.; Zhang,\nW.; and Hu, S.-M. 2021. ChoreoMaster: choreography-\noriented music-driven dance synthesis. ACM Transactions\non Graphics (TOG), 40(4): 1–13.\nDerakhshani, D. 2012. Introducing Autodesk Maya 2013.\nJohn Wiley & Sons.\nEllis, D. P. 2007. Beat tracking by dynamic programming.\nJournal of New Music Research, 36(1): 51–60.\nGoodridge, J. 1999. Rhythm and timing of movement in per-\nformance: Drama, dance and ceremony. Jessica Kingsley\nPublishers.\nGopalakrishnan, A.; Mali, A.; Kifer, D.; Giles, L.; and Oror-\nbia, A. G. 2019. A neural temporal model for human motion\nprediction. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 12116–12125.\nIonescu, C.; Papava, D.; Olaru, V .; and Sminchisescu, C.\n2013. Human3. 6m: Large scale datasets and predictive\nmethods for 3d human sensing in natural environments.\nIEEE transactions on pattern analysis and machine intel-\nligence, 36(7): 1325–1339.\nKochanek, D. H.; and Bartels, R. H. 1984. Interpolating\nsplines with local tension, continuity, and bias control. In\nProceedings of the 11th annual conference on Computer\ngraphics and interactive techniques, 33–41.\nLasseter, J. 1987. Principles of traditional animation applied\nto 3D computer animation. In Proceedings of the 14th an-\nnual conference on Computer graphics and interactive tech-\nniques, 35–44.\nLee, H. Y .; Yang, X.; Liu, M. Y .; Wang, T. C.; Lu, Y . D.;\nYang, M. H.; and Kautz, J. 2019. Dancing to music. Ad-\nvances in Neural Information Processing Systems, 32.\nLee, J.; Kim, S.; and Lee, K. 2018. Listen to\ndance: Music-driven choreography generation using au-\ntoregressive encoder-decoder network. arXiv preprint\narXiv:1811.00818.\nLi, J.; Yin, Y .; Chu, H.; Zhou, Y .; Wang, T.; Fidler, S.; and\nLi, H. 2020. Learning to Generate Diverse Dance Motions\nwith Transformer. arXiv preprint arXiv:2008.08171.\nLi, R.; Yang, S.; Ross, D. A.; and Kanazawa, A. 2021. AI\nChoreographer: Music Conditioned 3D Dance Generation\nwith AIST++. In The IEEE International Conference on\nComputer Vision (ICCV).\nLogan, B.; et al. 2000. Mel frequency cepstral coefﬁcients\nfor music modeling. In Ismir, volume 270, 1–11. Citeseer.\nLoper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.; and\nBlack, M. J. 2015. SMPL: A skinned multi-person linear\nmodel. ACM transactions on graphics (TOG), 34(6): 1–16.\nLuong, T.; Pham, H.; and Manning, C. D. 2015. Effective\nApproaches to Attention-based Neural Machine Translation.\nIn EMNLP.\nMahmood, N.; Ghorbani, N.; Troje, N. F.; Pons-Moll, G.;\nand Black, M. J. 2019. AMASS: Archive of motion capture\nas surface shapes. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 5442–5451.\nScarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.; and\nMonfardini, G. 2008. The graph neural network model.\nIEEE transactions on neural networks, 20(1): 61–80.\nSun, G.; Wong, Y .; Cheng, Z.; Kankanhalli, M. S.; Geng,\nW.; and Li, X. 2020. DeepDance: music-to-dance motion\nchoreography with adversarial learning. IEEE Transactions\non Multimedia, 23: 497–509.\nTakano, W.; Yamane, K.; and Nakamura, Y . 2010. Retrieval\nand Generation of Human Motions Based on Associative\nModel between Motion Symbols and Motion Labels. Jour-\nnal of the Robotics Society of Japan, 28(6): 723–734.\nTang, T.; Jia, J.; and Mao, H. 2018. Dance with melody: An\nlstm-autoencoder approach to music-oriented dance synthe-\nsis. In Proceedings of the 26th ACM international confer-\nence on Multimedia, 1598–1606.\nThomas, F.; Johnston, O.; and Thomas, F. 1995.The illusion\nof life: Disney animation. Hyperion New York.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. arXiv preprint arXiv:1706.03762.\nWilliams, R. 2012. The animator’s survival kit: a manual\nof methods, principles and formulas for classical, computer,\ngames, stop motion and internet animators. Macmillan.\nYan, S.; Li, Z.; Xiong, Y .; Yan, H.; and Lin, D. 2019. Convo-\nlutional sequence generation for skeleton-based action syn-\nthesis. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 4394–4402.\nZhuang, W.; Wang, C.; Xia, S.; Chai, J.; and Wang, Y .\n2020. Music2dance: Music-driven dance generation using\nwavenet. arXiv preprint arXiv:2002.03761.\n1279",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7443001866340637
    },
    {
      "name": "Motion capture",
      "score": 0.6399855017662048
    },
    {
      "name": "Parametric statistics",
      "score": 0.621735155582428
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.5553791522979736
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5429193377494812
    },
    {
      "name": "Animation",
      "score": 0.5338512659072876
    },
    {
      "name": "Kinematics",
      "score": 0.4729592204093933
    },
    {
      "name": "Transformer",
      "score": 0.43815168738365173
    },
    {
      "name": "Computer animation",
      "score": 0.4272930920124054
    },
    {
      "name": "Dance",
      "score": 0.4199076294898987
    },
    {
      "name": "Motion (physics)",
      "score": 0.4134123921394348
    },
    {
      "name": "Computer vision",
      "score": 0.40613123774528503
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.2923066020011902
    },
    {
      "name": "Engineering",
      "score": 0.1128363311290741
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Classical mechanics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}