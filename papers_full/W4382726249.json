{
    "title": "Learning the language of QCD jets with transformers",
    "url": "https://openalex.org/W4382726249",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3034321596",
            "name": "Thorben Finke",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A1989696066",
            "name": "Michael Krämer",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A1302409250",
            "name": "Alexander Mück",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A3135256458",
            "name": "Jan Tönshoff",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A3034321596",
            "name": "Thorben Finke",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A1989696066",
            "name": "Michael Krämer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1302409250",
            "name": "Alexander Mück",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3135256458",
            "name": "Jan Tönshoff",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4225743392",
        "https://openalex.org/W2798860952",
        "https://openalex.org/W2988402619",
        "https://openalex.org/W3128540831",
        "https://openalex.org/W4381734930",
        "https://openalex.org/W3189302471",
        "https://openalex.org/W4312126958",
        "https://openalex.org/W3208994037",
        "https://openalex.org/W2586765566",
        "https://openalex.org/W2915621743",
        "https://openalex.org/W2916245928",
        "https://openalex.org/W2998758436",
        "https://openalex.org/W3101164173",
        "https://openalex.org/W3172441158",
        "https://openalex.org/W3098229010"
    ],
    "abstract": "A bstract Transformers have become the primary architecture for natural language processing. In this study, we explore their use for auto-regressive density estimation in high-energy jet physics, which involves working with a high-dimensional space. We draw an analogy between sentences and words in natural language and jets and their constituents in high-energy physics. Specifically, we investigate density estimation for light QCD jets and hadronically decaying boosted top jets. Since transformers allow easy sampling from learned densities, we exploit their generative capability to assess the quality of the density estimate. Our results indicate that the generated data samples closely resemble the original data, as evidenced by the excellent agreement of distributions such as particle multiplicity or jet mass. Furthermore, the generated samples are difficult to distinguish from the original data, even by a powerful supervised classifier. Given their exceptional data processing capabilities, transformers could potentially be trained directly on the massive LHC data sets to learn the probability densities in high-energy jet physics.",
    "full_text": "JHEP06(2023)184\nPublished for SISSA by\n Springer\nReceived: March 21, 2023\nRevised: May 30, 2023\nAccepted: June 19, 2023\nPublished: June 27, 2023\nLearning the language of QCD jets with transformers\nThorben Finke,a Michael Krämer,a Alexander Mücka and Jan Tönshoﬀb\naInstitute for Theoretical Particle Physics and Cosmology (TTK), RWTH Aachen University,\nD-52056 Aachen, Germany\nbChair of Computer Science 7 (Logic and Theory of Discrete Systems),\nDepartment of Computer Science, RWTH Aachen University,\nAachen, Germany\nE-mail: finke@physik.rwth-aachen.de, mkraemer@physik.rwth-aachen.de,\nmueck@physik.rwth-aachen.de, toenshoff@informatik.rwth-aachen.de\nAbstract: Transformers have become the primary architecture for natural language\nprocessing. In this study, we explore their use for auto-regressive density estimation in\nhigh-energy jet physics, which involves working with a high-dimensional space. We draw an\nanalogy between sentences and words in natural language and jets and their constituents\nin high-energy physics. Speciﬁcally, we investigate density estimation for light QCD jets\nand hadronically decaying boosted top jets. Since transformers allow easy sampling from\nlearned densities, we exploit their generative capability to assess the quality of the density\nestimate. Our results indicate that the generated data samples closely resemble the original\ndata, as evidenced by the excellent agreement of distributions such as particle multiplicity\nor jet mass. Furthermore, the generated samples are diﬃcult to distinguish from the original\ndata, even by a powerful supervised classiﬁer. Given their exceptional data processing\ncapabilities, transformers could potentially be trained directly on the massive LHC data\nsets to learn the probability densities in high-energy jet physics.\nKeywords: Jets and Jet Substructure, Speciﬁc QCD Phenomenology\nArXiv ePrint: 2303.07364\nOpen Access, c⃝ The Authors.\nArticle funded by SCOAP3. https://doi.org/10.1007/JHEP06(2023)184\nJHEP06(2023)184\nContents\n1 Introduction 1\n2 Setup 3\n2.1 Transformer encoder for density estimation in high dimensions 3\n2.2 Data set and data format 4\n3 Results 6\n3.1 The density estimate 7\n3.2 Jet generation 8\n4 Conclusion 10\nA Further results 12\nA.1 Sampling QCD jets from the full distribution 12\nA.2 Sampling top jets 12\n1 Introduction\nThe Large Hadron Collider (LHC) produces a huge amount of highly structured, high-\ndimensional data. Estimating the probability density of this data is a crucial task, as it\nallows one to improve experimental analyses, generate synthetic data, or perform anomaly\ndetection. The LHC data is typically reduced to hand-crafted, high-level features to extract\nrelevant information, a process that is task-dependent and requires expert knowledge.\nPrevious machine learning advances in high-energy physics (HEP) have shown the beneﬁts\nof using low-level information, which can capture important correlations that may be missed\nby high-level observables. However, density estimation in many dimensions is inherently\nchallenging due to the curse of dimensionality.\nDensity estimation in HEP has primarily focused on generating data samples that follow\na given distributionp(x), see the reviews [1, 2] and references therein. For this purpose,\nthe density need not to be tractable, i.e. it is not necessary to be able to calculatep(x) for\ngiven x. Generative adversarial networks (GANs) [3] are an example of models that can\nonly sample from a distribution. In contrast, normalizing ﬂows [4] allow for both sampling\nand likelihood evaluation. Normalizing ﬂows optimize a bijective mapping with a tractable\nJacobian between a simple base distribution and the data distribution, enabling sampling\nand likelihood evaluation. However, normalizing ﬂows have the disadvantage of a ﬁxed\ninput size due to the bijective mapping, and they are typically only applied to high-level\nfeatures. While normalizing ﬂows can be applied to high-dimensional physics data, there is\na risk of manifold collapse [5, 6]. Nevertheless, refs. [7–10] provide examples of successful\napplications of normalizing ﬂows to such data.\n– 1 –\nJHEP06(2023)184\nAn auto-regressive approach to density estimation was presented in refs. [11, 12]. This\napproach involves clustering the set of particle momenta within a jet into a binary tree,\nwhich is then sequentially analysed by a recurrent neural network. The resulting probability\ndensity can be used for discrimination tasks and generating data samples.\nWe present a novel method for auto-regressive density estimation in jet physics that\nutilizes the transformer-encoder architecture introduced in ref. [13]. Our approach is closely\nrelated to TraDE (Transformers for Density Estimation) introduced in ref. [14], but we\nsimplify the method to suit our speciﬁc needs. Transformers have become the leading\narchitecture in the ﬁeld of natural language processing (NLP) [15]. When compared to\nrecurrent neural networks, transformers oﬀer distinct advantages, especially in handling\nlong-range correlations and improving training eﬃciency. Transformers are now being\napplied to various HEP tasks, such as jet tagging [16, 17], reconstruction [18, 19], learning\njet representations [20], and data generation [21–23]. These applications modify the\ntransformer, in particular its attention mechanism, to satisfy desirable physics properties,\nsuch as permutational invariance or explicit pairwise interactions.\nWe take a diﬀerent approach and pre-process the data to be more similar to natural\nlanguage. To represent jet constituents, we discretize their properties by binning their\ntransverse momentum,pT , and the diﬀerence in rapidity and azimuthal angle relative to\nthe jet axis,∆ ηand ∆ φ, respectively. We will argue that this discretization does not result\nin signiﬁcant information loss. The discrete particle states can then be viewed as words in a\ndictionary, and the process of combining particles to form jets is analogous to constructing\nsentences in natural language processing.\nTo estimate the density, we use an auto-regressive approach, where the transformer\nis trained to predict the probability of a jet constituentp(xi) given all prior constituents\nof the jetx1 to xi−1. In this way, we can model the joint density of the entire jet withn\nconstituents as the product of the probabilities of each constituent\np(x) = p(x1)p(x2|x1) ...p(xn|x1 ...xn−1). (1.1)\nWe introduce a very basic grammar to the language of jets by ordering jet constituents in\npT . The transformer setup is ﬂexible, e.g. it allows density estimation as well as sampling\nfor jets with varying numbers of constituents. An alternative approach to sampling jets\nwith variable multiplicity using GANs has been presented in ref. [24].\nWe use Monte Carlo data to train a standard transformer architecture and determine\nthe probability density for QCD and top-jets. By evaluating the likelihood ratio of the\ndensities, we demonstrate that the transformer learns properties speciﬁc to the physics of\ntop and QCD jets. To further assess the quality of the density estimation, we generate data\nsamples from the learned probability distributions. The generated samples display excellent\nagreement with the Monte Carlo data, as demonstrated by distributions such as particle\nmultiplicity or jet mass, and are diﬃcult to distinguish from the data, even for a powerful\nsupervised classiﬁer.\nThis paper is structured as follows: in section 2 we describe the transformer architecture\ntogether with the way we pre-process the data. We show results on density estimation\n– 2 –\nJHEP06(2023)184\nas well as sampling in section 3. Finally, we conclude the paper in section 4 and outline\npossible future research directions. Additional results are presented in the appendices.\n2 Setup\nThis section provides a detailed description of our setup. Section 2.1 presents the trans-\nformer architecture, the training procedure, and the sampling from the learned probability\ndistribution. Section 2.2 introduces the data set and its pre-processing.\n2.1 Transformer encoder for density estimation in high dimensions\nWe use the encoder part of a transformer network as proposed in ref. [13]. As mentioned in\nthe introduction and discussed in detail in section 2.2, the features of each jet constituent\nare discretized, i.e. we consider a large but ﬁnite number of diﬀerent particles, where each\nxi in eq. (1.1) is represented by a tuple of integers that describepT , ∆ η, and∆ φ. These\nparticles correspond to the words in an NLP dictionary. An embedding layer is used to\nmap the particles back to a trainable continuous representation.\nWe perform the embedding separately forpT , ∆ ηand ∆ φ. Each embedding consists of\na trainable matrix with rows corresponding to the bins in the corresponding feature and\na ﬁxed number of columns determined by the embedding dimension. We obtain the ﬁnal\nembedding of a particle by summing the three vectors of the embedding dimension. In\nNLP, this embedding learns to map words with similar meanings close to each other in the\nembedded space.\nThe embedding is followed by a block of layers with multi-head self-attention, as\nimplemented in PyTorch [25] asTransformerEncoderLayer. The attention mechanism\nallows the network to learn which positions are especially linked. As in NLP, where words\nare strongly correlated, the transformer learns the correlations hidden in the jet probability\ndistribution. The multi-head attention allows the network to consider various correlations\nin a single processing step. This block is followed by a ﬁnal fully connected layer that maps\nto the desired output dimension and assigns a probability to each particle of our particle\ndictionary using a softmax activation. As we discretized the features, i.e. we work with a\nﬁnite dictionary, we only have to deal with probabilities rather than probability densities.\nUsing Gaussian mixtures, one could also extend the method to continuous features [14].\nWe have set the embedding dimension and the output dimension of eachTrans-\nformerEncoderLayer to 256. Our model consists of eight such layers, each with four\nheads. We apply LayerNorm and Dropout of 0.1 within and after the lastTrans-\nformerEncoderLayer. With this setup, our network has about 13M trainable parameters.\nThese parameters are optimized for 50 epochs using the Adam optimizer [26] with an initial\nlearning rate of5 ×10−4, a batch size of 100 and a cosine learning rate schedule with\na ﬁnal learning rate of10−6. To maximize the likelihood, we minimize the categorical\ncross-entropy loss between the predicted next particle and the actual next particle in a\njet. Given the exploratory nature of this study, we have not performed hyperparameter\noptimization. Note that training the transformer model on a NVIDIA Tesla V100 SXM2\nGPU takes approximately 5 hours.\n– 3 –\nJHEP06(2023)184\nDuring training we use two masks on our data. A ﬁxed length input is desirable to\nensure an eﬃcient batch training. Thus, we need to perform padding on jets that have\nless constituents and mask these padded values using a padding mask. An additional\ncausal mask is used during training to ensure that the network only uses the preceding jet\nconstituents x1 to xi−1 when predicting the current jet constituentxi. To technically treat\nthe ﬁrst jet constituent on the same footing as the others, we add a meaningless zeroth jet\nconstituent x0 at the beginning of each jet. For the ﬁrst particle, the network learns the\nmarginal distribution over the training set. To indicate the end of a jet, we use a stop token\nas discussed in section 2.2, which allows for the density estimation and the generation of\njets with ﬂexible length.\nUsing the trained network, we obtain the probabilityp(x) for a given input jet by\nmultiplying the probabilitiesp(xi) that the network assigns to the constituents of the jet\n(including the stop token), according to eq. (1.1). In practice, we calculate the log-probability\nand sum up the logarithms of the individual probabilities. For inference, the causal mask\nallows us to obtain the probability for all particles in the jet at once, with a single forward\npass through the network, as was the case during training.\nThe trained network can also be used to sample jets. However, unlike training and\ninference, sampling requires an iterative procedure due to the auto-regressive nature of the\ndensity estimator, which can impact the speed of sampling. To begin the sampling process,\nwe feed only the zeroth jet constituent into the transformer, and the network provides\nthe probabilities for the ﬁrst particle. We then sample according to the probabilities, add\nthe sampled particle to the jet, and repeat this procedure until a maximum number of\nconstituents is reached, or the stop token is sampled to indicate that the jet is complete.\nAs it is well known in NLP, text generation from a transformer network is improved\nby suppressing the sampling of extremely unlikely possibilities, see for example ref. [27].\nHere, we adopt this procedure by only sampling from the 5000 most likely particles when\npredicting the next particle in a jet, which is usually referred to as top-k sampling in NLP.\nOur choice ofk = 5000 is motivated at the end of section 2.2 after discussing the structure\nof the data set. Top-k sampling reduces the generic problem that the transformer is not\nable to reduce probabilities for unlikely events to arbitrarily low values. Further discussion\non sampling from the full probability distribution without a top-k restriction is provided in\nappendix A.1.\n2.2 Data set and data format\nTo demonstrate the eﬀectiveness of our method, we use the top tagging reference data\nset [28] in this study. This data set comprises 2 milllion events, split equally between light\nQCD jets and hadronically decaying top jets. The jets are clustered using the anti-kT\nalgorithm with R = 0 .8 and selected withpT ∈[550,650] GeV and|η|< 2. The data\nset contains the 200 highestpT constituents in each jet. We consider only the 50 highest\npT constituents for training the transformer network, but perform density evaluation and\nsampling with up to 100 constituents to emphasize the ﬂexibility of the setup. We use 600k\njets for training and reserve 200k jets each for validation and testing, respectively.\n– 4 –\nJHEP06(2023)184\n0.0 0.2 0.4 0.6 0.8 1.0\nϵtop\n100\n101\n102\n103\n104\n1/ϵQCD\nOriginal\nDiscrete\nFigure 1. ROC curves in background rejection1/ϵQCD and signal eﬃciency ϵtop for top tagging,\nusing the ParticleNet architecture [30] on continuous and discretized data. The data is discretized\naccording to the discription provided in the text. The performance matches the ParticleNet\nperformance stated in ref. [31]. Discretization does not cause a signiﬁcant decrease in performance.\nWe represent a jet as a list of its constituents sorted by decreasingpT . While alternative\norderings inspired by physics, such as those derived from jet clustering algorithms [11, 12, 29],\ncould be employed, we deliberately choose the simplepT order. This choice is based on\nthe expectation that the transformer model has the ability to learn non-trivial correlations\nbetween the jet components without the need for complex task-speciﬁc jet setup. For each\nconstituent i, we usepT,i, ∆ ηi = ηi −ηjet, and∆ φi = φi −φjet as features. We discretize\nthese features by binning. We divide the range[−0.8,0.8] (set according to the jet radius)\ninto 29 equidistant bins for both∆ ηand ∆ φ. A similar binning is performed when using\njet images. To account for the steeply fallingpT spectrum, we work in log-space and use 39\nequidistant bins in the range[log(pT )min,log(pT )max], where99.9% of all jet constituents in\nthe training data satisfypT >log(pT )min, andlog(pT )max is deﬁned by the constituent with\nthe largestpT . We add a bin 0 as an underﬂow bin and use bin 40 and 30 as an overﬂow\nbin forpT and ∆ η,∆ φ, respectively. Each constituent is then described by three integers\ncorresponding to the respective binsˆpT ∈[0,40], ∆ ˆη∈[0,30], and ∆ ˆφ ∈[0,30]. These\ntuples of integers serve as input for the transformer network.\nAs described in section 2.1, we map these tuples of integers back to a continuous space\nusinganembedding. Onecouldarguethatwecouldinsteaddirectlyusethecontinuousvalues\nof the features as input. However, we will sample discrete values. These sampled constituents\nare successively used as input to sample consecutive constituents. Using continuous values\nduring training and discrete values for sampling might reduce the sampling accuracy.\nTo determine if a signiﬁcant amount of information is lost using discretized data,\nwe train a ParticleNet classiﬁer [30] to distinguish between top jets and light QCD jets.\nWe excluded features related to the energies of jet constituents or the jet itself, as this\ninformation is lost with the features selected for density estimation. The ROC curves for\nthis classiﬁer for both continuous and discrete data, see ﬁgure 1, show that there is indeed\nno signiﬁcant loss of information for this task. The same ParticleNet classiﬁer is later used\n– 5 –\nJHEP06(2023)184\n0 20 40 60 80 100\nConstituent position\n0\n2\n4\n6\n8\n10\n12\n14\nNumber of particles ×10−3\nFigure 2. Number of distinct particles that are present at each constituent position.\nin section 3.2 to assess the quality of our generated samples. Speciﬁcally, the classiﬁer is\ntasked with distinguishing between the generated samples and the actual data.\nTo indicate the end of a jet, we utilize a stop token, which is always a possible prediction\nfor the next particle. To ensure that the network can generate jets with varying numbers of\nconstituents, the stop token is added to the particle dictionary as an additional particle.\nDuring training, the stop particle is only added as the last particle of the jet if the jet has\nless thanN constituents, whereN = 50 is the maximum number of training constituents.\nThis approach enables the network to learn what constitutes a complete jet and allows\nthe generation of jets with more thanN constituents. The probability of a jet with\nn < Nconstituents includes the probability of the stop token appearing at constituent\nposition n+ 1.\nNote that not all possible particles are present in the training data; for QCD jets,\nroughly 35% of particles are absent from the training set. Figure 2 illustrates the number\nof distinct particles present at each constituent position, with less than half of the possible\nstates being populated even at the peak, and a sharp drop in numbers for positions above\n20. Some particles may be present in a more extensive training set, and it is part of the\ntask of density estimation to also extrapolate the density into these regions. However, quite\na few particles will actually have zero probability. Sampling from those particles might\nlead to unsatisfactory results because the transformer cannot suppress the probability of a\nparticle to zero. To avoid this problem we use top-k sampling as introduced at the end of\nsection 2.1. To sample mostly from particles that are present in the training set we chose\nk = 5000. We found that varying k by a factor of two did not have a signiﬁcantly impact\non the results presented in section 3.\n3 Results\nEmploying the setup described in section 2, the transformer is trained to learn the density\nof QCD and top jets. Learning the density in a high-dimensional space is challenging,\nand evaluating the accuracy of the density estimate is equally diﬃcult. In section 3.1, we\n– 6 –\nJHEP06(2023)184\n−800 −700 −600 −500 −400 −300 −200 −100 0\nlog(p)\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nNormalized distribution\nQCD\nTop\n−800 −700 −600 −500 −400 −300 −200 −100 0\nlog(p)\n0\n20\n40\n60\n80\n100\nMultiplicity\nQCD\nTop\nFigure 3. Left: distribution of the estimated jet probability (100 leading constituents) for the QCD\njets (black) and the top jets (blue) in the data set. The transformer is either trained on QCD jets\n(solid) or top jets (dashed). Right: correlation between the estimated probability and the number of\nconstituents. Contours contain 25%, 50% and 75% of the density using a kernel density estimate\n(on 50k of the test set). The dashed line shows the jet probability obtained by assigning the same\nprobability to each particle in our dictionary, i.e.log(p) = log(1/Nd) ×n, whereNd is the size of\nour particle dictionary andn is the number of constituents for a given jet.\ndescribe some aspects of our density estimate, speciﬁcally the learned jet probabilities.\nAdditionally, in section 3.2, we discuss the generation of jets by sampling from the learned\nprobability distribution to further evaluate the quality of the density estimate.\n3.1 The density estimate\nThe transformer assigns a probability to a given input jet. The distributions of all jet\nprobabilities are shown in ﬁgure 3 (left). While we train on the 50 leadingpT constituents,\nwe show the probability taking into account the 100 leadingpT constituents. As we will\nsee below, the transformer accurately extrapolates to larger jet multiplicities that are not\npresent during training. Note that there is no computational limit to using more constituents\nfor training. We have used 50 constituents to demonstrate the extrapolation capabilities of\nthe sampling. While each individual jet has a small probability, the jet probabilities vary\nextensively, primarily due to diﬀerent jet multiplicities.\nThe correlation between the number of constituents and the probability assigned to a jet\nis shown in the right panel of ﬁgure 3. The probability decreases approximately exponentially\nwith increasing number of constituents, because each jet constituent introduces an additional\nfactor in eq. (1.1). To guide the eye, the probability for a uniform prediction for all particles\nin our particle dictionary is included in ﬁgure 3. The decrease in probability for higher\nmultiplicity is (partly) compensated by an increase in the phase space for the jets, i.e. in the\nnumber of possible jet realizations, and results in the well-known multiplicity distribution\nto be discussed in section 3.2.\nIn ﬁgure 3 (left), we also display the probability distribution for top jets, which the\nnetwork did not encounter during training. The distribution for top jets is shifted toward\nsmaller probabilities, mainly due to their larger multiplicities. As shown in the right panel\n– 7 –\nJHEP06(2023)184\n−40 −30 −20 −10 0 10 20\ns\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nNormalized distribution\nQCD\nTop\n0.0 0.2 0.4 0.6 0.8 1.0\nϵtop\n100\n101\n102\n103\n104\n1/ϵQCD\nParticleNet\nTransformer\nFigure 4. Left: Log-likelihood ratios for top and QCD jets. Right: ROC curves for top vs. QCD\nclassiﬁcation; the supervised ParticleNet classiﬁer [30] is shown for comparison.\nof ﬁgure 3, the distribution has approximately the same slope but is shifted to higher\nmultiplicities. The distinction between QCD and top jets based on their underlying physics\ncan be evaluated by training the transformer on top jets (represented by the dashed lines in\nﬁgure 3). As expected, training on top jets, the distribution of probabilities indeed shifts to\nlarger probabilities for the top jets and to smaller probabilities for QCD jets. Although this\nshift is small compared to the trivial multiplicity oﬀset, it encodes the diﬀerent physics of\nthe two jet types, since the probabilities for a given jet still change by orders of magnitude.\nTo demonstrate that the transformer not only learns some generic properties but indeed\nQCD and top speciﬁc features, we investigate the classiﬁcation performance of the likelihood\nratio, see also refs. [11, 12, 32]. We calculates= log(pt(x)) −log(pq(x)) for any given jet,\nwhere t and q represent training on top and QCD jets, respectively. If the densities were\nperfectly estimatedswould be the ideal classiﬁcation score and monotonically related to the\nscore of an ideal supervised classiﬁer. We show the distribution ofs for top and QCD jets\nin the test set in ﬁgure 4 (left), together with the resulting ROC curve (right). While the\nlikelihood ratio calculated from the low-level features of the jets is not expected to match\nthe performance of a fully supervised classiﬁer like ParticleNet [30], it still demonstrates\nsome separation capability. Reﬁning the training of the models within the classiﬁcation\nframework, as demonstrated in ref. [12], could potentially further optimise the discriminative\npower. In addition, we believe that a signiﬁcant improvement in the performance of the\nlikelihood-based classiﬁer can be achieved by increasing the amount of training data, as\nthe current limitation appears to be due to overﬁtting with a training set consisting of\nonly 600k jets. Notably, evaluating the likelihood-based classiﬁer on the training set even\noutperforms ParticleNet.\n3.2 Jet generation\nIn this section, we employ the transformer network as a generative model to sample jets.\nThe transformer network is trained as described in section 2 using the 600k light QCD\ntraining jets. While the network is only trained on the 50 leadingpT constituents, it samples\n– 8 –\nJHEP06(2023)184\n0 20 40 60 80 100\nMultiplicity\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\nNormalized distribution\nData\nSamples\n−2 0 2 4 6\nlog(pT )\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nNormalized distribution\n−0.8 −0.6 −0.4 −0.2 0.0 0.2 0.4 0.6 0.8\n∆ η\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nNormalized distribution\n0 100 200 300 400\nmjet\n10−6\n10−5\n10−4\n10−3\n10−2\nNormalized distribution\nFigure 5. Comparison of data and sample distributions for QCD jets. We compare the particle\nmultiplicity (top left),log(pT ) (top right),∆ η(bottom left), andmjet (bottom right) distributions\nof all jet constituents. We use 200k jets for both the generated samples as well as the data from the\ntest set.\njets with an arbitrary number of constituents with high ﬁdelity. Generating jets with up\nto 100 constituents takes∼20ms per jet when sampling from the full distribution. The\nsampling time is slightly reduced for top-k sampling.1 Where we compare jet samples with\nour test data, we use discretized values for both.\nIn ﬁgure 5, we compare distributions of 200k sampled events with those from the test\nset data. The results for the particle multiplicity are particularly interesting. We observe\nthat the transformer does not only learn the multiplicity as seen in the training set where\nthe number of constituents is truncated at 50. The transformer is also able to extrapolate\nthe multiplicity to larger values. Hence, it learns the structure of jets to the extent that it\nknows how to terminate them.\nIn ﬁgure 5, we also present the distributions of the input featurespT and ∆ η, along\nwith the jet mass calculated assuming massless constituents and the central values for the\ninput features in each bin. We do not show the distribution of∆ φfor brevity since it closely\nresembles that of∆ η. The pT distribution of all constituents is accurately reproduced,\nand the jet mass is well-reproduced. Although the angular distributions exhibit some\nminor deviations, with the sampling being slightly too central on average for QCD jets, the\ngenerated samples show an impressive overall agreement with the data.\n1Timing was tested on a NVIDIA Tesla V100-SXM2-16GB GPU, generating 10k jets with a batch size\nof 100.\n– 9 –\nJHEP06(2023)184\n0.0 0.2 0.4 0.6 0.8 1.0\nϵSamples\n100\n101\n102\n103\n104\n1/ϵData\nFigure 6. ROC curves for a ParticleNet classiﬁer to distinguish 200k generated samples from the\n200k jets of the test data set. We display three separate curves, each corresponding to independent\ntraining, sampling, and classiﬁcation. The dashed line shows the ROC curve of a random classiﬁer.\nTo assess the quality of our generated samples beyond one-dimensional distributions,\nwe train a ParticleNet classiﬁer (see section 2) to diﬀerentiate between the samples and\nthe data. This is an important test because generative models often produce samples\nthat are easily distinguishable from the real data by a classiﬁer (see for example the\ndiscussion in refs. [7, 10]). The resulting ROC curves, shown in ﬁgure 6, indicate that our\ngenerative model produces samples of high ﬁdelity. We display three separate curves, each\ncorresponding to independent training, sampling, and classiﬁcation. Despite the ParticleNet\nclassiﬁer’s high discriminative power, its AUC score of 0.62 is only slightly better than\nrandom. The success of our generative model in passing this test can be attributed to its\nexcellent density estimation capabilities. Results for sampling without applying the top-k\nmethod are discussed in appendix A.1. Results for evaluating the quality of the density\nestimation for top jets by sampling can be found in appendix A.2.\n4 Conclusion\nTransformers are extremely powerful deep learning architectures that have set new standards\nin natural language processing. In particular, large and powerful models can be trained if\nsuﬃcient amounts of data are available. In this study, we investigate the use of transformers\nfor determining probability densities in jet physics. We consider low-level information, such\nas the transverse momentum and angular position of the jet constituents, rather than relying\non hand-crafted high-level features. While low-level information can capture important\n– 10 –\nJHEP06(2023)184\ncorrelations in the data, density estimation in such a high-dimensional space remains a\nchallenging task.\nWe draw an analogy between sentences and words in natural language and jets and their\nconstituents in high-energy physics. To make the data more similar to natural language, we\npre-process the features of the jet constituents by discretizing them. We have shown that\nthis discretization does not lead to a signiﬁcant loss of information. The discrete particle\nstates can then be viewed as words in a dictionary, and the process of combining particles\ninto jets is analogous to constructing sentences in natural language processing.\nWe use an auto-regressive approach to determine the probability density of QCD and\ntop-jets. In this approach, the transformer is trained to determine the probability of a jet\nconstituent given all the previous constituents of the jet. The design of the transformer is\nﬂexible, allowing for both density estimation and the generation of artiﬁcial jet samples\nwith diﬀerent numbers of constituents.\nWe train a standard transformer architecture on 600k Monte Carlo data to determine\nthe probability density for QCD and top-jets. By evaluating the likelihood ratio of the\ndensities for QCD and top-jets, we demonstrate that the transformer learns properties\nspeciﬁc to the physics of top and QCD jets. To assess the quality of the density estimation\nin more detail, we use the transformer to generate data samples from the learned probability\ndistributions. To suppress the sampling of extremely unlikely particle conﬁgurations, we\nutilize top-k sampling, a method commonly used in natural language processing.\nThe generated data samples exhibit impressive overall agreement with the data, as\ndemonstrated by the excellent agreement of distributions such as particle multiplicity or jet\nmass. Additionally, the generated samples are diﬃcult to distinguish from the data, even\nfor a highly powerful supervised classiﬁer, a feature which is hard to achieve with other\ngenerative models on low-level LHC data.\nOur analysis has shown that transformers can eﬀectively learn the probability densities\nof jets. Given their ability to process vast amounts of data, larger and more powerful\ntransformers can be trained on the enormous data sets generated so far at the LHC and\nexpected in the future.\nMoving forward, also our speciﬁc study on jets can be extended in several directions.\nOne interesting avenue is investigating how the transformer architecture can be generalized\nto continuous variables, either by turning to Gaussian mixture models or even in a more\nstraightforward way by increasing the particle dictionary to a size where the binning is\npractically irrelevant. Another promising direction is optimizing the hyperparameters and\nincreasing the training data to further improve the performance of the model. We plan to\npursue these questions in a future publication.\nAcknowledgments\nWe would like to thank Martin Grohe for discussions and Erik Buhmann, Gregor Kasieczka\nand David Shih for valuable comments and suggestions on the draft paper. TF is supported\nby the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under\ngrant 400140256 - GRK 2497: the physics of the heaviest particles at the Large Hadron\n– 11 –\nJHEP06(2023)184\nCollider. The research of MK and AM is supported by the Deutsche Forschungsgemein-\nschaft (DFG, German Research Foundation) under grant 396021762 - TRR 257 “Particle\nPhysics Phenomenology after the Higgs Discovery”. JT is supported by the Deutsche\nForschungsgemeinschaft (DFG, German Research Foundation) under grants GR 1492/16-1\nand KI 2348/1-1 “Quantitative Reasoning About Database Queries”. The authors gratefully\nacknowledge the computing time granted by the NHR4CES Resource Allocation Board\nand provided on the supercomputer CLAIX at RWTH Aachen University as part of the\nNHR4CES infrastructure. The calculations for this research were conducted with computing\nresources under the project rwth0934.\nA Further results\nA.1 Sampling QCD jets from the full distribution\nIn the main part of the paper, we present the results of sampling using the top-k method,\nwith a value ofk = 5000 . For more information, see section 2.1. In ﬁgure 7, we show\nthe results when sampling from the full probability distribution, as estimated by the\ntransformer. While the multiplicity and transverse momentum distributions, as well as the\nangular distributions, are almost identical to those obtained through top-k sampling, there\nis a slight oversampling of the multiplicity distribution in the overﬂow bin. Moreover, we\nobserve an unphysical tail at high jet masses, indicating that the transformer is oversampling\nvery unlikely particle conﬁgurations that it cannot adequately suppress.\nIn ﬁgure 8, we show the ROC curve obtained through a ParticleNet classiﬁer used to\ndiﬀerentiate between the samples from the full probability distribution and the data. While\nthe ROC curve appears similar to that of the top-5k samples for largeϵsample, there is a\nslightly larger fraction (about 5%) of sampled events that can be easily separated from\nthe data. With top-k sampling this fraction can be reduced to the 1% level, because less\nunphysical jets are sampled from the low-probability region where the transformer struggles\nto suppress these probabilities eﬃciently.\nA.2 Sampling top jets\nIn ﬁgure 9, we present distributions of various observables for sampled top jets. Compared\nto QCD jets, the multiplicity distribution of the data is somewhat less well-reproduced for\ntop jets. This is perhaps not surprising, as our training only includes 50 constituents, which\ncaptures only a fraction of the complexity of a typical top jet. However, the one-dimensional\ndistributions inlog(pT ), ∆ ηand mjet are well-reproduced, with a level of agreement between\nthe samples and data similar to that for QCD jets.\nFigure 10 shows the ROC curves for classiﬁcation between the top jet samples and\nthe data. Compared to ﬁgure 8 for QCD jets, we observe a slightly better separation\nbetween the samples and the data for top jets, with an AUC score of about 0.7 for the top-k\nresults. We also observe a wider spread in the results obtained by independent training,\nsampling and classiﬁcation, and a stronger inﬂuence of sampling from the 5k most probable\nparticles compared to QCD jets. These results suggest that more extensive training on\n– 12 –\nJHEP06(2023)184\n0 20 40 60 80 100\nMultiplicity\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\nNormalized distribution\nData\nTop-k\nFull\n−2 0 2 4 6\nlog(pT )\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nNormalized distribution\n−0.8 −0.6 −0.4 −0.2 0.0 0.2 0.4 0.6 0.8\n∆ η\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nNormalized distribution\n0 100 200 300 400\nmjet\n10−6\n10−5\n10−4\n10−3\n10−2\nNormalized distribution\nFigure 7. Comparison of data and sample distributions for QCD jets. As in ﬁgure 5, but now\nincluding the distributions obtained from sampling the full probability distribution (blue curve).\n0.0 0.2 0.4 0.6 0.8 1.0\nϵSamples\n100\n101\n102\n103\n104\n1/ϵData\nTop-k\nFull\nFigure 8. ROC curves for a ParticleNet classiﬁer to distinguish 200k generated QCD jet samples\nfrom the 200k jets of the test data set. As in ﬁgure 6, but now including the ROC curves obtained\nfrom sampling the full probability distribution (blue curve).\n– 13 –\nJHEP06(2023)184\n0 20 40 60 80 100\nMultiplicity\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nNormalized distribution\nData\nTop-k\nFull\n−2 0 2 4 6\nlog(pT )\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nNormalized distribution\n−0.8 −0.6 −0.4 −0.2 0.0 0.2 0.4 0.6 0.8\n∆ η\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nNormalized distribution\n0 100 200 300 400\nmjet\n10−6\n10−5\n10−4\n10−3\n10−2\n10−1\nNormalized distribution\nFigure 9. omparison of data and sample distributions in jet multiplicity,log(pT ), ∆ ηand the jet\nmass mjet for top jets. We show the distributions obtained from top-k and full sampling.\na larger data set is required to further improve the probability density of top jets, which\nhave an inherently more complex structure than QCD jets. We will address the need for\nmore comprehensive training, including a larger number of constituents and a much larger\ntraining data set in future research.\n– 14 –\nJHEP06(2023)184\n0.0 0.2 0.4 0.6 0.8 1.0\nϵSamples\n100\n101\n102\n103\n104\n1/ϵData\nTop-k\nFull\nFigure 10. ROC curves for a ParticleNet classiﬁer to distinguish 200k generated top jet samples\nfrom the 200k jets of the test data set. We show three ROC curves corresponding to independent\ntraining, sampling, and classiﬁcation for top-k and full sampling, respectively.\nOpen Access. This article is distributed under the terms of the Creative Commons\nAttribution License (CC-BY 4.0), which permits any use, distribution and reproduction in\nany medium, provided the original author(s) and source are credited. SCOAP3 supports\nthe goals of the International Year of Basic Sciences for Sustainable Development.\nReferences\n[1] M. Feickert and B. Nachman,A Living Review of Machine Learning for Particle Physics,\narXiv:2102.02770 [INSPIRE].\n[2] S. Badger et al.,Machine learning and LHC event generation, SciPost Phys.14 (2023) 079\n[arXiv:2203.07460] [INSPIRE].\n[3] I.J. Goodfellow et al.,Generative Adversarial Networks, arXiv:1406.2661 [INSPIRE].\n[4] D.J. Rezende and S. Mohamed,Variational Inference with Normalizing Flows,\narXiv:1505.05770.\n[5] G. Loaiza-Ganem, B.L. Ross, J.C. Cresswell and A.L. Caterini,Diagnosing and Fixing\nManifold Overﬁtting in Deep Generative Models, arXiv:2204.07172.\n[6] J.C. Cresswell et al.,CaloMan: Fast generation of calorimeter showers with density estimation\non learned manifolds, in the proceedings of the36th Conference on Neural Information\nProcessing Systems, New Orleans, U.S.A., 28 November – 9 December 2022\n[arXiv:2211.15380] [INSPIRE].\n[7] C. Krause and D. Shih,CaloFlow: Fast and Accurate Generation of Calorimeter Showers with\nNormalizing Flows, arXiv:2106.05285 [INSPIRE].\n– 15 –\nJHEP06(2023)184\n[8] C. Krause and D. Shih,CaloFlow II: Even Faster and Still Accurate Generation of\nCalorimeter Showers with Normalizing Flows, arXiv:2110.11377 [INSPIRE].\n[9] C. Krause, I. Pang and D. Shih,CaloFlow for CaloChallenge Dataset 1, arXiv:2210.14245\n[INSPIRE].\n[10] S. Diefenbacher et al.,L2LFlows: Generating High-Fidelity 3D Calorimeter Images,\narXiv:2302.11594 [INSPIRE].\n[11] A. Andreassen, I. Feige, C. Frye and M.D. Schwartz,JUNIPR: a Framework for Unsupervised\nMachine Learning in Particle Physics, Eur. Phys. J. C79 (2019) 102 [arXiv:1804.09720]\n[INSPIRE].\n[12] A. Andreassen, I. Feige, C. Frye and M.D. Schwartz,Binary JUNIPR: an interpretable\nprobabilistic model for discrimination, Phys. Rev. Lett.123 (2019) 182001\n[arXiv:1906.10137] [INSPIRE].\n[13] A. Vaswani et al.,Attention Is All You Need, Adv. Neural Inf. Process. Syst.30 (2017)\n[arXiv:1706.03762].\n[14] R. Fakoor, P. Chaudhari, J. Mueller and A.J. Smola,TraDE: Transformers for Density\nEstimation, arXiv:2004.02441.\n[15] T. Wolf et al.,HuggingFace’s Transformers: State-of-the-art Natural Language Processing,\narXiv:1910.03771.\n[16] V. Mikuni and F. Canelli,Point cloud transformers applied to collider physics, Mach. Learn.\nSci. Tech.2 (2021) 035027 [arXiv:2102.05073] [INSPIRE].\n[17] H. Qu, C. Li and S. Qian,Particle Transformer for Jet Tagging, arXiv:2202.03772 [INSPIRE].\n[18] S. Qiu et al.,Holistic approach to predicting top quark kinematic properties with the covariant\nparticle transformer, Phys. Rev. D107 (2023) 114029 [arXiv:2203.05687] [INSPIRE].\n[19] F.A. Di Bello et al.,Reconstructing particles in jets using set transformer and hypergraph\nprediction networks, arXiv:2212.01328 [INSPIRE].\n[20] B.M. Dillon et al.,Symmetries, safety, and self-supervision, SciPost Phys.12 (2022) 188\n[arXiv:2108.04253] [INSPIRE].\n[21] R. Kansal et al.,Evaluating generative models in high energy physics, Phys. Rev. D107 (2023)\n076017 [arXiv:2211.10295] [INSPIRE].\n[22] B. Käch, D. Krücker and I. Melzer-Pellmann,Point Cloud Generation using Transformer\nEncoders and Normalising Flows, arXiv:2211.13623 [INSPIRE].\n[23] M. Leigh et al.,PC-JeDi: Diﬀusion for Particle Cloud Generation in High Energy Physics,\narXiv:2303.05376 [INSPIRE].\n[24] E. Buhmann, G. Kasieczka and J. Thaler,EPiC-GAN: Equivariant Point Cloud Generation\nfor Particle Jets, arXiv:2301.08128 [INSPIRE].\n[25] A. Paszke et al.,PyTorch: An Imperative Style, High-Performance Deep Learning Library, in\nAdvances in Neural Information Processing Systems 32, Curran Associates, Inc. (2019),\np. 8024–8035.\n[26] D.P. Kingma and J. Ba,Adam: A Method for Stochastic Optimization, arXiv:1412.6980\n[INSPIRE].\n[27] A. Holtzman et al.,The Curious Case of Neural Text Degeneration, arXiv:1904.09751.\n– 16 –\nJHEP06(2023)184\n[28] G. Kasieczka, T. Plehn, J. Thompson and M. Russel,Top Quark Tagging Reference Dataset,\nDOI:10.5281/ZENODO.2603256.\n[29] G. Louppe, K. Cho, C. Becot and K. Cranmer,QCD-Aware Recursive Neural Networks for Jet\nPhysics, JHEP 01 (2019) 057 [arXiv:1702.00748] [INSPIRE].\n[30] H. Qu and L. Gouskos,ParticleNet: Jet Tagging via Particle Clouds, Phys. Rev. D101 (2020)\n056019 [arXiv:1902.08570] [INSPIRE].\n[31] A. Butter et al.,The Machine Learning landscape of top taggers, SciPost Phys.7 (2019) 014\n[arXiv:1902.09914] [INSPIRE].\n[32] B. Nachman and D. Shih,Anomaly Detection with Density Estimation, Phys. Rev. D101\n(2020) 075042 [arXiv:2001.04990] [INSPIRE].\n– 17 –"
}