{
  "title": "Exploring Dimensionality Reduction Techniques in Multilingual Transformers",
  "url": "https://openalex.org/W4307821210",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3168241018",
      "name": "Alvaro Huertas-Garcia",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2130044409",
      "name": "Alejandro Martín",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2907968039",
      "name": "Javier Huertas Tato",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2171177052",
      "name": "David Camacho",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A3168241018",
      "name": "Alvaro Huertas-Garcia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130044409",
      "name": "Alejandro Martín",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2907968039",
      "name": "Javier Huertas Tato",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2171177052",
      "name": "David Camacho",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3019166713",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4226516308",
    "https://openalex.org/W3144293453",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2969574947",
    "https://openalex.org/W2945830544",
    "https://openalex.org/W2024635814",
    "https://openalex.org/W1974051946",
    "https://openalex.org/W2584429674",
    "https://openalex.org/W3043684480",
    "https://openalex.org/W2910121883",
    "https://openalex.org/W3168641112",
    "https://openalex.org/W3118715115",
    "https://openalex.org/W4250903743",
    "https://openalex.org/W2970434686",
    "https://openalex.org/W2970172244",
    "https://openalex.org/W2951441396",
    "https://openalex.org/W4200546517",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W2950325582",
    "https://openalex.org/W2911627187",
    "https://openalex.org/W4210528010",
    "https://openalex.org/W2793927960",
    "https://openalex.org/W1493357981",
    "https://openalex.org/W2621404689",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4252222626",
    "https://openalex.org/W4234647355",
    "https://openalex.org/W4210269687",
    "https://openalex.org/W3138900755",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W3147854607",
    "https://openalex.org/W3091222518",
    "https://openalex.org/W2903613410",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2294798173",
    "https://openalex.org/W2295124130",
    "https://openalex.org/W3137162929",
    "https://openalex.org/W3097095909",
    "https://openalex.org/W3204903975",
    "https://openalex.org/W2911293880",
    "https://openalex.org/W3217467900",
    "https://openalex.org/W3100107515",
    "https://openalex.org/W2085030399",
    "https://openalex.org/W3030685711",
    "https://openalex.org/W3036910473",
    "https://openalex.org/W3209749143",
    "https://openalex.org/W6931636349",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W2108995755",
    "https://openalex.org/W2139047213",
    "https://openalex.org/W1976137296",
    "https://openalex.org/W2140095548",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4288028050",
    "https://openalex.org/W3192478068",
    "https://openalex.org/W3039695075",
    "https://openalex.org/W2598789561",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2110816042",
    "https://openalex.org/W2168176982",
    "https://openalex.org/W2166078937",
    "https://openalex.org/W2916020270",
    "https://openalex.org/W4253525579",
    "https://openalex.org/W1506806321",
    "https://openalex.org/W3100321043",
    "https://openalex.org/W3104033643"
  ],
  "abstract": "Abstract In scientific literature and industry, semantic and context-aware Natural Language Processing-based solutions have been gaining importance in recent years. The possibilities and performance shown by these models when dealing with complex Human Language Understanding tasks are unquestionable, from conversational agents to the fight against disinformation in social networks. In addition, considerable attention is also being paid to developing multilingual models to tackle the language bottleneck. An increase in size has accompanied the growing need to provide more complex models implementing all these features without being conservative in the number of dimensions required. This paper aims to provide a comprehensive account of the impact of a wide variety of dimensional reduction techniques on the performance of different state-of-the-art multilingual siamese transformers, including unsupervised dimensional reduction techniques such as linear and nonlinear feature extraction, feature selection, and manifold techniques. In order to evaluate the effects of these techniques, we considered the multilingual extended version of Semantic Textual Similarity Benchmark (mSTSb) and two different baseline approaches, one using the embeddings from the pre-trained version of five models and another using their fine-tuned STS version. The results evidence that it is possible to achieve an average reduction of $$91.58\\% \\pm 2.59\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mn>91.58</mml:mn> <mml:mo>%</mml:mo> <mml:mo>±</mml:mo> <mml:mn>2.59</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> in the number of dimensions of embeddings from pre-trained models requiring a fitting time $$96.68\\% \\pm 0.68\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mn>96.68</mml:mn> <mml:mo>%</mml:mo> <mml:mo>±</mml:mo> <mml:mn>0.68</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> faster than the fine-tuning process. Besides, we achieve $$54.65\\% \\pm 32.20\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mn>54.65</mml:mn> <mml:mo>%</mml:mo> <mml:mo>±</mml:mo> <mml:mn>32.20</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> dimensionality reduction in embeddings from fine-tuned models. The results of this study will significantly contribute to the understanding of how different tuning approaches affect performance on semantic-aware tasks and how dimensional reduction techniques deal with the high-dimensional embeddings computed for the STS task and their potential for other highly demanding NLP tasks.",
  "full_text": "https://doi.org/10.1007/s12559-022-10066-8\nExploring Dimensionality Reduction Techniques in Multilingual \nTransformers\nÁlvaro Huertas‑García1  · Alejandro Martín1 · Javier Huertas‑Tato1 · David Camacho1\nReceived: 20 May 2022 / Accepted: 5 October 2022 \n© The Author(s) 2022\nAbstract\nIn scientific literature and industry, semantic and context-aware Natural Language Processing-based solutions have been \ngaining importance in recent years. The possibilities and performance shown by these models when dealing with complex \nHuman Language Understanding tasks are unquestionable, from conversational agents to the fight against disinformation in \nsocial networks. In addition, considerable attention is also being paid to developing multilingual models to tackle the lan -\nguage bottleneck. An increase in size has accompanied the growing need to provide more complex models implementing all \nthese features without being conservative in the number of dimensions required. This paper aims to provide a comprehensive \naccount of the impact of a wide variety of dimensional reduction techniques on the performance of different state-of-the-art \nmultilingual siamese transformers, including unsupervised dimensional reduction techniques such as linear and nonlinear \nfeature extraction, feature selection, and manifold techniques. In order to evaluate the effects of these techniques, we con -\nsidered the multilingual extended version of Semantic Textual Similarity Benchmark (mSTSb) and two different baseline \napproaches, one using the embeddings from the pre-trained version of five models and another using their fine-tuned STS \nversion. The results evidence that it is possible to achieve an average reduction of 91.58%± 2.59% in the number of dimen-\nsions of embeddings from pre-trained models requiring a fitting time 96.68%± 0.68% faster than the fine-tuning process. \nBesides, we achieve 54.65%± 32.20% dimensionality reduction in embeddings from fine-tuned models. The results of this \nstudy will significantly contribute to the understanding of how different tuning approaches affect performance on semantic-\naware tasks and how dimensional reduction techniques deal with the high-dimensional embeddings computed for the STS \ntask and their potential for other highly demanding NLP tasks.\nKeywords Dimensionality reduction · Natural language processing · Semantic textual similarity · Multilingual \ntransformers · Language models\nIntroduction\nNatural Language Processing (NLP) includes various disci-\nplines that provide a system with the ability to process and \ninterpret natural language, just like humans use language as \na communication and reasoning tool [ 1]. Due to the recent \nincreases in computational power, parallelisation, the avail-\nability of large data sets, and recent advances in Artificial \nIntelligence, especially in the Machine Learning research \nfield, NLP has been steadily proliferating and has garnered \nimmense interest [ 1, 2]. In recent years, transformer-based \narchitectures [3] have become an indispensable staple in the \nNLP field. Transformer models can capture latent syntactic-\nsemantic information and encode text’s meaning as con -\ntextual vectors in a high-dimensional space referred to as \nembeddings [2, 4]. In contrast to previous approaches, such \nas Statistical Natural Language Processing, using the atten-\ntion mechanism provided by these architectures allows us to \nconsider many characteristics involved in human language.\nThe tremendous power of transformer-based models in \nNatural Language Understanding (NLU) and the new models \n * Álvaro Huertas-García \n alvaro.huertas.garcia@upm.es\n * David Camacho \n david.camacho@upm.es\n Alejandro Martín \n alejandro.martin@upm.es\n Javier Huertas-Tato \n javier.huertas.tato@upm.es\n1 Departamento de Sistemas Informáticos, Universidad \nPolitécnica de Madrid, Madrid, Spain\n/ Published online: 29 October 2022\nCognitive Computation (2023) 15:590–612\n1 3\nthat are continuously being proposed allows us to improve \nthe state-of-the-art results in varied NLP tasks dramatically, \nincluding question answering, sentence classification, and \nsentence-pair regression like Semantic Textual Similarity \n(STS) [2, 4–6]. The semantic evaluation performed in this \nSTS task is one of the “levels of language” that determines \nthe possible meanings of a sentence by focusing on the \ninteractions among word-level [7]. Given the many different \nlinguistic features involved, it entails a high degree of com-\nplexity. In STS tasks, the systems must compute how similar \ntwo sentences are considering all these features, returning a \nsimilarity score, usually ranging between 0 and 5 [8].\nRegarding sentence-pair regression tasks, to overcome \nthe massive computational overhead caused by the quadratic \ndependence on the input size of the attention mechanism in \ntransformers models [5, 9], the use of siamese architectures \nis a very effective method for efficiently deriving semanti -\ncally meaningful sentence embeddings [5, 9]. This approach \nis also called dual-encoder, bi-encoder or siamese architec -\ntures. As explained by Humeau et al. [ 9], the training of a \nsiamese architecture consists of two pre-trained transformer-\nbased models with tied weights that can be fine-tuned for \na specific task like computing separately semantic embed -\ndings for a pair of sentences and measure their similarity \nusing the extensively used cosine similarity function [ 10].\nDespite the practical features of cosine similarity, such as \nsymmetry and spatial interpretation, this similarity metric \nhas complexity O(N): time and memory grow linearly with \nthe number of dimensions of the vectors compared [ 11]. \nThus, dimensionality is a bottleneck for similarity compu -\ntation and embedding storage [12]. Moreover, an increasing \nnumber of studies using ensemble approaches based on the \nconcatenation of embeddings can be found in the litera -\nture [13–15], aiming to improve the results in state-of-the-art \ntasks but accentuating this issue. Given that the application \nof dimensionality reduction techniques can mitigate this bot-\ntleneck, it requires further exploration.\nAlthough in the history of NLP, the focus has mainly been \non proposing architectures for English tasks. Nevertheless, \ninterest in developing multilingual NLP tools has grown \nrecently to achieve diversity for transferring NLP capabilities \nto low-resource languages for which labelled (and unlabelled) \ndata is scarce [16, 17]. The incorporation of tasks in various \nlanguages in the SemEval [18] and CLEF [19] competitions \nis clear evidence of this, surmounting the language bottleneck, \nbut also the increasing number of newly introduced multilin-\ngual models [19], such as the recent BLOOM (BigScience \nLanguage Open-science Open-access Multilingual) [ 20] \nopen-sourced model with 176B parameters generated by Big-\nScience which has marked an inflexion point on the research \nand development of large language models (LLMs).\nThe present paper seeks to address how different dimen-\nsionality reduction techniques impact the performance of \npre-computed embeddings from multilingual transformer-\nbased models trained using a siamese architecture focus -\ning on semantic similarity by employing four different \napproaches where the reduction techniques are com -\npared with the pre-trained and fine-tuned versions of the  \nmodels.\nThis research aims to expand the current knowledge fur-\nther on the effect of dimensionality reduction techniques in \nNatural Language Processing [ 21–24]. The following are \nthe new contributions of our research concerning previous \nstudies:\n• A more comprehensive range of unsupervised dimension \nreduction techniques is included, from linear and nonlin-\near feature extraction to feature selection and manifold \ntechniques.\n• The effect of these techniques on pre-trained and fine-\ntuned pre-computed embeddings in the Semantic Textual \nSimilarity (STS) task is explored.\n• In contrast to previous work that explored reduction tech-\nniques in classical static word embeddings, this paper \ninvestigates the effect of dimension reduction in state-\nof-the-art contextual-based transformer models.\n• Unlike previous works focused on the English language, \nthis research analyses multilingual models, overcoming \nthe language bottleneck for the applicability of dimen -\nsion reduction of the embeddings of these models.\nThe remainder of the article is organised as follows: the \n“Related Work’’ section outlines previous work on dimen -\nsionality reduction techniques, their application in Deep \nLearning, and the importance of multilingual semantics in \nNLP. The “Methodology’’ section describes the approaches \nfollowed in this research to evaluate the impact of dimen -\nsionality reduction techniques, the multilingual models \nand the dimensionality reduction techniques applied, and \nthe “Experimental Setup ’’ section the data and processes \nused to fit and evaluate these techniques. The experimental \nresults are discussed in the “ Results’’ section. Finally, the \n“Conclusion’’ section summarises the results of this work \nand concludes.\nRelated Work\nDimensionality Reduction Techniques\nDimensionality reduction techniques aim to reduce the \ndimensionality of the data, removing irrelevant and redun -\ndant features while preserving critical information for sub -\nsequent applications, such as facilitating their visualisation \nand understanding or leading to more compact models with \nbetter generalisation ability [25–27].\n591Cognitive Computation  (2023) 15:590–612\n1 3\nThere are different non-mutual exclusive criteria to clas-\nsify dimensionality reduction techniques. Firstly, according \nto the reduction approach, these techniques can be classified \ninto feature selection and feature extraction techniques [28]. \nFeature selection involves selecting a subset of original fea-\ntures useful for building models, effectively removing some \nof the less relevant features from consideration [26, 29]. On \nthe other hand, feature extraction transforms the original data \ninto another feature space with specific criteria, creating new \nvariables by combining information from the original vari -\nables that capture much of the information contained in those \noriginal variables [ 25, 28]. Additionally, feature extraction \nmethods can be further subdivided into linear and nonlinear \naccording to the variable combinations applied [30].\nSecondly, according to the information available in the \ndatasets, dimensionality reduction techniques can be classi-\nfied as supervised and unsupervised [25, 26]. Supervised tech-\nniques require each data instance in the dataset to be labelled \naccordingly to the task, whereas unsupervised techniques are \ntask agnostic approaches and do not require labelled data.\nDimensional Reduction of Embeddings\nBroadly, embeddings can be categorised as pre-trained or \ndownstream fine-tuned embeddings [31], and pre-computed \nor on-the-fly embeddings [32, 33].\nThe first criterion to categorise embeddings is whether \nthey come from pre-trained models for general tasks or are \ntask-specific. Pre-trained embeddings are widely used as a \nstarting point for downstream applications. A clear exam -\nple is transformer models’ current ‘Pre-train and Fine-\ntune’ paradigm [ 31]. Training these models from scratch \nis prohibitively expensive in many cases. Alternatively, \nusing self-supervised trained checkpoints of these models \nand their pre-trained embeddings as a starting point to be \nlater fine-tuned for supervised downstream tasks is widely \nused. Unlike previous works in the literature that have only \nfocused on reduced pre-trained embeddings [21–23], in this \nwork, we are interested in evaluating the impact of dimen -\nsionality reduction on both types of embeddings, pre-trained \nand downstream fine-tuned embeddings.\nPre-computed embeddings in NLP are widespread, i.e. \nembeddings that may or may not be adjusted to a task but are \ngenerated beforehand and not at each use time. A straight -\nforward application of pre-computed embeddings is the \nsemantic search NLP task for locating relevant information \nfrom massive amounts of text data [ 34]. A semantic search \ntask uses semantic textual similarity to compare an input \ntext against a large set of texts from a database to extract \nrelevant related information-typically, a distance metric such \nas cosine similarity ranks which content should be extracted \nto an input query. However, computing the database \nembeddings each time a query is introduced is infeasible. \nAlternatively, it is preferable to compute the embeddings \nonce, store them, and use these pre-computed embeddings \nfor subsequent requests [33]. With this in mind, it is essen -\ntial to note the usefulness of reducing embedding dimen -\nsions, which can improve their utility in memory-constrained \ndevices and benefit several real-world applications.\nAs Camastra and Vinciarelli mention [ 35], using more \nfeatures than is strictly necessary leads to several problems, \npointing out that one of the main problems was the space \nneeded to store the data. As the amount of available infor -\nmation increases, the compression for storage becomes even \nmore critical [12, 36, 37]. Additionally, for the scope of this \nwork, it cannot be ignored that the application of dimensional \nreduction techniques for reducing pre-computed embedding \ndimensions neither improves the runtime nor the memory \nrequirement for running the models. It only diminishes the \nneeded space to store embeddings. Besides, it increases the \nspeed of making computations (i.e. to calculate the cosine \nsimilarity between two vectors), which also contributes to \ndecreasing the considerable impact on the energy and carbon \nfootprints generated during the production use of the models \nwhen pre-computed and stored embeddings are required [38]. \nResearch has tended to focus on implementing bigger and \nmore complex models rather than analysing methods to \nadjust the vector space to the desired task while conserv -\ning the required dimensions [38]. The additional problem is \nthat the storage of high-dimensional embeddings is challeng-\ning when dealing with large volume datasets [ 33]; besides, \nresearchers have already raised awareness of the storage limi-\ntation we are about to face if current technology is adopted \nand the storage utilisation growth rate persists [12].\nIn terms of performance, dimensionality reduction tech -\nniques can also contribute. The performance of Machine \nLearning (ML) and, in particular, Deep Learning (DL) models \nis highly dependent on the choice of data representation (or \nfeatures) to which they are applied [39]. For that reason, much \neffort during the deployment of ML and DL solutions is dedi-\ncated to obtaining a data representation that can support effec-\ntive learning. As a result, different fields where dimension \nreduction techniques are combined with ML and DL complex \nmodels can be found in the literature. For example, from their \napplication in time series forecasting [40] or health sciences \nfor cancer classification [41] to Natural Language Processing \n(NLP) to improve the clustering of text documents [ 42] or \nsentiment classification of opinion texts [17].\nRecently, the study of Raunak et al. [ 21, 22] has shed \nmore light on the importance of reducing the size of embed-\ndings produced by ML and DL models in the field of NLP. \nMore specifically, these authors focus on reducing the size \nof classical GloVe [43] and FastText [44] pre-trained word \nembeddings using PCA-based post-processing algorithms, \nachieving similar or even better performance than the origi-\nnal embeddings.\n592 Cognitive Computation  (2023) 15:590–612\n1 3\nOther works on the potential of reducing pre-computed \nembeddings dimensions have been carried out [23], explor-\ning the effect of Principal Components Analysis (PCA) [45, \n46] and Latent Semantic Analysis (LSA) [24] dimensionality \nreduction techniques as a post-processing step of pre-trained \nGloVe word embeddings for text classification tasks. These \nauthors also corroborated the usefulness of PCA for obtain-\ning more accurate results with lower computational costs \nconcluding that the PCA method is more suitable than LSA \nfor dimensionality reduction. In the same way, Shimomoto \net al. [47] propose solving topic classification and sentiment \nanalysis by using PCA to transform pre-computed word \nembeddings of a text into a linear subspace. Their results \nshowed the effectiveness of using the PCA subspace repre -\nsentation for text classification. Other authors have already \nproved this fact [ 48], showing that the storage, memory, \nand computation required by these large embeddings typi -\ncally results in low efficiency in NLP tasks, pointing out \nthe importance of methods to compress pre-computed and \npre-trained GloVe and FastText embeddings.\nAdditionally, researchers have explored dimensional \nreduction techniques for visualising semantic embeddings \nof small text corpora [49]. The authors explored four dimen-\nsion reduction strategies on pre-computed embeddings based \non PCA and t-distributed stochastic neighbour embedding \n(t-SNE) [ 50], concluding that both methods preserved \na significant amount of semantic information in the full \nembedding.\nSimilarly, the use of dimension reduction techniques is \nlikewise interesting in Semantic Similarity [27, 37]. As dis-\ncussed previously, in the Semantic Similarity task, the linear \nO(N) complexity of cosine similarity is one of the reasons \nwhy this distance metric is widely used in the community \nand this study. However, the complexity of cosine similar -\nity, although linear, is a limitation when dealing with many \ndimensions and a large amount of data. Precisely, in tasks \nwhere cosine similarity is used as a distance metric, the large \namount of data handled and the high number of dimensions \nof the embeddings generated by the models used represent a \nconstraint in both computational time and storage that leaves \nthe door open to the use of dimension reduction techniques.\nTo the best of our knowledge, in the literature, dimension \nreduction research on embeddings has focused on statistical \nmethods, such as Bag of Words and Term Frequency-Inverse \nDocument Frequency (TF-IDF) [ 27, 37], and classical pre-\ncomputed word embeddings, including the popular GloVe or \nFastText embeddings [21–24, 36, 49]. These classical word \nembeddings are more complex and powerful than statistical \nmethods. However, they are static, word-level, and contex -\ntual independent, and their main limitation is that they do not \nconsider what context the word is being used. Moreover, the \nvaried embedding techniques explored are limited, focusing \nmainly on PCA. Nevertheless, it should be mentioned that \nnovel dimension reduction approaches based on this clas -\nsical technique [27] and feature selection methods [ 37] are \nbeing proposed. In fact this proves the relevance and impor-\ntance of further exploring embedding dimension reduction. \nLikewise, these studies do not include multilingualism in \ntheir analyses, being limited to the English language.\nHence, the presented study follows the research line pro-\nposed by different authors [21–24] but takes a step forward, \nincluding a broader range of techniques and evaluating the \ncapability of dimensionality reduction techniques in both \npre-trained and fine-tuned pre-computed embeddings  \nfrom state-of-the-art contextual-based transformer models \nfrom the recently claimed multilingual point of view.\nSiamese and Non‑Siamese Architectures\nTraining in non-siamese transformer architectures, known \nas cross encoders, requires that two sentences be passed to \nthe transformer as input while a target value is predicted [9]. \nThis approach requires feeding the model with both sen -\ntences, which causes massive computational overhead \nbecause the attention is quadratic to the sequence length. \nAs previously described in the “ Introduction’’ section, sia-\nmese architectures are the main alternative to cope with this \nmassive computational overhead in transformers models. \nAs Reimers and Gurevych [ 5] measured, in a V100 GPU, \nfinding the pair with the highest similarity in a collection of \nn = 10000 sentences for a non-siamese model requires about \n65 h. On the other hand, the same task with a model trained \nwith a siamese architecture is reduced to ∼ 5 s.\nAlthough it should also be noted that models trained in a \nnon-siamese way generally obtain better results [ 51] since \nthey generate the embeddings considering the interaction \nof both sentences, the type of architecture only affects the \ncomputational time to obtain the embeddings. Consequently, \nthe architecture type does not affect the size of the computed \nembeddings, which makes dimension reduction techniques \nequally interesting and valuable for both architectures.\nTherefore, it is noteworthy that the present research pays \nspecial attention to the dimension reduction of the embed -\ndings of transformers models trained with siamese architec-\nture because, in Semantic Textual Similarity (STS) tasks, \nthey are a widely used solution that obtains results in a rea-\nsonable time and at a feasible computational cost. However, \nthese reduction techniques are equally helpful for other types \nof architecture.\nImportance of Multilingual Semantics\nSemantics has many applications in a wide range of domains \nand tasks. Recent developments regarding Information \nRetrieval tasks [34, 51, 52] have demonstrated the potential \nof combining semantic-aware models along with traditional \n593Cognitive Computation  (2023) 15:590–612\n1 3\nbaseline algorithms (e.g. BM25) [ 53]. Moreover, the use \nof semantic-aware models has proven to be an excellent \napproach to counteract informational disorders (i.e. misinfor-\nmation, disinformation, malinformation, misleading infor -\nmation, or any other kind of information pollution) [54–57] \nor to build automated fact-checking approaches [58]. Seman-\ntic similarity can be applied to organise data according to \ntext properties, formally an unsupervised thematic analy -\nsis [59]. Following the same criterion, the semantic similar-\nity measurement between a sentence and each word can be \napplied to extract the keywords with the highest semantic \ncontent from the sentence [ 60]. All these applications rely \non measuring semantic textual similarity (STS), making STS \na crucial task in NLP.\nThe language bottleneck is a significant limitation of \nthese semantic-aware solutions [ 61]. Language constitutes \none of the most significant barriers to be addressed since \na model’s ability to handle multiple languages is essential \nfor its widespread applications. In recent years, in the field \nof NLP, attention has been paid to multilingual models to \nachieve diversity in transferring NLP capabilities to low-\nresource languages [ 16, 18, 19]. Therefore, in this paper, \nspecial attention has been paid to multilingual transformers \nmodels, with the immediate goal of covering embedding \ndimensionality reduction in the semantic textual similar -\nity task in the world’s most widely spoken languages, even \nthough these techniques are relevant for monolingual and \nmultilingual transformers.\nAltogether, this work aims to broaden our knowledge of \nsemantic-aware transformer-based models by analysing the \nimpact of different dimensionality reduction techniques on \nthe performance of multilingual siamese transformers on \nsemantic textual similarity multilingual tasks. The results of \nthis study will contribute significantly to understanding how \ndifferent tuning approaches affect performance on semantic-\naware tasks and how dimensional reduction techniques deal \nwith the high-dimensional embeddings computed for the \nSTS task from the recently claimed multilingual point of \nview.\nMethodology\nThe main goal of this research lies in providing a deep \nanalysis of the use of different dimensionality reduction \ntechniques to reduce the size of the output embedding of \ndifferent multilingual transformer models and their impact \non performance.\nThe following subsections describe in detail the tech -\nniques explored to reduce embeddings, present the multi -\nlingual transformer models included in this study and the \ndifferent approaches applied to quantify the reduction mar -\ngin and its effect on the performance.\nDimensionality Reduction Techniques\nAs previously mentioned, dimensionality reduction tech -\nniques can be grouped according to two non-mutual \nexclusive criteria. This paper includes a range of types of \ndimensionality reduction techniques, including linear and \nnonlinear feature extraction and feature selection techniques. \nNevertheless, since the transformers models included in this \nstudy are employed in a siamese architecture to determine \nthe degree of similarity between a pair of sentences, they \noutput a pair of non-concatenative embeddings between \nwhich the similarity is estimated using the cosine distance. \nHence, for each labelled similarity score, there are two non-\nconcatenative embeddings. For this reason, even though \nwe have labelled the data, only unsupervised methods are \nexplored.\nThe dimensionality reduction techniques explored in this \nproject are:\n• Principal Component Analysis (PCA): Principal Com-\nponent Analysis [45, 46] is a powerful unsupervised lin-\near feature extraction technique that computes a set of \northogonal directions from the covariance matrix that \ncapture most of the variance in the data [ 62]. This is, it \ncreates new uncorrelated variables that maximise vari -\nance, and at the same time, most existing structure in \nthe data is retained. It is also important to note that this \nresearch uses a variant of PCA known as Incremental \nPrincipal Components Analysis (IPCA) [ 63]. This vari-\nant follows the same basic principles as PCA. However, \nit is much more memory efficient, as it applies PCA in \nbatches, avoiding storing entire data in memory and \nallowing PCA to be applied on large datasets.\n• Independent Component Analysis (ICA)  [64]: Inde-\npendent Component Analysis is an unsupervised feature \nextraction probabilistic method for learning a linear trans-\nformation to find components that are maximally inde -\npendent between them and non-Gaussian (non-normal), \nbut at the same time, they jointly maximise mutual infor-\nmation with the original feature space.\n• Kernel Principal Components Analysis (KPCA) [65]: \nKernel-based learning method for PCA. It uses kernel \nfunctions to construct a nonlinear version of the PCA \nlinear algorithm by first implicitly mapping the data into \na nonlinear feature space and then performing linear PCA \non the mapped patterns [ 62]. The kernels considered in \nthis project are the Polynomial, Gaussian RBF, Hyper -\nbolic Tangent (Sigmoid), and Cosine kernels.\n• Variance Threshold : Unsupervised feature selection \napproach that removes all features with a variance below \na threshold. Indeed, this technique selects a subset of fea-\ntures with large variances, considered more informative, \nwithout considering the desired outputs.\n594 Cognitive Computation  (2023) 15:590–612\n1 3\n• Uniform Manifold Approximation and Projection \nfor Dimension Reduction (UMAP) : The authors of \nUMAP [66] describe it as an algorithm that can be used \nfor unsupervised dimension reduction based on mani -\nfold learning techniques and topological data analysis. \nIn short, it first embeds data points in a new nonlinear \nfuzzy topological representation using neighbour graphs. \nSecondly, it learns a low-dimensional representation that \npreserves the complete information of this space, mini -\nmising Cross-Entropy. Compared to its counterparts, \nsuch as t-SNE, UMAP is fast, scalable, and allows bet -\nter control of the desired balance between the local and \nglobal structure to be preserved. Two main parameters \nplay a vital role in controlling this: (1) the number of \nsample points that defines a local neighbourhood in the \nfirst step, and (2) the minimum distance between embed-\nded points in low-dimensional space to be clustered in \nthe second step. Larger values of the number of neigh -\nbours tend to preserve more global information in the \nmanifold as UMAP has to consider more prominent \nneighbourhoods to embed a point. Likewise, larger mini-\nmum distance values prevent UMAP from packing points \ntogether and preserving the overall topological structure.\nAccording to the previous preprocessing steps required \nbefore dimensionality techniques, it should be noted that \nPCA and KPCA assume a Gaussian distribution, and the \nfeatures must be normalised; otherwise, the variances will \nnot be comparable. Therefore, the StandardScaler is applied \nbeforehand. Regarding ICA, non-Gaussian distribution is \nassumed, and the data is already withened by the algo -\nrithm, so no previous preprocessing step is necessary. For \nthe Variance Threshold, the best standardisation method \nis MinMaxScaler, as it transforms all features to the same \nscale but does not alter the initial variability. This allows \nthe variance selection threshold set to affect all dimensions \nequally. Finally, since there are no Gaussian assumptions \nunder UMAP and the cosine distance calculation benefits \nfrom scaling the features to a given range, MinMaxScaler is \napplied before UMAP. A summary of the necessary consid-\nerations about the above scaling steps and the characteristics \nof the dimensionality reduction techniques applied in this \nproject are listed in Table 1.\nFinally, it is worth mentioning that these dimensionality \nreduction techniques and preprocessing algorithms come \nfrom scikit-learn v1.0.2  [ 67], except for UMAP, which \nbelongs to umap-learn  v0.5.2 [ 66]. For the sake of repro -\nducibility, the different parameters and values used in the \nexperiments are presented in Table  2. Finally, the variance \nthreshold filters for the Variance Threshold technique tested \nare extracted by previously calculating the variance of each \nfeature (i.e. 768 variances for 768 embedding dimensions), \nextracting the deciles, and including the maximum and mini-\nmum of these variances.\nMultilingual Models\nRegarding the models included, we have tried to include \nthe open-source architectures widely used by the NLP com-\nmunity, such as the classic BERT and its robust version of \nmultilingual nature, such as XLM-RoBERTa. The effects \nof dimensionality reduction and fine-tuning process were \nexplored in the following pre-trained multilingual models \nextracted from Hugging Face [68]:\n• bert-base-multilingual-cased : BERT [4] transformer \nmodel pre-trained on a large corpus of 104 languages \nWikipedia articles using the self-supervised masked \nlanguage modelling (MLM) objective with ∼177M \nparameters.\nTable 1  Considerations about the previous scaling steps and the char -\nacteristics of the different dimensionality reduction techniques applied \nin this project\nPCA KPCA ICA Variance \nThreshold\nUMAP\nPreprocessor Standard Standard MinMax MinMax\nScalation ✘ ✘ ✘ ✘ ✘\nNormalisation ✘ ✘\nUnsupervised ✘ ✘ ✘ ✘ ✘\nFeature Selection ✘\nFeature Extraction ✘ ✘ ✘ ✘\nLinear ✘ ✘\nNon Linear ✘ ✘\nTable 2  Parameters with non-default values used in the previous scal-\ning steps and the dimensionality reduction techniques applied in this \nproject\nTechnique Parameters\nICA random_state = 0\nmax_iter = 320\nwhiten = True\ntol = 5e-4\nKPCA kernels = [sigmoid, polynomial, rbf, cosine]\neigen_solver = arpack\ncopy_X = False\nrandom_state = 0\nVariance Threshold threshold = [Min, Max, Decile of variance]\nUMAP pre-computed_knn = True\nmetric = cosine\nmin_dist = 1\nn_neighbors = [5, 10, 50, 100, 125]\nangular_rp_forest = True\n595Cognitive Computation  (2023) 15:590–612\n1 3\n• distilbert-base-multilingual-cased : Distilled version \nof the previous model, being on average twice as fast \nas this model, totalizing ∼134M parameters [ 69].\n• xlm-roberta-base : Base-sized XLM-RoBERTa [ 70] \nmodel totalizing ∼125M parameters. XLM-RoBERTa is \nRoBERTa model [ 71], a robust version of BERT, pre-\ntrained on CommonCrawl data containing 100 languages.\n• xlm-roberta-large : Large-sized XLM-RoBERTa [ 70] \nmodel totalizing ∼355M parameters.\n• LaBSE : Language-agnostic BERT Sentence Embed -\nding [ 72] model trained for encoding and reducing \nthe cosine distance between translation pairs with a \nsiamese architecture based on BERT, a task related to \nsemantic similarity. It trained over 6 billion translation \npairs for 109 languages. The authors also reported that \nit has zero-shot capabilities, producing decent results \nfor other not seen languages.\nTransformer-based models embed textual information into \nvectors of high dimensionality. The pre-trained multilingual \nmodels in this study generate embeddings with 768 dimen -\nsions and 1024 in the case of xlm-roberta-large. These default \ndimensions are considered to be reduced since a different \nnumber of dimensions from the default would entail losing the \nknowledge acquired during the self-supervised pre-training \nphase (e.g. mask word prediction), in which we are interested \nin studying the effects of the dimension reduction techniques.\nEvaluation Approaches\nWe have followed four approaches to evaluate and quantify the \nreduction margin and its effect on performance using different \nmethodologies. These four approaches are shown in Fig. 1.\nWe have included two different baseline approaches, one \nusing the embeddings from the pre-trained version of the \nfive previously mentioned models (i.e. Approach 1) and \nanother using their fine-tuned STS version (i.e. Approach  \n2). These baselines will allow us to have a standard to exam-\nine the dimensionality reduction capability and its effect on \nSTS task performance.\nAn approach that applies dimensionality reduction to \nthe generated embeddings is used for each baseline. Con -\nsequently, Approach 3 applies dimension reduction using \nApproach 1 as the baseline. Similarly, Approach 4 uses \nApproach 2 as the baseline.\nThese four approaches are described as follows:\n• Approach 1 — Pre-trained models. In the first approach, \nwe employ and directly evaluate the pre-trained models in \nthe mSTSb test split without applying any dimensional -\nity reduction. This approach is used as the baseline for \nApproach 3.\n• Approach 2 — Fine-tuned models . In this second \napproach, the pre-trained models are fine-tuned down -\nstream using the mSTSb train split and evaluated in the \nmSTSb test split without applying any dimensionality \nreduction technique. This approach is used as the base -\nline for Approach 4. The fine-tuning process will be dis-\ncussed in more detail in the “Transformers Fine-tuning’’ \nsection.\n• Approach 3 — Reduced embeddings from pre-trained \nmodels. In this approach, the embeddings generated by \nthe pre-trained models from Approach 1 in the mSTSb \ntrain split are used to fit the different dimension reduc -\ntion techniques and evaluate them in the mSTSb test \nsplit. Thus, an analysis between the results achieved in \nFig. 1  R epresentation of the \napproaches followed to evaluate \nthe impact of different dimen-\nsionality reduction techniques \nin multilingual transform-\ners. Where T represents the \npre-trained transformer model, \nT’ the fine-tuned transformer \nmodel, R’ the fitted dimension-\nality reduction technique, Eo the \noriginal output embedding, and \nEr the reduced embedding\n596 Cognitive Computation  (2023) 15:590–612\n1 3\nApproach 1 and Approach 3 will help to understand the \nimpact of dimensionality reduction techniques in the \nembeddings from pre-trained models.\n• Approach 4 — Reduced embeddings from fine-tuned \nmodels. This approach is equivalent to Approach 3 but \nuses the fine-tuned models in Approach 2, allowing us to \nassess the impact of dimensionality reduction techniques \nin fine-tuned embeddings.\nThe experimental design of these approaches will be \ndiscussed in more detail in the “ Baseline Approaches: \nApproach 1 and Approach 2’’ and “Dimensionality Reduced \nTechniques Fitting: Approach 3 and Approach 4’’ sections.\nExperimental Setup\nData\nThe multilingual extended STS Benchmark (mSTSb) [ 51] \ntrain set is used for fine-tuning the multilingual transform -\ners and fitting the variety of dimensional reduction tech -\nniques. This split comprises 16 languages 1 combined in 31 \nmono- and cross-lingual tasks with 5, 479 pairs of sentences \neach one. Likewise, the mSTSb test set is used to evaluate \nthe performance of the models obtained from the different \napproaches. The mSTSb test set comprises 31 multilingual \ntasks with 1, 379 pairs of sentences per task.\nTo evaluate the performance in mSTSb, the sentence \nembeddings for each pair of sentences are computed and \nthe semantic similarity is measured using the cosine simi -\nlarity metric. Then, the Spearman correlation coefficient (  /u1D70C \nor rs ) i s computed between the scores obtained and the gold \nstandard scores, as it is recognised as an official metric used \nfor semantic textual similarity tasks [73, 74].\nIt is important to note that the mSTSb data variety avail-\nable for fitting (i.e. train split) totals +183 k sentences (i.e. \n16 languages with 5, 749 pairs of sentences each). For linear \nPCA, this dataset is too large to fit in memory. To manage \nthis situation, an Incremental PCA (IPCA) approach [63] is \napplied. As previously mentioned, IPCA simply fits the PCA \nin batches, independent of the number of input data samples \nbut still dependent on the input data features.\nSimilarly, KPCA and UMAP are computationally more \nexpensive than their DIFaddend linear counterparts [62, 65]. \nFor this reason, these dimensionality reduction techniques \nwere fitted using a subset of 10k pairs of sentences (i.e. 20k \nsentences), always ensuring the number of data instances \nis larger than the number of dimensions. To perform this \nsubsampling, the following requirements were taken into \naccount: (1) all 16 languages must be equally represented, \ngiving a total of 625 sentence pairs for each language; (2) all \nsentences present in the original train split will be present \nat least once in some language; (3) the representation of \nthe different sentence pairs must be as random as possible. \nFollowing these criteria, we perform a sampling based on \nassigning sentences to a randomly selected language until \nwe reach the maximum number of sampled data. The differ-\nent sentence pairs are shuffled randomly at each iteration to \navoid bias in the order in which the sentences are assigned to \nthe languages. As each language reaches the maximum data, \nthat language is discarded. This ensures a random distribu -\ntion of samples in each language but includes the full range \nof sentences in the original train data.\nComputational Resources\nRegarding the computational resources used in this study, \nan Intel(R) Xeon(R) Bronze 3206R CPU at 1.90GHz is used \nto fit the reduction techniques in Approaches 3 and 4. On \nthe other hand, the pre-computed embeddings used in the \napproaches and the fine-tuning experiments from Approach \n2 are performed using a Quadro RTX 8000 48GB GPU.\nBaseline Approaches: Approach 1 and Approach 2\nAs shown in Fig.  2, baseline Approaches 1 and 2 share \nsimilar experimental designs. Both employ the mSTSb test \ndata split to generate the embeddings of the sentence pairs. \nAs mentioned previously, because they are siamese-trained \nmodels, one embedding is computed for each sentence in \nthe sentence pair. Then their similarity value is calculated \nwith the cosine distance, and the Spearman coefficient is \ncomputed using the reference values to obtain the baseline \nperformance for each multilingual model. The only differ -\nence between Approach 1 and Approach 2 is the version \nof the model used. In the case of Approach 1, it is the pre-\ntrained version of the model, while in Approach 2 it is the \nfine-tuned version in the mSTSb task. This fine-tuning pro-\ncess is explained in more detail in the following subsection.\nTransformers Fine‑tuning\nThe fine-tuning process of the models based on the siamese \ntraining strategy for Approach 2 was carried out following the \nmethodology described by Reimers et al. [5, 61]. As depicted \nin the fine-tuning process of Approach 2 in Fig.  2, for each \nsentence pair from the mSTSb training split, two networks \nwith tied weights from the same transformer model compute \nthe embeddings for each sentence separately. Then the cosine \nsimilarity between the two sentence embeddings is calcu -\nlated. Finally, the mean squared error (MSE) loss between \nthe predicted and the gold similarity scores are used as the 1 ar , cs, de, en, es, fr, hi, it, ja, nl, pl, pt, ru, tr, zh-CN, zh-TW\n597Cognitive Computation  (2023) 15:590–612\n1 3\nobjective function to update the tied weights. During train -\ning, the following hyperparameters were optimised: number \nof epochs, scheduler, weight decay, batch size, warmup ratio, \nand learning rate. The hyperparameter values explored, the \nrequired time for fine-tuning, and the results of the experi -\nments can be consulted in Table 8 and Weight and Biases 2.\nFig. 2  Diag ram of the meth-\nodology followed for testing \nthe different Approaches on \nthe multilingual STS bench-\nmark. Where T represents the \npre-trained transformer model, \nT’ the fine-tuned transformer \nmodel, R the dimensionality \nreduction technique, R’ the \nfitted dimensionality reduction \ntechnique, V i the cosine similar-\nity score computed for ith sen-\ntence pair, G i the gold standard \nsimilarity score for ith sentence \npair, MSE for Mean Squared \nError loss, and rs the Spearman \ncorrelation coefficient\nAPPROACH 3\n⋮\nTrain mSTSb\nStep 1 – DimensionalityR Fi\nStep 2 -T est\n⋮⋮\nTest mSTSb\n⋮ ⋮\nEmbeddings\nV1\nrs\nVn\nG1\nGn\ncosine\ndistance\n⋮ ⋮\n/uni22EF\nT\n⋮\nR R’\nR’\n⋮ ⋮\nReduced\nEmbeddings\nAPPROACH 4\n⋮\nTrain mSTSb\nStep 1 – DimensionalityR Fi\nStep 2 -T est\n⋮⋮ /uni22EF\nT’\nTest mSTSb\n⋮ ⋮\nEmbeddings\nV1\nrs\nVn\nG1\nGn\ncosine \ndistance\n⋮ ⋮\n⋮\nR R’\nR’\n⋮ ⋮\nReduced\nEmbeddings\n/uni22EF\nT\n/uni22EF\nT’\nAPPROACH 1\n⋮⋮ /uni22EF\nT\nTest mSTSb\n⋮ ⋮\nEmbeddings\nV1\nVn\nG1\nGn\ncosine\ndistance\nAPPROACH 2\nTrain mSTSb\nStep 1 - Fine-tuning\n/uni22EF\nT\n/uni22EF\nT’\n⋮ ⋮\nStep 2 -T est\n/uni22EF\ncosine\ndistance V1 G1\nMSEl oss\n⋮⋮\nTest mSTSb\n⋮ ⋮\nEmbeddings\nV1\nVn\nG1\nGn\ncosine\ndistance\n⋮ ⋮/uni22EF\nT’\nrs\nrs\nTied weightsu pdate\nHyperparameters \niza\n2 https:// wandb. ai/ huert as_ 97/ Paper DimRed\n598 Cognitive Computation  (2023) 15:590–612\n1 3\nDimensionality Reduced Techniques Fitting: \nApproach 3 and Approach 4\nFigure 2 provides an overview of the process of applying and \nevaluating dimensional reduction techniques in Approaches \n3 and 4. Both approaches follow the same methodology; \nthe only difference is the pre-trained or fine-tuned version \nof the model used to compute the embeddings for fitting \nthe dimensional reduction technique. The experimental \ndesign has two steps, and it is repeated for each number of \ndimensions explored for each technique and each model. \nA wide range of the number of dimensions is explored \nfor each dimensionality reduction technique, as shown in \nthe “Results’’ section.\nFirstly, a dimensionality reduction technique is fitted \nthrough the embeddings computed by a multilingual model \nusing the mSTSb train split. As these are unsupervised tech-\nniques (i.e. the gold similarity scores are not included), this \ntraining step does not consider the relationship between pairs \nof sentences. However, it uses individual sentences to fit \nthe technique. In other words, if the train split of mSTSb \ncontains 10 k pairs of sentences from 16 languages, 20 k \nseparately sentences are used to fit a dimensional reduction \ntechnique.\nFinally, the embeddings from paired sentences from the \nmSTSb test split are reduced with the fitted technique. The \ncosine similarity distance function is applied to obtain a \nsimilarity value, and finally, the Spearman correlation coef-\nficient to the gold similarity scores is computed. In contrast \nto the previous fitting step, in this second step, the relation-\nship between pairs of sentences is considered as we need to \nevaluate the techniques on mSTSb task.\nStatistical Comparison\nAdditionally, to test if the use of reduced embeddings has \na significant impact on the performance in comparison to \nthe baseline approaches, we compare the average Spear -\nman correlation coefficient of the five multilingual siamese \ntransformer models (see the “Multilingual Models’’ section) \nbetween each pair of baseline and reduced approaches (i.e. \nApproach 1 vs Approach 3, Approach 2 vs Approach 4). For \nthis purpose, as we are comparing the same set of models in \ndifferent approaches, the two-tailed paired T-test using a sig-\nnificance level of 0.05 is conducted to test the null hypothesis \nof identical average Spearman correlation coefficient scores.\nResults\nThis section aims to summarise the effect of a wide vari -\nety of dimensionality reduction techniques on the perfor -\nmance of multilingual siamese transformers by comparing \nthe baseline approaches (i.e. Approaches 1 and 2) with \nthe reduced approaches (i.e. Approaches 3 and 4) for each \nmodel independently. It must be noted that this work does \nnot pretend to provide a comparative analysis between the \ndifferent models presented in the “ Multilingual Models ’’ \nsection or to identify the best model for this task. In contrast, \nthis work focuses on applying these dimensionality reduc -\ntion techniques to reduce the dimensionality of the models’ \nembeddings. Thus, applying different dimensionality reduc-\ntion techniques does not affect the execution or the memory \nrequirement for running the models. It only diminishes the \nneeded space to store embeddings and increases the speed \nof computing the cosine similarity between them.\nDue to space reasons, average results across the 31 mono-\nlingual and cross-lingual tasks are presented instead of a \nbreakdown by language. The average of Spearman correla -\ntion coefficients is computed by transforming each correla-\ntion coefficient to a Fisher’s z value, averaging them, and \nthen back transforming to a correlation coefficient.\nApproach 1 vs Approach 3: Dimensionality \nReduction in Pre‑trained Embeddings\nAs can be seen in Fig.  3, for every model, the pre-trained \nperformance on mSTSb (i.e. Approach 1) is improved using \ndifferent dimensional reduction techniques. These results \nprove that dimension reduction techniques can somehow \nadjust the knowledge present in the pre-trained embeddings \nto the semantic similarity task. This fact becomes even more \nsignificant in the case of LaBSE, a model with zero-shot \ncapabilities trained on a task close to semantic similarity, \nwhich also greatly benefits from the use of dimension reduc-\ntion techniques, increasing by 0.4 points the Spearman cor-\nrelation coefficient (see Fig.  3 and Table  3). For the rest \nof the models, it is equally remarkable that the dimension \nreduction techniques improve the pre-training performance, \nalmost doubling the score in the models with the XLM-RoB-\nERTa architecture.\nClearly, the best technique in Approach 3 is ICA. Not \nonly because it obtains the most remarkable improve -\nment in pre-training performance for all models, as shown \nin Table  3, but also because it is the technique that most \nquickly and with the fewest dimensions overcomes the \npre-trained models of Approach 1 (see Table  5). From \nthe table results, the ICA technique improves the pre-\ntrained Approach 1 performances reducing an average of \n91.58%± 2.59% of the initial dimensions retaining 100% \nof the baseline Approach 1 performance. Remarkably, the \ntwo-tailed paired T-test comparing Approach 1 vs Approach \n3 using the values for this technique from Table  3 resulted \nin p = 0.041 , indicating that the performance improvement \nis significant when using ICA as a dimensional reduction \ntechnique. These findings corroborate the ideas of Raunak \n599Cognitive Computation  (2023) 15:590–612\n1 3\net al. [21], who maintained that reduced word embeddings \ncould achieve similar or better performance than original \npre-trained embeddings.\nThe most likely explanation for these results is the differ-\nence in the objective of ICA from the other feature extrac -\ntion techniques. Even though they all transform the initial \nFig. 3  Appr oach 3 average Spearman rs correlation coefficient in multilingual tasks from the mSTSb test as a function of the number of dimen -\nsion for the different dimensionality reduction techniques grouped by model\nTable 3  Average Spearman \nrs correlation coefficient \ncomparison between Approach \n1 (Ap. 1) and best dimensional \nreduction technique in \nApproach 3 (Ap. 3) for the \nmultilingual Transformers\nModel Ap. 1 rs Best Technique Dimensions Ap. 3  rs Fitting Time\nbert-base-multilingual-cased 0.4342 ICA 209 0.5019 4 m 16 s\ndistilbert-base-multilingual-cased 0.4531 ICA 169 0.523 2 m 47 s\nxlm-roberta-base 0.3274 ICA 249 0.5269 7 m 51 s\nxlm-roberta-large 0.2855 ICA 1024 0.5392 31 m 22 s\nLaBSE 0.7096 ICA 129 0.7488 2 m 27 s\n600 Cognitive Computation  (2023) 15:590–612\n1 3\nspace through combinations of dimensions into a new space, \nthe ICA technique is based on optimising mutual informa -\ntion [ 64, 75]. It tries to find a space where the new fea -\ntures (latent variables) are as independent as possible from \neach other but as dependent as possible on the initial space. \nTherefore, in the case of ICA, unlike other techniques such \nas PCA or KPCA, a higher number of components does not \nnecessarily mean an increase in the information retained or \nan improvement in the result (as can be seen in Fig.  3 and \nclearly in Fig. 3 where there is a decrease from 169 dimen -\nsions onwards). This would explain why a low number of \ndimensions would outperform Approach 1.\nLikewise, the fact that it is the technique that achieves \nthe best results in Approach 3 in all models could be due to \nthe assumptions and characteristics of both the ICA and the \npre-trained embeddings. First, the pre-trained embeddings \nprobably include non-relevant and noisy variables as these \nembeddings are not adjusted to the STS task. Secondly, since \nICA is a technique in which the original variables are related \nlinearly to the latent variables but for which the latent dis -\ntribution is non-Gaussian, the noise present in pre-trained \nembeddings agnostic of the STS task could be managed \nappropriately.\nInterestingly, these results also emphasise that the issue \nof non-Gaussianity is more relevant than the nonlinearity \nissue. Non-Gaussianity would be more important than how \nthe initial variables are combined, as the ICA technique out-\nperforms linear PCA and nonlinear KPCA. This is in good \nagreement with other studies comparing the performance \nof PCA and ICA as a method for feature extraction in visual \nobject recognition tasks [76, 77].\nAdditionally, the presence of noisy variables in the pre-\ntrained embeddings would also be corroborated by the low \nscores obtained from the Variance Threshold feature selec -\ntion technique, which entirely depends on the original vari-\nables and cannot manage these noisy distributions.\nConsequently, ICA shows excellent properties for obtain-\ning compacted embeddings versions of pre-trained models \nwith a significant decrease of dimensions that improve the \nresult in the task of semantic similarity at a multilingual \nlevel.\nFor all these reasons, we can understand unsupervised \ndimensionality reduction, specially ICA, as a method of fit-\nting pre-trained models for downstream tasks. As it will be \nseen in the next section and as might be expected, this unsu-\npervised dimensionality reduction downstream fitting is not \ncomparable to a supervised fitting such as the fine-tuning of \nApproach 2. However, downstream fitting by unsupervised \ndimensionality reduction techniques may present interesting \nadvantages such as the fact that being unsupervised is task \nagnostic resulting in models with higher generalizability and \nwith a lower number of dimensions. Also, these dimension-\nality reduction techniques do not require GPUs and apply \na more interpretable methodology than a Deep Learning \nmodel fine-tuning such as transformers.\nApproach 2 vs Approach 4: Dimensionality \nReduction in Fine‑tuned Embeddings\nAlthough it was stated at the beginning of this section that \nmodel comparison is not an objective of the paper, different \nversions of the same architecture have been included for a \nmore comprehensive evaluation of the effects of dimension-\nality reduction techniques (i.e. xlm-roberta-base  and xlm-\nroberta-large, bert-base-multilingual-cased  and distilbert-\nbase-multilingual-cased ) have been included in the study. \nBased on the complexity and learning potential of the mod-\nels, one would expect the xlm-roberta-large model to per -\nform better than the xlm-roberta-base model. Similarly, the \nbert-base-multilingual-cased  model would be expected to \nbe superior to the distilbert-base-multilingual-cased model. \nIn Approach 1, however, the opposite is true. Only when \nfine-tuning occurs in the task (Approaches 2 and 4) is it \nobserved how the performance of the models is in line with \nthe expected complexity (see Table 4). These results provide \nwider support for the importance of supervised fine-tuning.\nSimilarly, fine-tuning also alters the impact of dimen -\nsionality reduction techniques on the results of multilingual \nmodels. Compared to Approach 3, when fine-tuning, the \nfeature selection techniques and nonlinearity become more \nimportant, ICA becomes less critical, and the minimum \nnumber of dimensions that outperform the baseline approach \nincreases.\nAs can be seen in Fig.  4 and Table  4, the promotion of \nVariance Threshold feature selection as one of the best tech-\nniques for some models in Approach 4 could be attributed to \nthe fact that fine-tuning adjust the embeddings to the task, \nreducing the presence of noisy variable and taking advantage \nof the variable selection process. This would be in line with \nthe results obtained in Approach 3, where feature extraction \ntechniques more adequately handled the presence of unad -\njusted variables. This further supports the argument that they \ncan reduce dimensions and generate new feature representa-\ntions to help improve performance on learning problems.\nFurthermore, Table  6 shows the lack of ICA dominance \nand the emergence of the KPCA-sigmoid technique as \nthe method with fewer dimensions improves the baseline \nApproach 2 ( 59.32%± 29.92% reduced dimensions retain -\ning 99.00%± 2.00% baseline performance). It reveals that \nmanaging the non-Gaussianity issue is less relevant than the \nnonlinearity issue after fine-tuning. The fine-tuning process \nalso impacts the reduction capabilities of the dimension -\nality reduction techniques since considering the technique \nthat retains the maximum performance with the lowest \ndimensions for each model in Table  6 shows that the ini -\ntial dimensions from the baseline Approach 2 are reduced \n601Cognitive Computation  (2023) 15:590–612\n1 3\nby an average of 54.65%± 32.20% . Alt hough this average \nreduction is lower than the achieved earlier in the compari -\nson of Approach 3 with the baseline Approach 1, it is still \nremarkable that even after fine-tuning, the multilingual \nperformance can be exceeded with half of the dimensions. \nFinally, the two-tailed paired T-test comparing Approach \nTable 4  A verage Spearman rs correlation coefficient comparison between Approach 2 (Ap. 2) and best dimensional reduction technique in \nApproach 4 (Ap. 4) for the multilingual Transformers\nModel Ap. 2 rs Best Technique Dimensions Ap. 4  rs Fitting Time\nbert-base-multilingual-cased-fine-tuned 0.7045 ICA 568 0.7117 12 m 38 s\ndistilbert-base-multilingual-cased-fine-tuned 0.6863 VarThres 692 0.6842 2 s\nxlm-roberta-base-fine-tuned 0.7470 VarThres 673 0.7495 3 s\nxlm-roberta-large-fine-tuned 0.8150 KPCA-sigmoid 1024 0.8176 20 m 6 s\nLaBSE-fine-tuned 0.8242 KPCA-sigmoid 768 0.8243 19 m 25 s\nFig. 4  Appr oach 4 average Spearman rs correlation coefficient in multilingual tasks from the mSTSb test as a function of the number of dimen -\nsions for the different dimensionality reduction techniques grouped by model\n602 Cognitive Computation  (2023) 15:590–612\n1 3\n2 vs Approach 4 using the values from Table  4 resulted in \np = 0.255 , r evealing no significant difference in perfor -\nmance using these dimensional reduction techniques.\nExecution Time Comparison\nAlthough it is out of the main scope of the paper, this sub -\nsection compares the different dimension reduction tech -\nniques regarding the execution time required to fit each tech-\nnique. As mentioned in the “Data’’ section, it is fundamental \nto note that KPCA and UMAP were fitted using a subset of \n10k pairs of sentences due to their computational cost, which \nis more expensive than their linear counterparts. Therefore, \nfor fair comparisons, IPCA, ICA, and variation threshold \ntechniques are compared on the one hand and KPCA and \nUMAP on the other.\nFinally, we analyse the reduction of computational \ntime by comparing the fitting time of these techniques in \nApproach 3 and 4 concerning the fine-tuning process per -\nformed in Approach 2, following the experimental design \ndescribed in the “Experimental Setup’’ section.\nDimensionality Reduction Techniques Comparison: IPCA, \nICA and Variance Threshold\nConsidering the wide range of dimensions explored for each \nmodel, we analyse the fitting time for the minimum, the in-\nbetween, and the maximum number of dimensions technique \nin the mSTSb train split to compare the different techniques \nin execution time for Approaches 3 and 4. The results of \nthese tests and the dimensions associated with these times \nare presented in Table 7. It should be noted that the number \nof dimensions shown in this table for the variance threshold \ntechnique differs from the other techniques since the number \nof reduced dimensions depends on the number of dimen -\nsions that exceed the variance threshold explored.\nTable 7, shows that the technique that requires the longest \nfitting time of IPCA, ICA and variance threshold is ICA, \nwhile the variance threshold is the one that requires the \nshortest time.\nRegarding computational complexity, IPCA has constant \nmemory complexity O (bd2) [67], while the ICA algorithm \nis O(2md(d+ 1)n) [78], where n is the number of samples \nof dimensions d, b is the batch size for the IPCA algorithm, \nand m is the number of iterations of the ICA algorithm. We \ncan observe that when IPCA and ICA are compared for the \nsame number of dimensions and number of samples, the \ncomputational burden of the ICA technique is the number \nof iterations ( m) since they critically increase execution \ntime. As stated in [78], this effect is more noticeable as the \ndimension increases, making the use of ICA computation -\nally unfeasible when the number of dimensions is moderate \nor high. Our results support these findings, as the ICA’s \nfitting time explodes when the number of dimensions is \nhigh (see Table  7). On the other hand, the IPCA execution \ntime remains practically constant because the batches avoid \nattempting to load all the data into memory at the same time.\nFinally, as expected, the feature selection technique is the \nleast time-consuming since it only requires computing the \nvariance for each dimension and filtering it according to a \nselected variance threshold.\nConsequently, there is a trade-off between the number of \ndimensions to be reduced, the fitting time, and the percent -\nage of performance to be retained. We would recommend \nthat for the case of pre-trained embedding reduction, the \nICA technique should be the priority if a low number of \ndimensions is explored since it is the best at retaining and \nimproving the initial performance at a feasible cost. How -\never, suppose the target number of dimensions to be reduced \nis medium or high. In that case, we recommend the IPCA \ntechnique since it can retain and improve performance with a \nconstant computational cost. Finally, suppose that the execu-\ntion time is clearly prioritised. In that case, we recommend \nthe variance threshold feature selection technique, even if \nthe number of dimensions is reduced and the performance \nretained is not the best, as this technique cannot handle the \nnoise in pre-trained features not adjusted for a downstream \ntask.\nDimensionality Reduction Techniques Comparison: KPCA \nand UMAP\nRegarding the KPCA technique, using a kernel function \nbefore performing linear PCA increases the execution time. \nThese kernel functions map the data into a nonlinear feature \nspace, so the execution time varies depending on the kernel \ntype. From the results shown in Table 7, the sigmoid kernel \nrequires the most execution time and the cosine kernel the \nleast. Despite these previous results, the selection of the ker-\nnel type depends on each case, and different kernels should \nbe explored, as previous studies suggest [62, 65].\nFinally, it can be seen from the results shown in Table  7 \nthat the execution time of UMAP is similar to KPCA. \nNevertheless, several considerations must be taken into \naccount in the case of the UMAP technique. As discussed \nin the “ Dimensionality reduction Techniques ’’ section, \nthe number of neighbours parameter is proportional to the \namount of global information retained. As shown in Table 2, \ndifferent values of the number of neighbours are explored \nfor each model. The number of neighbours that provided the \nbest results were 10, 5, 5, 5 and 125 for each model in the \nsame order as the models shown in Table 7. The value of this \nparameter determines the execution time of the technique; \nincreasing the time, the greater the number of neighbours \n603Cognitive Computation  (2023) 15:590–612\n1 3\nto consider for embedding a sentence. For this reason, the \ndegree of similarity between KPCA and UMAP run times \nvaries from one model to another. Similarly, although the \ntable only includes the execution times for the best UMAP \nparameter values found for each model, the choice of these \nparameters requires a prior exploration process that can be \nvery long, depending on how intensive the search for the best \nparameter values is. Hence, this technique has an additional \ncost to consider.\nFitting Dimensionality Reduction Technique \nand Fine‑tuning Comparison\nAnother point to discuss is comparing the execution times \nrequired by dimension reduction techniques with the fine-\ntuning process. It should be noted that the fine-tuning times \nshown in Table 8 do not include the time required for hyper-\nparameter optimization, which, depending on how extensive \nthe search is, can extend the time required to obtain a fine-\ntuned model. Therefore, the time shown refers to the execu-\ntion time needed to fine-tune each model once the optimal \nhyperparameters are found.\nIn practical application, the time required for fitting a \ndimensionality reduction technique on pre-trained embed -\ndings instead of fine-tuning the model downstream is what \nmatters. For this purpose, we use the fine-tuning process \nexecution times shown in Table  8 compared to Table  5, \nwhich shows the fitting time of the dimension reduction \ntechniques of Approach 3 that improves the result of the \nbaseline Approach 1.\nFor space reasons, even though Table  5 shows a break -\ndown of the performance for each technique, the technique \nused to compare the fine-tuning of each model is the tech -\nnique that, with the lowest number of dimensions, retains \nthe highest possible performance (highlighted in bold in \nTable  5). As stated previously, the best technique for all \nmodels is ICA. This technique requires a fitting time of an \naverage of 96.68%± 0.68% faster than the fine-tuning pro -\ncess on all models.\nTaken together, we can see the advantage in execution \ntime of using reduction techniques in Approach 3 with \nrespect to the fine-tuning process carried out in Approach \n2. Remarkably, it is exciting to point out the potential of \nICA as an alternative to the fine-tuning process. Although it \nhas previously been proven that fine-tuning achieves better \nperformance than fitting dimension reduction techniques, \nthis technique does not require GPU. This technique reduces \nthe execution time by more than 96%. This fact, together \nwith the ability to improve the performance of pre-trained \nembeddings and the generalisation capability of this unsu -\npervised technique, reveals the potential of this technique as \nan alternative to fine-tuning.\nUMAP: a Case of Study\nIn this section, we pay special attention to the recently pro -\nposed UMAP technique [ 66]. The case of UMAP shows \nthat for the STS task, it is not a suitable technique to reduce \nthe dimensionality of the embeddings since it is the one \nthat retains the lowest percentage of the baseline perfor -\nmances in both pre-trained and fine-tuned embeddings (see \nTables 5 and 6). Considering this fact, it is interesting to note \nthat the potential of this technique resides in the fact that it \nquickly saturates, i.e. the maximum retained performance is \nreached with a small number of dimensions in Approach 3 \nwith an average of 94.65%± 6.07% of reduced initial dimen-\nsions retaining 49.00%± 11.14% performance concerning \nthe reference Approach 1, and more notably in Approach 4 \nwith an average of 98.42%± 0.72% of reduced initial dimen-\nsions retaining 76.00%± 3.74% performance for the refer -\nence Approach 2.\nThis saturation behaviour can be attributed to the \ndemonstrated functionality of UMAP for generating \nvisualisations by reducing high-dimensional data to 2- \nor 3-dimensions. As reported in other works [ 79, 80], \nthe most significant potential of this technique lies pre -\ncisely in visualisation, where this saturation capability is \nexploited. UMAP is a nonlinear graph-based method for \ndimensionality reduction that is not meant to extract fea -\ntures as methods like PCA do [ 66]. Instead, its primary \ngoal is to represent the input data as a high-dimensional \ngraph and then reconstruct it in a lower-dimensional space \nwhile retaining structure. These characteristics explain the \nlack of good results for the UMAP technique in the mSTSb \ntask in Approach 3, where feature extraction has proven to \nhelp manage the noisy pre-trained features. Furthermore, \nthis would explain why we observe UMAP saturation with \nfewer dimensions in Approach 4, as the embeddings are \nalready fine-tuned, and UMAP can exploit its potential to \npreserve information.\nPreviously, it has been proven that its fitting requires an \nexecution time similar to other techniques such as KPCA. \nIt is also important to note that its computational cost limits \nthe total number of data instances to be used for fitting since \nthe first step is expensive. It requires computing a graphical \nrepresentation of the dataset and then learning an embedding \nfor that graph.\nDespite the previous conclusion, UMAP is time-consuming. \nThis technique highly depends on the parameters used, and \nmany parameters can be explored. Additionally, the slow pro-\ncess of embedding new data in a fitted UMAP must also be \nconsidered. UMAP does not build a function that directly maps \nhigh-dimensional points down to a reduced space. Instead, it \nfirst computes a graph representing the whole data and then \nlearns an embedding for it. Thus, embedding a new point \n604 Cognitive Computation  (2023) 15:590–612\n1 3\nrequires calculating its nearest neighbours from the fitting \ndata and embedded in the learnt graph. Again, this process \nexecution time increases with the number of neighbours value \nselected when. Therefore, UMAP can transform new data, \nalbeit more slowly than other techniques that allow this.\nPrevious Works Comparison\nFinally, we discuss our results concerning previous related \nwork in dimensionality reduction of pre-computed embed -\ndings. Before this discussion proceeds, it is essential to \nTable 5  Analysis of the lowest reduced number of dimensions from \nApproach 3 that improves the result of the baseline Approach 1 for a \nspecific performance threshold retained. For instance, 100% threshold \nrepresents that the technique achieves at least the 100% of the base -\nline approach score\nModel (Ap. 1 Avg rs) Technique Threshold Performance \nRetained\nDimensions  \n(% reduction)\nAp. 3 Avg rs Fitting Time\nbert-base-multilingual-cased (0.4342) IPCA 100% 209 (73%) 0.4251 27 s\nICA 100% 89 (88%) 0.4779 1 m 10 s\npoly 95% 249 (68%) 0.4130 49 s\nrbf 95% 448 (42%) 0.4138 1 m 38 s\nsigmoid 100% 129 (83%) 0.4425 40 s\ncosine 100% 209 (73%) 0.4350 36 s\nUMAP 50% 129 (83%) 0.2176 35 s\nVarThres 85% 161(79%) 0.3727 2 s\ndistilbert-base-multilingual-cased (0.4531) IPCA 100% 209 (73%) 0.4553 38 s\nICA 100% 49 (94%) 0.4564 43 s\npoly 95% 369 (52%) 0.4310 1 m 6 s\nrbf 95% 608 (21%) 0.4305 2 m 13 s\nsigmoid 100% 129 (83%) 0.4642 38 s\ncosine 100% 209 (73%) 0.4537 33 s\nUMAP 40% 49 (94%) 0.3942 18 s\nVarThres 95% 238 (69%) 0.438 2 s\nxlm-roberta-base (0.3274) IPCA 100% 89 (88%) 0.3711 38 s\nICA 100% 49 (94%) 0.4043 56 s\npoly 100% 129 (83%) 0.3439 25 s\nrbf 100% 129 (83%) 0.3439 50 s\nsigmoid 100% 49 (94%) 0.3425 29 s\ncosine 100% 89 (88%) 0.3709 15 s\nUMAP 40% 10 (99%) 0.1320 15 s\nVarThres 100% 52 (93%) 0.3310 2 s\nxlm-roberta-large (0.2885) IPCA 100% 116 (89%) 0.3149 1 m 37 s\nICA 100%  63 (94%) 0.3642 1 m 52 s\npoly 100% 223  (78%) 0.2927 45 s\nrbf 100% 276 (73%) 0.2934 1 m 8 s\nsigmoid 100% 63 (94%) 0.2927 32 s\ncosine 100% 116 (89%) 0.3191 25 s\nUMAP 45% 10 (99%) 0.1365 15 s\nVarThres 100% 598 (42%) 0.2917 3 s\nLaBSE (0.7096) IPCA 100% 129 (83%) 0.7251 37 s\nICA 100%  89 (88%) 0.7431 1 m 35 s\npoly 100% 169 (78%) 0.7181 34 s\nrbf 100% 408 (47%) 0.7106 1 m 35 s\nsigmoid 100% 89 (88%) 0.7127 34 s\ncosine 100% 129 (83%) 0.7232 21 s\nUMAP 70% 10 (99%) 0.5026 33 s\nVarThres 85% 217 (72%) 0.6148 2 s\n605Cognitive Computation  (2023) 15:590–612\n1 3\nremark that our work differs from previous work in the liter-\nature as we explore a more comprehensive range of unsuper-\nvised dimension reduction techniques, evaluating them on \npre-trained and fine-tuned pre-computed embeddings from \nstate-of-the-art multilingual contextual-based transformer \nmodels in the Semantic Textual Similarity (STS) task.\nOur experimental results are in agreement with Raunak  \net al. [ 21, 22], Singh et al. [ 27] and Thirumoorthy and \nMuneeswaran [ 37], which corroborates the hypothesis \nthat reduction in pre-trained embeddings can maintain \nor improve the performance of the original embeddings. \nSimilarly, our results are consistent with those obtained by \nTable 6  Analysis of the lowest reduced number of dimensions from \nApproach 4 that improves the result of the baseline Approach 2 for a \nspecific performance threshold retained. For instance, 100% threshold \nrepresents that the technique achieves at least the 100% of the base -\nline approach score\nModel (Ap. 2 Avg rs) Technique Threshold Performance \nRetained\nDimensions  \n(% reduction)\nAp. 4 Avg rs Fitting \nTime\nbert-base-multilingual-cased-fine-tuned (0.7045) IPCA 95% 49 (94%) 0.6710 34 s\nICA 100% 169 (78%) 0.7047 3 m 37 s\npoly 95% 129 (83%) 0.6716 31 s\nrbf 95% 169 (78%) 0.6738 54 s\nsigmoid 100% 329 (57%) 0.7048 1 m 34 s\ncosine 95% 49 (94%) 0.6707 11 s\nUMAP 70% 10 (99%) 0.5398 32 s\nVarThres 100% 393 (53%) 0.7046 2 s\ndistilbert-base-multilingual-cased-fine-tuned (0.6863) IPCA 95% 49 (94%) 0.6533 35 s\nICA 95% 49 (94%) 0.6556 56 s\npoly 95% 129 (83%) 0.6542 30 s\nrbf 95% 129 (83%) 0.6520 49 s\nsigmoid 95% 49 (94%) 0.6601 29 s\ncosine 95% 89 (88%) 0.6631 16 s\nUMAP 75% 10 (99%) 0.5189 25 s\nVarThres 95% 66 (91%) 0.6620 2 s\nxlm-roberta-base-fine-tuned (0.7470) IPCA 95% 49 (94%) 0.7198 36 s\nICA 95% 49 (94%) 0.7208 59 s\npoly 95% 89 (88%) 0.7112 25 s\nrbf 95% 129 (83%) 0.7134 49 s\nsigmoid 100% 289 (62%) 0.7472 1 m 45 s\ncosine 95% 49 (94%) 0.7195 11 s\nUMAP 75% 10 (99%) 0.5724 31 s\nVarThres 100% 411 (46%) 0.7491 3 s\nxlm-roberta-large-fine-tuned (0.8150) IPCA 95% 63 (94%) 0.7910 51 s\nICA 95% 63 (94%) 0.7950 1 m 16 s\npoly 95% 63 (94%) 0.7774 23 s\nrbf 95% 63 (94%) 0.7760 42 s\nsigmoid 100% 223 (78%) 0.8151 50 s\ncosine 95% 63 (94%) 0.7916 13 s\nUMAP 80% 10 (99%) 0.6584 38 s\nVarThres 95% 95 (91%) 0.7936 3 s\nLaBSE-fine-tuned (0.8242) IPCA 95% 89 (88%) 0.8014 34 s\nICA 95% 89 (88%) 0.7986 2 m 3 s\npoly 95% 89 (88%) 0.7898 25 s\nrbf 95% 129 (83%) 0.7932 47 s\nsigmoid 100% 728 (5%) 0.8243 7 m 11 s\ncosine 95% 89 (88%) 0.8001 21 s\nUMAP 80% 23 (97%) 0.6640 35 s\nVarThres 95% 227 (70%) 0.7964 2 s\n606 Cognitive Computation  (2023) 15:590–612\n1 3\nTruşcă et al. [ 23] and Shimomoto et al. [ 47]. The authors \nemploy PCA as the only unsupervised technique. We have \nconfirmed that this feature extraction technique has great \npotential to reduce the dimensions of pre-trained embed -\ndings. Our results further reveal that incremental PCA \n(IPCA) is also suitable for embedding reduction. Besides, \nwe found that for embeddings already fine-tuned to the \ndownstream task, using the KPCA, the nonlinear version of \nPCA, is much more helpful for preserving performance and \nreducing dimensions.\nRegarding execution time, our results are similar to Saeed \net al. [17]. These authors combined the unsupervised PCA \ntechnique and the supervised LDA technique with classical \nNLP techniques based on N-grams and classical ML mod -\nels (Decision Trees, Logistic Regression, or Naive Bayes) \nfor sentiment classification of monolingual Arabic texts. \nTable 7  T ime required to fit the \ndifferent dimension reduction \ntechniques in the mSTSb \ntask. Owing to the fact that \na wide range of dimensions \nis explored, the fitting time \nfor the minimum, in-between \nand maximum number of \ndimensions are reported for \neach model and technique. The \nmeasurements are performed \non an Intel(R) Xeon(R) Bronze \n3206R CPU at 1.90GHz\nModel Technique Time Min \nDimension\nTime In-\nbetween  \nDimension\nTime Max Dimension\nbert-base-multilingual-cased IPCA 27 s (10) 29 s (448) 30 s (768)\nICA 53 s (10) 8 m 43 s (448) 52 m 48 s (768)\npoly 18 s (10) 1 m 26 s (448) 2 m 17 s (768)\nrbf 36 s (10) 1 m 38 s (448) 3 m 29 s (768)\nsigmoid 24 s (10) 2 m 15 s (448) 5 m 53 s (768)\ncosine 8 s (10) 55 s (408) 1 m 24 s (768)\nUMAP 20 s (10) 1 m 30 s (448) 3 m 29 s (768)\nVarThres 2 s (65) 2 s (516) 2 s (767)\ndistilbert-base-multilingual-cased IPCA 36 s (10) 40 s (448) 44 s (768)\nICA 29 s (10) 8 m 30 s (448) 29 m 31 s (768)\npoly 16 s (10) 1 m 24 s (448) 2 m 35 s (768)\nrbf 35 s (10) 1 m 44 s (448) 2 m 57 s (768)\nsigmoid 23 s (10) 3 m 58 s (448) 15 m 40 s (768)\ncosine 8 s (10) 51 s (448) 1 m 51 s (768)\nUMAP 13 s (10) 1 m 38 s (448) 4 m 19 s (768)\nVarThres 2 s (66) 2 s (507) 2 s (767)\nxlm-roberta-base IPCA 35 s (10) 35 s (448) 40 s (768)\nICA 56 s (10) 9 m 54 s (448) 21 m 9 s (768)\npoly 16 s (10) 1 m 2 s (448) 2 m 12 s (768)\nrbf 33 s (10) 1 m 38 s (448) 2 m 2 s (768)\nsigmoid 23 s (10) 1 m 43 s (448) 3 m 26 s (768)\ncosine 7 s (10) 56 s (448) 1 m 36 s (768)\nUMAP 15 s (10) 2 m (448s) 4 m 32 s (768)\nVarThres 2 s (1) 2 s (448) 2 s (768)\nxlm-roberta-large IPCA 54 s (10) 54 s (490) 1 m 46 s (1024)\nICA 57 s (10) 23 m 13 s (490) 32 m 15 s (1024)\npoly 16 s (10) 1 m 24 s (490) 3 m 35 s (1024)\nrbf 35 s (10) 1 m 30 s (490) 4 m 49 s (1024)\nsigmoid 23 s (10) 2 m 58 s (490) 8 m 28 s (1024)\ncosine 9 s (10) 1 m 38 s (490) 2 m 41 s (1024)\nUMAP 15 s (10) 1 m 57 s (490) 7 m 24 s (1024)\nVarThres 3 s (10) 3 s (598) 3 s (1024)\nLaBSE IPCA 35 s (10) 36 s (448) 39 s (768)\nICA 36 s (10) 12 m 55 s (448) 52 m 39 s (768)\npoly 17 s (10) 1 m 24 s (448) 2 m 35 s (768)\nrbf 35 s (10) 1 m 41 s (448) 2 m 50 s (768)\nsigmoid 24 s (10) 4 m 25 s (448) 19 m 37 s (768)\ncosine 8 s (10) 55 s (448) 1 m 44 s (768)\nUMAP 33 s (10) 2 m 11 s (448) 4 m 18 s (768)\nVarThres 2 s (94) 2 s (615) 2 s (767)\n607Cognitive Computation  (2023) 15:590–612\n1 3\nThe authors reported an improvement in results over previ -\nous work by reducing dimensions by up to 93% with a 97% \nshorter run time.\nEven though the NLP task addressed in the present study \nis STS from a multilingual perspective (including Arabic) \nand not sentiment classification, our findings align with \nthese previous results. The ICA technique reduces an aver -\nage of 91.58%± 2.59% of the initial dimensions, retaining \n100% of the baseline Approach 1 performance, requiring a \nfitting time of 96.68%± 0.68% faster than the fine-tuning \nprocess.\nGiven these points, we can safely conclude that our \nresults are in line with previous work extending their find -\nings to state-of-the-art contextual-based models from a mul-\ntilingual approach. This paper provides new insights into \ndimensionality reduction techniques for a space- and time-\nefficient data representation.\nConclusion\nIn this investigation, the goal was to assess the impact of a \nvariety of dimensionality reduction techniques on the per -\nformance of pre-computed multilingual siamese fashion \ntransformers embeddings on semantic textual similarity \ntasks from mSTSb, to expand our knowledge of semantic-\naware transformer-based models. To this end, two differ -\nent baseline approaches are reduced (i.e. Approach 1 and \nApproach 2), one using the pre-trained version of the models \nand the second further fine-tuning them on the downstream \nSTS task. Particular attention is paid to analysing which \ntechniques best and with the fewest dimensions improve the \nperformance of the baseline approaches.\nFrom the research, it is possible to conclude that dimen -\nsionality reduction techniques can help reduce the number of \ndimensions of the embeddings while improving the results \nTable 8  T ime required to \nfine-tune the different models \nin the mSTSb task. The fine-\ntuning process is carried out \nin Quadro-RTX 8000 GPU. \nThe best hyperparameters \nconfiguration considered for \neach model is also reported\nModel Time Hyperparameters\nbert-base-multilingual-cased-fine-tuned 39 m 18 s bach_size: 32\nlr: 2e-5\nepochs: 2\nscheduler: warmuplinear\nwarmup_ratio: 0.2\nwieght_decay: 0.2\ndistilbert-base-multilingual-cased-fine-tuned 15 m 31 s bach_size: 64\nlr: 2e-5\nepochs: 2\nscheduler: warmuplinear\nwarmup_ratio: 0.3\nwieght_decay: 0.7\nxlm-roberta-base-fine-tuned 27 m 49 s bach_size: 64\nlr: 5e-5\nepochs: 2\nscheduler: warmuplin-\near_hard_restarts\nwarmup_ratio: 0.1\nwieght_decay: 0.5\nxlm-roberta-large-fine-tuned 1 h 2 m 32 s bach_size: 64\nlr: 1e-5\nepochs: 2\nscheduler: warmupcosine\nwarmup_ratio: 0.2\nwieght_decay: 0\nLaBSE-fine-tuned 59 m 39 s bach_size: 32\nlr: 3e-6\nepochs: 2\nscheduler: warmupcosine\nwarmup_ratio: 0.1\nwieght_decay: 0.5\n608 Cognitive Computation  (2023) 15:590–612\n1 3\nif using pre-trained embeddings from Approach 1 and pre -\nserving the performance when using fine-tuned embeddings \nfrom Approach 2. Nevertheless, the dimensionality reduc -\ntion is more considerable in the pre-trained version with \nan average of 91.58%± 2.59% compared to the average of \n54.65%± 32.20% of the fine-tuned version. Special atten -\ntion is given to ICA in the pre-trained scenario, which ade -\nquately managed the noisy variables present in not adjusted \nembeddings. This technique also proved to be a reasonable \nalternative to fit the models in the downstream task in an \nunsupervised way, leading to a generalised adjusted version \nof the models with downstream multitasking capabilities. \nNevertheless, it has also been proved that this unsupervised \nfitting is not comparable to supervised fine-tuning. On the \nother hand, the fine-tuned scenario revealed the relevance of \nfeature selection techniques and the significance of nonlin -\near KPCA techniques for dimensionality reduction.\nAdditionally, although our execution time experiments \nreveal that ICA is the most time-consuming technique \namong the dimension reduction techniques, reducing pre-\ntrained embeddings (Approach 3) by gaining generalisa -\ntion power with this unsupervised technique is still faster \nthan performing downstream fine-tuning (Approach 2). \nMoreover, ICA does not require GPUs for fitting, reinforc -\ning its potential as an alternative to fine-tuning. Overall, \na good trade-off between performance against available \ncomputational resources, execution time, and the number \nof dimensions to be reduced must be considered when \nchoosing the approach to follow.\nThe results of our experiments are consistent with pre -\nvious results from the literature, corroborating the hypoth -\nesis that reduction in embeddings can maintain or improve \nthe performance of the original embeddings by extend -\ning their evaluation to state-of-the-art contextual-based \nmodels from a multilingual approach. In this way, we can \nestablish that dimensionality reduction techniques could \nalso be leveraged for contextualised embeddings.\nTo our knowledge, this is the first study to investigate the \neffect of dimensionality reduction techniques and transform-\ners models in a multilingual semantic-awareness scenario. \nThis study analyses alternatives to the storage limitation we \nare about to face if the current trends of using large datasets \nand the growth rate of storage utilisation persist. Based on \nthe promising findings presented in this article, continued \nresearch into the impact of dimensionality reduction tech -\nniques in other highly demanded NLP tasks appears to be \ntotally justified. Furthermore, in future research, we intend \nto focus on testing the reduced models presented in this work \nin real-world applications. Besides, we hope to carry out \nfurther experimental investigation, including other dimen -\nsionality reduction approaches, such as creating a distilled \nversion of pre-trained models or exploring the novel feature \nextraction and feature selection methods proposed in [ 27, \n37].\nAs stated previously, the findings presented in this \nstudy of multilingual semantic similarity are of direct \npractical applicability. Combining dimensionality reduc -\ntion techniques with transformer models could also help \nreduce the embedding size and make ensemble approaches \npossible. Finally, further studies about the multitasking \ngeneralisation capabilities of ICA for pre-trained models \nare still required.\nFunding O pen Access funding provided thanks to the CRUE-CSIC \nagreement with Springer Nature. This research has been supported \nby the Spanish Ministry of Science and Education under FightDIS \n(PID2020-117263GB-100), by MCIN/AEI/10.13039/501100011033/ \nand European Union NextGenerationEU/PRTR for XAI-Disinfodemics \n(PLEC2021-007681) grant, by Comunidad Autónoma de Madrid under \nS2018/ TCS-4566 (CYNAMON) grant, by BBVA Foundation grants  \nfor scientific research teams SARS-CoV-2 and COVID-19 under the \ngrant: “CIVIC: Intelligent characterisation of the veracity of the infor-\nmation related to COVID-19 ”, and by IBERIFIER (Iberian Digital \nMedia Research and Fact-Checking Hub), funded by the European \nCommission under the call CEF-TC-2020-2, grant number 2020-EU-\nIA-0252. Finally, David Camacho has been supported by the Comu -\nnidad Autónoma de Madrid under Convenio Plurianual with the Uni -\nversidad Politécnica de Madrid in the actuation line of “ Programa de \nExcelencia para el Profesorado Universitario ” and by the research \nproject DisTrack: Tracking disinformation in Online Social Networks \nthrough Deep Natural Language Processing, granted by Barcelona  \nMobile World Capital Foundation.\nData Availability The Multilingual Semantic Textual Similarity Bench-\nmark (mSTSb) used for the purpose of this article can be found in \nhttps:// github. com/ Huert as97/ Multi lingu al- STSB.\nDeclarations \nResearch Involving Human Participants and/or Animals  This ar ticle \ndoes not contain any studies with human participants or animals per -\nformed by any of the authors.\nInformed Consent Informed consent was obtained from all individual \nparticipants included in the study.\nConflict of Interest The authors declare no competing interests.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta -\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n609Cognitive Computation  (2023) 15:590–612\n1 3\nReferences\n 1. Otter DW, Medina JR, Kalita JK. A survey of the usages of deep \nlearning for natural language processing. IEEE Trans Neural Netw \nLearn Syst. 2021;32(2):604–24. https:// doi. org/ 10. 1109/ TNNLS. \n2020. 29796 70.\n 2. Tay Y, Dehghani M, Bahri D, Metzler D. Efficient transformers: a \nsurvey. ACM Computing Surveys. 2022. https:// doi. org/ 10. 1145/ \n35308 11.\n 3. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez \nAN, et al. Attention is All You Need. In: Proceedings of the 31st \nInternational Conference on Neural Information Processing Sys -\ntems. NIPS'17. Red Hook, NY, USA: Curran Associates Inc.; \n2017. p. 6000–10. https:// doi. org/ 10. 5555/ 32952 22. 32953 49.\n 4. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training \nof deep bidirectional transformers for language understanding. In: \nProceedings of the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1. Minneapolis, Minnesota: Associa-\ntion for Computational Linguistics; 2019. p. 4171–86. https:// doi. \norg/ 10. 18653/ v1/ N19- 1423.\n 5. Reimers N, Gurevych I. Sentence-BERT: Sentence embeddings \nusing Siamese BERT-Networks. In: Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language \nProcessing (EMNLP-IJCNLP). Hong Kong, China: Association \nfor Computational Linguistics; 2019. p. 3982–92. https:// doi. org/ \n10. 18653/ v1/ D19- 1410.\n 6. Huertas-Tato J, Martin A, Camacho D. BERTuit: Understand -\ning Spanish language in Twitter through a native transformer. \n2022. https:// doi. org/ 10. 48550/ ARXIV. 2204. 03465.\n 7. Chowdhary KR. Natural language processing. New Delhi: \nSpringer India; 2020. p. 603–49. https:// doi. org/ 10. 1007/ 978- 81- \n322- 3972-7_ 19.\n 8. Cer D, Diab M, Agirre E, Lopez-Gazpio I, Specia L. SemEval-2017 \nTask 1: Semantic textual similarity multilingual and crosslingual \nfocused evaluation. In: Proceedings of the 11th International Work-\nshop on Semantic Evaluation (SemEval-2017). Vancouver, Canada: \nAssociation for Computational Linguistics; 2017. p. 1–14. https:// \ndoi. org/ 10. 18653/ v1/ S17- 2001.\n 9. Humeau S, Shus ter K, Lachaux MA, Weston J. Poly-encoders: \nArchitectures and pre-training strategies for fast and accurate \nmulti-sentence scoring. In: International Conference on Learning \nRepresentations (ICLR). Online, 2020.  https:// doi. org/ 10. 48550/ \nARXIV. 1905. 01969.\n 10. Zhelezniak V , Savkov A, Shen A, Hammerla N. Correlation \ncoefficients and semantic textual similarity. In: Proceedings of \nthe 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers). Minneapolis, \nMinnesota: Association for Computational Linguistics; 2019. p. \n951–62.  https://  doi. org/ 10. 18653/ v1/ N19- 1100.\n 11. Sidorov G, Gelbukh A, Gómez-Adorno H, Pinto D. Soft similar-\nity and soft cosine measure: Similarity of features in vector space \nmodel. Computación y Sistemas. 2014;18(3):491–504. https:// doi. \norg/ 10. 13053/ cys- 18-3- 2043.\n 12. Cambria E, Wang H, White B. Guest editorial: Big social data \nanalysis. Knowl Based Syst. 2014;69:1–2. https://  doi. org/ 10. \n1016/j.  knosys. 2014. 07. 002.\n 13. Araque O, Corcuera-Platas I, Sánchez-Rada JF, Iglesias CA. \nEnhancing deep learning sentiment analysis with ensemble \ntechniques in social applications. Exp Syst App. 2017;77:236–\n46. https://  doi. org/ 10. 1016/j.  eswa. 2017. 02. 002.\n 14. Zhou Y, Yang Y, Liu H, Liu X, Savage N. Deep learning based \nfusion approach for hate speech detection. IEEE Access. \n2020;8:128923–9. https:// doi. org/ 10. 1109/ ACCESS. 2020. 30092 44.\n 15. Khan A, Sohail A, Zahoora U, Qureshi AS. A survey of the recent \narchitectures of deep convolutional neural networks. Artif Intell Rev. \n2020;53(8):5455–516. https:// doi. org/ 10. 1007/ s10462- 020- 09825-6.\n 16. Chau EC, Smit h NA. Specializing multilingual language mod -\nels: an empirical study. In: Proceedings of the 1st Workshop on \nMultilingual Representation Learning. Punta Cana, Dominican \nRepublic: Association for Computational Linguistics; 2021. p. \n51–61. https://  doi. org/ 10. 18653/ v1/ 2021. mrl-1.5 .\n 17. Saeed RMK, Rady S, Gharib TF. Optimizing sentiment classifica-\ntion for Arabic opinion texts. Cognit Comput. 2021;13(1):164–78. \nhttps:// doi. org/ 10. 1007/ s12559- 020- 09771-z.\n 18. Herbelot A, Zhu X, Palmer A, Schneider N, May J, Shutova E, \neditors. Proceedings of the Fourteenth Workshop on Semantic \nEvaluation. Barcelona (online): International Committee for Com-\nputational Linguistics; 2020.\n 19. Ferro N. What happened in CLEF... for a while? In: Experimental \nIR Meets Multilinguality, Multimodality, and Interaction. Cham: \nSpringer International Publishing; 2019. p. 3–45. https:// doi. org/ \n10. 1007/ 978-3- 030- 28577-7_1\n 20. Introducing the World’s Largest Open Multilingual Language Model: \nBLOOM. 2022. Available from: https:// bigsc ience. huggi ngface. co/ \nblog/ bloom.\n 21. Raunak V, Gupta V, Metze F. Effective dimensionality reduction \nfor word embeddings. In: Proceedings of the 4th Workshop on \nRepresentation Learning for NLP (RepL4NLP-2019). Florence, \nItaly: Association for Computational Linguistics; 2019. p. 235–43. \nhttps:// doi. org/ 10. 18653/ v1/ W19- 4328.\n 22. Raunak V, Kumar V, Gupta V, Metze F. On dimensional linguistic \nproperties of the word embedding space. In: Proceedings of the \n5th Workshop on Representation Learning for NLP. Online: Asso-\nciation for Computational Linguistics; 2020. p. 156–65. https:// \ndoi. org/ 10. 18653/ v1/ 2020. repl4 nlp-1. 19.\n 23. Truşcă MM, Aldea A, Grădinaru SE, Albu C. Post-processing \nand dimensionality reduction for extreme learning machine \nin text classification. Econ Comput Econ Cybern Stud Res. \n2021;55(4):37–50. https:// doi. org/ 10. 24818/ 18423 264/ 55.4. 21. 03.\n 24. Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman  \nR. Indexing by latent semantic analysis. J Am Soc Info Sci. 1990; \n41(6):391–407. https:// doi. org/ 10. 1002/ (SICI) 1097- 4571(199009) \n 41: 6< 391:: AID- ASI1>3. 0. CO;2-9.\n 25. Sun W, Du Q. Hyperspectral band selection: a review. IEEE Geo-\nsci Remote Sens Mag. 2019;7(2):118–39. https:// doi. org/ 10. 1109/ \nMGRS. 2019. 29111 00.\n 26. Solorio-Fernández S, Carrasco-Ochoa JA, Martínez-Trinidad JF. A \nreview of unsupervised feature selection methods. Artif Intell Rev. \n2020;53(2):907–48. https:// doi. org/ 10. 1007/ s10462- 019- 09682-y.\n 27. Singh KN, Devi SD, Devi HM, Mahanta AK. A novel approach \nfor dimension reduction using word embedding: an enhanced \ntext classification approach. Int J Info Manage Data Insights. \n2022;2(1):100061. https:// doi. org/ 10. 1016/j. jjimei. 2022. 100061.\n 28. Maxwell AE, Warner TA, Fang F. Implementation of machine-\nlearning classification in remote sensing: an applied review. Int \nJ Remote Sens. 2018;39(9):2784–817. https:// doi. org/ 10. 1080/ \n01431 161. 2018. 14333 43.\n 29. Patel AA. Hands-on unsupervised learning using Python: How \nto build applied machine learning solutions from unlabeled data. \nSebastopol, California: O’Reilly; 2019.\n 30. Hira ZM, Gillies DF. A review of feature selection and feature \nextraction methods applied on microarray data. Adv Bioinformat-\nics. 2015;2015:198363–13. https:// doi. org/ 10. 1155/ 2015/ 198363.\n610 Cognitive Computation  (2023) 15:590–612\n1 3\n 31. X u D, Yen IEH, Zhao J, Xiao Z. Rethinking network pruning – \nunder the pre-train and fine-tune paradigm. In: Proceedings of the \n2021 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technolo-\ngies. Online: Association for Computational Linguistics; 2021. p. \n2376–82. https:// doi. org/ 10. 18653/ v1/ 2021. naacl main. 188.\n 32. Bahdanau D, Bosc T, Jastrzebski S, Grefenstette E, Vincent P, Bengio \nY. Learning to compute word embeddings on the fly. 2017. https:// \ndoi. org/ 10. 48550/ ARXIV. 1706. 00286.\n 33. Johnson J, Douze M, Jégou H. Billion-scale similarity search with \nGPUs. IEEE Trans Big Data. 2021;7(3):535–47. https:// doi. org/ \n10. 1109/ TBDATA. 2019. 29215 72.\n 34. Mitra B, Craswell N. An introduction to neural information \nretrieval. Foundations and Trends® in Information Retrieval. \n2018;13(1):1–126. https:// doi. org/ 10. 1561/ 15000 00061.\n 35. Camastra F, Vinciarelli A. Feature extraction methods and mani-\nfold learning methods. In: Machine Learning for Audio, Image \nand Video Analysis. London: Springer London; 2008. p. 305–\n41. https:// doi. org/ 10. 1007/ 978-1- 84800- 007-0_ 11.\n 36. Egger R. In: Egger R, editor. Text representations and word \nembeddings. Cham: Springer International Publishing; 2022. p. \n335–61. https:// doi. org/ 10. 1007/ 978-3- 030- 88389-8_ 16.\n 37. Thirumoorthy K, Muneeswaran K. Feature selection for text clas-\nsification using machine learning approaches. Natl Acad Sci Lett. \n2022;45(1):51–6. https:// doi. org/ 10. 1007/ s40009- 021- 01043-0.\n 38. Strubell E, Ganesh A, McCallum A. Energy and policy considera-\ntions for deep learning in NLP. In: Proceedings of the 57th Annual \nMeeting of the Association for Computational Linguistics. Flor -\nence, Italy: Association for Computational Linguistics; 2019. p. \n3645–50. https:// doi. org/ 10. 18653/ v1/ P19- 1355.\n 39. Bengio Y, Courville A, Vincent P. Representation learning: a \nreview and new perspectives. IEEE Trans Pattern Anal Mach Intell. \n2013;35(8):1798–828. https:// doi. org/ 10. 1109/ TPAMI. 2013. 50.\n 40. Choi SW, Kim BHS. Applying PCA to deep learning forecasting \nmodels for predicting PM2.5. Sustainability. 2021;13(7).  https:// \ndoi. org/ 10. 3390/ su130 73726.\n 41. Menaga D, Revathi S. Probabilistic Principal Component Analysis \n(PPCA) based dimensionality reduction and deep learning for can-\ncer classification. In: Dash SS, Das S, Panigrahi BK, editors. Intell \nComput Appl. Singapore: Springer Singapore; 2021. p. 353–68. \nhttps:// doi. org/ 10. 1007/ 978- 981- 15- 5566-4_ 31.\n 42. Kushwaha N, Pant M. Textual data dimensionality reduction - a \ndeep learning approach. Multimedia Tools Appl. 2020;79(15–\n16):11039–50. https:// doi. org/ 10. 1007/ s11042- 018- 6900-x.\n 43. Pennington J, Socher R, Manning C. GloVe: Global Vectors for \nWord Representation. In: Proceedings of the 2014 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP). \nDoha, Qatar: Association for Computational Linguistics; 2014. \np. 1532–43. https:// doi. org/ 10. 3115/ v1/ D14- 1162.\n 44. Bojanowski P, Grave E, Joulin A, Mikolov T. Enriching word vec-\ntors with subword information. Trans Assoc Comput Linguistics. \n2017;5:135–46. https:// doi. org/ 10. 1162/ tacl_a_ 00051.\n 45. Pearson K. On lines and planes of closest fit to systems of \npoints in space. London Edinburgh Dublin Philos Mag J Sci. \n1901;2(11):559–72. https:// doi. org/ 10. 1080/ 14786 44010 94627 20.\n 46. Jolliffe IT, Cadima J. Principal component analysis: a review and \nrecent developments. Philos Trans Royal Soc Math Phys Eng Sci. \n2016;374(2065). https:// doi. org/ 10. 1098/ rsta. 2015. 0202.\n 47. Shimomo to EK, Portet F, Fukui K. Text classification based \non the word subspace representation. Pattern Anal Appl: PAA. \n2021;24(3):1075–93. https:// doi. org/ 10. 1007/ s10044- 021- 00960-6.\n 48. Song H, Zou D, Hu L, Yuan J. Embedding compression with right \ntriangle similarity transformations. In: Artificial Neural Networks \nand Machine Learning - ICANN 2020. Lecture Notes in Com -\nputer Science. Cham: Springer International Publishing; 2020. p. \n773–85. https:// doi. org/ 10. 1007/ 978-3- 030- 61616-8_ 62.\n 49. Choudhary R, Doboli S, Minai AA. A comparative study of meth-\nods for visualizable semantic embedding of small text corpora. In: \n2021 International Joint Conference on Neural Networks (IJCNN); \n2021. p. 1–8. https:// doi. org/ 10. 1109/ IJCNN 52387. 2021. 95342 50.\n 50. Hinton G, Roweis S. Stochastic neighbor embedding. In: Proceed-\nings of the 15th International Conference on Neural Information \nProcessing Systems. NIPS’02. Cambridge, MA, USA: MIT Press; \n2002. p. 857–64.\n 51. Huertas-García Á, Huertas-Tato J, Martín A, Camacho D. Coun-\ntering misinformation through semantic-aware multilingual mod-\nels. In: Intelligent Data Engineering and Automated Learning – \nIDEAL 2021. Cham: Springer International Publishing; 2021. p. \n312–23. https:// doi. org/ 10. 1007/ 978-3- 030- 91608-4_ 31.\n 52. Nogueira R, Jiang Z, Pradeep R, Lin J. Document ranking \nwith a pretrained sequence-to-sequence model. In: Findings of \nthe Association for Computational Linguistics: EMNLP 2020. \nOnline: Association for Computational Linguistics; 2020. p. \n708–18. https:// doi. org/ 10. 18653/ v1/ 2020. findi ngs- emnlp. 63.\n 53. Robertson S, Zaragoza H, Taylor M. Simple BM25 extension to \nmultiple weighted fields. In: Proceedings of the Thirteenth ACM \nInternational Conference on Information and Knowledge Manage-\nment. CIKM ’04. New York, NY, USA: Association for Computing \nMachinery; 2004. p. 42–9. https:// doi. org/ 10. 1145/ 10311 71. 10311 81.\n 54. Wardle C, Derakhshan H. Information disorder: Toward an inter-\ndisciplinary framework for research and policy making. Council of \nEurope; 2017. Available from: https:// rm. coe. int/ infor mation- disor der- \n toward- an- inter disci plina ry- frame workf or- resea rc/ 16807 6277c.\n 55. Carmi E, Yates SJ, Lockley E, Pawluczuk A. Data citizenship: \nRethinking data literacy in the age of disinformation, misinforma-\ntion, and malinformation. Internet Policy Rev. 2020;9(2). https:// \ndoi. org/ 10. 14763/ 2020.2. 1481.\n 56. Gaglani J, Gandhi Y, Gogate S, Halbe A. Unsupervised WhatsApp \nfake news detection using semantic search. In: 2020 4th Interna -\ntional Conference on Intelligent Computing and Control Systems \n(ICICCS); 2020. p. 285–9. https:// doi. org/ 10. 1109/ ICICC S48265. \n2020. 91209 02.\n 57. Huertas-García Á, Huertas-Tato J, Martín A, Camacho D. CIVIC-\nUPM at CheckThat!2021: Integration of transformers in misin -\nformation detection and topic classification. In: Proceedings of \nthe Working Notes of CLEF 2021 - Conference and Labs of the \nEvaluation Forum. vol. 2936 of CEUR Workshop Proceedings. \nBucharest, Romania: CEUR-WS.org; 2021. p. 520–30.\n 58. Martín A, Huertas-Tato J, Huertas-García Á, Villar-Rodríguez \nG, Camacho D. FacTeR-Check: Semi-automated fact-checking \nthrough semantic similarity and natural language inference. \nKnowl Based Syst. 2022;251:109265. https:// doi. org/ 10. 1016/j. \nknosys. 2022. 109265.\n 59. Grootendorst M. BERTopic: Neural topic modeling with a class-\nbased TF-IDF procedure. arXiv: arXiv: 2203. 05794 [Preprint]. 2022.\n 60. Grootendorst M. KeyBERT: Minimal keyword extraction with \nBERT. Zenodo; 2020. https:// doi. org/ 10. 5281/ zenodo. 44612 65.\n 61. Reimers N, Gurevych I. Making monolingual sentence embed -\ndings multilingual using knowledge distillation. In: Proceedings of \nthe 2020 Conference on Empirical Methods in Natural Language \nProcessing (EMNLP). Online: Association for Computational \nLinguistics; 2020. p. 4512–25. https:// doi. org/ 10. 18653/ v1/ 2020. \nemnlp- main. 365.\n 62. Muller KR, Mika S, Ratsch G, Tsuda K, Scholkopf B. An intro -\nduction to kernel-based learning algorithms. IEEE Trans Neural \nNetw. 2001;12(2):181–201. https:// doi. org/ 10. 1109/ 72. 914517.\n 63. Ross DA, Lim J, Lin RS, Yang MH. Incremental learning for \nrobust visual tracking. Int J Comput Vis. 2007;77(1–3):125–41. \nhttps:// doi. org/ 10. 1007/ s11263- 007- 0075-7.\n 64. Hyv ärinen A. Independent component analysis: Recent \nadvances. Philos Trans Royal Soc A Math Phys Eng Sci. \n2013;371(1984):20110534. https:// doi. org/ 10. 1098/ rsta. 2011. 0534.\n611Cognitive Computation  (2023) 15:590–612\n1 3\n 65. Sc hölkopf B, Smola A, Müller KR. Nonlinear component analysis \nas a Kernel Eigenvalue problem. Neural Comput. 1998;10(5):1299–\n319. https:// doi. org/ 10. 1162/ 08997 66983 00017 467.\n 66. McInnes L, Heal y J, Saul N, Großberger L. UMAP: Uniform \nManifold Approximation and Projection. J Open Source Softw. \n2018;3(29):861. https:// doi. org/ 10. 21105/ joss. 00861.\n 67. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, \nGrisel O, et al. Scikit-learn: Machine learning in Python. J Mach \nLearn Res. 2011;12:2825–30. https:// doi. org/ 10. 48550/ ARXIV. \n1201. 0490.\n 68. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. \nTransformers: State-of-the-art natural language processing. In: \nProceedings of the 2020 Conference on Empirical Methods in \nNatural Language Processing: System Demonstrations. Online: \nAssociation for Computational Linguistics; 2020. p. 38–45. https:// \ndoi. org/ 10. 18653/ v1/ 2020. emnlp- demos.6.\n 69. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled \nversion of BERT: Smaller, faster, cheaper and lighter. arXiv: \narXiv: 1910. 01108 [Preprint]. 2019.\n 70. Conneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, \nGuzmán F, et al. Unsupervised cross-lingual representation learn-\ning at scale. 2019. https:// doi. org/ 10. 48550/ ARXIV. 1911. 02116.\n 71. Liu Z, Lin W , Shi Y, Zhao J. A robustly optimized BERT pre-\ntraining approach with post-training. In: Chinese Computational \nLinguistics: 20th China National Conference, CCL 2021, Hohhot, \nChina, August 13-15, 2021, Proceedings. Berlin, Heidelberg: \nSpringer-Verlag; 2021. p. 471–84. https:// doi. org/ 10. 1007/ 978-3- \n030- 84186-7_ 31.\n 72. Feng F, Yang Y, Cer D, Arivazhagan N, Wang W. Language-\nagnostic BERT sentence embedding. In: Proceedings of the 60th \nAnnual Meeting of the Association for Computational Linguistics. \nvol.1. Dublin, Ireland: Association for Computational Linguistics; \n2022. p. 878–91. https:// doi. org/ 10. 18653/ v1/ 2022. acl- long. 62.\n 73. Reimers N, Beyer P, Gurevych I. Task-oriented intrinsic evalua -\ntion of semantic textual similarity. In: Proceedings of COLING \n2016, the 26th International Conference on Computational Lin -\nguistics: Technical Papers. Osaka, Japan: The COLING 2016 \nOrganizing Committee; 2016. p. 87–96.\n 74. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman S. GLUE: \na multi-task benchmark and analysis platform for natural language \nunderstanding. In: Proceedings of the 2018 EMNLP Workshop \nBlackboxNLP: Analyzing and Interpreting Neural Networks for \nNLP. Brussels, Belgium: Association for Computational Linguis-\ntics; 2018. p. 353–5. https:// doi. org/ 10. 18653/ v1/ W18- 5446.\n 75. Bishop CM. Pattern recognition and machine learning (informa -\ntion science and statistics). Berlin, Heidelberg: Springer-Verlag; \n2006.\n 76. Liu C. Enhanced independent component analysis and its applica-\ntion to content based face image retrieval. IEEE Trans Syst Man \nCybern B - Cybern. 2004;34(2):1117–27. https:// doi. org/ 10. 1109/ \nTSMCB. 2003. 821449.\n 77. Ekenel HK, Sankur B. Multiresolution face recognition. Image \nVis Comput. 2005;23(5):469–77. https:// doi. org/ 10. 1016/j. imavis. \n2004. 09. 002.\n 78. Laparra V, Camps-Valls G, Malo J. Iterative Gaussianization: From \nICA to random rotations. IEEE Trans Neural Netw. 2011;22(4):537–\n49. https:// doi. org/ 10. 1109/ TNN. 2011. 21065 11.\n 79. Cao J, Spielmann M, Qiu X, Huang X, Ibrahim DM, Hill AJ, et al. \nThe single-cell transcriptional landscape of mammalian organo -\ngenesis. Nature. 2019;566(7745):496.  https://  doi. org/ 10. 1038/ \ns41586- 019- 0969-x.\n 80. Carter S, Armstrong Z, Schubert L, Johnson I, Olah C. Activation \natlas. Distill. 2019. https:// doi. org/ 10. 23915/ disti ll. 00015.\nPublisher's Note  Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\n612 Cognitive Computation  (2023) 15:590–612\n1 3",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7899599671363831
    },
    {
      "name": "Dimensionality reduction",
      "score": 0.7003769278526306
    },
    {
      "name": "Bottleneck",
      "score": 0.5345209240913391
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5230575203895569
    },
    {
      "name": "Natural language processing",
      "score": 0.4460507333278656
    },
    {
      "name": "Algorithm",
      "score": 0.4336474537849426
    },
    {
      "name": "Machine learning",
      "score": 0.42913639545440674
    },
    {
      "name": "Embedded system",
      "score": 0.0
    }
  ]
}