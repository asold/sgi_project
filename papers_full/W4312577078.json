{
  "title": "Vision Transformer Based Model for Describing a Set of Images as a Story",
  "url": "https://openalex.org/W4312577078",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5016068746",
      "name": "Zainy M. Malakan",
      "affiliations": [
        "The University of Western Australia"
      ]
    },
    {
      "id": "https://openalex.org/A5023496317",
      "name": "Ghulam Mubashar Hassan",
      "affiliations": [
        "The University of Western Australia"
      ]
    },
    {
      "id": "https://openalex.org/A5089986388",
      "name": "Ajmal Mian",
      "affiliations": [
        "The University of Western Australia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2987862245",
    "https://openalex.org/W3049209276",
    "https://openalex.org/W3128339783",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2924176943",
    "https://openalex.org/W4312561350",
    "https://openalex.org/W3126515765",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2998106530",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W6783420912",
    "https://openalex.org/W4285186657",
    "https://openalex.org/W6600226620",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W4220790454",
    "https://openalex.org/W4282968790",
    "https://openalex.org/W3217340782",
    "https://openalex.org/W3130160131",
    "https://openalex.org/W4200438403",
    "https://openalex.org/W3035392611",
    "https://openalex.org/W4200034322",
    "https://openalex.org/W3083587785",
    "https://openalex.org/W4312463400",
    "https://openalex.org/W3017628311",
    "https://openalex.org/W2768287968",
    "https://openalex.org/W2979739834",
    "https://openalex.org/W2963033554",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W2559780844"
  ],
  "abstract": null,
  "full_text": "Vision Transformer Based Model for Describing\na Set of Images as a Story\nZainy M. Malakan1,2[0000−0002−6980−0992],\nGhulam Mubashar Hassan1[0000−0002−6636−8807], and\nAjmal Mian1[0000−0002−5206−3842]\n1 The University of Western Australia, Perth WA 6009, Australia\n{ghulam.hassan,ajmal.mian}@uwa.edu.au\n2 Umm Al-Qura University, Makkah 24382, Saudi Arabia\n{zmmalakan}@uqu.edu.sa\nAbstract. Visual Story-Telling is the process of forming a multi sen-\ntence story from a set of images. Appropriately including visual variation\nand contextual information captured inside the input images is one of\nthe most challenging aspects of visual storytelling. Consequently, stories\ndeveloped from a set of images often lack cohesiveness, relevance, and\nsemantic relationship. In this paper, we propose a novel Vision Trans-\nformer Based Model for describing a set of images as a story. The pro-\nposed method extracts the distinct features of the input images using a\nVision Transformer (ViT). Firstly, input images are divided into 16X16\npatches and bundled into a linear projection of flattened patches. The\ntransformation from a single image to multiple image patches captures\nthe visual variety of the input visual patterns. These features are used\nas input to a Bidirectional-LSTM which is part of the sequence encoder.\nThis captures the past and future image context of all image patches.\nThen, an attention mechanism is implemented and used to increase the\ndiscriminatory capacity of the data fed into the language model, i.e. a\nMogrifier-LSTM. The performance of our proposed model is evaluated\nusing the Visual Story-Telling dataset (VIST), and the results show that\nour model outperforms the current state of the art models.\nKeywords: Storytelling · Vision Transformer · Image Processing.\n1 Introduction\nVisual description or storytelling (VST) seeks to create a sequence of meaningful\nsentences to narrate a set of images. It has attracted significant interest from the\nvision to language field. However, compared to image [17,7] and video [24,28,18]\ncaptioning, narrative storytelling [21,14,19] has more complex structures and\nincorporates themes that do not appear explicitly in the given set of images.\nMoreover, describing a set of images is challenging because it demands algorithms\nto not only comprehend the semantic information, such as activities and objects\nin each of the five images along with their relationships, but also demands fluency\nin the phrases as well as the visually unrepresented notions.\narXiv:2210.02762v3  [cs.CV]  14 Jul 2023\n2 Zainy M. Malakan, Ghulam Mubashar Hassan, and Ajmal Mian\nFig. 1.An example of three vision description techniques includes a single picture\ncaption, story-like caption, and narrative storytelling, which is our aim.\nRecent storytelling techniques utilise sequence-to-sequence (seq2seq) models\n[20,14] to produce narratives based story on a set of images. The key idea behind\nthese approaches is to implement a convolutional neural network (CNN), ( i.e,\na sequence encoder), to extract the visual features of the set of images. Then,\ncombining these visual features, a complete set of image representations is ob-\ntained. The next step is to input this representational vector into a hierarchical\nlong-short-term memory (LSTM) model to form a sequence of sentences as a\nstory. This approach has dominated this area of research owing to its capacity\nto generate high-quality and adaptable narratives.\nFigure 1 illustrates the technical challenges between single image captioning\nstyle, isolation style, and storytelling style for a set of five images. For example,\nthe first sentence of all the three blocks in Figure 1 annotations show the follow-\ning: “A picture of cars around.”, “The car is parked in the street.”, and “I went\nto the park yesterday, and there were many cars there.”. The first description is\nknown as image captioning style which conveys the actual and physical picture\ninformation. The second description is known as storytelling in-isolation style\nwhich catches the image content as well, but it is not linked to the following\nsentence. The final description is known as first-person storytelling style which\nexplains more inferences about the image as a story-based sentence and also\nlinks to the subsequent sentence.\nVision Transformer Based Model for Describing a Set of Images as a Story 3\nIn order to solve the above challenges and difficulties, we propose a novel\nmethodology that explores the significance of spatial dimension conversion and\nits efficacy on Vision Transformer (ViT) [6] based model. Our method proceeds\nby extracting the feature vectors from the given images by dividing them into\n16X16 patches and feeding them into a Bidirectional-LSTM (Bi-LSTM). This\nmodels the visual patches as a temporal link among the set of images. By us-\ning the Bidirectional-LSTM, we represent the temporal link between patches\nin both forward and backward directions. To preserve the visual-specific con-\ntext and relevance, we convert the visual features and contextual vectors from\nBi-LSTM into a shared latent space using a Mogrifier-LSTM architecture [22].\nDuring the first layer’s gated modulation, the initial gating step scales the in-\nput embedding based on the ground truth context, producing a contextualized\nrepresentation of the input. This combination of multi-view feature extraction\nand highly context-dependent input information allows the language model to\nprovide more meaningful and contextual descriptions of the input set of images.\nThe following is a summary of the contributions presented in this paper:\n– We propose a novel ViT sequence encoder framework, that utilises multi-\nview visual information extraction for appropriate narrative based story on\nthe given set of images as input.\n– We take into account the context of the past as well as the future and\nemploy an attention mechanism over the contextualized characteristics that\nhave been obtained from Vision Transformer (ViT) to construct semantically\nrich narratives from a language model.\n– We propose to combine Mogrifier-LSTM with enriched visual characteristics\n(patches) and semantic inputs to generate data-driven narratives that are\ncoherent and relevant.\n– We demonstrate the utility of our proposed method through multiple eval-\nuation metrics on the largest known Visual Story-Telling dataset (VIST)\n[12]3. In addition, we compare the performance of our technique with exist-\ning state of the art techniques and show that it outperforms them on various\nevaluation metrics.\n2 Related Works\nThis section presents a review of literature on different visual captions that\ndirectly relate to narrative storytelling techniques, followed by the literature on\nvisual storytelling methods.\n2.1 Visual Understanding\nVisual understanding algorithms, which include image and video captioning, are\nthe most significant sort of networks utilized to tackle the problem of narrative\nstorytelling. Since it is most relevant to our study, we briefly discuss the recent\n3 https://visionandlanguage.net/VIST/\n4 Zainy M. Malakan, Ghulam Mubashar Hassan, and Ajmal Mian\nliterature on neural network-based image and video captioning. Typically, these\nmodels extract a vector of visual features using a CNN and then transmit this\nvector to a language model for caption synthesis.\nImage Captioning (IC) consists of a single frame ( i.e, an image) defined\nby a single phrase. Approaches may be further classified as rule-based methods\n[2,23] and deep learning-based methods [27,11]. The rule-based approaches apply\nthe traditional methodology of recognizing a restricted number of pre-defined\nobjects, activities and locations in the image, and describing them in natural\nlanguage using template-based techniques. On the other hand, due to recent\nadvances in deep learning, the vast majority of current methods are dependent on\ndeep learning as well as scientifically advanced techniques such as attention [31],\nreinforcement learning [29], semantic attributes integration [15], and modeling\nof subjects and objects [5]. However, none of these algorithms are designed to\nproduce a narrative-based description of a set or collection of images.\nVideo Captioning (VC)defined as multi-frame description that can explain\nmany frames (i.e, a video) in a single statement. VC and storytelling techniques\nare quite similar as they both utilize an encoder-decoder framework. The encoder\nis composed of a 2D/3D CNN that extracts visual information from a set of\ninput frames. This information is subsequently converted into normal language\nphrases using a decoder or a language model based on either a recurrent neural\nnetwork [4,25] or a transformer network [33,16,13]. Although VC methods can\ndescribe multi-frames in a single caption efficiently, it does not generate a story\nor multi-sentence descriptions for a given set of images.\n2.2 Storytelling Methods\nTelling a story based on a set of images is an easy task for humans, but an ex-\ntremely difficult task for machines. Coherent, relevant, and grammatically cor-\nrect sentences must be generated for a story-based description. For example, Park\net al. (2015) [26] illustrated that using bidirectional recurrent neural network\n(BRNN) is more efficient than a usual recurrent neural network (RNN) because\nBRNN captures forward and backward image features, which enables the model\nto interact with the whole story’s sentences. Similarly, Sequence-to-Sequence\n(Seq2Seq) techniques, which utilize CNN+Bi-LSTM [14]4 or CNN+GRU [30] as\nan encoder and RNN as a decoder enhanced storytelling prediction from a set\nof images.\nIn addition, the concept of designing composite rewards as a strategy for\nstorytelling problems was introduced [10] 5, which improved the natural flow of\nthe generated story. A novel decoder-encoder framework using Mogrifier-LSTM\n[20] was also proposed to improve the coherence, relevance, and information of\n4 https://github.com/tkim-snu/GLACNet\n5 https://github.com/JunjieHu/ReCo-RL\nVision Transformer Based Model for Describing a Set of Images as a Story 5\nthe generated story. Recently, the object detection technique usingYOLOv5 [21]\nis embedded with the encoder to improve the relevance of the story sentences.\nDifferent from previous works, our proposed method derives characteristics\nfrom the multiple visual features (i.e., patch features) based on the human-\nlike approach to generate stories. This helps to propose an approach that is\nboth computationally efficient and capable of producing coherent, relevant, and\ninformative stories.\nFig. 2.An overview of our proposed model which consists of a sequence encoder and\ndecoder. The sequence encoder process is implemented by both the Vision Transformer\n(ViT) and the Bidirectional-LSTM. The decoder process is performed by the Mogrifier-\nLSTM as well as the standard LSTM.\n3 Proposed Method\nFigure 2 presents the overall architecture of our proposed model which comprises\nof Vision Transformer (ViT), sequence encoder and decoder modules. In the\nfirst step, the image features are extracted using ViT, which divides each image\n6 Zainy M. Malakan, Ghulam Mubashar Hassan, and Ajmal Mian\ninto 16X16 patches and encodes them. Then all extracted image patch features\nare further encoded by Bidirectional-LSTM module which extracts the temporal\ncontext of the images. The connection between the sequence encoding and image\nfeatures is captured by the Attention module on two levels: the patch level and\nthe image-set patch level. Finally, the decoder module is responsible for the\ngeneration of a sequence of sentences as human story-like by making use of the\nMogrifier-LSTM architecture. The following discussion delves into the specifics\nof the aforementioned three modules.\n3.1 Vision Transformer (ViT)\nA set of I images are fed by the data-loader as Is = (I1, I2, ..., Is), where\nI = [I1, I2, ..., IN ] s.t. IN ∈ RH×W×C, (1)\ns ∈ {1, 2, 3, 4, 5} which is a set of five images with HxWxC (Height x Width\nx Channels) shape that presents a unique representation of storytelling from\nthe dataset. To extract image features, we utilized Vision Transformer (ViT)\n[6] which breaks the given I image into N equal-sized, non-overlapping patches\nof shape (P, P, C) and linearly maps each patch to a visual representation. We\ndefine the extracted features as the combination of patches from the ViT model\nas follows:\nI0 = [I1\npE; I2\npE; ...; IN\np E] s.t. E ∈ R(P2.C)×D (2)\nwhere P is the defined parameter as in grid order (left to right, up to down) while\nC represents the total number of channels. Then we flatten all patches which\nproduces n line feature vectors of shape (1 , P2⋆C). The patches that have been\nflattened are multiplied by a trainable embedding tensor of shape ( P2⋆C, D),\nwhich gains the ability to linearly project each flat patch to dimension D. As a\nresult, we produce rich embedded patches of shape n = (1, D) ∈ R(1,D).\n3.2 Features Encoding\nThe purpose of visual storytelling is first to comprehend the flow of events oc-\ncurring in each image and then to produce a consistent narrative similar to how\nhumans narrate a story. As a set of P = P1, P2, ..., Pl, where P represents the to-\ntal number of image patches included in I as well as the number of corresponding\ncontexts in each story. In order to represent these relationship features, we utilize\na Bidirectional-LSTM, which compiles the sequential information of P patches\nin both forward and backward direction. Our sequence encoder requires an input\nof image feature vector fi at every time step ‘t’ where i ∈ {1, 2, ..,5}. Eventually,\nthe sequence encoder part of the model encodes the whole image set, compris-\ning all the image patches and provides contextual information hse = [− →hse; ← −hse]\nthrough the final hidden-state at time step number t = 5.\nVision Transformer Based Model for Describing a Set of Images as a Story 7\n3.3 Story Generation\nSince modelling sequential inputs must lead to generating coherent sentences,\nthe solution to the challenge lies in how well the model learns the context.\nThis is particularly problematic for issues that need high levels of coherence\nand relevance. To solve this, we utilize the standard LSTM [9], which forms\nthe current hidden state denoted by h⟨t⟩, based on the previous hidden state,\nrepresented by hprev, and refreshes its memory state c⟨t⟩. Further, standard\nLSTM utilizes input gates Γi, forget gates Γf , and output gates Γo which are\ndetermined as follows:\nΓ⟨t⟩\nf = σ(Mf [hprev, wt] + Bf ), (3)\nΓ⟨t⟩\ni = σ(Mi[hprev, wt] + Bi), (4)\n˜c⟨t⟩ = tanh (Mc[hprev, wt] + Bc), (5)\nc⟨t⟩ = Γ⟨t⟩\nf ⊙ c⟨t−1⟩ + Γ⟨t⟩\ni ⊙ ˜c⟨t⟩, (6)\nΓ⟨t⟩\no = σ(Mo[hprev, wt] + Bo), (7)\nh⟨t⟩ = Γ⟨t⟩\no ⊙ tanh(c⟨t⟩) (8)\nwhere w is the word vector embedded in the input at time step ‘t’ (for simplicity,\nwe eliminate t), M∗ represents the transformation matrix that is learned at\neach state, B∗ are the biases, σ shows the logistic sigmoid function, and ⊙ is\nthe product of the vectors’ Hadamard transform. In our generation module, the\nattention vector ζi from the sequence encoder output is used to set up the LSTM\nhidden state h.\nFurthermore, we boost the standard LSTM functionality to generate more\ncohesive and relevant story-like sentences by integrating a Mogrifier-LSTM [22].\nThe two inputs, w and hprev, modulate each other in an odd and even fashion\nbefore being sent into the standard LSTM. In order to accomplish this goal,\nthe Mogrifier-LSTM instead scales the columns of each of its weight matrices\nthroughout M∗ via Mogrifier-LSTM gated modulation. In formal terms, w is\ngated based on the previous step hprev as gated input. A similar approach of\ngating prior time step output is used with the previous gated input.\nFollowing the completion of five rounds of mutual gating, as recommended by\nMalakan et al. [21], the most highly indexed versions of w and hprev are subse-\nquently fed into the standard LSTM in the order shown in Figure 2. Therefore, it\nmay also be stated as: mogrification (w, cprev, hprev) = LSTM (w↑, cprev, h↑\nprev)\nwhere w↑ and h↑\nprev are the most significant possible indexed for the LSTM\ninputs wi and hi\nprev respectively. Mathematically,\nwi = 2σ(Mi\nxhhi−1\nprev) ⊙ wi−2, for odd i ∈ [1, 2, ..., r], (9)\nhi\nprev = 2σ(Mi\nhxwi−1) ⊙ hi−2\nprev, for even i ∈ [1, 2, ..., r], (10)\n8 Zainy M. Malakan, Ghulam Mubashar Hassan, and Ajmal Mian\nwhere Hadamard product is ⊙, which w−1 = w, h0\nprev = hprev = ζi and r\nrepresents the total number of mogrification rounds which is a mogrifier hyper-\nparameter. In addition, the default standard LSTM configuration, with r set to\n0, operates without gated mogrification at the input stage. The use of matrix\nmultiplication with a constant of 2 ensures that the resulting transformations of\nthe matrices Mi\nxh and Mi\nhx are close to the identity matrix.\n3.4 Data Pre-processing And Model Training\nA vocab size of 6464 was extracted from the Visual Story-Telling dataset (VIST)\nwith a minimum word count threshold of 8. In addition, we used the size of 256\nas a dimension of word embedding vectors. Then, all VIST images are resized to\n224 × 224 pixels from the original size and used as an input to the pre-trained\nVision Transformer (ViT). For the training parameters, the Adam optimizer\nwas used, and the learning rate was set at 0.001, while the weight decay was\nestablished at 1e-5. Also, a teacher-forcing strategy was utilized in our proposed\nmodel to help the model train faster.\nAll of these settings were calibrated on our NVIDIA GPU, which has 12 GB\nof memory. For the maximum possible usage of available memory, the batch size\nwas set to 8 during training. This ensured that we obtained the most out of\nthe memory that was available to us. It’s worth mentioning that greater GPU\nmemory provides increased batch sizes, which assist the model to train faster.\nThe model has been successfully trained for a total of 83 epochs utilizing about\n390K steps. Finally, each epoch of our model was saved locally on our computer.\nThen, the optimum performance of the model was carefully chosen from epoch\n59 since, after epoch 59, the model began to overfit the data and the loss error\nbegan to increase, resulting in decreased model accuracy.\n4 Experiments and Results\nFirst, we introduce the Visual Story-Telling dataset (VIST) used to evaluate\nour proposed model. Next, we discuss the results of our proposed model and\ncompare them to other state of the art models. Finally, we give detailed analysis\nof a few cases in terms of the generated stories and the scores.\n4.1 Dataset\nVisual Story-Telling dataset (VIST) [12]6 is the only publicly accessible dataset\nthat we are aware for storytelling problems. It comprises 210,819 distinct images\nthat can be found in 10,117 different albums on Flickr and is arranged in sets of\nfive different images. Two types of stories accompany each set of images. One is\ncalled Description In Isolation (DII) and includes individual image descriptions\nthat can be useful for research in image captioning. The second one is called\n6 https://visionandlanguage.net/VIST/\nVision Transformer Based Model for Describing a Set of Images as a Story 9\nTable 1.A comparison of our proposed model with the recently published methods\non the Visual Story-Telling dataset (VIST). Quantitative results were obtained using\nseven different automated measures of evaluation. “-” indicates that the authors of the\ncorresponding study did not publish the results. The higher scores represent higher\naccuracy and the results in bold represent the best scores.\nModel B-1 B-2 B-3 B-4 CIDErROUGE-LMETEOR\nAREL 2018 [32] 0.5360.3150.1730.0990.038 0.286 0.352\nGLACNet 2018 [14] 0.56 0.3210.1710.0910.041 0.264 0.306\nHCBNet 2019 [1] 0.59 0.3480.1910.1050.051 0.274 0.34\nHCBNet(w/o prev. sent. attention) [1]0.59 0.3380.1800.0970.057 0.271 0.332\nHCBNet(w/o description attention) [1]0.58 0.3450.1940.1080.043 0.271 0.337\nHCBNet(VGG) 2019 [1] 0.59 0.34 0.1860.1040.051 0.269 0.334\nReCo-RL 2020 [10] - - - 0.1240.086 0.299 0.339\nBLEU-RL 2020 [10] - - - 0.1440.067 0.301 0.352\nVS with MPJA 2021 [8] 0.6010.3250.1330.0820.042 0.303 0.344\nCAMT 2021 [20] 0.64 0.3610.2010.1840.042 0.303 0.335\nRand+RNN 2021 [3] - - 0.1330.0610.022 0.272 0.311\nSAES Encoder-Decoder OD 2021 [21] 0.64 0.3630.1960.1060.051 0.294 0.330\nSAES Encoder-Decoder OD & Noun 2021 [21]0.63 0.3570.1950.1090.048 0.299 0.331\nSAES Encoder OD 2021 [21] 0.650.3720.2040.12 0.054 0.303 0.335\nOur Proposed Model 0.63 0.3750.2150.1230.044 0.310 0.354\nStory In Sequence (SIS) which is more relevant to storytelling problems and\ncomprises a whole paragraph in precisely five sentences representing a story. In\nall dataset statements, it is essential to note that the names of the individuals\nare adjusted by “[male and female]”, places by “[location]”, and organizations\nby “[organization]”.\n4.2 Performance Comparison\nAutomatic evaluation metrics are the most common technique for estimating\nthe effectiveness of the automatically generated story. Therefore, we validate\nour proposed model using automatic evaluation metrics, which also allows us to\ncompare it to the current state-of-the-art methods. Table 1 displays the most\nrecent frameworks used in storytelling challenges. These frameworks were pub-\nlished since 2018 and obtained promising results on the VIST dataset. We com-\npare our proposed model using multiple evaluation metrics, which are: BLEU-1,\nBLEU-2, BLEU-3, BLEU-4, CIDEr, METEOR, and ROUGE-L. The script for\ncomputing the evaluation measures was released by [10] 7.\nFrom the experiments, we observe that our model performs better than the\nstate-of-the-art models on all of the given evaluation measures, except for the\nBLEU-1, BLEU-4, and CIDEr. Table 1 presents the results for all of the men-\ntioned models sorted by the year in which they were released. Overall, our pro-\nposed model outperforms the compared models by 0.3 points in BLUE-2, 1.1\npoints in BLUE-3, 0.7 points in ROUGE-L and 0.2 points in METEOR.\n7 https://github.com/JunjieHu/ReCo-RL\n10 Zainy M. Malakan, Ghulam Mubashar Hassan, and Ajmal Mian\nFig. 3.Examples of our generated stories in comparison to the CAMT model [20] and\nground truth. Text highlighted in green indicates high relevance to the image/story,\nwhile text highlighted in yellow means that it is not highly relevant but instead contains\ngeneral information. BLEU-1 (B-1), BLEU-2 (B-2), BLEU-3 (B-3), BLEU-4 (B-4),\nROUGE (R), and METEOR (M) scores are shown below each story, with bold scores\nindicating the highest value.\nVision Transformer Based Model for Describing a Set of Images as a Story 11\n4.3 Storytelling Example Analysis\nAutomatic metrics are not a perfect reflection of the accuracy of the stories.\nTherefore, we conducted an in-depth analysis of the stories produced by our\nproposed model and the ground truth. In addition, we compared our stories\nwith stories produced by the recently proposed CAMT 2021 model [20].\nFigure 3 illustrates two different stories from a set of five images from our\nproposed model, followed by the stories that were Generated using CAMT. The\nhighlighted text in green shows parts that are highly relevant to the story, while\nthe highlighted text in yellow indicates less relevant or general information that\nis not obvious from the images. We also report the automatic evaluation metrics\nbelow each story in Fig. 3.\nText Generation Analysis: Both selected models have shown a persuasive\nexample of a narrative that is representative of how humans write a story, and\nboth of these examples are captivating. In contrast to CAMT model, our pro-\nposed model is able to extract more useful information from each input image.\nFor instance, the 3rd sentence in the first scenario shows more relevance to the\nstory, i.e. “they were so happy to be married,” as compared to CAMT model\nwhich predicted a less relevant sentence “they were very excited.” In addition, we\nnoticed that the third and the last sentences in the second examples, which are\n“They were all smiling” and “Everyone was happy to be at the event.”; the two\nsentences do not relate to the image itself in any manner, and the information\nthey provide seems to be generic and applicable to many images. On the other\nhand, our proposed model generated a story that was more logically consistent\nwith the story and relevant to the images.\nGenerated Story Scores: It is essential to demonstrate the model’s perfor-\nmance in contrast with traditional automated evaluation metrics. Each set of\nimages comes with a total of five different stories that were written by real peo-\nple (i.e., ground truth), as mentioned in section 4.1. One of these stories was\nextracted randomly and removed from the collection. Next, we compared the\nstory generated by our proposed model, CAMT model and the removed ground\ntruth story (we name this Human Generated Story or HGS) with the rest of the\nfour stories in the collection of VIST dataset. In BLEU-1, we found that our pro-\nposed model obtains the highest score on the second example, with almost 0.17\npoints more than HGS and 0.65 points more than the CAMT model; in BLEU-2,\nour generated story obtains over 0.14 points more than HGS and CAMT model;\nin BLEU-3, our model obtains 0.9 points more than the HGS and CAMT model\nin the first example, but the CAMT model receives 0.2 points more than ours\nand 0.16 points over the HGS in the second example; in BLEU-4, CAMT ob-\ntains almost 0.77 points more than our model and the HGS in the first example,\nwhile the HGS receives 0.74 points more than our model and 0.10 points over\nthe CAMT model in the second example; in ROUGE-L, we reported that our\nmodel obtains almost 0.2 points more than both the HGS and the CAMT model\n12 Zainy M. Malakan, Ghulam Mubashar Hassan, and Ajmal Mian\nin both examples; and in METEOR, we find that our model obtains almost\n0.7 points more than both the HGS and the CAMT model in both examples.\nOn the other hand, the performance of our proposed model is sufficiently high\nacross practically all of the automated evaluation metrics, with the exception of\nBLEU-4, as is shown through Figure 3.\n5 Conclusion\nThis article presented a novel storytelling approach for describing a set of images\nin a coherent manner. Our proposed framework is robust, which consists of a se-\nquence encoder that receives multi-view image patches from Vision Transformer\n(ViT) as an input to a Bidirectional-LSTM, and a decoder with a standard\nLSTM enhanced by Mogrifier-LSTM that has five rounds of mogrifircation. Fur-\nthermore, we utilize an attention mechanism that enables our model to capture\na specific significant context in response to a particular visual area while still\nkeeping the more significant story context in mind. We found that our proposed\nmodel performs better on most of the automatic evaluation metrics than current\nstate-of-the-art approaches except for BLEU-1, BLEU-4 and CIDEr scores. Ad-\nditionally, we presented a comprehensive analysis of multiple examples, which\nindicated that our generated stories are more relevant and coherent.\nAcknowledgments: This research received complete funding from the Aus-\ntralian Government, which was sponsored through the Australian Research Coun-\ncil (DP190102443).\nReferences\n1. Al Nahian, M.S., Tasrin, T., Gandhi, S., Gaines, R., Harrison, B.: A hierarchical\napproach for visual storytelling using image description. In: International Confer-\nence on Interactive Digital Storytelling. pp. 304–317. Springer (2019)\n2. do Carmo Nogueira, T., Vinhal, C.D.N., da Cruz J´ unior, G., Ullmann, M.R.D.:\nReference-based model using multimodal gated recurrent units for image caption-\ning. Multimedia Tools and Applications 79(41), 30615–30635 (2020)\n3. Chen, H., Huang, Y., Takamura, H., Nakayama, H.: Commonsense knowledge\naware concept selection for diverse and informative visual storytelling. arXiv\npreprint arXiv:2102.02963 (2021)\n4. Cho, K., Van Merri¨ enboer, B., Bahdanau, D., Bengio, Y.: On the proper-\nties of neural machine translation: Encoder-decoder approaches. arXiv preprint\narXiv:1409.1259 (2014)\n5. Ding, S., Qu, S., Xi, Y., Sangaiah, A.K., Wan, S.: Image caption generation with\nhigh-level image features. Pattern Recognition Letters 123, 89–95 (2019)\n6. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale (2020).\nhttps://doi.org/10.48550/ARXIV.2010.11929\nVision Transformer Based Model for Describing a Set of Images as a Story 13\n7. Fei, Z., Yan, X., Wang, S., Tian, Q.: Deecap: Dynamic early exiting for efficient\nimage captioning. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 12216–12226 (2022)\n8. Guo, Y., Wu, H., Zhang, X.: Steganographic visual story with mutual-perceived\njoint attention. EURASIP Journal on Image and Video Processing 2021(1), 1–14\n(2021)\n9. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n9(8), 1735–1780 (1997)\n10. Hu, J., Cheng, Y., Gan, Z., Liu, J., Gao, J., Neubig, G.: What makes a good story?\ndesigning composite rewards for visual storytelling. In: AAAI. pp. 7969–7976 (2020)\n11. Huang, L., Wang, W., Chen, J., Wei, X.Y.: Attention on attention for image cap-\ntioning. In: Proceedings of the IEEE International Conference on Computer Vision.\npp. 4634–4643 (2019)\n12. Huang, T.H., Ferraro, F., Mostafazadeh, N., Misra, I., Agrawal, A., Devlin, J.,\nGirshick, R., He, X., Kohli, P., Batra, D., et al.: Visual storytelling. In: Proceedings\nof the 2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies. pp. 1233–1239 (2016)\n13. Jiang, W., Zhou, W., Hu, H.: Double-stream position learning transformer net-\nwork for image captioning. IEEE Transactions on Circuits and Systems for Video\nTechnology (2022)\n14. Kim, T., Heo, M.O., Son, S., Park, K.W., Zhang, B.T.: Glac net: Glocal atten-\ntion cascading networks for multi-image cued story generation. arXiv preprint\narXiv:1805.10973 (2018)\n15. Li, G., Zhu, L., Liu, P., Yang, Y.: Entangled transformer for image captioning.\nIn: Proceedings of the IEEE International Conference on Computer Vision. pp.\n8928–8937 (2019)\n16. Li, L., Gao, X., Deng, J., Tu, Y., Zha, Z.J., Huang, Q.: Long short-term relation\ntransformer with global gating for video captioning. IEEE Transactions on Image\nProcessing 31, 2726–2738 (2022)\n17. Li, Y., Pan, Y., Yao, T., Mei, T.: Comprehending and ordering se-\nmantics for image captioning. In: 2022 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR). pp. 17969–17978 (2022).\nhttps://doi.org/10.1109/CVPR52688.2022.01746\n18. Lin, K., Li, L., Lin, C.C., Ahmed, F., Gan, Z., Liu, Z., Lu, Y., Wang, L.: Swinbert:\nEnd-to-end transformers with sparse attention for video captioning. In: Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\npp. 17949–17958 (2022)\n19. Liu, Y., Fu, J., Mei, T., Chen, C.W.: Let your photos talk: Generating narra-\ntive paragraph for photo stream via bidirectional attention recurrent neural net-\nworks. Proceedings of the AAAI Conference on Artificial Intelligence 31(1) (Feb\n2017). https://doi.org/10.1609/aaai.v31i1.10760, https://ojs.aaai.org/index.\nphp/AAAI/article/view/10760\n20. Malakan, Z.M., Aafaq, N., Hassan, G.M., Mian, A.: Contextualise, attend, mod-\nulate and tell: Visual storytelling. In: Proceedings of the 16th International\nJoint Conference on Computer Vision, Imaging and Computer Graphics The-\nory and Applications - Volume 5: VISAPP,. pp. 196–205. SciTePress (2021).\nhttps://doi.org/10.5220/0010314301960205\n21. Malakan, Z.M., Hassan, G.M., Jalwana, M.A.A.K., Aafaq, N., Mian, A.: Se-\nmantic attribute enriched storytelling from a sequence of images. In: 2021 Dig-\nital Image Computing: Techniques and Applications (DICTA). pp. 1–8 (2021).\nhttps://doi.org/10.1109/DICTA52665.2021.9647213\n14 Zainy M. Malakan, Ghulam Mubashar Hassan, and Ajmal Mian\n22. Melis, G., Koˇ cisk´ y, T., Blunsom, P.: Mogrifier lstm. In: International Confer-\nence on Learning Representations (2020), https://openreview.net/forum?id=\nSJe5P6EYvS\n23. Mogadala, A., Shen, X., Klakow, D.: Integrating image captioning with rule-based\nentity masking. arXiv preprint arXiv:2007.11690 (2020)\n24. Pan, B., Cai, H., Huang, D.A., Lee, K.H., Gaidon, A., Adeli, E., Niebles, J.C.:\nSpatio-temporal graph for video captioning with knowledge distillation. In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion. pp. 10870–10879 (2020)\n25. Pang, S., Chen, Z., Yin, F.: Video super-resolution using a hierarchical recur-\nrent multireceptive-field integration network. Digital Signal Processing122, 103352\n(2022)\n26. Park, C.C., Kim, G.: Expressing an image stream with a sequence of natural sen-\ntences. Advances in neural information processing systems 28 (2015)\n27. Phukan, B.B., Panda, A.R.: An efficient technique for image captioning using deep\nneural network. arXiv preprint arXiv:2009.02565 (2020)\n28. Seo, P.H., Nagrani, A., Arnab, A., Schmid, C.: End-to-end generative pretraining\nfor multimodal video captioning. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 17959–17968 (2022)\n29. Shen, X., Liu, B., Zhou, Y., Zhao, J., Liu, M.: Remote sensing image captioning\nvia variational autoencoder and reinforcement learning. Knowledge-Based Systems\np. 105920 (2020)\n30. Wang, J., Fu, J., Tang, J., Li, Z., Mei, T.: Show, reward and tell: Automatic\ngeneration of narrative paragraph from photo stream by adversarial training. In:\nThe AAAI Conference on Artificial Intelligence (AAAI), 2018. (February 2018)\n31. Wang, J., Wang, W., Wang, L., Wang, Z., Feng, D.D., Tan, T.: Learning visual\nrelationship and context-aware attention for image captioning. Pattern Recognition\n98, 107075 (2020)\n32. Wang, X., Chen, W., Wang, Y.F., Wang, W.Y.: No metrics are perfect: Adversarial\nreward learning for visual storytelling. arXiv preprint arXiv:1804.09160 (2018)\n33. Zhou, L., Zhou, Y., Corso, J.J., Socher, R., Xiong, C.: End-to-end dense video\ncaptioning with masked transformer. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. pp. 8739–8748 (2018)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8529812097549438
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7222996354103088
    },
    {
      "name": "Transformer",
      "score": 0.602618932723999
    },
    {
      "name": "Computer vision",
      "score": 0.5918359160423279
    },
    {
      "name": "Encoder",
      "score": 0.5320141315460205
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4410952627658844
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4179367125034332
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}