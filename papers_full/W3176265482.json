{
  "title": "MA-BERT: Learning Representation by Incorporating Multi-Attribute Knowledge in Transformers",
  "url": "https://openalex.org/W3176265482",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5101904472",
      "name": "You Zhang",
      "affiliations": [
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A5113414416",
      "name": "Jin Wang",
      "affiliations": [
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A5085633092",
      "name": "Liang-Chih Yu",
      "affiliations": [
        "Yuan Ze University"
      ]
    },
    {
      "id": "https://openalex.org/A5109266150",
      "name": "Xuejie Zhang",
      "affiliations": [
        "Yunnan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963499089",
    "https://openalex.org/W2492583839",
    "https://openalex.org/W2963305465",
    "https://openalex.org/W2970261805",
    "https://openalex.org/W2963467630",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963703618",
    "https://openalex.org/W2016043834",
    "https://openalex.org/W2251292973",
    "https://openalex.org/W2604205681",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2269331510",
    "https://openalex.org/W3008736151",
    "https://openalex.org/W4288375408",
    "https://openalex.org/W3104789011",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2097726431",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2941531368",
    "https://openalex.org/W2563010554",
    "https://openalex.org/W4211186029",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2740167620",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2758755084",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4205184193",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2108646579"
  ],
  "abstract": "Incorporating attribute information such as user and product features into deep neural networks has been shown to be useful in sentiment analysis.Previous works typically accomplished this in two ways: concatenating multiple attributes to word/text representation or treating them as a bias to adjust attention distribution.To leverage the advantages of both methods, this paper proposes a multi-attribute BERT (MA-BERT) to incorporate external attribute knowledge.The proposed method has two advantages.First, it applies multi-attribute transformer (MA-Transformer) encoders to incorporate multiple attributes into both input representation and attention distribution.Second, the MA-Transformer is implemented as a universal layer and stacked on a BERT-based model such that it can be initialized from a pre-trained checkpoint and fine-tuned for the downstream applications without extra pretraining costs.Experiments on three benchmark datasets show that the proposed method outperformed pre-trained BERT models and other methods incorporating external attribute knowledge.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2338\nMA-BERT: Learning Representation by Incorporating Multi-Attribute\nKnowledge in Transformers\nYou Zhang†, Jin Wang†, 1, Liang-Chih Yu‡, 2 and Xuejie Zhang†, 3\n†School of Information Science and Engineering, Yunnan University, Yunnan, P.R. China\n‡Department of Information Management, Yuan Ze University, Taiwan\nContact:{wangjin1, xjzhang3}@ynu.edu.cn, lcyu@saturn.yzu.edu.tw2\nAbstract\nIncorporating attribute information such as\nuser and product features into deep neural net-\nworks has been shown to be useful in senti-\nment analysis. Previous works typically ac-\ncomplished this in two ways: concatenating\nmultiple attributes to word/text representation\nor treating them as a bias to adjust attention dis-\ntribution. To leverage the advantages of both\nmethods, this paper proposes a multi-attribute\nBERT (MA-BERT) to incorporate external at-\ntribute knowledge. The proposed method has\ntwo advantages. First, it applies multi-attribute\ntransformer (MA-Transformer) encoders to in-\ncorporate multiple attributes into both input\nrepresentation and attention distribution. Sec-\nond, the MA-Transformer is implemented as a\nuniversal layer and stacked on a BERT-based\nmodel such that it can be initialized from\na pre-trained checkpoint and ﬁne-tuned for\nthe downstream applications without extra pre-\ntraining costs. Experiments on three bench-\nmark datasets show that the proposed method\noutperformed pre-trained BERT models and\nother methods incorporating external attribute\nknowledge.\n1 Introduction\nTo learn a distributed text representation for sen-\ntiment classiﬁcation (Pang and Lee, 2008; Liu,\n2012), conventional deep neural networks, such as\nconvolutional neural networks (CNN) (Kim, 2014)\nand long short-term memory (LSTM) (Hochre-\niter and Schmidhuber, 1997), and common in-\ntegration technics, such as self-attention mecha-\nnisms (Vaswani et al., 2017; Chaudhari et al., 2019)\nand dynamic routing algorithms (Gong et al., 2018;\nSabour et al., 2017), are usually applied to com-\npose the vectors of constituent words. To further en-\nhance the performance, pre-trained models (PTMs),\nsuch as BERT (Devlin et al., 2019), ALBERT (Lan\net al., 2019), RoBERTa (Liu et al., 2019), and XLM-\nText\nAttr\nWord \nEmbeddings\nFeature\nExtractor\n(a) Word representation\nText\nAttr\nWord \nEmbeddings\nFeature\nExtractor\n(b) Text representation\nAttribute \nEmbeddings\nAttribute \nEmbeddings\nText\nAttr\nWord \nEmbeddings\nFeature\nExtractor\nAttribute \nEmbeddings\nAttention \nmap\n(c) Bias terms in self-attention (d) Bilinear interaction in multi-attribute \ntransformer\nText\nAttr\nWord \nEmbeddings\nAttribute \nEmbeddings\nQ\nK\nV\nScaled Dot-Product \nAttention\nFigure 1: Different strategies to incorporate external\nattribute knowledge into deep neural networks.\nRoBERTa (Conneau et al., 2019) can be ﬁne-tuned\nand transferred for sentiment analysis tasks. Practi-\ncally, PTMs were ﬁrst fed a large amount of unan-\nnotated data, and trained using a masked language\nmodel or next sentence prediction to learn the usage\nof various words and how the language is written\nin general. Then, the models are transferred to an-\nother task to be fed another smaller task-speciﬁc\ndataset.\nThe abovementioned methods only use features\nfrom plain texts. Incorporating attribute informa-\ntion such as users and products can improve senti-\nment analysis task performance. Previous works\ntypically incorporated such external knowledge by\nconcatenating these attributes into word and text\nrepresentations (Tang et al., 2015), as shown in\nFigs. 1(a) and (b). Such methods are often intro-\nduced in shallow models to attach attribute informa-\ntion to modify the representation of either words\nor texts. However, this may lack interaction be-\ntween attributes and the text since it equally aligns\nwords to attribute features, thus the model is un-\nable to emphasize important tokens. Several works\nhave used attribute features as a bias term in self-\nattention mechanisms to model meaningful rela-\n2339\ntions between words and attributes (Wu et al., 2018;\nChen et al., 2016b; Dong et al., 2017; Dou, 2017),\nas shown in Fig. 1(c). By using the softmax func-\ntion for normalization to calculate the attention\nscore, the incorporated attribute features only im-\npact the allocation of the attention weights. As a re-\nsult, the representation of input words has not been\nupdated, and the information of these attributes\nwill be lost. For example, depending on individual\npreferences for chili, readers may focus on reviews\ntalking about spicy, but only those who like chili\nwould consider such review recommendations use-\nful. However, current self-attention models that\nlearn text representations by adjusting the weights\nof spicy may still produce the same word repre-\nsentation of spicyfor different persons, leading to\nconfusion in distinguishing people who like chili\nor not.\nTo address the above problems, this study pro-\nposes a multi-attribute BERT (MA-BERT) model\nwhich applies multi-attribute transformer (MA-\nTransformer) encoders to incorporate external at-\ntribute knowledge. Different from being incorpo-\nrated into the attention mechanism as bias terms,\nmultiple attributes can be injected into both atten-\ntion maps and input token representations using\nbilinear interaction, as shown in Fig. 1(d). In ad-\ndition, the MA-Transformer is implemented as a\nuniversal layer and stacked on a BERT-based model\nsuch that it can be initialized from a pre-training\ncheckpoint and ﬁne-tuned for downstream tasks\nwithout extra pre-training costs. Experiments are\nconducted on three benchmark datasets (IMDB,\nYelp-2013, and Yelp-2014) for sentiment polarity\nclassiﬁcation. The results show that the proposed\nMA-BERT model outperformed pre-trained BERT\nmodels and other methods incorporating external\nattribute knowledge.\nThe remainder of this paper is organized as fol-\nlows. Section 2 provides a detailed description of\nthe proposed methods. The empirical experiments\nare reported with analysis in Section 3. Conclu-\nsions are ﬁnally drawn in Section 4.\n2 Multi-Attribute BERT Model\nFig. 2 shows an overview of the MA-BERT model.\nIt mainly consists of two parts, including a BERT-\nbased PTM model and several MA-Transformer\nencoders as extra layers stacked on BERT. Both\ncomponents are described in detail below.\nw1\nE[cls] E1\n[CLS]\nt[CLS] t1\nE2 EL-1\nt2 tL-1\nw2 wL-1...\n...\n...\n...\n...\nBERT\nSingle text si\nMA-Transformer Encoders\nSample Tuples\nEA\nAi\nMA-BERT\n(A, s)\nh[CLS]\nFigure 2: Overall architecture of the MA-BERT model.\n2.1 BERT Encoder\nBy applying a word piece tokenizer (Wu et al.,\n2016), the input text can be denoted as a sequence\nof tokens, i.e., s = {w0,w1,w2,...,w L−1},\nwhere L is the length of the text and w0 =\n[CLS] is a special classiﬁcation token. More-\nover, its corresponding attributes are denoted as\nA = {a1,a2,...,a M }, where M is the number of\nattributes in the text. Thus, the i-th input sample\ncan be denoted as a tuple, i.e., (Ai,si).\nTo learn the hidden representation, the pre-\ntrained language model BERT (Devlin et al., 2019)\nwas used, achieving impressive performance for\nvarious natural language processing (NLP) tasks.\nWe then fed the token sequence into the BERT\nmodel to obtain the representation, denoted as,\nT = [t0,...,t L−1] =fBERT([w0,...,w L−1]; θBERT) (1)\nwhere T ∈RL×dt is the output representation of\nall tokens; θBERT is the trainable parameters of\nBERT, which is initialized from a pretrained check-\npoint and then ﬁne-tuned during the model training;\ndt=768 is the dimensionality of the output repre-\nsentation.\nAccording to Wu et al. (2018) and Wang et al.\n(2017), all the attributes are mapped to attribute\nembeddings EA = [EA,1,EA,2,...,E A,M ] ∈\nRM×dE , which are randomly initialized and up-\ndated in the following training phase.\nMulti-Attribute Attention. To incorporate mul-\ntiple attributes into the MA-Transformer, we in-\ntroduce multi-attribute attention (MAA), which is\nexpressed as,\nY = MAA(T,EA)= [U1,...,U M ] Wo (2)\nUm = Att(T,EA,m) =softmax\n(\nQmKm\n⊤\n√\nd\n)\nVm (3)\n2340\nwhere Um is the attention from m-th attribute;\nWo ∈R(M·d)×dt is the output linear projection\nand ddenotes the dimensionality of Q, K and V;\nQ, Kand V are matrices that package the queries,\nkeys and values, which are deﬁned as,\nQm = T ·Wq,m ⊙EA,m (4)\nKm = T ·Wk,m ⊙EA,m (5)\nVm = T ·Wv,m ⊙EA,m (6)\nwhere Qm, Km and Vm ∈ RL×dE are bilinear\ntransformations (Huang et al., 2019) applied on\nthe input representation T and attribute representa-\ntion EA,m. Wq,m, Wk,m and Wv,m ∈Rdt×dE are\nweight matrices for query, key and value projec-\ntions, and ·and ⊙respectively denote the inner and\nthe Hadamard product.\nSimilar to Vaswani et al. (2017), we also intro-\nduced multi-head mechanism for MA-Transformer,\ndenoted as,\nUm =\nK\n⊕\nk=1\nAtt(T,Ek\nA,m) ∈RL×(K·dE) (7)\nwhere K is the number of heads for each at-\ntribute and ⊕denotes the concatenation operator;\nEk\nA,m ∈RdE is the m-th attribute representation\nin the k-th head, and its dimensionality should be\nensured that dE = dt/K. Given that different\nheads can capture different relation types along\nwith text representations, different parameters are\nconsidered for different heads.\n2.2 MA-Transformer\nTaking the representation of both text T and at-\ntribute A as input, an MA-Transformer encoder\nthen processes the same as a standard transformer\nencoder (Vaswani et al., 2017) to generate Y ∈\nRL×dt. Then, Y is connected by a normalization\nlayer and a residual layer from the input represen-\ntation T. The intermediate output is then passed to\na two-layered feed-forward network with a recti-\nﬁed linear unit (ReLU) activate function. Similarly,\nresidual and normalization layers are connected to\ngenerate the ﬁnal output which is taken as the input\nfor the next encoder.\nBy stacking several MA-Transformer encoders\non the BERT model, the MA-BERT model gener-\nates a review representation h[CLS] consistent with\nthe special token [CLS]. Then, a classiﬁer com-\nprised of a linear projection and a softmax activa-\ntion (with the dimension identical to the number of\nclasses) is used for classiﬁcation.\n3 Comparative Experiments\nDatasets. Following the experimental settings\nused in Tang et al. (2015), the proposed MA-\nBERT model is evaluated using three benchmark\ndatasets 1 (IMDB, Yelp-2013, and Yelp-2014). The\nevaluation metrics include accuracy (Acc.) and root\nmean squared error ( RMSE). Higher Acc. and\nlower RMSE scores indicate higher performance.\nImplementation Details. The baseline meth-\nods can be divided into three groups. The ﬁrst\ngroup includes the methods without user and prod-\nuct information such as CNN (Kim, 2014), BiL-\nSTM (Hochreiter and Schmidhuber, 1997), neu-\nral sentiment classiﬁcation ( NSC) (Chen et al.,\n2016a) and its variant with a local attention mech-\nanism (NSC+LA). For the BERT-based methods,\nthe uncased-base-BERT model consisting of 12\nlayers of transformer encoders was implemented\nfor comparison. ToBERT(Pappagari et al., 2019)\nwas trained non-end2end using a word-to-segment\nstrategy in a two-stage way.\nThe second group includes existing methods\nincorporating user and product information such\nas NSC with user (U) and product (P) informa-\ntion incorporated into an attention (A) mecha-\nnism (NSC+UPA), user product neural network\n(UPNN) (Tang et al., 2015), hierarchical model\nwith separate user attention and product atten-\ntion (HUAPA) (Wu et al., 2018), and the chunk-\nwise importance matrix model (CHIM) (Amplayo,\n2019).\nThe third group includes a set of BERT-based\nmethods incorporating user and product infor-\nmation using different strategies, presented in\nFigs. 1(a)-(c). In detail, an uncased-base-BERT\nmodel ﬁrst extracted ﬁxed feature vectors from\ntexts. Then, the BERT Concat (word)model in-\ncorporates attribute features into each word vec-\ntor and stacks another 6 transformer encoders as\nthe feature extractor. Similarly, the BERT Con-\ncat (text)incorporates attribute features into the\nrepresentation of the special token [CLS] for the\nclassiﬁcation. Finally, the BERT Attention (bias)\napplied 6 more MA-Transformers which only in-\nject attributes into Qand Kto calculate attention\nscore instead of V in Eq. (6).\nThe proposed MA-BERT models applied 6 MA-\nTransformer encoders to incorporate user and prod-\nuct attributes, and stacking over the BERT model.\n1http://ir.hit.edu.cn/˜dytang/paper/\nacl2015/dataset.7z\n2341\nModels IMDB Yelp-2013 Yelp-2014\nAcc. (%) RMSE Acc. (%) RMSE Acc. (%) RMSE\nwithout user and product information\nCNN (UPNN w/o UP) 40.5 1.629 57.7 0.812 58.5 0.808\nBiLSTM 43.3 1.494 58.4 0.764 59.2 0.733\nNSC 44.3 1.465 62.7 0.701 63.7 0.686\nNSC+LA 48.7 1.381 63.1 0.706 63.0 0.715\nBERT 51.8 1.191 67.7 0.627 67.2 0.630\nToBERT 50.8 1.194 66.7 0.626 66.9 0.620\nwith user and product information\nUPNN 43.5 1.602 59.6 0.803 60.8 0.764\nNSC+UPA 53.3 1.281 65.0 0.692 66.7 0.654\nHUAPA 55.0 1.185 68.3 0.628 68.6 0.626\nCHIMembedding 56.4 1.161 67.8 0.646 69.2 0.629\nBERT Concat (word) 56.8 1.106 69.9 0.602 70.9 0.582\nBERT Concat (text) 54.6 1.168 68.5 0.616 71.0 0.590\nBERT Attention (bias) 52.5 1.177 68.0 0.635 67.6 0.617\nMA-BERT 57.3 1.042 70.3 0.588 71.4 0.573\nTable 1: Comparative results of different methods for sentiment classiﬁcation. The boldface ﬁgures indicate the\nbest results among all methods and underscored ﬁgures represent the best performance for each group of methods.\nAll results are averaged over ﬁve runs.\nEach attribute is initialized in a uniform distribu-\ntion U ∼(−0.25,0.25) with the dimension of 768\n(dt) and head number of 12 ( K). Thus, the di-\nmension of each head (dE) is set to 64. All other\nhyper-parameters in MA-Transformer encoders are\nidentical with BERT-transformer encoders due to\ntheir isomorphic structure. For all models, the\nAdamW (Loshchilov and Hutter, 2017) optimizer\nwas used with a base learning rate of 2e-5 in a\nwarmup linear schedule. Early stopping (Prechelt,\n1998) strategy with a patience of 3 epochs was also\napplied to avoid overﬁtting. The code for this paper\nis available at: https://github.com/yoyo-yun/\nMA-Bert.\nComparative Results and Discussion. Table 1\nshows the comparative results of different methods\nfor sentiment ordinal classiﬁcation. For models\nwithout user and product attributes, BiLSTM out-\nperforms CNN (UPNN w/o UP), due to its abil-\nity to encode text. Furthermore, both NSC and\nNSC+LA outperformed BiLSTM mainly because\nof its hierarchical structure.\nIncorporating the user and product attributes im-\nproved performance. For example, UPNN achieved\na better result than its variant without user and prod-\nuct attributes, i.e., CNN (UPNN w/o UP). In addi-\ntion, both NSC+UPA and HUAPA introduced the\nuser and product information as a bias to guide the\nhierarchical attention, and thus outperformed NSC\nand NSC+LA.\nThe proposed MA-BERT achieved the best per-\nformance on all datasets. Compared with baselines\nwithout user and product attributes, the MA-BERT\ncan leverage implicit attribute features to boost\nperformance. MA-BERT outperformed methods\nalready incorporating user and product attributes\n(i.e., NSC+UPA, HUAPA and CHIMembedding) be-\ncause the proposed model can incorporate attribute\nknowledge to both the attention map and input rep-\nresentation.\nThe BERT and ToBERT models achieved im-\nprovement on all datasets against the conventional\nmodels, due to the large knowledge migration from\npre-training. Unfortunately, a lack of implicit extra\nfeatures resulted in performance lower than that\nof the proposed MA-BERT model. MA-BERT\nalso outperformed BERT Concat (word), BERT\nConcat (text) and BERT Attention (bias), indicat-\ning that the proposed MA-Transformer architecture\ncan improve existing incorporation strategies. Fur-\nthermore, the proposed MA-BERT could be initial-\nized from the pre-trained checkpoint of BERT, thus\nmaking full use of the parameter settings without\nbringing additional costs for pre-training.\n4 Conclusion\nThis paper proposes a MA-BERT model capable of\nincorporating multiple attributes into BERT-based\nPTMs for learning attribute-speciﬁc text representa-\ntion. Different from existing attention models, the\nMA-Transformer can inject external knowledge\nto both attention maps and the input representa-\ntion.Additionally, the proposed model could be ex-\ntended to other tasks by using the MA-Transformer\nencoder as a universal layer and stacking it on a\nBERT-based model. Future work will attempt to\n2342\nincorporate such or similar multiple attributes into\nPTMs in the pre-training phases.\nAcknowledgments\nThis work was supported by the National Natural\nScience Foundation of China (NSFC) under Grant\nNo. 61966038 and 61762091, and by the Ministry\nof Science and Technology, Taiwan, ROC, under\nGrant No. MOST107-2628-E-155-002-MY3. The\nauthors would like to thank the anonymous review-\ners for their constructive comments.\nReferences\nReinald Kim Amplayo. 2019. Rethinking Attribute\nRepresentation and Injection for Sentiment Classi-\nﬁcation. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5601–5612, Stroudsburg, PA, USA. Association for\nComputational Linguistics.\nSneha Chaudhari, Gungor Polatkan, Rohan Ramanath,\nand Varun Mithal. 2019. An Attentive Survey of At-\ntention Models. arXiv preprint arXiv:1904.02874.\nHuimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin,\nand Zhiyuan Liu. 2016a. Neural Sentiment Classiﬁ-\ncation with User and Product Attention. In Proceed-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP-2016) , pages\n1650–1659.\nTao Chen, Ruifeng Xu, Yulan He, Yunqing Xia, and\nXuan Wang. 2016b. Learning User and Product Dis-\ntributed Representations Using a Sequence Model\nfor Sentiment Analysis. IEEE Computational Intel-\nligence Magazine, 11(3):34–44.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\nCross-lingual Representation Learning at Scale.\narXiv preprint arXiv:1911.02116.\nJ. Devlin, M. W. Chang, K. Lee, and K. Toutanova.\n2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT 2019), pages 4171–4186.\nLi Dong, Shaohan Huang, Furu Wei, Mirella Lapata,\nMing Zhou, and Ke Xu. 2017. Learning to gen-\nerate product reviews from attributes. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics\n(EACL-2017), volume 1, pages 623–632.\nZi-Yi Dou. 2017. Capturing User and Product Informa-\ntion for Document Level Sentiment Analysis with\nDeep Memory Network. In Proceedings of the con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP-2017), pages 521–526.\nJingjing Gong, Xipeng Qiu, Shaojing Wang, and Xuan-\njing Huang. 2018. Information Aggregation via Dy-\nnamic Routing for Sequence Encoding. In Proceed-\nings of the 27th International Conference on Com-\nputational Linguistics, pages 2742–2752.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation ,\n9(8):1735–1780.\nTongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019.\nFibinet: Combining feature importance and bilinear\nfeature interaction for click-through rate prediction.\nIn 13th ACM Conference on Recommender Systems\n(RecSys-2019), pages 169–177.\nYoon Kim. 2014. Convolutional Neural Networks\nfor Sentence Classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP-2014), pages 1746–\n1751, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. arXiv\npreprint arXiv:1909.11942.\nBing Liu. 2012. Sentiment analysis and opinion min-\ning. Synthesis Lectures on Human Language Tech-\nnologies, 5(1):1–167.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled Weight Decay Regularization. arXiv preprint\narXiv:1711.05101.\nBo Pang and Lillian Lee. 2008. Opinion Mining and\nSentiment Analysis: Foundations and Trends in In-\nformation Retrieval. Foundations and Trends® in\nInformation Retrieval, 2(1-2):1–135.\nRaghavendra Pappagari, Piotr ˙Zelasko, Jes ´us Villalba,\nYishay Carmiel, and Najim Dehak. 2019. Hierarchi-\ncal Transformers for Long Document Classiﬁcation.\narXiv preprint arXiv:1910.10781.\nLutz Prechelt. 1998. Automatic early stopping using\ncross validation: quantifying the criteria. Neural\nNetworks, 11(4):761–767.\nSara Sabour, Nicholas Frosst, and Geoffrey E Hinton.\n2017. Dynamic Routing Between Capsules. In Pro-\nceedings of the 31st Conference on Neural Informa-\ntion Processing Systems (NIPS-2017) , pages 3859–\n3869.\n2343\nDuyu Tang, Bing Qin, and Ting Liu. 2015. Learning\nSemantic Representations of Users and Products for\nDocument Level Sentiment Classiﬁcation. In Pro-\nceedings of the 53rd Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing (ACL-2015), pages 1014–1023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in neural information pro-\ncessing systems(nips-2017), pages 5598–6008.\nWenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and\nXiaokui Xiao. 2017. Coupled multi-layer attentions\nfor co-extraction of aspect and opinion terms. In the\nProceedings of the 31st AAAI Conference on Artiﬁ-\ncial Intelligence (AAAI-2017), pages 3316–3322.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s Neural Machine\nTranslation System: Bridging the Gap between\nHuman and Machine Translation. arXiv preprint\narXiv:1609.08144.\nZhen Wu, Xin Yu Dai, Cunyan Yin, Shujian Huang,\nand Jiajun Chen. 2018. Improving review repre-\nsentations with user attention and product attention\nfor sentiment classiﬁcation. In Proceedings of The\nThirty-Second AAAI Conference on Artiﬁcial Intelli-\ngence (AAAI-18), pages 5989–5996, New Orleans,\nLouisiana, USA.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6983684301376343
    },
    {
      "name": "Transformer",
      "score": 0.6274847388267517
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4688110649585724
    },
    {
      "name": "Representation (politics)",
      "score": 0.41093218326568604
    },
    {
      "name": "Natural language processing",
      "score": 0.35123544931411743
    },
    {
      "name": "Engineering",
      "score": 0.13406231999397278
    },
    {
      "name": "Electrical engineering",
      "score": 0.10114419460296631
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I189210763",
      "name": "Yunnan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99908691",
      "name": "Yuan Ze University",
      "country": "TW"
    }
  ]
}