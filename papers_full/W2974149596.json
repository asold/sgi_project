{
  "title": "Adapting Language Models for Non-Parallel Author-Stylized Rewriting",
  "url": "https://openalex.org/W2974149596",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5073911898",
      "name": "Bakhtiyar Syed",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5016194897",
      "name": "Gaurav Verma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5059650977",
      "name": "B. Srinivasan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5087775309",
      "name": "N Anandhavelu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5073786634",
      "name": "Vasudeva Varma",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2152917816",
    "https://openalex.org/W6670934142",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6671632903",
    "https://openalex.org/W1879024793",
    "https://openalex.org/W6745881328",
    "https://openalex.org/W1939882552",
    "https://openalex.org/W2116899917",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W6680001452",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W6741121127",
    "https://openalex.org/W2131526828",
    "https://openalex.org/W6754422564",
    "https://openalex.org/W6681835404",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W1919907269",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2195242508",
    "https://openalex.org/W6760931613",
    "https://openalex.org/W6751230656",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W189559790",
    "https://openalex.org/W2793585215",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W6738394178",
    "https://openalex.org/W6793926042",
    "https://openalex.org/W6756029386",
    "https://openalex.org/W2951854283",
    "https://openalex.org/W2973330127",
    "https://openalex.org/W2022204871",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2964008635",
    "https://openalex.org/W2952566153",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963062932",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2964331441",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2735574368",
    "https://openalex.org/W2949696181",
    "https://openalex.org/W2084807254",
    "https://openalex.org/W2080675590",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2898799786",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2133365465",
    "https://openalex.org/W2962753250",
    "https://openalex.org/W2146888100",
    "https://openalex.org/W2927085091",
    "https://openalex.org/W2963631950",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2897827272",
    "https://openalex.org/W2963241138",
    "https://openalex.org/W2133564696"
  ],
  "abstract": "Given the recent progress in language modeling using Transformer-based neural models and an active interest in generating stylized text, we present an approach to leverage the generalization capabilities of a language model to rewrite an input text in a target author's style. Our proposed approach adapts a pre-trained language model to generate author-stylized text by fine-tuning on the author-specific corpus using a denoising autoencoder (DAE) loss in a cascaded encoder-decoder framework. Optimizing over DAE loss allows our model to learn the nuances of an author's style without relying on parallel data, which has been a severe limitation of the previous related works in this space. To evaluate the efficacy of our approach, we propose a linguistically-motivated framework to quantify stylistic alignment of the generated text to the target author at lexical, syntactic and surface levels. The evaluation framework is both interpretable as it leads to several insights about the model, and self-contained as it does not rely on external classifiers, e.g. sentiment or formality classifiers. Qualitative and quantitative assessment indicates that the proposed approach rewrites the input text with better alignment to the target style while preserving the original content better than state-of-the-art baselines.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nAdapting Language Models for Non-Parallel Author-Stylized Rewriting\nBakhtiyar Syed,‡ Gaurav V erma,† Balaji Vasan Srinivasan,†\nAnandhavelu Natarajan,† Vasudeva Varma‡\n‡IIIT Hyderabad,†Adobe Research\nsyed.b@research.iiit.ac.in\n{gaverma, balsrini, anandvn}@adobe.com\nvv@iiit.ac.in\nAbstract\nGiven the recent progress in language modeling using\nTransformer-based neural models and an active interest in\ngenerating stylized text, we present an approach to lever-\nage the generalization capabilities of a language model to\nrewrite an input text in a target author’s style. Our proposed\napproach adapts a pre-trained language model to generate\nauthor-stylized text by ﬁne-tuning on the author-speciﬁc cor-\npus using a denoising autoencoder (DAE) loss in a cascaded\nencoder-decoder framework. Optimizing over DAE loss al-\nlows our model to learn the nuances of an author’s stylewith-\nout relying on parallel data, which has been a severe limi-\ntation of the previous related works in this space. To evalu-\nate the efﬁcacy of our approach, we propose a linguistically-\nmotivated framework to quantify stylistic alignment of the\ngenerated text to the target author at lexical, syntactic and sur-\nface levels. The evaluation framework is both interpretable as\nit leads to several insights about the model, and self-contained\nas it does not rely on external classiﬁers, e.g. sentiment or\nformality classiﬁers. Qualitative and quantitative assessment\nindicates that the proposed approach rewrites the input text\nwith better alignment to the target style while preserving the\noriginal content better than state-of-the-art baselines.\nIntroduction\nThere has been a growing interest in studying style in natu-\nral language and solving tasks related to it (Hu et al. 2017;\nShen et al. 2017; Subramanian et al. 2018; Fu et al. 2018;\nV adapalli et al. 2018; Niu and Bansal 2018). Tasks like genre\nclassiﬁcation (Kessler, Numberg, and Sch ¨utze 1997), au-\nthor proﬁling (Garera and Yarowsky 2009), sentiment anal-\nysis (Wilson, Wiebe, and Hoffmann 2005), social relation-\nship classiﬁcation (Peterson, Hohensee, and Xia 2011) have\nbeen of active interest to the community. Recently, styl-\nized text generation (Hovy 1990; Inkpen and Hirst 2006)\nand style transfer (Li et al. 2018; Prabhumoye et al. 2018;\nFu et al. 2018) have gained traction; both these tasks aim\nto generate realizations of an input text that align to a target\nstyle. A majority of the work here is focused around generat-\ning text with different levels of sentiment (Shen et al. 2017;\nFicler and Goldberg 2017) and formality (Jain et al. 2019)\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n2ULJLQDO\u00037H[W\n7KH\u0003IURQW\u0003GHVN\u0003VWDĳ\u0003\nZDV\n\u0003SURIHVVLRQDO\u0003DQG\u0003YHU\\\u0003\nKHOSIXO\u0011\u0003\n7KH\u0003IURQW\u0003GHVN\u0003VWDĳ\u0003ZDV\u0003\n\u0003D\n\u0003\nSURIHVVLRQDO\u0003DQG\u0003YHU\\\u0003KHOSIXO\u0003\nRQH\u0011\u0003\n\u0003\u0003\u00037KH\u0003\nKRUVH\u0010GRFWRU\u0003FDPH\u000f\u0003\nD\u0003SOHDVDQW\u0003PDQ\u0003DQG\u0003IXOO\u0003RI\u0003\nKRSH\u0003DQG\u0003SURIHVVLRQDO\u0003\nLQWHUHVW\u0003LQ\u0003\nWKH\u0003FDVH\u0011\u0003\n7KH\n\u0003PDQ\nV\u0003IDFH\u0003ZDV\u0003D\u0003\nSOHDVDQW\u0003\nPDQ\u0003DQG\u0003IXOO\u0003RI\u0003\nKRSH\u0003DQG\u0003SURIHVVLRQDO\u0003\nLQWHUHVW\u0003\nLQ\u0003JHQHUDO\u0011\u0003\n6W\\OL]HG\u00037H[W\n6LU\u0003$UWKXU\u0003&RQDQ\u0003\n'R\\OHŖV\u00036W\\OH\n*HRUJH\u0003$OIUHG\u0003\n+HQW\\ŖV\u00036W\\OH\n,\u0003VDLG\u0003,\u0003ZDV\u0003LQ\u0003D\n\u0003\nGUHDGIXO\u0003KXUU\\\u000f\n\u0003DQG\u0003,\u0003ZLVKHG\n\u0003\nZH\u0003FRXOG\u0003JHW\u0003WKLV\u0003EXVLQHVV\n\u0003\nSHUPDQHQWO\\\u0003PDSSHG\u0003RXW\u0011\n=DQH\u0003*UH\\ŖV\u00036W\\OH\n\u0003\n\u0003,\u0003VDLG\u0003,\u0003ZDV\u0003LQ\u0003D\n\u0003\nGUHDGIXO\u0003PRPHQW\nV\u0003WLPH\u000f\n\u0003\nDQG\u0003,\u0003ZLVKHG\u0003ZH\u0003FRXOG\n\u0003\nJHW\u0003WKLV\u0003EXVLQHVV\u0003\nVHWWOHG\n\u0011\n6W\\OH/0\n\u000b)LQH\u0010WXQHG\u0003/0V\f\nFigure 1: An overview of generating author-stylized text us-\ning StyleLM, our proposed model.\nand also a combination of these attributes (Subramanian et\nal. 2018). The interest along these lines has given rise to an-\nnotated and parallel data that comprise of paired realizations\nthat lie on opposite ends of formality and sentiment spec-\ntrum (Rao and Tetreault 2018; Mathews, Xie, and He 2016).\nThe dimensions of style considered across all these works\nare psycholinguistic aspects of text and the aim is to transfer\nthe text across different levels of the chosen aspect.\nHowever, there has been lack of explorations that aim to\ngenerate text across author styles – wherein the notion of\nstyle is not a speciﬁc psycholinguistic aspect but an amal-\ngam of the author’s linguistic choices expressed in their\nwriting (Jhamtani et al. 2017; Tikhonov and Yamshchikov\n2018). While the work by Jhamtani et al. (2017) tries to gen-\nerate “Shakespearized” text from Modern English and is in\na similar vein, it relies on the availability of parallel data.\nSince the availability of parallel data is not always guaran-\nteed and it is arduous to curate one, such an approach can-\nnot scale for different authors. We therefore propose a novel\nframework for author-stylized rewriting without relying on\nparallel data with source-text to target-text mappings. Figure\n1 shows a few examples where an input text is rewritten in\nthe style of a chosen author by our model.\nOur approach for generating author-stylized text involves\nleveraging the generalization capabilities of state-of-the-art\nlanguage models and adapting them to incorporate the stylis-\ntic characteristics of a target author without the need of par-\nallel data. We ﬁrst pre-train a language model on a combi-\n9008\nnation of author corpus (Lahiri 2014) and Wikipedia data\nusing the masked language modeling objective (Devlin et\nal. 2019). Drawing inspiration from the unsupervised ma-\nchine translation setup of Lample and Conneau (2019), we\ncascade two copies of this pre-trained language model into\nan encoder-decoder framework, where the parameters of the\nencoder and decoder are initialized with the pre-trained lan-\nguage model. This cascaded framework is ﬁne-tuned on a\nspeciﬁc target author’s corpus of text by reconstructing the\noriginal text from its noisy version and optimizing on a de-\nnoising autoencoder loss. The ﬁne-tuned model thus adapts\nitself towards the style of the target author as we show via\nour experimental analysis.\nAuthor-stylized rewriting takes a text, which may or may\nnot have a distinctive style, and rewrites it in a style that can\nbe attributed to a target author. Since the writing style of au-\nthors is determined by several linguistically active elements\nthat are expressed at lexical, syntactic, and semantic levels, it\nis challenging to evaluate the stylistic alignment of rewritten\ntext to target author’s style. To this end, we propose a novel\nand interpretable framework that is linguistically motivated,\nto quantify the extent of stylistic alignment at multiple lev-\nels. As we elaborate upon in the later sections, our evalu-\nation suggests that the proposed approach performs better\nthan three relevant and competitive baselines – showing sig-\nniﬁcant adaption to the writing style of target authors, both\nqualitatively and quantitatively. Notably, our approach per-\nforms on par (and better in certain dimensions) with state-\nof-the-art method for stylistic rewritingusing parallel data,\nwithout leveraging the parallel nature of underlying data.\nThe key contributions of this workare threefold.\n1. We propose and evaluate an approach to generate author-\nstylized text without relying on parallel data by adapting\nstate-of-the-art language models.\n2. We propose an evaluation framework to assess the efﬁ-\ncacy of stylized text generation that accounts for align-\nment of lexical and syntactic aspects of style. Contrary to\nexisting evaluation techniques, our evaluation framework\nis linguistically-aware and easily interpretable.\n3. Our proposed approach shows signiﬁcant improvement\nin author-stylized text generation over baselines, both in\nquantitative and qualitative evaluations.\nRelated Work\nStylized Text Generation: In recent times, several explo-\nrations that aim to generate stylized text deﬁne a psycholin-\nguistic aspect, like, formality or sentiment (Shen et al. 2017;\nFicler and Goldberg 2017; Jain et al. 2019) and trans-\nfer text along this dimension. The approaches themselves\ncan range from completely supervised, which is contingent\non the availability of parallel data (Ficler and Goldberg\n2017), to unsupervised (Shen et al. 2017; Li et al. 2018;\nJain et al. 2019). Some of the inﬂuential unsupervised ap-\nproaches include (a) using readily available classiﬁcation-\nbased discriminators to guide the process of generation (Fu\net al. 2018), (b) using simple linguistic rules to achieve\nalignment with the target style (Li et al. 2018), or(c) us-\ning auxiliary modules (called scorers) that score the gen-\neration process on aspects like ﬂuency, formality and se-\nmantic relatedness while deciding on the learning scheme\nof the encoder-decoder network (Jain et al. 2019). How-\never, in the context of our setting, it is not possible to build\na classiﬁcation-based discriminator or scorers to generate\nauthor-stylized text. Moreover, linguistic-rule based genera-\ntions are intractable given the large number of rules required\nto deﬁne a target author’s style. To this end, we aim to adapt\nstate-of-the-art language models to generate author-stylized\ntext from non-parallel data. The choice of using language\nmodels is motivated by the fact that stylisticrewriting builds\non the task of simple text generation (i.e., writing).\nThere are some works that adapt an input text to the writ-\ning style of a speciﬁc author (Jhamtani et al. 2017; Tikhonov\nand Yamshchikov 2018). While Tikhonov and Yamshchikov\n(2018) generate author-stylized poetry by learning the style\nend-to-end using conditioning and concatenated embed-\ndings of the stylistic variables, theirs is not arewriting task.\nJhamtani et al. (2017) aim to generate “Shakespearized” ver-\nsion of modern English language using parallel data. Our\nproposed approach aims to overcome this shortcoming by\nonly relying on non-parallel data and only requires the cor-\npus of the target author text for stylistic rewriting. As we\nshow later, the proposed framework is comparable (even bet-\nter in some of the dimensions) to Jhamtani et al.’s approach\nacross content preservation and style transmission metrics\nwithout utilizing the parallel corpus.\nLanguage Models: Generative pre-training of sentence\nencoders (Radford et al. 2018; Devlin et al. 2019; Howard\nand Ruder 2018) has led to strong improvements on several\nnatural language tasks. Their approach is based on learning\na Transformer (V aswani et al. 2017) language model on a\nlarge unsupervised text corpus and then ﬁne-tuning on clas-\nsiﬁcation and inference-based natural language understand-\ning (NLU) tasks. Building up on this, Lample and Conneau\n(2019) extend this approach to learn cross-lingual language\nmodels. Taking inspiration from this, we extend the genera-\ntive pre-training for our task of author-stylized rewriting.\nThe recently proposed language model GPT-2 (Radford\net al. 2019) is pre-trained on a large and diverse dataset\n(WebText) and is shown to perform well across several do-\nmains and datasets including natural language generation.\nThe unsupervised pre-training is setup to model the genera-\ntion probability of the next word, given the previous words,\ni.e., P(y\nt | y1:t−1,x) – more generally referred to as the\ncausal language modeling (CLM) objective. Speciﬁc to the\ntask of text generation, it takes an input prompt (x) and aims\nto generate text that adheres to the input context. As sub-\nstantiated in the later sections, GPT-2, when ﬁne-tuned on\nauthor-speciﬁc corpus, shows signiﬁcant stylistic alignment\nwith the writing style of target author. However, given the\ninherent differences involved in the setup of stylisticrewrit-\ning and stylized text generation, it performs poorly on con-\ntent preservation. While in stylistic rewriting, the objective\nis to retain the information in the input text in the styl-\nized generation, stylistic generation by GPT-2 generates the\ncontent that is related to the input prompt and hence ﬁne-\ntuned GPT-2 cannot address the task of stylistic rewriting.\n9009\nIn the cross-lingual language modeling literature, a recent\nexploration by Lample and Conneau (2019) learns cross-\nlingual language models by ﬁrst pre-training on3 different\nlanguage modelling objectives: (i) causal language model\n(CLM), (ii) masked language model (MLM) – similar to\nBERT (Devlin et al. 2019), and (iii) translation language\nmodel (TLM) - which is a supervised setup leveraging par-\nallel corpora. Following the pre-training, Lample and Con-\nneau cascade the encoder and decoder to address the tasks\nof supervised cross-lingual classiﬁcation and machine trans-\nlation by ﬁne-tuning on a combination of denoising auto-\nencoder (DAE) and back-translation losses. Taking inspira-\ntion from this work, we pre-train a language model on a large\ncorpus using MLM objective and then ﬁne-tune it on author-\nspeciﬁc corpus using DAE loss in an encoder-decoder setup.\nUsing DAE loss ensures that we don’t rely on availability of\nparallel corpora, while the pre-trained language model facil-\nitates the task of rewriting by building a ﬁrm substratum.\nEvaluating Stylized Generation: Fu et al. (2018) pro-\npose an evaluation framework to assess the efﬁcacy of style\ntransfer models on two axes: (i) content preservation and\n(ii) transfer strength. While the former caters to the con-\ntent overlap between input and generated text (quantiﬁed\nusing BLEU (Papineni et al. 2002)), the latter takes into ac-\ncount the alignment of generated text with target style. In\ntheir setup, as it is with many others, the notion of target\nstyle is a psycholinguistic aspect (formality or sentiment)\nfor which classiﬁers or scorers are readily available and are\nhence used to quantify the transfer strength (Jain et al. 2019;\nLi et al. 2018; Mir et al. 2019). However, for evaluatingau-\nthor-stylized text generations the evaluation frameworks are\nnot well established. Jhamtani et al. (2017) and Tikhonov\nand Yamshchikov (2018) overcome this by using the content\npreservation metrics as a proxy of transfer strength, leverag-\ning the availability of the ground-truth stylized text. The un-\navailability of a suitable metric for transfer strength is partic-\nularly pronounced in evaluating unsupervised approaches as\nthere is no target data to compare the generations against. To\nthis end, we propose a linguistically-aware and interpretable\nevaluation framework which quantiﬁes alignment of multi-\nple lexical and syntactic aspects of style in the generated text\nwith respect to the target author’s style.\nProposed Approach: StyleLM\nThere are two key aspects to our approach – pre-training a\nTransformer-based language model on a large dataset that\nacts as a substratum and ﬁne-tuning on author-speciﬁc cor-\npus using DAE loss to enable stylized rewriting. The entire\napproach isnot contingent on the availability of parallel data\nand the models are learned in a self-supervised manner.\nFigure 2 illustrates the proposed framework for stylis-\ntic rewriting. We ﬁrst pre-train the Transformer-based lan-\nguage model on a large unsupervised corpus using the\nmasked language modeling (MLM) objective (Devlin et al.\n2019). The choice of using a Transformer-based architec-\nture is based on their recent success in language modeling\n(V aswani et al. 2017; Devlin et al. 2019; Radford et al. 2018;\n2019). The MLM objective encourages the LM to predict the\nmasked word(s) from the input sequence of words leverag-\ning bidirectional context information of the input.\nGiven a source sentencex , x\n\\u is a modiﬁed version of\nx where its token from positionu is masked by replacing it\nwith a mask token[MASK]- thus keeping the length of the\nmasked sentence unchanged. The MLM objective pre-trains\nthe language model by predicting the original tokenx\nu, tak-\ning the masked sequence x\\u as input, while learning the\nparameters θ for the conditional probability of the language\nmodel. We minimize the log-likelihood given by,\nL(θ;X)= 1\n|X| Σx∈X logP(xu | x\\u;θ) 1 (1)\nwhere, X denotes the entire training corpus. For pre-training\nthe language model using the MLM objective, following De-\nvlin et al. (2019), we randomly mask15% of the tokens in\neach input sequence, replace them with the[MASK] token\n80% of the time, by a random token10% of the time, and\nkeep them unchanged10%of the time. A difference between\nour model and the MLM proposed by Devlin et al. (2019) is\nthe use of text streams of sentences (truncated at 256 to-\nkens) in contrast to pairs of sentences. This has been shown\nto give considerable gains for text generation tasks (Lample\nand Conneau 2019). Also, unlike Devlin et al. (2019), we do\nnot use theNext Sentence Prediction(NSP) objective.\nThe language model (LM) above learns to predict the\nmasked words over a large corpus, but does not incorporate\nany style-related ﬁne-tuning that facilitates rewriting the in-\nput text in a given target author’s style. To achieve this, we\ncascade two instances of the pre-trained LM in an encoder-\ndecoder setup where one instance acts as the encoder and the\nother acts as a decoder. In other words, the learnable param-\neters of both encoder and decoder are initialized using the\npre-trained LM. Note that the architecture of Transformer-\nbased language models allows two exact instances of the\npre-trained LM to be cascaded, without explicitly aligning\nthe encoder’s output and the decoder’s input (Bahdanau,\nCho, and Bengio 2014) since the attention-mechanism is in-\nherent in the design of Transformers (V aswani et al. 2017).\nLample and Conneau (2019) successfully used such a cas-\ncading to bootstrap the iterative process of the model initial-\nization for the unsupervised machine translation task. Tak-\ning inspiration from this, we ﬁne-tune the encoder-decoder\non the DAE loss, given by,\nL\nDAE = Ex∼S[−logP(x | C(x))] (2)\nwhere, C(x)is the noisy version of the input sentencex and\nS are the sentences in target author’s corpus. To obtain a\nnoisy versionC(x)of input textx, we drop every word inx\nwith a probabilitypdrop and also blank the input words with\na probabilitypblank2.\nWhen the pre-trained language model is cascaded as the\nencoder and decoder, and further ﬁne-tuned with a noisy ver-\nsion of the text, the encoder generates the masked words\n1The equation given here describes MLM for one token (Song\net al. 2019). In practice, multiple tokens are masked in the orig-\ninal BERT architecture and for our experiments, which is just an\nextension of the above idea for training speed-up.\n2replace the word with[BLANK ].\n9010\nMasked Language Modeling\nEncoder\nTransformer\nLayer =1\n0 1\n+\nTransformer\nLayer = 2\nTransformer\nLayer = 3\nTransformer\nLayer = 12\n...\nn-2Position\n/nobreakspaceEmbeddings ...\nDecoder\nAttention\nDenoising Auto-Encoder Loss\nTransformer Network\n...\nDecoderEncoder\nDecoderEncoder\nII. Author-speciﬁc FinetuningI. Unsupervised Pretraining\nTraining corpus\nEach author corpus as a subset of training data separately\nParallel ﬁne-tuning for 10 authors\nA1 Ai A10\nToken \nEmbeddings ...\nNote: [/s] is the new sentence marker.\n[/s] [MASK] into [MASK]\nAttention\nAttention\nText\nStreams\nC(A10)C(A1)\nNoised\nversion \nof Ai\nTrain [/s] the...\nI. Author-corpus\n(Gutenberg) - a total of\n141/nobreakspaceauthors\n~3.6M passages\nII. Wikipedia\narticles\n~1M passages\n+\n~\n...\nText\nn-1 n\ntown\nA1 Ai A10\nC(Ai)\nFigure 2: ProposedStyleLM model. We ﬁrst pre-train a language model on large English corpus (I. Unsupervised Pretraining)\nand then cascade the pre-trained LMs into an encoder-decoder like framework (as represented by the curved arrows). The\nencoder-decoder is ﬁne-tuned separately on each of the target author’s corpus using DAE loss (II. Author-speciﬁc ﬁne-tuning).\n(since that is the original objective of the pre-trained LM).\nHowever, since the input to the decoder, which is same as\nthe output of the encoder, has no masked words, it tries to\nreconstruct the clean version of the noisy input text. In other\nwords, ﬁne-tuning the encoder-decoder on target author’s\ncorpus using the DAE loss (equation 2) pushes the model’s\ndecoder towards inducing target author’s style while rewrit-\ning the input text from the encoder.\nImplementation Details During pre-training with MLM,\nwe use the Transformer encoder (V aswani et al. 2017)\n(12-layer) with GELU activations (Hendrycks and Gimpel\n2017), 512 hidden units,16 heads, a dropout rate of0.1 and\nlearned positional embeddings. We train our models with the\nAdam optimizer (Kingma and Ba 2014), and a learning rate\nof 10\n−4 . We use streams of256 tokens and a mini-batches\nof size 32. We train our model on the MLM objective un-\ntil the language model’s perplexity shows no improvement\nover the validation dataset. For ﬁne-tuning on a target author,\nwhich involves reconstruction of thewhole input passage\n3\nfrom its noisy version we use the same pre-trained MLM\nTransformer initialization for both the encoder and decoder,\nsimilar to Lample and Conneau (2019), with the same hy-\nperparameters used for pre-training.p\ndrop and pblank are set\nto 0.1 and the model is ﬁne-tuned until convergence.\nTo handle the vocabulary size for such a huge dataset,\nwe use Byte Pair Encoding (BPE) (Sennrich, Haddow, and\nBirch 2015) on the combined training dataset and learn80k\nBPE codes on the dataset. Since we use BPE codes on the\ncombination of the training dataset of the141 authors, we\ncan scale these for any author at will – thus the ability to\nadapt to any author in the Gutenberg corpus or beyond.\n3Unlike the MLM which predicts only a part of the input.\nEvaluation Framework\nDataset We collated a subset of the Gutenberg corpus\n(Lahiri 2014) consisting of 142 authors and 2,857 books\nwritten by them. For evaluating on a completely unseen\nauthor (a zero-shot setting), we set aside the writings by\nMark Twain from the training corpus. The remaining authors\nare used as training corpus during pre-training resulting in\n∼ 3.6M passages. To diversify the pre-training dataset, we\nuse 1million passages from Wikipedia (Radford et al. 2018)\nalong with ∼ 3.6M passages from the Gutenberg corpus –\nleading to a total of∼ 4.6M passages for pre-training the\nLM. Of these, we set aside5000passages for validation and\n5000for test during the pre-training stage.\nTo ﬁne-tune the encoder-decoder framework from the pre-\ntrained LM, we pick a subset of10 authors from the Guten-\nberg corpus and independently treat them as target authors\nto generate author-stylized text. The10 chosen authors are:\nSir Arthur Conan Doyle, Charles Dickens, George Alfred\nHenty, Nathaniel Hawthorne, Robert Louis Stevenson, Rud-\nyard Kipling, Thomas Hardy, William Makepeace Thack-\neray, and Zane Grey. We ﬁne-tune independently for each of\nthe 10 target authors and evaluate the efﬁcacy of our pro-\nposed approach using a novel evaluation framework with\nroots in linguistic literature, described in a later section.\nFor inference during test-time, we use the following three\ncorpora to obtain our source sentences : (a) texts from books\nwritten by Mark Twain, (b) Opinosis Review dataset (Gane-\nsan, Zhai, and Han 2010), (c) a Wikipedia article on Ar-\ntiﬁcial Intelligence (https://en.wikipedia.org/wiki/Artiﬁcial\nintelligence) which does not appear in the original mix of\nthe Wikipedia training corpus. Texts from these sources span\na diverse range of topics and writing styles – while Mark\nTwain’s writings are literary, Opinosis reviews are everyday,\nthe Wikipedia article on AI presents an interesting scenario\nwhere many of the words in the source text are not present\nin target author’s corpus, given the different timelines.\n9011\nWe evaluate our performance against4 baselines - 3 of\nwhich are trained on non-parallel data, while the 4th one\nuses parallel data.\n1. Vanilla GPT-2 based generation: Radford et al. (2019)\nshow that language models present considerable promise as\nunsupervised multi-task learners. We use their vanilla GPT-2\npre-trained Transformer decoder (Radford et al. 2019) as our\nﬁrst baseline.\n4. The GPT-2 is fed a prompt directly during\ninference and the generated outputs are compared against\nother generations.\n2. Author ﬁne-tuned GPT-2: The second baseline is the\nﬁne-tuned GPT-2 model for the cross-entropy loss on each\nof the target author’s corpus separately. We use the stylized\ntext generated by providing a prompt to the ﬁne-tuned model\nfor comparisons.\n3. Denoising-LM : no author-speciﬁc ﬁne-tuning: This\nbaseline is similar to our StyleLM network, but ﬁne-tuned\non the entire corpora using the DAE loss (as opposed to just\nthe author-speciﬁc corpus). The purpose of this baseline is to\nevaluate the content preservation capabilities of our setup.\n4. Supervised Stylized Rewriting: Jhamtani et al. (2017)\npropose an LSTM-based encoder-decoder architecture for\ngenerating a “Shakespearized” text originally written in\nmodern English, by leveraging parallel data. We compare\nthis baseline only for generating Shakespearized text (using\ntheir data). We train the other three baselines and StyleLM\nby treating Shakespeare’s corpus as the target author’s cor-\npus (without using the parallel nature of the data).\nProposed Evaluation Methodology\nFollowing existing literature on style transfer and stylized\ntext generation, we evaluate our proposed frameworks along\ntwo axes:content preservationand stylistic alignment.\nContent preservation aims to measure the degree to\nwhich the generated stylized outputs have the same meaning\nas the corresponding input sentences. Following existing lit-\nerature, we use the BLEU metric\n5 (Papineni et al. 2002) and\nthe ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-3 and\nROUGE-L (Lin 2004)).\nThe core contribution of our evaluation framework is in\nthe linguistic-motivation used to quantify thestylistic align-\nment of a generated piece of text with the target style\nwe wish to achieve. While there have been several studies\naround formality and sentiment transfer on text, the same\nevaluation criteria does not apply to our setting because of\ntwo reasons: (a) the classiﬁer-based evaluation, which is\nfacilitated by readily available classiﬁers for aspects like\nsentiment and formality, cannot be used to evaluate stylis-\ntic alignment with respect to an author’s style, and(b) au-\nthor style is an amalgam of several linguistic aspects which\nare much more granular than the psycholinguistic concepts.\nTo this end, taking motivation from V erma and Srinivasan\n(2019), we formulate a multi-level evaluation scheme that\nidentiﬁes and quantiﬁes stylistic expression atsurface, lex-\nical and syntactic level. Once we quantify the stylistic ex-\n4In our experimental setup, we utilise the pre-trained 124M\nparameter model for generation - https://github.com/openai/gpt-2\n5BLEU score is measured withmulti-bleu-detok.perl\npression, we use standard distance metrics to measure the\nstylistic alignment with target.\nLinguists have identiﬁed style, especially in English lan-\nguage, to be expressed at three levels – surface, lexical and\nsyntactic (Strunk 2007; DiMarco and Hirst 1988; Crystal\nand Davy 2016). We ﬁrst discuss the expression of stylis-\ntic elements as well their quantiﬁcation. After quantifying\nthe stylistic expressions at these levels, we discuss their in-\ncorporation into out evaluation framework.\nLexical elements of style are expressed at theword-level.\nFor instance, an authors choice words may be more subjec-\ntive than objective (home vs. residence), or more formal than\ninformal (palatable vs. tasty). For instance, we found that\nRudyard Kipling, known for his classics of children’s lit-\nerature, had a higher tendency to use more concrete words\n(like, gongs, rockets, torch, etc.) unlike Abraham Lincoln,\nwho being a political writer, used more abstract words (like\nfreedom, patriotism, etc.). Inspired from Brooke and Hirst\n(2013), we consider four different spectrums to take lexical-\nstyle into account: (i) subjective-objective, (ii) concrete-\nabstract, (iii) literary-colloquial, and(iv) formal-informal.\nFor quantifying these lexical elements, we use a list of\nseed words for each of the eight categories above, viz.\nsubjective, objective, concrete, abstract, literary, colloquial,\nformal and informal (Brooke and Hirst 2013). Following\nBrooke and Hirst (2013), we compute normalized point-\nwise mutual information index (PMI) to obtain a raw style\nscore for each dimension, by leveraging co-occurrences of\nwords in the large corpus. The raw scores are normalized\nto obtain style vectors for every word, followed by a trans-\nformation of style vectors into k-Nearest Neighbor (kNN)\ngraphs, where label propagation is applied. Since the eight\noriginal dimensions lie on the two extremes of four differ-\nent spectrums, i.e., subjective-objective, concrete-abstract,\nliterary-colloquial, and formal-informal, we compute4aver-\nages across the entire author-speciﬁc corpus. The averages,\nin the range[0,1], denote the tendency of author using sub-\njective, concrete, literary, or formal words, in contrast to us-\ning objective, abstract, colloquial, or informal words, as ev-\nidenced in their historical works\n6.\nSyntactic elements relate to the syntax of the sentence\n– while some authors construct complex sentences, others\nconstruct simple sentences. For instance, as per the writings\nof Abraham Lincoln available in the Gutenberg corpus, a\nmajority of his sentences can be categorized as compound-\ncomplex, while those of Rudyard Kipling’s are mostly sim-\nple sentences (which are better suited to children). Taking in-\nspiration from Feng, Banerjee, and Choi (2012), we catego-\nrize syntactic style into5different categories – (a) simple (b)\ncompound (c) complex (d) complex-compound sentences,\n(e) others. For quantifying these stylistic elements, we com-\npute the fraction of sentences that are categorized into the\n5 categories by the algorithm proposed by Feng, Banerjee,\nand Choi (2012). Since any given sentence will deﬁnitely lie\n6The ﬁnal output is a 4 dimensional vector with each of the\nelements, let’s saylsub ∈ [0, 1].. The value oflsub will denote the\ntendency of the author to choose subjective words instead of their\nobjective counterparts, which can be given by1 −lsub\n9012\nSource Original Text NH’s Style CD’s Style GAH’s Style\nOpinosis\nThe staff was so polite and\ncatered to our every need.\nThe staff was so polite\nand kind to our every\nneed.\nThe staff was so polite\nand obliged to our every\nneed.\nThe staff was so po-\nlite and ready to\naccept our every\nneed.\nFront desk staff were not\nsuper easy to work with\nbut...\nWestern desk, the staff\nwere not abilities easyto\nwork with,but...\nfront desk and staff were\nnot extra easy to work\nwith, but...\nThe won desk staff were\nnot force easy to\nwork with, but...\nMark Twain\nI asked him if he learned\nto talk out of a book, and\nif I could borrow it any-\nwhere?\nI asked him whether he\nhad learned to talk of a\ndream, and if I could bor-\nrow it.\nI asked him if he had\nlearned to talk out of a\nbook; and if I could bor-\nrow it.\nI asked him if he learned\nto talk out of a man’s\nmind and if I could bor-\nrow it\nMeanwhile, if we under-\nstand each other now, I\nwill go to work again.\nAnd if we under-\nstand each other’s ,\nI go to work.\nAnd if we understand\neach other ,I will go to\nwork.\nThen if we under-\nstand each other’s\nwords I will go to\nwork.\nAI Wiki\nIf the AI is programmed\nfor “reinforcement learn-\ning”, goals can be implic-\nitly induced byrewarding\nsome types of behavior or\npunishing others.\nIf the human mind is\nbosoms for Heaven’s\nsake , he can be implic-\nitly induced by rewarded\nsome types of behavior\nor punishment.\nIf the brain is learn for\nmen’s object can be im-\nplicitly induced by grati-\nﬁcation some kind of be-\nhaviour or punishment’s\npunish’s\nIf the round is\nturn for one’s\npoint he can be\nimplicitly induced by\ndone some type\nof conduct or\npunishing.\nTable 1: Samples of stylized text generated byStyleLM. The target authors are Nathaniel Hawthorne (NH), Charles Dickens\n(CD) and George Alfred Henty (GAH). The source text has been taken from Opinosis, Mark Twain and AI Wiki, as indicated.\nin only one of the5categories, the5dimensional vector av-\neraged across the sentences in a corpus can be thought of as\nprobability distribution over the5categories.\nSurface elements relate to statistical observations con-\ncerning aspects like the average number of(i) commas, (ii)\nsemicolons, (iii) colons per sentence, (iv) sentences in a\nparagraph, and(v) number of words in a sentence. We quan-\ntify the surface-level elements into a5dimensional vector.\nAlthough the above enumerations of stylistic elements\nwithin a level, whether lexical, syntactic or surface, are not\nexhaustive, they are indicative of the stylistic expression at\ndifferent levels. Computing the above statistics on an author-\nspeciﬁc corpus gives an interpretable notion of the con-\ncerned author’s writing style. Such a notion of style spans\nacross multiple linguistic levels and has a considerable gran-\nularity. To this end, to quantify the stylistic alignment be-\ntween generated text and the target text, we ﬁrst compute\nthese statistics for both the generated corpus and the target\nauthor’s corpus. Then, we use standard distance metrics to\nobtain the extent of stylistic alignment at different linguistic\nlevels. For lexical and surface-level alignment, we use mean\nsquared error (MSE). Since syntactic style vector is a proba-\nbility distribution over different syntactic categories, we use\nJensen-Shannon divergence (otherwise known as symmetric\nKL divergence) to measure the alignment.\nResults and Analysis\nQualitative Evaluation Table 1 presents samples of\nauthor-stylized text generated using StyleLM for some of the\nauthors. Key highlights include the switch between ‘kind’,\n‘obliged’ and ‘ready to accept’ for the source word ‘catered’.\nThe modiﬁcation of the word ‘super’ – which is used in a\ncolloquial sense, to ‘extra’ without sacriﬁcing the seman-\ntic meaning, demonstrates author-speciﬁc adaptation across\ndifferent time frames. Similar observation can be made by\nnoting the adaptation of ‘AI is programmed’ to ‘brain is to\nlearn’ and ‘rewarding‘ to ‘gratiﬁcation‘ on ﬁne-tuning for\nCharles Dickens’ writing style. Qualitative assessment of the\ngenerated samples depict the efﬁcacy of our approach by il-\nlustrating alignment with the target author’s style as well as\nsigniﬁcant content preservation.\nQuantitative Evaluation Our evaluation framework as-\nsesses the capability of our proposedStyleLM model across\nboth content preservation and stylistic alignment metrics.\nThe results for stylized rewriting of the test corpus to the\nvarious author’s style (10 in total) are presented in in Table\n2. All the ﬁne-tunedStyleLM models are tested on a test set\nthat spans different domains –(a) Opinosis (Ganesan, Zhai,\nand Han 2010) which contains sentences extracted from user\nreviews on a variety of topics from Tripadvisor (hotels), Ed-\nmunds.com (cars) and Amazon.com (various electronics),\n(b) text from Mark Twain’s books, and(c) a Wikipedia page\non Artiﬁcial Intelligence\n7. To reiterate, the objective is to\nrewrite the above test corpora into a style that reﬂects the\nstyle of target author we ﬁne-tuned for. The averaged values\nfor all 10 authors, as well as the standard deviation, across\nboth content preservation as well as stylistic alignment met-\nrics, are given in Table 2.\nIt can be inferred from Table 2 that in terms of stylistic\nalignment, GPT-2 (FT), i.e., author ﬁne-tuned GPT-2, per-\nforms comparable to LM + DAE, i.e., denoising LM with\n7We did not include any of these in the pre-training nor in the\nﬁne-tuning stage. As such, our model has never seen this data.\n9013\nData Source Model Content Preservation(↑) Stylistic Alignment(↓)\nBLEU ROUGE-1 ROUGE-2 ROUGE-3 ROUGE-L Lexical (MSE) Syntactic (JSD) Surface (MSE)\nOpinosis\nGPT-2 18.3±2.3 0.51±0.06 0.29±0.09 0.20±0.06 0.36±0.08 0.48±0.06 0.27±0.09 0.45±0.01\nGPT-2 (FT) 24.3±1.6 0.58±0.07 0.36±0.08 0.27±0.11 0.42±0.09 0.32±0.08 0.23±0.02 0.40±0.03\nL M+D A E 41.1±1.3 0.77±0.11 0.49±0.05 0.39±0.07 0.61±0.08 0.33±0.05 0.23±0.01 0.38±0.02\nStyleLM 43.4±1.7 0.73±0.13 0.53±0.06 0.41±0.08 0.68±0.07 0.29±0.04 0.19±0.01 0.31±0.04\nMark Twain\nGPT-2 16.7±2.4 0.43±0.03 0.26±0.07 0.16±0.04 0.29±0.09 0.41±0.08 0.29±0.03 0.42±0.05\nGPT-2 (FT) 22.9±1.6 0.49±0.06 0.38±0.08 0.21±0.06 0.37±0.08 0.35±0.07 0.25±0.02 0.39±0.06\nL M+D A E 31.7±1.5 0.68±0.14 0.44±0.07 0.27±0.07 0.45±0.10 0.37±0.03 0.24±0.01 0.37±0.03\nStyleLM 34.4±1.8 0.61±0.16 0.48±0.06 0.31±0.06 0.53±0.08 0.32±0.03 0.21±0.02 0.33±0.03\nAI Wiki\nGPT-2 12.6±2.1 0.37±0.04 0.19±0.09 0.09±0.05 0.25±0.08 0.49±0.07 0.31±0.02 0.46±0.05\nGPT-2 (FT) 15.4±1.5 0.43±0.09 0.23±0.06 0.13±0.04 0.29±0.07 0.40±0.03 0.28±0.03 0.42±0.05\nL M+D A E 23.7±1.6 0.59±0.12 0.31±0.08 0.18±0.06 0.37±0.09 0.41±0.04 0.26±0.02 0.41±0.03\nStyleLM 26.7±1.9 0.54±0.13 0.34±0.08 0.23±0.08 0.46±0.09 0.34±0.02 0.22±0.01 0.36±0.04\nTable 2: Evaluating content preservation and stylistic alignment. We evaluate the performance ofStyleLM against three baselines\nand on three test sets across multiple content preservation and stylistic alignment metrics. The reported numbers are mean and\nstandard deviations (μ±σ) across all the10 target authors. FT denotes author-speciﬁc ﬁne-tuning;↑ / ↓ indicates that higher /\nlower is better, respectively.\nModel Content Preservation(↑) Stylistic Alignment(↓)\nBLEU ROUGE-1 ROUGE-2 ROUGE-3 ROUGE-L Lexical (MSE) Syntactic (JSD) Surface (MSE)\nGPT-2 18.1 0.43 0.18 0.11 0.22 0.41 0.30 0.43\nGPT-2 (FT) 21.3 0.48 0.24 0.16 0.28 0.36 0.26 0.39\nL M+D A E 30.2 0.55 0.30 0.19 0.38 0.32 0.24 0.36\nJhamtani et al. (2017) 31.3 0.57 0.33 0.23 0.43 0.29 0.17 0.33\nStyleLM 33.8 0.53 0.31 0.25 0.44 0.28 0.21 0.34\nTable 3: Comparison against supervised baseline. Similar to Table 2, we evaluate the performance of all the models against the\napproach of (Jhamtani et al. 2017) which relies on parallel data. For author-speciﬁc ﬁne-tuning ofStyleLM and GPT-2 (FT), we\nuse Shakespeare’s corpus but without exploiting its parallel nature with modern English corpus.\nno author-speciﬁc ﬁne-tuning, across all the three datasets\nand on each of the three stylistic levels. However, the con-\ntent preservation for LM + DAE is better than that of GPT-2\n(FT). The vanilla GPT-2, however, shows the least impres-\nsive in terms of both content preservation as well stylis-\ntic alignment. Speciﬁcally, the poor performance on con-\ntent preservation can be attributed to the fact that GPT-2\nand GPT-2 (FT) are both trained for generating continua-\ntions of input prompts and not for the task of stylisticrewrit-\ning. It is nonetheless encouraging to see that ﬁne-tuning the\nGPT-2 language model on author-speciﬁc corpus, i.e., GPT-\n2 (FT), increases the extent of stylistic alignment with tar-\nget author’s style, establishing GPT-2 (FT) as a competitive\nbaseline to compare stylistic alignment against.\nWhile LM + DAE, i.e., denoising LM without author-\nspeciﬁc ﬁne-tuning, shows good performance in terms of\ncontent preservation and stylistic alignment, our proposed\napproach, StyleLM, shows considerable gains across all the\nmetrics, against the LM + DAE. This observation conﬁrms\nour hypothesis that the author-speciﬁc ﬁne-tuning using\nDAE loss teaches the model to better learn the stylistic char-\nacteristics of the target author. Consistency of results across\nthe diverse test sets shows a broader coverage in terms of\napplicability of the presented results.\nInterestingly, we notice that ROUGE-1 scores for the\nbaseline LM + DAE (without author ﬁne-tuning) are slightly\nhigher than those for StyleLM. A closer inspection of the\ngenerated samples from the two models reveals that this is\nbecause the stylized generations of the former are not as\nstructurally coherent as those of the latter; i.e., while the pre-\ndicted words are more accurate, they are not predicted in the\ncorrect order. This is further substantiated by the higher val-\nues for ROUGE-2, ROUGE-3 and ROUGE-L scores.\nComparison with Supervised Approach While StyleLM\nperforms better than the other unsupervised stylized gener-\nation models as shown in Table 2, it is critical to determine\nits performance w.r.t. the supervised approach proposed\nby Jhamtani et al. (2017). We compare their LSTM-based\nencoder-decoder approach with GPT-2, GPT-2 (FT), LM +\nDAE and StyleLM after ﬁne-tuning them on Shakespeare’s\ncorpus. As we show in Table 3, StyleLM performs better\nthan the supervised approach in terms of BLEU, ROUGE-\n3, ROUGE-L, and lexical stylistic alignment. The perfor-\nmance, as quantiﬁed by rest of the metrics, is comparable\nto that of (Jhamtani et al. 2017). Given thatStyleLM was\ntrained without leveraging the parallel nature of the data, the\nresults are promising and demonstrate the abilities of our\nproposed model in generating author-stylized text while pre-\nserving the original content.\nConclusion & Future Work\nIn this work, we address the task of author-stylized rewriting\nby proposing a novel approach that leverages the generaliza-\ntion capabilities of language models. Building on the top of\nlanguage models, we ﬁne-tune on target author’s corpus us-\ning denoising autoencoder loss to allow for stylistic adapta-\ntion in the process of reconstruction, without relying on par-\nallel data. We also propose a new interpretable framework\nto evaluate stylistic alignment at multiple linguistic levels.\nWe show that our proposed approach is able to capture the\nstylistic characteristics of target authors while rewriting the\n9014\ninput text and performs not only better than other relevant\nand competitive baselines, but is also competent to an en-\ntirely supervised approach that relies on parallel data.\nThe linguistic understanding of style, on which the pro-\nposed evaluation framework is based, can be used to guide\nthe process of generating stylized text. The process of gen-\neration can be tuned to comply with attributes of style at dif-\nferent levels by penalizing or rewarding the (mis)alignment\nwith these elemental attributes of style. Our plan is to ex-\nplore this in further details, as part of future work.\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural\nmachine translation by jointly learning to align and translate.\narXiv:1409.0473.\nBrooke, J., and Hirst, G. 2013. A multi-dimensional bayesian\napproach to lexical style. InNAACL-HLT.\nCrystal, D., and Davy, D. 2016.Investigating english style. Rout-\nledg.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. Bert:\nPre-training of deep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT.\nDiMarco, C., and Hirst, G. 1988. Stylistic grammars in language\ntranslation. In Proceedings of CoLing. ACL.\nFeng, S.; Banerjee, R.; and Choi, Y . 2012. Characterizing stylistic\nelements in syntactic structure. InEMNLP-CoNLL.\nFicler, J., and Goldberg, Y . 2017. Controlling linguistic style as-\npects in neural language generation.arXiv:1707.02633.\nFu, Z.; Tan, X.; Peng, N.; Zhao, D.; and Yan, R. 2018. Style transfer\nin text: Exploration and evaluation. InAAAI.\nGanesan, K.; Zhai, C.; and Han, J. 2010. Opinosis: a graph-based\napproach to abstractive summarization of highly redundant opin-\nions. In Proceedings of CoLing. ACL.\nGarera, N., and Yarowsky, D. 2009. Modeling latent biographic\nattributes in conversational genres. InACL-IJCNLP.\nHendrycks, D., and Gimpel, K. 2017. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear units.ArXiv\nabs/1606.08415.\nHovy, E. H. 1990. Pragmatics and natural language generation.\nArtiﬁcial Intelligence.\nHoward, J., and Ruder, S. 2018. Universal language model ﬁne-\ntuning for text classiﬁcation. InProceedings of ACL.\nHu, Z.; Yang, Z.; Liang, X.; Salakhutdinov, R.; and Xing, E. P .\n2017. Toward controlled generation of text. InICML.\nInkpen, D., and Hirst, G. 2006. Building and using a lexical knowl-\nedge base of near-synonym differences.Computational linguistics.\nJain, P .; Mishra, A.; Azad, A.; and Sankaranarayanan, K. 2019.\nUnsupervised controllable text formalization. InAAAI.\nJhamtani, H.; Gangal, V .; Hovy, E.; and Nyberg, E. 2017.\nShakespearizing modern language using copy-enriched sequence-\nto-sequence models. EMNLP.\nKessler, B.; Numberg, G.; and Sch¨utze, H. 1997. Automatic detec-\ntion of text genre. InProceedings of EACL. ACL.\nKingma, D. P ., and Ba, J. 2014. Adam: A method for stochastic\noptimization. CoRR abs/1412.6980.\nLahiri, S. 2014. Complexity of Word Collocation Networks: A\nPreliminary Structural Analysis. InSRW EACL.\nLample, G., and Conneau, A. 2019. Cross-lingual language model\npretraining. arXiv:1901.07291.\nLi, J.; Jia, R.; He, H.; and Liang, P . 2018. Delete, retrieve,\ngenerate: A simple approach to sentiment and style transfer.\narXiv:1804.06437.\nLin, C.-Y . 2004. ROUGE: A package for automatic evaluation of\nsummaries. In Text Summarization Branches Out.\nMathews, A. P .; Xie, L.; and He, X. 2016. Senticap: Generating\nimage descriptions with sentiments. InAAAI.\nMir, R.; Felbo, B.; Obradovich, N.; and Rahwan, I. 2019. Evaluat-\ning style transfer for text. In\nNAACL-HLT.\nNiu, T., and Bansal, M. 2018. Polite dialogue generation without\nparallel data. Transactions of ACL6.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. InACL.\nPeterson, K.; Hohensee, M.; and Xia, F. 2011. Email formality in\nthe workplace: A case study on the enron corpus. InWorkshop on\nLanguages in Social Media.\nPrabhumoye, S.; Tsvetkov, Y .; Salakhutdinov, R.; and Black, A. W.\n2018. Style transfer through back-translation.arXiv:1804.09000.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018.\nImproving language understanding by generative pre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised multitask\nlearners.\nRao, S., and Tetreault, J. 2018. Dear sir or madam, may i introduce\nthe gyafc dataset: Corpus, benchmarks and metrics for formality\nstyle transfer. InNAACL-HLT.\nSennrich, R.; Haddow, B.; and Birch, A. 2015. Neural ma-\nchine translation of rare words with subword units. ArXiv\nabs/1508.07909.\nShen, T.; Lei, T.; Barzilay, R.; and Jaakkola, T. 2017. Style transfer\nfrom non-parallel text by cross-alignment. InNeurIPS.\nSong, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y . 2019. Mass:\nMasked sequence to sequence pre-training for language generation.\narXiv:1905.02450.\nStrunk, W. 2007.The elements of style. Penguin.\nSubramanian, S.; Lample, G.; Smith, E.; Denoyer, L.; Ranzato, M.;\nand Boureau, Y . 2018. Multiple-attribute text style transfer.\nTikhonov, A., and Yamshchikov, I. P . 2018. Guess who? multi-\nlingual approach for the automated generation of author-stylized\npoetry. In IEEE SLT.\nV adapalli, R.; Syed, B.; Prabhu, N.; Srinivasan, B.; and V arma, V .\n2018. Sci-blogger: A step towards automated science journalism.\nIn CIKM.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. InNeurIPS.\nV erma, G., and Srinivasan, B. V . 2019. A lexical, syntactic,\nand semantic perspective for understanding style in text. ArXiv\nabs/1909.08349.\nWilson, T.; Wiebe, J.; and Hoffmann, P . 2005. Recognizing con-\ntextual polarity in phrase-level sentiment analysis. InProceedings\nof EMNLP. ACL.\n9015",
  "topic": "Stylized fact",
  "concepts": [
    {
      "name": "Stylized fact",
      "score": 0.8969626426696777
    },
    {
      "name": "Computer science",
      "score": 0.817422091960907
    },
    {
      "name": "Formality",
      "score": 0.7114798426628113
    },
    {
      "name": "Language model",
      "score": 0.6974388957023621
    },
    {
      "name": "Transformer",
      "score": 0.6241644620895386
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5656225681304932
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5530157089233398
    },
    {
      "name": "Natural language processing",
      "score": 0.5314503908157349
    },
    {
      "name": "Generalization",
      "score": 0.4858379065990448
    },
    {
      "name": "Encoder",
      "score": 0.47437912225723267
    },
    {
      "name": "Autoencoder",
      "score": 0.4408141076564789
    },
    {
      "name": "Rewriting",
      "score": 0.4318251609802246
    },
    {
      "name": "Deep learning",
      "score": 0.21465638279914856
    },
    {
      "name": "Programming language",
      "score": 0.16160479187965393
    },
    {
      "name": "Linguistics",
      "score": 0.15501898527145386
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 4
}