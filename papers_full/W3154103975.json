{
    "title": "Multilingual Language Models Predict Human Reading Behavior",
    "url": "https://openalex.org/W3154103975",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5025749600",
            "name": "Nora Hollenstein",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5049501422",
            "name": "Federico Pirovano",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100383731",
            "name": "Ce Zhang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5025151670",
            "name": "Lena A. Jäger",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5087195265",
            "name": "Lisa Beinborn",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2170167574",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3092042135",
        "https://openalex.org/W2973047874",
        "https://openalex.org/W2991275804",
        "https://openalex.org/W3102483398",
        "https://openalex.org/W2947012833",
        "https://openalex.org/W3022176767",
        "https://openalex.org/W3027353876",
        "https://openalex.org/W2995647371",
        "https://openalex.org/W3092292656",
        "https://openalex.org/W3084060007",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W2512498397",
        "https://openalex.org/W1535546891",
        "https://openalex.org/W3033254023",
        "https://openalex.org/W2053130360",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W2948384082",
        "https://openalex.org/W2013112874",
        "https://openalex.org/W2767771229",
        "https://openalex.org/W2914598080",
        "https://openalex.org/W2394756230",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2341311217",
        "https://openalex.org/W2160580906",
        "https://openalex.org/W1967390364",
        "https://openalex.org/W2951025380",
        "https://openalex.org/W3100748148",
        "https://openalex.org/W2931720176",
        "https://openalex.org/W2760692555",
        "https://openalex.org/W2172573173",
        "https://openalex.org/W2901667036",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2946676565",
        "https://openalex.org/W3099634120",
        "https://openalex.org/W3018642446",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2518578398",
        "https://openalex.org/W3102610439",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3092478955",
        "https://openalex.org/W2251049744",
        "https://openalex.org/W2995015695",
        "https://openalex.org/W2512721747",
        "https://openalex.org/W3103099263",
        "https://openalex.org/W1974841121"
    ],
    "abstract": "We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 106–123\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n106\nMultilingual Language Models Predict Human Reading Behavior\nNora Hollenstein1, Federico Pirovano1, Ce Zhang1, Lena Jäger2,3, Lisa Beinborn4\n1 ETH Zurich\n2 University of Zurich\n3 University of Potsdam\n4 Vrije Universiteit Amsterdam\n{noraho,fpirovan,ce.zhang}@inf.ethz.ch,\njaeger@cl.uzh.ch,l.beinborn@vu.nl\nAbstract\nWe analyze if large language models are\nable to predict patterns of human reading\nbehavior. We compare the performance of\nlanguage-speciﬁc and multilingual pretrained\ntransformer models to predict reading time\nmeasures reﬂecting natural human sentence\nprocessing on Dutch, English, German, and\nRussian texts. This results in accurate models\nof human reading behavior, which indicates\nthat transformer models implicitly encode rel-\native importance in language in a way that is\ncomparable to human processing mechanisms.\nWe ﬁnd that BERT and XLM models success-\nfully predict a range of eye tracking features.\nIn a series of experiments, we analyze the\ncross-domain and cross-language abilities of\nthese models and show how they reﬂect human\nsentence processing.\n1 Introduction\nWhen processing language, humans selectively at-\ntend longer to the most relevant elements of a sen-\ntence (Rayner, 1998). This ability to seamlessly\nevaluate relative importance is a key factor in hu-\nman language understanding. It remains an open\nquestion how relative importance is encoded in\ncomputational language models. Recent analy-\nses conclude that the cognitively motivated “at-\ntention” mechanism in neural models is not a good\nindicator for relative importance (Jain and Wal-\nlace, 2019). Alternative methods based on salience\n(Bastings and Filippova, 2020), vector normaliza-\ntion (Kobayashi et al., 2020), or subset erasure\n(De Cao et al., 2020) are being developed to in-\ncrease the post-hoc interpretability of model predic-\ntions but the cognitive plausibility of the underlying\nrepresentations remains unclear.\nIn human language processing, phenomena of\nrelative importance can be approximated indirectly\nby tracking eye movements and measuring ﬁxation\nFigure 1: From the ﬁxation times in milliseconds of a\nsingle subject in the ZuCo 1.0 dataset, the feature vec-\ntor described in Section 3.2 for the wors “Mary” would\nbe [2, 233, 233, 431, 215.5, 1, 1, 1].\nduration (Rayner, 1977). It has been shown that\nﬁxation duration and relative importance of text\nsegments are strongly correlated in natural reading,\nso that direct links can be established on the token\nlevel (Malmaud et al., 2020). In the example in\nFigure 1, the newly introduced entity Mary French\nis ﬁxated twice and for a longer duration because it\nis relatively more important for the reader than the\nentity Laurence, which had been introduced in the\nprevious sentence. Being able to reliably predict\neye movement patterns from the language input\nwould bring us one step closer to understand the\ncognitive plausibility of these models.\nContextualized neural language models are less\ninterpretable than conceptually motivated psy-\ncholinguistic models but they achieve high per-\nformance in many language understanding tasks\nand can be ﬁtted successfully to cognitive features\nsuch as self-paced reading times and N400 strength\n(Merkx and Frank, 2020). Moreover, approaches\nto directly predict cognitive signals (e.g., brain ac-\ntivity) indicate that neural representations implic-\nitly encode similar information as humans (Wehbe\net al., 2014; Abnar et al., 2019; Sood et al., 2020;\nSchrimpf et al., 2020). However, it has not been an-\nalyzed to which extent transformer language mod-\nels are able to directly predict human behavioral\nmetrics such as gaze patterns.\nThe performance of computational models can\n107\nbe improved even further if their inductive bias is\nadjusted using human cognitive signals such as eye\ntracking, fMRI, or EEG data (Hollenstein et al.,\n2019; Toneva and Wehbe, 2019; Takmaz et al.,\n2020). While psycholinguistic work mainly fo-\ncuses on very speciﬁc phenomena of human lan-\nguage processing that are typically tested in ex-\nperimental settings with constructed stimuli (Hale,\n2017), we focus on directly generating token-level\npredictions from natural reading.\nWe ﬁne-tune transformer models on human eye\nmovement data and analyze their ability to pre-\ndict human reading behavior focusing on a range\nof reading features, datasets, and languages. We\ncompare the performance of monolingual and mul-\ntilingual transformer models. Multilingual mod-\nels represent multiple languages in a joint space\nand aim at a more universal language understand-\ning. As eye tracking patterns are consistent across\nlanguages for certain phenomena, we hypothe-\nsize that multilingual models might provide cog-\nnitively more plausible representations and outper-\nform language-speciﬁc models in predicting read-\ning measures. We test this hypothesis on 6 datasets\nof 4 Indo-European languages, namely English,\nGerman, Dutch and Russian.1\nWe ﬁnd that pretrained transformer models are\nsurprisingly accurate at predicting reading time\nmeasures in four Indo-European languages. Multi-\nlingual models show an advantage over language-\nspeciﬁc models, especially when ﬁne-tuned on\nsmaller amounts of data. Compared to previ-\nous psycholinguistic reading models, the accuracy\nachieved by the transformer models is remarkable.\nOur results indicate that transformer models im-\nplicitly encode relative importance in language in a\nway that is comparable to human processing mech-\nanisms. As a consequence, it should be possible to\nadjust the inductive bias of neural models towards\nmore cognitively plausible outputs without having\nto resort to large-scale cognitive datasets.\n2 Related Work\nUsing eye movement data to modify the inductive\nbias of language processing models has resulted in\nimprovements for several NLP tasks (e.g., Barrett\net al. 2016; Hollenstein and Zhang 2019). It has\nalso been used as a supervisory signal in multi-task\nlearning scenarios (Klerke et al., 2016; Gonzalez-\n1Code available on GitHub: https://github.com/\nDS3Lab/multilingual-gaze\nGarduno and Søgaard, 2017) and as a method to\nﬁne-tune the attention mechanism (Barrett et al.,\n2018). We use eye tracking data to evaluate how\nwell transformer language models predict human\nsentence processing. Therefore, in this section,\nwe discuss previous work on probing transformers\nmodels as well as on modelling human sentence\nprocessing.\n2.1 Probing Transformer Language Models\nContextualized neural language models have be-\ncome increasingly popular, but our understanding\nof these black box algorithms is still rather limited\n(Gilpin et al., 2018). Current intrinsic evaluation\nmethods do not capture the cognitive plausibility of\nlanguage models (Manning et al., 2020; Gladkova\nand Drozd, 2016). In previous work of interpreting\nand probing language models, human behavioral\ndata as well as neuroimaging recordings have been\nleveraged to understand the inner workings of the\nneural models. For instance, Ettinger (2020) ex-\nplores the linguistic capacities of BERT with a set\nof psycholinguistic diagnostics. Toneva and We-\nhbe (2019) propose an interpretation approach by\nlearning alignments between the models and brain\nactivity recordings (MEG and fMRI). Hao et al.\n(2020) propose to evaluate language model quality\nbased on the degree to which they exhibit human-\nlike behavior such as predictability measures col-\nlected from human subjects. However, their metric\ndoes not reveal any details about the commonalities\nbetween the model and human sentence processing.\nThe beneﬁts of multilingual models are contro-\nversial. Transformer models trained exclusively\non a speciﬁc language often outperform multilin-\ngual models trained on various languages simul-\ntaneously, even after ﬁne-tuning. This curse of\nmultilinguality (Conneau et al., 2020; Vuli´c et al.,\n2020) has been shown for Spanish (Canete et al.,\n2020), Finnish (Virtanen et al., 2019) and Dutch\n(Vries et al., 2019). In this paper we investigate\nwhether a similar effect can be observed when lever-\naging these models to predict human behavioral\nmeasures, or whether in that case the multilingual\nmodels provide more plausible representations of\nhuman reading due to the common eye tracking\neffects across languages.\n2.2 Modelling Human Sentence Processing\nPrevious work of neural modelling of human sen-\ntence processing has focused on recurrent neu-\nral networks, since their architecture and learn-\n108\nLanguage Corpus Subjs. Sents. Sent. length Tokens Types Word length Flesch\nEnglish\nDundee 10 2,379 21.7 (1–87) 51,497 9,488 4.9 (1–20) 53.3\nGECO 14 5,373 10.5 (1–69) 56,410 5,916 4.6 (1–33) 77.4\nZuCo 30 1,053 19.5 (1–68) 20,545 5,560 5.0 (1–29) 50.6\nDutch GECO 19 5,190 11.64 (1–60) 59,716 5,575 4.5 (1–22) 57.5\nGerman PoTeC 30 97 19.5 (5–51) 1,895 847 6.5 (2–33) 36.4\nRussian RSC 103 144 9.4 (5–13) 1,357 993 5.7 (1–18) 64.7\nTable 1: Descriptive statistics of all eye tracking datasets.2 Sentence length and word length are expressed as the\nmean with the min-max range in parentheses. The last column shows the Flesch Reading Ease score (Flesch, 1948)\nwhich ranges from 0 to 100 (higher score indicates easier to read). Adaptations of the Flesch score were used for\nDutch (nl), German (de) and Russian (ru) (see Appendix B).\ning mechanism appears to be cognitively plausi-\nble (Keller, 2010; Michaelov and Bergen, 2020).\nHowever, recent work suggests that transformers\nperform better at modelling certain aspects of the\nhuman language understanding process (Hawkins\net al., 2020). While Merkx and Frank (2020) and\nWilcox et al. (2020) show that the psychometric pre-\ndictive power of transformers outperforms RNNs\non eye tracking, self-paced reading times and N400\nstrength, they do not directly predict cognitive fea-\ntures. Schrimpf et al. (2020) show that contex-\ntualized monolingual English models accurately\npredict language processing in the brain.\nContext effects are known to inﬂuence ﬁxations\ntimes during reading (Morris, 1994). The notion of\nusing contextual information to process language\nduring reading has been well-established in psy-\ncholinguistics (e.g., Inhoff and Rayner 1986 and\nJian et al. 2013). However, to the best of our knowl-\nedge, we are the ﬁrst to study to which extent the\nrepresentations learned by transformer language\nmodels entail these human reading patterns.\nCompared to neural models of human sentence\nprocessing, we predict not only individual metrics\nbut a range of eye tracking features covering the\nfull reading process from early lexical access to\nlate syntactic processing. By contrast, most models\nof reading focus on predicting skipping probability\n(Reichle et al., 1998; Matthies and Søgaard, 2013;\nHahn and Keller, 2016). Sood et al. (2020) propose\na text saliency model which predicts ﬁxation du-\nrations that are then used to compute the attention\nscores in a transformer network.\n3 Data\nWe predict eye tracking data only from naturalistic\nreading studies in which the participants read full\n2Note that the exact numbers might differ slightly from the\noriginal publications due to different preprocessing methods.\nsentences or longer spans of naturally occurring\ntext in their own speed. The data from these stud-\nies exhibit higher ecological validity than studies\nwhich rely on artiﬁcially constructed sentences and\npaced presentation (Alday, 2019).\n3.1 Corpora\nTo conduct a cross-lingual comparison, we use eye\ntracking data collected from native speakers of four\nlanguages (see Table 1 for details).\nEnglish The largest number of eye tracking data\nsources are available for English. We use eye track-\ning features from three English corpora: (1) The\nDundee corpus (Kennedy et al., 2003) contains 20\nnewspaper articles from The Independent, which\nwere presented to English native readers on a screen\nﬁve lines at a time. (2) The GECO corpus (Cop\net al., 2017) contains eye tracking data from En-\nglish monolinguals reading the entire novel The\nMysterious Affair at Styles by Agatha Christie. The\ntext was presented on the screen in paragraphs. (3)\nThe ZuCo corpus (Hollenstein et al., 2018, 2020)\nincludes eye tracking data of full sentences from\nmovie reviews and Wikipedia articles.3\nDutch The GECO corpus (Cop et al., 2017) ad-\nditionally contains eye tracking data from Dutch\nreaders, which were presented with the same novel\nin their native language.\nGerman The Potsdam Textbook Corpus (PoTeC,\nJäger et al. 2021) contains 12 short passages of 158\nwords on average from college-level biology and\nphysics textbooks, which are read by expert and\nlaymen German native speakers. The full passages\nwere presented on multiple lines on the screen.\n3We use Tasks 1 and 2 from ZuCo 1.0 and Task 1 from\nZuCo 2.0.\n109\nShort Name Language Model Checkpoint Reference\nBERT-NL Dutch WIETSEDV /BERT-BASE -DUTCH -CASED Vries et al. (2019)\nBERT-EN English BERT-BASE -UNCASED Wolf et al. (2019)\nBERT-DE German BERT-BASE -GERMAN -CASED Chan et al. (2019)\nBERT-RU Russian D EEP PAVLOV/RUBERT -BASE -CASED Yu and Arkhipov (2019)\nBERT-MULTI 104 languages BERT-BASE -MULTILINGUAL -CASED Wolf et al. (2019)\nXLM -EN English XLM -MLM -EN-2048 Lample and Conneau (2019)\nXLM -ENDE English + German XLM -MLM -ENDE -1024 Lample and Conneau (2019)\nXLM -17 17 languages XLM -MLM -17-1280 Lample and Conneau (2019)\nXLM -100 100 languages XLM -MLM -100-1280 Lample and Conneau (2019)\nTable 2: Pretrained transformer language models analyzed in this work.\nRussian The Russian Sentence Corpus (RSC,\nLaurinavichyute et al. 2019) contains 144 naturally\noccurring sentences extracted from the Russian Na-\ntional Corpus.4 Full sentences were presented on\nthe screen to monolingual Russian-speaking adults\none at a time.\n3.2 Eye Tracking Features\nA ﬁxation is deﬁned as the period of time where the\ngaze of a reader is maintained on a single location.\nFixations are mapped to words by delimiting the\nboundaries around the region on the screen belong-\ning to each word w. A word can be ﬁxated more\nthan once. For each token w in the input text, we\npredict the following eight eye tracking features\nthat encode the full reading process from early lex-\nical access up to subsequent syntactic integration.\nWord-level characteristics We extract basic fea-\ntures that encode word-level characteristics: (1)\nnumber of ﬁxations ( NFIX), the number of times\na subject ﬁxates w, averaged over all subjects; (2)\nmean ﬁxation duration (MFD), the average ﬁxation\nduration of all ﬁxations made on w, averaged over\nall subjects; (3) ﬁxation proportion ( FPROP), the\nnumber of subjects that ﬁxated w, divided by the\ntotal number of subjects.\nEarly processing We also include features to\ncapture the early lexical and syntactic processing,\nbased on the ﬁrst time a word is ﬁxated: (4) ﬁrst\nﬁxation duration (FFD ), the duration, in millisec-\nonds, of the ﬁrst ﬁxation on w, averaged over all\nsubjects; (5) ﬁrst pass duration (FPD ), the sum of\nall ﬁxations on w from the ﬁrst time a subject ﬁx-\nates w to the ﬁrst time the subject ﬁxates another\ntoken, averaged over all subjects.\nLate processing Finally, we also use measures\nreﬂecting the late syntactic processing and general\n4https://ruscorpora.ru\ndisambiguation, based on words which were ﬁxated\nmore than once: (6) total reading time (TRT), the\nsum of the duration of all ﬁxations made on w, av-\neraged over all subjects; (7) number of re-ﬁxations\n(NREFIX ), the number of times w is ﬁxated after\nthe ﬁrst ﬁxation, i.e., the maximum between 0 and\nthe NFIX-1, averaged over all subjects; (8) re-read\nproportion (REPROP), the number of subjects that\nﬁxated w more than once, divided by the total num-\nber of subjects.\nThe values of these eye tracking features vary\nover different ranges (see Appendix A). FFD , for\nexample, is measured in milliseconds, and aver-\nage values are around 200 ms, whereas REPROP\nis a proportional measure, and therefore assumes\nﬂoating-point values between 0 and 1. We standard-\nize all eye tracking features independently (range:\n0–100), so that the loss can be calculated uniformly\nover all feature dimensions.\nEye movements depend on the stimulus and are\ntherefore language-speciﬁc but there exist universal\ntendencies which remain stable across languages\n(Liversedge et al., 2016). For example, the average\nﬁxation duration in reading ranges from 220 to\n250 ms independent of the language. Furthermore,\nword characteristics such as word length, frequency\nand predictability affect ﬁxation duration similarly\nacross languages but the effect size depends on\nthe language and the script (Laurinavichyute et al.,\n2019; Bai et al., 2008). The word length effect,\ni.e., the fact that longer words are more likely to be\nﬁxated, can be observed across all four languages\nincluded in this work (see Appendix A).\n4 Language Models\nWe compare the ability to predict eye tracking\nfeatures in two models: BERT and XLM. Both\nmodels are trained on the transformer architec-\nture (Vaswani et al., 2017) and yield state-of-the-\n110\nHe is of three quarters Irish andone quarterFrenchdescent.\n0\n20\n40\n60\n80\n100fProp\nZuCo (en)\ncorrect\nXLM-100\nBERT-en\nmean\n.\n0\n20\n40\n60\n80\n100nFix\nRSC (ru)\ncorrect\nXLM-100\nBERT-ru\nmean\nFigure 2: True and predicted feature values for two example sentences. On the left the ﬁxation proportion (FPROP)\nvalues for an English sentence from the ZuCo dataset, and on the right the number of ﬁxations (NFIX) values for a\nRussian sentence from the RSC dataset.\nart results for a wide range of NLP tasks (Liang\net al., 2020). The multilingual BERT model simply\nconcatenates the Wikipedia input from 104 lan-\nguages and is optimized by performing masked\ntoken and next sentence prediction as in the mono-\nlingual model (Devlin et al., 2019) without any\ncross-lingual constraints. In contrast, XLM adds a\ntranslation language modeling objective, by explic-\nitly using parallel sentences in multiple languages\nas input to facilitate cross-lingual transfer (Lam-\nple and Conneau, 2019). Both BERT and XLM\nuse subword tokenization methods to build shared\nvocabulary spaces across languages.\nWe use the pretrained checkpoints from the Hug-\ngingFace repository for monolingual and multilin-\ngual models (details in Table 2).5\n5 Method\nWe ﬁne-tune the models described above on the\nfeatures extracted from the eye tracking datasets.\nThe eye tracking prediction uses a model for to-\nken regression, i.e., the pretrained language models\nwith a linear dense layer on top of it. The ﬁnal\ndense layer is the same for all tokens, and performs\na projection from the dimension of the hidden size\nof the model (e.g., 768 for BERT-EN or 1,280 for\nXLM -100 ) to the dimension of the eye tracking fea-\nture space (8, in our case). The model is trained for\nthe regression task using the mean squared error\n(MSE) loss.\nTraining Details We split the data into 90%\ntraining data, 5% validation and 5% test data. We\ninitially tuned the hyper-parameters manually and\nset the following values for all models: We use an\nAdamW optimizer (Loshchilov and Hutter, 2018)\nwith a learning rate of 0.00005 and a weight decay\nof 0.01. The batch size varies depending on the\n5https://huggingface.co/transformers/\npretrained_models.html\nmodel dimensions (see Appendix C.2). We employ\na linear learning rate decay schedule over the to-\ntal number of training steps. We clip all gradients\nexceeding the maximal value of 1. We train the\nmodels for 100 epochs, with early stopping after 7\nepochs without an improvement on the validation\naccuracy.\nEvaluation Procedure As the features have\nbeen standardized to the range 0–100, the mean\nabsolute error (MAE) can be interpreted as a per-\ncentage error. For readability, we report the pre-\ndiction accuracy as 100−MAE in all experiments.\nThe results are averaged over batches and over 5\nruns with varying random seeds. For a single batch\nof sentences, the overall MAE is calculated by con-\ncatenating the words in each sentence and the fea-\nture dimensions for each word, and padding to the\nmaximum sentence length. The per-feature MAE\nis calculated by concatenating the words in each\nsentence. For example, for a batch of B sentences,\neach composed of L words, and G eye tracking\nfeatures per word, the overall MAE is calculated\nover a vector of B*L*G dimensions. In contrast,\nthe MAE for each individual feature is calculated\nover a vector of B*L dimensions.\n6 Results & Discussion\nTables 3 and 4 show that all models predict the eye\ntracking features with more than 90% accuracy for\nEnglish and Dutch. For English, the BERT models\nyield high performance on all three datasets with\nstandard deviations below 0.15. The results for\nthe XLM models are slightly better on average but\nexhibit much higher standard deviations. Similar\nto the results presented by Lample and Conneau\n(2019), we ﬁnd that more training data from mul-\ntiple languages improves prediction performance.\nFor instance, the XLM -100 model achieves higher\naccuracy than the XLM -17 model in all cases. For\n111\nModel Dundee (en) GECO (en) ZuCo (en) ALL (en)\nBERT-EN 92.63 (0.05) 93.68 (0.14) 93.42 (0.02) 93.71 (0.06)\nBERT-MULTI 92.73 (0.06) 93.73 (0.12) 93.74 (0.05) 93.74 (0.07)\nXLM -EN 90.41 (2.16) 91.15 (1.42) 92.03 (2.11) 90.88 (1.50)\nXLM -ENDE 92.79 (0.15) 93.89 (0.12) 93.76 (0.15) 93.96 (0.08)\nXLM -17 92.11 (1.68) 91.79 (1.75) 92.05 (2.25) 93.80 (0.38)\nXLM -100 92.99 (0.05) 93.04 (1.40) 93.97 (0.09) 93.96 (0.06)\nTable 3: Prediction accuracy over all eye tracking features for the English corpora, including the concatenated\ndataset. Standard deviation is reported in parentheses.\nModel GECO (nl) PoTeC (de) RSC (ru) ALL-LANGS\nBERT-NL 91.81 (0.23) – – –\nBERT-DE – 78.38 (1.69) – –\nBERT-RU – – 78.73 (1.38) –\nBERT-MULTI 91.90 (0.16) 76.86 (2.42) 76.54 (3.59) 94.72 (0.07)\nXLM -ENDE – 80.94 (0.88) – –\nXLM -17 91.04 (0.70) 86.26 (1.31) 90.96 (3.96) 94.46 (0.83)\nXLM -100 92.31 (0.22) 86.57 (0.54) 94.70 (0.60) 94.94 (0.11)\nTable 4: Prediction accuracy over all eye tracking features for the Dutch, German and Russian corpora, and for all\nfour languages combined in a single dataset. Standard deviation is reported in parentheses.\nthe smaller non-English datasets, PoTeC (de) and\nRSC (ru), the multilingual XLM models clearly\noutperform the monolingual models. For the En-\nglish datasets, the differences are minor.\nSize Effects More training data results in higher\nprediction accuracy even when the eye track-\ning data comes from various languages and was\nrecorded in different reading studies by different de-\nvices (ALL-LANGS, ﬁne-tuning on the data of all\nfour languages together). However, merely adding\nmore data from the same language (ALL (en), ﬁne-\ntuning on the English data from Dundee, GECO\nand ZuCo together) does not result in higher per-\nformance.\nTo analyze this further, we perform an ablation\nstudy on varying amounts of training data. The re-\nsults are shown in Figure 3 for Dutch and English.\nThe performance of the XLM models remains sta-\nble even with a very small percentage of eye track-\ning data. The performance of the BERT models,\nhowever, drops drastically when ﬁne-tuning on less\nthan 20% of the data. Similar to Merkx and Frank\n(2020) and Hao et al. (2020) we ﬁnd that the model\narchitecture, along with the composition and size\nof the training corpus have a signiﬁcant impact on\nthe psycholinguistic modeling performance.\nEye Tracking Features The accuracy results are\naveraged over all eye tracking features. For a better\nunderstanding of the prediction output, we plot the\ntrue and the predicted values of two selected fea-\ntures (FPROP and NFIX) for two example sentence\nin Figure 2. In both examples, the model predic-\ntions strongly correlate with the true values. The\ndifference to the mean baseline is more pronounced\nfor the FIXPROPfeature.\nFigure 4 presents the quantitative differences\nacross models in predicting the individual eye track-\ning features.6 Across all datasets, ﬁrst pass dura-\ntion (FPD ) and number of re-ﬁxations ( NREFIX )\nare the most accurately predicted features. Propor-\ntions (FPROP and REPROP) are harder to predict\nbecause these features are even more dependent\non subject-speciﬁc characteristics. Nevertheless,\nwhen comparing the prediction accuracy of each\neye tracking feature to a baseline which always\npredicts the mean values, the predicted features\nFPROP and REPROP achieve the largest improve-\nments relative to the mean baseline. See Figure 5\nfor a comparison between all features for the best\nperforming model XLM -100 on all six datasets.\nPerformance of Pretrained ModelsTo test the\nlanguage models’ abilities on predicting human\nreading behavior only from pretraining on textual\ninput, we take the provided model checkpoints and\nuse them to predict the eye tracking features with-\nout any ﬁne-tuning. The detailed results are pre-\nsented in Appendix D.1. The achieved accuracy ag-\ngregated over all eye tracking features lies between\n75-78% for English. For Dutch, the models achieve\n6Plots for the remaining datasets are in Appendix D.2\n112\n1 2 5 10 20 40 60 80 100\n% of training data\n75\n80\n85\n90\n95Accuracy\nGECO (nl)\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\n1 2 5 10 20 40 60 80 100\n% of training data\n75\n80\n85\n90\n95Accuracy\nALL (en)\nBERT-en\nBERT-multi\nXLM-en\nXLM-17\nXLM-100\nFigure 3: Data ablation study for Dutch and English. The results are aggregated over all eye tracking features.\nIn addition to the mean across ﬁve runs, the shaded areas represent the standard deviation. The dashed line is\nthe result of the pretrained BERT-MULTI model without ﬁne-tuning. Results are aggregated over all eye tracking\nfeatures.\n70 80 90 100\nBERT-en\nBERT-multi\nXLM-en\nXLM-ende\nXLM-17\nXLM-100\nDundee (en)\n40 50 60 70 80 90 100\nPrediction accuracy\nBERT-ru\nBERT-multi\nXLM-17\nXLM-100\nRSC (ru)\nnFix MFD fProp FFD FPD TRT nRefix reProp\nFigure 4: Results of individual eye tracking features for\nall models on the Dundee and RSC corpora.\nnFix\n0\n2\n4\n6\n8\n10\n12\n14\n16Accuracy improvement\nMFD fProp FFD\nDundee GECO-en ZuCo GECO-nl PoTeC RSC\nFPD TRT nRefix reProp\nXLM-100\nFigure 5: Improvement of prediction accuracy for the\nXLM -100 model relative to the mean baseline for each\neye tracking feature.\n84% accuracy but for Russian merely 65%. Across\nthe same languages the results between the differ-\nent language models are only minimal. However,\non the individual eye tracking features, the pre-\ntrained models do not achieve any improvements\nover the mean baseline (see Appendix D.1).\n7 Data Sensitivity\nFor the main experiment, we always tested the mod-\nels on held-out data from the same dataset. In this\nsection, we examine the inﬂuence of dataset prop-\nerties (text domain and language) on the prediction\naccuracy. In a second step, we analyze the inﬂu-\nence of more universal input characteristics (word\nlength, text readability).\n7.1 Cross-Domain Evaluation\nFigure 6 shows the results when evaluating the eye\ntracking predictions on out-of-domain text for the\nEnglish datasets. For instance, we ﬁne-tune the\nmodel on the newspaper articles of the Dundee\ncorpus and test on the literary novel of the GECO\ncorpus. We can see that the overall prediction accu-\nracy across all eye tracking features is constantly\nabove. 90% in all combinations. This shows that\nour eye tracking prediction model is able to general-\nize across domains. We ﬁnd that the cross-domain\ncapabilities of BERT are slightly better than for\nXLM. BERT-EN performs best in the cross-domain\nevaluation, possibly because its training data is\nmore domain-general since it includes text from\nWikipedia and books.\n7.2 Cross-Language Evaluation\nFigure 7 shows the results for cross-language eval-\nuation to probe the language transfer capabilities\nof the multilingual models. We test models ﬁne-\ntuned on language A on the test set of language\nB. It can be seen that BERT-MULTI generalizes bet-\nter across languages than the XLM models. This\nmight be due to the fact that the multilingual BERT\nmodel is trained on one large vocabulary of many\nlanguages but the XLM models are trained with\na cross-lingual objective and language informa-\ntion. Hence, during ﬁne-tuning on eye tracking\n113\nFigure 6: Cross-domain evaluation on pretrained English models. The results are expressed as the difference in\nthe prediction error compared to the in-domain prediction. A smaller error (i.e., a color more similar to the color\nof the diagonal) represents better domain adaptation.\nFigure 7: Cross-language evaluation on multilingual models across English, Dutch, German and Russian data. The\nresults are expressed as the difference in the prediction error compared to the prediction on the same language. A\nsmaller error (i.e., a color more similar to the color of the diagonal) represents better language transfer.\ndata from one language the XLM models lose some\nof their cross-lingual abilities. Our results are in\nline with Pires et al. (2019) and Karthikeyan et al.\n(2020), who showed that BERT learns multilingual\nrepresentations in more than just a shared vocabu-\nlary space but also across scripts. When ﬁne-tuning\nBERT-MULTI on English or Dutch data and test-\ning on Russian, we see surprisingly high accuracy\nacross scripts, even outperforming the in-language\nresults. The XLM models, however, show the ex-\npected behavior where transferring within the same\nscript (Dutch, English, German) works much better\nthan transferring between the Latin and Cyrillic\nscript (Russian).\n7.3 Input Characteristics\nGaze patterns are strongly correlated with word\nlength. Figure 8 shows that the models accurately\nlearn to predict higher ﬁxation proportions for\nlonger words. We observe that the predictions of\nthe XLM -100 model follow the trend in the origi-\nnal data most accurately. Similar patterns emerge\nfor the other languages (see Appendix D.3). No-\ntably, the pretrained models before ﬁne-tuning do\nnot reﬂect the word length effect.\nOn the sentence level, we hypothesize that eye\ntracking features are easier to predict for sentences\nwith a higher readability. Figure 9 shows the accu-\nracy for predicting the number of ﬁxations (NFIX)\nin a sentence relative to the Flesch reading ease\nscore. Interestingly, the pretrained models with-\nout ﬁne-tuning conform to the expected behavior\nand show a consistent increase in accuracy for sen-\ntences with a higher reading ease score. After ﬁne-\ntuning on eye tracking data, this behavior is not as\nvisible anymore since the language models achieve\nconstantly high accuracy independent of the read-\nability of the sentences.\nThese results might be explained by the nature\nof the Flesch readability score, which is based only\n114\n5 10 15 20\nword length\n0\n20\n40\n60\n80\n100(predicted) fixation proportion\nGECO (nl)\nGaze Data\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\nFigure 8: Prediction accuracy of FPROP with respect to\nword length. The gray dashed line is the result of the\npretrained BERT-MULTI model without ﬁne-tuning.\non the structural complexity of the text (see Ap-\npendix B for a description of the Flesch Reading\nEase score). Our results indicate that language\nmodels trained purely on textual input are more\ncalibrated towards such structural characteristics,\ni.e., the number of syllables in a word and the num-\nber of words in a sentences. Hence, the Flesch\nreading ease score might not be a good approxima-\ntion for text readability. In future work, comparing\neye movement patterns and text difﬁculty should\nrely on readability measures that take into account\nlexical, semantic, syntactic, and discourse features.\nThis might reveal deviating patterns between pre-\ntrained and ﬁne-tuned models.\nOur analyses indicate that the models learn to\ntake properties of the input into account when pre-\ndicting eye tracking patterns. These processing\nstrategies are similar to those observed in humans.\nNevertheless, the connection between readability\nand relative importance in text needs to be analysed\nin more detail to establish how well these properties\nare learned by the language models.\n8 Conclusion\nWhile the superior performance of pretrained trans-\nformer language models has been established, we\nhave yet to understand to which extent these mod-\nels are comparable to human language processing\nbehavior. We take a step in this direction by ﬁne-\ntuning language models on eye tracking data to\npredict human reading behavior.\nWe ﬁnd that both monolingual and multilingual\nmodels achieve surprisingly high accuracy in pre-\ndicting a range of eye tracking features across four\nlanguages. Compared to the XLM models, BERT-\n0 20 40 60 80 100\nFlesch reading ease\n86\n88\n90\n92\n94\n96Prediction accuracy\nGECO (nl)\nFine-tuned\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\nPre-trained\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\n0 20 40 60 80 100\nFlesch reading ease\n86\n88\n90\n92\n94\n96Prediction accuracy\nALL (en)\nFine-tuned\nBERT-en\nBERT-multi\nXLM-17\nXLM-100\nPre-trained\nBERT-en\nBERT-multi\nXLM-17\nXLM-100\nFigure 9: Prediction accuracy for NFIX relative to the\nFlesch reading ease score of the sentence. A higher\nFlesch score indicates that a sentence is easier to read.\nThe dashed lines show the results of the pretrained lan-\nguage models without ﬁne-tuning on eye tracking data.\nMULTI is more robust in its ability to generalize\nacross languages, without being explicitly trained\nfor it. In contrast, the XLM models perform better\nwhen ﬁne-tuned on less eye tracking data. Gener-\nally, ﬁxation duration features are predicted more\naccurately than ﬁxation proportion, possibly be-\ncause the latter show higher variance across sub-\njects. We observe that the models learn to reﬂect\ncharacteristics of human reading such as the word\nlength effect and higher accuracy in more easily\nreadable sentences.\nThe ability of transformer models to achieve\nsuch high results in modelling reading behavior\nindicates that we can learn more about the com-\nmonalities between language models and human\nsentence processing. By predicting behavioral met-\nrics such as eye tracking features we can investigate\nthe cognitive plausibility within these models to ad-\njust or intensify the human inductive biases.\n115\nAcknowledgements\nLena Jäger was partially funded by the German\nFederal Ministry of Education and Research under\ngrant 01|S20043.\nReferences\nSamira Abnar, Lisa Beinborn, Rochelle Choenni, and\nWillem Zuidema. 2019. Blackbox meets blackbox:\nRepresentational similarity & stability analysis of\nneural language models and brains. In Proceedings\nof the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n191–203.\nPhillip M Alday. 2019. M/EEG analysis of naturalis-\ntic stories: A review from speech to language pro-\ncessing. Language, Cognition and Neuroscience ,\n34(4):457–473.\nToni Amstad. 1978. Wie verständlich sind unsere\nZeitungen? Unpublished doctoral dissertation, Uni-\nversity of Zürich, Switzerland.\nXuejun Bai, Guoli Yan, Simon P Liversedge, Chuanli\nZang, and Keith Rayner. 2008. Reading spaced and\nunspaced Chinese text: Evidence from eye move-\nments. Journal of Experimental Psychology: Hu-\nman Perception and Performance, 34(5):1277.\nMaria Barrett, Joachim Bingel, Nora Hollenstein,\nMarek Rei, and Anders Søgaard. 2018. Sequence\nclassiﬁcation with human attention. In Proceedings\nof the 22nd Conference on Computational Natural\nLanguage Learning, pages 302–312.\nMaria Barrett, Joachim Bingel, Frank Keller, and An-\nders Søgaard. 2016. Weakly supervised part-of-\nspeech tagging using eye-tracking data. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics, volume 2, pages\n579–584.\nJasmijn Bastings and Katja Filippova. 2020. The ele-\nphant in the interpretability room: Why use atten-\ntion as explanation when we have saliency methods?\nIn Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 149–155.\nJosé Canete, Gabriel Chaperon, Rodrigo Fuentes, and\nJorge Pérez. 2020. Spanish pre-trained BERT model\nand evaluation data. PML4DC at ICLR.\nBranden Chan, Timo Möller, Malte Pietsch, and Tanay\nSoni. 2019. German BERT.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451.\nUschi Cop, Nicolas Dirix, Denis Drieghe, and Wouter\nDuyck. 2017. Presenting GECO: An eyetracking\ncorpus of monolingual and bilingual sentence read-\ning. Behavior Research Methods, 49(2):602–615.\nNicola De Cao, Michael Sejr Schlichtkrull, Wilker\nAziz, and Ivan Titov. 2020. How do decisions\nemerge across layers in neural models? interpreta-\ntion with differentiable masking. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 3243–\n3255.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nWH Douma. 1960. De Leesbaarheid Van Landbouw-\nbladen. Een Onderzoek Naar en Een Toepassing Van\nLeesbaarheidsformules.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nRudolph Flesch. 1948. A new readability yardstick.\nJournal of applied psychology, 32(3):221.\nLeilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Ba-\njwa, Michael Specter, and Lalana Kagal. 2018. Ex-\nplaining explanations: An overview of interpretabil-\nity of machine learning. In 2018 IEEE 5th Interna-\ntional Conference on data science and advanced an-\nalytics (DSAA), pages 80–89. IEEE.\nAnna Gladkova and Aleksandr Drozd. 2016. Intrinsic\nevaluations of word embeddings: What can we do\nbetter? In Proceedings of the 1st Workshop on Eval-\nuating Vector-Space Representations for NLP, pages\n36–42.\nAna Valeria Gonzalez-Garduno and Anders Søgaard.\n2017. Using gaze to predict text readability. In Pro-\nceedings of the 12th Workshop on Innovative Use of\nNLP for Building Educational Applications , pages\n438–443.\nMichael Hahn and Frank Keller. 2016. Modeling hu-\nman reading with neural attention. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing.\nJohn Hale. 2017. Models of human sentence compre-\nhension in computational psycholinguistics. In Ox-\nford Research Encyclopedia of Linguistics.\n116\nYiding Hao, Simon Mendelsohn, Rachel Sterneck,\nRandi Martinez, and Robert Frank. 2020. Probabilis-\ntic predictions of people perusing: Evaluating met-\nrics of language model performance for psycholin-\nguistic modeling. In Proceedings of the Workshop\non Cognitive Modeling and Computational Linguis-\ntics, pages 75–86.\nRobert Hawkins, Takateru Yamakoshi, Thomas L Grif-\nﬁths, and Adele Goldberg. 2020. Investigating rep-\nresentations of verb bias in neural language models.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4653–4663.\nNora Hollenstein, Maria Barrett, Marius Troendle,\nFrancesco Bigiolli, Nicolas Langer, and Ce Zhang.\n2019. Advancing NLP with cognitive language pro-\ncessing signals. arXiv preprint arXiv:1904.02682.\nNora Hollenstein, Jonathan Rotsztejn, Marius Troen-\ndle, Andreas Pedroni, Ce Zhang, and Nicolas Langer.\n2018. ZuCo, a simultaneous EEG and eye-tracking\nresource for natural sentence reading. Scientiﬁc\nData.\nNora Hollenstein, Marius Troendle, Ce Zhang, and\nNicolas Langer. 2020. ZuCo 2.0: A dataset of phys-\niological recordings during natural reading and an-\nnotation. In Proceedings of The 12th Language Re-\nsources and Evaluation Conference, pages 138–146.\nNora Hollenstein and Ce Zhang. 2019. Entity recog-\nnition at ﬁrst sight: Improving NER with eye move-\nment information. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers).\nAlbrecht Werner Inhoff and Keith Rayner. 1986.\nParafoveal word processing during eye ﬁxations in\nreading: Effects of word frequency. Perception &\npsychophysics, 40(6):431–439.\nLena Jäger, Thomas Kern, and Patrick Haller.\n2021. Potsdam Textbook Corpus (PoTeC): Eye\ntracking data from experts and non-experts read-\ning scientiﬁc texts. available on OSF, DOI\n10.17605/OSF.IO/DN5HP.\nSarthak Jain and Byron C Wallace. 2019. Attention is\nnot explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556.\nYu-Cin Jian, Ming-Lei Chen, and Hwa-wei Ko. 2013.\nContext effects in processing of Chinese academic\nwords: An eye-tracking investigation. Reading Re-\nsearch Quarterly, 48(4):403–413.\nKaliyaperumal Karthikeyan, Zihan Wang, Stephen\nMayhew, and Dan Roth. 2020. Cross-lingual ability\nof multilingual BERT: An empirical study. In Inter-\nnational Conference on Learning Representations.\nFrank Keller. 2010. Cognitively plausible models of\nhuman language processing. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics: Short Papers, pages 60–67.\nAlan Kennedy, Robin Hill, and Joël Pynte. 2003. The\nDundee corpus. In Proceedings of the 12th Euro-\npean Conference on Eye Movement.\nSigrid Klerke, Yoav Goldberg, and Anders Søgaard.\n2016. Improving sentence compression by learning\nto predict gaze. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1528–1533.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057–7075.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems.\nAK Laurinavichyute, Irina A Sekerina, SV Alexeeva,\nand KA Bagdasaryan. 2019. Russian Sentence Cor-\npus: Benchmark measures of eye movements in\nreading in Cyrillic. Behavior research methods ,\n51(3):1161–1178.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, et al. 2020. XGLUE:\nA new benchmark dataset for cross-lingual pre-\ntraining, understanding and generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6008–6018.\nSimon P Liversedge, Denis Drieghe, Xin Li, Guoli Yan,\nXuejun Bai, and Jukka Hyönä. 2016. Universality in\neye movements and reading: A trilingual investiga-\ntion. Cognition, 147:1–20.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In Proceedings of the\nInternational Conference on Learning Representa-\ntions.\nJonathan Malmaud, Roger Levy, and Yevgeni Berzak.\n2020. Bridging information-seeking human gaze\nand machine reading comprehension. In Proceed-\nings of the 24th Conference on Computational Natu-\nral Language Learning, pages 142–152.\nChristopher D Manning, Kevin Clark, John Hewitt, Ur-\nvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artiﬁcial neural networks\ntrained by self-supervision. Proceedings of the Na-\ntional Academy of Sciences.\n117\nFranz Matthies and Anders Søgaard. 2013. With blink-\ners on: Robust prediction of eye movements across\nreaders. Proceedings of the 2013 Conference on\nempirical methods in natural language processing\n(EMNLP), pages 803–807.\nDanny Merkx and Stefan L Frank. 2020. Com-\nparing transformers and RNNs on predicting hu-\nman sentence processing data. arXiv preprint\narXiv:2005.09471.\nJames Michaelov and Benjamin Bergen. 2020. How\nwell does surprisal explain N400 amplitude under\ndifferent experimental conditions? In Proceedings\nof the 24th Conference on Computational Natural\nLanguage Learning, pages 652–663.\nRobin K Morris. 1994. Lexical and message-level\nsentence context effects on ﬁxation times in read-\ning. Journal of Experimental Psychology: Learning,\nMemory, and Cognition, 20(1):92.\nI Oborneva. 2006. Automatic assessment of the com-\nplexity of educational texts on the basis of statistical\nparameters.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001.\nKeith Rayner. 1977. Visual attention in reading: Eye\nmovements reﬂect cognitive processes. Memory &\nCognition, 5(4):443–448.\nKeith Rayner. 1998. Eye movements in reading and\ninformation processing: 20 years of research. Psy-\nchological bulletin, 124(3):372.\nErik D Reichle, Alexander Pollatsek, Donald L Fisher,\nand Keith Rayner. 1998. Toward a model of eye\nmovement control in reading. Psychological review,\n105(1):125.\nMartin Schrimpf, Idan A Blank, Greta Tuckute, Ca-\nrina Kauf, Eghbal A Hosseini, Nancy G Kanwisher,\nJoshua B Tenenbaum, and Evelina Fedorenko. 2020.\nArtiﬁcial neural networks accurately predict lan-\nguage processing in the brain. BioRxiv.\nEkta Sood, Simon Tannert, Philipp Mueller, and An-\ndreas Bulling. 2020. Improving natural language\nprocessing tasks with human gaze-guided neural at-\ntention. Advances in Neural Information Processing\nSystems, 33.\nEce Takmaz, Sandro Pezzelle, Lisa Beinborn, and\nRaquel Fernández. 2020. Generating image descrip-\ntions via sequential cross-modal alignment guided\nby human gaze. In Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4664–4677.\nMariya Toneva and Leila Wehbe. 2019. Interpret-\ning and improving natural-language processing (in\nmachines) with natural language-processing (in the\nbrain). In Advances in Neural Information Process-\ning Systems, pages 14928–14938.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Lu-\noma, Juhani Luotolahti, Tapio Salakoski, Filip Gin-\nter, and Sampo Pyysalo. 2019. Multilingual is\nnot enough: BERT for Finnish. arXiv preprint\narXiv:1912.07076.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. BERTje: A Dutch BERT\nModel. arXiv preprint arXiv:1912.09582.\nIvan Vuli´c, Edoardo Maria Ponti, Ira Leviant, Olga Ma-\njewska, Matt Malone, Roi Reichart, Simon Baker,\nUlla Petti, Kelly Wing, Eden Bar, et al. 2020. Multi-\nSimLex: A large-scale evaluation of multilingual\nand cross-lingual lexical semantic similarity. Com-\nputational Linguistics, pages 1–73.\nLeila Wehbe, Ashish Vaswani, Kevin Knight, and Tom\nMitchell. 2014. Aligning context-based statistical\nmodels of language with brain activity during read-\ning. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 233–243.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu,\nPeng Qian, and Roger Levy. 2020. On the predic-\ntive power of neural language models for human\nreal-time comprehension behavior. arXiv preprint\narXiv:2006.01912.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nKuratov Yu and M Arkhipov. 2019. Adaptation of deep\nbidirectional multilingual transformers for Russian\nlanguage. Computational Linguistics and Intellec-\ntual Technologies, (18):333–339.\n118\nA Eye Tracking Data\nTable 6 presents information about the range of the\neye tracking features.\nFigure 10 shows the word length effect found in\neye tracking data recorded during reading. i.e., the\nfact that longer words are more likely to be ﬁxated.\nThis effect is observable across all languages.\n0 5 10 15 20 25 30\nword length\n0.2\n0.4\n0.6\n0.8\n1.0fixation proportion\nDundee (en)\nGECO (en)\nZuCo (en)\nGECO (nl)\nPoTeC (de)\nRSC (ru)\nALL-LANGS\nFigure 10: Word length effect on all datasets in all four\nlanguages.\nFigure 11 shows the mean ﬁxation duration (MFD)\nfor adjectives, nouns, verbs, and adverbs for all six\ndatasets. We use spacy7 to perform part-of-speech\ntagging for our analyses. For Russian we load an\nexternally trained model8, for Dutch, English and\nGerman we use the provided pretrained models.\nFigure 12 shows an additional analysis where we\nexplore which parts-of-speech can be predicted\nmore accurately by the language models.\nB Readability Scores\nWe use the Flesch Reading Easy score (Flesch,\n1948) to deﬁne the readability of the English text in\nthe eye tracking corpora. This score indicates how\ndifﬁcult a text passage is to understand. Since this\nscore relies on language-speciﬁc weighting factors,\nwe apply the Flesch Douma adaptation for Dutch\n(Douma, 1960), the adaptation by Amstad (1978)\nfor German, and the adaptation by Oborneva (2006)\nfor Russian.\nC Implementation Details\nC.1 Tokenization\nWhen using BERT or XLM for token classiﬁcation\nor regression, a pressing implementation issue is\n7spaCy.io\n8https://github.com/buriy/spacy-ru\nrepresented by the subword tokenizers employed\nby the models. This tokenizer, in fact, handles un-\nknown tokens by recursively splitting every word\nuntil all subtokens belong to its vocabulary. For\nexample, the name of the Greek mythological hero\n“Philammon” is tokenized into the three subtokens\n“[‘phil’, ‘##am’, ‘##mon’]”. In this case, our mod-\nels for token regression would produce an eight-\ndimensional output for all three subtokens, and we\nhad the choice as to what to do in order to compute\nthe loss, having only one target for the full word\n“Philammon”. We chose to compute the loss only\nwith respect to the ﬁrst subtoken.\nC.2 Training Setup\nAs described in the main paper, all experi-\nments are run over 5 random seeds, which are\n{12, 79, 237, 549, 886}.\nAll models were ﬁne-tuned on a single GPU Titan\nX with 12 GB memory. Due to memory restrictions\nof the GPUs and the dimensions of the language\nmodels, the batch size was adapted as needed. Ta-\nble 5 shows the batch sizes for each model.\nModel Batch size\nBERT-EN, BERT-NL, 16\nBERT-MULTI\nBERT-DE, BERT-RU, 8\nXLM -ENDE , XLM -17,\nXLM -100\nXLM -EN 2\nTable 5: Batch sizes used for each of the language mod-\nels.\nOn average the validation accuracy of BERT mod-\nels stops improving after ∼50 epochs, while the\nXLM models only take ∼10 epochs. There is no\nnoteworthy difference in training speed between\nmonolingual and multilingual models.\nD Detailed Results\nIn this section we present addition plots that\nstrengthen the results shown in the main paper.\nD.1 Pretrained Baseline\nTables 7 and 8 show the prediction accuracy of the\npretrained models.\nMoreover, Figure 13 shows the results of individ-\nual gaze features for all pretrained models (without\n119\nﬁne-tuning) on the Dundee (en) and RSC (ru) cor-\npora.\nFigure 14 presents the differences in prediction\naccuracy for the pretrained XML -100 model pre-\ndictions relative to the mean baseline for each eye\ntracking feature. The pretrained models clearly can-\nnot outperform the mean baseline for any language\nor dataset.\nD.2 Individual Feature Results\nFigure 15 shows the prediction accuracy of the\nﬁne-tuned language models for the individual eye\ntracking features for all datasets.\nD.3 Word Length Effect\nFigure 16 presents the comparison between models\npredictions and original word length effects for\nfurther languages.\n120\nCorpus NFIX MFD FPROP FFD FPD TRT NREFIX RE PROP\nDundee (en) 0.8 (0.5) 119.5 (62.1) 0.6 (0.3) 120.7 (63.4) 140.6 (88.5) 156.1 (105.5) 0.2 (0.3) 0.2 (0.2)\nGECO (en) 0.8 (0.5) 128.4 (59.0) 0.6 (0.2) 129.3 (60.1) 143.3 (77.5) 168.2 (102.4) 0.2 (0.3) 0.2 (0.2)\nZuCo (en) 1.1 (0.7) 78.4 (34.8) 0.7 (0.3) 77.3 (34.4) 92.3 (52.2) 129.8 (89.7) 0.4 (0.5) 0.3 (0.2)\nGECO (nl) 0.8 (0.6) 121.3 (80.1) 0.6 (0.4) 121.8 (81.1) 134.1 (98.0) 158.1 (131.2) 0.2 (0.4) 0.1 (0.2)\nPoTeC (de) 2.7 (2.9) 217.5 (117.3) 0.8 (0.4) 167.9 (157.4) 224.7 (264.2) 675.6 (727.0) 1.7 (2.2) 0.6 (0.5)\nRSC (ru) 0.8 (0.4) 203.4 (115.1) 0.6 (0.3) 233.6 (49.5) 285.1 (101.9) 314.2 (179.8) 0.1 (0.1) 0.1 (0.1)\nTable 6: Mean and standard deviation for all eye tracking features of the corpora used in this work.\nADJ N V ADV\n0\n200\n400\n600\n800\n1000mean fixation duration (in ms)\nDundee (en)\nADJ N V ADV\nGECO (en)\nADJ N V ADV\nZuCo (en)\nADJ N V ADV\nGECO (nl)\nADJ N V ADV\nPoTeC (de)\nADJ N V ADV\nRSC (ru)\nFigure 11: Mean ﬁxation duration (MFD) for the most common parts of speech across all six datasets.\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\n92\n94\n96Accuracy\nADJ\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\nN\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\nV\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\nADV\nGECO (nl)\nBERT-en\nBERT-multi\nXLM-17\nXLM-100\n89\n90\n91\n92\n93Accuracy\nADJ\nBERT-en\nBERT-multi\nXLM-17\nXLM-100\nN\nBERT-en\nBERT-multi\nXLM-17\nXLM-100\nV\nBERT-en\nBERT-multi\nXLM-17\nXLM-100\nADV\nALL (en)\nFigure 12: Accuracy of the language models predicting the mean ﬁxation duration (MFD) across various parts of\nspeech for Dutch (left) and English (right).\nModel Dundee GECO (en) ZuCo (en) ALL (en)\nBERT-EN 77.42 (0.21) 77.67 (0.13) 76.06 (0.38) 78.69 (0.09)\nBERT-MULTI 77.41 (0.21) 77.68 (0.13) 76.07 (0.37) 78.66 (0.07)\nXLM -EN 77.21 (0.29) 77.65 (0.24) 75.97 (0.60) 78.47 (0.11)\nXLM -ENDE 77.40 (0.29) 77.67 (0.10) 76.10 (0.41) 78.66 (0.12)\nXLM -17 77.31 (0.23) 77.66 (0.19) 75.99 (0.39) 78.39 (0.15)\nXLM -100 77.35 (0.29) 77.63 (0.34) 75.93 (0.43) 78.49 (0.11)\nTable 7: Prediction accuracy of the pretrained language models aggregated over all eye tracking features for the\nEnglish corpora, including the concatenated dataset. Standard deviation is reported in parentheses.\n121\nModel GECO (nl) PoTeC (de) RSC (ru) ALL-LANGS\nBERT-NL 84.20 (0.10) - - -\nBERT-DE - 73.55 (3.07) - -\nBERT-RU - - 64.83 (2.09) -\nBERT-MULTI 84.28 (0.10) 73.47 (3.01) 64.82 (2.11) 86.22 (0.29)\nXLM -ENDE - 73.49 (2.99) - -\nXLM -17 83.93 (0.16) 73.17 (2.86) 65.02 (2.11) 85.84 (0.27)\nXLM -100 83.94 (0.27) 73.28 (2.91) 64.67 (2.10) 85.94 (0.38)\nTable 8: Prediction accuracy of the pretrained language models aggregated over all eye tracking features for the\nDutch, German and Russian corpora, and for all four languages combined in a single dataset. Standard deviation\nis reported in parentheses.\n40 50 60 70 80 90 100\nBERT-en\nBERT-multi\nXLM-en\nXLM-ende\nXLM-17\nXLM-100\nDundee (en)\n20 30 40 50 60 70 80 90 100\nPrediction accuracy\nBERT-ru\nBERT-multi\nXLM-17\nXLM-100\nRSC (ru)\nnFix MFD fProp FFD FPD TRT nRefix reProp\nFigure 13: Results of individual gaze features for all pretrained models (without ﬁne-tuning) on the Dundee (en)\nand RSC (ru) corpora.\nnFix\n30\n20\n10\n0\n10\nAccuracy improvement\nMFD fProp FFD\nDundee GECO-en ZuCo GECO-nl PoTeC RSC\nFPD TRT nRefix reProp\nXLM-100\nFigure 14: Differences in prediction accuracy for the pretrained XLM-100 model predictions (without ﬁne-tuning\non eye tracking data) relative to the mean baseline for each eye tracking feature.\n122\n80 85 90 95 100\nBERT-en\nBERT-multi\nXLM-en\nXLM-ende\nXLM-17\nXLM-100\nGECO (en)\n80 85 90 95 100\nBERT-en\nBERT-multi\nXLM-en\nXLM-ende\nXLM-17\nXLM-100\nZuCo (en)\n80 85 90 95 100\nBERT-en\nBERT-multi\nXLM-en\nXLM-ende\nXLM-17\nXLM-100\nALL (en)\n70 75 80 85 90 95 100\nBERT-nl\nBERT-multi\nXLM-17\nXLM-100\nGECO (nl)\n20 30 40 50 60 70 80 90 100\nBERT-de\nBERT-multi\nXLM-ende\nXLM-17\nXLM-100\nPoTeC (de)\n20 30 40 50 60 70 80 90 100\nPrediction accuracy\nBERT-ru\nBERT-multi\nXLM-17\nXLM-100\nRSC (ru)\nnFix MFD fProp FFD FPD TRT nRefix reProp\nFigure 15: Results of individual eye tracking features for all ﬁne-tuned models on all datasets not presented in the\nmain paper.\n123\n5 10 15 20\nword length\n0\n20\n40\n60\n80\n100(predicted) fixation proportion\nALL (en)\nGaze Data\nBERT-en\nBERT-multi\nXLM-17\nXLM-100\n5 10 15\nword length\n0\n20\n40\n60\n80\n100(predicted) fixation proportion\nRSC (ru)\nGaze Data\nBERT-ru\nBERT-multi\nXLM-17\nXLM-100\n5 10 15 20\nword length\n0\n20\n40\n60\n80\n100(predicted) fixation proportion\nPoTeC (de)\nGaze Data\nBERT-de\nBERT-multi\nXLM-17\nXLM-100\n5 10 15 20\nword length\n0\n20\n40\n60\n80\n100(predicted) fixation proportion\nALL-LANGS\nGaze Data\nBERT-multi\nXLM-17\nXLM-100\nFigure 16: Word length versus predicted ﬁxation probability for Russian, German and English. The gray dashed\nline is the result of the pretrained BERT-MULTI model without ﬁne-tuning."
}