{
  "title": "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation",
  "url": "https://openalex.org/W3132503749",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3090914134",
      "name": "Jeya Maria Jose Valanarasu",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2906456010",
      "name": "Poojan Oza",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A1960093542",
      "name": "Ilker Hacihaliloglu",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2118029367",
      "name": "Vishal M. Patel",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A3090914134",
      "name": "Jeya Maria Jose Valanarasu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2906456010",
      "name": "Poojan Oza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1960093542",
      "name": "Ilker Hacihaliloglu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118029367",
      "name": "Vishal M. Patel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963881378",
    "https://openalex.org/W6600007113",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6603963165",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2981994674",
    "https://openalex.org/W2592905743",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W2964150021",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2288892845",
    "https://openalex.org/W3090974769",
    "https://openalex.org/W3092462072",
    "https://openalex.org/W3034655146",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W2805654231",
    "https://openalex.org/W2979839221",
    "https://openalex.org/W2907750714",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W3104211200",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3130695101",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2147484997",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W2917049430",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3011199263",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2963341956"
  ],
  "abstract": null,
  "full_text": "Medical Transformer: Gated Axial-Attention for\nMedical Image Segmentation\nJeya Maria Jose Valanarasu1, Poojan Oza 1, Ilker Hacihaliloglu2, and Vishal M.\nPatel1\n1 Johns Hopkins University, Baltimore, MD, USA\n2 Rutgers, The State University of New Jersey, NJ, USA\nAbstract. Over the past decade, deep convolutional neural networks\nhave been widely adopted for medical image segmentation and shown to\nachieve adequate performance. However, due to inherent inductive biases\npresent in convolutional architectures, they lack understanding of long-\nrange dependencies in the image. Recently proposed transformer-based\narchitectures that leverage self-attention mechanism encode long-range\ndependencies and learn representations that are highly expressive. This\nmotivates us to explore transformer-based solutions and study the feasi-\nbility of using transformer-based network architectures for medical image\nsegmentation tasks. Majority of existing transformer-based network ar-\nchitectures proposed for vision applications require large-scale datasets\nto train properly. However, compared to the datasets for vision applica-\ntions, in medical imaging the number of data samples is relatively low,\nmaking it diﬃcult to eﬃciently train transformers for medical imaging\napplications. To this end, we propose a gated axial-attention model which\nextends the existing architectures by introducing an additional control\nmechanism in the self-attention module. Furthermore, to train the model\neﬀectively on medical images, we propose a Local-Global training strat-\negy (LoGo) which further improves the performance. Speciﬁcally, we op-\nerate on the whole image and patches to learn global and local features,\nrespectively. The proposed Medical Transformer (MedT) is evaluated on\nthree diﬀerent medical image segmentation datasets and it is shown that\nit achieves better performance than the convolutional and other related\ntransformer-based architectures. Code: https://github.com/jeya-maria-\njose/Medical-Transformer\nKeywords: Transformers· Medical Image Segmentation· Self-Attention.\n1 Introduction\nDeveloping automatic, accurate, and robust medical image segmentation meth-\nods have been one of the principal problems in medical imaging as it is essential\nfor computer-aided diagnosis and image-guided surgery systems. Segmentation\nof organs or lesion from a medical scan helps clinicians make an accurate diagno-\nsis, plan the surgical procedure, and propose treatment strategies. Following the\narXiv:2102.10662v2  [cs.CV]  6 Jul 2021\n2 JMJ Valanarasu et al.\npopularity of deep convolutional neural networks (ConvNets) in computer vi-\nsion, ConvNets were quickly adopted for medical image segmentation. Networks\nlike U-Net [17], V-Net [15], 3D U-Net [4], Res-UNet [27], Dense-UNet [13], Y-\nNet [14], U-Net++ [31], KiU-Net [22,21] and U-Net3+ [8] have been proposed\nspeciﬁcally for performing image and volumetric segmentation for various medi-\ncal imaging modalities. These methods achieve impressive performance on many\ndiﬃcult datasets, proving the eﬀectiveness of ConvNets in learning discrimina-\ntive features to segment the organ or lesion from a medical scan.\nConvNets are currently the basic building blocks of most methods proposed\nfor image segmentation. However, they lack the ability to model long-range de-\npendencies present in an image. More precisely, in ConvNets each convolutional\nkernel attends to only a local-subset of pixels in the whole image and forces the\nnetwork to focus on local patterns rather than the global context. There have\nbeen works that have focused on modeling long-range dependencies for ConvNets\nusing image pyramids [29], atrous convolutions [3] and attention mechanisms [9].\nHowever, it can be noted that there is still a scope of improvement for modeling\nlong-range dependencies as the majority of previous methods do not focus on\nthis aspect for medical image segmentation tasks.\n(a) (b) (c) (d) (e)\nFig. 1.(a) Input Ultrasound of in vivo preterm neonatal brain ventricle. Predictions\nby (b) U-Net, (c) Res-UNet, (d) MedT, and (e) Ground Truth. The red box highlights\nthe region which are miss-classiﬁed by ConvNet based methods due to lack of learned\nlong-range dependencies. The ground truth here was segmented by an expert clinician.\nAlthough it shows some bleeding inside the ventricle area, it does not correspond to the\nsegmented area. This information is correctly captured by transformer-based models.\nTo ﬁrst understand why long-range dependencies matter for medical images,\nwe visualize an example ultrasound scan of a preterm neonate and segmentation\npredictions of brain ventricles from the scan in Fig 1. For a network to provide an\neﬃcient segmentation, it should be able to understand which pixels correspond\nto the mask and which to the background. As the background of the image is\nscattered, learning long-range dependencies between the pixels corresponding to\nthe background can help in the network to prevent miss-classifying a pixel as the\nmask leading to reduction of false positives (considering 0 as background and\n1 as segmentation mask). Similarly, whenever the segmentation mask is large,\nlearning long-range dependencies between the pixels corresponding to the mask\nis also helpful in making eﬃcient predictions. In Fig 1 (b) and (c), we can see\nthat the convolutional networks miss-classify the background as a brain ventricle\nwhile the proposed transformer-based method does not make that mistake. This\nhappens as our proposed method learns long-range dependencies of the pixel\nregions with that of the background.\nMedical Transformer 3\nIn many natural language processing (NLP) applications, transformers [5]\nhave shown to be able to encode long-range dependencies. This is due to the\nself-attention mechanism which ﬁnds the dependency between given sequential\ninput. Following their popularity in NLP applications, transformers have been\nadopted to computer vision applications very recently [6,20]. With regard to\ntransformers for segmentation tasks, Axial-Deeplab [24] utilized the axial at-\ntention module [7], which factorizes 2D self-attention into two 1D self-attentions\nand introduced position-sensitive axial attention design for segmentation. In Seg-\nmentation Transformer (SETR) [30], a transformer was used as encoder which\ninputs a sequence of image patches and a ConvNet was used as decoder resulting\nin a powerful segmentation model. In medical image segmentation, transformer-\nbased models have not been explored much. The closest works are the ones that\nuse attention mechanisms to boost the performance [16,26]. However, the en-\ncoder and decoder of these networks still have convolutional layers as the main\nbuilding blocks.\nIt was observed that that the transformer-based models work well only when\nthey are trained on large-scale datasets [6]. This becomes problematic while\nadopting transformers for medical imaging tasks as the number of images, with\ncorresponding labels, available for training in any medical dataset is relatively\nscarce. Labeling process is also expensive and requires expert knowledge. Speciﬁ-\ncally, training with fewer images causes diﬃculty in learning positional encoding\nfor the images. To this end, we propose a gated position-sensitive axial attention\nmechanism where we introduce four gates that control the amount of informa-\ntion the positional embedding supply to key, query, and value. These gates are\nlearnable parameters which make the proposed mechanism to be applied to any\ndataset of any size. Depending on the size of the dataset, these gates would\nlearn whether the number of images would be suﬃcient enough to learn proper\nposition embedding. Based on whether the information learned by the positional\nembedding is useful or not, the gate parameters either converge to 0 or to some\nhigher value. Furthermore, we propose a Local-Global (LoGo) training strategy,\nwhere we use a shallow global branch and a deep local branch that operates\non the patches of the medical image. This strategy improves the segmentation\nperformance as we do not only operate on the entire image but focus on ﬁner\ndetails present in the local patches. Finally, we propose Medical Transformer\n(MedT), which uses our gated position-sensitive axial attention as the building\nblocks and adopts our LoGo training strategy.\nIn summary, this paper (1) proposes a gated position-sensitive axial attention\nmechanism that works well even on smaller datasets, (2) introduces Local-Global\n(LoGo) training methodology for transformers which is eﬀective, (3) proposes\nMedical-Transformer (MedT) which is built upon the above two concepts pro-\nposed speciﬁcally for medical image segmentation, and (4) successfully improves\nthe performance for medical image segmentation tasks over convolutional net-\nworks and fully attention architectures on three diﬀerent datasets.\n4 JMJ Valanarasu et al.\n2 Medical Transformer (MedT)\n2.1 Self-Attention Overview\nLet us consider an input feature map x ∈RCin×H×W with height H, weight\nW and channels Cin. The output y ∈RCout×H×W of a self-attention layer is\ncomputed with the help of projected input using the following equation:\nyij =\nH∑\nh=1\nW∑\nw=1\nsoftmax\n(\nqT\nijkhw\n)\nvhw, (1)\nwhere queries q = WQx, keys k = WKx and values v = WV x are all projections\ncomputed from the input x. Here, qij, kij, vij denote query, key and value at\nany arbitrary location i ∈ {1, . . . , H}and j ∈ {1, . . . , W}, respectively. The\nprojection matrices WQ, WK, WV ∈RCin×Cout are learnable. As shown in Eq. 1,\nthe values v are pooled based on global aﬃnities calculated using softmax( qT k).\nHence, unlike convolutions the self-attention mechanism is able to capture non-\nlocal information from the entire feature map. However, computing such aﬃnities\nare computationally very expensive and with increased feature map size it often\nbecomes infeasible to use self-attention for vision model architectures. Moreover,\nunlike convolutional layer, self-attention layer does not utilize any positional\ninformation while computing the non-local context. Positional information is\noften useful in vision models to capture structure of an object.\nGlobal Branch\nLocal Branch\nEncoder\nBlock\nDecoder\nBlock\nImage\nSegmentation \nMask\n1x1\nConv Add\nConv\n1x1 Norm\nGated\nMulti-\nHead\nAttn\nHeight\nGated\nMulti-\nHead\nAttn\nWidth\nConv\n1x1 +NormInput\nEncoder - Gated Axial Transformer Layer\n(a)\n(b)\nConv\nBlock\nX \nW V W K W Q \nr Q r K \nG Q G K \nr V \nG V 1 G V 2 \nsoftmax\nyY \nGates\nPositional \nEmbeddings\nWeights\nMatrix\nMultiplication\nAddition\n(c)\nGated Axial Attention Layer\nResample\nPatches\nPatches\nFig. 2. (a) The main architecture diagram of MedT which uses LoGo strategy for\ntraining. (b) The gated axial transformer layer which is used in MedT. (c) Gated\nAxial Attention layer which is the basic building block of both height and width gated\nmulti-head attention blocks found in the gated axial transformer layer.\nMedical Transformer 5\nAxial-Attention To overcome the computational complexity of calculating the\naﬃnities, self-attention is decomposed into two self-attention modules. The ﬁrst\nmodule performs self-attention on the feature map height axis and the second\none operates on the width axis. This is referred to as axial attention [7]. The\naxial attention consequently applied on height and width axis eﬀectively model\noriginal self-attention mechanism with much better computational eﬃcacy. To\nadd positional bias while computing aﬃnities through self-attention mechanism,\na position bias term is added to make the aﬃnities sensitive to the positional\ninformation [18]. This bias term is often referred to as relative positional en-\ncodings. These positional encodings are typically learnable through training and\nhave been shown to have the capacity to encode spatial structure of the image.\nWang et al. [24] combined both the axial-attention mechanism and positional\nencodings to propose an attention-based model for image segmentation. Addi-\ntionally, unlike previous attention model which utilizes relative positional en-\ncodings only for queries, Wang et al.[24] proposed to use it for all queries, keys\nand values. This additional position bias in query, key and value is shown to\ncapture long-range interaction with precise positional information [24]. For any\ngiven input feature map x, the updated self-attention mechanism with positional\nencodings along with width axis can be written as:\nyij =\nW∑\nw=1\nsoftmax\n(\nqT\nijkiw + qT\nijrq\niw + kT\niwrk\niw\n)\n(viw + rv\niw), (2)\nwhere the formulation in Eq. 2 follows the attention model proposed in [24]\nand rq, rk, rv ∈ RW×W for the width-wise axial attention model. Note that\nEq. 2 describes the axial attention applied along the width axis of the tensor. A\nsimilar formulation is also used to apply axial attention along the height axis and\ntogether they form a single self-attention model that is computationally eﬃcient.\n2.2 Gated Axial-Attention\nWe discussed the beneﬁts of using the axial-attention mechanism proposed in\n[24] for visual recognition. Speciﬁcally, the axial-attention proposed in [24] is able\nto compute non-local context with good computational eﬃciency, able to encode\npositional bias into the mechanism and enables the ability to encode long-range\ninteraction within an input feature map. However, their model is evaluated on\nlarge-scale segmentation datasets and hence it is easier for the axial-attention\nto learn positional bias at key, query and value. We argue that for experiments\nwith small-scale datasets, which is often the case in medical image segmentation,\nthe positional bias is diﬃcult to learn and hence will not always be accurate in\nencoding long-range interactions. In the case where the learned relative positional\nencodings are not accurate enough, adding them to the respective key, query and\nvalue tensor would result in reduced performance. Hence, we propose a modiﬁed\naxial-attention block that can control the inﬂuence positional bias can exert\nin the encoding of non-local context. With the proposed modiﬁcation the self-\n6 JMJ Valanarasu et al.\nattention mechanism applied on the width axis can be formally written as:\nyij =\nW∑\nw=1\nsoftmax\n(\nqT\nijkiw + GQqT\nijrq\niw + GKkT\niwrk\niw\n)\n(GV 1viw + GV 2rv\niw), (3)\nwhere the self-attention formula closely follows Eq. 2 with added gating mecha-\nnism. Also, GQ, GK, GV 1, GV 2 ∈R are learnable parameters and together they\ncreate gating mechanism which control inﬂuence of the learned relative positional\nencodings have on encoding non-local context. Typically, if a relative positional\nencoding is learned accurately, the gating mechanism will assign it high weight\ncompared to the ones which are not learned accurately. Fig 2 (c) illustrates the\nfeed-forward in a typical gated axial attention layer.\n2.3 Local-Global Training\nIt is evident that a transformer on patches is faster but patch-wise training\nalone is not suﬃcient for the tasks like medical image segmentation. Patch-wise\ntraining restricts the network in learning any information or dependencies for\ninter-patch pixels. To improve the overall understanding of the image, we propose\nto use two branches in the network, i.e., a global branch which works on the\noriginal resolution of the image, and a local branch which operates on patches of\nthe image. In the global branch, we reduce the number of gated axial transformer\nlayers as we observe that the ﬁrst few blocks of the proposed transformer model\nis suﬃcient to model long range dependencies. In the local branch, we create\n16 patches of size I/4 ×I/4 of the image where I is the dimensions of the\noriginal image. In the local branches, each patch is feed forwarded through the\nnetwork and the output feature maps are re-sampled based on their location to\nget the output feature maps. The output feature maps of both of the branches are\nthen added and passed through a 1 ×1 convolution layer to produce the output\nsegmentation mask. This strategy improves the performance as the global branch\nfocuses on high-level information and the local branch can focus on ﬁner details.\nThe proposed Medical Transformer (MedT) uses gated axial attention layer as\nthe basic building block and uses LoGo strategy for training. It is illustrated in\nFig 2 (a). More details on the architecture and an ablation study with regard to\nthe architecture can be found in the supplementary ﬁle.\n3 Experiments and Results\n3.1 Dataset details\nWe use Brain anatomy segmentation (ultrasound) [25,23], Gland segmentation\n(microscopic) [19] and MoNuSeg (microscopic) [11,12] datasets for evaluating our\nmethod. More details about the datasets can be found in the supplementary.\nMedical Transformer 7\n3.2 Implementation details\nWe use binary cross-entropy (CE) loss between the prediction and the ground\ntruth to train our network and can be written as:\nLCE(p,ˆp) = −\n(\n1\nwh\nw−1∑\nx=0\nh−1∑\ny=0\n(p(x, y) log(ˆp(x, y))) + (1−p(x, y)) log(1−ˆp(x, y))\n)\nwhere w and h are the dimensions of the image, p(x, y) corresponds to the pixel\nin the image and ˆ p(x, y) denotes the output prediction at a speciﬁc location\n(x, y). The training details are provided in the supplementary document.\nFor baseline comparisons, we ﬁrst run experiments on both convolutional and\ntransformer-based methods. For convolutional baselines, we compare with fully\nconvolutional network (FCN) [1], U-Net [17], U-Net++ [31] and Res-Unet [27].\nFor transformer-based baselines, we use Axial-Attention U-Net with residual\nconnections inspired from [24]. For our proposed method, we experiment with\nall the individual contributions. In gated axial attention network, we use axial\nattention U-Net with all its axial attention layers replaced with the proposed\ngated axial attention layers. In LoGo, we perform local global training for axial\nattention U-Net without using the gated axial attention layers. In MedT, we\nuse gated axial attention as the basic building block for global branch and axial\nattention without positional encoding for local branch.\n3.3 Results\nTable 1.Quantitative comparison of the proposed methods with convolutional and\ntransformer based baselines in terms of F1 and IoU scores.\nType Network Brain US GlaS MoNuSeg\nF1 IoU F1 IoU F1 IoU\nFCN [1] 82.79 75.02 66.61 50.84 28.84 28.71\nConvolutional\nBaselines U-Net [17] 85.37 79.31 77.78 65.34 79.43 65.99\nU-Net++ [31] 86.59 79.95 78.03 65.55 79.49 66.04\nRes-UNet [27] 87.50 79.61 78.83 65.95 79.49 66.07\nFully Attention\nBaseline\nAxial Attention\nU-Net [24] 87.92 80.14 76.26 63.03 76.83 62.49\nGated Axial Attn.88.39 80.7 79.91 67.85 76.44 62.01\nProposed LoGo 88.54 80.84 79.68 67.69 79.56 66.17\nMedT 88.8481.3481.0269.6179.5566.17\nFor quantitative analysis, we use F1 and IoU scores for comparison. The\nquantitative results are tabulated in Table 1. It can be noted that for datasets\nwith relatively more images like Brain US, fully attention (transformer) based\nbaseline performs better than convolutional baselines. For GlaS and MoNuSeg\ndatasets, convolutional baselines perform better than fully attention baselines\nas it is diﬃcult to train fully attention models with less data [6]. The proposed\nmethod is able to overcome such issue with the help of gated axial attention\nand LoGo both individually perform better than the other methods. Our ﬁnal\narchitecture MedT performs better than Gated axial attention, LoGo and all\nthe previous methods. The improvements over fully attention baselines are 0.92\n8 JMJ Valanarasu et al.\n%, 4.76 % and 2.72 % for Brain US, GlaS and MoNuSeg datasets, respectively.\nImprovements over the best convolutional baseline are 1.32 %, 2.19 % and 0.06\n%. All of these values are in terms of F1 scores. For the ablation study, we use\nthe Brain US data for all our experiments. The results for the same has been\ntabulated in Table 2.\nFurthermore, we visualize the predictions from U-Net [17], Res-UNet [27],\nAxial Attention U-Net [24] and our proposed method MedT in Fig 3. It can be\nseen that the predictions of MedT captures the long range dependencies really\nwell. For example, in the second row of Fig 3, we can observe that the small seg-\nmentation mask highlighted on red box goes undetected in all the convolutional\nbaselines. However, as fully attention model encodes long range dependencies,\nit learns to segment well thanks to the encoded global context. In the ﬁrst and\nfourth row, other methods make false predictions at the highlighted regions as\nthose pixels are in close proximity to the segmentation mask. As our method\ntakes into account pixel-wise dependencies that are encoded with gating mecha-\nnism, it is able to learn those dependencies better than the axial attention U-Net.\nThis makes our predictions more precise as they do not miss-classify pixels near\nthe segmentation mask.\nTable 2.Ablation Study\nNetworkU-Net [17]Res-UNet [27]Axial UNet [24]Gated Axial UNetGlobal\nonly\nLocal\nonlyLoGoMedT\nF1 Score85.37 87.5 87.92 88.39 87.6777.5588.5488.84\nInput U-Net Res U-NetAxial Attn. U-NetMedT GT\nFig. 3.Qualitative results on sample test images from Brain US, Glas and MoNuSeg\ndatasets. The red box highlights regions where exactly MedT performs better than the\nother methods in comparison making better use of long range dependencies.\n4 Conclusion\nIn this work, we explored the use of transformer-based architectures for med-\nical image segmentation. Speciﬁcally, we propose a gated axial attention layer\nMedical Transformer 9\nwhich is used as the building block for multi-head attention models. We also pro-\nposed a LoGo training strategy to train the image in both full resolution as well\nin patches. The global branch helps learn global context features by modeling\nlong-range dependencies, where as the local branch focus on ﬁner features by op-\nerating on patches. Using these, we propose MedT (Medical Transformer) which\nhas gated axial attention as its main building block for the encoder and uses\nLoGo strategy for training. Unlike other transformer-based model the proposed\nmethod does not require pre-training on large-scale datasets. Finally, we conduct\nextensive experiments on three datasets where we achieve a good performance\nfor MedT over ConvNets and other related transformer-based architectures.\nAcknowledgment\nThis work was supported by the NSF grant 1910141.\nReferences\n1. Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: A deep convolutional\nencoder-decoder architecture for image segmentation. IEEE transactions on pat-\ntern analysis and machine intelligence 39(12), 2481–2495 (2017)\n2. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,\nY.: Transunet: Transformers make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306 (2021)\n3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic\nimage segmentation with deep convolutional nets and fully connected crfs. arXiv\npreprint arXiv:1412.7062 (2014)\n4. C ¸ i¸ cek,¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3d u-net:\nlearning dense volumetric segmentation from sparse annotation. In: International\nconference on medical image computing and computer-assisted intervention. pp.\n424–432. Springer (2016)\n5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n6. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n7. Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multi-\ndimensional transformers. arXiv preprint arXiv:1912.12180 (2019)\n8. Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W.,\nWu, J.: Unet 3+: A full-scale connected unet for medical image segmentation. In:\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). pp. 1055–1059. IEEE (2020)\n9. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: Criss-cross\nattention for semantic segmentation. In: Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision. pp. 603–612 (2019)\n10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n10 JMJ Valanarasu et al.\n11. Kumar, N., Verma, R., Anand, D., Zhou, Y., Onder, O.F., Tsougenis, E., Chen,\nH., Heng, P.A., Li, J., Hu, Z., et al.: A multi-organ nucleus segmentation challenge.\nIEEE transactions on medical imaging 39(5), 1380–1391 (2019)\n12. Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A., Sethi, A.: A\ndataset and a technique for generalized nuclear segmentation for computational\npathology. IEEE transactions on medical imaging 36(7), 1550–1560 (2017)\n13. Li, X., Chen, H., Qi, X., Dou, Q., Fu, C.W., Heng, P.A.: H-denseunet: hybrid\ndensely connected unet for liver and tumor segmentation from ct volumes. IEEE\ntransactions on medical imaging 37(12), 2663–2674 (2018)\n14. Mehta, S., Mercan, E., Bartlett, J., Weaver, D., Elmore, J.G., Shapiro, L.: Y-\nnet: joint segmentation and classiﬁcation for diagnosis of breast biopsy images. In:\nInternational Conference on Medical Image Computing and Computer-Assisted\nIntervention. pp. 893–901. Springer (2018)\n15. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In: 2016 fourth international confer-\nence on 3D vision (3DV). pp. 565–571. IEEE (2016)\n16. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\nK., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning\nwhere to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018)\n17. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234–241. Springer (2015)\n18. Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position represen-\ntations. In: Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies,\nVolume 2 (Short Papers). pp. 464–468 (2018)\n19. Sirinukunwattana, K., Pluim, J.P., Chen, H., Qi, X., Heng, P.A., Guo, Y.B., Wang,\nL.Y., Matuszewski, B.J., Bruni, E., Sanchez, U., et al.: Gland segmentation in colon\nhistology images: The glas challenge contest. Medical image analysis 35, 489–502\n(2017)\n20. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´ egou, H.: Training\ndata-eﬃcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877 (2020)\n21. Valanarasu, J.M.J., Sindagi, V.A., Hacihaliloglu, I., Patel, V.M.: Kiu-net: Over-\ncomplete convolutional architectures for biomedical image and volumetric segmen-\ntation. arXiv preprint arXiv:2010.01663 (2020)\n22. Valanarasu, J.M.J., Sindagi, V.A., Hacihaliloglu, I., Patel, V.M.: Kiu-net: Towards\naccurate segmentation of biomedical images using over-complete representations.\nIn: International Conference on Medical Image Computing and Computer-Assisted\nIntervention. pp. 363–373. Springer (2020)\n23. Valanarasu, J.M.J., Yasarla, R., Wang, P., Hacihaliloglu, I., Patel, V.M.: Learning\nto segment brain anatomy from 2d ultrasound with less data. IEEE Journal of\nSelected Topics in Signal Processing 14(6), 1221–1234 (2020)\n24. Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., Chen, L.C.: Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint\narXiv:2003.07853 (2020)\n25. Wang, P., Cuccolo, N.G., Tyagi, R., Hacihaliloglu, I., Patel, V.M.: Automatic real-\ntime cnn-based neonatal brain ventricles segmentation. In: 2018 IEEE 15th In-\nternational Symposium on Biomedical Imaging (ISBI 2018). pp. 716–719. IEEE\n(2018)\nMedical Transformer 11\n26. Wang, X., Han, S., Chen, Y., Gao, D., Vasconcelos, N.: Volumetric attention for 3d\nmedical image segmentation and detection. In: International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention. pp. 175–184. Springer\n(2019)\n27. Xiao, X., Lian, S., Luo, Z., Li, S.: Weighted res-unet for high-quality retina vessel\nsegmentation. In: 2018 9th international conference on information technology in\nmedicine and education (ITME). pp. 327–331. IEEE (2018)\n28. Zhang, Y., Liu, H., Hu, Q.: Transfuse: Fusing transformers and cnns for medical\nimage segmentation. arXiv preprint arXiv:2102.08005 (2021)\n29. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 2881–2890 (2017)\n30. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T.,\nTorr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840 (2020)\n31. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net\narchitecture for medical image segmentation. In: Deep learning in medical image\nanalysis and multimodal learning for clinical decision support, pp. 3–11. Springer\n(2018)\nSupplementary Material for Medical\nTransformer: Gated Axial-Attention for Medical\nImage Segmentation\nJeya Maria Jose Valanarasu1, Poojan Oza 1, Ilker Hacihaliloglu2, and Vishal M.\nPatel1\n1 Johns Hopkins University, Baltimore, MD, USA\n2 Rutgers, The State University of New Jersey, NJ, USA\nIn this supplementary material, we describe more details about the datasets\nthat we used; provide more intricate details on our proposed architecture and\ntraining strategy; conduct an ablation study for our proposed methods; conduct\nan analysis on the number of parameters and present some more results.\n1 Dataset details\nIn this section, we describe the datasets that we use in this paper in detail.\n1.1 Brain US Dataset\nIntraventricular hemorrhage (IVH) which results in the enlargement of brain\nventricles is one of the main causes of preterm brain injury. The main imaging\nmodality used for diagnosis of brain disorders in preterm neonates is cranial US\nbecause of its safety and cost-eﬀectiveness. Also, absence of septum pellucidum\nis an important biomarker for septo-optic dysplasia diagnosis. Automatic seg-\nmentation of brain ventricles and septum pellucidum from these US scans is\nessential for accurate diagnosis and prognosis of these ailments. After obtaining\ninstitutional review board (IRB) approval, US scans were collected from 20 dif-\nferent premature neonates (age < 1 year). The total number of images collected\nwere 1629 with annotations out of which 1300 were allocated for training and\n329 for testing. We resize the images to 128 ×128 for all our experiments.\n1.2 GLAS Dataset\nGLAnd Segmentation (GLAS) datatset [19] contains microscopic images of Hema-\ntoxylin and Eosin (H&E) stained slides and the corresponding ground truth an-\nnotations by expert pathologists. It contains a total of 165 images which are split\ninto 85 images for training and 80 for testing. Since the images in the dataset\nare of diﬀerent sizes, we resize every image to a resolution of 128 ×128 for all\nour experiments.\nTitle Suppressed Due to Excessive Length 13\n1.3 MoNuSeg Dataset\nMoNuSeg dataset [11,12] was created using H&E stained tissue images captured\nat 40x magniﬁcation. This dataset is diverse as it contains images across multiple\norgans and patients. The training data contains 30 images with around 22000\nnuclear boundary annotations. The test data contains 14 images which have over\n7000 nuclear boundary annotations. We resize the images to 512 ×512 for all\nour experiments.\n2 MedT details\nMedical Transformer (MedT) uses gated axial attention layer as the basic build-\ning block and uses LoGo strategy for training. MedT has two branches - a global\nbranch and local branch. The input to both of these branches are the feature\nmaps extracted from an initial conv block. This block has 3 conv layers, each\nfollowed by a batch normalization and ReLU activation. In the encoder of both\nbranches, we use our proposed transformer layer while in the decoder, we use\na conv block. The encoder bottleneck contains a 1 ×1 conv layer followed by\nnormalization and two layers of multi-head attention layers where one operates\nalong height axis and the other along width axis. Each multi-head attention block\nis made up of the proposed gated axial attention layer. Note that each multi-\nhead attention block has 8 gated axial attention heads. The output from the\nmulti-head attention blocks are concatenated and passed through another 1 ×1\nconv which are added to residual input maps to produce the output attention\nmaps. In each decoder block, we have a conv layer followed by an upsampling\nlayer and ReLU activation. We also have skip connections between each encoder\nand decoder blocks in both the branches.\nIn the global branch of MedT, we have 2 blocks of encoder and 2 blocks of\ndecoder. In the local branch, we have 5 blocks of encoder and 5 blocks of decoder.\n3 Training details\nWe use a batch size of 4, Adam optimizer [10] and a learning rate of 0.001 for our\nexperiments. The network is trained for 400 epochs. While training the gated\naxial attention layer, we do not activate the training of the gates for the ﬁrst 10\nepochs. We use a Nvidia Quadro 8000 GPU for all our experiments.\n4 Analysis\nIn this section, we present an analysis over some of the parameters and methods\nwe used for our proposed method.\n14 JMJ Valanarasu et al.\n4.1 Ablation Study\nFor the ablation study, we use the Brain US data for all our experiments. We ﬁrst\nstart with a standard U-Net. Then, we add residual connections to the U-Net\nmaking it a Res-UNet. Now, we replace all the convolutional layers in the encoder\nof Res-UNet with axial attention layers. This conﬁguration is Axial Attention\nUNet inspired from [24]. Note that in this conﬁguration we have an additional\nconv block at the front for feature extraction. Next, we replace all the axial\nattention layers from the previous conﬁguration with gated axial attention layers.\nThis conﬁguration is denoted as Gated Axial attention. We then experiment\nusing only the global branch and local branch individually from LoGo strategy.\nThis shows that using just 2 layers in the global branch is enough to get a decent\nperformance. The local branch in this conﬁguration is tested on the patches\nextracted from the image. Then, we combine both the branches to train the\nnetwork in an end-to-end fashion which is denoted as LoGo. Note that in this\nconﬁguration the attention layers used are just axial attention layers [24]. Finally,\nwe replace the axial attention layers in LoGo with gated axial attention layers\nwhich leads to MedT. The ablation study shows that each individual components\nof MedT provides useful contribution to improve the performance.\nTable 1.Ablation Study\nNetwork U-Net [17] Res-\nUNet [27]\nAxial\nUNet [24]\nGated\nAxial\nUNet\nGlobal\nonly\nLocal\nonly LoGo MedT\nF1 Score 85.37 87.5 87.92 88.39 87.67 77.55 88.54 88.84\n4.2 Number of Parameters\nTable 2.Comparison in terms of number of parameters between the proposed method\nwith the existing methods.\nNetwork FCN [1] U-Net [17] U-Net [17]\n(mod)\nRes-\nUNet [27]\nRes\nUNet [27]\n(mod)\nAxial\nUNet [24]\nGated\nAxial\nUNet\nMedT\nParameters 12.5 M 3.13 M 1.3 M 5.32 M 1.34 M 1.3 M 1.3 M 1.4 M\nF1 Score 82.79 87.71 85.37 87.73 87.5 87.92 88.39 88.84\nAlthough MedT is a multi-branch network, we reduce the number of param-\neters by using only 2 layers of encoder and decoder in the global branch and\nmaking the local branch operate on only patches of image. Also, the proposed\ngated axial attention block adds only 4 more learnable parameters to the layer.\nTitle Suppressed Due to Excessive Length 15\nIn Table 2, we compare the number of parameters with other methods. U-Net\ncorresponds to the original implementation according to [17]. U-Net (mod) cor-\nresponds to the U-Net conﬁguration with reduced number of ﬁlters so as to\nmatch the number of parameters in MedT. Similarly, Res-UNet and Res-UNet\n(mod) corresponds to conﬁgurations with more and less number of parameters\nby adjusting the number of ﬁlters. We do this to show that even with more num-\nber of parameters, the baselines do not exceed MedT in terms of performance\nindicating that the improvement is not due to slight change in the number of\nparameters.\n5 Results\nFig. 1.Qualitative Results. The red box highlights the regions where our proposed\nmethod outperforms the convolutional baselines.\nWe present some additional qualitative results on top of the qualitative re-\nsults presented in the main paper. In Fig 1, we visualize the predictions for our\nproposed method MedT along with the predictions for baselines UNet and Res-\nUNet for a couple of US scans. In both the samples, it can be seen that the\nregions that are highlighted in the red box are miss-classiﬁed to be brain ven-\ntricles for the convolutional baselines. However, our proposed attention based\nMedT does not make the same mistake.\n6 Concurrent works\nVery recently, TransUNet [2] was proposed which uses a transformer-based en-\ncoder operating on sequences of image patches and a convolutional decoder with\nskip connections for medical image segmentation. As TransUNet is inspired by\n16 JMJ Valanarasu et al.\nViT, it is still dependent on pretrained weights obtained by training on a large\nimage corpus. TransFuse [28] was recently proposed for polyp segmentation tasks\nusing a parallel CNN branch and transformer branch fused using a BiFusion\nmodule. Unlike these works, we explore the feasibility of applying transform-\ners working on only self-attention mechanisms as an encoder for medical image\nsegmentation and without any need for pre-training.\nReferences\n1. Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: A deep convolutional\nencoder-decoder architecture for image segmentation. IEEE transactions on pat-\ntern analysis and machine intelligence 39(12), 2481–2495 (2017)\n2. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,\nY.: Transunet: Transformers make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306 (2021)\n3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic\nimage segmentation with deep convolutional nets and fully connected crfs. arXiv\npreprint arXiv:1412.7062 (2014)\n4. C ¸ i¸ cek,¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3d u-net:\nlearning dense volumetric segmentation from sparse annotation. In: International\nconference on medical image computing and computer-assisted intervention. pp.\n424–432. Springer (2016)\n5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n6. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n7. Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multi-\ndimensional transformers. arXiv preprint arXiv:1912.12180 (2019)\n8. Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W.,\nWu, J.: Unet 3+: A full-scale connected unet for medical image segmentation. In:\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). pp. 1055–1059. IEEE (2020)\n9. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: Criss-cross\nattention for semantic segmentation. In: Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision. pp. 603–612 (2019)\n10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n11. Kumar, N., Verma, R., Anand, D., Zhou, Y., Onder, O.F., Tsougenis, E., Chen,\nH., Heng, P.A., Li, J., Hu, Z., et al.: A multi-organ nucleus segmentation challenge.\nIEEE transactions on medical imaging 39(5), 1380–1391 (2019)\n12. Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A., Sethi, A.: A\ndataset and a technique for generalized nuclear segmentation for computational\npathology. IEEE transactions on medical imaging 36(7), 1550–1560 (2017)\n13. Li, X., Chen, H., Qi, X., Dou, Q., Fu, C.W., Heng, P.A.: H-denseunet: hybrid\ndensely connected unet for liver and tumor segmentation from ct volumes. IEEE\ntransactions on medical imaging 37(12), 2663–2674 (2018)\nTitle Suppressed Due to Excessive Length 17\n14. Mehta, S., Mercan, E., Bartlett, J., Weaver, D., Elmore, J.G., Shapiro, L.: Y-\nnet: joint segmentation and classiﬁcation for diagnosis of breast biopsy images. In:\nInternational Conference on Medical Image Computing and Computer-Assisted\nIntervention. pp. 893–901. Springer (2018)\n15. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In: 2016 fourth international confer-\nence on 3D vision (3DV). pp. 565–571. IEEE (2016)\n16. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\nK., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning\nwhere to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018)\n17. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234–241. Springer (2015)\n18. Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position represen-\ntations. In: Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies,\nVolume 2 (Short Papers). pp. 464–468 (2018)\n19. Sirinukunwattana, K., Pluim, J.P., Chen, H., Qi, X., Heng, P.A., Guo, Y.B., Wang,\nL.Y., Matuszewski, B.J., Bruni, E., Sanchez, U., et al.: Gland segmentation in colon\nhistology images: The glas challenge contest. Medical image analysis 35, 489–502\n(2017)\n20. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´ egou, H.: Training\ndata-eﬃcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877 (2020)\n21. Valanarasu, J.M.J., Sindagi, V.A., Hacihaliloglu, I., Patel, V.M.: Kiu-net: Over-\ncomplete convolutional architectures for biomedical image and volumetric segmen-\ntation. arXiv preprint arXiv:2010.01663 (2020)\n22. Valanarasu, J.M.J., Sindagi, V.A., Hacihaliloglu, I., Patel, V.M.: Kiu-net: Towards\naccurate segmentation of biomedical images using over-complete representations.\nIn: International Conference on Medical Image Computing and Computer-Assisted\nIntervention. pp. 363–373. Springer (2020)\n23. Valanarasu, J.M.J., Yasarla, R., Wang, P., Hacihaliloglu, I., Patel, V.M.: Learning\nto segment brain anatomy from 2d ultrasound with less data. IEEE Journal of\nSelected Topics in Signal Processing 14(6), 1221–1234 (2020)\n24. Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., Chen, L.C.: Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint\narXiv:2003.07853 (2020)\n25. Wang, P., Cuccolo, N.G., Tyagi, R., Hacihaliloglu, I., Patel, V.M.: Automatic real-\ntime cnn-based neonatal brain ventricles segmentation. In: 2018 IEEE 15th In-\nternational Symposium on Biomedical Imaging (ISBI 2018). pp. 716–719. IEEE\n(2018)\n26. Wang, X., Han, S., Chen, Y., Gao, D., Vasconcelos, N.: Volumetric attention for 3d\nmedical image segmentation and detection. In: International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention. pp. 175–184. Springer\n(2019)\n27. Xiao, X., Lian, S., Luo, Z., Li, S.: Weighted res-unet for high-quality retina vessel\nsegmentation. In: 2018 9th international conference on information technology in\nmedicine and education (ITME). pp. 327–331. IEEE (2018)\n28. Zhang, Y., Liu, H., Hu, Q.: Transfuse: Fusing transformers and cnns for medical\nimage segmentation. arXiv preprint arXiv:2102.08005 (2021)\n18 JMJ Valanarasu et al.\n29. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 2881–2890 (2017)\n30. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T.,\nTorr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840 (2020)\n31. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net\narchitecture for medical image segmentation. In: Deep learning in medical image\nanalysis and multimodal learning for clinical decision support, pp. 3–11. Springer\n(2018)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7491058707237244
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6803168058395386
    },
    {
      "name": "Segmentation",
      "score": 0.6713728904724121
    },
    {
      "name": "Transformer",
      "score": 0.6709252595901489
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6165747046470642
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5641336441040039
    },
    {
      "name": "Deep learning",
      "score": 0.4577426612377167
    },
    {
      "name": "Image segmentation",
      "score": 0.4175938665866852
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36221474409103394
    },
    {
      "name": "Computer vision",
      "score": 0.3338872492313385
    },
    {
      "name": "Machine learning",
      "score": 0.3220752477645874
    },
    {
      "name": "Engineering",
      "score": 0.12300264835357666
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}