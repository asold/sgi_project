{
  "title": "A Deep Dive into Single-Cell RNA Sequencing Foundation Models",
  "url": "https://openalex.org/W4387866792",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3041885414",
      "name": "Rebecca Boiarsky",
      "affiliations": [
        "Moscow Institute of Thermal Technology",
        "Broad Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2110711090",
      "name": "Nalini Singh",
      "affiliations": [
        "Moscow Institute of Thermal Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2886156837",
      "name": "Alejandro Buendia",
      "affiliations": [
        "Moscow Institute of Thermal Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2160087672",
      "name": "Gad Getz",
      "affiliations": [
        "Broad Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2138531792",
      "name": "David N Sontag",
      "affiliations": [
        "Moscow Institute of Thermal Technology",
        "Broad Institute"
      ]
    },
    {
      "id": "https://openalex.org/A3041885414",
      "name": "Rebecca Boiarsky",
      "affiliations": [
        "Broad Institute",
        "Moscow Institute of Thermal Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110711090",
      "name": "Nalini Singh",
      "affiliations": [
        "Moscow Institute of Thermal Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2886156837",
      "name": "Alejandro Buendia",
      "affiliations": [
        "Moscow Institute of Thermal Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2160087672",
      "name": "Gad Getz",
      "affiliations": [
        "Broad Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2138531792",
      "name": "David N Sontag",
      "affiliations": [
        "Broad Institute",
        "Moscow Institute of Thermal Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2943495267",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W4378806473",
    "https://openalex.org/W4384821687",
    "https://openalex.org/W2900694973",
    "https://openalex.org/W4281758439",
    "https://openalex.org/W2787894218",
    "https://openalex.org/W2951506174",
    "https://openalex.org/W2951560273",
    "https://openalex.org/W4316096017",
    "https://openalex.org/W3126902084",
    "https://openalex.org/W2931036699",
    "https://openalex.org/W1583837637"
  ],
  "abstract": "Abstract Large-scale foundation models, which are pre-trained on massive, unlabeled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and biology. In this paper, we explore two foundation models recently developed for single-cell RNA sequencing data, scBERT and scGPT. Focusing on the fine-tuning task of cell type annotation, we explore the relative performance of pre-trained models compared to a simple baseline, L1-regularized logistic regression, including in the few-shot setting. We perform ablation studies to understand whether pretraining improves model performance and to better understand the difficulty of the pre-training task in scBERT. Finally, using scBERT as an example, we demonstrate the potential sensitivity of fine-tuning to hyperparameter settings and parameter initializations. Taken together, our results highlight the importance of rigorously testing foundation models against well established baselines, establishing challenging fine-tuning tasks on which to benchmark foundation models, and performing deep introspection into the embeddings learned by the model in order to more effectively harness these models to transform single-cell data analysis. Code is available at https://github.com/clinicalml/sc-foundation-eval .",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7299548387527466
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6486802101135254
    },
    {
      "name": "Hyperparameter",
      "score": 0.6015814542770386
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5904483199119568
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.5787622928619385
    },
    {
      "name": "Machine learning",
      "score": 0.5728473663330078
    },
    {
      "name": "Deep learning",
      "score": 0.5265890955924988
    },
    {
      "name": "Task (project management)",
      "score": 0.4955800771713257
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I107606265",
      "name": "Broad Institute",
      "country": "US"
    }
  ],
  "cited_by": 43
}