{
  "title": "Dynamic out-of-vocabulary word registration to language model for speech recognition",
  "url": "https://openalex.org/W3135151149",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5077234015",
      "name": "Norihide Kitaoka",
      "affiliations": [
        "Toyohashi University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5014231830",
      "name": "Bohan Chen",
      "affiliations": [
        "Nagoya University"
      ]
    },
    {
      "id": "https://openalex.org/A5077215462",
      "name": "Yuya Obashi",
      "affiliations": [
        "Tokushima University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4301749008",
    "https://openalex.org/W2142222482",
    "https://openalex.org/W2032942114",
    "https://openalex.org/W28518826",
    "https://openalex.org/W2140281218",
    "https://openalex.org/W2065694008",
    "https://openalex.org/W2972456275",
    "https://openalex.org/W2550123561",
    "https://openalex.org/W2011481288",
    "https://openalex.org/W1983032739",
    "https://openalex.org/W2586419604",
    "https://openalex.org/W2080018251",
    "https://openalex.org/W2100068632",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2045038459",
    "https://openalex.org/W72347498",
    "https://openalex.org/W2474824677",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W1501139663",
    "https://openalex.org/W2123261808",
    "https://openalex.org/W1528022942"
  ],
  "abstract": null,
  "full_text": "Kitaokaetal. EURASIPJournalonAudio,Speech,andMusic\nProcessing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13636-020-00193-1\nRESEARCH OpenAccess\nDynamicout-of-vocabularyword\nregistrationtolanguagemodelforspeech\nrecognition\nNorihideKitaoka1* ,BohanChen 2 andYuyaObashi 3\nAbstract\nWeproposeamethodofdynamicallyregisteringout-of-vocabulary(OOV)wordsbyassigningthepronunciationsof\nthesewordstopre-insertedOOVtokens,editingthepronunciationsofthetokens.Todothis,weaddOOVtokensto\nanadditional,partialcopyofourcorpus,eitherrandomlyortopart-of-speech(POS)tagsintheselectedutterances,\nwhentrainingthelanguagemodel(LM)forspeechrecognition.ThisresultsinanLMcontainingOOVtokens,towhich\nwecanassignpronunciations.Wealsoinvestigatetheimpactofacousticcomplexityandthe“natural”occurrence\nfrequencyofOOVwordsontherecognitionofregisteredOOVwords.TheproposedOOVwordregistrationmethodis\nevaluatedusingtwomodernautomaticspeechrecognition(ASR)systems,JuliusandKaldi,usingDNN-HMMacoustic\nmodelsandN-gramlanguagemodels(plusanadditionalevaluationusingRNNre-scoringwithKaldi).Our\nexperimentalresultsshowthatwhenusingtheproposedOOVregistrationmethod,modernASRsystemscan\nrecognizeOOVwordswithoutre-trainingthelanguagemodel,thattheacousticcomplexityofOOVwordsaffects\nOOVrecognition,andthatdifferencesbetweenthe“natural”andtheassignedoccurrencefrequenciesofOOVwords\nhavelittleimpactonthefinalrecognitionresults.\nKeywords: Speechrecognition,Out-of-vocabularywords,OOVregistration,Languagemodel\n1 Introduction\nThe language models (LMs) of automatic speech recog-\nnition (ASR) systems are often trained statistically using\ncorpora with fixed vocabularies. Thus, when applying\nASRs (e.g., in dialog systems), we always encounter out-\nof-vocabulary (OOV) words such as the names of new\nmovie stars or new Internet slang, such as “jsyk” (just\nso you know). This can be problematic since such OOV\nwordsareoftencloselyrelatedtothemaintopicbeingdis-\ncussed. Accurate recognition of these OOV words would\nundoubtedlyresultinahugeimprovementintheaccuracy\nofASR-basedsentenceunderstandingsystems.\nSubword acoustic models are often used to recog-\nnize OOV words. For example, phone models have been\n*Correspondence:kitaoka@tut.jp\n1ToyohashiUniversityofTechnology,1-1HibarigaokaTempaku-cho,\nToyohashi,Japan\nFulllistofauthorinformationisavailableattheendofthearticle\nused with a phone bigram to recognize arbitrary phone\nsequences [1]. Morphemes have also been used as sub-\nwords [2, 3]. This kind of “speech typewriter” cannot\nachievecomparativeOOVwordrecognitionperformance\nascanbeachievedforin-vocabularywords,however.\nAnotherapproachtotacklingtheOOVproblemisOOV\ndetection. In the 2000’s, various OOV detection methods\nwere proposed, most of them based on confidence mea-\nsures. Various acoustic and linguistic features were fed\ninto, for example, a Fisher Linear Discriminant Analysis\n(FLDA)-based classifier [4] to classify speech regions into\nOOVs and IVs (in-vocabulary words). Decoding graph\ninformation and semantic information were also used to\nclassify OOVs and IVs within a boosting classification\nalgorithm [5]. Word/subword hybrid systems were also\nused for OOV word detection [6]. In [7], the OOV word\ndetectiontaskwastreatedasasequencelabelingproblem.\nAMaxEntclassifierusedadditionalfeatures,basedonthe\n©TheAuthor(s).2021 OpenAccess ThisarticleislicensedunderaCreativeCommonsAttribution4.0InternationalLicense,which\npermitsuse,sharing,adaptation,distributionandreproductioninanymediumorformat,aslongasyougiveappropriatecredit\ntotheoriginalauthor(s)andthesource,providealinktotheCreativeCommonslicence,andindicateifchangesweremade. The\nimagesorotherthirdpartymaterialinthisarticleareincludedinthearticle’sCreativeCommonslicence,unlessindicated\notherwiseinacreditlinetothematerial. Ifmaterialisnotincludedinthearticle’sCreativeCommonslicenceandyourintended\nuseisnotpermittedbystatutoryregulationorexceedsthepermitteduse,youwillneedtoobtainpermissiondirectlyfromthe\ncopyrightholder. Toviewacopyofthislicence,visit http://creativecommons.org/licenses/by/4.0/.\nKitaokaetal. EURASIPJournalonAudio,Speech,andMusicProcessing          (2021) 2021:4 Page2of8\nlocal lexical context, as well as global features from a lan-\nguage model using the entire utterance. In another study,\nOOVs were detected by searching a confusion network\n[8]. After detection, recovery of the OOV word is gener-\nally performed [9, 10]. These approaches can be used to\nrecognize OOV words, but recovery often fails due to a\nlack of lexical information. Thus, when we want to recog-\nnize particular, known words which are unknown to both\nthe language model and the dictionary used in the speech\nrecognizer, it is better to register the words to the model\nandthedictionary.\nIf the system can reference OOV words, it is better to\nuse an LM which includes these words. One possible way\nto create an LM which includes OOV words is to use\ntraining data which includes these words. Such training\ndata is often created by gathering text from the web or\nby replacing words similar to the OOV words in the data\nwith the targeted OOV words. However, as LMs become\nmore and more complex, training them takes more and\nmore time. Therefore, the ability to dynamically add new\nwords to LMs without re-training is considered to be\na necessary feature of modern ASR systems. The most\ncommon approach used to dynamically update LMs is to\nassign probabilities to OOV words. This is achieved by\nmodifying the LM’s probability distribution. These meth-\nods attempt to assign the parameters of OOV words to\nthe existing LM along with their meanings and part-of-\nspeech (POS) tags. For example, in [11, 12], researchers\nproposed using similar, in-vocabulary (IV) words to esti-\nmate the N-gram probabilities of OOV words. Recently,\nword embedding has been adopted to measure the simi-\nlaritybetweenIVandOOVwords[ 13].Someresearchers\nhave tackled this problem using word class N-grams [14].\nIn [15], a probability was assigned to OOV words based\nonawordclasswhoseprobabilityhadalreadybeendeter-\nmined. These approaches have achieved a modest level\nof performance, but the system must have some kind of\nsemanticknowledgetomeasurethesimilaritiesanddeter-\nmine which class the OOV words should be assigned to.\nWhen using a class N-gram, words in each class share\na class probability, while the probability of each word\nis approximated by combining the class probability and\nthe target word’s probability within the class; thus, word\nprobability estimation is degraded, resulting in the per-\nformance of the speech recognition system also being\ndegraded. Moreover, our proposed method can be used\nin speech recognition systems which do not support class\nN-grams.\nAlternatively, we propose a simple but powerful corpus\nmodification method in which we artificially add OOV\ntokens during the training of the language model, creat-\ning an LM with OOV tokens chosen a priori. The system\nthen registers OOV words to the positions of the OOV\ntokens in the LM, resulting in an LM which includes the\nOOV words. For example, to train an LM, we create a\ntrainingcorpusbyinsertingorreplacingsomewordswith\nOOV tokens. After training the LM using this corpus, we\nobtain an LM containing OOV tokens. To apply this LM\ntospeechrecognition,wereplaceOOVtokenswithwords\nwhich are not contained in the pronunciation dictionary\n(i.e., OOV words). When using our method, degradation\nof the estimation of in-vocabulary word probabilities is\nsuppressed to the minimum, because the probability of\neach OOV and IV word is estimated. As part of this\nstudy, we investigated how OOV tokens could be added\nto the training data. Since ASR systems create recog-\nnition hypotheses based on the output probabilities of\nboth their acoustic models (AMs) and their LMs, the\nimpactoftheacousticcomplexityofOOVwordsonOOV\nword recognition is unclear, although one might assume\nthat OOV words with “unique” pronunciations are eas-\nier to recognize. In this paper, we define the acoustic\ncomplexity of a word using the number of moras in the\nw o r d .T h a ti s,t h eg r e a t e rt h en u m b e ro fm o r a s,t h em o r e\ncomplex the word is. Finally, since OOV words should\nhave a “natural” occurrence frequency (i.e., the frequency\nof their occurrence in a virtual, universal corpus), the\nimpact of the difference between an OOV word’s “nat-\nural” probability and its assigned probability for OOV\nword recognition also needs to be investigated. The\npresentstudyaimstosupplyglobalanswerstoallofthese\nquestions by experimentally investigating recognition of\nsimulated OOV words using two popular modern ASR\nsystems.\n2 DynamicOOVregistration\n2.1 Method\nIn our proposed method, we first train a language model\nusing a corpus which includes OOV tokens. These OOV\ntokens are inserted into the corpus artificially, and thus,\neach OOV token appears as both a context word and\na target word in the LM. Then, when using the LM in\nthe recognition phase, the target OOV word pronuncia-\ntionswhichtheuserwantstherecognizertorecognizeare\nassigned to the OOV tokens. This is realized by editing\nthe pronunciations of the OOV tokens. Immediately after\ntraining the LM, the OOV tokens do not have any asso-\nciate pronunciations. To assign a particular OOV token\nto a particular target to be recognized, we edit the pro-\nnunciation dictionary to link the OOV token to the pro-\nnunciationofthetargetword.Themeritofourmethodis\nits ability to control the frequency of OOV occurrences.\nThe more OOV tokens we insert, the larger the probabil-\nities that can be assigned to the OOV words. In addition,\nOOVwordscanbeaddedtotheLMwithouttheneedfor\nre-training.\nThe procedure used by the proposed method is as fol-\nlows:\nKitaokaetal. EURASIPJournalonAudio,Speech,andMusicProcessing          (2021) 2021:4 Page3of8\n1 Makeapartialcopy ofthecorpusm,tobeused for\nOOVtokeninsertion.This procedureis optional,\nbecauseOOVtokenscanalsobeinsertedinto\noriginalcorpus,butinour experiment,wefirstmade\nacopyofsomeofthesentencesandinserttheOOV\ntokensintothecopiedsentences,asdescribeinthe\nnextstep.\n2 AddOOVtokens(e.g.,“OOV1,”“OOV2,” ... ,\n“OOVN”)totheutterancesinthecopiedcorpus,in\nordertogenerate“additionalutterances.”\n3 Usethecorpusandtheadditionalutterancestotrain\nthelanguagemodel.Asaresultofthisprocedure,we\nobtainanLM whichincludesOOVtokens.\n4 Editthepronunciationsofthetokensforthe\nparticularOOVwordswewanttoregister.\nGenerally,apronunciationdictionaryiscreatedfor\nall ofthein-vocabularywordsoftheLM,but\npronunciationsarenotassignedtoanyoftheOOV\ntokens.Inour method,weeditthedictionaryto\nassignthepronunciationswewouldlike torecognize\ntotheassociatedtokens.\n5 Performspeech recognition.\nAn illustration of the editing of the pronunciation dic-\ntionary is shown in Fig.1. One of the key points to be\ndetermined when using our method is where the OOV\ntokens should be inserted. Thus, we compared the effec-\ntivenessofusingthefollowingtwomethodsinstep2:\n• Randominsertion—onetokenisaddedtoeach\n“additionalutterance”atarandomlocation\n• POStag-basedreplacement—onewordisreplacedin\neach“additionalutterance”whosePOStagisthe\nsameasthetoken’sPOStag\nThe proposed method can dynamically register new\nOOV words by editing the pronunciations of the OOV\ntokens. Different types of OOV tokens for OOV words\nwith different properties (properties which are assumed\nto be known, e.g., acoustic complexity and POS tag) can\nalso be prepared. For example, under the random inser-\ntion condition only, the pronunciation of the OOV words\nwasassumedtobeknown,whileunderthePOStag-based\nreplacement condition, we assumed that we knew each\nOOVword’sPOSandpronunciation.\n2.2 Impactofwordproperties\nTheimpactofusingeachofthesetwotypesofwordprop-\nerties (acoustic complexity and POS tag) on the proposed\nOOVregistrationmethodisalsoinvestigatedintheexper-\niment phase of this study. To investigate the impact of\nthe acoustic complexity of the OOV words, we defined\nfour levels of acoustic complexity, which were measured\naccording to the number of moras the OOV words con-\ntained, which could range from 2 to 5 moras. To investi-\ngatetheimpactofthedifferencebetweenthe“natural”and\nassigned probabilities of the OOV words, we evaluated\nfour different insertion scales (i.e., the number of utter-\nances generated for each OOV token), which were either\n500, 1000, 2000, or 5000 utterances. During the experi-\nmental phase, we inserted four different OOV tokens at\neach level of acoustic complexity to generate additional\nutterancesforeachinsertionscaleconditionandeachreg-\nistrationcondition(i.e.,randomandPOStagbased),tobe\nused to register the words in Table1. All of these addi-\ntional utterances were then used to train the LM, along\nwiththebasiccorpus,whichmeansthatweadded (500+\n1000+2000+5000)×2×4 = 68,000additionalutterance\ntoourcorpus.Asthenumberofutterancesintheoriginal\nFig.1 AnexampleofOOVwordregistration\nKitaokaetal. EURASIPJournalonAudio,Speech,andMusicProcessing          (2021) 2021:4 Page4of8\nTable1 Selectedout-of-vocabulary(OOV)wordsandtheir\nfrequencyofoccurrence\ncorpus was about 440,000, the additional utterances were\nequaltoonesixthoftheoriginalcorpus.\nWe also set two baseline conditions in our experiment:\nONE and ALL. Under the ONE condition, the LM is\ntrained using the basic corpus, which includes only pro-\nnunciation information and the smoothing probability of\nthe OOV words given by the LM toolkit. Under this con-\ndition, only very rare words lacking sufficient statistical\nconfidence are removed and replaced with OOV tokens;\nthus, only a small number of OOV tokens remain in the\ncorpus. In our experiment, words with an occurrence fre-\nquency of 1 were removed. As a result, the probabilities\nof the N-gram model corresponding to the OOV words\nbecomes small, but Kneser-Ney smoothing gives them\nsomelevelofprobability.UndertheALLcondition,which\nis considered to be the ideal condition, the LM is trained\nusingtheoriginalcorpuswhichincludesalloftheselected\nOOVwords.\n3 Experimentalsetup\n3.1 Data\nThe Corpus of Spontaneous Japanese (CSJ) [ 16]w a s\nselected to be the original corpus used in our experi-\nment. It contains speech signals from speeches delivered\nin Japanese at domestic conferences. The corpus includes\ntranscriptions of about 7 million words, along with vari-\nous annotations such as POS and phonetic labels. In our\nexperiment, we used the entire training set of the CSJ\nas our original corpus, which has a total length of 240 h\nand consists of 986 speeches. It contains about 440,000\nutterances,withavocabularyofaround70,000words.For\nrecognition, CSJ evaluation Set 1 (one of three available\nCSJ evaluation sets) was used, which contains 10 differ-\nent lectures with a total of 1272 utterances (about 26,000\nwords). The total length of the test set was approximately\n2h.\nWe randomly selected 16 words as our OOV words,\nbased on their acoustic complexity and occurrence fre-\nquency in the evaluation set, all of which were nouns1.\nTheselectedwordsandtheiroccurrencefrequencyinthe\ncorpora are shown in Table12. Four words were selected\nfor each level of acoustic complexity, and the average\nrecognition/detection accuracy of these four words was\nused to evaluate the performance of the proposed OOV\nregistration method with OOV words of that level of\nacoustic complexity. Note that in each recognition trial,\nwe attempted to register and recognize each of the four\nOOV words at each level of acoustic complexity (all 16\nwords), at that particular insertion scale. Because we\nfound it challenging to find enough words with 2 moras,\nwe decided to use two words with multiple pronuncia-\ntions,someofwhichcontainadifferentnumberofmoras.\nBoth the word (“six”) and the word (“sound”)\nhave multiple pronunciations. can be pronounced\n“roku,”“mui,”“muq,”“ro,”“riku,”or“muyu.”Exceptfor“ro,”\nall of these pronunciations have 2 moras. Likewise,\nc a nb ep r o n o u n c e d“ o t o , ”“ i N , ”“ o N , ”o r“ n e . ”E x c e p tf o r\n“ne,” all of these pronunciations have 2 moras. The utter-\nances in the evaluation set, including OOV words, can be\ndivided according to their level of acoustic complexity as\nf o l l o ws:5 8a r e2m o r aw o r ds,6 9a r e3m o r aw o r ds,4 9a r e\n4morawords,and41are5morawords 3.\n3.2 Automaticspeechrecognitionsystems\nWe used the Julius [17]a n dK a l d i[18]A S Rs y s t e m s\nin our experiments. The Julius toolkit provides a pre-\ntrained, deep neural network (DNN) hidden Markov\nmodel (HMM) for acoustic modeling (AM), which uses\na corpus of Japanese newspaper article sentences (JNAS)\nand part of the CSJ speech corpus (simulated speech) as\ntraining data. For its language model, we used a forward\nbi-gramplusbackwardtri-gramLM,trainedusingSRILM\n[19]withentries(uni-gram/bi-gram/tri-gram)thatappear\nmore than one time, as well as modified Kneser-Ney\nsmoothing for back-off. Since Julius is a classical decoder\nwhichdirectlyusesacousticmodelsandN-gramlanguage\nmodels, we can register new words only by changing the\npronunciationsoftheOOVtokens.\n1For example,thePOStagoftheword (“six”)is“noun/numeral.”\n2The Englishtranslationsof theselectedOOVwords are -six, -rate,\n-vocabulary, -sound, -standard, -set, -conversation,\n-rule, -research, -analysis, -straightline, -translation,\n-approach, -message, -timing,and\n-category.\n3Somesentencescontained OOVwords withmorethanonelevelof acoustic\ncomplexity(e.g., one 2moraOOV word andone 3moraOOV word). But\neach utteranceincludedatmostonlyoneof thefourOOV words ofa\nparticularlevelofacousticcomplexity.\nKitaokaetal. EURASIPJournalonAudio,Speech,andMusicProcessing          (2021) 2021:4 Page5of8\nTheKalditoolkitprovidesseveralexampletraining/test\npipelines for different corpora, including the CSJ corpus\nwhich was used in our experiment. In particular, lin-\neardiscriminant analysis (LDA) and maximum likelihood\nlinear transform (MLLT) were used to process the origi-\nnalMel-frequencycepstrumcoefficient(MFCC)features.\nThese processed MFCC features were then used as the\ninput(basicfeatures)forDNN-HMMtraining.Aforward\ntri-gram LM with similar training conditions as those\nused to train the LM for the Julius ASR was also used\nas one of the Kaldi LMs. In this case, we had to com-\npose Finite State Transducers (FSTs) a priori. Thus, in the\ntrainingphase,wemadealexiconfile(called“L.fst”inthe\nKaldi toolkit) and a grammar file (“G.fst”) first, using the\ntraining data, which included the OOV tokens with their\ntentativepronunciations.Next,weconstructedanewL.fst\nfile using the lexicon in which particular OOV word pro-\nnunciations were registered to the OOV tokens. We then\ncomposed it with the other FSTs to create the final FST\n(“HCLG. fst”). When using a WFST-based ASR system\nsuch as Kaldi, it is not as easy to register OOV words to\nthe lexicon as it is with classical decoders. On the other\nhand,wedonotneedtoretrainthelanguagemodelusing\na huge text corpus that includes particular OOV words.\nA recurrent neural network (RNN)-based LM was also\nintroduced as an alternative LM for Kaldi, specifically,\nthe N-gram plus RNN re-scoring LM proposed in [20].\nT h eR N NL Mw a su s e dt oc a l c u l a t et h es c o r e so ft h eN -\nbest hypotheses provided by the N-gram LM. We trained\nthe RNN language model using a text corpus with OOV\ntokens. In the testing phase, we viewed the OOV tokens\nas particular OOV words; thus, we could re-score the\nhypothesis including the particular OOV words using the\nRNN language model. The output scores of the RNN LM\nwere then linearly interpolated with the scores provided\nbytheoriginalN-gramLMusingthefollowequation:\nP(hi) = λPN(hi) + (1− λ)PR(hi),( 1 )\nwhereP(hi) isthefinalLMscoreofhypothesis i,PN(hi) is\ntheprobabilitycalculatedbytheN-gramLM, PR(hi) isthe\nprobability calculated by the RNN LM, andλ is the inter-\npolation weight which was set to 0.5 in our experiment4.\nOtherdetailsoftheRNNLMusedinthisstudyareshown\ninTable 2\nAs mentioned above, we added a relatively large num-\nber of additional utterances containing our OOV words\nto the basic corpus, so the speech recognition word error\nrates (WERs) of the Julius and Kaldi N-gram LMs used in\nour experiment were both a bit higher (i.e., worse) than\nthereportedbestWERsfortheseASRs,whichare29.20%\nand 17.07%, respectively. RNN LM re-scoring improved\nKaldi’sWERto14.23%.\n4Wealsotriedsetting λ to0.25 and0.75,buttherewasalmostnochange in\ntherecognition results.\nTable2 ExperimentalconditionsfortheRNNlanguagemodel\nParameter Value\nNumberofhiddenunits 500\nNumberofclasses 200\nN-bestforre-scoring 10\nRNNvocabularysize 10,000\n3.3 Evaluationmethod\nRecognition accuracy in our experiments was measured\nusing utterance level false rejection rates (FRR) and false\nalarmrates(FAR)whentheASRsinterpretedOOVutter-\nancesduringtheexperiment.IfOOVwordsweresuccess-\nfullyrecognized,theutterancecontainingthisOOVword\nwas counted as one “hit” (true positive, TP); if the ASR\nsystem did not recognize the OOV word, it was counted\nas one “miss” (false negative, FN); if the ASR system did\nnot recognized any OOV words in an utterance without\nan OOV word, it was counted as one “correct dismissal”\n(true negative, TN); and finally, if the ASR system recog-\nnized any OOV words in an utterance without an OOV\nword, it was counted as one “false alarm” (false positive,\nFP).Thefalserejectionratewasthencalculatedasfollows:\nFRR = FN\nTP+ FN,( 2 )\nandthefalsealarmratewascalculatedasfollows:\nFAR= FP\nTN+ FP.( 3 )\nNote that since the (TP + FN) totals (which added up\nto about 50 at each level of acoustic complexity) were\nmuch smaller than the (TN + FP) totals (which added\nup to about 1200 at each level), the false rejection rates\nwere much higher than the false alarm rates in our\nexperiment. Also note that although speech recognition\nperformance is often evaluated using word error rates\n(WERs),theWERsachievedduringourexperimentswere\nheavily dependent on the frequency of OOV words in a\nparticular speech sample. However, the frequency of par-\nticular OOV words was not very high; thus, the effects\non the WERs were small. Just for reference, the WERs\nwere approximately 29%, 17%, and 14% for the Julius-\nbased, Kaldi-based, and Kaldi+RNN-LM-based systems,\nrespectively.\n4 Results\nDetection FRRs and FARs for the Julius-based ASR sys-\ntem are shown in Tables3 and 4, respectively, while the\ndetectionFRRsandFARsfortheKaldi-basedASRsystem\nusingtheN-gramLMareshowninTables 5and6,respec-\ntively. The detection FRRs and FARs for the Kaldi ASR\nsystem using the N-gram plus RNN re-scoring LM are\ns h o w ni nT a b l e s7 and 8, respectively. In these tables, “r”\nKitaokaetal. EURASIPJournalonAudio,Speech,andMusicProcessing          (2021) 2021:4 Page6of8\nTable3 FalserejectionratesforOOVwordswhenusing\nJulius-basedASR\nPosition Freq. 2mora 3mora 4mora 5mora\nONE – – .964 .106 .143 .044\nALL – – .506 .128 .071 .022\n500r Random 500 .602 .138 .089 .022\n1kr Random 1000 .566 .128 .071 .022\n2kr Random 2000 .422 .117 .071 .022\n5kr Random 5000 .349 .096 .089 .022\n500c POS 500 .590 .128 .071 .022\n1kc POS 1000 .458 .096 .054 .022\n2kc POS 2000 .349 .117 .054 .044\n5kc POS 5000 .313 .085 .071 0\nrepresents the random insertion condition, and “c” repre-\nsentsthePOS-basedreplacementcondition.Forexample,\nunder the condition “500r-2 mora,” the pronunciation of\neach of the selected 2 mora OOV words was registered as\nonepre-trainedOOVtoken,whichwasrandomlyinserted\ninto500existingutterances.\nIn general, FRRs were much lower (better) when using\nthe proposed method than under the ONE condition,\nbut higher (worse) than under the ALL condition5,a n d\nthe FARs when using the proposed method were higher\n(worse) than under both the ONE and ALL conditions.\nUnder some experimental conditions, such as the 4-mora\ncondition using Julius, the recognition performances of\nthe ideal baseline (ALL) and the proposed approaches\nwere similar. Compared to Kaldi, Julius achieved bet-\nter OOV word detection accuracy in our experiments.\nAdditionally, since POS tag-based replacement provided\nmore information about the OOV words than random\ninsertion, FRRs were lower (better) when using the POS\ntag-based replacement method than when using the ran-\ndom insertion method, while the FARs were similar. The\nN-gram plus RNN re-scoring LM achieved slightly better\nrecognition results than the N-gram LM. Note that the\neffects of OOV words may extended forward and back-\nward in the sentences. Most of the false alarms were the\nresult of substitutions of in-vocabulary words (IVs) with\nOOVwords,whichaffectedthecontextwords.According\nto our results, FARs were suppressed to low values, and\nthus, the effects of these false alarms were limited. As for\nFRRs,mostofthefalserejectionweretheresultofsubsti-\ntutions of OOVs with IVs. When not using our method,\nOOVs are always replaced with IVs, and these replace-\nments negatively affect context word recognition. Thus,\nthe negative effect of these false rejections on the con-\ntextwordswhenusingourmethodisnoworsethanwhen\n5Baselinemethods,described inSection 2.2\nTable4 FalsealarmratesforOOVwordswhenusing\nJulius-basedASR\nPosition Freq. 2mora 3mora 4mora 5mora\nONE – – .001 .004 0 0\nALL – – .004 .002 .001 0\n500r Random 500 .010 .003 0 0\n1kr Random 1000 .014 .003 .001 0\n2kr Random 2000 .026 .004 .001 0\n5kr Random 5000 .045 .006 .002 0\n500c POS 500 .015 .003 0 0\n1kc POS 1000 .019 .002 .002 0\n2kc POS 2000 .030 .003 .002 0\n5kc POS 5000 .049 .009 .003 .001\nnot using our method. Our proposed method achieved\nvery good OOV detection performance according to the\nFRRs shown in Tables3, 5,a n d7. Generally, the OOV’s\nareimportantwords(propernouns,forexample)inmany\nspeech recognition tasks, so even though our method\ninvolves a small degradation in overall speech recogni-\ntion performance, it is clearly worth applying in order to\nrecognize key OOV words.\nTheseresults alsoshowthat acoustic complexity affects\nthe accuracy of OOV word detection. When an OOV\nword has relatively low acoustic complexity, i.e., when\nthe audio signal contains less information, increasing the\nnumberofadditionalutterancescansignificantlyimprove\ndetection accuracy. When the acoustic complexity of the\nOOV word is sufficiently high (more than 3 mora in our\nexperiment), a small number of additional utterances, or\nsometimesevenjustthesmoothingprobability,canresult\nin acceptable performance. These results suggest that we\nshould prepare a large number of additional utterances\nTable5 FalserejectionratesforOOVwordswhenusing\nKaldi-basedASRwithN-gramLM\nPosition Freq. 2mora 3mora 4mora 5mora\nONE – – 1 .977 .643 .5\nALL – – .313 .174 .089 .022\n500r Random 500 .783 .384 .232 .109\n1kr Random 1000 .687 .314 .268 .087\n2kr Random 2000 .566 .233 .268 .087\n5kr Random 5000 .537 .233 .25 .087\n500c POS 500 .747 .314 .25 .065\n1kc POS 1000 .614 .291 .232 .022\n2kc POS 2000 .506 .233 .214 .043\n5kc POS 5000 .390 .233 .214 .065\nKitaokaetal. EURASIPJournalonAudio,Speech,andMusicProcessing          (2021) 2021:4 Page7of8\nTable6 FalsealarmratesforOOVwordswhenusingKaldi-based\nASRwithN-gramLM\nPosition Freq. 2mora 3mora 4mora 5mora\nONE – – 0 0 0 0\nALL – – .011 .008 0 0\n500r Random 500 .005 .012 .002 0\n1kr Random 1000 .019 .011 .004 .001\n2kr Random 2000 .019 .008 .004 .002\n5kr Random 5000 .032 .011 .003 .004\n500c POS 500 .009 .008 .002 .001\n1kc POS 1000 .009 .008 .002 .001\n2kc POS 2000 .015 .008 .002 .001\n5kc POS 5000 .038 .013 .002 .002\nfor OOV words with lower levels of acoustic complexity,\nsince LMs require more information to recognize OOV\nwordswithsimplepronunciations,whileOOVwordswith\nsufficiently high acoustic complexity can be registered by\nASR systems using only their pronunciation information\n(and back-off probabilities). As for the scale of additional\nutterances required, the FRRs and FARs of OOV words\nwith different “natural” occurrence frequencies (2-mora\nOOVs> 4-moraOOVs ≥ 3-moraOOVs > 5-moraOOVs)\nshowed similar recognition tendencies when the inser-\ntion rates were increased. Our results suggest that the\ndifference between an OOV word’s “natural” occurrence\nfrequency and its assigned frequency has little impact on\nthefinaldetectionresults.\n5C o n c l u s i o n\nIn this study, we proposed several corpus modification\nmethods for dynamic OOV word registration which do\nnot require language model re-training. The proposed\nTable7 FalserejectionratesforOOVwordswhenusing\nKaldi-baseASRwithN-gramplusRNNre-scoringLM\nPosition Freq. 2mora 3mora 4mora 5mora\nONE – – 1 .988 .696 .5\nALL – – .253 .151 .071 .022\n500r Random 500 .747 .395 .214 .109\n1kr Random 1000 .639 .326 .232 .152\n2kr Random 2000 .602 .302 .214 .130\n5kr Random 5000 .482 .267 .214 .130\n500c POS 500 .687 .244 .232 .044\n1kc POS 1000 .518 .256 .196 .043\n2kc POS 2000 .410 .289 .214 .065\n5kc POS 5000 .373 .221 .179 .022\nTable8 FalsealarmratesforOOVwordswhenusingKaldi-based\nASRwithN-gramplusRNNre-scoringLM\nPosition Freq. 2mora 3mora 4mora 5mora\nONE – – 0 0 0 0\nALL – – .011 .007 .001 0\n500r Random 500 .010 .011 .001 0\n1kr Random 1000 .015 .010 .001 .003\n2kr Random 2000 .021 .008 .002 .003\n5kr Random 5000 .029 .013 .002 .005\n500c POS 500 .009 .007 .002 .001\n1kc POS 1000 .013 .008 .001 0\n2kc POS 2000 .019 .010 .002 .001\n5kc POS 5000 .034 .015 .001 .001\nmethods were tested under two training conditions, ran-\ndom insertion and replacement based on part-of-speech.\nWe also investigated the impact of acoustic complexity\non OOV word detection by manipulating the number of\nm o r a si nt h eO O Vw o r d s ,a sw e l la st h ei m p a c to ft h e\n“natural” occurrence frequencies of OOV words by using\ndifferent insertion rates. The proposed OOV word reg-\nistration method was evaluated using two modern ASR\nsystems which both utilize DNN-HMM acoustic mod-\nels and N-gram language models. We also conducted\nan additional evaluation with one of the systems, using\nRNN re-scoring. Our experimental results demonstrated\nthe effectiveness of the proposed OOV word registra-\ntion method and also showed that the difference between\nan OOV word’s “natural” occurrence frequency and its\nassigned occurrence frequency had little impact on final\nfalserejectionandfalsealarmrates.\nIn addition, our results revealed that the acoustic com-\nplexity of the OOV words had a clear impact on the\nperformance of the proposed OOV word registration\nmethod.ModernASRsystemscanrecognizeOOVwords\nofhighacousticcomplexitywithverylittlelanguageinfor-\nmation about them. On the other hand, ASRs need a\nsufficient amount of language information to success-\nfullydetectOOVwordsoflowacousticcomplexity.These\nresults are consistent with [21], which found that the\nacoustic information provided by acoustic models was\nmuch more informative than the linguistic information\nprovidedbythelanguagemodel,eventhoughtheacoustic\nmodelsusedin[ 21]wereGMM-HMM-based.\nOur proposed method was only tested using a Japanese\ncorpus,soitisstillnotclearwhetheritwillworkwellwith\ncorpora of other languages. However, statistical language\nmodels such as N-grams and RNN models work in vari-\nous languages as well as Japanese, so we believe that this\nm e t h o dc a na l s ob ea p p l i e dt oi m p r o v eO O Vd e t e c t i o n\nKitaokaetal. EURASIPJournalonAudio,Speech,andMusicProcessing          (2021) 2021:4 Page8of8\nin other languages. In future work, we will validate that\nour method can be used effectively in other languages,\ngenerally.Wewillalsoapplyourproposedmethodtorec-\nognizing utterances which contain OOV words which are\nnot in the training data, such as trendy slang, by obtain-\ning examples from social networking websites, etc., and\nregisteringthemtotherecognizer.\nAcknowledgements\nThisworkwassupportedinpartbyJSPSKAKENHIGrantNumbersJP19H01125\nand20H05562.\nAuthors’contributions\nNKproposedtheunderlyingidea,whileBCandYOimplementedthemethod\nandcarriedouttheexperiments.ThepaperwasmostlywrittenbyNKandBC.\nTheauthorsreadandapprovedthefinalmanuscript.\nCompetinginterests\nTheauthorsdeclarethattheyhavenocompetinginterests.\nAuthordetails\n1ToyohashiUniversityofTechnology,1-1HibarigaokaTempaku-cho,\nToyohashi,Japan. 2NagoyaUniversity,1Furo-choChikusa-ku,Nagoya,Japan.\n3TokushimaUniversity,2-1Minamijohsanjima-cho,Tokushima,Japan.\nReceived:24March2020 Accepted:18December2020\nReferences\n1. I.Bazzi,J.R.Glass,in ICSLP-2000.Modelingout-of-vocabularywordsfor\nrobustspeechrecognition(ISCA,2000),pp.401–404\n2. I.Bazzi,J.R.Glass,in ICSLP-2002.Amulti-classapproachformodelling\nout-of-vocabularywords(ISCA,2002),pp.1613–1616\n3. M.Creutz,T.Hirsimaki,M.Kurimo,A.Puurula,J.Pylkkonen,V.Siivola,M.\nVarjokallio,E.Arisoy,M.Saraclar,A.Stolcke,in NAACL-HLT2007.Analysisof\nmorph-basedspeechrecognitionandthemodelingofout-of-vocabulary\nwordsacrosslanguages(ACL,2007),pp.380–397\n4. H.Sun,G.Zhang,M.Xu,in EUROSPEECH2003.Usingwordconfidence\nmeasureforOOVwordsdetectioninaspontaneousspokendialog\nsystem(ISCA,2003),pp.2713–2716\n5. B.Lecouteux,G.Linares,B.Favre,in EUROSPEECH2003.Usingword\nconfidencemeasureforOOVwordsdetectioninaspontaneousspoken\ndialogsystem(ISCA,2003),pp.2713–2716\n6. A.Rastrow,A.Sethy,B.Ramabhadran,in ICASSP2009.Anewmethodfor\nOOVdetectionusinghybridword/fragmentsystem(IEEE,2009),\npp.3953–3956\n7. C.Parada,M.Dredze,D.Filimonov,F.Jelinek,in NAACL2010.Contextual\ninformationimprovesOOVdetectioninspeech(ACL,2010),pp.216–224\n8. A.Martin,T.Kwiatkowski,M.Ostendorf,L.Zettlemoyer,in IEEESpoken\nLanguageTechnologyWorkshop2012 .Usingsyntacticandconfusion\nnetworkstructureforout-of-vocabularyworddetection(IEEE,2012),\npp.159–164\n9. S.Thomas,K.Audhkhasi,Z.Tuske,Y.Huang,M.Picheny,in\nINTERSPEECH2019.DetectionandrecoveryofOOVsforimprovedEnglish\nbroadcastnewscaptioning(ISCA,2019),pp.2973–2977\n10. N.Sawada,H.Nishizaki,in The5thJointMeetingofASA/ASJ,Journalof\nAcousticalSocietyofAmerica .Correctphonemesequenceestimation\nusingrecurrentneuralnetworkforspokentermdetection,vol.140\n(AcousticalSocietyofAmerica,2016),p.3061\n11. S.Yamahata,Y.Yamaguchi,A.Ogawa,H.Masataki,O.Yoshioka,S.\nTakahashi,Automaticvocabularyadaptationbasedonsemanticand\nacousticsimilarities.IEICETrans.Inf.Syst. E97-D,1488–1496(2014)\n12. W.Naptali,M.Tsuchiya,S.Nakagawa,Class-basedn-gramlanguage\nmodelfornewwordsusingout-of-vocabularytoin-vocabularysimilarity.\nIEICETrans.Inf.Syst. E95-D,2308–2317(2012)\n13. A.Currey,I.Illina,D.Fohr,in IEEESpokenLanguageTechnologyWorkshop\n(SLT).Dynamicadjustmentoflanguagemodelsforautomaticspeech\nrecognitionusingwordsimilarity(IEEE,2016),pp.426–432\n14. S.Martin,J.Liermann,H.Ney,Algorithmforbigramandtrigramword\nclustering.SpeechComm. 24,19–37(2005)\n15. A.Allauzen,J.-L.Gauvain,in Acoustics,Speech,andSignalProcessing,2005.\nProceedings.(ICASSP’05).OpenvocabularyASRforaudiovisualdocument\nindexation(IEEE,2005),pp.1013–1016\n16. K.Maekawa,in ISCAandIEEEWorkshoponSpontaneousSpeechProcessing\nandRecognition.CorpusofSpontaneousJapanese:itsdesignand\nevaluation(ISCAandIEEE,2003),pp.7–12\n17. A.Lee,T.Kawahara,in APSIPAASC2009:Asia-PacificSignalandInformation\nProcessingAssociation,2009AnnualSummitandConference .Recent\ndevelopmentofopen-sourcespeechrecognitionengineJulius(APSIPA,\n2009),pp.131–137\n18. D.P.A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,N.Goel,M.\nHannemann,P.Motlicek,Y.Qian,P.Schwarz,J.Silovsky,G.Stemmer,K.\nVesely,in IEEE2011WorkshoponAutomaticSpeechRecognitionand\nUnderstanding.TheKaldispeechrecognitiontoolkit(IEEE,2011)\n19. A.Stolcke,in SeventhInternationalConferenceonSpokenLanguage\nProcessing.SRILM-anextensiblelanguagemodelingtoolkit(ISCA,2002),\npp.901–904\n20. T.Mikolov,S.Kombrink,A.Deoras,L.Burge,J.Cernocky,in IEEEWorkshop\nonAutomaticSpeechRecognitionandUnderstanding(ASRU) .RNNLM-\nrecurrentneuralnetworklanguagemodelingtoolkit(IEEE,2011)\n21. N.Kitaoka,D.Enami,S.Nakagawa,Effectofacousticandlinguistic\ncontextsonhumanandmachinespeechrecognition.Comput.Speech\nLang.28,769–787(2014)\nPublisher’sNote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsin\npublishedmapsandinstitutionalaffiliations.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.895872950553894
    },
    {
      "name": "Speech recognition",
      "score": 0.7863816618919373
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5842206478118896
    },
    {
      "name": "Vocabulary",
      "score": 0.5806918144226074
    },
    {
      "name": "Natural language processing",
      "score": 0.5709395408630371
    },
    {
      "name": "Word (group theory)",
      "score": 0.5707070827484131
    },
    {
      "name": "Language model",
      "score": 0.5651893019676208
    },
    {
      "name": "Hidden Markov model",
      "score": 0.5077717900276184
    },
    {
      "name": "Natural language",
      "score": 0.41759246587753296
    },
    {
      "name": "Linguistics",
      "score": 0.07700484991073608
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I136259955",
      "name": "Toyohashi University of Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I60134161",
      "name": "Nagoya University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I922474255",
      "name": "Tokushima University",
      "country": "JP"
    }
  ]
}