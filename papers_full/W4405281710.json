{
  "title": "Performance Evaluation of Large Language Models in Cervical Cancer Management Based on a Standardized Questionnaire: Comparative Study",
  "url": "https://openalex.org/W4405281710",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5099841105",
      "name": "Warisijiang Kuerbanjiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5102699138",
      "name": "Shengzhe Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5099841106",
      "name": "Yiershatijiang Jiamaliding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2155938389",
      "name": "Yuexiong Yi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3128646645",
    "https://openalex.org/W4310601632",
    "https://openalex.org/W4315754639",
    "https://openalex.org/W4224300925",
    "https://openalex.org/W4324370640",
    "https://openalex.org/W4379140845",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4390617165",
    "https://openalex.org/W4391069701",
    "https://openalex.org/W4366824288",
    "https://openalex.org/W4366823098",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4388840552",
    "https://openalex.org/W1963976781",
    "https://openalex.org/W4388584897",
    "https://openalex.org/W3015112546",
    "https://openalex.org/W4402095805",
    "https://openalex.org/W4385474373",
    "https://openalex.org/W4389165073",
    "https://openalex.org/W3082965341",
    "https://openalex.org/W3187467055",
    "https://openalex.org/W4387799763",
    "https://openalex.org/W4389217180",
    "https://openalex.org/W4389210014",
    "https://openalex.org/W4390166236",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W4394579747",
    "https://openalex.org/W4387929411",
    "https://openalex.org/W4386566508",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3199643281",
    "https://openalex.org/W4391444080",
    "https://openalex.org/W2739349903",
    "https://openalex.org/W4400392484",
    "https://openalex.org/W4400396863",
    "https://openalex.org/W4390921272",
    "https://openalex.org/W4387230383",
    "https://openalex.org/W4402769601"
  ],
  "abstract": "Background Cervical cancer remains the fourth leading cause of death among women globally, with a particularly severe burden in low-resource settings. A comprehensive approach—from screening to diagnosis and treatment—is essential for effective prevention and management. Large language models (LLMs) have emerged as potential tools to support health care, though their specific role in cervical cancer management remains underexplored. Objective This study aims to systematically evaluate the performance and interpretability of LLMs in cervical cancer management. Methods Models were selected from the AlpacaEval leaderboard version 2.0 and based on the capabilities of our computer. The questions inputted into the models cover aspects of general knowledge, screening, diagnosis, and treatment, according to guidelines. The prompt was developed using the Context, Objective, Style, Tone, Audience, and Response (CO-STAR) framework. Responses were evaluated for accuracy, guideline compliance, clarity, and practicality, graded as A, B, C, and D with corresponding scores of 3, 2, 1, and 0. The effective rate was calculated as the ratio of A and B responses to the total number of designed questions. Local Interpretable Model-Agnostic Explanations (LIME) was used to explain and enhance physicians’ trust in model outputs within the medical context. Results Nine models were included in this study, and a set of 100 standardized questions covering general information, screening, diagnosis, and treatment was designed based on international and national guidelines. Seven models (ChatGPT-4.0 Turbo, Claude 2, Gemini Pro, Mistral-7B-v0.2, Starling-LM-7B alpha, HuatuoGPT, and BioMedLM 2.7B) provided stable responses. Among all the models included, ChatGPT-4.0 Turbo ranked first with a mean score of 2.67 (95% CI 2.54-2.80; effective rate 94.00%) with a prompt and 2.52 (95% CI 2.37-2.67; effective rate 87.00%) without a prompt, outperforming the other 8 models (P&lt;.001). Regardless of prompts, QiZhenGPT consistently ranked among the lowest-performing models, with P&lt;.01 in comparisons against all models except BioMedLM. Interpretability analysis showed that prompts improved alignment with human annotations for proprietary models (median intersection over union 0.43), while medical-specialized models exhibited limited improvement. Conclusions Proprietary LLMs, particularly ChatGPT-4.0 Turbo and Claude 2, show promise in clinical decision-making involving logical analysis. The use of prompts can enhance the accuracy of some models in cervical cancer management to varying degrees. Medical-specialized models, such as HuatuoGPT and BioMedLM, did not perform as well as expected in this study. By contrast, proprietary models, particularly those augmented with prompts, demonstrated notable accuracy and interpretability in medical tasks, such as cervical cancer management. However, this study underscores the need for further research to explore the practical application of LLMs in medical practice.",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.8353396654129028
    },
    {
      "name": "Cervical cancer",
      "score": 0.6128863096237183
    },
    {
      "name": "Medicine",
      "score": 0.41783857345581055
    },
    {
      "name": "Computer science",
      "score": 0.39481765031814575
    },
    {
      "name": "Cancer",
      "score": 0.2942894697189331
    },
    {
      "name": "World Wide Web",
      "score": 0.21100932359695435
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210120234",
      "name": "Zhongnan Hospital of Wuhan University",
      "country": "CN"
    }
  ],
  "cited_by": 4
}