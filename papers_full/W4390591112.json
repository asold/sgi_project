{
    "title": "Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex)",
    "url": "https://openalex.org/W4390591112",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2600749606",
            "name": "Maryan Rizinski",
            "affiliations": [
                "Boston University"
            ]
        },
        {
            "id": "https://openalex.org/A3211379536",
            "name": "Hristijan Peshov",
            "affiliations": [
                "Saints Cyril and Methodius University of Skopje"
            ]
        },
        {
            "id": "https://openalex.org/A2599468099",
            "name": "Kostadin Mishev",
            "affiliations": [
                "Saints Cyril and Methodius University of Skopje"
            ]
        },
        {
            "id": "https://openalex.org/A2303259068",
            "name": "Milos Jovanovik",
            "affiliations": [
                "Saints Cyril and Methodius University of Skopje"
            ]
        },
        {
            "id": "https://openalex.org/A2065201609",
            "name": "Dimitar Trajanov",
            "affiliations": [
                "Boston University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3013586388",
        "https://openalex.org/W4205296446",
        "https://openalex.org/W2143010037",
        "https://openalex.org/W2768583960",
        "https://openalex.org/W2088535455",
        "https://openalex.org/W1809255060",
        "https://openalex.org/W2965613425",
        "https://openalex.org/W2794904450",
        "https://openalex.org/W2952342634",
        "https://openalex.org/W6604335243",
        "https://openalex.org/W2805540443",
        "https://openalex.org/W2046862913",
        "https://openalex.org/W2108907749",
        "https://openalex.org/W3081953587",
        "https://openalex.org/W2901065167",
        "https://openalex.org/W6737837687",
        "https://openalex.org/W2464559416",
        "https://openalex.org/W3129801732",
        "https://openalex.org/W4205432304",
        "https://openalex.org/W3098363382",
        "https://openalex.org/W2470931188",
        "https://openalex.org/W1797037330",
        "https://openalex.org/W2084046180",
        "https://openalex.org/W2935044269",
        "https://openalex.org/W4210827551",
        "https://openalex.org/W3125952890",
        "https://openalex.org/W2034090215",
        "https://openalex.org/W1977561632",
        "https://openalex.org/W6795374831",
        "https://openalex.org/W2025478229",
        "https://openalex.org/W2964236337",
        "https://openalex.org/W2993843842",
        "https://openalex.org/W3011570378",
        "https://openalex.org/W2146338426",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3043424630",
        "https://openalex.org/W6780086851",
        "https://openalex.org/W6737947904",
        "https://openalex.org/W2625464253",
        "https://openalex.org/W3122944446",
        "https://openalex.org/W3124532232",
        "https://openalex.org/W3126123456",
        "https://openalex.org/W2313281002",
        "https://openalex.org/W2768476704",
        "https://openalex.org/W2169397596",
        "https://openalex.org/W6635364467",
        "https://openalex.org/W2513699236",
        "https://openalex.org/W3045913839",
        "https://openalex.org/W3132055528",
        "https://openalex.org/W1650824498",
        "https://openalex.org/W6679346799",
        "https://openalex.org/W2086277751",
        "https://openalex.org/W2110620835",
        "https://openalex.org/W2964325543",
        "https://openalex.org/W2905466209",
        "https://openalex.org/W6781984573",
        "https://openalex.org/W6770187502",
        "https://openalex.org/W2969631555",
        "https://openalex.org/W4293732086",
        "https://openalex.org/W6794404659",
        "https://openalex.org/W4225527411",
        "https://openalex.org/W6779154463",
        "https://openalex.org/W3110383635",
        "https://openalex.org/W4210254834",
        "https://openalex.org/W4298110867",
        "https://openalex.org/W2073572878",
        "https://openalex.org/W2753259282",
        "https://openalex.org/W3161665006",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2962862931",
        "https://openalex.org/W3102944297",
        "https://openalex.org/W3081145646",
        "https://openalex.org/W1589554437",
        "https://openalex.org/W2613111945",
        "https://openalex.org/W3123756285"
    ],
    "abstract": "Lexicon-based sentiment analysis in finance leverages specialized, manually annotated lexicons created by human experts to extract sentiment from financial texts effectively. Although lexicon-based methods are simple to implement and fast to operate on textual data, they require considerable manual annotation efforts to create, maintain, and update the lexicons. These methods are also considered inferior to the deep learning-based approaches, such as transformer models, which have become dominant in various natural language processing (NLP) tasks due to their remarkable performance. However, their efficacy comes at a cost: these models require extensive data and computational resources for both training and testing. Additionally, they involve significant prediction times, making them unsuitable for real-time production environments or systems with limited processing capabilities. In this paper, we introduce a novel methodology named eXplainable Lexicons (XLex) that combines the advantages of both lexicon-based methods and transformer models. We propose an approach that utilizes transformers and SHapley Additive exPlanations (SHAP) for explainability to automatically learn financial lexicons. Our study presents four main contributions. Firstly, we demonstrate that transformer-aided explainable lexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald (LM) lexicon. This enhancement leads to a significant reduction in the need for human involvement in the process of annotating, maintaining, and updating the lexicons. Secondly, we show that the resulting lexicon outperforms the standard LM lexicon in sentiment analysis of financial datasets. Our experiments show that XLex outperforms LM when applied to general financial texts, resulting in enhanced word coverage and an overall increase in classification accuracy by 0.431. Furthermore, by employing XLex to extend LM, we create a combined dictionary, XLex&#x002B;LM, which achieves an even higher accuracy improvement of 0.450. Thirdly, we illustrate that the lexicon-based approach is significantly more efficient in terms of model speed and size compared to transformers. Lastly, the proposed XLex approach is inherently more interpretable than transformer models. This interpretability is advantageous as lexicon models rely on predefined rules, unlike transformers, which have complex inner workings. The interpretability of the models allows for better understanding and insights into the results of sentiment analysis, making the XLex approach a valuable tool for financial decision-making.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nSentiment Analysis in Finance: From\nTransformers Back to eXplainable\nLexicons (XLex)\nMARYAN RIZINSKI1, 2, HRISTIJAN PESHOV2, KOSTADIN MISHEV2, MILOS JOVANOVIK2,\nAND DIMITAR TRAJANOV2, 1, (Member, IEEE)\n1Department of Computer Science, Metropolitan College, Boston University, Boston, MA 02215, USA\n2Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, 1000 Skopje, North Macedonia\nCorresponding author: Maryan Rizinski (e-mail: rizinski@bu.edu).\nABSTRACT Lexicon-based sentiment analysis in finance leverages specialized, manually annotated\nlexicons created by human experts to effectively extract sentiment from financial texts. Although lexicon-\nbased methods are simple to implement and fast to operate on textual data, they require considerable\nmanual annotation efforts to create, maintain, and update the lexicons. These methods are also considered\ninferior to the deep learning-based approaches, such as transformer models, which have become dominant\nin various natural language processing (NLP) tasks due to their remarkable performance. However, their\nefficacy comes at a cost: these models require extensive data and computational resources for both training\nand testing. Additionally, they involve significant prediction times, making them unsuitable for real-time\nproduction environments or systems with limited processing capabilities. In this paper, we introduce a novel\nmethodology named eXplainable Lexicons (XLex) that combines the advantages of both lexicon-based\nmethods and transformer models. We propose an approach that utilizes transformers and SHapley Additive\nexPlanations (SHAP) for explainability to automatically learn financial lexicons. Our study presents four\nmain contributions. Firstly, we demonstrate that transformer-aided explainable lexicons can enhance the\nvocabulary coverage of the benchmark Loughran-McDonald (LM) lexicon. This enhancement leads to a\nsignificant reduction in the need for human involvement in the process of annotating, maintaining, and\nupdating the lexicons. Secondly, we show that the resulting lexicon outperforms the standard LM lexicon in\nsentiment analysis of financial datasets. Our experiments show that XLex outperforms LM when applied to\ngeneral financial texts, resulting in enhanced word coverage and an overall increase in classification accuracy\nby 0.431. Furthermore, by employing XLex to extend LM, we create a combined dictionary, XLex+LM,\nwhich achieves an even higher accuracy improvement of 0.450. Thirdly, we illustrate that the lexicon-based\napproach is significantly more efficient in terms of model speed and size compared to transformers. Lastly,\nthe proposed XLex approach is inherently more interpretable than transformer models. This interpretability\nis advantageous as lexicon models rely on predefined rules, unlike transformers, which have complex inner\nworkings. The interpretability of the models allows for better understanding and insights into the results of\nsentiment analysis, making the XLex approach a valuable tool for financial decision-making.\nINDEX TERMS Machine learning, natural language processing, text classification, sentiment analysis,\nfinance, lexicons, lexicon learning, transformers, SHAP, explainability\nI. INTRODUCTION\nThe financial industry generates massive amounts of data,\nfrom transactional data to news articles and social media\nposts [1], [2]. This big data poses significant challenges and\nopportunities for financial institutions as they struggle to\nextract insights and make sense of the vast amounts of in-\nformation generated every day. Extracting meaningful trends\nand actionable knowledge from such an immense quantity of\ndata is so complex and time-consuming that it makes it im-\npossible to perform by any individual actor or stakeholder in\nthe financial market. Thus, automatic approaches for big data\nanalytics are becoming essential in addressing the underlying\nchallenges in finance [3]–[5].\nSentiment analysis can play a crucial role in analyzing,\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ninterpreting, and extracting insights from financial big data.\nSentiment analysis has become increasingly important in\nthe field of finance and fintech, where it gained popular-\nity in a wide range of applications. One of the main use\ncases of sentiment analysis in finance is to predict stock\nmarket trends [6]–[9]. By analyzing news articles, social\nmedia posts, balance sheets, cash flow statements, and other\nsources of financial information, sentiment analysis can be\nused to capture market sentiment, which can help investors\nin making more informed decisions. For example, if senti-\nment analysis indicates that the overall market sentiment is\nnegative, investors may choose to sell their stocks to avoid\npotential losses. Additionally, sentiment analysis can help\nfinancial institutions and regulators monitor financial markets\nand investors’ behavior to detect potential manipulations,\nspeculations, or fraudulent activities.\nAnother application of sentiment analysis in finance is\nto assess the creditworthiness of individuals and companies\n[10]–[14]. By analyzing social media activity, customer re-\nviews, and other sources of data, sentiment analysis can\nprovide insights into the financial behavior and reputation\nof borrowers. This can help lenders make more informed\ndecisions about lending and pricing, ultimately reducing the\nrisk of default and improving profitability.\nIn fintech, sentiment analysis can be used to improve cus-\ntomer experience and engagement [15]–[19]. By analyzing\ncustomer feedback, fintech companies can better identify\nand address customer needs, preferences, and problems. This\ninformation can be used to develop personalized products and\nservices that are tailored to customer expectations, thereby\nresulting in increased customer satisfaction and loyalty. Ad-\nditionally, sentiment analysis can help fintech companies\nmonitor their brand reputation and detect potential issues\nbefore they become widespread, improving overall brand\nimage and customer trust [20]–[22].\nLexicon-based sentiment analysis is a commonly used\napproach that relies on pre-defined sets of words known\nas lexicons [23]–[25]. Lexicons are manually annotated by\nexperts in the field and assign sentiment scores to individual\nwords (positive, negative, or neutral). While knowledge ex-\ntraction using lexicons exhibits a simplistic implementation\nand fast operation on textual data, considerable manual an-\nnotation efforts are required to create, maintain, and update\nsuch lexicons. However, even after such laborious annotation,\nsome relevant words may still not be included in the lexicon,\npotentially leading to reduced sentiment classification accu-\nracy. Furthermore, lexicons tailored for one domain, such as\nfinance, cannot be easily reused in other domains. As indi-\ncated in the seminal study by Loughran and McDonald [26],\ndictionaries developed for other disciplines may misclassify\ncommon words in financial texts, highlighting the importance\nof domain-specific lexicons. There are also generic lexicons\nused for general-purpose sentiment analysis. However, they\nare known to be imprecise in various domains, introducing\ninaccuracies and biases [26].\nAnother approach to sentiment analysis is by using ma-\nchine learning (ML) [27]–[30] and deep learning (DL) tech-\nniques [25], [31]–[34]. ML/DL techniques are based on\nsophisticated algorithms that can capture complex linguistic\npatterns. For example, DL approaches, such as the state-\nof-the-art (SOTA) transformer models [35], [36], can learn\ncontextual and semantic information as well as capture long-\nterm dependencies in text, making them effective in capturing\nthe nuances of sentiment in text [37]. However, transformer\nmodels typically require massive amounts of text data which\ncan be computationally expensive to train and implement\n[38].\nSentiment extraction from financial texts requires the use\nof domain-specific language. The traditional approach for\nsentiment analysis in finance is to use manually annotated\nlexicons, such as the Loughran-McDonald (LM) lexicon. To\ncreate the LM lexicon, its authors employed the Release\n4.0 of the 2of12inf dictionary as a basis and extended it\nusing 10-X fillings 1. The LM authors do not extract all\nwords from the fillings; they rather use only those words\nthat appear frequently (with a frequency count of 50 or\nmore)2. This means that LM will not have a recall even for\na large number of the sentences present in the 10-X fillings.\nThe approach of using lexicons annotated by experts has its\nlimitations as manual editing efforts are required to maintain\nand update such lexicons. While transformers have shown\nsuperior performance in sentiment classification tasks, little\nwork has been done to investigate how these approaches can\nbe combined to create improved lexicons automatically.\nIn this paper, we explore the potential of transformers\nand ML explainability tools such as SHapley Additive ex-\nPlanations (SHAP) [39] for automating the creation of lex-\nicons, reducing their maintenance efforts, and expanding\ntheir vocabulary coverage. We propose a new methodology\nfor building eXplainable Lexicons (XLex) using pre-trained\ntransformer models and explainable ML tools. The results\ndemonstrate that the proposed methodology leads to the\ncreation of new lexicons that outperform the current state-\nof-the-art sentiment lexicons in finance.\nOur research focuses on sentiment analysis in the field\nof finance, driven by the recognition of the Loughran-\nMcDonald lexicon as a standard baseline for sentiment analy-\nsis in this domain. As indicated in a 2016 paper by the LM au-\nthors [40], the LM dictionary has been used in various studies\nto measure the sentiment in newspaper articles and columns\nsuch as [41]–[43], among others. This well-established lex-\nicon provides a solid foundation for our study, allowing\nus to pose a well-defined research question. Furthermore,\nthis work is part of a broader research project conducted\nby our team, which explores the application of advanced\nNLP techniques in finance-related contexts. Consequently,\nour primary emphasis lies on sentiment analysis within the\nfield of finance.\n1Detailed documentation on the LM development can be found in\nhttps://sraf.nd.edu/loughranmcdonald-master-dictionary/\n2It is worth mentioning that the XLex methodology that we develop in\nthis paper is not bound to the frequency count.\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nWe compare the newly created explainable lexicon with\nthe LM lexicon (known to outperform general-purpose lex-\nicons in financial contexts) on financial datasets to assess\nthe overall potential and performance of the methodology.\nOur study demonstrates that generated lexicons can improve\nthe accuracy and coverage of lexicons annotated by domain\nexperts, potentially leading to faster and more automated data\nprocessing pipelines tailored to productive NLP applications\nwhile reducing the manual work needed by domain experts.\nAdditionally, we show that our methodology has a generic ar-\nchitecture and can be applied in other areas beyond financial\napplications.\nDictionary-based sentiment models have their own advan-\ntages and disadvantages. To use a dictionary-based sentiment\nmodel, the text to be analyzed is first preprocessed to remove\nstop words, punctuation, and other non-alphanumeric char-\nacters. Then, each word in the preprocessed text is matched\nagainst the words in the sentiment dictionary and assigned\na sentiment score based on its associated sentiment value.\nThe sentiment scores for each word in the text are then\naggregated to obtain an overall sentiment score for the text.\nThis approach is relatively simple and straightforward, as\nit does not require any training or complex modeling. The\nsentiment dictionary is fixed and does not change during\nanalysis, making it easy to use and implement. Another\nadvantage of dictionary-based sentiment models is their in-\nterpretability. Since the sentiment scores assigned to each\nword in the dictionary are pre-defined, it is easy to understand\nwhy a particular text was classified as positive, negative,\nor neutral. This can be useful for analyzing the sentiment\nof text in various applications, such as customer feedback\nanalysis, social media monitoring, and market research. With\nits inherent interpretability, utilizing lexicons for sentiment\nanalysis can also aid in examining the relationship between\nthe polarity of news articles and the movements of stock\nprices [44]. In addition, dictionary-based sentiment models\nhave low computational requirements, making them suitable\nfor the real-time analysis of high-volume text sources like\nsocial media streams. They can be implemented on low-\npowered devices, such as mobile phones, which is useful\nfor applications that require quick sentiment analysis results.\nDespite their benefits, dictionary-based models also exhibit\nlimitations. They may fail to capture the nuances and com-\nplexities of natural languages, such as sarcasm and irony, and\nmay exhibit biases towards certain words or sentiment values.\nAdditionally, these models might be ineffective for analyzing\ntext in multiple languages or domains with specialized termi-\nnology.\nThe paper is organized as follows. In Section II, we make\na review of the relevant literature. Section III describes the\nmethodology and data processing pipeline for extracting\nwords and generating explainable lexicons using transform-\ners and SHAP explainability. In Section IV, we explain in\ndetail the constituent phases of the pipeline to create an\nexplainable lexicon based on SHAP that is used to expand the\nstandard LM lexicon. We use this explainable lexicon in Sec-\ntion V to create a new model for sentiment classification. We\ndemonstrate the effectiveness of our approach in Section VI,\nwhere we show it outperforms the LM lexicon. Specifically,\nwe provide a discussion assessing the performance of the\nmodel in sentiment classification tasks on financial datasets.\nWe use the last Section VII to give concluding remarks and\nsuggest directions for future research.\nII. RELATED WORK\nThe process of lexicon-based sentiment analysis has tradi-\ntionally focused on creating lexicons by manually labeling\nthe sentiment of the words included in the lexicons. While\nsuch lexicons are of high quality, they require laborious cu-\nration and domain expertise [23]. Thus, lexicons created for\none domain use specialized vocabulary and may not be suit-\nable or directly applicable to other domains. As the polarity\nof words may vary across disciplines, domain dependence in\nsentiment analysis has been emphasized by researchers in the\nfield [40], [45], [46]. [26] showed that word lists curated for\nother domains misclassify common words in financial texts.\nFor example, the word “liability” is considered neutral in\nfinance, but it usually conveys a negative polarity in general-\npurpose applications, making the reuse difficult in special-\nized lexicons. In their seminal study [26], the authors created\nan expertly annotated lexicon, called Loughran-McDonald\n(LM) lexicon, to more accurately capture sentiments in finan-\ncial texts. Other dictionaries used in finance include General\nInquirer (GI) [47], Harvard IV-4 (HIV4), and Diction, but\ntheir performance is known to be inferior compared to the\nLM lexicon in sentiment classification tasks in finance.\nGiven these drawbacks, statistical methods have been\nproposed for automatic lexicon learning. For example, [48]\nshowed that emoticons or hashtags in tweet messages can be\nused to avoid manual lexicon annotation and to significantly\nimprove lexicon coverage while effectively leveraging the\nabundance of training data. While [48] relied on calculating\npointwise mutual information (PMI) between words and\nemoticons, [49] uses a simple neural network to train lexicons\nthat improve the accuracy of predicting emoticons in tweets.\nThe study in [50] takes a different approach that proves to\nbe beneficial; it recognizes that supervised solutions can be\nexpensive due to the need to perform burdensome labeling\nof data. The data labeling process is not only challenging\nand costly but also suffers from the drawback of producing\nlimited lexicon coverage. Therefore, as its main contribution,\n[50] proved that semantic relationships between words can\nbe effectively used for lexicon expansion, contrary to what\nhas been widely assumed in the semantic analysis literature.\nTheir method uses word embeddings to expand lexicons\nin the following way: it adds new words whose sentiment\nvalues are inferred from “close” word vectors that are already\npresent in the lexicon. Surprisingly, the experimental analysis\nin [50] showed that the unsupervised method proposed by\nthe authors is as competitive as state-of-the-art supervised\nsolutions such as transformers (BERT) without having to rely\non any training (labeled) data.\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAutomatic lexicon building has been studied in several\npapers in the literature. For instance, certain approaches have\nshown that taking negation into account improves the perfor-\nmance of financial sentiment lexicons on various sentiment\nclassification tasks [51]. Adapting lexicons that depend on\nword context is studied in [52]; this work captures the context\nof words as they appear in tweet messages and uses it to\nupdate their prior sentiment accordingly. The methodology\nin [52] showed improvement in lexicon performance due to\nthe sentiment adaptation to the underlying context. Earlier\nworks explored various directions such as automatic lexicon\nexpansion for domain-oriented sentiment analysis [53], lexi-\ncon generation from a massive collection of web resources\n[54], construction of polarity-tagged corpus from HTML\ndocuments [55], etc.\nInducing domain-specific sentiment lexicons from small\nseed words and domain-specific corpora is studied in [56],\nwhere it is shown that this approach outperforms methods\nthat rely on hand-curated resources. The approach is vali-\ndated by showing that it accurately captures the sentiment\nmood of important economic topics of interest, such as data\nfrom the Beige Book of the U.S. Federal Reserve Board\n(FED) and data from the Economic Bulletin of the Eu-\nropean Central Bank (ECB). Combining word embeddings\nwith semantic similarity metrics between words and lexicon\nvocabulary is shown to better extract subjective sentiment\ninformation from lexicons [57]. This paper emphasizes that\nthe capability to automatically infer embedding models leads\nto higher vocabulary coverage. The experiments in [57]\nalso demonstrate that lexicon words largely determine the\nperformance of the resulting sentiment analysis, meaning\nthat similar lexicons (i.e., with similar vocabulary) result in\nsimilar performance.\nThe comparable performance among lexicons containing\nsimilar vocabulary is one of our main reasons to explore the\npotential of transformers to automatically learn and expand\nknown lexicons in an explainable way. The power of NLP\ntransformers to accurately extract sentiment from financial\ntexts is presented in [37], where the authors perform a com-\nprehensive analysis with more than one hundred experiments\nto prove the capabilities of transformers, and in particu-\nlar, how their word embeddings outperform lexicon-based\nknowledge extraction approaches or statistical methods.\nDue to the complexity of machine learning (ML) tech-\nniques, especially deep learning models, the outputs of the\nmodels are hard to visualize, explain and interpret. In re-\ncent years, this gives rise to a vast literature on ML model\nexplainability. A state-of-the-art technique for explainability\nis considered SHAP (SHapley Additive exPlanations), which\nuses Shapley values from game theory to explain the output\nof ML models [39].\nThe potential of SHAP is explored in different use cases.\nSHAP has been recently proven beneficial for diagnosing the\nexplainability of text classification models based on Con-\nvolutional Neural Networks (CNNs) [58]. When combined\nwith CNNs, SHAP is effective in explaining local feature\nimportance while also taking advantage of CNN’s potential\nto reduce the high feature dimensionality of NLP tasks.\nCNN is known to outperform other ML algorithms for text\nclassification, which implies that the SHAP-based analysis\nof CNN in [58] can be potentially carried out to explain any\ntext classification tasks. The increased interest in SHAP has\nalso been extended to the financial domain, where SHAP\nvalues are used for topics such as interpreting financial time\nseries [59] and financial data of bankrupt companies [60]. A\ncomprehensive study has been performed in [61] to evaluate\nSHAP in the context of ethically responsible ML in finance.\nThe SHAP method has been adapted for explaining SOTA\ntransformer language models such as BERT with the goal\nof improving the visualizations of the generated explanations\n[62].\nExtracting sentiment from news text, social media, and\nblogs has gained increasing interest in economics and fi-\nnance. The study in [63] proposes a fine-grained aspect-based\nsentiment analysis to identify sentiment associated with spe-\ncific topics of interest in each sentence of a document. Busi-\nness news texts are used to compile a comprehensive domain-\nspecific lexicon in [64]. A hybrid lexicon that combines\ncorpus-based and dictionary-based methods with statistical\nand semantic measures is proposed in [65], showing that\nsentiments extracted from a large dataset of financial tweets\nexhibit a correlation with market trends.\nSentiment analysis of news articles using lexicons has\nbeen performed on the BBC news dataset in [24]. The work\noutlines the two main lexicon approaches to sentiment anal-\nysis, namely dictionary-based and corpus-based methods,\nbut it does not involve machine learning techniques. The\nstudy in [66] recognized that focusing entirely on machine\nlearning by ignoring the knowledge encoded in sentiment\nlexicons may not be optimal. Thus, the authors presented a\nmethod that incorporates domain-specific lexicons as prior\nknowledge into algorithms such as SVM and showed that it\ncould improve the accuracy of sentiment analysis tasks.\nWhile acknowledging the advantages of deep learning\nmethods, the results in [67] showed that lexicon-based meth-\nods are to be preferred for use-cases with low-resource\nlanguages or limited computational resources at the expense\nof slightly lower performance. The authors performed a\ncomparative study between the BERT Base Italian XXL\nlanguage model and the NooJ-based lexical system with\nSentix and SentIta lexicons, thereby validating the idea of\nusing lexicons in use cases with scarce datasets. The paper\nused SHAP to perform qualitative analysis between the two\napproaches, but SHAP was not used to improve the coverage\nof existing lexicons. To the best of our knowledge, SHAP has\nstill not been explored for the purpose of automatic lexicon\ngeneration.\nIII. THE XLex WORD EXTRACTION METHODOLOGY\nThe construction of a lexicon for sentiment analysis com-\nprises several consecutive stages, each involving suitable text\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nprocessing3. To facilitate sentiment analysis, the lexicon must\nincorporate words from both positive and negative polarities.\nIn this section, we explain the steps involved in generating the\npositive and negative sentiment sets, which will subsequently\nbe merged to form an explainable lexicon.\nThe architecture of the data processing pipeline is depicted\nin Figure 1. The individual components of the pipeline are\nelaborated in detail in the following subsections of the paper.\nA. A TRANSFORMER-BASED MODEL FOR SENTIMENT\nANALYSIS\nTo develop an explainable lexicon, we begin by using a\ntransformer-based model tailored for sentiment analysis.\nSpecifically, FinBERT is a notable model in this domain,\ndesigned explicitly for analyzing financial texts [68]. How-\never, FinBERT is fine-tuned on a closed dataset comprising\n10,000 sentences from analyst reports sourced from Thomp-\nson Reuters’s proprietary Investext database. To ensure a\ncontrolled environment over the fine-tuning process, we take\ncharge of the fine-tuning process by using a publicly available\nfinancial sentiment dataset and one of the available base\ntransformer models, specifically RoBERTa. According to a\nsurvey [37], the RoBERTa model demonstrates exceptional\nperformance in various finance-related sentiment classifica-\ntion tasks, achieving an accuracy of 94% so we decided to\nuse RoBERTa as our starting model.\nTo provide a comprehensive analysis, we also included re-\nsults obtained using the FinBERT model. While both models\nproduce comparable results (as shown in Table 14), the key\nadvantage of using the fine-tuned RoBERTa model lies in the\ncustomized approach and rigorous control we have over the\ndata and fine-tuning process, thereby boosting our flexibility\nto perform experiments.\nThe datasets utilized for learning the explainable dictio-\nnaries are presented in Table 12, where they are labeled as\n“Source” datasets. The results of the XLex model are then\nevaluated using the datasets labeled as “Evaluation”.\nTo construct the sentiment dictionaries, we use SHAP for\ninterpreting the output of the pre-trained transformer model.\nThis approach aids us in identifying individual words and\nclassifying them as either positive or negative in sentiment.\nThis approach is discussed in detail in Subsection III-B.\nThe RoBERTa-Large model is originally trained in a\nself-supervised manner on an extensive corpus of English\ntext4. The RoBERTa-Large model comprises 24 layers, 1024\nhidden units, 16 attention heads, and is based on a total\nof 355 million parameters. Subsequently, we fine-tune the\nfoundational RoBERTa model using the approach outlined\nin [37]. The fine-tuning is conducted on a merged dataset\ncomprising the Financial PhraseBank [69] and SemEval-\n3The source code and datasets related to the XLex methodology, including\nall conducted experiments, are accessible on GitHub at the following link:\nhttps://github.com/hristijanpeshov/SHAP-Explainable-Lexicon-Model\n4For a comprehensive explanation of the pretraining methodology em-\nployed for RoBERTa-Large, the reader is directed to the official Hugging-\nFace documentation: https://huggingface.co/roberta-large\n2017-Task5 datasets [70]. This fine-tuning procedure ensures\nthat the resulting model is specialized for the domain of\nfinancial sentiment analysis. This fine-tuned model will also\nbe referred to as the RoBERTa-based model for convenience\nin the following sections.\nThese two constituent datasets are composed of financial\nheadlines extracted from two different sources. The sentences\nin the Financial PhraseBank corpus are selected using ran-\ndom sampling from English news on all listed companies in\nthe OMX Helsinki stock index. The sampling is performed\nto ensure that the selected sentences represent both small and\nlarge companies, different industries as well as different news\nsources. The dataset contains 4846 sentences annotated with\nthree polarities: positive, negative, and neutral. On the other\nhand, SemEval-2017-Task5 is the dataset used for the “Fine-\nGrained Sentiment Analysis” problem posed by Task 5 of\nthe SemEval 2017 competition. It consists of approximately\n1200 news headlines related to large companies operating\nworldwide. The headlines are extracted from various internet\nsources, including Yahoo Finance. The sentiment score of\neach sentence in the dataset is labeled with a real number\nranging from -1 to 1. A summary of the statistics of the two\ndatasets is given in Table 1.\nAs illustrated in Table 1, there is an imbalance between the\nnumber of positive and negative sentences in both datasets.\nThe number of neutral sentences also differs drastically when\ncompared to the number of positive or negative sentences. To\naddress the problem, balancing is performed by extracting\n1093 positive and 1093 negative sentences, which are then\nmerged into one dataset. This dataset is used for training and\nevaluation of the model that we take from [37]. The sentences\nin the dataset are shuffled and divided into 80% training set\nand 20% test set. The training and test sets contain 1748 and\n438 sentences, respectively. Both the training and test sets are\nbalanced, i.e., they contain the same number of positive and\nnegative sentences. The statistics of the resulting dataset are\nshown in Table 2.\nB. EXTRACTING WORDS AND THEIR ANALYSIS WITH\nSHAP\nThe first step in creating the lexicon involves extracting\nwords from financial sentences and labeling them as positive\nor negative. For this purpose, we use the previously intro-\nduced model together with a tokenizer. The model classifies\nthe sentiment of the input sentences, while the tokenizer deals\nwith tokenization, i.e., dividing the sentences into component\nwords. The model and tokenizer are then passed to the\nSHAP explainer, which generates explanations for the model\ndecisions.\nSHAP is considered a state-of-the-art technique for ML\nmodel explainability [71]. Its approach uses Shapley values\nfrom game theory to explain the output of ML models [72].\nGame theory is characterized by two elements: a game and\nplayers. In SHAP, the game consists of reproducing the\nresults of the model being explained (in our case, that is,\nthe NLP model for sentiment analysis), while the players\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 1: Architecture of the data processing pipeline for generating the explainable lexicon (XLex). The upper section of\nthe figure, labeled as “Extract positive and negative words using SHAP”, illustrates the word extraction process using SHAP,\nfollowed by post-processing steps to generate separate positive and negative word datasets from the chosen source datasets. The\nlower section of the figure, referred to as “Create explainable lexicon”, encompasses adding explainability features, handling\nduplicates, and merging the positive and negative datasets to form the comprehensive explainable lexicon XLex. The pipeline\nconcludes by merging XLex with the Loughran-McDonald (LM) lexicon, resulting in the combined XLex+LM lexicon.\nare the features (the financial statement, i.e., its constituent\nwords) that are passed as input to the model. SHAP evaluates\nthe contribution of each feature to the model predictions and\nassigns each feature an importance value, called a SHAP\nvalue. SHAP values are calculated for each feature across all\nsamples of the dataset to assess the contribution of individual\nfeatures to the model’s output [39]. It is important to note\nthat SHAP explains the predictions locally, meaning that the\ncontributions of the features (words) on the model prediction\nare related to a specific sample in the dataset. A different\nsample can yield other values for the features’ contributions.\nHowever, due to the additive nature of SHAP values, it is also\npossible to aggregate them, allowing us to calculate global\nvalues for the overall contribution of the features across all\nsamples.\nTABLE 1: Polarity distribution of sentences in the Financial\nPhraseBank and SemEval-2017-Task5 datasets.\nSentiment Dataset\nFinancial PhraseBank SemEval2017-Task5\nNeutral 2879 38\nNegative 604 451\nPositive 1363 654\nTotal 4846 1143\nTo evaluate the features’ contributions on the model pre-\ndiction for a given sample, SHAP creates a copy of the model\nfor each combination of the input features. Each of these\nmodels is the same, the only difference is the combination\nof features passed to the model. In one of these combinations\nnone of the features is passed to the model. In that case, the\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2: The explainable lexicon (XLex) and the LM lexicon are merged to form the combined XLex+LM lexicon. Before\nthe merging process, the “Source” feature is introduced to both XLex and LM, and all features (excluding the “word” feature)\nare appropriately prefixed to enable identification of XLex features as well as LM features within the combined lexicon.\nHandling of missing values takes place subsequent to the merging.\nFIGURE 3: The LM lexicon undergoes a preparatory adjustment process to enable its seamless integration with the explainable\nXLex lexicon, resulting in the formation of the combined XLex+LM lexicon. This adjustment process includes the extraction\nof positive and negative words, subsequent word processing, handling of duplicates, and the final step of merging the positive\nand negative word sets.\nTABLE 2: Statistics of the train and test sets used for fine-\ntuning the initial RoBERTa-Large model.\nPolarity Financial PhraseBank & SemEval2017-Task5\nTrain set Test set Total\nNegative 874 219 1093\nPositive 874 219 1093\nTotal 1748 438 2186\nmodel results in a mean value for the prediction; the value\nis obtained by averaging the labels of the dataset on which\nthe model was trained. This value is called a base value. The\nbase value is the value that would be predicted if no features\nare known for the model’s current output [39]. In this way, by\nadding a certain feature to the input, the SHAP explainer can\nrecord the changes to the predicted value and can measure the\ncontribution of that feature. Each of the features can increase\nor decrease the predicted value. Finally, to obtain the value\npredicted by the model (when all features are present), SHAP\naggregates the contribution values (which can be positive or\nnegative) for each feature and superposes the result to the\nbase value (the prediction with no input features provided).\nUsing this process, SHAP explains the contribution (impor-\ntance) of each feature in a given sample. In other words,\nSHAP measures the difference in the predicted value caused\nby the presence or absence of a feature. The additive nature\nof the aggregation is where the name SHAP comes from,\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 4: A list of explainability features based on SHAP\nadded in the explainable and LM lexicons. For the LM\nlexicon, all features except “Category” are assigned the value\nof 1 as their default value.\nnamely Shapley Additive exPlanations.\nThe input parameter passed to the SHAP explainer is a\nsentence. The NLP model evaluates the sentiment of the\nsentence by making a sentiment classification decision, while\nSHAP provides an explanation for the decision. The explana-\ntion of the SHAP explainer returns three arrays: base values\narray, data array, and values array. The base values array\ncontains two numeric values: a base value for the positive\nclass and a base value for the negative class. These base\nvalues represent the values that would be predicted for a\nparticular sentence if no input features are known. In this\ncase, it is the mean value of the labels for each of the\nclasses obtained across all instances (samples) on which\nthe model was trained. The data array contains the tokens\n(the constituent words), which are obtained by applying the\ntokenizer to the input sentence. The elements of the values\narray represent the weights, that is, the contribution of each of\nthe words (tokens) in the calculation of the sentiment of the\nsentence. The weights in the values array are real numbers\nranging from -1 to 1. The weights represent the importance\nof a particular word (token) and its contribution to the final\nvalue predicted by the sentiment classification model. The\ndata array and the values array have the same number of\nelements.\nAs mentioned earlier, the weights are additive which al-\nlows them to be superposed. By adding the weights to the\nbase value, the explainer arrives at the value predicted by the\nsentiment model. Visually, this superposition is represented\nusing diagrams where the calculated weights “push” the base\nvalue to the “right” or to the “left”, causing the model to\nincrease or decrease its predicted value. By doing so, it is\npossible to explain how the model arrived at a given decision\nand how different parts of a sentence contributed to the\nmodel’s output. Specifically, in terms of sentiment analysis\nwith SHAP, this helps understand why a given NLP model\nclassified a sentence as positive or negative and how each\nof the constituent words of the sentence contributed to that\nclassification decision.\nA visual example is given in Figure 6. The figure shows\nthat positive importance values, marked red, “push” the base\nvalue to the “right” (increasing the model’s predicted value),\nwhile negative weights, marked blue, “push” the base value\nto the “left” (reducing the model’s predicted value). The\nexample is visualized from the perspective of the positive\nclass, meaning that each \"push\" to the \"right\" increases the\nprobability of predicting a positive sentiment for the given\nsentence. Each \"push\" to the \"left\" decreases this probability,\nthat is, it increases the probability of predicting a negative\nsentiment for the sentence. Using these preliminaries about\nthe SHAP explainer, we will next create two sets containing\npositive and negative words as explained in Subsection III-C.\nC. CREATING A POSITIVE AND NEGATIVE DATASET\nAND THEIR POSTPROCESSING\nThe sentiment classification in the previous subsection is\nperformed on datasets containing financial sentences. These\ndatasets are denoted as source datasets in Table 12. Using\nSHAP, each of the words in a sentence is marked as positive\nor negative in the given context. The decision to label a\nparticular word as positive or negative depends on whether\nit contributes with a positive or negative weight to the final\ndecision of the model. As a result, two new datasets are\ngenerated. One dataset contains all words across all sentences\nthat contribute to the positive sentiment of each sentence\n(we refer to the words and dataset as “positive” words\nand “positive” dataset, respectively), while the other dataset\ncontains all words across all sentences that contribute to\nthe negative sentiment (“negative” words, “negative” set). In\naddition to the words themselves, these datasets store a few\nadditional parameters for each word, such as the mean value\nof the weights (importance values) obtained by the SHAP\nexplainer for all of the word appearances, the sum of these\nvalues as well as their maximum and minimum values. The\ndatasets also store the total number (count) of appearances of\neach of the words. All numerical entries in the datasets are\nrepresented by their absolute value.\nAfter creating the positive and negative datasets, we per-\nform post-processing to filter the extracted words. The goal\nis to keep only the words that are valid and have meaning.\nThe word post-processing process is explained as follows.\nThe post-processing begins by transforming all words into\nlowercase letters. Then, all entries consisting of one or two\nletters are removed since they are of little sentiment utility to\nthe datasets. These entries are typically fragments of words\nthat are obtained due to the limitations of the tokenizer. The\nRoBERTa tokenizer is limited by the size and coverage of the\nvocabulary that is used to train the tokenizer. This leads to\nincorrect or imprecise tokenization of certain words that are\neither not sufficiently represented in the training vocabulary\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 5: The process of dealing with duplicate entries between the positive and negative words for each of the explainable\n(XLex) and LM lexicons. In the case of the LM lexicon, features designated as “Opposite” are assigned a default value of 0.\nThe “Total Count” feature can be obtained by deriving it from the values of “Count (Selected)” and “Count (Opposite)”.\nFIGURE 6: An example of using SHAP for evaluating the word contributions to the sentiment of a sentence.\nor are not represented at all. As a result, these words are not\naccurately represented since they are divided into parts based\non more common entries found in the tokenizer’s vocabulary.\nThus, entries with one or two letters are deemed unnecessary\nand are removed due to their insufficient contribution to the\nsentiment analysis.\nTo obtain valid and useful words, we apply another filter to\nthe datasets. Using a dictionary of English words, we remove\nall words that are not contained in the English dictionary.\nThis is done to address the limitation of the tokenizer and also\nto provide a dataset containing only valid words. The last step\nin the post-processing removes auxiliary words that do not\ncarry meaning in the sentence, such as adverbs, prepositions,\npronouns, and conjunctions (stop words).\nThese preliminary steps and the data stored for each word\nare necessary to develop an explainable lexicon which will\nbe shown in Section IV.\nIV. XLex LEXICON CREATION METHODOLOGY\nIn the previous section, we demonstrated the use of a trans-\nformer model for sentiment analysis in combination with\nSHAP to process finance-related sentences, which resulted in\nthe creation of two datasets. One dataset contains all words\nwith positive sentiment in a given context (positive dataset),\nwhile the other contains all words with negative sentiment\nin a given context (negative dataset). These two datasets\nare used to create an explainable lexicon, as will be shown\nlater on. We will evaluate the performance of the explainable\nlexicon employing the model proposed in the subsequent\nSection V. The evaluation results are presented in Section\nVI.\nThis section explains in detail the methodology for gener-\nating the explainable lexicon as well as the process of merg-\ning it with the LM lexicon. The methodology encompasses\nfour phases, as shown in Figure 1.\nA. PHASE 1: LEMMATIZATION AND REMOVAL OF\nDUPLICATE WORDS WITHIN THE POSITIVE AND\nNEGATIVE DATASETS\nIn this phase, we lemmatize the words in the positive and\nnegative datasets obtained in Section III. As a result of the\nlemmatization, each of the words is replaced by its lemma,\nthat is, by its basic form. The goal is to bring different forms\nof a certain word to their common lemma, thereby avoiding\ndifferent interpretations of the same word. However, this\ncauses duplicate words to appear in the datasets. After their\nlemmatization, different forms of a word (which until that\nmoment are uniquely represented) can have the same lemma.\nThe purpose of Phase 1 is to make the datasets consistent\nby removing duplicate words. Avoiding duplicates will also\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nresult in a single source of information related to a particular\nword.\nEach of the duplicates may result in different values for\nthe number of appearances, the average SHAP value, the\nsum SHAP value, as well as for the maximum and minimum\nSHAP values. Thus, the goal is to merge the duplicates so\nthat each word is characterized by a single (unique) value\nfor each of these features. The removal of duplicates is\nperformed separately in each of the two datasets (the positive\nand the negative set). As will be shown, there are words that\nare labeled both as positive and negative, i.e., words that\nare present in both datasets. Dealing with these duplicates\nbetween the two datasets is done in Phase 2.\nTo calculate the unique values for the features of a par-\nticular word, it is necessary to aggregate the values of the\nfeatures across all duplicates. The method for aggregating the\nduplicates for each feature (column) is shown in Table 3. The\naggregation function represents how the values of all dupli-\ncates are combined (aggregated) for a certain feature. From\nthe table, it can be seen that the feature indicating the number\nof appearances of the word is obtained as a summation of the\nnumber of appearances for each of the duplicates. The reason\nfor this is that each of the duplicates represents the same word\nafter lemmatization, so the number of occurrences of that\nword will be represented as the sum of all the occurrences of\nthe duplicates. The same approach is applied to calculate the\ntotal (sum) SHAP value. After lemmatization, all duplicates\nof a word have the same form, so the sum SHAP value\nof all occurrences is the sum of the values of this feature\nacross all duplicates. The maximum value is represented by\nan aggregation function that takes the maximum along this\ncolumn for all duplicates. The maximum SHAP value of all\nduplicates is a suitable representative of the maximum SHAP\nvalue of that word. The minimum SHAP value is handled\nsimilarly. To obtain the minimum SHAP value for the word,\nit is necessary to aggregate it with a function that calculates\nthe minimum SHAP value from all duplicates. As can be seen\nfrom Table 3, no aggregation is performed for the feature\n“Average SHAP value” because it is obtained by dividing the\nsum SHAP value by the number of occurrences of the word.\nTable 4 illustrates an example of merging duplicate words\nin the positive dataset and getting unique values for the\nfeatures. The example presents three different words that\nresult in the same lemma after performing lemmatization,\ndemonstrating how duplicate words are handled. In order to\nhave only one instance of the word “acquire” in the dataset,\nit is necessary to merge these three instances into one. This\nis done as per the definition of the aggregation functions\ngiven in Table 3. In the example, the sum is calculated based\non the number of occurrences of all duplicates (9 + 4 +\n5 = 18), which results in a total of 18 occurrences of the\nword “acquire”. The sum of the sum SHAP values across all\nduplicates (3.05+1.4+0.88 = 5.33) represents a sum SHAP\nvalue of 5.33 for the word. To get the average SHAP value, it\nis necessary to divide the sum SHAP value by the number of\noccurrences (5.33 ÷ 18 = 0.3), which gives an average SHAP\nvalue of 0.3 for the word “acquire”. The maximum SHAP\nvalue for the word is 0.6, while the minimum SHAP value is\n0.02.\nThe method demonstrated in this phase is applied to each\nof the two datasets separately. The example above shows how\nthis process is performed for one word in the positive dataset,\nbut the same procedure is used for all other duplicate words in\nthat dataset, as well as for all duplicate words in the negative\ndataset. This is indicated with the elements “Lemmatization”\nand “Removal of duplicates” in Figure 1.\nB. PHASE 2: HANDLING OF DUPLICATE WORDS\nBETWEEN DATASETS\nA particular word can be present in both the positive and neg-\native datasets, leading to word overlaps between the datasets.\nGiven our goal to generate a lexicon as a combination of the\ntwo datasets, each word should be represented by a single\ninstance in the resulting lexicon. To overcome the overlaps,\nwe use the following approach. If an overlapping word has a\nhigher sum SHAP value in the positive dataset ( SHAP pos\nsum)\nwhen compared to the negative one ( SHAP neg\nsum), then the\nword is labeled as positive. Similarly, ifSHAP neg\nsum is higher\nthan or equal to SHAP pos\nsum, then the word is labeled as\nnegative. The decision criteria are shown in Equation 1:\nselected dataset=\n(\npositive, SHAP pos\nsum > SHAPneg\nsum\nnegative, otherwise\n(1)\nIf a certain word is labeled as positive or negative (in the\nselected dataset), it is removed from the opposite dataset.\nTo keep the information about the word removed from the\nopposite dataset, new columns are introduced in the datasets.\nThe new columns are given in Table 5 under “Features added\nin Phase 2”. A complete representation of the words in the\ntwo datasets, including their features from both polarities, is\nachieved by adding these columns. Using Equation 1 as a\ndecision criterion and keeping information about the word\nremoved from the opposite dataset is shown in Figure 5.\nTable 5 shows the features added in Phase 2 in addition to\nthe existing features of the datasets. The table also consol-\nidates brief explanations for each of the features. It should\nbe noted that the label “opposite” represents the set that was\nnot selected during the decision, in accordance with Equation\n1. Thus, if Equation 1 decides that an overlapping word\nbelongs to the positive dataset, in that case, the “opposite”\ndataset is the negative dataset. This word is removed from\nthe negative dataset, and all its values from the negative\ndataset are placed in the positive dataset, in the corresponding\ncolumns marked as “opposite”. Similarly, if the decision\ncriteria decide that the word belongs to the negative dataset,\nin that case “opposite” represents the positive dataset. This\nword is removed from the positive dataset, and all its values\nfrom the positive dataset are placed in the negative set in the\ncorresponding columns marked “opposite”.\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 3: Aggregation functions to handle duplicates across the numerical features of the sentiment dataset. No aggregation\nis performed for the average SHAP value as it is obtained by dividing the sum SHAP value by the total number of word\nappearances. Another feature that is not aggregated is Category since it is a categorical variable.\nFeatures\nNumber of\napperances\nSum\nSHAP value\nAverage\nSHAP value\nMaximum\nSHAP value\nMinimum\nSHAP value\nAggregation function Sum Sum N/A Max Min\nTABLE 4: An example for aggregating duplicates in the positive dataset. Duplicates are handled similarly in the negative\ndataset.\nDuplicates Features\nOriginal\nword Lemma Number of\nappearances\nTotal\nSHAP\nvalue\nAverage\nSHAP\nvalue\nMaximum\nSHAP\nvalue\nMinimum\nSHAP\nvalue\nacquire acquire 9 3.05 0.34 0.6 0.05\nacquired acquire 4 1.4 0.35 0.5 0.23\nacquiring acquire 5 0.88 0.18 0.43 0.02\nSingle instance\nafter aggregation (acquire) 18 5.33 0.3 0.6 0.02\nTABLE 5: Features of the words in the lexicons.\nFeature name Feature notation Feature description\nWord feature\nWord word A word that appears in the lexicon\nInitial features\nCount (Selected) count Number of appearances of the word\nSum SHAP Value (Selected) SHAPsum Sum SHAP value of the word\nAverage SHAP Value (Selected) SHAPavg Average SHAP value of the word\nMaximum SHAP Value (Selected) SHAPmax Maximum SHAP value of the word\nMinimum SHAP Value (Selected) SHAPmin Minimum SHAP value of the word\nFeatures added in Phase 2\nTotal Count counttotal\nTotal number of appearances of the\nword (in the two sentiments)\nCount (Opposite) countopp\nTotal number of appearances\nin the opposite sentiment\nSum SHAP Value (Opposite) SHAP opp\nsum\nSum SHAP value of the word\nin the opposite sentiment\nAverage SHAP Value (Opposite) SHAP opp\navg\nAverage SHAP value of the word\nin the opposite sentiment\nMaximum SHAP Value (Opposite) SHAP opp\nmax\nMaximum SHAP value of the word\nin the opposite sentiment\nMinimum SHAP Value (Opposite) SHAP opp\nmin\nMinimum SHAP value of the word\nin the opposite sentiment\nSHAP Ratio (Selected) SHAPratio\nRatio between SHAPavg\nand the sum of SHAPavg and SHAP opp\navg\nSHAP Ratio (Opposite) SHAP opp\nratio\nRatio between SHAP opp\navg\nand the sum of SHAPavg and SHAP opp\navg\nFeatures added in Phase 3\nCategory category Category of the word (positive or negative)\nFeatures added in Phase 4\nSource src Source lexicon from which\nthe word originates from\nIf a word is decided to belong to the positive set, then the\nSHAP ratio (SHAPratio) is calculated as the ratio between\nthe average SHAP value of the word from the positive dataset\nand the sum of the average SHAP values of the word from the\npositive and negative datasets. This is shown in Equation 2:\nSHAPratio = SHAP pos\navg\nSHAP pos\navg + SHAP neg\navg\n(2)\nThe opposite value of the SHAP ratio is expressed as the\nratio between the average SHAP value of the word from the\nopposite dataset (in this case, it is the negative dataset) and\nthe sum of the average SHAP values of the word from the\npositive and negative sets. This is shown in Equation 3.\nSHAP opp\nratio = SHAP neg\navg\nSHAP pos\navg + SHAP neg\navg\n= 1− SHAPratio\n(3)\nSimilar steps are taken if the word is decided to belong to the\nnegative dataset. The only difference is that SHAPratio is\ncalculated based on the average SHAP value of the negative\ndataset (SHAP neg\navg ), while SHAP opp\nratio is calculated based\non the average SHAP value of the positive set (SHAP pos\navg ).\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTable 6 shows an illustrative example with the word “op-\ntion” that appears in both datasets (positive and negative).\nAs can be seen, this word has a sum SHAP value in the\npositive and negative dataset of SHAP pos\nsum = 0 .39 and\nSHAP neg\nsum = 0.023, respectively. Given that SHAP pos\nsum >\nSHAP neg\nsum, it is decided that the word belongs to the positive\ndataset and is removed from the negative dataset. Before\nremoving the word from the negative dataset, the values of\nits features from the negative dataset are added to the positive\ndataset in the corresponding columns labeled as “opposite”.\nThe values added in the “opposite” columns are given as\nfollows: countopp = 7, SHAP opp\nsum = 0.023, SHAP opp\navg =\n0.0033, SHAP opp\nmax = 0.009, SHAP opp\nmin = 0.0001. The\nSHAP ratio of the word in the two datasets is calculated\nas SHAPratio = 0.026\n0.026+0.0033 = 0 .887; SHAP opp\nratio =\n0.0033\n0.026+0.0033 = 0.113. All other features of the word in the\nselected (positive) dataset remain unchanged.\nIf a word appears only in one of the datasets, a zero value is\nassigned to each of the features labeled as “opposite” because\nthat word does not appear in the opposite sentiment. Also,\naccording to Equation 2, SHAPratio evaluates to 1 since\nSHAP opp\navg for the corresponding word is 0.\nC. PHASE 3: MERGING THE POSITIVE AND NEGATIVE\nDATASETS\nIn this phase, the two datasets, positive and negative, are\nmerged into a single dataset. The feature “Category” is\nimportant for the merging. Possible values for this feature\nare “positive” and “negative”, depending on whether the\nword is in the positive or negative dataset. All words from\nthe positive dataset have “positive” as the value for this\nfeature, while all words from the negative dataset have the\nvalue “negative”. The purpose of the “Category” feature is to\ndelineate positive words from negative words in the resulting\nexplainable lexicon. Using this feature, the two datasets are\nmerged by simply adding all the data points (i.e., all words\ntogether with all their features) from the negative set to the\npositive one. An excerpt of the explainable lexicon after the\nmerge is shown in Table 7. 5 This finalizes the creation of\nthe explainable lexicon containing words that are automati-\ncally extracted with the help of transformers and SHAP. A\ndistinctive property of this lexicon is the usage of SHAP\nvalues, especially SHAPavg, which will be later used to\nperform sentiment analysis. As will be shown by the results\nin Section VI, SHAPavg is a good indicator of the sentiment\nof a particular word. In addition to this feature, SHAPratio\nand count are also introduced as parameters that will be\nused in sentiment analysis when determining the polarity of\na particular sentence. This is explained in detail in Section V.\nOur aim is to use the explainable lexicon to improve and\nextend the LM lexicon. To compare their performance, these\ntwo lexicons are combined into a final lexicon which for the\nremainder of the paper will be interchangeably referred to as\n5To ensure the diagrams fit within the page limits, only a subset of the\ndataset features are depicted in this and subsequent lexicon-related diagrams.\nthe combined lexicon or XLex+LM lexicon. We will use the\ncombined lexicon to perform sentiment analysis on financial\nsentences, thereby evaluating the possible improvement of\nthe combined lexicon over the plain vanilla LM lexicon. The\nresults obtained by analyzing the combined lexicon will be\nshown and discussed in Section VI. In the next and last phase,\nPhase 4, we explain the process of combining the explainable\nand LM lexicons into the combined lexicon.\nD. PHASE 4: MERGING WITH THE\nLOUGHRAN-MCDONALD DICTIONARY\nIn this last phase, we combine the explainable lexicon with\nthe LM lexicon. However, before this can be done, it is\nnecessary that the words in the LM lexicon undergo similar\nprocessing as in the case of the explainable lexicon so that the\nLM words obtain the same set of features. The processing of\nwords of the LM lexicon is given in Figure 3 and is explained\nas follows.\n1) Processing of the LM Lexicon\nWhile the Loughran-McDonald lexicon consists of seven\nsentiment datasets, only its positive and negative components\n(datasets) are of interest to the combined (XLex+LM) lexi-\ncon. Similarly to the datasets used to create the explainable\nlexicon, the words from the LM datasets are first trans-\nformed into lowercase letters and then lemmatized. Duplicate\nwords are obtained due to lemmatization. These datasets\nconsist only of words without any other additional features\n(columns), so there is no need to aggregate the duplicates\nfor a particular word. Instead, all duplicates are removed,\nleaving only one instance of the word in the datasets. To be\nable to combine this lexicon with the explainable lexicon, it\nis necessary to ensure they have the same features. Thus, all\nfeatures from the explainable lexicon (shown in Table 5) are\nadded to the LM datasets.\nAs a first step, the initial features and the features intro-\nduced in Phase 2 are added to each of the LM datasets. These\nnewly added features (except for those labeled as“opposite”)\nare assigned a value of 1 as their main (default) value. Since\nthese words do not contain values for the corresponding\nfeatures, it is necessary to assign them a specific value. The\nvalue 1 is chosen as the main value to indicate if the word\nis present in the given dataset. While 1 is a high value to\nbe assigned to SHAPavg, this default value assignment is\ncompensated with the model coefficients that are introduced\nin Section V. On the other hand, those features labeled as\nopposite are assigned a value of 0 since there are no words\nfrom one dataset that overlap with the other dataset. This\nassignment of values is a consequence of the fact that the\nwords originating from the LM datasets are not obtained in\nan explainable way using SHAP; thus, they do not have the\ncharacteristics shown in Table 5.\nIn addition, the feature “Category” is added to all the\nwords from the LM datasets. For the words from the positive\nand negative LM dataset, this column is filled with the value\n“positive” or “negative” respectively. As was the case with\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 6: An example with the word “option” and its features in the dataset.\nWord option\nDataset Features (columns)\ncount SHAPsum SHAPavg SHAPmax SHAPmin\nPositive 15 0.39 0.026 0.2 0.07\nNegative 7 0.023 0.0033 0.009 0.0001\nTABLE 7: An excerpt of selected features of the explainable lexicon.\nWord Count\n(Selected) Total Count\n(Opposite) Category\nAverage\nSHAP Value\n(Selected)\nAverage\nSHAP Value\n(Opposite)\nSum\nSHAP Value\n(Selected)\nSum\nSHAP Value\n(Opposite)\nnew 426 577 151 positive 0.090629 0.021480 38.608165 3.243527\namp 311 568 257 negative 0.032435 0.037868 10.087175 9.732118\nworld 267 513 246 positive 0.046991 0.027468 12.546591 6.757057\nyear 384 461 77 negative 0.050818 0.032912 19.514242 2.534230\nchina 213 426 213 positive 0.042231 0.032894 8.995204 7.006360\ngroup 297 398 101 positive 0.079659 0.026396 23.658838 2.666006\ncompany 291 387 96 positive 0.076470 0.030276 22.252625 2.906498\nenergy 327 357 30 positive 0.109173 0.019223 35.699520 0.576700\nbank 202 351 149 positive 0.069276 0.036797 13.993703 5.482755\npower 312 328 16 positive 0.136059 0.019770 42.450354 0.316315\nstate 258 301 43 negative 0.039640 0.011132 10.227108 0.478668\nmillion 168 285 117 negative 0.048877 0.050262 8.211418 5.880613\nuse 147 284 137 positive 0.042997 0.025001 6.320554 3.425115\ncountry 240 280 40 negative 0.037619 0.023715 9.028585 0.948618\nmarket 177 273 96 positive 0.048187 0.043074 8.529055 4.135104\ntrump 231 269 38 negative 0.086255 0.022430 19.924996 0.852340\ntime 192 264 72 negative 0.045261 0.026059 8.690184 1.876225\napple 224 255 31 positive 0.112486 0.022463 25.196845 0.696366\nchinese 139 248 109 negative 0.035228 0.026792 4.896641 2.920374\nglobal 173 240 67 positive 0.078297 0.040759 13.545395 2.730843\nbillion 168 233 65 negative 0.055175 0.048924 9.269427 3.180079\nban 149 228 79 negative 0.066532 0.032665 9.913209 2.580523\nday 180 223 43 negative 0.064980 0.026559 11.696429 1.142023\nthe datasets from the explainable lexicon, the purpose of the\n“Category” feature in the LM datasets is to be able to identify\nthe origin of a given word in the merged LM lexicon, i.e.,\nwhether the word originates from the positive or negative\nLM dataset. After this, the two LM datasets are merged\ninto a single consolidated dataset by simply adding the data\npoints (i.e., the words together with all their features) from\nthe negative to the positive LM dataset. This concludes the\nprocessing of the LM lexicon. In the next subsection, the LM\nlexicon will be merged with the explainable lexicon to arrive\nat the combined (XLex+LM) lexicon. A visual overview of\nthe LM lexicon after merging the positive and negative LM\ndatasets, along with some of the added features, is shown in\nTable 8.\n2) Obtaining the XLex+LM Lexicon by Merging the XLex and\nLM Lexicons\nAs a final step, we merge the explainable lexicon (XLex)\ncreated in Phase 3 with the LM lexicon. We make two\nchanges in the lexicons before merging them. We introduce a\nnew feature (column) called src (“source”) as shown in Table\n5. Since two different lexicons will be merged into one, the\npurpose of this feature is to indicate the origin of a certain\nword in the merged lexicon, i.e., whether the word originates\nfrom the explainable or LM lexicon. The feature is filled in\nwith the value “XLex” and “LM” if the word originates from\nthe explainable and LM lexicon, respectively. Thesrc feature\nallows flexibility in selecting the lexicon that is used by the\nsentiment analysis model in the evaluation process. Thus, it\nis possible to select the explainable lexicon (XLex), the LM\nlexicon, or the combined (XLex+LM) lexicon.\nWe also add a prefix to all features in the lexicons (i.e.,\nall features indicated in Table 5). The only exception is the\ncolumn that contains the word itself (“word” column) since\nthat column is used to merge the two lexicons. Adding the\nprefix is done with the same purpose, namely to have the\nflexibility to select a lexicon for the sentiment analysis model.\nSelecting a certain lexicon means taking into account only\nits words and features in the sentiment analysis and not the\nwords and features of the other lexicon. Before merging\nthe lexicons, they have the same names for the features, so\nto distinguish these features in the combined lexicon, it is\nnecessary to name them differently. We add prefixes “XLex”\nand “LM” to denote the columns from the explainable and\nLM lexicon, respectively. This is shown in Tables A.1-A.2 in\nthe Appendix A. The prefix “XLex” stands for “eXplainable\nLexicon” and indicates that the lexicon is created using ex-\nplainability tools. The prefix “LM” is an abbreviation for the\nLoughran-McDonald lexicon, indicating that these features\nare related to the LM lexicon. With these two changes, it is\npossible to completely extract the explainable or LM lexicon\nfrom the combined lexicon.\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 8: An excerpt of selected features of the LM lexicon.\nWord Count\n(Selected) Total Count\n(Opposite) Category\nAverage\nSHAP Value\n(Selected)\nAverage\nSHAP Value\n(Opposite)\nSum\nSHAP Value\n(Selected)\nSum\nSHAP Value\n(Opposite)\nsurpasses 1 1 0 positive 1 0 1 0\ntransparency 1 1 0 positive 1 0 1 0\ntremendous 1 1 0 positive 1 0 1 0\ntremendously 1 1 0 positive 1 0 1 0\nunmatched 1 1 0 positive 1 0 1 0\nunparalleled 1 1 0 positive 1 0 1 0\nunsurpassed 1 1 0 positive 1 0 1 0\nupturn 1 1 0 positive 1 0 1 0\nvaluable 1 1 0 positive 1 0 1 0\nversatile 1 1 0 positive 1 0 1 0\nversatility 1 1 0 positive 1 0 1 0\nvibrancy 1 1 0 positive 1 0 1 0\nvibrant 1 1 0 positive 1 0 1 0\nwin 1 1 0 positive 1 0 1 0\nwinner 1 1 0 positive 1 0 1 0\nworthy 1 1 0 positive 1 0 1 0\nabandon 1 1 0 negative 1 0 1 0\nabandonment 1 1 0 negative 1 0 1 0\nabdicate 1 1 0 negative 1 0 1 0\nabdicates 1 1 0 negative 1 0 1 0\nabdication 1 1 0 negative 1 0 1 0\naberrant 1 1 0 negative 1 0 1 0\naberration 1 1 0 negative 1 0 1 0\nAfter merging the lexicons, all words will appear with\none instance in the combined dataset, including words that\nappear in both lexicons. The features of a given word in the\ncombined lexicon will contain the feature values of both the\nexplainable and LM lexicons for that word. This is shown\nin Table A.3. If a particular word does not appear in both\nlexicons, it will also be represented by a single instance in\nthe combined lexicon, but its instance will be populated only\nwith the features of that word from the lexicon in which\nit exists, not the features from the other lexicon. This is\nshown in Table A.4. Words of this type do not appear in\nboth lexicons, and therefore, for the lexicon in which they\ndo not appear, there is no value that can be assigned to\nthem. This is the reason why after merging the lexicons, there\nare words with missing feature values for certain columns,\nas indicated with “NaN” (missing value) in Table A.4. For\ndifferent columns, the missing values are handled differently.\nThe feature “XLex Category” is filled with the value “none”\nbecause that word does not appear in the explainable lexicon.\nSimilarly, the feature “LM Category” is filled in with the\nvalue “none” because the word is not present in the LM\nlexicon. If the column “XLex Source” has the value “NaN”,\nthen it means that the word is from the LM lexicon, so the\ncolumn “LM Source” has the value “LM”. To indicate that\nthe word does not appear in the explainable lexicon, the\n“NaN” value of the “XLex Source” column is replaced by\nthe “LM” value. Similarly, if the “LM Source” column has\nthe value “NaN”, then it means that the word is from the\nexplainable lexicon, so the “XLex Source” column has the\nvalue “XLex”. To indicate that the word is not contained\nin the LM lexicon, the value “NaN” of the column “LM\nSource” is replaced by the value “XLex”. All other columns\nthat contain “NaN” values (for the corresponding lexicon in\nwhich the given word does not appear) are assigned the value\nof 0. If a word does not appear in a given lexicon, the value\nfor all its features is 0. Figure 2 summarizes the handling of\nmissing (“NaN”) values that arise due to the merging of the\ntwo lexicons.\nAfter handling the invalid feature values, an excerpt of the\nlexicon’s content is shown in Table 9. Comparing Table A.4\nand Table 9 can reveal the effect of replacing invalid values\nfor certain columns. A normalized version of the combined\nlexicon is then created. To obtain the normalized lexicon,\nthe values of each of the numerical features are modified\naccording to Equation 4:\nvnorm = v(f)\nmax(f) (4)\nwhere v(f) represents the value of a feature for a given word,\nwhile max(f) is the maximum value of that feature across\nall words. This step concludes the creation of the combined\nXLex+LM lexicon, which can now be used as a basis for\nperforming sentiment analysis.\nIn the next section, we define a model for sentiment\nanalysis based on the combined lexicon.\nV. MODEL FOR SENTIMENT ANALYSIS BASED ON\nEXPLAINABLE LEXICONS\nIn this section, we develop a model for sentiment analysis.\nThe model is designed to make lexicon-based decisions,\nnamely using the combined XLex+LM lexicon. To perform\nsentiment classification, the model can also use the explain-\nable or LM lexicon as input since both can be extracted from\nthe combined lexicon. To determine the sentiment of sen-\ntences, it is necessary to pass the following input parameters\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 9: The combined lexicon after handling invalid values.\nWord XLex Count\n(Selected) XLex Total XLex Count\n(Opposite)\nXLex Average\nSHAP Value\n(Selected)\nXLex Source\nLM Average\nSHAP Value\n(Selected)\nLM Category\nLM Sum\nSHAP Value\n(Opposite)\nLM Source\nLM Max\nSHAP Value\n(Opposite)\nabide 1.0 1.0 0.0 0.003838 XLex 0 none 0 XLex 0\nabo 1.0 1.0 0.0 0.000976 XLex 0 none 0 XLex 0\naboard 2.0 2.0 0.0 0.122640 XLex 0 none 0 XLex 0\nabolition 1.0 1.0 0.0 0.006430 XLex 0 none 0 XLex 0\nabroad 3.0 3.0 0.0 0.039073 XLex 0 none 0 XLex 0\nwriteoff 0.0 0.0 0.0 0.000000 LM 1 negative 0 LM 0\nwriteoffs 0.0 0.0 0.0 0.000000 LM 1 negative 0 LM 0\nwrongful 0.0 0.0 0.0 0.000000 LM 1 negative 0 LM 0\nwrongfully 0.0 0.0 0.0 0.000000 LM 1 negative 0 LM 0\nwrongly 0.0 0.0 0.0 0.000000 LM 1 negative 0 LM 0\nto the model: the combined lexicon, the lexicon’s features\nthat will be used to make decisions about the sentiment of the\nsentences, as well as the source of the words, that is, which\nof the lexicons will be used in the analysis (explainable,\nLM or combined lexicon). There are three features used\nfor decision-making purposes: SHAPavg, SHAPratio, and\ncount. Each of these characteristics can make the decision\nindividually, but they can also be used together in any com-\nbination. The details of how these decision features are used\ntogether are explained later in this section.\nAfter defining the model and its input parameters, we use\nthe model to perform sentiment classification of financial\nsentences. The datasets used in the process of sentiment\nclassification and the corresponding results are outlined in\nSection VI. We use the evaluation method of our model. The\ninput parameters passed to the method are the sentences to\nbe evaluated, the actual labels (sentiment) of those sentences,\nas well as 4 or 2 coefficients, depending on whether the\ncombined lexicon or any of the constituent lexicons is used\nindividually. The purpose of these coefficients is to control\nhow much each of the lexicons will contribute to the decision\nas well as how much importance will be given to the selected\ncategory relative to the opposite category.\nWe now explain how to calculate the sentiment of a certain\nsentence using the sentiment analysis model, relying on the\ncombined lexicon. The explanation applies to one sentence,\nbut the same process is applied to every sentence in the\ndataset. To determine the sentiment of a particular sentence,\nit is first split into its component words using a tokenizer.\nWe employ two types of tokenizers in our methodology.\nFor XLex, we use the corresponding RoBERTa/FinBERT\ntokenizers, and for LM, we use NLTK, which is a standard\nrule-based tokenizer. After applying the tokenizers, every\nword is transformed into lowercase letters and lemmatized.\nAll words in the combined lexicon are lemmatized, so in\norder to follow an identical approach, we also lemmatize the\nwords from the evaluation sentences. Each of the sentences\nis represented as a set of words wi, 1 ≤ i ≤ n:\nsentence = {w1, w2, ..., wn} (5)\nWe calculate the sentiment value of every word wi in\na given sentence. Before calculating this sentiment value,\nit is necessary to calculate a cumulative value for each of\nthe lexicons selected in the sentiment analysis (explainable\nand LM) and for each of the word categories (positive and\nnegative). The term “cumulative value” refers to the sum of\nthe values of the word’s features. For a specific word and\nfor a specific lexicon, the cumulative value for the positive\ncategory is calculated as per Equation 6:\nvc\npos(wi) =\nnX\ni=1\nvpos(xi) (6)\nwhere xi, 1 ≤ i ≤ n, are the features selected to make the de-\ncision in the sentiment analysis. These features are the same\nfor each of the selected lexicons and for each of the word\ncategories. vpos(xi) is the value of the feature xi of the pos-\nitive word category. The sum of all these features represents\nthe cumulative value of a given word in the selected lexicon\nas per the positive category. As previously mentioned, these\ndecision-making features can be at most three ( SHAPavg,\nSHAPratio, and count) and at least one. At the same time,\nit is possible to use any other combination of them. Similarly\nto the positive word category, the cumulative value of a given\nword in the selected lexicon with respect to the negative\ncategory is calculated as per Equation 7:\nvc\nneg(wi) = (−1)\nnX\ni=1\nvneg(xi) (7)\nwhere xi, 1 ≤ i ≤ n, are the selected features used to make\nsentiment decisions while vneg(xi) is the value of the feature\nxi in the negative word category. The sum across all the\nselected features represents the cumulative value of a given\nword in the negative category for the selected lexicon. As can\nbe seen in Equation 7, the sum is multiplied by −1, ensuring\nthat the cumulative value for the negative word category is\nalways negative. Initially, all feature values are positive in\neach of the two lexicons as well as in the combined lexicon,\ni.e., given by their absolute values. Thus, it is necessary\nto multiply the cumulative value by −1 for the negative\ncategory. As will be pointed out later in this subsection,\nthis facilitates the calculation of the sentiment value of the\nanalyzed word. It should be noted that if a certain word does\nnot exist in one of the categories or in one of the lexicons, the\ncumulative value evaluates to 0 according to Equations 6-7.\nThe sentiment value of a given word can be calculated after\ndetermining the cumulative value for each lexicon and each\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ncategory. If the combined XLex+LM lexicon is chosen for\nperforming the sentiment analysis, the sentiment value of the\nword is obtained using Equation 8:\nvsent(wi) =cxlp ∗ vc\nxl(wi) +cxlo ∗ vc\nxl,opp(wi)+\n+ clmp ∗ vc\nlm(wi) +clmo ∗ vc\nlm,opp(wi) (8)\nThe variables used in Equation 8 are summarized and ex-\nplained in Table 10. The coefficients (parameters) in Equa-\ntion 8 are introduced to control the contribution of each\nlexicon and each category on the sentiment classification de-\ncision. As will be explained in Section VI, the parameters can\nbe fine-tuned to investigate which values lead to improved\nsentiment classification performance.\nIf only the explainable lexicon is passed to the sentiment\nanalysis model when determining the sentiment of the sen-\ntences, the sentiment value of a given word is calculated\nusing Equation 9:\nvsent(wi) =cxlp ∗ vc\nxl(wi) +cxlo ∗ vc\nxl,opp(wi) (9)\nAs can be seen, only two coefficients are used in this equation\ninstead of four since only one of the lexicons is selected.\nOn the other hand, if only the LM lexicon is selected as an\ninput to the sentiment analysis model, the sentiment value of\na given word is calculated by Equation 10:\nvsent(wi) =clmp ∗ vc\nlm(wi) +clmo ∗ vc\nlm,opp(wi) (10)\nEquation 10 also has only two parameters instead of four\nsince only one of the lexicons is selected.\nAfter calculating the sentiment value of every word in a\nsentence using the above equations, the sentiment value of\nthe sentence is evaluated as the sum of the sentiment value of\neach of the constituent words. This is given in Equation 11:\nvsent(sentence) =\nnX\ni=1\nvsent(wi) (11)\nwhere sentence is represented as a set of words (Equation\n5). In this way, the sentiment value of a certain sentence is\ncalculated. Next, we determine the polarity of a sentence, i.e.,\nwhether it is positive, negative, or neutral. To calculate the\nsentiment polarity spol of a sentence from its sentiment value,\nwe check whether the sentiment value is positive, negative, or\nequal to 0 as follows:\nspol(sentence) =\n\n\n\npositive : vsent(sentence) > 0\nnegative : vsent(sentence) < 0\nneutral : otherwise\n(12)\nThe sentiment model uses Equation 12 to calculate the\nsentiment of each sentence that is subject to sentiment\nanalysis. After calculating the sentiment of each sentence,\nwe evaluate the sentiment classification performance of the\nmodel. The evaluation is performed using the predicted and\nactual sentiments of each of the sentences. For this purpose,\nwe use standard classification metrics such as accuracy, F1\nscore, and MCC. We also generate a classification report\nand confusion matrix. The results regarding the accuracy\nmetric are presented in Table 14, while the F1 and MCC\nscores are given in Appendix B. The confusion matrix and\nclassification report are presented in Figure 7 and Table\n18, respectively. The confusion matrix and classification\nreport are generated for the XLex+LM model achieving\nthe highest accuracy across the experiments performed. As\nshown later in the paper, the XLex+LM model achieves\nits highest accuracy of 84.3% when constructed with the\nnasdaq dataset as its source dataset and evaluated on the\nfinancial_phrase_bank dataset.\nEquations 11-12 show why it is necessary to involve\nmultiplication by −1 in Equation 7. The sentiment value\nof a certain sentence is the sum of the sentiment values of\nthe constituent words of that sentence. The sentiment of the\nsentence depends on whether its sentiment value is positive\nor negative. Thus, it is important to ensure that a positive\nword leads to a positive sentiment value while a negative\nword leads to a negative sentiment value. This is achieved by\nthe equations for calculating the cumulative value (Equations\n6-7).\nThe methodology explained in this section completes the\nentire process – from automatic word extraction, word clas-\nsification, and postprocessing to creating an explainable lex-\nicon with SHAP, combining it with the manually annotated\nLM lexicon, and finally creating a model that will classify the\nsentiment of finance-related sentences. The results obtained\nby applying this model to different datasets of financial\nsentences are shown in the next section.\nVI. RESULTS AND DISCUSSION\nWe present results obtained by the model introduced in the\nprevious section using the combined XLex+LM lexicon.\nA. USED DATASETS\nTables 11-12 present the datasets used to build the explain-\nable lexicons as well as the datasets on which these lexicons\nare evaluated. Table 11 summarizes these datasets by giving\ntheir descriptions, while Table 12 contains summary statistics\nabout the datasets. Each of the datasets consists of financial\nsentences, where each sentence is labeled with its sentiment\npolarity. It should be noted that these datasets do not contain\nthe sentences that were used to train the initial model given\nin Section III with the goal of avoiding bias in the experi-\nments. In addition, the evaluation datasets do not include any\nsentences that are present in the source datasets.\nThe label “Source” in the “Purpose” column in Table 12\ndenotes that the corresponding dataset is used to extract\nwords with SHAP and to generate an explainable lexicon.\nThe label “Evaluation” in the “Purpose” column in Table\n12 denotes that the corresponding dataset is used to evaluate\nthe generated explainable lexicons. Details about generating\nthe explainable lexicons from these datasets are shown as\nfollows.\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 10: Explanations of the variables used in the equations for calculating the sentiment value of a given word (Equations\n8-10).\nVariable Description\nPrimary category The category selected as the primary category in\nPhase 2 (positive or negative)\nOpposite category The category that was not selected as the primary\ncategory in Phase 2 (positive or negative)\ncxlp\nCoefficient of influence on the cumulative value\nof the primary category in relation to the explainable lexicon\nvcxl Cumulative value of the primary category in relation\nto the explainable lexicon\ncxlo\nCoefficient of influence on the cumulative value\nof the opposite category in relation to the explainable lexicon\nvcxl,opp Cumulative value of the opposite category in relation to the\nexplainable lexicon\nclmp\nCoefficient of influence on the cumulative value\nof the primary category in relation to the LM lexicon\nvclm Cumulative value of the primary category in relation\nto the LM lexicon\nclmo\nCoefficient of influence on the cumulative value\nof the opposite category in relation to the LM lexicon\nvclm,opp Cumulative value of the opposite category\nin relation to the LM lexicon\nTABLE 11: Descriptions of the datasets that are used in the process of obtaining results from the sentiment analysis.\nDataset Description\nsentfin Sentfin: Dataset of financial news with entity-sentiment annotations\nfiqa_labeled_df Fiqa: Aspect-based dataset of financial sentences\nsem_eval Financially relevant news headlines annotated for fine-grained sentiment\nfinancial_phrase_bank Financial PhraseBank: Manually annotated financial\nsentences about companies listed on the OMX Helsinki stock index\nfpb_fiqa Financial PhraseBank + Fiqa\nnasdaq Financial news about companies listed on the NASDAQ index\nfiqa_fpb_sentfin_neutral All neutral sentences from Fiqa, Financial PhraseBank and Sentfin\nTABLE 12: Statistics of the datasets used in the process of generating results from the sentiment analysis giving the number of\npositive, negative, and neutral sentences as well as indicating the purpose of the datasets (used as a source or for evaluation).\nLabel Total number\nof sentences Positive Negative Neutral Purpose\nfiqa_labeled_df 201 139 62 0 Evaluation\nsem_eval 353 270 83 0 Evaluation\nfpb_fiqa 1542 1236 306 0 Evaluation\nfinancial_phrase_bank 885 774 111 0 Evaluation\nfinancial_phrase_bank 2960 89 0 2871 Source\nnasdaq 9202 3067 5903 232 Source\nfiqa_fpb_sentfin_neutral 6086 0 0 6086 Source\nThe datasets utilized in this study were primarily obtained\nfrom Kaggle, with the exception of the SemEval-2017-Task5\ndataset, which was accessed from the official page of the\nSemEval competition. For extracting words with SHAP,\nwe conducted a thorough search on Kaggle to find suit-\nable financial-related datasets comprising textual statements,\nsentences, or news headlines for sentiment analysis. Our\nselection criteria included datasets studied in the literature\nor containing relevant data about companies listed on the\nstock market, ensuring diverse sources for extracting positive\nand negative words for building the explainable lexicon. As\nfor the evaluation datasets, we not only considered financial\ntextual data but also ensured that they were appropriately\nannotated by financial experts. This selection process was\nimplemented to ensure the validity and robustness of these\ndatasets for evaluation purposes.\nB. SUMMARIZATION OF THE GENERATED LEXICONS\nWe generate three different explainable lexicons. The words\nin the lexicons are generated from three different sources.\nThe datasets serving as the sources of the lexicons are marked\nas “Source” in the “Purpose” column of Table 12. Each\nof the lexicons is created using the method described in\nSections III-IV. The purpose of using different sources is\nto verify the ability of the method presented in this paper\nto successfully generate explainable lexicons under different\nconditions (given that different sources exhibit varied data).\nIn Table 12, the Sentfin dataset is denoted as a “Source”\ndataset because it is used only for the purposes of word\nextraction. The Sentfin dataset comprises headlines and cor-\nresponding sentiment labels for the financial entities men-\ntioned in the headlines. Each financial entity in a headline\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 13: Statistics of the lexicons on which sentiment analysis is performed. The lexicons are obtained using the RoBERTa\ntransformer model.\nLexicon Total number\nof words Positive Negative\nfiqa_fpb_sentfin_neutral 3313 1635 1678\nnasdaq 5751 2537 3214\nfinancial_phrase_bank 2729 1342 1387\nLoughran–McDonald 1731 246 1485\nis assigned a sentiment label. However, the dataset lacks sen-\ntiment labels specifically for the headlines themselves, which\nrenders it unsuitable for evaluation purposes. Nonetheless,\nwe utilize this dataset as a “Source” since the sentiment labels\nfor the headlines are not required for our word extraction and\nclassification process.\nWe also want to evaluate the effectiveness of the method-\nology to automatically label the words with the appropriate\nsentiments. Summary data of the explainable lexicons is\nshown in Table 13, which also gives information about the\nLM lexicon. Each of the explainable lexicons from Table 13\nis combined with the LM lexicon, and the resulting lexicons\nare used in the process of evaluating the model performance.\nThe results of the analysis are shown in the next subsection.\nC. RESULTS FROM THE SENTIMENT ANALYSIS\nThe model takes (uses) two parameters that can be fine-\ntuned: decision coefficients and features that will be used to\nmake sentiment decisions. We perform a grid search to find\nthe optimal values of the model parameters that maximize\nthe accuracy, F1 and MCC of our model when performing\nsentiment analysis.\nAlthough it is possible to use all three decision features\n(SHAPavg, SHAPratio, and count), we conducted a grid\nsearch to identify the most effective combination. Our re-\nsults revealed that SHAPavg has the dominant impact on\naccuracy, F1 and MCC, and we, therefore, selected it as the\nprimary decision feature. Then we performed a second grid\nsearch by using both the standard (without normalization)\nand normalized versions of the explainable lexicons in order\nto find the optimal values of the coefficients cxlp, cxlo, clmp\nand clmo. We chose 0.1, 0.3, 0.5, 0.7, and 0.9 as possible\nvalues for these coefficients to distinguish between different\nlevels of impact (0.1 denotes weak impact, while 0.9 denotes\nstrong impact). By applying permutations with repetition, all\npermutations of the values for the coefficients are obtained.\nThere are five possible values that can be assigned to three\ncoefficients (we exclude the clmo coefficient as obsolete be-\ncause there is no word shared between positive and negative\nwords in the LM dictionary; thus, there are no “opposite”\nwords in the LM dictionary). Thus, the total number of\npermutations is 53 = 125. These permutations are combined\nwith each of the three explainable lexicons (using both stan-\ndard and normalized versions of the lexicons), and each of the\nfour evaluation datasets. The only exception is the financial\nphrase bank explainable lexicon and the financial phrase bank\nevaluation dataset. We do not evaluate this combination to\navoid biased results. As a result, we arrive at a total of 2750\nmodels that are created for the purpose of grid search (125\npermutations × 2 explainable lexicons × 2 lexicon versions\n× 4 evaluation datasets + 125 permutations × 1 explainable\nlexicon × 2 lexicon versions × 3 evaluation datasets). For\neach of the models in the grid search set of models, we are\nusing the SHAPavg feature as the decision maker.\nAfter obtaining the results, our primary goal is to identify\nthe combination of coefficients that yields the highest aggre-\ngated average across the accuracy, F1, and MCC metrics.\nTo calculate this aggregated average, we first determine the\naverage values for accuracy, F1, and MCC scores across\nall experiments. We compute these average values for each\ncombination of coefficients, considering both the explainable\nlexicons XLex and XLex+LM obtained from the three dis-\ntinct source datasets (nasdaq, fpb and sentfin). This results\nin a total of3×2×3 = 18average values for each coefficient\ncombination. The aggregated average for a specific combina-\ntion of coefficients is obtained by summing these 18 average\nvalues, allowing us to represent each coefficient combination\nusing a single consolidated parameter.\nThrough the grid search procedure, we discovered that\nthe coefficients (cxlp, cxlo, clmp, clmo) = (0.3, 0.1, 0.1, 0.5)\nform the combination that achieves the highest aggregated\naverage, thus defining the optimal model parameters. It is\nworth noting that the choice of the value for the clmo co-\nefficient does not impact the grid search procedure, as there\nare no “opposite” words in the LM dictionary. Hence, we can\nsafely assume that clmo = 0.5 without affecting the outcome\nof the grid search.\nOnce the optimal model parameters are selected, we pro-\nceed by generating the results. For this purpose, we use the\ncombined lexicons from Table 13 that are also available in\ntheir normalized form. Each of these lexicons serves as a\nbasis for performing sentiment analysis using the model pro-\nposed in Section V. Each of the created models is evaluated\non all evaluation sets in Table 12 (these are the sets containing\nthe label “Evaluation” in the “Purpose” column). Only the\nmodel that uses the Financial PhraseBank dataset as a source\nfor the combined lexicon is not evaluated on that same dataset\nin order to avoid model bias. The results about accuracy are\nshown in Table 14a. Additional classification metrics, such\nas the F1 and MCC scores, are given in Tables B.1-B.2 in\nAppendix B.\n18 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 14: Accuracy obtained using the XLex methodology based on the RoBERTa and FinBERT transformer models. The\ncolumns “LM”, “XLex”, and “XLex+LM” show the accuracy of the models on the corresponding dataset. The columns “LM\non LM”, “XLex on LM”, “(XLex+LM) on LM” show the accuracy on portions of the datasets where we have a recall from LM\n(i.e., all instances where the LM either provided no answer or were unable to make a decision are removed from the datasets).\nThe approach with the highest accuracy among the three approaches is represented in bold.\n(a) RoBERTa-based results\nSource of the lexicon Normalized Evaluation set Accuracy on whole dataset Accuracy only on the part of the dataset for\nwhich the LM-based model has an answer\nLM XLex XLex+\nLM\nLM on\nLM\nXLex on\nLM\n(XLex+LM)\non LM\nnasdaq Yes financial_phrase_bank 0.303 0.836 0.843 0.753 0.784 0.801\nnasdaq Yes fiqa_labeled_df 0.313 0.721 0.731 0.808 0.795 0.821\nnasdaq Yes fpb_fiqa 0.296 0.761 0.776 0.748 0.726 0.766\nnasdaq Yes sem_eval 0.275 0.745 0.765 0.752 0.721 0.775\nnasdaq No financial_phrase_bank 0.303 0.837 0.843 0.753 0.781 0.795\nnasdaq No fiqa_labeled_df 0.313 0.697 0.706 0.808 0.795 0.821\nnasdaq No fpb_fiqa 0.296 0.75 0.768 0.748 0.72 0.764\nnasdaq No sem_eval 0.275 0.745 0.756 0.752 0.729 0.76\nfiqa_fpb_sentfin_neutral Yes financial_phrase_bank 0.303 0.808 0.808 0.753 0.801 0.801\nfiqa_fpb_sentfin_neutral Yes fiqa_labeled_df 0.313 0.697 0.711 0.808 0.808 0.846\nfiqa_fpb_sentfin_neutral Yes fpb_fiqa 0.296 0.733 0.746 0.748 0.748 0.779\nfiqa_fpb_sentfin_neutral Yes sem_eval 0.275 0.725 0.756 0.752 0.69 0.775\nfiqa_fpb_sentfin_neutral No financial_phrase_bank 0.303 0.81 0.811 0.753 0.798 0.801\nfiqa_fpb_sentfin_neutral No fiqa_labeled_df 0.313 0.672 0.692 0.808 0.795 0.846\nfiqa_fpb_sentfin_neutral No fpb_fiqa 0.296 0.733 0.746 0.748 0.744 0.777\nfiqa_fpb_sentfin_neutral No sem_eval 0.275 0.722 0.759 0.752 0.674 0.775\nfinancial_phrase_bank Yes fiqa_labeled_df 0.313 0.632 0.682 0.808 0.731 0.859\nfinancial_phrase_bank Yes fpb_fiqa 0.296 0.676 0.707 0.748 0.687 0.764\nfinancial_phrase_bank Yes sem_eval 0.275 0.671 0.697 0.752 0.698 0.767\nfinancial_phrase_bank No fiqa_labeled_df 0.313 0.662 0.692 0.808 0.744 0.821\nfinancial_phrase_bank No fpb_fiqa 0.296 0.691 0.716 0.748 0.7 0.762\nfinancial_phrase_bank No sem_eval 0.275 0.671 0.694 0.752 0.705 0.767\nAverage accuracy 0.296 0.727 0.746 0.766 0.744 0.793\n(b) FinBERT-based results\nSource of the lexicon Normalized Evaluation set Accuracy on whole dataset Accuracy only on the part of the dataset for\nwhich the LM-based model has an answer\nLM XLex XLex+\nLM\nLM on\nLM\nXLex on\nLM\n(XLex+LM)\non LM\nnasdaq Yes financial_phrase_bank 0.303 0.768 0.779 0.753 0.736 0.761\nnasdaq Yes fiqa_labeled_df 0.313 0.761 0.771 0.808 0.833 0.859\nnasdaq Yes fpb_fiqa 0.296 0.747 0.752 0.748 0.754 0.767\nnasdaq Yes sem_eval 0.275 0.725 0.728 0.752 0.775 0.783\nnasdaq No financial_phrase_bank 0.303 0.694 0.722 0.753 0.694 0.764\nnasdaq No fiqa_labeled_df 0.313 0.761 0.761 0.808 0.846 0.846\nnasdaq No fpb_fiqa 0.296 0.709 0.724 0.748 0.73 0.767\nnasdaq No sem_eval 0.275 0.714 0.722 0.752 0.767 0.791\nfiqa_fpb_sentfin_neutral Yes financial_phrase_bank 0.303 0.831 0.836 0.753 0.756 0.77\nfiqa_fpb_sentfin_neutral Yes fiqa_labeled_df 0.313 0.791 0.811 0.808 0.795 0.846\nfiqa_fpb_sentfin_neutral Yes fpb_fiqa 0.296 0.78 0.796 0.748 0.736 0.777\nfiqa_fpb_sentfin_neutral Yes sem_eval 0.275 0.756 0.785 0.752 0.721 0.798\nfiqa_fpb_sentfin_neutral No financial_phrase_bank 0.303 0.816 0.823 0.753 0.753 0.77\nfiqa_fpb_sentfin_neutral No fiqa_labeled_df 0.313 0.786 0.801 0.808 0.795 0.833\nfiqa_fpb_sentfin_neutral No fpb_fiqa 0.296 0.774 0.791 0.748 0.736 0.779\nfiqa_fpb_sentfin_neutral No sem_eval 0.275 0.756 0.785 0.752 0.729 0.806\nfinancial_phrase_bank Yes fiqa_labeled_df 0.313 0.751 0.771 0.808 0.782 0.833\nfinancial_phrase_bank Yes fpb_fiqa 0.296 0.792 0.799 0.748 0.749 0.766\nfinancial_phrase_bank Yes sem_eval 0.275 0.734 0.776 0.752 0.698 0.814\nfinancial_phrase_bank No fiqa_labeled_df 0.313 0.741 0.751 0.808 0.795 0.821\nfinancial_phrase_bank No fpb_fiqa 0.296 0.778 0.788 0.748 0.743 0.767\nfinancial_phrase_bank No sem_eval 0.275 0.739 0.776 0.752 0.705 0.806\nAverage accuracy 0.296 0.759 0.775 0.766 0.756 0.797\nVOLUME 4, 2016 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nD. DISCUSSION\nTable 14a reveals that the model achieves overall best accu-\nracy results in sentiment analysis when the combined lexicon\nXLex+LM is used as a basis for the analysis. The same\napplies when the explainable lexicon XLex is used as a basis.\nThe highest accuracy of the model based on the combined\nXLex+LM lexicon is 0.843 (column 6 in Table 14a); if senti-\nment classification is performed using only the explainable\nlexicon under the same conditions (i.e., the same source\nof the lexicon and the same evaluation set), the obtained\naccuracy evaluates to 0.837 (column 5 in Table 14a). For the\nsame experiment, the accuracy evaluates to 0.303 (column 4\nin Table 14a) if only the LM lexicon is taken as a basis for\nsentiment classification. The reason for this result is due to\nthe insufficient word coverage of the LM lexicon since the\nLM lexicon does not contain the words that make up a large\npart of the sentences of the evaluation set. Therefore, those\nexpressions remain unanswered. As can be seen from Table\n15, the sentiment analysis performed with the LM lexicon\nleads to a large number of unanswered sentences for each\nof the datasets. The percentage of unanswered sentences is\nabout 60% for each dataset. These unanswered expressions\nare considered wrongly answered, leading to very low accu-\nracy of the LM lexicon. On the other hand, Table 15 shows\nthat there are almost no unanswered sentences when using\nexplainable lexicons combined with the LM lexicon. Hence,\nthe results show that explainable lexicons are advantageous\nover manually annotated lexicons as they achieve larger vo-\ncabulary coverage and higher accuracy in sentiment analysis.\nExplainable lexicons are able to achieve larger vocabulary\ncoverage because they can automatically extract words and\nclassify them using explainable ML models. Consequently,\nthe combined lexicon also leads to a larger vocabulary cover-\nage.\nAs evidenced by the data presented in Table 14, it can\nbe observed that XLex consistently surpasses LM in all\nexperiments, resulting in an overall increase of 0.431 in terms\nof classification accuracy. This improvement remains evident\nwhen we extend LM with XLex. The combined XLex+LM\ndictionary leads to an overall 0.450 increase in accuracy over\nLM. Moreover, as observed by Tables B.1-B.2 in Appendix\nB, it is noteworthy to highlight that the XLex+LM model\nconsistently outperforms the LM model in terms of both F1\nand MCC scores across all conducted experiments. Notably,\nthe explainable lexicon XLex alone exhibits improvements\nover LM, leading to a 0.155 increase in F1 and a 0.090\nenhancement in MCC. Even higher are the results achieved\nby the combined lexicon, XLex+LM, which demonstrates an\nincrease of 0.226 in F1 and a 0.190 rise in MCC compared\nto LM. The enhancements in performance are determined by\ncomputing the difference between the average of the respec-\ntive metric (accuracy, F1, or MCC) for XLex (XLex+LM)\nand the same metric averaged for LM. The metric’s average is\nderived by averaging its values across all experiments. Table\n16 consolidates the performance enhancements of XLex and\nXLex+LM over LM in terms of accuracy, F1 and MCC.\nTo test XLex methodology’s effectiveness in the worst-\ncase scenario, we conducted an evaluation only on portions\nof the datasets where we have a recall from LM (i.e., filtering\nout all instances where the LM either provided no answer or\nwere unable to make a decision). This creates a dataset with\na strong bias in favor of LM, ultimately resulting in higher\nclassification accuracy than the case when using LM on the\nwhole dataset. For example, LM did not provide answers for\n522 out of 885 sentences in the financial_phrase_bank\ndataset. This means that the evaluation in this case was\nconducted on only 363 sentences, effectively reducing the\noriginal dataset by 59% (Table 15 illustrates the reduction\nin the size of the evaluation datasets).\nOn this heavily constrained dataset towards LM, first, we\ntested the accuracy of the LM dictionary, and the results\nare shown in the “LM on LM” column in Table 14a. Then\nwe applied the XLex on this LM-contained dataset, and we\nobtained slightly worse results (column “XLex on LM”). The\naccuracy decreased by only 1% in the case of the FinBERT-\nbased model and by 2.2% in the RoBERTa-based model.\nThis experiment indicates that despite the fact that XLex is\ntrained on a general dataset and is automatically created, it\nproduces comparable results to the state-of-the-art expert-\nannotated dictionary in this worst-case scenario.\nFurthermore, we wanted to explore if XLex could be used\nto extend the LM in this worst-case scenario, so we evaluated\nthe performance of the combined dictionary (XLex+LM) on\nthe LM-constrained datasets. Our findings reveal that a com-\nbined dictionary always leads to improvement of the results\n(column “(XLex+LM) on LM” in Table 14a). The average\naccuracy increased by 3% for the FinBERT-based model and\n2.65% for the RoBERTa-based model. These results show\nthat the proposed methodology can also be effectively used\nas an automated dictionary enhancement methodology that\ncan help in extending the expert annotated dictionaries.\nTo gain a deeper understanding of the decision-making\nprocesses of XLex and LM, we present a detailed analysis\nof several text instances. We will use the test setup involv-\ning the standard XLex-based model (without normalization)\nin combination with nasdaq and financial_phrase_bank\nused as the source and evaluation dataset. As expected,\nour observations revealed that LM’s errors stem from its\ninsufficient word coverage. For instance, in the sentences\n“Finnish engineering and technology company Metso Oyj\nsaid on May 27, 2008, it completed the acquisition of paper\nmachinery technology from Japanese engineering company\nMitsubishi Heavy Industries (MHI) for an undisclosed sum”\nand “Nokia also noted the average selling price of handsets\ndeclined during the period, though its mobile phone profit\nmargin rose to more than 22 percent from 13 percent in the\nyear-ago quarter” , LM incorrectly places emphasis solely\non the words “undisclosed” and “decline”, respectively. LM\nclassifies both words with a negative sentiment, disregarding\nthe rest of the words in the respective sentences, which leads\nto inaccurate predictions. In contrast, XLex exhibits a larger\n20 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 15: Number of sentences on which the models based on a given lexicon did not give answers. The results are obtained\nbased on the lexicons generated using the RoBERTa transformer model.\nEvaluation dataset Number of\nsentences Lexicons\nfiqa_fpb_sentfin nasdaq financial_phrase_bank Loughran-McDonald\nfiqa_labeled_df 201 3 2 6 130\nsem_eval 353 1 0 2 224\nfinancial_phrase_bank 885 0 0 / 522\nfpb_fiqa 1542 4 3 8 937\nTABLE 16: Average improvements of XLex and XLex+LM\nover LM in terms of accuracy, F1 and MCC. The values\nrepresent the differences in average metric scores (accuracy,\nF1, or MCC) between XLex (XLex+LM) and LM, both cal-\nculated by averaging the values of the corresponding metrics\nacross all experiments.\nImprovement over LM\nModel Acc F1 MCC\nXLex (RoBERTa-based) 0.431 0.155 0.090\nXLex+LM (RoBERTa-based) 0.450 0.226 0.190\nXLex (FinBERT-based) 0.463 0.227 0.183\nXLex+LM (FinBERT-based) 0.479 0.250 0.253\nvocabulary coverage and can assign sentiment scores to\nother relevant words in these sentences, resulting in accurate\npredictions. We also analyzed cases where XLex made errors\nwhile LM produced correct predictions, such as the sentences\n“Finnish airline Finnair is starting the temporary layoffs of\ncabin crews in February 2010” and “The financial impact is\nestimated to be an annual improvement of EUR 2.0 m in the\ndivision’s results, as of fiscal year 2008”. In these cases, LM\ncorrectly classified the words “layoffs” and “improvement”,\nrespectively, which had a dominant impact on the sentiment\nclassification for the two sentences. While XLex formed its\ndecision score on more words, its classification ultimately\nresulted in an inaccurate prediction. However, for these cases,\nthe combined lexicon XLex+LM resulted in accurate predic-\ntions.\nIt is worth mentioning that we explored two distinct ap-\nproaches in our initial experimental setup, one utilizing Fin-\nBERT and the other employing RoBERTa. FinBERT is a pre-\ntrained large language model specifically designed for finan-\ncial text analysis [68]. It is based on the BERT architecture\nand trained on a large corpus of financial text data to better\nunderstand and analyze financial language and documents. It\nhas been reported that FinBERT performs well in financial\napplications compared to general-purpose language models.\nIn our analysis, the FinBERT model demonstrated a similar\nperformance to RoBERTa with a slight accuracy improve-\nment when utilizing XLex and XLex+LM. We want to note\nthat we have made a deliberate choice to base our main\nanalysis on RoBERTa, as FinBERT is fine-tuned on a closed\nproprietary dataset. In contrast, our model (using RoBERTa)\nis fine-tuned on publicly available datasets, enabling us to\nexercise precise control over the fine-tuning process while\nstill achieving satisfactory results. All the details regarding\nthe RoBERTa and FinBERT-based models are shown in Table\n14, Table 16, Table 17 and Appendix B. 6 The results for\nFinBERT were obtained following the identical grid search\nprocedure as that applied to RoBERTa.\nBesides the achieved high accuracy and increased vo-\ncabulary coverage, the proposed explainable lexicons also\nlead to two additional benefits: speed and size. The speed\nfor processing sentences is an important factor in real-time\nproduction systems. If NLP processing is worth doing at all\nin a system, it is worth doing it fast [73].\nTable 17 shows a comparative analysis of model sizes and\nsentiment classification execution times for three models:\nthe XLex-based model, featured in Section V, the fine-tuned\nRoBERTa transformer model, presented in Section III, and\nthe FinBERT model. The analysis involves conducting ex-\nperiments using the XLex-based model with lexicons learned\nfrom various source datasets, assessing its performance in\nsentiment classification across different evaluation datasets.\nWe explore both normalized and non-normalized versions of\nthe lexicons. In contrast, the RoBERTa and FinBERT-based\nmodels do not rely on source datasets by design, as they are\npre-trained models that can be readily used for sentiment\nclassification. Consequently, the corresponding entries in\ncolumns 2-4 of Table 17 remain empty.\nThe execution speed of the models is evaluated across the\nevaluation datasets using a central processing unit (CPU) in\nGoogle Colab to ensure a fair comparison under identical\nconditions. The execution time of each model is determined\nby calculating the average of the times recorded from 10\nexperimental runs. For the experiments, we used Google\nColab’s free tier, which provides an Intel Xeon CPU with\none physical and two logical cores running at 2.20GHz\npaired with 12GB of RAM. As can be seen in Table 17, the\nresults reveal a substantial difference in the execution time of\nthe models. The XLex-based model leads to a significantly\nsmaller execution time compared to the RoBERTa and Fin-\nBERT transformer models by a factor of about 87 and 21,\nrespectively. The factor is determined by dividing the average\nCPU speed of the RoBERTa (FinBERT) model by that of the\nXLex-based model (the averages are calculated considering\nthe respective experiments for each of the two models). This\nmakes the lexicon-based model suitable for tasks that need\n6The source code and results about the comparison between RoBERTa\nand FinBERT can be found at: https://github.com/hristijanpeshov/SHAP-\nExplainable-Lexicon-Model/tree/master/notebooks\nVOLUME 4, 2016 21\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 17: Comparison of the XLex-based model with the RoBERTa and FinBERT transformer models in terms of model\nspeed and size. The XLex-based model utilizes various source datasets and undergoes evaluation across different evaluation\ndatasets. The execution speed of the models is assessed in a CPU environment available within the free tier of Google Colab to\nensure a fair comparison under identical conditions. The CPU environment uses an Intel Xeon CPU with one physical and two\nlogical cores running at 2.20GHz, equipped with 12GB of RAM.\nModel Source\ndataset Is normalized? Num. of words\nin source dataset Eval dataset Num. of sentences\nin eval dataset Model size Processing time (CPU)\nin seconds\nXLex-based model\nnasdaq No 7094 fpb_fiqa 1542 363 KB 11.48\nnasdaq No 7094 fiqa_labeled_df 201 363 KB 1.44\nnasdaq No 7094 financial_phrase_bank 885 363 KB 7.47\nnasdaq No 7094 sem_eval 353 363 KB 1.56\nnasdaq Yes 7094 fpb_fiqa 1542 363 KB 11.68\nnasdaq Yes 7094 fiqa_labeled_df 201 363 KB 1.44\nnasdaq Yes 7094 financial_phrase_bank 885 363 KB 7.48\nnasdaq Yes 7094 sem_eval 353 363 KB 1.59\nfinancial_phrase_bank No 4344 fpb_fiqa 1542 202 KB 11.71\nfinancial_phrase_bank No 4344 fiqa_labeled_df 201 202 KB 1.21\nfinancial_phrase_bank No 4344 sem_eval 353 202 KB 1.46\nfinancial_phrase_bank Yes 4344 fpb_fiqa 1542 202 KB 11.61\nfinancial_phrase_bank Yes 4344 fiqa_labeled_df 201 202 KB 1.44\nfinancial_phrase_bank Yes 4344 sem_eval 353 202 KB 1.44\nfiqa_fpb_sentfin_neutral No 4868 fpb_fiqa 1542 233 KB 11.7\nfiqa_fpb_sentfin_neutral No 4868 fiqa_labeled_df 201 233 KB 1.29\nfiqa_fpb_sentfin_neutral No 4868 financial_phrase_bank 885 233 KB 7.53\nfiqa_fpb_sentfin_neutral No 4868 sem_eval 353 233 KB 1.6\nfiqa_fpb_sentfin_neutral Yes 4868 fpb_fiqa 1542 233 KB 11.74\nfiqa_fpb_sentfin_neutral Yes 4868 fiqa_labeled_df 201 233 KB 1.14\nfiqa_fpb_sentfin_neutral Yes 4868 financial_phrase_bank 885 233 KB 7.51\nfiqa_fpb_sentfin_neutral Yes 4868 sem_eval 353 233 KB 1.6\nRoBERTa transformer model\n— — — fpb_fiqa 1542 1.32 GB 960.05\n— — — fiqa_labeled_df 201 1.32 GB 111.5\n— — — financial_phrase_bank 885 1.32 GB 600.12\n— — — sem_eval 353 1.32 GB 183.45\nFinBERT transformer model\n— — — fpb_fiqa 1542 417.8 MB 232.26\n— — — fiqa_labeled_df 201 417.8 MB 27.95\n— — — financial_phrase_bank 885 417.8 MB 143.99\n— — — sem_eval 353 417.8 MB 38.02\nto be performed quickly and in real time and still lead to\nreasonably accurate predictions.\nTABLE 18: Classification report showcasing the perfor-\nmance of the XLex-based model (using RoBERTa) based on\nthe combined XLex+LM lexicon. The model is constructed\nusing the lexicon created with the nasdaq dataset as its\nsource dataset. The model evaluation is performed on the\nfinancial_phrase_bank dataset. This model achieves the\nhighest accuracy among all XLex+LM models tested, achiev-\ning an accuracy rate of 84.3%.\nprecision recall f1-score support\nNegative sentences 0.43 0.72 0.54 111\nPositive sentences 0.96 0.86 0.91 774\naccuracy 0.84 885\nmacro avg 0.69 0.79 0.72 885\nweighted avg 0.89 0.84 0.86 885\nSimilar to other neural network models, RoBERTa can\nleverage parallel architectures to enhance its processing\nspeed. Employing RoBERTa on a GPU, as opposed to a CPU,\nyields a substantial reduction in execution time, achieving\nan overall speedup factor of approximately 26. The GPU\ntests were performed on the NVIDIA Tesla T4 GPU com-\nputing environment available within Google Colab’s free\ntier. Although the XLex-based model can be parallelized, its\ncurrent implementation lacks the necessary capabilities for\nparallel processing. As a result, evaluating its performance\nin a GPU environment would lead to an unfair comparison.\nConsequently, Table 17 presents results only for the CPU\ncomparison. The parallelization of the XLex-based model\ngoes beyond the scope of this paper; however, it can serve\nas a potential avenue for future work.\nIn the GPU environment, we perform the word extraction\nprocess using SHAP and transformer models. The word ex-\ntraction process with the RoBERTa and FinBERT models on\n9202 sentences from the nasdaq dataset took approximately\n23 and 10 hours, respectively. The extended time required\nfor word extraction is primarily attributed to the slow perfor-\nmance and time-consuming nature of SHAP’s operations.\nThe XLex methodology provides flexibility in selecting\nthe underlying transformer model, allowing for the easy inte-\ngration of any preferred model. Thus, we were also interested\nin assessing the speed performance of the FinBERT model.\nWe conducted tests in the same CPU environment. Table\n17 shows that the FinBERT model leads to better execution\nspeeds than the RoBERTa model. However, the FinBERT\nmodel exhibits a smaller size compared to RoBERTa. Sim-\nilarly to RoBERTa, the FinBERT model also underwent\ntesting on a GPU, resulting in a nearly 12-fold improvement\nin execution speed compared to running it on a CPU. As\nexpected, the XLex-based model outperforms the FinBERT\nmodel in terms of speed.\nThe second important aspect of the lexicon-based model\nis the size. The model size is an important factor to con-\nsider when deciding which model to be used in production\nsystems. Transformer models are trained on large datasets\n22 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 7: Confusion matrix showcasing the performance\nof the XLex-based model (using RoBERTa) based on the\ncombined XLex+LM lexicon. The model is constructed\nusing the lexicon created with the nasdaq dataset as its\nsource dataset. The model evaluation is performed on the\nfinancial_phrase_bank dataset. This model achieves the\nhighest accuracy among all XLex+LM models tested, achiev-\ning an accuracy rate of 84.3%.\nand are often larger than the free disk space available on\nresource-constrained devices. For the lexicon-based model\nproposed in Section V, the model size is actually repre-\nsented by the size of the lexicon. The size comparison\nbetween the RoBERTa transformer model and the lexicon-\nbased model is shown in Table 17. The difference in size\nis considerable. As can be seen, the lexicon-based model is\nabout three orders of magnitude smaller than the RoBERTa\ntransformer model. The XLex-based model exhibits an\nidentical size for each specific source dataset, with the\nXLex-based model registering sizes of 363KB, 202KB, and\n233KB for the nasdaq, financial_phrase_bank (fpb), and\nfiqa_fpb_sentfin_neutral source datasets, respectively.\nAdditionally, the RoBERTa-based model maintains a size\nof 1.32GB, regardless of the evaluation dataset used. The\nsize of the FinBERT model is 417.8MB. Although there\nare approaches to make transformer-based models smaller\n[74], transformers are not suitable for certain use cases,\nsuch as environments with limited computational resources\nor embedded devices. On the other hand, Table 17 shows\nthat lexicon-based models have a size that is suitable for such\napplications.\nAnother important advantage of lexicon-based approaches\nis their interpretability. Lexicon-based sentiment models are\ngenerally more interpretable than transformer-based senti-\nment models because they rely on a pre-defined set of rules\nthat are easy to understand and interpret. In a lexicon-based\nsentiment model, each word is assigned a sentiment score\nbased on its associated sentiment value in the sentiment\ndictionary, and the overall sentiment of the text is calculated\nbased on the sum or average of the sentiment scores of the\nwords in the text. This makes it easy to understand why a\nparticular text was classified as positive, negative, or neutral,\nas the sentiment scores assigned to each word in the text\nare transparent and interpretable. Moreover, the sentiment\ndictionary can be customized for specific domains or use\ncases, allowing for more accurate and relevant sentiment\nanalysis. In contrast, transformer-based sentiment models are\nbased on more complex deep learning architectures that are\nmore difficult to interpret. Transformer models use large\nneural networks to learn the context and meaning of words in\na sentence and assign a sentiment score to the sentence based\non this understanding. While transformer-based sentiment\nmodels can achieve higher accuracy than dictionary-based\nmodels, the sentiment scores assigned to each word or phrase\nin the sentence are not as transparent or interpretable as they\nare generated by a complex neural network that learns its\nown set of rules based on the training data. This lack of\ninterpretability can be a limitation for applications where it is\nimportant to understand why a particular text was classified\nas positive, negative, or neutral. Transformer-based sentiment\nmodels can be useful for tasks that require a more nuanced\nunderstanding of sentiment, as they can capture the complex\nrelationships between words and the context in which they\nare used. However, it is important to note that while explain-\nable AI (XAI) methods like SHAP can provide some level of\ninterpretability for transformer-based models, they may not\nalways provide a complete understanding of how the model\nworks due to the black-box nature of its inner workings.\nAdditionally, the interpretability of XAI methods is often\nlimited by the complexity of the model and may not be able\nto fully capture the nuances of natural language. Therefore,\nit is important to use XAI methods in conjunction with\nother approaches to ensure accurate and reliable sentiment\nanalysis.\nDue to their inherent advantages, explainable lexicons\ncould potentially be used to replace standard lexicons that are\nnowadays still established in various domains. For example,\nthe proposed lexicon in this paper could be used to replace\nthe LM lexicon in the domain of finance. However, it is\nessential to use domain experts before nominating an explain-\nable lexicon as the new standard for a specific application\nor domain. The domain experts can give an expert opinion\nwhen validating the sentiment scores of its constituent words.\nThe involvement of domain experts in the lexicon review\nprocess could be a possible direction of future research as\nit could improve transparency and objectivity. Only then we\ncan have a lexicon that is not only superior in terms of speed,\nsize, and interpretability but also validated by human experts.\nThis is especially important in critical applications where the\nquality of the results directly affects people’s lives or safety.\nExamples of such applications may include not only finance\nbut also knowledge extraction in medicine, legal document\nVOLUME 4, 2016 23\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nanalysis, and risk assessment. By having an expert review\nvalidate the lexicon, the dictionary becomes more accurate\nand reliable, enhancing its usefulness and value to the users.\nIt also ensures that the lexicon is consistent with the standard\nconventions of the language while meeting the needs and\nexpectations of the intended audience.\nVII. CONCLUSION\nIn this paper, we present a novel XLex methodology that\nleverages NLP transformer models and SHAP explainability\nto automatically enhance the vocabulary coverage of the\nLoughran-McDonald (LM) lexicon in sentiment analysis sce-\nnarios for financial applications. Our results demonstrate that\nstandard domain-specific lexicons, such as the LM lexicon,\ncan be expanded in an explainable way with new words\nwithout the need for laborious annotation involvement of\nhuman experts, a process that is both expensive and time-\nconsuming.\nTo ensure the robustness of our findings, we employ\na multi-faceted validation strategy that integrates multiple\ndatasets. Specifically, we learn the lexicon on one dataset and\nsubsequently test its effectiveness on various other datasets.\nWe have conducted 22 separate experiments, and in all of\nthem, the proposed XLex methodology leads to increased\nperformance compared to LM. Specifically, we evaluated the\nXLex methodology in two separate instances: one employing\na fine-tuned RoBERTa-based model and the other utilizing a\npre-trained FinBERT model. The results yielded are largely\ncomparable, while also emphasizing the robustness of the\nXLex methodology and its effectiveness to work with various\ntransformer models.\nThe use of generated (XLex) or combined lexicons\n(XLex+LM) leads to significant improvements in sentiment\nanalysis results compared to using the manually annotated\nlexicon alone. This improvement is demonstrated by higher\naccuracy and larger vocabulary coverage, directly addressing\nthe limitations of standard, manually annotated lexicons.\nOverall, the proposed XLex methodology holds great\npromise in advancing the field of sentiment analysis, par-\nticularly in applications where interpretability is of utmost\nimportance. Unlike transformer models that rely on complex\ninner workings of neural networks, lexicon models depend\non pre-defined rules, making it easy to interpret why a par-\nticular text is classified as positive, negative, or neutral. The\nenhanced interpretability provided by explainable lexicons\nmakes them especially well-suited for critical applications\nwhere the quality of the results directly affects people’s lives\nor safety. Examples of such applications include finance,\nmedicine, legal document analysis, and risk assessment. In\nthese areas, the transparency and explainability of the anal-\nysis process are essential for building trust and ensuring the\nresponsible use of AI technologies.\nAs a future work, it would be beneficial to investigate\nthe integration of explainable lexicons with other NLP tech-\nniques, to further enhance the performance and applicability\nof sentiment analysis. It is also essential to evaluate the ro-\nbustness of explainable lexicons against various challenges,\nsuch as changes in language use, evolving domains, and the\npresence of adversarial examples.\nThe proposed methodology is general and adaptable, of-\nfering opportunities for future research to explore its appli-\ncation across other domains beyond finance. Adopting the\nXLex methodology for different domains has the potential to\nsignificantly impact various industries, enhancing the accu-\nracy and interpretability of sentiment analysis results while\nreducing the time and cost associated with manual lexicon\ndevelopment.\nVIII. ACKNOWLEDGMENT\nThis work is partially based on COST Action CA18209 –\nNexusLinguarum “European network for Web-centred lin-\nguistic data science”, supported by COST (European Coop-\neration in Science and Technology).\n24 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n.\nAPPENDIX A\nTABLE A.1: Features of the explainable lexicon after adding the XLex prefix.\nWord XLex Count\n(Selected) XLex Total XLex Count\n(Opposite) XLex Category\nXLex Sum\nSHAP Value\n(Selected)\nXLex Average\nSHAP Value\n(Selected)\nXLex Max\nSHAP Value\n(Selected)\nXLex Min\nSHAP Value\n(Selected)\nXLex Ratio\n(Selected)\nXLex Sum\nSHAP Value\n(Opposite)\nXLex Average\nSHAP Value\n(Opposite)\nabet 1 1 0 positive 0.025090 0.025090 0.025090 0.025090 1.000000 0.000000 0.000000\nabide 1 1 0 positive 0.003838 0.003838 0.003838 0.003838 1.000000 0.000000 0.000000\nabo 1 1 0 positive 0.000976 0.000976 0.000976 0.000976 1.000000 0.000000 0.000000\naboard 2 2 0 positive 0.245279 0.122640 0.210718 0.034561 1.000000 0.000000 0.000000\nabolition 1 1 0 positive 0.006430 0.006430 0.006430 0.006430 1.000000 0.000000 0.000000\nken 4 7 3 negative 0.114558 0.028640 0.086224 0.003522 0.504271 0.084463 0.028154\npeter 9 15 6 negative 0.146561 0.016285 0.068352 0.000112 0.477515 0.106909 0.017818\nuri 1 2 1 negative 0.016012 0.016012 0.016012 0.016012 0.565765 0.012289 0.012289\nmilitary 76 98 22 negative 2.958678 0.038930 0.169050 0.001473 0.773876 0.250255 0.011375\ndepth 1 2 1 positive 0.018833 0.018833 0.018833 0.018833 0.997290 0.000051 0.000051\nTABLE A.2: Features of the LM lexicon after adding the LM prefix.\nWord LM Count\n(Selected) LM Total LM Count\n(Opposite) LM Category\nLM Sum\nSHAP Value\n(Selected)\nLM Average\nSHAP Value\n(Selected)\nLM Max\nSHAP Value\n(Selected)\nLM Min\nSHAP Value\n(Selected)\nLM Ratio\n(Selected)\nLM Sum\nSHAP Value\n(Opposite)\nLM Average\nSHAP Value\n(Opposite)\nabet 1 1 0 negative 1 1 1 1 1 0 0\naccomplish 1 1 0 positive 1 1 1 1 1 0 0\nadvance 1 1 0 positive 1 1 1 1 1 0 0\nadvantage 1 1 0 positive 1 1 1 1 1 0 0\nadvantageous 1 1 0 positive 1 1 1 1 1 0 0\nwriteoff 1 1 0 negative 1 1 1 1 1 0 0\nwriteoffs 1 1 0 negative 1 1 1 1 1 0 0\nwrongful 1 1 0 negative 1 1 1 1 1 0 0\nwrongfully 1 1 0 negative 1 1 1 1 1 0 0\nwrongly 1 1 0 negative 1 1 1 1 1 0 0\nTABLE A.3: Values of selected features of the combined lexicon for words that appear in both the explainable and LM lexicon.\nWord XLex Count\n(Selected) XLex Total XLex Count\n(Opposite)\nXLex Average\nSHAP Value\n(Selected)\nXLex Source\nLM Average\nSHAP Value\n(Selected)\nLM Category\nLM Sum\nSHAP Value\n(Opposite)\nLM Source\nLM Max\nSHAP Value\n(Opposite)\nabet 1.0 1.0 0.0 0.025090 XLex 1 negative 0 LM 0\naccomplish 1.0 1.0 0.0 0.078244 XLex 1 positive 0 LM 0\nadvance 9.0 9.0 0.0 0.113294 XLex 1 positive 0 LM 0\nadvantage 7.0 7.0 0.0 0.431898 XLex 1 positive 0 LM 0\nadvantageous 1.0 1.0 0.0 0.441719 XLex 1 positive 0 LM 0\ngood 65.0 68.0 3.0 0.149197 XLex 1 positive 0 LM 0\npose 10.0 20.0 10.0 0.027118 XLex 1 negative 0 LM 0\ngain 14.0 16.0 2.0 0.157810 XLex 1 positive 0 LM 0\nevasion 2.0 3.0 1.0 0.032876 XLex 1 negative 0 LM 0\ndefeat 10.0 12.0 2.0 0.077834 XLex 1 negative 0 LM 0\nTABLE A.4: Values of selected features of the combined lexicon for words that appear in either the explainable or the LM\nlexicon.\nWord XLex Count\n(Selected) XLex Total XLex Count\n(Opposite)\nXLex Average\nSHAP Value\n(Selected)\nXLex Source\nLM Average\nSHAP Value\n(Selected)\nLM Category\nLM Sum\nSHAP Value\n(Opposite)\nLM Source\nLM Max\nSHAP Value\n(Opposite)\nabide 1.0 1.0 0.0 0.003838 XLex NaN NaN NaN NaN NaN\nabo 1.0 1.0 0.0 0.000976 XLex NaN NaN NaN NaN NaN\naboard 2.0 2.0 0.0 0.122640 XLex NaN NaN NaN NaN NaN\nabolition 1.0 1.0 0.0 0.006430 XLex NaN NaN NaN NaN NaN\nabroad 3.0 3.0 0.0 0.039073 XLex NaN NaN NaN NaN NaN\nwriteoff NaN NaN NaN NaN NaN 1 negative 0 LM 0\nwriteoffs NaN NaN NaN NaN NaN 1 negative 0 LM 0\nwrongful NaN NaN NaN NaN NaN 1 negative 0 LM 0\nwrongfully NaN NaN NaN NaN NaN 1 negative 0 LM 0\nwrongly NaN NaN NaN NaN NaN 1 negative 0 LM 0\nAPPENDIX B\nVOLUME 4, 2016 25\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE B.1: F1 score obtained across different experiments using the XLex methodology based on the RoBERTa transformer\nmodel. The highest values are represented in bold.\nSource of the lexicon Normalized Evaluation set F1 on whole dataset F1 only on the part of the dataset for\nwhich the LM-based model has an answer\nLM XLex XLex+\nLM\nLM on\nLM\nXLex on\nLM\n(XLex+LM)\non LM\nnasdaq Yes financial_phrase_bank 0.287 0.451 0.714 0.688 0.448 0.731\nnasdaq Yes fiqa_labeled_df 0.297 0.457 0.473 0.789 0.739 0.802\nnasdaq Yes fpb_fiqa 0.295 0.434 0.466 0.726 0.437 0.739\nnasdaq Yes sem_eval 0.305 0.664 0.721 0.752 0.704 0.775\nnasdaq No financial_phrase_bank 0.287 0.457 0.72 0.688 0.446 0.726\nnasdaq No fiqa_labeled_df 0.297 0.437 0.454 0.789 0.739 0.802\nnasdaq No fpb_fiqa 0.295 0.426 0.459 0.726 0.433 0.738\nnasdaq No sem_eval 0.305 0.672 0.713 0.752 0.717 0.759\nfiqa_fpb_sentfin_neutral Yes financial_phrase_bank 0.287 0.451 0.689 0.688 0.474 0.731\nfiqa_fpb_sentfin_neutral Yes fiqa_labeled_df 0.297 0.434 0.456 0.789 0.752 0.827\nfiqa_fpb_sentfin_neutral Yes fpb_fiqa 0.295 0.424 0.448 0.726 0.458 0.751\nfiqa_fpb_sentfin_neutral Yes sem_eval 0.305 0.434 0.476 0.752 0.676 0.775\nfiqa_fpb_sentfin_neutral No financial_phrase_bank 0.287 0.448 0.694 0.688 0.463 0.731\nfiqa_fpb_sentfin_neutral No fiqa_labeled_df 0.297 0.416 0.439 0.789 0.747 0.827\nfiqa_fpb_sentfin_neutral No fpb_fiqa 0.295 0.425 0.448 0.726 0.457 0.748\nfiqa_fpb_sentfin_neutral No sem_eval 0.305 0.434 0.48 0.752 0.66 0.775\nfinancial_phrase_bank Yes fiqa_labeled_df 0.297 0.409 0.447 0.789 0.688 0.84\nfinancial_phrase_bank Yes fpb_fiqa 0.295 0.4 0.43 0.726 0.428 0.739\nfinancial_phrase_bank Yes sem_eval 0.305 0.416 0.445 0.752 0.462 0.767\nfinancial_phrase_bank No fiqa_labeled_df 0.297 0.43 0.456 0.789 0.692 0.802\nfinancial_phrase_bank No fpb_fiqa 0.295 0.413 0.439 0.726 0.436 0.738\nfinancial_phrase_bank No sem_eval 0.305 0.416 0.444 0.752 0.467 0.767\nTABLE B.2: MCC obtained across different experiments using the XLex methodology based on the RoBERTa transformer\nmodel. The highest values are represented in bold.\nSource of the lexicon Normalized Evaluation set MCC on whole dataset MCC only on the part of the dataset for\nwhich the LM-based model has an answer\nLM XLex XLex+\nLM\nLM on\nLM\nXLex on\nLM\n(XLex+LM)\non LM\nnasdaq Yes financial_phrase_bank 0.192 0.362 0.453 0.489 0.364 0.539\nnasdaq Yes fiqa_labeled_df 0.216 0.372 0.432 0.594 0.492 0.615\nnasdaq Yes fpb_fiqa 0.209 0.306 0.418 0.519 0.312 0.524\nnasdaq Yes sem_eval 0.265 0.332 0.477 0.567 0.417 0.601\nnasdaq No financial_phrase_bank 0.192 0.383 0.471 0.489 0.359 0.531\nnasdaq No fiqa_labeled_df 0.216 0.313 0.374 0.594 0.492 0.615\nnasdaq No fpb_fiqa 0.209 0.281 0.399 0.519 0.301 0.525\nnasdaq No sem_eval 0.265 0.349 0.465 0.567 0.437 0.578\nfiqa_fpb_sentfin_neutral Yes financial_phrase_bank 0.192 0.392 0.434 0.489 0.46 0.539\nfiqa_fpb_sentfin_neutral Yes fiqa_labeled_df 0.216 0.303 0.371 0.594 0.523 0.659\nfiqa_fpb_sentfin_neutral Yes fpb_fiqa 0.209 0.286 0.376 0.519 0.375 0.54\nfiqa_fpb_sentfin_neutral Yes sem_eval 0.265 0.308 0.466 0.567 0.355 0.593\nfiqa_fpb_sentfin_neutral No financial_phrase_bank 0.192 0.376 0.443 0.489 0.412 0.539\nfiqa_fpb_sentfin_neutral No fiqa_labeled_df 0.216 0.249 0.32 0.594 0.499 0.659\nfiqa_fpb_sentfin_neutral No fpb_fiqa 0.209 0.288 0.373 0.519 0.373 0.531\nfiqa_fpb_sentfin_neutral No sem_eval 0.265 0.31 0.481 0.567 0.322 0.593\nfinancial_phrase_bank Yes fiqa_labeled_df 0.216 0.25 0.363 0.594 0.375 0.683\nfinancial_phrase_bank Yes fpb_fiqa 0.209 0.245 0.351 0.519 0.303 0.528\nfinancial_phrase_bank Yes sem_eval 0.265 0.296 0.422 0.567 0.386 0.581\nfinancial_phrase_bank No fiqa_labeled_df 0.216 0.306 0.398 0.594 0.385 0.615\nfinancial_phrase_bank No fpb_fiqa 0.209 0.284 0.383 0.519 0.325 0.529\nfinancial_phrase_bank No sem_eval 0.265 0.296 0.418 0.567 0.4 0.581\n26 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE B.3: F1 score obtained across different experiments using the XLex methodology based on the FinBERT transformer\nmodel. The highest values are represented in bold.\nSource of the lexicon Normalized Evaluation set F1 on whole dataset F1 only on the part of the dataset for\nwhich the LM-based model has an answer\nLM XLex XLex+\nLM\nLM on\nLM\nXLex on\nLM\n(XLex+LM)\non LM\nnasdaq Yes financial_phrase_bank 0.287 0.421 0.432 0.688 0.441 0.694\nnasdaq Yes fiqa_labeled_df 0.297 0.497 0.505 0.789 0.802 0.836\nnasdaq Yes fpb_fiqa 0.295 0.448 0.458 0.726 0.477 0.74\nnasdaq Yes sem_eval 0.305 0.684 0.693 0.752 0.774 0.783\nnasdaq No financial_phrase_bank 0.287 0.389 0.408 0.688 0.419 0.696\nnasdaq No fiqa_labeled_df 0.297 0.498 0.5 0.789 0.815 0.823\nnasdaq No fpb_fiqa 0.295 0.428 0.442 0.726 0.464 0.74\nnasdaq No sem_eval 0.305 0.676 0.692 0.752 0.766 0.791\nfiqa_fpb_sentfin_neutral Yes financial_phrase_bank 0.287 0.708 0.724 0.688 0.673 0.702\nfiqa_fpb_sentfin_neutral Yes fiqa_labeled_df 0.297 0.522 0.537 0.789 0.765 0.823\nfiqa_fpb_sentfin_neutral Yes fpb_fiqa 0.295 0.462 0.484 0.726 0.458 0.75\nfiqa_fpb_sentfin_neutral Yes sem_eval 0.305 0.69 0.741 0.752 0.715 0.798\nfiqa_fpb_sentfin_neutral No financial_phrase_bank 0.287 0.696 0.711 0.688 0.673 0.702\nfiqa_fpb_sentfin_neutral No fiqa_labeled_df 0.297 0.518 0.53 0.789 0.765 0.811\nfiqa_fpb_sentfin_neutral No fpb_fiqa 0.295 0.461 0.483 0.726 0.459 0.751\nfiqa_fpb_sentfin_neutral No sem_eval 0.305 0.696 0.742 0.752 0.724 0.806\nfinancial_phrase_bank Yes fiqa_labeled_df 0.297 0.486 0.511 0.789 0.471 0.814\nfinancial_phrase_bank Yes fpb_fiqa 0.295 0.461 0.486 0.726 0.453 0.739\nfinancial_phrase_bank Yes sem_eval 0.305 0.432 0.488 0.752 0.452 0.814\nfinancial_phrase_bank No fiqa_labeled_df 0.297 0.476 0.496 0.789 0.479 0.802\nfinancial_phrase_bank No fpb_fiqa 0.295 0.448 0.477 0.726 0.446 0.74\nfinancial_phrase_bank No sem_eval 0.305 0.429 0.488 0.752 0.452 0.806\nTABLE B.4: MCC obtained across different experiments using the XLex methodology based on the FinBERT transformer\nmodel. The highest values are represented in bold.\nSource of the lexicon Normalized Evaluation set MCC on whole dataset MCC only on the part of the dataset for\nwhich the LM-based model has an answer\nLM XLex XLex+\nLM\nLM on\nLM\nXLex on\nLM\n(XLex+LM)\non LM\nnasdaq Yes financial_phrase_bank 0.192 0.315 0.352 0.489 0.414 0.489\nnasdaq Yes fiqa_labeled_df 0.216 0.497 0.525 0.594 0.604 0.673\nnasdaq Yes fpb_fiqa 0.209 0.375 0.415 0.519 0.451 0.526\nnasdaq Yes sem_eval 0.265 0.417 0.452 0.567 0.565 0.613\nnasdaq No financial_phrase_bank 0.192 0.279 0.326 0.489 0.378 0.493\nnasdaq No fiqa_labeled_df 0.216 0.503 0.516 0.594 0.632 0.648\nnasdaq No fpb_fiqa 0.209 0.334 0.381 0.519 0.425 0.526\nnasdaq No sem_eval 0.265 0.415 0.465 0.567 0.546 0.624\nfiqa_fpb_sentfin_neutral Yes financial_phrase_bank 0.192 0.454 0.494 0.489 0.418 0.5\nfiqa_fpb_sentfin_neutral Yes fiqa_labeled_df 0.216 0.565 0.608 0.594 0.53 0.648\nfiqa_fpb_sentfin_neutral Yes fpb_fiqa 0.209 0.394 0.47 0.519 0.386 0.54\nfiqa_fpb_sentfin_neutral Yes sem_eval 0.265 0.39 0.511 0.567 0.43 0.636\nfiqa_fpb_sentfin_neutral No financial_phrase_bank 0.192 0.441 0.478 0.489 0.424 0.5\nfiqa_fpb_sentfin_neutral No fiqa_labeled_df 0.216 0.557 0.592 0.594 0.53 0.624\nfiqa_fpb_sentfin_neutral No fpb_fiqa 0.209 0.398 0.472 0.519 0.392 0.543\nfiqa_fpb_sentfin_neutral No sem_eval 0.265 0.407 0.517 0.567 0.451 0.648\nfinancial_phrase_bank Yes fiqa_labeled_df 0.216 0.456 0.531 0.594 0.465 0.637\nfinancial_phrase_bank Yes fpb_fiqa 0.209 0.382 0.468 0.519 0.361 0.524\nfinancial_phrase_bank Yes sem_eval 0.265 0.297 0.483 0.567 0.371 0.653\nfinancial_phrase_bank No fiqa_labeled_df 0.216 0.429 0.489 0.594 0.5 0.615\nfinancial_phrase_bank No fpb_fiqa 0.209 0.346 0.443 0.519 0.341 0.523\nfinancial_phrase_bank No sem_eval 0.265 0.288 0.483 0.567 0.388 0.648\nVOLUME 4, 2016 27\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nREFERENCES\n[1] M. Hasan, J. Popp, J. Oláh et al., “Current landscape and influence of big\ndata on finance,” Journal of Big Data, vol. 7, no. 1, pp. 1–17, 2020.\n[2] I. Goldstein, C. S. Spatt, and M. Ye, “Big data in finance,” The Review of\nFinancial Studies, vol. 34, no. 7, pp. 3213–3225, 2021.\n[3] N. Mohamed and J. Al-Jaroodi, “Real-time big data analytics: Applica-\ntions and challenges,” in 2014 international conference on high perfor-\nmance computing & simulation (HPCS). IEEE, 2014, pp. 305–310.\n[4] V . Ravi and S. Kamaruddin, “Big data analytics enabled smart financial\nservices: opportunities and challenges,” inBig Data Analytics: 5th Interna-\ntional Conference, BDA 2017, Hyderabad, India, December 12-15, 2017,\nProceedings 5. Springer, 2017, pp. 15–39.\n[5] M. Cao, R. Chychyla, and T. Stewart, “Big data analytics in financial\nstatement audits,” Accounting Horizons, vol. 29, no. 2, pp. 423–429, 2015.\n[6] J. Smailovi ´c, M. Grˇcar, N. Lavraˇc, and M. Žnidaršiˇc, “Predictive sentiment\nanalysis of tweets: A stock market application,” in Human-Computer\nInteraction and Knowledge Discovery in Complex, Unstructured, Big\nData: Third International Workshop, HCI-KDD 2013, Held at SouthCHI\n2013, Maribor, Slovenia, July 1-3, 2013. Proceedings . Springer, 2013,\npp. 77–88.\n[7] A. Derakhshan and H. Beigy, “Sentiment analysis on stock social media\nfor stock price movement prediction,” Engineering Applications of Artifi-\ncial Intelligence, vol. 85, pp. 569–578, 2019.\n[8] R. Ren, D. D. Wu, and T. Liu, “Forecasting stock market movement\ndirection using sentiment analysis and support vector machine,” IEEE\nSystems Journal, vol. 13, no. 1, pp. 760–770, 2018.\n[9] R. Yang, L. Yu, Y . Zhao, H. Yu, G. Xu, Y . Wu, and Z. Liu, “Big data\nanalytics for financial market volatility forecast based on support vector\nmachine,” International Journal of Information Management, vol. 50, pp.\n452–462, 2020.\n[10] F.-T. Tsai, H.-M. Lu, and M.-W. Hung, “The effects of news sentiment and\ncoverage on credit rating analysis,” 2010.\n[11] S. Gül, Ö. Kabak, and I. Topcu, “A multiple criteria credit rating approach\nutilizing social media data,”Data & Knowledge Engineering, vol. 116, pp.\n80–99, 2018.\n[12] H.-M. Lu, F.-T. Tsai, H. Chen, M.-W. Hung, and S.-H. Li, “Credit rating\nchange modeling using news and financial ratios,” ACM Transactions on\nManagement Information Systems (TMIS), vol. 3, no. 3, pp. 1–30, 2012.\n[13] D. Zhang, W. Xu, Y . Zhu, and X. Zhang, “Can sentiment analysis help\nmimic decision-making process of loan granting? a novel credit risk eval-\nuation approach using gmkl model,” in 2015 48th Hawaii International\nConference on System Sciences. IEEE, 2015, pp. 949–958.\n[14] B. Yoon, Y . Jeong, and S. Kim, “Detecting a risk signal in stock investment\nthrough opinion mining and graph-based semi-supervised learning,”IEEE\nAccess, vol. 8, pp. 161 943–161 957, 2020.\n[15] J. R. McColl-Kennedy, M. Zaki, K. N. Lemon, F. Urmetzer, and A. Neely,\n“Gaining customer experience insights that matter,” Journal of service\nresearch, vol. 22, no. 1, pp. 8–26, 2019.\n[16] L. Ziora, “The sentiment analysis as a tool of business analytics in\ncontemporary organizations,”Studia Ekonomiczne, vol. 281, pp. 234–241,\n2016.\n[17] H. Mili, I. Benzarti, M.-J. Meurs, A. Obaid, J. Gonzalez-Huerta, N. Haj-\nSalem, and A. Boubaker, “Context aware customer experience manage-\nment: A development framework based on ontologies and computational\nintelligence,” Sentiment Analysis and Ontology Engineering: An Environ-\nment of Computational Intelligence, pp. 273–311, 2016.\n[18] X. Tian, J. S. He, and M. Han, “Data-driven approaches in fintech: a\nsurvey,”Information Discovery and Delivery, 2021.\n[19] C.-C. Chen, H.-H. Huang, and H.-H. Chen, “Fintech applications,” inFrom\nOpinion Mining to Financial Argument Mining. Springer, 2021, pp. 73–\n87.\n[20] H. Liu, I. Chatterjee, M. Zhou, X. S. Lu, and A. Abusorrah, “Aspect-\nbased sentiment analysis: A survey of deep learning methods,” IEEE\nTransactions on Computational Social Systems , vol. 7, no. 6, pp. 1358–\n1375, 2020.\n[21] F. Benedetto and A. Tedeschi, “Big data sentiment analysis for brand\nmonitoring in social media streams by cloud computing,” Sentiment\nAnalysis and Ontology Engineering: An Environment of Computational\nIntelligence, pp. 341–377, 2016.\n[22] D. Alessia, F. Ferri, P. Grifoni, and T. Guzzo, “Approaches, tools and\napplications for sentiment analysis implementation,”International Journal\nof Computer Applications, vol. 125, no. 3, 2015.\n[23] M. Taboada, J. Brooke, M. Tofiloski, K. V oll, and M. Stede, “Lexicon-\nbased methods for sentiment analysis,”Computational linguistics, vol. 37,\nno. 2, pp. 267–307, 2011.\n[24] S. Taj, B. B. Shaikh, and A. F. Meghji, “Sentiment analysis of news arti-\ncles: a lexicon based approach,” in 2019 2nd international conference on\ncomputing, mathematics and engineering technologies (iCoMET). IEEE,\n2019, pp. 1–5.\n[25] M. Wankhade, A. C. S. Rao, and C. Kulkarni, “A survey on sentiment\nanalysis methods, applications, and challenges,” Artificial Intelligence\nReview, vol. 55, no. 7, pp. 5731–5780, 2022.\n[26] T. Loughran and B. McDonald, “When is a liability not a liability? textual\nanalysis, dictionaries, and 10-ks,” The Journal of finance , vol. 66, no. 1,\npp. 35–65, 2011.\n[27] E. Boiy and M.-F. Moens, “A machine learning approach to sentiment\nanalysis in multilingual web texts,”Information retrieval, vol. 12, pp. 526–\n558, 2009.\n[28] A. Sharma and S. Dey, “A comparative study of feature selection and\nmachine learning techniques for sentiment analysis,” inProceedings of the\n2012 ACM research in applied computation symposium, 2012, pp. 1–7.\n[29] S. Malviya, A. K. Tiwari, R. Srivastava, and V . Tiwari, “Machine learning\ntechniques for sentiment analysis: A review,” SAMRIDDHI: A Journal of\nPhysical Sciences, Engineering and Technology , vol. 12, no. 02, pp. 72–\n78, 2020.\n[30] M. Neethu and R. Rajasree, “Sentiment analysis in twitter using machine\nlearning techniques,” in 2013 fourth international conference on comput-\ning, communications and networking technologies (ICCCNT) . IEEE,\n2013, pp. 1–5.\n[31] L. Zhang, S. Wang, and B. Liu, “Deep learning for sentiment analysis:\nA survey,” Wiley Interdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, vol. 8, no. 4, p. e1253, 2018.\n[32] A. Yadav and D. K. Vishwakarma, “Sentiment analysis using deep learning\narchitectures: a review,” Artificial Intelligence Review, vol. 53, no. 6, pp.\n4335–4385, 2020.\n[33] N. C. Dang, M. N. Moreno-García, and F. De la Prieta, “Sentiment analysis\nbased on deep learning: A comparative study,”Electronics, vol. 9, no. 3, p.\n483, 2020.\n[34] D. Tang, B. Qin, and T. Liu, “Deep learning for sentiment analysis:\nsuccessful approaches and future challenges,” Wiley Interdisciplinary Re-\nviews: Data Mining and Knowledge Discovery, vol. 5, no. 6, pp. 292–303,\n2015.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[36] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[37] K. Mishev, A. Gjorgjevikj, I. V odenska, L. T. Chitkushev, and D. Trajanov,\n“Evaluation of sentiment analysis in finance: from lexicons to transform-\ners,” IEEE Access, vol. 8, pp. 131 662–131 682, 2020.\n[38] X. S. Huang, F. Perez, J. Ba, and M. V olkovs, “Improving transformer\noptimization through better initialization,” in International Conference on\nMachine Learning. PMLR, 2020, pp. 4475–4483.\n[39] S. Lundberg and S.-I. Lee, “A unified approach to interpreting model\npredictions,” arXiv preprint arXiv:1705.07874, 2017.\n[40] T. Loughran and B. McDonald, “Textual analysis in accounting and\nfinance: A survey,” Journal of Accounting Research , vol. 54, no. 4, pp.\n1187–1230, 2016.\n[41] P. C. Tetlock, “Giving content to investor sentiment: The role of media in\nthe stock market,” The Journal of finance, vol. 62, no. 3, pp. 1139–1168,\n2007.\n[42] C. Dougal, J. Engelberg, D. Garcia, and C. A. Parsons, “Journalists and\nthe stock market,” The Review of Financial Studies , vol. 25, no. 3, pp.\n639–679, 2012.\n[43] U. G. Gurun and A. W. Butler, “Don’t believe the hype: Local media slant,\nlocal advertising, and firm value,” The Journal of Finance, vol. 67, no. 2,\npp. 561–598, 2012.\n[44] L. Dodevska, V . Petreski, K. Mishev, A. Gjorgjevikj, I. V odenska,\nL. Chitkushev, and D. Trajanov, “Predicting companies stock price di-\nrection by using sentiment analysis of news articles,” in Proceedings\nof the 15th Annual International Conference on Computer Science and\nEducation in Computer Science, 2019, pp. 37–42.\n[45] T. Loughran and B. McDonald, “Measuring readability in financial disclo-\nsures,” the Journal of Finance, vol. 69, no. 4, pp. 1643–1671, 2014.\n28 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[46] S. Krishnamoorthy, “Sentiment analysis of financial news articles using\nperformance indicators,” Knowledge and Information Systems , vol. 56,\nno. 2, pp. 373–394, 2018.\n[47] P. J. Stone, D. C. Dunphy, and M. S. Smith, “The general inquirer: A\ncomputer approach to content analysis.” 1966.\n[48] S. M. Mohammad, S. Kiritchenko, and X. Zhu, “Nrc-canada: Build-\ning the state-of-the-art in sentiment analysis of tweets,” arXiv preprint\narXiv:1308.6242, 2013.\n[49] D. T. V o and Y . Zhang, “Don’t count, predict! an automatic approach to\nlearning sentiment lexicons for short text,” in Proceedings of the 54th\nAnnual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), 2016, pp. 219–224.\n[50] F. Viegas, M. S. Alvim, S. Canuto, T. Rosa, M. A. Gonçalves, and\nL. Rocha, “Exploiting semantic relationships for unsupervised expansion\nof sentiment lexicons,” Information Systems, vol. 94, p. 101606, 2020.\n[51] T. Bos and F. Frasincar, “Automatically building financial sentiment lex-\nicons while accounting for negation,” Cognitive Computation, pp. 1–19,\n2021.\n[52] H. Saif, Y . He, M. Fernandez, and H. Alani, “Adapting sentiment lexicons\nusing contextual semantics for sentiment analysis of twitter,” in European\nSemantic Web Conference. Springer, 2014, pp. 54–63.\n[53] H. Kanayama and T. Nasukawa, “Fully automatic lexicon expansion for\ndomain-oriented sentiment analysis,” in Proceedings of the 2006 confer-\nence on empirical methods in natural language processing, 2006, pp. 355–\n363.\n[54] N. Kaji and M. Kitsuregawa, “Building lexicon for sentiment analysis\nfrom massive collection of html documents,” in Proceedings of the 2007\njoint conference on empirical methods in natural language processing\nand computational natural language learning (EMNLP-CoNLL) , 2007,\npp. 1075–1083.\n[55] ——, “Automatic construction of polarity-tagged corpus from html docu-\nments,” inProceedings of the COLING/ACL 2006 Main Conference Poster\nSessions, 2006, pp. 452–459.\n[56] W. L. Hamilton, K. Clark, J. Leskovec, and D. Jurafsky, “Inducing domain-\nspecific sentiment lexicons from unlabeled corpora,” in Proceedings of\nthe conference on empirical methods in natural language processing.\nconference on empirical methods in natural language processing , vol.\n2016. NIH Public Access, 2016, p. 595.\n[57] O. Araque, G. Zhu, and C. A. Iglesias, “A semantic similarity-based\nperspective of affect lexicons for sentiment analysis,” Knowledge-Based\nSystems, vol. 165, pp. 346–359, 2019.\n[58] W. Zhao, T. Joshi, V . N. Nair, and A. Sudjianto, “Shap values\nfor explaining cnn-based text classification models,” arXiv preprint\narXiv:2008.11825, 2020.\n[59] K. E. Mokhtari, B. P. Higdon, and A. Ba¸ sar, “Interpreting financial time\nseries with shap values,” in Proceedings of the 29th Annual International\nConference on Computer Science and Software Engineering , 2019, pp.\n166–172.\n[60] X. Xiaomao, Z. Xudong, and W. Yuanfang, “A comparison of feature\nselection methodology for solving classification problems in finance,” in\nJournal of Physics: Conference Series, vol. 1284, no. 1. IOP Publishing,\n2019, p. 012026.\n[61] M. Rizinski, H. Peshov, K. Mishev, L. T. Chitkushev, I. V odenska, and\nD. Trajanov, “Ethically responsible machine learning in fintech,” IEEE\nAccess, vol. 10, pp. 97 531–97 554, 2022.\n[62] E. Kokalj, B. Škrlj, N. Lavra ˇc, S. Pollak, and M. Robnik-Šikonja, “Bert\nmeets shapley: Extending shap explanations to transformer-based classi-\nfiers,” in Proceedings of the EACL Hackashop on News Media Content\nAnalysis and Automated Report Generation, 2021, pp. 16–21.\n[63] S. Consoli, L. Barbaglia, and S. Manzan, “Fine-grained, aspect-based\nsentiment analysis on economic and financial lexicon,” Knowledge-Based\nSystems, vol. 247, p. 108781, 2022.\n[64] A. Moreno-Ortiz, J. Fernández-Cruz, and C. P. C. Hernández, “Design\nand evaluation of sentiecon: A fine-grained economic/financial sentiment\nlexicon from a corpus of business news,” in Proceedings of The 12th\nLanguage Resources and Evaluation Conference, 2020, pp. 5065–5072.\n[65] M. Yekrangi and N. Abdolvand, “Financial markets sentiment analysis:\nDeveloping a specialized lexicon,” Journal of Intelligent Information\nSystems, vol. 57, pp. 127–146, 2021.\n[66] J. Fang and B. Chen, “Incorporating lexicon knowledge into svm learning\nto improve sentiment classification,” in Proceedings of the workshop on\nsentiment analysis where AI meets psychology (SAAIP 2011) , 2011, pp.\n94–100.\n[67] R. Catelli, S. Pelosi, and M. Esposito, “Lexicon-based vs. bert-based\nsentiment analysis: A comparative study in italian,” Electronics, vol. 11,\nno. 3, p. 374, 2022.\n[68] A. H. Huang, H. Wang, and Y . Yang, “Finbert: A large language model\nfor extracting information from financial text,” Contemporary Accounting\nResearch, vol. 40, no. 2, pp. 806–841, 2023.\n[69] P. Malo, A. Sinha, P. Takala, O. Ahlgren, and I. Lappalainen, “Learning\nthe roles of directional expressions and domain concepts in financial news\nanalysis,” in 2013 IEEE 13th International Conference on Data Mining\nWorkshops. IEEE, 2013, pp. 945–954.\n[70] K. Cortis, A. Freitas, T. Daudert, M. Huerlimann, M. Zarrouk, S. Hand-\nschuh, and B. Davis, “Semeval-2017 task 5: Fine-grained sentiment anal-\nysis on financial microblogs and news.” Association for Computational\nLinguistics (ACL), 2017.\n[71] S. Mazzanti, “Shap values explained exactly how you wished some-\none explained to you,” https://towardsdatascience.com/shap-explained-\nthe-way-i-wish-someone-explained-it-to-me-ab81cc69ef30, 2020, [On-\nline; accessed 05-July-2021].\n[72] S. Lundberg, “SHapley Additive exPlanations,”\nhttps://github.com/slundberg/shap, 2018, [Online; accessed 29-Jan-\n2023].\n[73] M. Honnibal and I. Montani, “spaCy meets Transformers: Fine-tune\nBERT, XLNet and GPT-2,” https://explosion.ai/blog/spacy-transformers,\n2019, [Online; accessed 10-February-2023].\n[74] N. Lathia, “When is a neural net too big for production?” https://neal-\nlathia.medium.com/when-is-a-neural-net-too-big-for-production-\n4315452193ef, 2019, [Online; accessed 10-March-2023].\nMARYAN RIZINSKIreceived the B.S. and M.S.\ndegrees in electrical engineering and informa-\ntion technologies from University Ss. Cyril and\nMethodius in Skopje, where he is a Ph.D. can-\ndidate in computer science. He is currently an\nengineering manager at Bosch, with over ten years\nof industry experience leading globally-distributed\nsoftware engineering teams. His expertise spans\nmultiple aspects of the software project lifecycle\nmanagement, from planning, requirement gather-\ning, and analysis, estimations to driving delivery, rollout, and troubleshoot-\ning for international customers. Throughout his professional career, he has\nmanaged the implementation of Internet of Things (IoT) and fiber-optics\ninfrastructure projects and has been mentoring and consulting startup IT\ncompanies. He is also a lecturer of computer science at Boston University’s\nMetropolitan College, where he is teaching and facilitating networking and\ndata science classes. His doctoral research focuses on novel approaches for\nusing machine learning (ML) and natural language processing (NLP) in the\nfinancial industry and other related areas. His research aims to enable more\naccurate decision-making and address fundamental problems of improving\nthe explainability of deep-learning models and addressing ML-related ethi-\ncal challenges in finance applications. His past research interests focused on\ncomputer networking, wireless communications, and new Internet and IoT\narchitectures.\nHRISTIJAN PESHOV received the Bachelor of\nScience degree in software engineering and infor-\nmation systems from the Faculty of Computer Sci-\nence and Engineering, Saints Cyril and Methodius\nUniversity in Skopje, in 2022. He also works as a\nSoftware Engineer. His research interests include\ndata science, machine learning, explainable AI,\nnatural language processing, and network analy-\nsis.\nVOLUME 4, 2016 29\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nKOSTADIN MISHEV received the bachelor’s\ndegree in informatics and computer engineering\nand the master’s degree in computer networks\nand e-technologies degree from Saints Cyril and\nMethodius University, Skopje, in 2013 and 2016,\nrespectively, where he is currently pursuing the\nPh.D. degree. He is also a Teaching and a Research\nAssistant with the Faculty of Computer Science\nand Engineering, Saints Cyril and Methodius Uni-\nversity. His research interests include Data Sci-\nence, Natural Language Processing, Semantic Web, Web technologies, and\nComputer Networks.\nMILOS JOVANOVIKis an Associate Professor at\nthe Faculty of Computer Science and Engineer-\ning, at the Ss. Cyril and Methodius University\nin Skopje. He’s also a Senior R&D Knowledge\nGraphs Engineer at OpenLink Software, London,\nUK. He obtained his Ph.D. in 2016 at the Ss.\nCyril and Methodius University in Skopje, in the\nfield of Computer Science and Engineering, with\na doctoral thesis in the domain of Linked Data.\nHe has published over 50 scientific papers and has\nparticipated in over 30 research projects on international and national levels.\nHis main research interests include Knowledge Graphs, Linked Data, Open\nData, and Data Science.\nDIMITAR TRAJANOV(Member, IEEE) received\na Ph.D. degree in computer science. He is a Full\nprofessor at the Faculty of Computer Science\nand Engineering - ss. Cyril and Methodius Uni-\nversity – Skopje and Visiting Research Profes-\nsor at Boston University. From March 2011 until\nSeptember 2015, he was the founding Dean of\nthe Faculty of Computer Science and Engineering,\nand in his tenure, the faculty became the largest\ntechnical Faculty in Macedonia. Dimitar Trajanov\nis the leader of the Regional Social Innovation Hub, established in 2013\nas a cooperation between UNDP and the Faculty of Computer Science\nand Engineering. Dimitar Trajanov is the author of more than 180 journal\nand conference papers and seven books. He has been involved in more\nthan 70 research and industry projects, of which in more than 40 projects\nas a project leader. His research interests include Data Science, Machine\nLearning, NLP, FinTech, Semantic Web, Open Data, Social Innovation, e-\ncommerce, Technology for Development, and Climate Change.\n30 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349970\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}