{
  "title": "Does Syntactic Knowledge in Multilingual Language Models Transfer Across Languages?",
  "url": "https://openalex.org/W2913407411",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2912270483",
      "name": "Prajit Dhar",
      "affiliations": [
        "Leiden University"
      ]
    },
    {
      "id": "https://openalex.org/A2139068014",
      "name": "Arianna Bisazza",
      "affiliations": [
        "Leiden University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2351252181",
    "https://openalex.org/W2121352910",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W2963549838",
    "https://openalex.org/W2963247703",
    "https://openalex.org/W2767862400",
    "https://openalex.org/W2964170290",
    "https://openalex.org/W2150257418",
    "https://openalex.org/W2357843841",
    "https://openalex.org/W4300062960",
    "https://openalex.org/W1530224645",
    "https://openalex.org/W2056977501",
    "https://openalex.org/W2251743902",
    "https://openalex.org/W2197284999",
    "https://openalex.org/W2752630748",
    "https://openalex.org/W2566957588",
    "https://openalex.org/W2962705709",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2963633299",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2963826397"
  ],
  "abstract": "Recent work has shown that neural models can be successfully trained on multiple languages simultaneously. We investigate whether such models learn to share and exploit common syntactic knowledge among the languages on which they are trained. This extended abstract presents our preliminary results",
  "full_text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 374–377\nBrussels, Belgium, November 1, 2018.c⃝2018 Association for Computational Linguistics\n374\nDoes Syntactic Knowledge in Multilingual Language Models\nTransfer Across Languages?\nPrajit Dhar Arianna Bisazza\nLeiden Institute of Advanced Computer Science\nLeiden University, The Netherlands\n{p.dhar,a.bisazza}@liacs.leidenuniv.nl\nAbstract\nRecent work has shown that neural models can\nbe successfully trained on multiple languages\nsimultaneously. We investigate whether such\nmodels learn to share and exploit common\nsyntactic knowledge among the languages on\nwhich they are trained. This extended abstract\npresents our preliminary results.\n1 Introduction\nRecent work has shown that state-of-the-art neu-\nral models of language and translation can be suc-\ncessfully trained on multiple languages simulta-\nneously without changing the model architecture\n( ¨Ostling and Tiedemann, 2017; Johnson et al.,\n2017). In some cases this leads to improved per-\nformance compared to models only trained on\na speciﬁc language, suggesting that multilingual\nmodels learn to share useful knowledge cross-\nlingually through their learned representations.\nWhile a large body of research exists on the mul-\ntilingual mind, the mechanisms explaining knowl-\nedge sharing in computational multilingual mod-\nels remain largely unknown: What kind of knowl-\nedge is shared among languages? Do multilingual\nmodels mostly beneﬁt from a better modeling of\nlexical entries or do they also learn to share more\nabstract linguistic categories?\nWe focus on the case of language models (LM)\ntrained on two languages, one of which (L1)\nis over-resourced with respect to the other (L2),\nand investigate whether the syntactic knowledge\nlearned for L1 is transferred to L2. To this end\nwe use the long-distance agreement benchmark re-\ncently introduced by Gulordava et al. (2018).\n2 Background\nThe recent advances in neural networks have\nopened the way to the design of architecturally\nsimple multilingual models for various NLP tasks,\nsuch as language modeling or next word predic-\ntion (Tsvetkov et al., 2016; ¨Ostling and Tiede-\nmann, 2017; Malaviya et al., 2017; Tiedemann,\n2018), translation (Dong et al., 2015; Zoph et al.,\n2016; Firat et al., 2016; Johnson et al., 2017),\nmorphological reinﬂection (Kann et al., 2017)\nand more (Bjerva, 2017). A practical beneﬁt\nof training models multilingually is to transfer\nknowledge from high-resource languages to low-\nresource ones and improve task performance in\nthe latter. Here we aim at understanding how lin-\nguistic knowledge is transferred among languages,\nspeciﬁcally at the syntactic level, which to our\nknowledge has not been studied so far.\nAssessing the syntactic abilities of monolin-\ngual neural LMs trained without explicit super-\nvision has been the focus of several recent stud-\nies: Linzen et al. (2016) analyzed the performance\nof LSTM LMs at an English subject-verb agree-\nment task, while Gulordava et al. (2018) extended\nthe analysis to various long-range agreement pat-\nterns in different languages. The latter study found\nthat state-of-the-art LMs trained on a standard log-\nlikelihood objective capture non-trivial patterns of\nsyntactic agreement and can approach the perfor-\nmance levels of humans, even when tested on syn-\ntactically well-formed but meaningless ( nonce)\nsentences.\nCross-language interaction during language\nproduction and comprehension by human subjects\nhas been widely studied in the ﬁelds of bilin-\ngualism and second language acquisition (Keller-\nman and Sharwood Smith; Odlin, 1989; Jarvis\nand Pavlenko, 2008) under the terms of language\ntransfer or cross-linguistic inﬂuence. Numerous\nstudies have shown that both the lexicons and the\ngrammars of different languages are not stored in-\ndependently but together in the mind of bilinguals\nand second-language learners, leading to observ-\n375\nable lexical and syntactic transfer effects (Koot-\nstra et al., 2012). For instance, through a cross-\nlingual syntactic priming experiment, Hartsuiker\net al. (2004) showed that bilinguals recently ex-\nposed to a given syntactic construction (passive\nvoice) in their L1 tend to reuse the same construc-\ntion in their L2.\nWhile the neural networks in this study are not\ndesigned to be plausible models of the human\nmind learning and processing multiple languages,\nwe believe there is interesting potential at the in-\ntersection of these research ﬁelds.\n3 Experiment\nWe consider the scenario where L1 is over-\nresourced compared to L2 and train our bilingual\nmodels by joint training on a mixed L1/L2 corpus\nso that supervision is provided simultaneously in\nthe two languages ( ¨Ostling and Tiedemann, 2017;\nJohnson et al., 2017). We leave the evaluation of\npre-training (or transfer learning) methods (Zoph\net al., 2016; Nguyen and Chiang, 2017) to future\nwork.\nThe monolingual LM is trained on a small L2\ncorpus (LML2). The bilingual LM is trained on\na shufﬂed mix of the same small L2 corpus and\na large L1 corpus, where L2 is oversampled to\napproximately match the amount of L1 sentences\n(LML1+L2). See Table 1 for the actual training\nsizes. For our preliminary experiments we have\nchosen French as the helper language (L1) and\nItalian as the target language (L2). Since French\nand Italian share many morphosyntactic patterns,\naccuracy on the Italian agreement tasks is ex-\npected to beneﬁt from adding French sentences to\nthe training data if syntactic transfer occurs.\nData and training details:We train our LMs\non French and Italian Wikipedia articles extracted\nusing the WikiExtractor tool.1 For each language,\nwe maintain a vocabulary of the 50k most fre-\nquent tokens, and replace the remaining tokens\nby <unk>. For the bilingual LM, all words are\nprepended with a language tag so that vocabular-\nies are completely disjoint. Their union (100K\ntypes) is used to train the model. This is the least\noptimistic scenario for linguistic transfer but also\nthe most controlled one. In future experiments we\nplan to study how transfer is affected by varying\ndegrees of vocabulary overlap.\n1https://github.com/attardi/\nwikiextractor\nFollowing the setup of Gulordava et al. (2018),\nwe train 2-layer LSTM models with embedding\nand hidden layers of 650 dimensions for 40\nepochs. The trained models are evaluated on the\nItalian section of the syntactic benchmark pro-\nvided by Gulordava et al. (2018), which includes\nvarious non-trivial number agreement construc-\ntions.2 Note that all models are trained on a regular\ncorpus likelihood objective and do not receive any\nspeciﬁc supervision for the syntactic tasks.\n4 Results and Conclusions\nTable 1 shows the results of our preliminary ex-\nperiments. The unigram baseline simply picks,\nfor each sentence, the most frequent word form\nbetween singular or plural. As an upper-bound\nwe report the agreement accuracy obtained by a\nmonolingual model trained on a large L2 corpus.\nTable 1: Accuracy on the Italian agreement set by the\nunigram baseline, monolingual and bilingual LMs.\nAgreementIT\nModel Training (#tok) Orig. Nonce\nUnigram — 54.9 54.5\nLSTMIT 10MIT 80.7 79.9\nLSTMFR+IT 80MFR + 8×10MIT 82.4 77.5\nLSTMIT (large) 80M IT 88.2 82.6\nThe effect of mixing the small Italian corpus\nwith the large French one does not appear to be\nmajor. Agreement accuracy increases slightly in\nthe original sentences, where the model is free to\nrely on collocational cues, but decreases slightly in\nthe nonce sentences, where the model must rely on\npure grammatical knowledge. Thus there is cur-\nrently no evidence that syntactic transfer occurs\nin our setup. A possible explanation is that the\nbilingual model has to ﬁt the knowledge from two\nlanguage systems into the same number of hidden\nlayer parameters and this may cancel out the ben-\neﬁts of being exposed to a more diverse set of sen-\ntences. In fact, the bilingual model achieves a con-\nsiderably worse perplexity than the monolingual\none (69.9 vs 55.62) on an Italian-only held-out set.\nFor comparison, ¨Ostling and Tiedemann (2017)\nobserved slightly better perplexities when mix-\ning a small number of related languages, however\n2For more details on the benchmark and LM\nconﬁgurations refer to https://github.com/\nfacebookresearch/colorlessgreenRNNs\n376\ntheir setup was considerably different (character-\nlevel LSTM with highly overlapping vocabulary).\nThis is work in progress. We are currently look-\ning for a bilingual LM conﬁguration that will re-\nsult in better target language perplexity and, pos-\nsibly, better agreement accuracy. We also plan\nto extend the evaluation to other, less related,\nlanguage pairs and different multilingual training\ntechniques. Finally, we plan to examine whether\nlexical syntactic categories (POS) are represented\nin a shared space among the two languages.\nAcknowledgments\nThis research was partly funded by the Nether-\nlands Organization for Scientiﬁc Research (NWO)\nunder project number 639.021.646. The experi-\nments were conducted on the DAS computing sys-\ntem (Bal et al., 2016).\nReferences\nHenri Bal, Dick Epema, Cees de Laat, Rob van Nieuw-\npoort, John Romein, Frank Seinstra, Cees Snoek,\nand Harry Wijshoff. 2016. A medium-scale dis-\ntributed system for computer science research: In-\nfrastructure for the long term. Computer, 49(5):54–\n63.\nJohannes Bjerva. 2017. One model to rule them\nall: Multitask and multilingual modelling for lexi-\ncal analysis. CoRR, abs/1711.01100.\nDaxiang Dong, Hua Wu, Wei He, Dianhai Yu, and\nHaifeng Wang. 2015. Multi-task learning for mul-\ntiple language translation. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 1723–1732, Beijing,\nChina. Association for Computational Linguistics.\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio.\n2016. Multi-way, multilingual neural machine\ntranslation with a shared attention mechanism. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 866–875, San Diego, California. Association\nfor Computational Linguistics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205. Associ-\nation for Computational Linguistics.\nRobert J. Hartsuiker, Martin J. Pickering, and Eline\nVeltkamp. 2004. Is syntax separate or shared be-\ntween languages?: Cross-linguistic syntactic prim-\ning in spanish-english bilinguals. Psychological Sci-\nence, 15(6):409–414.\nScott Jarvis and Anna Pavlenko. 2008. Crosslinguistic\ninﬂuence in language and cognition. Routledge.\nMelvin Johnson, Mike Schuster, Quoc Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernand a Vigas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nKatharina Kann, Ryan Cotterell, and Hinrich Sch ¨utze.\n2017. One-shot neural cross-lingual transfer for\nparadigm completion. In Proceedings of the 55th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n1993–2003, Vancouver, Canada. Association for\nComputational Linguistics.\nEric Kellerman and ed. Sharwood Smith, Michael.\nCrosslinguistic inﬂuence in second language acqui-\nsition. Pergamon.\nGerrit Jan Kootstra, Janet G. Van Hell, and Ton Dijk-\nstra. 2012. Priming of code-switches in sentences:\nThe role of lexical repetition, cognates, and lan-\nguage proﬁciency. Bilingualism: Language and\nCognition, 15(4):797819.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associ-\nation for Computational Linguistics, 4:521–535.\nChaitanya Malaviya, Graham Neubig, and Patrick Lit-\ntell. 2017. Learning language representations for ty-\npology prediction. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2529–2535. Association for Com-\nputational Linguistics.\nToan Q. Nguyen and David Chiang. 2017. Trans-\nfer learning across low-resource, related languages\nfor neural machine translation. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 296–301, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nTerence Odlin. 1989. Language Transfer: Cross-\nLinguistic Inﬂuence in Language Learning . Cam-\nbridge Applied Linguistics. Cambridge University\nPress.\nRobert ¨Ostling and J¨org Tiedemann. 2017. Continuous\nmultilinguality with language vectors. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers , pages 644–649, Valencia,\nSpain. Association for Computational Linguistics.\n377\nJ¨org Tiedemann. 2018. Emerging language spaces\nlearned from massively multilingual corpora.CoRR,\nabs/1802.00273.\nYulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui,\nGuillaume Lample, Patrick Littell, David\nMortensen, Alan W Black, Lori Levin, and\nChris Dyer. 2016. Polyglot neural language\nmodels: A case study in cross-lingual phonetic\nrepresentation learning. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 1357–1366, San\nDiego, California. Association for Computational\nLinguistics.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1568–1575, Austin,\nTexas. Association for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8614364862442017
    },
    {
      "name": "Exploit",
      "score": 0.6991448998451233
    },
    {
      "name": "Natural language processing",
      "score": 0.6573863625526428
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6278941631317139
    },
    {
      "name": "Linguistics",
      "score": 0.3488377034664154
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I121797337",
      "name": "Leiden University",
      "country": "NL"
    }
  ],
  "cited_by": 31
}