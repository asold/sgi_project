{
  "title": "SCELMo: Source Code Embeddings from Language Models",
  "url": "https://openalex.org/W2997275048",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288789612",
      "name": "Karampatsis, Rafael-Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2609452214",
      "name": "Sutton, Charles",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2807730630",
    "https://openalex.org/W2907705732",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2888223970",
    "https://openalex.org/W2162456950",
    "https://openalex.org/W2064296229",
    "https://openalex.org/W2963965612",
    "https://openalex.org/W2963499994",
    "https://openalex.org/W2964150020",
    "https://openalex.org/W2963935794",
    "https://openalex.org/W2950898568",
    "https://openalex.org/W2891185194",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2942293597",
    "https://openalex.org/W2507974895",
    "https://openalex.org/W2463895987",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2806718802",
    "https://openalex.org/W2238673293",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2736762043",
    "https://openalex.org/W2914933759",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2274071363",
    "https://openalex.org/W2156723666",
    "https://openalex.org/W3122945969",
    "https://openalex.org/W2947167994",
    "https://openalex.org/W2793157084"
  ],
  "abstract": "Continuous embeddings of tokens in computer programs have been used to support a variety of software development tools, including readability, code search, and program repair. Contextual embeddings are common in natural language processing but have not been previously applied in software engineering. We introduce a new set of deep contextualized word representations for computer programs based on language models. We train a set of embeddings using the ELMo (embeddings from language models) framework of Peters et al (2018). We investigate whether these embeddings are effective when fine-tuned for the downstream task of bug detection. We show that even a low-dimensional embedding trained on a relatively small corpus of programs can improve a state-of-the-art machine learning system for bug detection.",
  "full_text": "arXiv:2004.13214v1  [cs.SE]  28 Apr 2020\nSCELM O: S O U R C E CO D E EM B E D D IN G S\nF RO M LA N G UAG E MO D E L S\nRafael - Michael Karampatsis\nDepartment of Computer Science\nThe University of Edinburgh\nEdinburgh, EH8 9AB, UK\nr.m.karampatsis@sms.ed.ac.uk\nCharles Sutton\nGoogle Research, The University of Edinburgh\nMountain V iew , CA, United States\ncharlessutton@google.com\nABSTRACT\nContinuous embeddings of tokens in computer programs have b een used to support a variety of\nsoftware development tools, including readability, code s earch, and program repair. Contextual\nembeddings are common in natural language processing but ha ve not been previously applied in\nsoftware engineering. W e introduce a new set of deep context ualized word representations for com-\nputer programs based on language models. W e train a set of emb eddings using the ELMo (em-\nbeddings from language models) framework of Peters et al (20 18). W e investigate whether these\nembeddings are effective when ﬁne-tuned for the downstream task of bug detection. W e show that\neven a low-dimensional embedding trained on a relatively sm all corpus of programs can improve a\nstate-of-the-art machine learning system for bug detectio n.\n1 I NTRODUC TI ON\nLearning rich representations for source code is an open pro blem that has the potential to enable software engineer-\ning and development tools. Some work on machine learning for source code has used hand engineered features\n(Long & Rinard, 2016, e.g.), but designing and implementing such features can be tedious and error-prone. For this\nreason, other work considers the task of learning a represen tation of source code from data (Allamanis et al., 2018a).\nMany models of source code are based on learned representati ons called embeddings, which transform words into a\ncontinuous vector space (Mikolov et al., 2013). Currently i n software engineering (SE) researchers have used static\nembeddings (Harer et al., 2018; White et al., 2019; Pradel & S en, 2018), which map a word to the same vector regard-\nless of its context. However, recent work in natural languag e processing (NLP) has found that contextual embeddings\ncan lead to better performance (Peters et al., 2018; Devlin e t al., 2018; Y ang et al., 2019; Liu et al., 2019). Contextu-\nalized embeddings assign a different vector to a word based o n the context it is used. For NLP this has the advantage\nthat it can model phenomena like polysemy. A natural questio n to ask is if these methods would also be beneﬁcial for\nlearning better SE representations.\nIn this paper, we introduce a new set of contextual embedding s for source code. Contextual embeddings have several\npotential modelling advantages that are speciﬁcally suite d to modelling source code:\n•Surrounding names contain important information about an i dentiﬁer. For example, for a variable name,\nsurrounding tokens might include functions that take that v ariable as an argument or assignments to the\nvariable. These tokens provide indirect information about possible values the variable could take, and so\nshould affect its representation. Even keywords can have ve ry different meanings based on their context. For\ninstance, a private function is not the same as a private vari able or a private class (in the case of Java / C++).\n•Contextual embeddings assign a different representation t o a variable each time it is used in the program. By\ndoing this, they can potentially capture how a variable’s va lue evolves through the program execution.\n•Contextual embeddings enable the use of transfer learning. Pre-training a large neural language model and\nquerying it for contextualized representations while simu ltaneously ﬁne-tuning for the speciﬁc task is a very\neffective technique for supervised tasks for which there is a small amount of supervised data available. As a\nresult only a small model needs to be ﬁne-tuned atop the pre-t rained model, without the need for task-speciﬁc\narchitectures nor the need of training a large model for each task separately.\nIn this paper, we highlight the potential of contextual code embeddings for program repair. Automatically ﬁnding bugs\nin code is an important open problem in SE. Even simple bugs ca n be hard to spot and repair. A promising approach\n1\nto this end is name-based bug detection, introduced by DeepB ugs (Pradel & Sen, 2018). The current state-of-the-art\nin name-based bug detection relies on static representatio ns from W ord2V ec (Mikolov et al., 2013) to learn a classiﬁer\nthat distinguishes correct from incorrect code for a speciﬁ c bug pattern. W e introduce a new set of contextualized\nembeddings for code and explore its usefulness on the task of name-based bug detection. Our method signiﬁcantly\noutperforms DeepBugs as well as other static representatio ns methods on both the DeepBugs dataset as well as a new\npreviously unused test set of JavaScript projects.\n2 R ELATED WORK\nUnsupervised static word embeddings have been extensively used to improve the accuracy of supervised tasks in\nNLP (Turian et al., 2010). Notable examples of such methods a re W ord2V ec (Mikolov et al., 2013) and GloV e\n(Pennington et al., 2014). However, the above models learn o nly a single context-independent word representation. T o\novercome this problem some models (Wieting et al., 2016; Boj anowski et al., 2017) enhance the representations with\nsubword information, which can also somewhat deal with out- of-vocabulary words. Another approach is to learn a dif-\nferent representation for every word sense (Neelakantan et al., 2014) but this requires knowing the set of word senses\nin advance. More recent methods overcome the above issues by learning contextualized embeddings. Melamud et al.\n(2016) encode the context surrounding a pivot word using a bi directional LSTM. Peters et al. (2018) use a deep bidi-\nrectional LSTM, learning word embeddings as functions of it s internal states, calling the method Embeddings using\nLanguage Models (ELMo). W e discuss ELMo in detail in Section 3. Devlin et al. (2018) introduced bidirectional\nencoder representations from transformers (BER T). This me thod learns pre-trained contextual embeddings by jointly\nconditioning on left and right context via an attention mech anism.\nProgram repair is an important task in software engineering and programming languages. For a detailed review\nsee Monperrus (2018); Gazzola et al. (2019). Many recent pro gram repair methods are based on machine learn-\ning. Y in et al. (2018) learn to represent code edits using a ga ted graph neural network (GGNN) (Li et al., 2016).\nAllamanis et al. (2018b) learn to identify a particular clas s of bugs called variable misuse bugs, using a GGNN.\nChen et al. (2019) introduce SequenceR which learns to trans form buggy lines into ﬁxed ones via machine transla-\ntion. Our work is orthogonal to these approaches and can be us ed as input in other models.\nFinally, our work is also related to code representation met hods many of which have also been used in program repair.\nHarer et al. (2018) learn W ord2V ec embeddings for C/C++ toke ns to predict software vulnerabilities. White et al.\n(2019) learn W ord2V ec embeddings for Java tokens and utiliz e them in program repair. Alon et al. (2019) learn code\nembeddings using abstract syntax tree paths. A more detaile d overview can be found in (Allamanis et al., 2018a;\nChen & Monperrus, 2019).\n3 E MBEDDING S FROM LANGUAGE MODELS (ELM O)\nELMo (Peters et al., 2018) computes word embeddings from the hidden states of a language model. Consequently,\nthe embeddings of each token depend on its context of the inpu t sequence, even out-of-vocabulary (OOV) tokens have\neffective input representations. In this section, we brieﬂ y describe the ELMo embeddings.\nThe ﬁrst step is that a neural language model is trained to max imize the likelihood of a training corpus. The architecture\nused by ELMo a bidirectional LSTM with L layers and character convolutions in the input layer. Let th e input\nbe a sequence of tokens (t1, ...t N ). For each token tk, denote by xLM\nk the input representation from the character\nconvolution. Consequently, this representation passes th rough L layers of forward and backward LSTMs. Then\neach layer j ∈{ 1, ..., L }of the forward LSTM computes a hidden state\n−−→\nhLM\nk,j , and likewise the hidden states of the\nbackward LSTM are denoted by\n←−−\nhLM\nk,j . The parameters for the token representation and for the out put softmax layer\nare tied for both directions, while different parameters ar e learned for each direction of the LSTMs.\nAfter the language model has been trained, we can use it withi n another downstream task by combining the hidden\nstates of the language model from each LSTM layer. This proce ss is called ELMo. For each token tk of a sentence\nin the test set, the language model computes 2L + 1hidden states, one in each direction for each layer, and then the\ninput layer. T o make the following more compact, we can write these as hLM\nk,0 = xLM\nk for the input layer, and then\nhLM\nk,j = [\n−−→\nhLM\nk,j ,\n←−−\nhLM\nk,j ] for all of the other layers. The set of these vectors is\nRk = {hLM\nk,j |j = 0, ..., L }. (1)\nT o create the ﬁnal representation that is fed to downstream t asks, ELMo collapses the set of representations into a\nsingle vector Ek for token tk. A simplistic approach is to only select the top layer, so tha t Ek = hLM\nk,L . A more general\n2\none, which we use in this work, is to combine the layers via ﬁne -tuned task speciﬁc weights s = (s1 . . . s L) for every\nlayer. Then we can compute the embedding for token k as\nEk = γ\nL∑\nj=0\nsj hLM\nk,j , (2)\nwhere γ is an additional scalar parameter that scales the entire vec tor. In our experiments we did not performed ﬁne-\ntuning and thus used equal weights sj = 1/ (L + 1)for each layer and γ = 1. However, our implementation also\nsupports all the aforementioned ways of collapsing the set o f representations.\nA potential drawback of the method is that it still utilizes a softmax output layer with a ﬁxed vocabulary that does not\nscale effectively and it still predicts UNK for OOV tokens wh ich may have a negative effect on the representations.\n4 S OURCE CODE ELM O\nW e describe Source Code ELMo (SCELMo), which trains ELMo on c orpora of source code. However, we note that\nnormally ELMo models in other domains are able to effectivel y utilize much larger representations. The code was\ntokenized using the esprima JavaScript tokenizer 1. For training the ELMo model we used a corpus of 150,000\nJavaScript Files (Raychev et al. 2016) consisting of variou s open-source projects. This corpus has previously been\nused on several tasks (Raychev et al., 2016; Pradel & Sen, 201 8; Bavishi et al., 2018). W e applied the patch released\nby Allamanis et al. (2018a) to ﬁlter out code duplication as t his phenomenon was shown on this and other corpora\nto result in inﬂation of performance metrics. This resulted in 64750 training ﬁles and 33229 validation ﬁles. Since\nthe validation set contains ﬁles from the same projects as th e train the contained instances might be too similar and\nunrealistic overestimating. T o address this we also create d a test set of 500 random JavaScript projects sampled from\nthe top 20,000 open-source JavaScript projects as of May 201 9. The test corpus has not been previously utilized in\nprevious work and is a better reﬂection of the performance of the learned bug detectors. Lastly, it is important to know\nwhat the performance of the method will be if we do not have acc ess to training data from the projects on which we\nwould like to ﬁnd bugs. This is common in practice for many rea l case scenarios. For training the ELMo model, we\nuse an embedding size of 100 features for each of the forward a nd backward LSTMs so that each layer sums up to 200\nfeatures.\n5 C ONTEXTUAL EMBEDDINGS FOR PROGRAM REPAIR\nIn this section, we describe how contextual embeddings can b e incorporated within a recent machine learning-based\nbug detection system, the DeepBugs system of Pradel & Sen (20 18). In the ﬁrst part of this section, we give back-\nground about the DeepBugs system, and then we describe how we incorporate SCELMo within DeepBugs. DeepBugs\ntreats the problem of ﬁnding a bug as a classiﬁcation problem . The system considers a set of speciﬁc bug types, which\nare small mistakes that might be made in a program, such as swa pping two arguments. For each bug type, DeepBugs\ntrains a binary classiﬁer that takes a program statement as i nput and predicts whether the statement contains that type\nof bug. At test time, this classiﬁer can be run for every state ment in the program to attempt to detect bugs.\nIn order to train the model both examples of correct and incor rect (buggy) code are necessary. DeepBugs treats\nthe existing code as correct and randomly mutates it to obtai n buggy code. T o obtain training examples, we extract\nall expressions from the source code which are either the fun ction calls with exactly two arguments and all binary\nexpressions. T o create instances of buggy code we mutate eac h of the correct instances. As such, arguments in\nfunction calls are swapped, the binary operator in binary ex pressions is replaced with another random one, and ﬁnally\nrandomly either the left or the right operand is replaced by a nother random binary operand that appears in the same\nﬁle. Then the classiﬁcation task is a binary task to predict w hether the instance is correct, i.e., it comes from the\noriginal code, or whether it is buggy, i.e. it was one of the ra ndomly mutated examples. The validation and test sets are\nmutated in the same way as the training set. The split between correct and buggy instances has 50/50 class distribution\nas for each original code instance exactly one mutated buggy counterpart is created.\nThe architecture for the classiﬁer is a feedforward network with a single hidden layer of 200 dimensions with Relu\nactivations and a sigmoid output layer. For both the input an d hidden layers a dropout of 0.2. The network was trained\nin all experiments for 10 epochs with a batch size of 50 and the RMSProp optimizer. W e note that for maintaining\na consistent comparison with DeepBugs we kept all the above p arameters as well as the optimizer’s parameters ﬁxed\nto the values reported in Pradel & Sen (2018). Tuning these pa rameters would probably result in at least a small\nperformance increase for our method.\n1 https://esprima.org/\n3\n1 // Argument order is inversed.\n2 var delay = 1000;\n3 setTimeout(delay, function() { // Function should be first.\n4 logMessage(msgValue);\n5 });\nListing 1: Swapped Arguments Bug\n1 // && instead of || was used.\n2 var p = new Promise();\n3 if (promises === null && promises.length === 0) {\n4 p.done(error, result);\n5 }\nListing 2: Incorrect Binary Operator\n1 // Call to .length is missing.\n2 if ( index < matrix ) {\n3 do_something();\n4 }\nListing 3: Incorrect Binary Operand\nFigure 1: Bug type examples.\nIn our experiments, we consider three bug types that address a set of common programming mistakes: swapped argu-\nments of function calls, using the wrong binary operator and using an incorrect binary operand in a binary expression.\nThe methodology can easily be applied to other bug types. Fig ure 1 illustrates an example of each of the three bug\ntypes.\n5.1 I N P U T TO T H E CL A S S IFIE R\nA key question is how a statement from the source code is conve rted into a feature vector that can be used within the\nclassiﬁer. DeepBugs uses a set of heuristics that, given a st atement and a bug type, return a sequence of identiﬁers\nfrom the statement that are most likely to be relevant. For in stance, for the call to setTimeout in Listing 1 the following\nsequence of identiﬁers would be extracted: [setT imeout, delay, function]. A detailed description of the heuristics is\navailable in Appendix A.\nThese heuristics result in a sequence of program identiﬁers . These are converted to continuous vectors using word\nembeddings, concatenated, and this is the input to the class iﬁer. DeepBugs uses W ord2V ec embeddings trained on a\ncorpus of code. In our experiments, we train classiﬁers usin g three different types of word embeddings. First, we kept\nthe 10,000 most frequent identiﬁers/literals and assigned to each of them a random embedding of 200 features. Second,\nto reproduce the results of Pradel & Sen (2018), we use the CBO W variant of W ord2V ecto learn representations\nconsisting of 200 features for the 10,000 most frequent iden tiﬁers/literals. Finally, we train a F astT extembeddings\n(Bojanowski et al., 2017) on the training set to learn identi ﬁer embeddings that contain subword information. The\nsubwords used by FastT ext are all the character trigrams tha t appear in the training corpus. Identiﬁers are therefore\ncomposed of multiple subwords. T o represent an identiﬁer, w e sum the embeddings of each of its subwords and\nsumming them up. This allows the identiﬁer embeddings to con tain information about the structure and morphology\nof identiﬁers. This also allows the FastT ext embeddings, un like the W ord2V ec ones, to represent OOV words as a\ncombination of character trigrams.\nNote that DeepBugs can detect bugs only in statements that do not contain OOV (out-of-vocabulary) identiﬁers, be-\ncause its W ord2V ec embeddings cannot extract features for O OV names. Instead our implementation does not skip\nsuch instances. Since the original work discarded any insta nces that contain OOV identiﬁers we neither know how the\nmethod performs on such instances nor how often those appear in the utilized dataset of DeepBugs. Moreover, Deep-\nBugs supported only a speciﬁc subset of AST nodes and skipped the rest. For example if a call’s argument is a complex\nexpression consisting of other expressions then the call wo uld be skipped. However, we expanded the implementation\nto support all kinds of AST nodes and to not skip instances wit h nested expressions as discussed in Appendix A. W e\nnote that we still skip an instance if one of its main parts (e. g., a function call’s argument) is a complex expression\nlonger than 1,000 characters as such expressions might be ov erly long to reason about.\n4\nT able 1: Comparison of ELMo versus non-contextual embeddin gs for bug detection on a validation set of projects.\nData is restricted to expressions that contain only single n ames.\nRandom W ord2V ec FastT ext No-Context ELMo SCELMo\nSwapped Arguments 86.18% 87.38% 89.55% 90.02% 92.11%\nWrong Binary Operator 90.47% 91.05% 91.11% 92.47% 100.00%\nWrong Binary Operand 75.56% 77.06% 79.74% 81.71% 84.23%\n5.2 C O N N E CT IN G SCELM O TO T H E BU G DE T E CTO R\nW e investigated two variants of the bug detection model, whi ch query SCELMo in different ways to get features for\nthe classiﬁer. The ﬁrst utilizes the heuristic of Section A t o extract a small set of identiﬁers or literals that represen t\nthe code piece. For example, for an incorrect binary operand instance we extract one identiﬁer or literal for the left\nand right operands respectively, and we also extract its bin ary operator. Then, those are concatenated to form a query\nto the network. In the case of function calls we extract the id entiﬁer corresponding to the name of the called function,\none identiﬁer or literal for the ﬁrst and second argument res pectively and an identiﬁers for the expression on which the\nfunction is called. W e also add the appropriate syntax token s (a ’. ’ if necessary, ’, ’ between the two arguments, and left\nand right parentheses) to create a query that resembles a fun ction call. This baseline approach creates simplistic ﬁxed\nsize queries for the network but does not utilize its full pot ential since the queries do not necessarily resemble actual\ncode, nor correct code similar to the sequences in the traini ng set for the embeddings. W e will refer to this baseline as\nNo-Context ELMo.\nOur proposed method, we compute SCELMo embeddings to the lan guage model all the tokens of the instances for\nwhich we need representations. V alid instances are functio ns calls that contain exactly two arguments and binary\nexpressions. T o create a ﬁxed-size representation we extra ct only the features corresponding a ﬁxed set of tokens.\nSpeciﬁcally, for functions calls we use the representation s corresponding to the ﬁrst token of the expression on which\nthe function is called, the function name, the ﬁrst token of t he ﬁrst argument and the ﬁrst token of the second argument.\nWhile, for binary expressions we use those of the ﬁrst token o f the left operand, the binary operator, and the ﬁrst token\nof the right operand. Since the representations contain con textual information, the returned vectors can capture\ninformation about the rest of the tokens in the code sequence .\n6 R ESULTS\nW e next discuss the experiments we performed and their corre sponding results. W e measured the performance of the\nthree baselines as well as those of non-contextual ELMO and S CELMO. Measuring the performance of non-contextual\nELMO allows us to evaluate how much improvement is due to spec iﬁcs of the language model architecture, such as\nthe character convolutional layer which can handle OOVs, an d how much is due to the contextual information itself.\n6.1 P E RF O RM A N CE O N VA L IDAT IO N SE T\nIn our ﬁrst experiment we evaluate the performance of the met hods in tasks where training data from the same projects\nare available. The evaluation performed in this experiment gives a good estimation of how our method performs com-\npared to the previous state-of-the-art technique of DeepBu gs. One main difference however is that the evaluation now\nalso includes instances which contain OOV . As a consequence the bug detections tasks are harder than those presented\nby Pradel & Sen (2018) as their evaluation does not include in both the training and validation set any instance for\nwhich an extracted identiﬁer is OOV . T able 1 illustrates the performance of the baselines and our models. As one would\nexpect the FastT ext baseline improves over W ord2V ec for all bug types due to the subword information. Moreover,\nour model SCELMo massively outperforms all other methods. L astly, even no-context ELMo the heuristic version of\nSCELMo that does not utilize contextual information at test time outperforms the baseline methods showcasing how\npowerful the pretrained representations are.\n6.2 I N CL U D IN G CO M P L E X EX P RE S S IO N S\nIn our next experiment we also included instances that conta in elements that are complex or nested expressions. For\ninstance, in the original work if one the arguments of a funct ion call or one of the operands of a binary expression is an\nexpression consisting of other expressions then the instan ce would not be included in the dataset. Several AST node\n5\nT able 2: Comparison of SCELMo versus static embeddings on bu g detection on a validation set of projects. Complex\nexpressions are included in this validation set.\nRandom W ord2V ec FastT ext No-Context ELMo SCELMo\nSwapped Arguments 86.37% 87.68% 90.37% 90.83% 92.27%\nWrong Binary Operator 91.12% 91.68% 91.92% 92.75% 100.00%\nWrong Binary Operand 72.73% 74.31% 77.41% 79.65% 87.10%\nT able 3: Comparison of SCELMo versus static embeddings on bu g detection on an external test set of 500 JavaScript\nprojects.\nRandom W ord2V ec FastT ext No-Context ELMo SCELMo\nSwapped Arguments 75.79% 78.22% 79.40% 81.37% 84.25%\nWrong Binary Operator 82.95% 85.54% 83.15% 86.54% 99.99%\nWrong Binary Operand 67.46% 69.50% 72.55% 75.74% 83.59%\ntypes such as a NewExpression node or an ObjectExpression were not supported. Figure 2 a few examples of\ninstances that would be previously skipped 2 . Such instances were skipped by Pradel & Sen (2018) and not in cluded\nin their results. W e do note though that we still skip very lon g expressions that contain more than 1000 tokens.\n1 // First argument is binary expression\n2 doComputation(x + find_min(components), callback);\n1 // Second argument is an unsupported node\n2 factory.test(simulator, new Car(’Eagle’, ’Talon TSi’, 1993));\nFigure 2: Examples of instances that would be skipped by Deep Bugs.\nSimilarly to the previous experiment SCELMo signiﬁcantly o utperforms all other models. This is evident in T able 2.\nLastly, we clarify that the results of this section should no t be directly compared to those of the previous one as for\nthis experiment the training set is also larger.\n6.3 E X T E RNA L TE S T EV A L UAT IO N\nThe last experiment’s objective is to showcase how the vario us models would perform on unseen projects as this better\nillustrates the generalizability of the techniques. The co nﬁguration utilized is identical to that of the previous sec tion.\nBy looking at T able 3 one can notice that the baselines have a m ajor drop in performance. This is a common ﬁnding\nin machine learning models of code, namely, that applying a t rained model to a new software project is much more\ndifﬁcult than to a new ﬁle in the same project. In contrast, SC ELMo offers up to 15% improvement in accuracy\ncompared to W ord2V ec baseline. In fact, impressively enoug h SCELMo on the external test set is better than the\nevaluation set one of the baselines.\n6.4 OOV S TAT IS T ICS\nIn order to better understand the above results we measured t he OOV rate of the basic elements of the code instances\nappearing in the dataset. Here the OOV rate is calculated bas ed on the vocabulary of 10000 entries utilized by the\nW ord2V ec and random baseline models. These are illustrated in T ables 4 and 5. W e measured the OOV rates for both\nthe version of the dataset used in Section 6.4, which we call T rain and V alidation, and that used in Section 6.2, which\nwe call Extended Train and Extended V alidation.\nT ables 4 and 5 describe the OOV rates for different parts of th e expression types that are considered by the DeepBugs\nbug detector. A detailed description of the identiﬁers extr action heuristic can be found in Appendix A. W e ﬁrst focus\n2 The AST is extracted using the acorn parser https://github.com/acornjs/acorn\n6\non the swapped arguments bug pattern and consider all of the m ethod call that have exactly two arguments. Each\nmethod call contains the function name, a name of the ﬁrst arg ument, a name of the second argument, and a base object.\nThe base object is the identiﬁer that would be extracted from the expression (if such an expression exists) on which\nthe function is called. For instance, from the following exp ression: window .navigator .userAgent.indexOf(”Chrome”),\nuserAgent would be extracted as the base object. T able 4 shows for each o f the components how often they are OOV .\nIn the expanded version of the dataset if one of the arguments is a complex expression then it is converted into a name\nbased on the heuristic described in Section A. The resulting statistics contain valuable information as for instance, i t\nis almost impossible for the W ord2V ec baseline to reason abo ut a swap arguments bug if the identiﬁers extracted for\nboth arguments are OOV .\nIn a similar manner for the incorrect operand and operator bu g patterns we consider all the binary operations. Each\nbinary expression consists of a left and right operand and a n ame is extracted for each of them. For each operand we\nalso measured the frequency with which the operand correspo nds to certain common types such as identiﬁer, literal or\na ThisExpression.\nT able 4: OOV statistics for calls with exactly two arguments (Swapped arguments instances). The statistics are\ncalculated on variants of the DeepBugs dataset.\nTrain Expanded Train V alidation Expanded V alidation\nT wo Arguments Calls 574656 888526 289061 453486\nCalls Missing Base Object 25.07% 28.63% 25.63% 28.80%\nBase Object Missing or OOV 34.56% 37.38% 35.57% 38.07%\nFunction Name OOV 20.69% 17.07% 20.33% 16.94%\nFirst Argument OOV 31.01% 36.99% 31.64% 37.15%\nSecond Argument OOV 27.25% 22.86% 27.94% 23.49%\nBoth Arguments OOV 11.33% 9.57% 11.96% 10.16%\nBase and Function Name OOV 10.20% 8.32% 10.39% 8.61%\nBase and Arguments OOV 4.21% 3.31% 4.88% 3.77%\nFunction Name and Arguments OOV 2.86% 2.26% 2.85% 2.28%\nAll Elements OOV 1.53% 1.18% 1.61% 1.27%\nT able 5: OOV statistics for binary operations.\nTrain Expanded Train V alidation Expanded V alidation\nBinary Operations 1075175 1578776 540823 797108\nLeft Operand OOV 25.40% 28.84% 26.04% 29.55%\nRight Operand OOV 20.37% 23.98% 20.74% 24.55%\nBoth Operands OOV 7.82% 11.29% 8.24% 11.88%\nUnknown Left Operand T ype 83.36% 87.80% 83.14% 87.74%\nUnknown Right Operand T ype 48.48% 47.23% 48.47% 47.05%\nBoth Operand T ypes Unknown 33.34% 36.06% 33.20% 35.87%\nAll OOV or Unknown 3.59% 4.03% 3.81% 4.3%\n7 I S NEURAL BUG -FINDIN G USEFUL IN PRACTICE ?\nAlthough related work (Pradel & Sen, 2018; Allamanis et al., 2018b; V asic et al., 2019) has shown that there is great\npotential for embedding based neural bug ﬁnders, the evalua tion has mostly focused on synthetic bugs introduced by\nmutating the original code. However, there is no strong indi cation that the synthetic bugs correlate to real ones, apart\nfrom a small study of the top 50 warnings for each bug type prod uced by DeepBugs. A good example is the mutation\noperation utilized for the incorrect binary operator bug. A lot of the introduced bug instances could result in syntacti c\nerrors. This can potentially create a classiﬁer with a high b ias towards correlating buggy code to syntactically incorr ect\ncode, thus hindering the model’s ability to generalize on re al bugs. Ideally, in an industrial environment we would like\nthe resulting models to achieve a false positive rate of less than 10 % (Sadowski et al., 2015). Sadly, high true positive\nrates are not to be expected as well since static bug detector s were shown to be able to detect less than 5% of bugs\n7\nT able 6: Real bug mined instances.\nSwapped Arguments Wrong Binary Operator Wrong Binary Opera nd\nMined Instances 303 80 1007\nT able 7: Real bug identiﬁcation task recall and false positi ve rate (FPR).\nW ord2V ec-Recall W ord2V ec-FPR SCELMo-Recall SCELMo-FPR\nSwapped Arguments 3.34% 0.33% 49.67% 33.78%\nWrong Binary Operator 8.95% 7.70% 0.00% 0.00%\nWrong Binary Operand 11.99% 12.11% 15.81% 14.34%\n(Habib & Pradel, 2018) contained in the Defects4J corpus (Ju st et al., 2014) and less than 12% in a single-statement\nbugs corpus (Karampatsis & Sutton, 2019). W e note that in the second case the static analysis tool is given credit by\nreported any warning for the buggy line, so the actual percen tage might lower than the reported one.\nW e next make a ﬁrst step on investigating the practical usefu lness of our methods by applying the classiﬁers of the\nprevious section on a small corpus of real JavaScript bugs. H owever, we think that this is a very hard yet interesting\nproblem that should be carefully examined in future work. In order to mine a corpus of real bug changes we used\nthe methodology described in (Karampatsis & Sutton, 2019). W e note that we adapted their implementation to utilize\nthe Rhino JavaScript parser 3. Their methodology extracts bug ﬁxing commits and ﬁlters th em to only keep those that\ncontain small single-statement changes. Finally, it class iﬁes each pair of modiﬁed statements by whether the ﬁt a set\nof mutation patterns. The resulting dataset is shown in T abl e 6.\nFinally, we queried the DeepBugs and SCELMo with each buggy i nstance as well as its ﬁxed variant and measured\nthe percentage of correctly classiﬁed instances for each of the two categories. W e also ignored any instances for which\nthe JavaScript parser utilized for both failed to extract an AST . W e classiﬁed as bugs any instances that were assigned\na probability to be a bug > 75%. In an actual system this threshold should ideally be tuned o n a validation set.\nT able 7 suggests that there might indeed be some potential fo r future practical applications of neural bug ﬁnding\ntechniques. Both are able to uncover some of the bugs. Howeve r, the results also suggest that careful tuning of the\npredictions threshold might be necessary, especially if we take into account the industrial need to comply with a low\nfalse positive rate (FPR). For instance, raising SCELMo’s p rediction threshold to 80% for the swap arguments bug\nresults in ﬁnding only 3.34% of the bugs but correctly classi fying 100% of the repaired function calls, thus achieving\n0.0% false positive rate. Moreover, since SCELMo could not u ncover any of the real binary operator bugs, future work\ncould investigate the effect of utilizing different mutati on strategies for the purpose of artiﬁcial bug-induction. F uture\nwork could also investigate if ﬁne-tuning on small set of rea l bugs could result in more robust classiﬁers.\n8 C ONCLUSIO N\nW e have presented SCELMo, which is to our knowledge the ﬁrst l anguage-model based contextual embeddings for\nsource code. Contextual embeddings have many potential adv antages for source code, because surrounding tokens can\nindirectly provide information about tokens, e.g. about li kely values of variables. W e highlight the utility of SCELMo\nembeddings by using them within a recent state-of-the-art m achine learning based bug detector. The SCELMo em-\nbeddings yield a dramatic improvement in the synthetic bug d etection performance benchmark, especially on lines of\ncode that contain out-of-vocabulary tokens and complex exp ressions that can cause difﬁculty for the method. W e also\nshowed and discussed the performance of the resulting bug de tectors on a dataset of real bugs raising useful insights\nfor future work.\nACK N OW L E D G E M E N T S\nThis work was supported in part by the EPSRC Centre for Doc-to ral Training in Data Science, funded by the UK\nEngineeringandPhysical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.\n3 https://github.com/mozilla/rhino\n8\nREFERENC ES\nMiltiadis Allamanis, Earl T . Barr, Premkumar Devanbu, and C harles Sutton. A survey of machine learning for big\ncode and naturalness. ACM Comput. Surv ., 51(4):81:1–81:37, July 2018a. ISSN 0360-0300. doi: 10.11 45/3212695.\nURL http://doi.acm.org/10.1145/3212695.\nMiltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khade mi. Learning to represent pro-\ngrams with graphs. In International Conference on Learning Representations , 2018b. URL\nhttps://openreview.net/forum?id=BJOFETxR-.\nUri Alon, Meital Zilberstein, Omer Levy, and Eran Y ahav. Cod e2vec: Learning distributed representations of code.\nProc. ACM Program. Lang., 3(POPL):40:1–40:29, January 2019. ISSN 2475-1421. doi: 1 0.1145/3290353. URL\nhttp://doi.acm.org/10.1145/3290353.\nRohan Bavishi, Michael Pradel, and Koushik Sen. Context2na me: A deep learning-based ap-\nproach to infer natural variable names from usage contexts. CoRR, abs/1809.05193, 2018. URL\nhttp://arxiv.org/abs/1809.05193.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and T omas M ikolov. Enriching word vectors with subword informa-\ntion. T ransactions of the Association for Computational Linguistics, 5:135–146, 2017. doi: 10.1162/tacl a 00051.\nURL https://www.aclweb.org/anthology/Q17-1010.\nZimin Chen and Martin Monperrus. A literature study of embed dings on source code. CoRR, abs/1904.03061, 2019.\nURL http://arxiv.org/abs/1904.03061.\nZimin Chen, Steve Kommrusch, Michele Tufano, Louis-No ¨ el P ouchet, Denys Poshyvanyk, and Martin Monperrus.\nSequencer: Sequence-to-sequence learning for end-to-end program repair. CoRR, abs/1901.01808, 2019. URL\nhttp://arxiv.org/abs/1901.01808.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T out anova. BER T: pre-training of deep\nbidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL\nhttp://arxiv.org/abs/1810.04805.\nL. Gazzola, D. Micucci, and L. Mariani. Automatic software r epair: A survey. IEEE T ransactions on Software\nEngineering, 45(01):34–67, jan 2019. ISSN 1939-3520. doi: 10.1109/TSE .2017.2755013.\nAndrew Habib and Michael Pradel. How many of all bugs do we ﬁnd ? a study of static bug detectors. In Pro-\nceedings of the 33rd ACM/IEEE International Conference on A utomated Software Engineering , ASE 2018, pp.\n317–328, New Y ork, NY , USA, 2018. ACM. ISBN 978-1-4503-5937 -5. doi: 10.1145/3238147.3238213. URL\nhttp://doi.acm.org/10.1145/3238147.3238213.\nJacob A. Harer, Louis Y . Kim, Rebecca L. Russell, Onur Ozdemi r, Leonard R. Kosta, Akshay Rangamani, Lei H.\nHamilton, Gabriel I. Centeno, Jonathan R. Key, Paul M. Ellin gwood, Marc W . McConley, Jeffrey M. Opper,\nSang Peter Chin, and T omo Lazovich. Automated software vuln erability detection with machine learning. CoRR,\nabs/1803.04497, 2018. URL http://arxiv.org/abs/1803.04497.\nRen ´ e Just, Darioush Jalali, and Michael D. Ernst. Defects4 j: A database of existing faults to enable controlled testin g\nstudies for java programs. In Proceedings of the 2014 International Symposium on Softwar e T esting and Analysis,\nISST A 2014, pp. 437–440, New Y ork, NY , USA, 2014. ACM. ISBN 97 8-1-4503-2645-2. doi: 10.1145/2610384.\n2628055. URL http://doi.acm.org/10.1145/2610384.2628055.\nRafael-Michael Karampatsis and Charles Sutton. How Often D o Single-Statement Bugs Occur? The ManySStuBs4J\nDataset. arXiv preprint arXiv:1905.13334, 2019. URL https://arxiv.org/abs/1905.13334.\nY ujia Li, Richard Zemel, Marc Brockschmidt, and Daniel T arl ow . Gated graph se-\nquence neural networks. In Proceedings of ICLR’16 , April 2016. URL\nhttps://www.microsoft.com/en-us/research/publication/gated-graph-sequence-neural-networks/.\nY inhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and V eselin Stoyanov. Roberta: A robustly optimized BER T pretraining approach. CoRR, abs/1907.11692,\n2019. URL http://arxiv.org/abs/1907.11692.\n9\nFan Long and Martin Rinard. Automatic patch generation by le arning correct code. In Proceedings of the 43rd\nAnnual ACM SIGPLAN-SIGACT Symposium on Principles of Progr amming Languages , POPL ’16, pp. 298–\n312, New Y ork, NY , USA, 2016. ACM. ISBN 978-1-4503-3549-2. d oi: 10.1145/2837614.2837617. URL\nhttp://doi.acm.org/10.1145/2837614.2837617.\nOren Melamud, Jacob Goldberger, and Ido Dagan. context2vec : Learning generic context embedding with bidirec-\ntional LSTM. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pp.\n51–61, Berlin, Germany, August 2016. Association for Compu tational Linguistics. doi: 10.18653/v1/K16-1006.\nURL https://www.aclweb.org/anthology/K16-1006.\nT omas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and\nphrases and their compositionality. In C. J. C. Burges, L. Bo ttou, M. W elling, Z. Ghahramani, and K. Q. W einberger\n(eds.), Advances in Neural Information Processing Systems 26, pp. 3111–3119. Curran Associates, Inc., 2013. URL\nhttp://papers.nips.cc/paper/5021-distributed-repres entations-of-words-and-phrases-and-their-compositio nality.pdf.\nMartin Monperrus. Automatic software repair: A bibliograp hy. ACM Comput. Surv ., 51(1):17:1–17:24, January 2018.\nISSN 0360-0300. doi: 10.1145/3105906. URL http://doi.acm.org/10.1145/3105906.\nArvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. Efﬁcient non-parametric estimation\nof multiple embeddings per word in vector space. In Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pp. 1059–1069, Doha, Qatar, October 2014. Association for Computa-\ntional Linguistics. doi: 10.3115/v1/D14-1113. URL https://www.aclweb.org/anthology/D14-1113.\nJeffrey Pennington, Richard Socher, and Christopher Manni ng. Glove: Global vectors for word representation. In\nProceedings of the 2014 Conference on Empirical Methods in N atural Language Processing (EMNLP), pp. 1532–\n1543, Doha, Qatar, October 2014. Association for Computati onal Linguistics. doi: 10.3115/v1/D14-1162. URL\nhttps://www.aclweb.org/anthology/D14-1162.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Ch ristopher Clark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language T echnologies, V olume 1 (Long P apers), pp. 2227–\n2237, New Orleans, Louisiana, June 2018. Association for Co mputational Linguistics. doi: 10.18653/v1/N18-1202.\nURL https://www.aclweb.org/anthology/N18-1202.\nMichael Pradel and Koushik Sen. Deepbugs: A learning approa ch to name-based bug detection. Proc. ACM\nProgram. Lang. , 2(OOPSLA):147:1–147:25, October 2018. ISSN 2475-1421. d oi: 10.1145/3276517. URL\nhttp://doi.acm.org/10.1145/3276517.\nV eselin Raychev, Pavol Bielik, Martin V echev, and Andreas K rause. Learning programs from noisy data. In Proceed-\nings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’16,\npp. 761–774, New Y ork, NY , USA, 2016. ACM. ISBN 978-1-4503-3 549-2. doi: 10.1145/2837614.2837671. URL\nhttp://doi.acm.org/10.1145/2837614.2837671.\nCaitlin Sadowski, Jeffrey van Gogh, Ciera Jaspan, Emma S ¨ od erberg, and Collin Winter. Tricorder: Building a\nprogram analysis ecosystem. In Proceedings of the 37th International Conference on Softwa re Engineering -\nV olume 1, ICSE ’15, pp. 598–608, Piscataway, NJ, USA, 2015. IEEE Pres s. ISBN 978-1-4799-1934-5. URL\nhttp://dl.acm.org/citation.cfm?id=2818754.2818828.\nJoseph Turian, Lev Ratinov, and Y oshua Bengio. W ord represe ntations: A simple and general method for semi-\nsupervised learning. In Proceedings of the 48th Annual Meeting of the Association fo r Computational Linguis-\ntics, ACL ’10, pp. 384–394, Stroudsburg, P A, USA, 2010. Associat ion for Computational Linguistics. URL\nhttp://dl.acm.org/citation.cfm?id=1858681.1858721.\nMarko V asic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh singh. Neural program repair by\njointly learning to localize and repair. In International Conference on Learning Representations , 2019. URL\nhttps://openreview.net/forum?id=ByloJ20qtm.\nMartin White, Michele Tufano, Matias Martinez, Martin Monp errus, and Denys Poshyvanyk. Sorting and transforming\nprogram repair ingredients via deep learning code similari ties. pp. 479–490, 02 2019. doi: 10.1109/SANER.2019.\n8668043.\n10\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu . Charagram: Embedding words and sentences via\ncharacter n-grams. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,\npp. 1504–1515, Austin, T exas, November 2016. Association f or Computational Linguistics. doi: 10.18653/v1/\nD16-1157. URL https://www.aclweb.org/anthology/D16-1157.\nZhilin Y ang, Zihang Dai, Y iming Y ang, Jaime G. Carbonell, Ru slan Salakhutdinov, and Quoc V . Le. XL-\nNet: Generalized autoregressive pretraining for language understanding. CoRR, abs/1906.08237, 2019. URL\nhttp://arxiv.org/abs/1906.08237.\nPengcheng Y in, Graham Neubig, Miltiadis Allamanis, Marc Br ockschmidt, and Alexander L. Gaunt. Learning to\nrepresent edits. CoRR, abs/1810.13337, 2018. URL http://arxiv.org/abs/1810.13337.\n11\nA N AME EXTRACTIO N HEURISTI C\nIn order for DeepBugs to operate it is necessary to extract id entiﬁers or literals for each expression part of the stateme nt.\nThe bug detector for swapped arguments utilizes the followi ng elements of the function call:\nBase Object: The expression on which the function is called.\nCallee: The called function.\nArgument 1: The expression consisting the ﬁrst argument of the called fu nction.\nArgument 2: The expression consisting the ﬁrst argument of the called fu nction.\nSimilarly the bug detectors for incorrect binary operators and operands utilize the following elements of the binary\nexpression:\nBinary Operator: The binary operator utilized in the expression.\nLeft Operand: The left operand of the binary expression.\nRight Operand: The right operand of the binary expression.\nW e next describe the extraction heuristic, which is shared b y all the bug detectors. The heuristic takes as input a node\nn representing an expression and returns name(n) based on the following rules:\n•Identiﬁer: return its name.\n•Literal: return its value.\n•this expression: return this.\n•Update expression with argument x: return name(x).\n•Member expression accessing a property p: return name(p).\n•Member expression accessing a property base[p]: return name(base).\n•Call expression base.callee (... ): return name(callee).\n•Property node n: If n.key does not exist return name(n.value ). If name(n.key ) does not exist return\nname(n.value ) . Otherwise randomly return either name(n.value ) or name()n.key ).\n•Binary expression with left operand l and right operand r: Run the heuristic on both l and r to retrieve\nname(l) and name(r). If name(l) does not exist return name(r). If name(r) does not exist return\nname(l). Otherwise randomly return either name(l) ir name(r).\n•Logical expression with left operand l and right operand r: Run the heuristic on both l and r to retrieve\nname(l) and name(r). If name(l) does not exist return name(r). If name(r) does not exist return\nname(l). Otherwise randomly return either name(l) ir name(r).\n•Assignment expression with left operand l and right operand r: Run the heuristic on both l and r to re-\ntrieve name(l) and name(r). If name(l) does not exist return name(r). If name(r) does not exist return\nname(l). Otherwise, randomly return either name(l) ir name(r).\n•Unary expression with argument u : Return name(u).\n•Array expression with elements li : For all li that name(li) exists randomly choose one of them and return\nname(li).\n•Conditional expression with operands c, l, and r: Randomly choose one out of c, l, r for which a name exists\nand return its name.\n•Function expression: return function .\n•Object expression: return {.\n•New expression with a constructor function call c: return name(c).\nAll random decisions follow a uniform distribution.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5938388705253601
    },
    {
      "name": "Programming language",
      "score": 0.5567853450775146
    },
    {
      "name": "Code (set theory)",
      "score": 0.5542280077934265
    },
    {
      "name": "Source code",
      "score": 0.4162783920764923
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.07889664173126221
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 34
}