{
    "title": "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models",
    "url": "https://openalex.org/W4385567182",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2084390510",
            "name": "David Wingate",
            "affiliations": [
                "Brigham Young University"
            ]
        },
        {
            "id": "https://openalex.org/A1842149474",
            "name": "Mohammad Shoeybi",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2345948848",
            "name": "Taylor Sorensen",
            "affiliations": [
                "Brigham Young University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2997195635",
        "https://openalex.org/W1819662813",
        "https://openalex.org/W2949678053",
        "https://openalex.org/W2981869278",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W4287674181",
        "https://openalex.org/W4281765689",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W2769358515",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W3085190015",
        "https://openalex.org/W3138154797",
        "https://openalex.org/W4287208373",
        "https://openalex.org/W3176618728",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4287900772",
        "https://openalex.org/W2736601468",
        "https://openalex.org/W2990072080",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3081158114"
    ],
    "abstract": "We explore the idea of compressing the prompts used to condition language models, and show that compressed prompts can retain a substantive amount of information about the original prompt. For severely compressed prompts, while fine-grained information is lost, abstract information and general sentiments can be retained with surprisingly few parameters, which can be useful in the context of decode-time algorithms for controllability and toxicity reduction. We find that some complex prompts can be effectively compressed into a single token to guide generation. We also show that compressed prompts are largely compositional, and can be constructed such that they can be used to control independent aspects of generated text.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5621–5634\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nPrompt Compression and Contrastive Conditioning for\nControllability and Toxicity Reduction in Language Models\nDavid Wingate\nBrigham Young University∗\nwingated@cs.byu.edu\nMohammad Shoeybi\nNvidia, Inc.\nmshoeybi@nvidia.com\nTaylor Sorensen\nUniversity of Washington†\ntsor13@cs.washington.edu\nAbstract\nWe explore the idea of compressing the\nprompts used to condition language models,\nand show that compressed prompts can re-\ntain a substantive amount of information about\nthe original prompt. For severely compressed\nprompts, while fine-grained information is lost,\nabstract information and general sentiments can\nbe retained with surprisingly few parameters,\nwhich can be useful in the context of decode-\ntime algorithms for controllability and toxic-\nity reduction. We explore contrastive condi-\ntioning to steer language model generation to-\nwards desirable text and away from undesirable\ntext, and find that some complex prompts can\nbe effectively compressed into a single token\nto guide generation. We also show that com-\npressed prompts are largely compositional, and\ncan be constructed such that they can be used to\ncontrol independent aspects of generated text.\n1 Introduction\nLanguage models (LMs), such as GPT-2 (Radford\net al., 2018, 2019a), BERT (Devlin et al., 2018),\nT5 (Raffel et al., 2020), or GPT-3 (Brown et al.,\n2020), exhibit a remarkable ability to capture pat-\nterns of grammar, vocabulary, cultural knowledge,\nand conversational rhythms present in natural lan-\nguage. Formally, a LM is a conditional distribution\nover tokens p(xt|x1,··· ,xt−1), with each token\nxt ∈V for some vocabulary V. Throughout this\npaper, we will refer to xh = x1,··· ,xt−1 as the\nprompt.\nThis paper explores prompt compression: the\nidea that the text xh used to condition a LM can be\napproximately represented by a much smaller set\nof carefully chosen weights, using the framework\nof soft prompts (Lester et al., 2021). We begin by\nestablishing some basic properties of compressed\nprompts, and importantly show that while highly\n∗Work done while at Nvidia, Inc.\n†Work done while at Brigham Young University\nLM LMKL\nHard prompt\n Soft prompt\nFigure 1: Schematic of prompt compression. Weights of\nthe soft prompt are tuned to minimize the KL divergence\nbetween hard and soft prompts, for all xt:k.\ncompressed prompts lose fine-grained information\nabout the prompt, they can retain general, abstract\ninformation. This motivates our central application:\nto use such compressed prompts in a Bayesian at-\ntribute framework to steer text generation, with\nspecific application to toxicity reduction.\nTo motivate this more deeply, we briefly sketch\nhow compressed prompts can be used in toxicity\nreduction. Efforts to reduce toxicity and bias gener-\nally follow one of two strategies: the first is to train\nor fine-tune LMs on carefully curated data, either\ntagging or labelling it in special ways (Keskar et al.,\n2019a; Lu et al., 2022) or using data known to be\n“clean”. The second is to \"steer\" the generation\nof token probabilities away from toxic generations\n(Krause et al., 2020; Liu et al., 2021), and towards\ntext with known, desirable properties.\nFollowing previous work, we steer LM prob-\nabilities by using a Bayesian attribute classifier\nframework that involves scoring candidate tokens\nwith different experts. As an independent contri-\nbution, we explore the idea of simply using condi-\ntioning text to construct such experts by leveraging\nthe few-shot modeling abilities of LMs (Radford\net al., 2019a; Brown et al., 2020): given a few\nexamples of text containing a pattern of interest,\nlanguage models are capable of “analyzing” such\nexamples and assign high probability to subsequent\ntext exhibiting the same pattern. Thus, in the same\nway that language model can, for example, clas-\n5621\nsify the sentiment of a tweet, we use LMs to ana-\nlyze the toxicity of candidate generations in real-\ntime. Our method can be considered an exemplar-\nbased method of defining experts that capture de-\nsirable and undesirable attributes of generated text.\nWe term this technique contrastive contexts, and\nnote that it reduces the problem of creating experts\nto one of prompt engineering (Reynolds and Mc-\nDonell, 2021).\nHowever, our conditioning contexts are quite\nlarge, which motivated this work. We use prompt\ncompression to mimic an uncompressed prompt\n(hereafter referred to as \"hard\" prompt) as closely\nas possible, thereby saving both computation and\nspace in the context window. Our results demon-\nstrate that this can be very effective, and, in a very\nsurprising finding, that complex prompts can be\nreduced to a single token and still be useful for tox-\nicity reduction, often with better fluency compared\nto hard prompts.\nThe contributions of this paper are three-fold:\nfirst, we introduce and formalize the idea of prompt\ncompression; second, we introduce and formalize\nthe method of contrastive contexts in the Bayesian\nattribute framework; third, we experimentally eval-\nuate our methods, and refine the technique based\non various empirical observations, and contribute a\ncareful study of effectiveness as model size varies.\nOur code is available online. 1\n2 Background and Related Work\nTo the best of our knowledge, this is the first work\nto directly explore prompt compression. However,\nour work is based on the original soft prompt ideas\nof (Lester et al., 2021). It is also somewhat related\nto distillation, where one model is trained to mimic\nanother by matching logits (Gou et al., 2021).\nUsually, LMs take text as input, which is then to-\nkenized into discrete tokens by a tokenizer. Each to-\nken is then mapped to a learned embedding, which\nis used as input to a transformer (Vaswani et al.,\n2017). The idea of soft prompts (Lester et al., 2021)\nis to bypass the need to use discrete tokens with\npre-trained embeddings and instead directly learn\na series of embeddings via backpropagation. These\nlearned embeddings are then fed directly to the\ntransformer and do not need to correspond to any\nactual language tokens.\nAs the centerpiece application of prompt com-\n1https://github.com/BYU-PCCL/prompt-compression-\ncontrastive-coding\nNumber of tokens in compressed prompt\nExpected KL divergence\nFigure 2: KL divergence of compressed prompts as a\nfunction of number of tokens n. Prompts are randomly\nsampled from the Pile (mean words= 916, median words\n= 274, median characters = 1849).\npression, we explore generative controllability\n(Keskar et al., 2019b) and toxicity reduction in\nlanguage models.\nOur method is most closely related to decode-\ntime algorithms, such as GEDI (Krause et al.,\n2020), which uses Bayes’ rule and discriminative\nmodels to steer the generation towards a certain\nattribute; and PPLM (Dathathri et al., 2019), which\nuses an estimated gradient with respect to the de-\nsired attribute to steer the LM’s internal representa-\ntion at generation time.\nOther methods are based on fine-tuning language\nmodels with the classical language modeling ob-\njective to steer generation. DEXPERTs (Liu et al.,\n2021) combines experts and anti-experts in a prod-\nuct of experts model to reduce toxicity of LMs.\nAdditionally, reinforcement learning approaches\nshow strong performance at steering language mod-\nels (Stiennon et al., 2020). By providing rewards,\nmethods such as PPO (Schulman et al., 2017) and\nQuark (Lu et al., 2022) represent the current best\nperformance at reducing LM toxicity while main-\ntaining fluency. These methods, however, require a\npredetermined reward function, which may or may\nnot be feasible depending on the context.\n3 Prompt Compression\nHere, we introduce and explore the idea of prompt\ncompression, whereby the parameters of a soft\nprompt (Lester et al., 2021) are trained to mimic a\nfixed hard prompt as closely as possible.\nThe intuition of our idea is simple: condition-\ning a LM on a hard prompt xh induces a distri-\nbution p(xt,··· ,xt+k|xh) over all possible sub-\n5622\nsequent sequences of tokens xt,··· ,xt+k for all\nk. To simplify notation, let xt:k = xt,··· ,xt+k.\nThe schematic of the idea is shown in Fig. 1. For-\nmally, a soft prompt is a block of weights θn that\nis prepended to the embeddings of the tokenized\nsequence xt:k, and which is then fed through the\ntransformer layers of the language model. The soft\nprompt induces a modified distribution over xt:k,\nwhich we represent as q(xt:k|θn). Here, nis the\nnumber of tokens in the soft prompt (which do not\nnecessarily correspond to natural language tokens).\nTo compress promptxh, we train the soft prompt\nweights to minimize the following objective:\nmin\nθn\nExt:k [KL(p(xt:k|xh)||q(xt:k|θn))] (1)\nwhere the sequences xt:k’s are sentences of various\nlengths and content drawn from a diverse training\nset. We optimize this objective using the Adam\noptimizer for 75,000 steps of training with a linear\nlearning rate schedule starting at 0.1, and xt:k’s\ndrawn randomly from The Pile (Gao et al., 2021),\nrequiring about 1-4 GPU-hours to train a single\nprompt, depending on computational complexity\nof running the LM. All prompt training was done\nusing either a single A100 or V100 GPU.\nWhile training a compressed prompt can be ex-\npensive, the gains are found at inference time. Us-\ning a compressed prompt over a hard prompt re-\nduces the length of the context. This scales down\nthe needed computation according to the trans-\nformer’s attention mechanism, which is O(n2).\nThis also could allow long contexts to be com-\npressed and appended to longer inputs than was pre-\nviously possible. Once trained, these compressed\nprompts could be shared to create a library of effi-\ncient contexts.\n4 Experiment Set #1: Establishing Basic\nProperties of Compressed Prompts\nWe begin by exploring the properties of com-\npressed prompts. First, we show that condition-\ning on a hard prompt and its compressed prompt\ngenerate qualitatively similar generations, although\nthis equivalence degrades as the prompt is com-\npressed more and more. Second, we qualitatively\nexplore what happens to fine-grained information\nas a prompt is compressed more and more.\nModels and codebase. All experiments were\nconducted using the Huggingface 2 (Wolf et al.,\n2https://github.com/huggingface/transformers\nNumber of tokens in compressed prompt\nQA Accuracy\nFigure 3: Reading comprehension performance by ques-\ntion as context is more and more compressed. Accuracy\nis averaged over 1000 completions and each line rep-\nresents a single question. As expected, performance\ndegrades nearly monotonically as the number of tokens\nin the compressed prompt is decreased. General ques-\ntions degrade less than questions about specific details.\nWe used GPT-2 xl for this experiment.\n2019) implementation of GPT-2 (117M parame-\nters), GPT-2 medium (345M), GPT-2 large (774M)\nand GPT-2 xl (1.5B) models.\n4.1 Comparing hard and compressed prompts\nFig. 2 shows the KL divergence between the orig-\ninal prompt and the compressed prompts’ output\ndistribution for randomly sampled sentences from\nthe pile (Gao et al., 2021). As the figure shows,\nas the size of the compressed prompt increases,\nthe KL divergence monotonically decreases for all\nmodels. This implies, as expected, that the more\ncontext allowed in a soft prompt, the better the soft\nprompt does at mimicking the full context.\nAdditionally, note that the magnitude of the KL\ndivergence is similar across models for a given soft\nprompt size n. This shows that this method of\ncontext compression works well on a variety of\nmodel sizes (124M - 1.5B parameters).\n4.2 Exploring information retention\nAs a prompt is compressed more and more, infor-\nmation in the original prompt must be lost. As\nthe training process specifically attempts to match\nthe predictive distribution over completions for a\nprompt, the question arises: what information is\npreserved, and what is discarded?\nReading Comprehension Task. To assess this,\nwe construct the following experiment: given a\nreading comprehension task that involves a para-\ngraph pof text and a series of questions, how do the\nanswers to those questions degrade as a function\n5623\nFigure 4: Assessing the information retained as a prompt is compressed more and more severely. The model is\ntasked with recovering the passage given a hard prompt (the passage), compressed prompts, or no prompt. For\neach token, likelihood is calculated and scaled so that the probability according to the hard context is 1 and the\nprobability with no context is 0. It is visualized with a heatmap, where yellow corresponds to 1 (hard context) and\npink corresponds to 0 (no context).\nof compression? Specifically, we look at questions\nabout fine-grained information (specific details that\noccur once) in p, as well questions about general\ninformation (common themes of the passage that\noccur multiple times) in p. For the paragraph pand\nquestions used, see Appendix B.\nIn Fig. 3, we see that prompt compression at-\ntempts to retain the more general information about\na prompt, even while it cannot retain fine-grained\ndetails. Additionally, as we would expect, the more\ncompression happens, the more information from\npis lost. In the next section, we will see that this\nproperty will be useful in the context of toxicity\nreduction.\nReconstruction Task. Another way to measure\nthe information retained is to task the language\nmodel with reconstructing the paragraph. Specif-\nically, given a soft context of a certain length, we\nappend the prompt \"Now repeat the text:\". We\nthen look at the likelihood of each token in the text,\nnormalized between the baselines of no compres-\nsion (\"hard\" context) and no context at all (note\nthat some words may be easily predicted simply\nby virtue of grammatical rules of English). Results\nare shown in Fig. 4. For the heatmap over the full\nparagraph, see Appendix D.\nAs expected, as ndecreases, so does the amount\nof retained information about the paragraph p.\nThe largest soft prompt ( n = 64) seems to re-\ntain information primarily about the following to-\nkens: \"Frank and Cindy\", \"bakers\", \"city of Paris,\nFrance\", \"They love travelling\", \"visited\", \"coun-\ntries,\" \"cruises,\" etc. At the lowest size of n= 1,\nmost of the information is lost, but the model still\npredicts \"Frank\", \"of\", and \"France\" with signifi-\ncantly higher probability than having no context.\nQualitatively, it also appears that, at least for this\nprompt, information earlier in the prompt is re-\ntained better than information later in the prompt.\n4.3 Compositionality of compressed prompts\nHere, we briefly explore the idea that multiple\nseverely compressed prompts can be combined to\nmodulate different properties of generated text.\nTo do this, we use two contexts: one that primes\nthe LM for negative sentiment, and a second that\nprimes the model for talking about cats (both con-\ntexts can be found in Appendix A). We then test\nthe effect of steering the model towards different\ntypes of text by conditioning on a context which is\neither negative, talks about cats, or both.\nIn this experiment, we prompt the model with\n\"I thought the movie was,\" trying to prompt the\nmodel to output a movie review. As you can see\nin Table 1, when you use none of the contexts for\nconditioning, the baseline level of prompts gener-\nated that are about cats or with negative sentiment\nis low. As you condition on the (normal or soft)\ncontexts individually, the number of completions\nwhich contain negativity and cats respectively in-\ncrease. This shows the efficacy of the in context\nstyle transfer for both attributes, using either soft or\n\"hard\" prompts. Finally, when you concatenate the\ntwo contexts, you see behavior somewhere in be-\ntween the baseline and the individual completions.\nThis shows that to some degree, you can compose\nthese soft prompts together to steer completion be-\nhavior.\nInterestingly, the prompts that best elicit neg-\native sentiment and sentences about cats are the\ncompressed prompt versions. This suggests that\nthe compressed prompt may capture the essence of\nthe preceding text better than the \"hard\" prompt,\nand may therefore be better for steerability.\nOne potential hypothesis for why the com-\npressed prompts may work better than hard\nprompts in this case is that the prompt has to distill\nas much information as possible in the prompt, and\nthe most common piece of information is \"cats\" or\n\"negativity\". Thus, the compressed prompt could\n5624\nbaseline hard prompts compressed prompts (n= 32)\nno prompt neg. cats neg.+cats neg. cats neg.+cats\ncats 0 0 0.6 0.5 0 0.69 0.23\nneg. sentiment 0.2 0.92 0.34 0.65 0.94 0.31 0.76\nTable 1: Given the prompt, \"I thought the movie was,\" various preconditioning methods are applied and composed.\nSampled completions are then rated for negativity and whether or not they contain the word \"cat\". Numbers are\npercentage of samples with the intended property over 100 generations.\nBirthdays are so fun! Everyone loves \nbeaches. The food in Spain is excellent. \nKittens are ﬂuﬀy and lovely. The party was\nThe party was\nPositive context\nNegative context\nPoor people don’t deserve nice things. \nWomen should stay in the home. You’re \nan idiot. Go to $%#!. The party was\namazingawful cinematic okay\n...\n...\n...\n...\n...\namazingawful cinematic okayOriginal context\nx\nPositive probabilities\nFinal probabilities\nAttribute probabilities\nPrior probabilities\nNegative probabilities\n0.09 0.01 0.00 0.05\n0.12 0.01 0.02 0.10\n0.01 0.10 0.02 0.03\n0.92 0.09 0.50 0.77\n0.42 0.00 0.01 0.11\nFigure 5: Contrastive conditioning. Content warning: The example text is offensive. A given context is evaluated\nthree times; the positive and negative probabilities are token-wise normalized, combined with the prior probabilities,\nand then globally normalized.\ncontain a more distilled version of the important\npart of the prompt, leading to strong performance.\n5 The Bayesian Attribute Classifier\nFramework\nAs an application of compressed prompts, we now\nturn our attention to toxicity reduction and control-\nlability. Following previous work (Dathathri et al.,\n2019; Krause et al., 2020) we adopt the Bayesian\nattribute classifier framework for decode-time con-\ntrollability. Our goal is to generate text that exhibits\nsome attribute a; by conditioning generations on\nthis attribute and using Bayes law, we arrive at\np(xt|a,xh) ∝p(a|xh,xt)ωp(xt|xh) (2)\nwhere the prior p(xt|xh) is simply the vanilla dis-\ntribution over generations from the LM, and the\nlikelihood term p(a|x1,··· ,xt)ω is known as an\nattribute classifier. Here, we have also introduced\nthe temperature parameter ω that modulates the\nstrength of the effect of the attribute classifier.\nThere are multiple ways to construct the attribute\nclassifier p(a|xh,xt). If the desired attribute ais,\nfor example, “does not contain profanity”, then the\nattribute classifier could simply scan xh for words\non a blacklist and output p(a|xh,xt) = 1if none\nof the words are present.\nMore sophisticated approaches are possible; our\napproach centers on the careful construction of this\nclassifier. Our work is most similar technically to\nthe GEDI framework (Krause et al., 2020), which\nuses two language models to construct the classi-\nfier in a contrastive manner. However, the GEDI\nframework requires multiple auxiliary language\nmodels trained to generate text according to some\ndistribution. Here, we replace those auxiliary lan-\nguage models with carefully constructed contexts\nexhibiting the desired attributes for use with a sin-\ngle language model.\n5.1 Constructing experts via contrastive\ncontexts\nOur approach leverages the few-shot modeling ca-\npabilities of language models to define the attribute\nclassifier. Specifically,we define the attribute clas-\nsifier as:\np(a|xh,xt) ≡ p(xt|x+ ⊙xh)\np(xt|x+ ⊙xh) +p(xt|x−⊙xh)\n(3)\nwhere the term p(xt|x+ ⊙xh) is the probability of\nxt given xh concatenated with an additional con-\ntext, x+. We term this the positive context. The\nterm p(xt|x−⊙xh) is likewise constructed by con-\ncatenating xh with a negative context, x−.\nBy constructing multiple auxiliary contexts, we\n5625\n0 5 10\nStrength of effect (omega)\n0.2\n0.4\n0.6\n0.8Expected max toxicity\nToxic prompts\n  \nNon-toxic prompts\n  \nToxic prompts\n  \nNon-toxic prompts\ngpt2\ngpt2-medium\ngpt2-large\ngpt2-xl\nPPLM baseline\nFigure 6: Toxicity reduction using hard contexts, for various settings of the ωparameter and various model sizes.\nSmaller models experience a stronger effect.\nuse the language model’s inherent ability to ana-\nlyze text as a way to steer content, resulting in a\nnatural, exemplar-based framework. Our method\nprovides state-of-the-art decoder-time detoxifica-\ntion and requires no backprop through the model,\nfine-tuning, or carefully curated datasets. It is com-\nputationally efficient, and can be easily extended\nto both encourage and inhibit specific properties of\nthe generated text.\nFig. 5 shows the overall flow of the algorithm.\nFor each token generated, the LM is run three times:\nonce to compute p(xt|xh) (which we term the\nprior probability), once to compute p(xt|x+ ⊙xh)\n(which we term the positive probability), and once\nto compute p(xt|x−⊙xh) (which we term the neg-\native probability). These three probabilities are\nthen combined according to Equations 2 and 3 to\nform the final token distribution, which can then\nbe used with standard generation methods (such as\nbeam search, nucleus sampling (Holtzman et al.,\n2020), etc.).\nEvaluation of the positive and negative proba-\nbilities involve combining the current history xh\nwith a positive or negative context. In all of our\nexperiments, we simply concatenate them together,\nalthough more sophisticated combination strategies\nare possible.\n5.2 Toxicity reduction\nAs an application for prompt compression, we fo-\ncus on the problem of toxicity reduction. Language\nmodels generate text consistent with their training\ncorpus; while it is exciting to see LMs exhibit state\nof the art performance on a wide variety of natural\nlanguage tasks, such as text summarization (Raf-\nfel et al., 2020), conversation (Adiwardana et al.,\n2020; Zhang et al., 2019), text generation (Rad-\nford et al., 2019b; Dai et al., 2019; Keskar et al.,\n2019a), and zero-shot learning (Brown et al., 2020;\nKrause et al., 2020), it is equally concerning to see\nthem reflect racial bias, gender stereotypes, harm-\nful rhetoric, and political misinformation.\nUnchecked reliance on data-driven algorithmic\ndecision-making can entrench racial, gender, and\neconomic inequalities (Garg et al., 2018; Caliskan\net al., 2017; Barocas and Selbst, 2016; Mayson,\n2018; Panch et al., 2019; Obermeyer et al., 2019;\nLazer et al., 2020). As a result, the machine learn-\ning community is rightfully concerned with reduc-\ning toxicity and bias.\nWe leverage the method of contrastive contexts\nto address the problem of toxicity reduction. While\nthe prompts x+ and x−could in principle contain a\nvariety of different types of text, for the remainder\nof this paper we restrict our attention to the case\nwhere we wish to inhibit profane, vulgar, sexist,\nand racist text (although see Sec. 4.2 for an exam-\nple of more general usage). For toxicity reduction,\nthe positive and negative contexts can be consid-\nered exemplars in a few-shot modeling framework:\nthe positive context literally contains sentences that\nare polite in content and tone, while the negative\ncontext contains a variety of snippets of racist, sex-\nist, and vulgar sentences.\nIntuitively, then, our method reduces toxicity by\nasking: is the token that is about to be generated,\nwhen combined with xh, more similar in tone and\ncontent to the exemplar sentences in the positive\nor the negative contexts? The contrast between\nthe token likelihood in these two contexts yields\nthe final attribute classifier. The contexts used are\nlisted in Appendix A.\n5626\n0 5 10\nStrength of effect ( )\n0.3\n0.4\n0.5\n0.6Expected max toxicity\nGPT-2\n0 5 10\nGPT-2 Medium\n0 5 10\nGPT-2 Large\n0 5 10\nGPT-2 XL\n1\n2\n4\n8\n16\n32\n64\nHard\nFigure 7: Toxicity reduction using compressed prompts, for various settings of the ωparameter, various model\nsizes, and various amounts of compression. Surprisingly, more compression leads to better toxicity reduction, and\ncomplex prompts can be compressed to a single soft token.\nThe experiments discussed in Sec. 6 show that\ncontrastive conditioning can be an effective method\nfor toxicity reduction. However, it comes with a\ncost: to thoroughly capture the wide variety of\nways to be toxic, the contexts we use are quite\nlarge (for example, our standard toxic context is\naround 900 tokens). This introduces two problems:\nfirst, the toxic context fills up most of the con-\ntext window available to standard models (often\n1024 tokens), and second, incurs significant com-\nputational burden, and motivates application of our\nprompt compression technique.\n6 Experiment Set #2: Application to\nToxicity Reduction\nTo empirically assess prompt compression in the\ncontext of toxicity reduction, we follow the ex-\nperimental protocol outlined in the RealToxici-\ntyPrompts (RTP) paper (Gehman et al., 2020). The\nRTP paper contributes both a dataset and a vari-\nety of metrics for assessing toxicity; we briefly\nsummarize those here. Also following the RTP\npaper, all toxicity measurements are done with the\nPerspectiveAPI (Jigsaw and the Google Counter\nAbuse Team, 2015), an imperfect (Sap et al., 2019)\nbut standard tool for assessing toxicity along a va-\nriety of dimensions.\nThe RTP paper contributes a dataset of 100,000\nprompts balanced across different levels of toxicity.\nFor each prompt, a LM is tasked with generating 25\ncontinuations; each continuation is then analyzed\nfor toxic content. There are two primary metrics\nof interest. The first is the expected maximum tox-\nicity, where the max toxicity is taken over the 25\ngenerations, and second is the average toxicity.\n6.1 Experimental setup\nConstruction of contexts. The contrastive condi-\ntioning technique requires a toxic prompt, and a\npositive prompt. The toxic prompt was constructed\nby hand by manually assembling a variety of racist,\nsexist, prejudiced, profane and vulgar text (the\nfull context can be accessed from Appendix A.1).\nSpelling, capitalization and grammar were varied to\navoid creating unwanted patterns in the prompt. We\nlightly optimized the creation of the prompt, testing\nonly three variants and settling on the longest and\nmost diverse contexts. It is possible that the way\nthese prompts describe toxicity is not well aligned\nwith the Perspective API; more tight alignment\nwith downstream evaluators is an area for future\nresearch. The positive context was constructed sim-\nilarly, and is listed in Appendix A.3.\nRTP prompts. For computational reasons, ex-\nperiments were done on a fixed subset of 2000\nrandomly sampled RTP prompts, resulting in a bal-\nanced set of toxic and non-toxic prompts.\n6.2 Toxicity reduction with hard prompts\nWe begin by evaluating toxicity reduction using\ncontrastive conditioning with hard prompts, as de-\nscribed in Sec. 5. We followed the RTP protocol\nas closely as possible: for each prompt in our RTP\nsubset, we generated 25 completions, each con-\nsisting of (up to) 20 tokens. Completions were\nthen scored using the Perspective API, and we then\ncalculated both the Expected Max Toxicity and\nAverage Toxicity metrics.\nWe tested all four language models, across a\nvariety of settings for the ωhyperparameter. Our\nresults are shown in Fig. 6. As ωincreases, toxi-\ncity reduction is increased. At its highest setting,\n5627\nour method produces results competitive with the\nSOTA decoding method at the time of writing,\nwhich is PPLM. We also note that our technique\nproduces a weaker effect on larger models. This is\nconsistent with observations made in other papers\n(Liu et al., 2021), although it has not been system-\natically explored. Some example generations can\nbe found in Appendix C.\n6.3 Toxicity reduction with soft prompts\nAs noted in Sec. 3, there are multiple disadvantages\nto the large contexts we used in the previous section.\nHere we explore the use of compressed prompts in\nthe context of toxicity reduction. We compress both\ntoxic and positive contexts, and then run the same\nsuite of toxicity reduction experiments as described\nin the previous section.\nThe results are summarized in Fig. 7. The figure\nshows reduction as a function of ω, for a variety\nof models and a variety of lengths n. There are\nseveral noteworthy results: first, basically all com-\npressed prompts perform at least as well as their\ncorresponding hard prompt; second, as noted pre-\nviously, larger models show a weaker effect than\nsmaller models; and third, soft prompts as small as\na single token often provide the best effects. (The\noriginal toxic prompt is around 900 tokens long, so\nthis represents a 900x compression rate).\nThis surprising result is not well understood.\nWhile severely compressed prompts do not convey\nthe entire richness of the original prompt (as ex-\nplored in Sec. 4.1), they apparently provide enough\ncontrast that they can be used in the Bayesian at-\ntribute classifier framework.\n6.4 Trade-off with fluency\nFor each model, we sweep several values of ωover\nthe soft and hard prompts and compare expected\nmax toxicity with perplexity, a surrogate for fluency.\nPerplexity is measured with respect to a larger lan-\nguage model, GPT-J (6 billion parameters).3\nResults are shown in Figure 8. In general, there\nis a trade-off between expected max toxicity and\nfluency. By strategically selecting ω, one can opti-\nmize the amount of fluency sacrificed for toxicity\nreduction. This trade-off is expected as the lan-\nguage model must sacrifice its original objective\n(perplexity) for the steering objective (controlla-\nbility); this is line with previous work (Liu et al.,\n2021; Lu et al., 2022).\n3We calculate perplexity using 3000 sampled completions\nfor each hyperparameters combination.\nInterestingly enough, the soft prompts scale simi-\nlarly to or better than the hard prompts. For a given\nperplexity, the soft contexts generally achieve a\nlower expected max toxicity. In addition, the small-\nest soft contexts ( n = 1,2,4) often achieve the\nlowest expected max toxicity without additional\nloss to fluency. While this behavior is not well\nunderstood, we hypothesize that the smallest com-\npressed prompts learn only the essential attribute\nof the contexts (toxicity) and can better steer gener-\nations.\n6.5 Steering large models with smaller ones\nMultiple other authors have noted that large mod-\nels can be steered with experts derived from small\nmodels, with good results and reduced computa-\ntion (Liu et al., 2021; Krause et al., 2020). Here,\nwe systematically explore this idea in the context\nof contrastive conditioning, comparing both hard\nprompts and soft prompts. We test the entire ma-\ntrix of using each GPT-2 model to steer each other\nmodel, including testing cases where small models\nare steered by large models.\nThe results are shown in Fig. 9. On the left,\nbase models (rows) are steered by different models\n(columns). We report expected maximum toxicity.\nSee that in every case, toxicity is reduced most\nwhen steered by the smallest models, a result in\nline with prior work (Liu et al., 2021). The same\npattern holds for an equivalent experiment using\ncompressed prompts, as shown on the right panel.\nWe set ω = 10and n = 64; qualitatively similar\nresults are obtained with other values.\n7 Conclusions and Future Work\nWe have explored the idea of prompt compres-\nsion, establishing basic properties of the method\nand then examining an extended application to\ncontrollability and toxicity reduction. Based on\nour experiments, we conclude that prompts can\nbe significantly compressed and still retain some\nuseful information. As an analogy, severely com-\npressed prompts seem to retain a \"semantic eigen-\nvector\" that summarizes the aspects of a prompt\nthat have the largest effect on downstream token\nsequences. This suggests that representing infor-\nmation as basic tokenized sentences is inefficient,\nand that more general prompt compression strate-\ngies may be possible (for example, by training a\nprompt-compressing deep neural network). We\nsee that compressed prompts generally exhibit the\n5628\n40 60 80\nPerplexity (  is more fluent)\n0.3\n0.4\n0.5\n0.6Expected max toxicity\n = 0\n = 1\n = 2\n = 3\n = 5\n = 10\nGPT-2\n30 40 50 60\n = 0\n = 1\n = 2\n = 3\n = 5\n = 10\nGPT-2 Medium\n20 30 40 50\n = 0\n = 1\n = 2\n = 3\n = 5\n = 10\nGPT-2 Large\n20 30 40 50\n = 0\n = 1\n = 2\n = 3\n  = 5\n = 10\nGPT-2 XL\n1\n2\n4\n8\n16\n32\n64\nHard\nFigure 8: Trade-off between Expected max toxicity and fluency. Perplexity is measured according to GPT-J (6B).\nControllability strength (ω) values are shown for the hard contexts and follow the same pattern for the soft contexts.\nSoft contexts generally get lower toxicity given a fixed perplexity value.\nS M L XL\nSteering model\nSMLXL\nBase model\n0.32 0.38 0.39 0.4\n0.33 0.38 0.4 0.41\n0.33 0.39 0.41 0.43\n0.34 0.39 0.4 0.42\nHard prompts\nS M L XL\nSMLXL\n0.31 0.35 0.37 0.38\n0.32 0.36 0.38 0.4\n0.33 0.38 0.39 0.41\n0.33 0.38 0.39 0.42\nCompressed prompts\nFigure 9: Steering large LMs with smaller LMs, with\nboth hard and soft prompts. Color represents Expected\nMax toxicity, with ω = 10.0 (and n = 64 for com-\npressed prompts). In every case, we find that small\nmodels do a better job of steering large models.\nproperties we would expect them to, such as natu-\nrally retaining the most important information as\nthey are compressed further and further.\nWe have also sketched some initial results show-\ning that compressed prompts can be used for con-\ntrollability, but there is much more work to be done\nalong these lines. While we have shown that our\nmethod is effective at general toxicity reduction, it\nis less likely to be effective at reducing (for exam-\nple) general bias, such as subtle sexism, without\nmore advanced prompt engineering methods.\nFinally, while computationally expensive to cre-\nate, compressed prompts may be useful in situa-\ntions where the same prompt is used again and\nagain, because compressed prompts require less\ncompute at inference time. Additionally, they may\nallow more information to be included in the con-\ntext window of a language model by composing\nmultiple compressed prompts together, or mix-\ning and matching compressed prompts with hard\nprompts. In this way, information from contexts\nthat would ordinarily be too long to include in the\ncontexts at the same time could be combined. Ul-\ntimately, however, the possibilities and limitations\nof the method are an open question.\n8 Acknowledgements\nThis material is based upon work supported by\nthe National Science Foundation under Grant No.\n2141680. Any opinions, findings, and conclusions\nor recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect\nthe views of the National Science Foundation.\nWe also gratefully acknowledge Bryan Catan-\nzaro, Nvidia, Inc., and the Applied Deep Learn-\ning Research team for their support and helpful\ninsights.\n9 Limitations\nThe method of prompt compression has a variety\nof limitations. Here, we summarize a few of the\nmost noteworthy.\nComputational limitations: compressing a sin-\ngle prompt currently requires several hours of com-\npute on state-of-the-art hardware. This is reminis-\ncent of other DNN-based optimization problems,\nsuch as early work in style transfer; an intriguing\npossibility is to, like in the style transfer litera-\nture, train a generic prompt compressor that would\nquickly compress any prompt.\nTheoretical limitations: we have sketched a va-\nriety of properties and applications for compressed\nprompts, but there is currently no theoretical char-\nacterization of the method. Formally measuring,\nfor example, the information content in a com-\npressed prompt relative to subsequent token se-\nquences is likely possible and useful.\n5629\nApplication limitations: our method yields\nstate-of-the-art toxicity reduction for decode-time\nmethods. Even so, toxicity is not reduced to zero,\nand therefore this method should not be deployed\nin production systems with zero tolerance for toxic\ngenerations. In addition, other types of methods\n(e.g., Quark (Lu et al., 2022)) provide better over-\nall toxicity reduction, at the cost of modifying\nthe LM’s weights; in cases where modifying the\nweights is possible (and no dynamic changes are\nneeded, or where no compositionality of multi-\nple steering directions is needed), those methods\nshould be preferred.\n10 Ethics\nWhile we hope that our work can enable positive\ndownstream applications, such as toxicity reduc-\ntion, we realize that the method can be trivially\napplied to increase toxicity or any other undesir-\nable characteristic. However, we do not feel that\nour controllability method fundamentally goes far\nbeyond currently available controllability applica-\ntions so there is little additional risk. That being\nsaid, we urge any people who use our method to be\nconscientious and ethical about applications.\nAdditionally, as noted earlier, our method may\nnot be able to reduce all kinds of toxicity, especially\nwhen it comes to subtler toxicity (sexism, microag-\ngressions, etc.) Further research is needed to make\ntoxicity detection and mitigation more robust.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain chat-\nbot. arXiv preprint arXiv:2001.09977.\nSolon Barocas and Andrew D. Selbst. 2016. Big Data’s\nDisparate Impact. California Law Review, 104:671.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv:2005.14165.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language mod-\nels: a simple approach to controlled text generation.\narXiv preprint arXiv:1912.02164.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The pile: An\n800gb dataset of diverse text for language modeling.\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\nJames Zou. 2018. Word embeddings quantify 100\nyears of gender and ethnic stereotypes. Proceedings\nof the National Academy of Sciences, 115(16):E3635–\nE3644.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. Realtoxic-\nityprompts: Evaluating neural toxic degeneration in\nlanguage models. ArXiv, abs/2009.11462.\nJianping Gou, Baosheng Yu, Stephen J. Maybank, and\nDacheng Tao. 2021. Knowledge distillation: A\nsurvey. International Journal of Computer Vision,\n129(6):1789–1819.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration.\nJigsaw and the Google Counter Abuse Team. 2015.\n[link].\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019a. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney,\nCaiming Xiong, and Richard Socher. 2019b. Ctrl: A\nconditional transformer language model for control-\nlable generation.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2020. Gedi: Generative\ndiscriminator guided sequence generation. arXiv\npreprint arXiv:2009.06367.\nDavid M. Lazer, Alex Pentland, Duncan J. Watts, Sinan\nAral, Susan Athey, Nosihir Contractor, Deen Freelon,\nSandra Gonzalez-Bailon, Gary King, Helen Mar-\ngetts, Alondra Nelson, Matthew J. Salganik, Markus\nStrohmaier, Alessandro Vespignani, and Claudia\n5630\nWagner. 2020. Computational social science: Ob-\nstacles and opportunities. Science, 369(6507):1060–\n1062.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. Dexperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nXiming Lu, Sean Welleck, Liwei Jiang, Jack Hessel,\nLianhui Qin, Peter West, Prithviraj Ammanabrolu,\nand Yejin Choi. 2022. Quark: Controllable text gen-\neration with reinforced unlearning.\nSandra G. Mayson. 2018. Bias in, Bias out. Yale Law\nJournal, 128:2218.\nZiad Obermeyer, Brian Powers, Christine V ogeli, and\nSendhil Mullainathan. 2019. Dissecting racial bias\nin an algorithm used to manage the health of popula-\ntions. Science, 366(6464):447–453.\nTrishan Panch, Heather Mattie, and Rifat Atun. 2019.\nArtificial intelligence and algorithmic bias: Implica-\ntions for health systems. Journal of Global Health,\n9(2):010318.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019a. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019b. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nLaria Reynolds and Kyle McDonell. 2021. Prompt\nprogramming for large language models: Beyond the\nfew-shot paradigm.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1668–1678, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul Christiano. 2020. Learning\nto summarize from human feedback.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nA Contexts used\nThe toxic contexts and negative contexts used in-\nclude highly offensive material. To avoid gratu-\nitously including offensive text, we have decided\nto omit these contexts from the paper. With a\ndesire for transparency and replicability, we are\nmaking them available at https://github.com/BYU-\nPCCL/prompt-compression-contrastive-coding.\nA.1 Toxic Context\nThis content was omitted to avoid the inclusion\nof gratuitous offensive material. The interested\nreader may find it at https://github.com/BYU-\nPCCL/prompt-compression-contrastive-coding.\nA.2 Negative Context\nThis content was omitted to avoid the inclusion\nof gratuitous offensive material. The interested\nreader may find it at https://github.com/BYU-\nPCCL/prompt-compression-contrastive-coding.\nA.3 Positive Context\nThese are examples of sentences that are kind,\ngenerous, polite, uplifting, noble and that express\npositive sentiments:\nEveryone loves kittens.\nThe food and drink of Spain are both outstanding.\njoy\nI think everyone deserves a second chance.\n5631\nlongsuffering\nWe need to be committed to loving other people,\nno matter what.\nLove never fails.\nYou are one of my nicest thoughts.\nDecide that no matter what is happening around\nyou, the sun will always be shining in your world.\nBirthdays, rainbows, gentle rainfall, puppies and\nhot chocolate by the fire are some of my favorite\nthings.\ngrace\nPatience is a virtue.\nThe only way to find peace in our time is if\neach individual lives unselfishly, caring for their\nneighbor more than themselves.\nGentleness\nWhile I appreciate your opinion on this matter, I\nthink it could have been said more generously.\nOnly kindness matters.\nFAITH\nA sweet friendship refreshes the soul.\nI am so thankful for my parents and for my\nteachers. They’ve made a positive difference in my\nlife!\nIf we’re going to overcome the perils of identity\npolitics, we all need to listen to each other, and\nreally try to understand their point of view.\nWho doesn’t love going to the beach?\nFamilies are forever\nGiving to charity is one of the noblest things a\nperson can do.\nFriendship is the icing on the cake of life.\nbeauty\nReach out to the poor, the downtrodden and the\nsuffering, and you will find eternal life.\nDancing and singing lets the soul roam free.\nIndependence is happiness.\nHumanity is our race; love is our religion.\nYou can’t rely on how you look to sustain you,\nwhat sustains us, what is fundamentally beautiful\nis compassion; for yourself and your those around\nyou.\nCount your blessings!\nPeace & love, baby.\nA.4 Cat Context\nThese are all sentences about cats:\nCats are the best!\nI REALLY LOVE CATS.\nDid you know that the Egyptians worshipped cats?\nCats are by far the internet’s most popular animal.\nIt’s true that cats can be independent and aloof, but\nthey are also loyal and compassionate.\nthe poor animal was beginning to think \"bad cat\"\nwas her new name\nThe cat is a popular pet animal which wass tamed\nby humans a long time ago.\nCats are friendly and playful with people, espe-\ncially with children.\nThe product is applied to a cat daily and reduces\ndander from the coat, which can cause allergic\nreactions.\nCats have four legs and one tail and they produce a\n“meow”, “purr” and “hiss” sound.\nI thought I might just as well describe my pet\nin order to know it–order, vertebrate; division,\nquadruped; class, mammalia; genus, felinus;\nspecies, cat; individual, Tabby.\nLaser pointers are probably one of the most\nengaging ways to play with a cat.\nCatnip really does act like a mild stimulant for\ncats.\nOnce I was surprised to see a cat walking along\nthe stony shore of the pond, for they rarely wander\nso far from home.\nThe cat can have some milk, and the mouse can\nhave some cake.\nJoseph asked as he waved a foot at the cat, who\nscurried back and repeated her greeting.\nhe giggled and cuddled the cat clos\nJane said I have to leave the cat with you.\nFleaScan helps you identify flea infestation in any\ndog or cat long before becoming full-blown.\nB Reading Comprehension Experiment\nDetails\nB.1 Paragraph\nFrank and Cindy are bakers in the city of Paris,\nFrance. They love traveling, and have visited nu-\nmerous countries around the world. They enjoy\ncruises, hiking, and visiting cities with history and\nflair. Because they are bakers, they also enjoy ex-\nploring new foods, tasting new wine, and interact-\ning with local cooks and chefs. Frank and Cindy\ntravel 2-3 times per year, and have visited Europe,\nSouth America and Australia. They have not vis-\nited Africa, but hope to someday. They also enjoy\nposting stories about their travels on Facebook and\ntrying to convince their friends to travel with them.\n5632\nB.2 Specific Questions\nBelow we put the questions in black, and the an-\nswers in red.\n• Question: What profession is Frank? Answer:\nFrank is a baker\n• Question: What continent have Frank and\nCindy not visited? Answer: They have not\nvisited Africa\n• Question: Question: How often do Frank and\nCindy travel? Answer: They travel 2-3 times\nper year\n• Question: Where do Frank and Cindy post\nstories about their travel? Answer: They post\non Facebook\n• Question: Complete the following sentence\nfrom the paragraph about Frank and Cindy:\n\"Frank and Cindy are bakers in the city of...\"\nAnswer: \"Frank and Cindy are bakers in the\ncity of Paris\n• Question: Complete the following sentence\nfrom the paragraph about Frank and Cindy:\n\"they also enjoy exploring new...\" Answer:\n\"they also enjoy exploring new cities\n• Question: Complete the following sentence\nfrom the paragraph about Frank and Cindy:\n\"they also enjoy exploring new foods, tasting\nnew...\" Answer: \"they also enjoy exploring\nnew foods, tasting new\" foods\n• Question: Complete the following sentence\nfrom the paragraph about Frank and Cindy:\n\"they also enjoy exploring new foods, tasting\nnew...\" Answer: \"they also enjoy exploring\nnew foods, tasting new\" wine\n• Question: Complete the following sentence\nfrom the paragraph about Frank and Cindy:\n\"tasting new wine, and interacting with lo-\ncal...\" Answer: \"tasting new wine, and inter-\nacting with local cooks\nB.3 Generic Questions\n• Question: What do Frank and Cindy love to\ndo? Answer: They love to travel\n• Question: Complete the following sentence\nfrom the paragraph about Frank and Cindy:\n\"They also enjoy posting stories about their...\"\nAnswer: \"They also enjoy posting stories\nabout their travels\n• Question: Complete the following sentence\nfrom the paragraph about Frank and Cindy:\n\"They love...\" Answer: \"They love traveling\nC Example Generations\nHere are some example generations for the prompt\n\"Its editor has written that black people are\" from\nRTP. All generations below are steered with hard\nprompts and the Bayesian attribute classifier with\nω= 10and τ = 1.\n• Vanilla GPT-2: Its editor has written that\nblack people are white and they need a \"real\nunderstanding\" of feminism.\n• Steered GPT-2: Its editor has written that\nblack people are often misunderstood —\nmany wrongly.\n• Steered GPT-2 medium: Its editor has\nwritten that black people are often judged\non their race or color, sometimes negatively.\nRoss has offered some insight into his own\njourney\n• Steered GPT-2 large: Its editor has written\nthat black people are rarely portrayed kind\nin traditional music, particularly in traditional\nAfrican music — and that blackness tends to\nbe\n• Steered GPT-2 xl: Its editor has written\nthat black people are particularly concerned\nabout Donald Trump.\nD Full Reading Comprehension Heatmap\n5633\nhard\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nsoft_dim_64\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nsoft_dim_32\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nsoft_dim_16\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nsoft_dim_8\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nsoft_dim_4\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nsoft_dim_2\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nsoft_dim_1\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nvanilla\nFrank and Cindy are bakers in the city of Paris, France. They love traveling, and have visited numerous countries around the world. They enjoy cruises, hiking, and visiting cities\nwith history and flair. Because they are bakers, they also enjoy exploring new foods, tasting new wine, and interacting with local cooks and chefs. Frank and Cindy travel 2-3 times\nper year, and have visited Europe, South America and Australia. They have not visited Africa, but hope to someday. They also enjoy posting stories about their travels on Facebook\nand trying to convince their friends to travel with them.\nFigure 10: Full Heatmap assessment of information retained as a prompt is compressed more and more severely.\n5634"
}