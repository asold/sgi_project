{
  "title": "Adaptor: Objective-Centric Adaptation Framework for Language Models",
  "url": "https://openalex.org/W4285179112",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5102979706",
      "name": "Michal Štefánik",
      "affiliations": [
        null,
        "Masaryk University"
      ]
    },
    {
      "id": "https://openalex.org/A5054652735",
      "name": "Vít Novotný",
      "affiliations": [
        "Masaryk University"
      ]
    },
    {
      "id": "https://openalex.org/A5046172961",
      "name": "Nikola Groverová",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5018729840",
      "name": "Petr Sojka",
      "affiliations": [
        "Masaryk University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3120706522",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2985449513",
    "https://openalex.org/W3172794097",
    "https://openalex.org/W4287213465",
    "https://openalex.org/W2887920589",
    "https://openalex.org/W4287646898",
    "https://openalex.org/W648786980",
    "https://openalex.org/W2111362445",
    "https://openalex.org/W3106096977",
    "https://openalex.org/W2890535735",
    "https://openalex.org/W3035032873",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W3035214886",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2116064496",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2942203175",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W3082928416",
    "https://openalex.org/W3100895823",
    "https://openalex.org/W3099630905",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W3095992020",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W4301183982",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3204406378",
    "https://openalex.org/W2964343359",
    "https://openalex.org/W2964345285",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W3087549734",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2963216553"
  ],
  "abstract": "This paper introduces Adaptor library, which transposes traditional model-centric approach composed of pre-training + fine-tuning steps to objective-centric approach, composing the training process by applications of selected objectives.We survey research directions that can benefit from enhanced objective-centric experimentation in multitask training, custom objectives development, dynamic training curricula, or domain adaptation.Adaptor aims to ease reproducibility of these research directions in practice. Finally, we demonstrate the practical applicability of Adaptor in selected unsupervised domain adaptation scenarios.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nSystem Demonstrations, pages 261 - 269\nMay 22-27, 2022 ©2022 Association for Computational Linguistics\nAdaptOr: Objective-Centric Adaptation Framework for Language Models\nMichal Štefánik1,2 and Vít Novotný1 and Nikola Groverová2 and Petr Sojka1\n1Faculty of Informatics, Masaryk University, Czech Republic\n2Gauss Algorithmic\nAbstract\nProgress in natural language processing re-\nsearch is catalyzed by the possibilities given\nby the widespread software frameworks. This\npaper introduces the AdaptOr library1 that trans-\nposes the traditional model-centric approach\ncomposed of pre-training + fine-tuning steps\nto objective-centric approach, composing the\ntraining process by applications of selected ob-\njectives. We survey research directions that\ncan benefit from enhanced objective-centric ex-\nperimentation in multi-task training, custom\nobjectives development, dynamic training cur-\nricula, or domain adaptation. AdaptOr aims to\nease the reproducibility of these research direc-\ntions in practice. Finally, we demonstrate the\npractical applicability of Adapt Or in selected\nunsupervised domain adaptation scenarios.\n“The measure of intelligence is the ability to change.”\n— Albert Einstein\n1 Introduction\nRecent development in Natural Language Pro-\ncessing (NLP) heavily benefits from a high level\nof maturity of open-source frameworks, such\nas Fairseq (Ott et al., 2019) or HuggingFace\nTransformers (Wolf et al., 2020). Thanks to\nthe standardized interfaces, these libraries allow\nfor immediate experimentation with the most\nrecent research results, practically fostering the\nspeed of further progress in the area. While\ntheir use is seamless for countless conventional\nuse-cases of transformer models and fine-tuning\nto a specific end-task (Devlin et al., 2019; Radford\nand Narasimhan, 2018), divergence from this\nframework requires feasible, but elaborate and\ncomplex customizations, increasing the risk of\nlogical errors and complicating the reproducibility\nof experiments. A characteristic group of problems\n1github.com/gaussalgo/adaptor\ntexti labeli\nsample()\nbatch()\nencode()\nM.Heado.forward()\nloss()\naggregate()\nObjectives\nregister \nhead()\nM.Heado\nModel \nModel \nAdaptor\nSchedule\nFigure 1: Overview of AdaptOr’s objective-centric train-\ning framework: Objective 1) registers its compatible\nhead on top of the shared model, 2) performs specific\ninput encoding, and 3) compute loss value based on\nits output. A Schedule implements a specific sampling\ncurricula and AdaptOr aggregates and propagates objec-\ntives’ losses and performs optimization.\nrequiring significant changes to the standard\npipeline are multi-step and multi-task adaptations.\nThis paper introduces the AdaptOr library, which\naims to simplify the more complex training pro-\ncesses that their training objectives can easier de-\nscribe. AdaptOr challenges the conventional model-\ncentric framework, where data and task selection\nare constrained by the requirements of the selected\nlanguage model architecture. Instead, it introduces\nan objective-centric training pipeline, with Objec-\ntive as the central abstraction of the process.\nThe AdaptOr framework aims to help NLP re-\nsearchers and practicioners engage in projects that\ninclude any of the following:\n• Multi-objective training: when training a\nlanguage model on more than one task or data\nset, including languages, AdaptOr can signif-\n261\nicantly simplify the custom code base that\nneeds to be implemented. Even if the objec-\ntive is custom, the user can avoid adjustments\nto other parts of the training pipeline.\n• Custom data schedule: when users need to\nperform dynamic data sampling, AdaptOr al-\nlows them to implement a custom Schedule\n(see Figure 2), leaving the data and model\nadjustment logic intact. This simplifies sys-\ntematic experimentation and reproducibility,\nand minimizes the risk of errors.\n• Objectives design & evaluation : Adapt Or\nexposes top-level declaration of training ob-\njectives, which enables easy experimentation\nwith custom objectives. Objective-level mon-\nitoring can provide custom behavioural in-\nsights and allows for pruning less promising\nexperiments earlier in the lengthy training pro-\ncess, saving computational costs.\n• Robustness evaluation : The objective-\ncentric paradigm provides an easy robust-\nness estimation by evaluating on out-of-\ndistribution samples. In the standard sequen-\ntial adaptation scenario, objective-centric eval-\nuation exposes characteristic flaws of adapta-\ntion, like exposure bias or catastrophic forget-\nting.\nThis paper is structured as follows: Section 2\nprovides an overview of recent work demonstrat-\ning the potential of multi-objective training in do-\nmain and task adaptation. Section 2.4 also de-\nscribes other software frameworks applicable for\nsimilar use cases. Section 3 describes the design of\nAdaptOr, showing the users how to confidently inte-\ngrate novel objectives and schedules. In Section 4,\nwe describe and implement a set of non-trivial,\nyet promising domain adaptation experiments us-\ning AdaptOr and collect their results. As AdaptOr\nremains under active development, we close in Sec-\ntion 5 with an outline of the upcoming features.\nWe welcome contributions of novel objectives and\nschedules.\n2 Background\nThis section provides an overview of recent work\nthat demonstrates the potential of multi-objective\ntraining and schedules that motivated the design\nof Adapt Or. Our overview consists of a non-\nexhaustive list of applications that Adapt Or aims\nto make more accessible for practical use and in\nfuture research.\n2.1 Multi-Task Training\nMulti-task training has a long history in both tra-\nditional machine learning (Caruana, 1997) and in\ndeep learning (Crawshaw, 2020). This section de-\nscribes examples of multi-task (i.e. multi-objective)\ntraining, outlining its benefits and potential.\nUnder some circumstances, multi-task training\nenhances distributional robustness of neural mod-\nels. Tu et al. (2020) demonstrate this on adversarial\ndata sets, exposing common heuristic biases of the\nlanguage models (McCoy et al., 2019). Enhanced\nmodel generalization can also be achieved by intro-\nducing one or more latent tasks that do not directly\ncorrespond to the end task but reflect specific de-\nsired properties of the model. One of a few studies\nin this direction is Sharpness-Aware Minimisation\nof Foret et al. (2021), performing multi-objective\ntraining on image classification using cross-entropy\nand a novel, sharpness-aware objective, reflecting\nthe model’s monotonicity on the local neighbor-\nhood. In context of Neural Machine Translation\n(NMT), Wang and Sennrich (2020) incorporate\nMinimum Risk Training (MRT) objective (Ranzato\net al., 2016), optimising an arbitrary sequence-level\nmeasure of outputs. In composition with the tra-\nditional token-level cross-entropy objective, MRT\nimproves distributional robustness.\nBy aggregating multiple objectives, Xie et al.\n(2019) show that combining sentence classification\nobjective with maximizing representation consis-\ntency to augmented samples fosters data efficiency.\nThe intuition on the benefits of multi-task train-\ning presumes that by optimizing the training by\nmultiple cost functions, the final model is less\nprone to the weaknesses of a specific task (Col-\nlobert et al., 2011), possibly reflecting on higher-\nlevel, task-invariant properties of language (Bengio\net al., 2013).\n2.2 Data-Sampling Schedules\nExposing a model to training samples in a sys-\ntematic schedule, also referred to as a curriculum,\ncan lead to an improvement of the accuracy of the\nfinal model (Bengio et al., 2009). While the pos-\nitive effects of more complex schedules based on\nsample “difficulty” with transformers remain to be\nexplored, multiple studies show the potential of\nconfidence-based sampling to improve accuracy\n262\nand generalization. Biased samples can be identi-\nfied, according to model’s confidence (Pleiss et al.,\n2020; Swayamdipta et al., 2020) or using Bayesian\nmethods such as the Product of Experts (Hinton,\n2002). Then, they can be either eliminated (Bras\net al., 2020) or downweighted (Utama et al., 2020).\nMore complex scheduling methods are applied\nin training NMT models. Bengio et al. (2015) use\ndecay schedule to sample from both references and\nthe previous outputs of a NMT model, minimiz-\ning the discrepancy between training and inference.\nZhang et al. (2019) successfully use the same sam-\npling strategy in a sequence-level objective. The\nresults of Lu et al. (2020) underline the potential of\nsampling in NMT training, suggesting that the ac-\ncuracy of transformers on reported MT benchmarks\ncan be outperformed by simpler RNN models by\ncombining objectives in decay schedule.\nDespite the reported improvements, we find that\ncustom scheduling strategies are rarely used. We\nattribute this to their complicated integration into\nthe standard training process. To foster the research\nand applicability of scheduling methods, AdaptOr\nmakes the implementation of custom scheduling\nstrategies easy, comprehensible, and reproducible.\n2.3 Domain Adaptation\nObjective-centric frameworks are well-suited for\ndomain adaptation techniques, where AdaptOr pro-\nvides support for combining traditional end-task ob-\njectives with unsupervised adaptation or auxiliary-\ntask objectives in a user-selected schedule. The\ngoal of domain adaptation is to maximize perfor-\nmance on a specific data domain, often denoted as\nthe adapted or target domain(Saunders, 2021).\nPerhaps the most common adaptation approach\nusing pre-trained language models is to con-\ntinue pre-training on unsupervised samples of the\nadapted domain (Luong and Manning, 2015; Lee\net al., 2019; Beltagy et al., 2019). This approach\nhas been successfully extended in various direc-\ntions. For instance, Gururangan et al. (2020) show\nthat adapting to a shared task on different domain\ncan enhance accuracy of the eventual application.\nIf supervised data is sparse, other auxiliary tasks,\ndescribed earlier in Section 2.1, can be used as\nconcurrent objectives (Xie et al., 2019).\nIn cases where larger volumes of data of given\ntask is available in a different language, adaptation\nusing cross-lingual transfer can be considered. Pre-\ntrained language models show that cross-lingual\ntransfer works well with large-data unsupervised\nobjectives (Conneau and Lample, 2019), but it can\nalso be applied for low-resource supervised objec-\ntive, such as very low-resource translation (Neubig\nand Hu, 2018).\nIf even unsupervised target-domain data is\nsparse, another option is to subset arbitrary un-\nsupervised sources to automatically identify sam-\nples of adapted domain, by applying domain clas-\nsifier (Jiang and Zhai, 2007; Elsahar and Gallé,\n2019). If the boundary between the training and\nthe adapted domain is known, an auxiliary objec-\ntive can minimise a discrepancy of representations\nbetween the training and possibly low-resource tar-\nget domain (Chadha and Andreopoulos, 2018).\nDespite the possibilities, adaptation can also in-\ntroduce undesired biases. In the scope of NMT,\nadaptation can cause problems of “catastrophic for-\ngetting”, when the model experiences performance\ndegradation on the originally well-performing do-\nmains (Saunders, 2021), or “exposure bias”, when\nthe model overfits the non-representative specifics\nof the target domain, such as the artifacts of data\ncollection (Ranzato et al., 2016). Additionally, by\nnormalizing a single type of bias, such as lexical\noverlap (McCoy et al., 2019), the model might\ndegrade its accuracy on other domains (Utama\net al., 2020). Addressing multiple biases concur-\nrently (Wu et al., 2020) can mitigate this problem.\nAdaptOr allows the knowledgeable user to con-\nstruct a reproducible and robust adaptation pipeline\nusing native multi-objective evaluation. Covering\nmultiple domains in separate objectives, AdaptOr\ncan expose the above pitfalls, without the need to\nimplement complex separate evaluation routines.\n2.4 Related Software Frameworks\nThe Adapters architecture (Houlsby et al., 2019),\nhaving only a small set of parameters, might be\na good fit when performing adaptation of trans-\nformer with modest hardware or data. Recently, the\nAdapterHub library (Pfeiffer et al., 2020) makes\ntraining and sharing of Adapters convenient. Com-\npared to Adapt Or, AdapterHub does not provide\nsupport for more complex adaptation cases, such as\nusing multiple objectives, scheduling, or extended\nevaluation. However, since both libraries build\nupon the HuggingFace Transformers library (Wolf\net al., 2020), their close integration is feasible.\nIf the robustness of models to heuristic short-\ncuts (McCoy et al., 2019) is the primary goal, the\n263\n1 class ParallelSchedule(Schedule):\n2 def_sample_objectives(self, split: str) -> Iterator[Objective]:\n3 while True:\n4 forobjectivein self.objectives[split].values():\n5 yieldobjective\nFigure 2: AdaptOr provides a convenient base for implementing custom sampling schedules. ParallelSchedule in\nthe figure demonstrates an implementation of the schedule sampling the update objectives in rotation. Further, the\nsampling can be easily conditioned on the state of Objectives such as the recent outputs, loss, or metrics evaluations.\nRobustness Gym library (Goel et al., 2021) pro-\nvides a comprehensive evaluation over an extend-\nable set of different kinds of heuristic biases. Ro-\nbustness Gym provides much deeper evaluation\ncompared to Adapt Or Evaluators, and could be\nintegrated as an Adapt Or Evaluator. Unlike Ro-\nbustness Gym, AdaptOr enables an evaluation of\nrobustness also on generative tasks, with specified\nout-of-domain data sets.\n3 Adapt Or Design\nThis section describes the structure and functions of\nthe AdaptOr framework. We introduce its primary\ncomponents bottom-up. Figure 3 depicts the rela-\ntions of these components and compares user inter-\naction with the traditional model-centric pipeline.\n3.1 LangModule\nA LangModule instance provides a management\nof inputs, outputs and objective-specific model\ncomponents, referred to as heads. Once an ob-\njective with given LangModule is instantiated, an\nobjective-compatible model is either initialised, or\ngiven by the user (see Section 3.2) and the parame-\nters of this model are merged with the parameters\nof the previously-registered objectives.\nThe merge works as follows: If no previous ob-\njective was registered, then the model of the given\nobjective is considered a base model. The models\nof the second- and later-registered objectives are\nthen merged with the base model: first, pairs of\nPyTorch modules of the same name in the base and\nthe new model are identified. If the dimensions\nand weights of these modules match, the respective\nmodule of the newly-adding model is replaced with\na module of the base model.\nIn the case of pre-trained transformers, the\nweights of heads are initialized randomly by de-\nfault, resulting in a registration of a distinct head\nfor each objective and sharing the remaining param-\neters. Users can control which parameters (not) to\nmerge by explicitly setting their respective weights\nas (non-)equal.\nIt is possible to use LangModule with any Py-\nTorch module that uses a HuggingFace tokenizer,\ncompatible with the given neural module. There-\nfore, LangModule is also suitable for other models\nsuch as recurrent networks.\n3.2 Objective\nObjectives are the primary component of AdaptOr’s\ntraining pipeline. Most importantly, an Objective\nserves two functions: sample encoding and loss\ncomputation. By implementing these and choosing\nthe type of a model’s head, AdaptOr users can de-\nfine and experiment with novel training objectives.\nIf they additionally provide an explicit definition of\nthe Objective’s model (theobjective_module\nattribute), the new objective does not even have to\ncomply with common model heads; shared param-\neters of the given objective_module would\nstill be merged with the given lang_module.\nIf no objective_module is given, the Ob-\njective will request that a LangModule assigns\nthe Objective a module of the Objective’s default\ncompatible_head (see Section 3.1).\nAdditionally, every Objective instance performs\nits own logging, evaluation, and state updates,\nsuch as its convergence, based on a valuation of\ngiven val_evaluators, or draws a progress\nbar, based on the state of its sample iteration. How-\never, the training flow is guided by a Schedule (see\nSection 3.3). Objectives can implement custom\ndata sampling, but if possible, we recommended to\ndo so in a custom Schedule instance.\nSince data encoding is also objective-specific,\nObjectives expose a higher-level user interface\nof data inputs than other frameworks: instead\nof encodings, users provide an Objective with\na texts_or_path and a labels_or_path\ncontaining raw texts and respective labels. AdaptOr\nprovides an implementation of standard Objectives\nfor sequence and token classification and sequence-\nto-sequence tasks. When implementing a custom\nObjective, note that sampling and encoding are per-\nformance bottlenecks on current high-end GPUs.\n264\n<retrieves>\nHeadotexts labels\n(sub)sample features\nObjectiveo\nSchedule\nUser side Library side  \n(HF Transformers)\ntexts labels\nBaseModel\nLangModule\nUser side Adaptor side\nHF Trainer\nCompatModel\n<selects> <selects>\n<retrieves>\nFigure 3: A comparison of interaction with a model-centric HuggingFace Trainer (left) and objective-centric AdaptOr\n(right): While in model-centric approach, user resolves text processing, sampling and encoding compatible with\nselected model of specific objective, objective-centric approach delegates these functionalities to Objective instances.\nExplicit definition of Objectives and Schedule on AdaptOr’s user side makes otherwise complex multi-objective and\ncustom-schedule experiments transparent and reproducible.\n3.3 Schedule\nSchedules control the training flow through the\ninterfaces provided by HuggingFace Transform-\ners library. Primarily, they deliver 1) a set of\nstandard stopping strategies based on the state\nof the Objectives and 2) an IterableDataset in-\nstance, constructed by sampling Objectives ac-\ncording to a sampling strategy implemented in\nits _sample_objectives. A Schedule also\nensures that outputs of distinct lang_modules’\nheads are delivered to the respective Objectives for\nloss computation.\nThis relatively complex sampling framework\nprovides a very simple interface for custom Sched-\nule implementations (see Section 2.2). For in-\nstance, a pre-definedParallelSchedule is im-\nplemented with three lines of code (see Figure 2).\n3.4 Adapter\nAn Adapter is customization of the HuggingFace\nTrainer with only minor adjustments. Specif-\nically, Adapter redirects loss computation to\na Schedule, which further distributes outputs\nto corresponding Objectives and extends na-\ntive training logs with logs of Objectives’ Eval-\nuators. Furthermore, Adapter adjusts persis-\ntence of the models so that a model of ev-\nery head can be reloaded without the use of\nAdaptOr, by simply using HuggingFace Transform-\ners’ AutoModelForXY.from_pretrained.\nBased on the actively-developed HuggingFace\nTransformers library, the AdaptOr allows its users\nto benefit from all other native features of Hugging-\nFace Transformers, such as the support for the most\nrecent models, custom logging platforms, or dis-\ntributed parallel training. Furthermore, it can sim-\nplify integration with other custom libraries (see\nSection 2.4).\n4 Experiments\nWe use AdaptOr in a set of domain adaptation exper-\niments for a machine translation use-case, aiming\nto answer the following research question: How\nwell can unsupervised objective(s) substitute la-\nbeled parallel data. In our methodology, we per-\nmute the easily-configurable parts of Adapt Or’s\ntraining configuration2 and compare the results of\nthe resulting model to a baseline adaptation sce-\nnario. We experiment with an architecture identical\nto the base model of Vaswani et al. (2017), with a\nconfiguration of Junczys-Dowmunt et al. (2018).\nData. We train the model on English-to-Czech\ntranslations on different domains of OPUS (Tiede-\nmann, 2012) chosen for their significant distinctive-\nness: we use Wikimedia as a large-scale, supervised\ndomain (denoted as in-domain, i.e. ID), OpenSub-\ntitles as an Adapted Domain (AD) and Bible for\nthe evaluation of a model’s robustness on Out-Of-\nDomain (OOD) samples.\nPre-training vs. fine-tuning. We simulate two\nbasic scenarios: training the model from a random\ninitialization and fine-tuning the existing transla-\ntion model with no control over its pre-training data.\nIn the latter cases, we perform fine-tuning from the\ncheckpoint of Tiedemann and Thottingal (2020).\nSchedules. We implement and experiment with\ntwo objective schedules: i) Sequential schedule,\nsampling and differentiating the model sequentially\n2Our code is available on https://github.com/\ngaussalgo/adaptor/tree/reprod/demo.py\n265\nSchedule Objectives BLEU IDBLEUAD BLEUOODBERTSIDBERTSADBERTSOOD\nPre-training 1) Seq2SeqID 28.18 5.34 0.91 0.833 0.738 0.671\nSequent.2) Seq2SeqID+ BackTrAD 5.10 15.01 2.57 0.740 0.805 0.733\n*3) Seq2SeqID+ Seq2SeqAD 4.96 17.37 2.64 0.756 0.816 0.726\nParallel 4) Seq2SeqID+ BackTrAD 31.06 16.99 2.46 0.852 0.817 0.722\n*5) Seq2SeqID+ Seq2SeqAD 29.72 18.55 2.98 0.843 0.813 0.732\nFine-tuning 6) Seq2SeqID 37.97 17.62 6.50 0.875 0.808 0.758\n7) BackTrAD 30.34 22.98 11.08 0.869 0.834 0.799\nParallel 8) Seq2SeqID+ DenoisAD 38.96 13.37 6.87 0.876 0.782 0.761\n9) Seq2SeqID+ BackTrAD 38.25 21.47 9.03 0.873 0.831 0.791\n*10) Seq2SeqID+ Seq2SeqAD 40.72 23.35 6.97 0.880 0.836 0.772\nTable 1: We evaluate the features of AdaptOr on multi-objective domain adaptation in machine translation: our\nexperiments compare the BLEU score and BERTScore of unsupervised adaptation ( Seq2seq + Denoising or\nBack-Translation) applied in different schedules, to no adaptation (1, 6) and a hypothetical supervised adaptation\n(*3, *5, *10). Results show that the Parallel schedule eliminates catastrophic forgettingand that unsupervised\nBack-translation is able to reach performance that is close to the supervised adaptation.\nby each objective until convergence by evalua-\ntion loss, or for a maximum of 100,000 updates.\nii) Parallel schedule, concurrently sampling train-\ning batches uniformly from every given objective.\nUsing gradient accumulation, we differentiate the\nmodel based on all given objectives. We perform\nupdates until the convergence of all objectives, or\nfor a maximum of 50,000 updates for each objec-\ntive.\nObjectives selection. We implement and experi-\nment with the following AdaptOr objectives:\n• Sequence-to-sequence (seq2seq) objective,\nas introduced by Vaswani et al. (2017), maps\na combination of encoder inputs in the source\nlanguage and previously-generated outputs as\ndecoder inputs to a distribution over the next-\npredicted tokens.\n• Denoising objective introduced by Lewis et al.\n(2020) is an unsupervised instance of the\nseq2seq objective that performs random to-\nken permutation on the input and trains the\nmodel to map such ‘noisy‘ text to the original\nversion of the input. We use this objective on\nthe target-data domain to enhance its compre-\nhension by the model.\n• Back-translation objective, as used e.g. by\nSennrich et al. (2016) is also an unsuper-\nvised seq2seq objective, which uses an ex-\nternal translator in reverse direction to obtain\npseudo-inputs. This objective is profitable\nwhen we have unlabeled data of the target\ndomain.\nUsing these components, we construct the fol-\nlowing experiments:\n• Baselines: pre-training (1) and fine-tuning\n(6) on ID data from a domain different from\nthe Application Domain (AD) using a single\ntraditional seq2seq objective.\n• Sequential adaptation: we pre-train using\nseq2seq on ID and afterwards adapt using ei-\nther unsupervised Back-translation (2), or su-\npervised seq2seq (3) on AD to quantify the\nunsupervised adaptation gap.\n• Parallel adaptation: we concurrently train\non both seq2seq and another unsupervised ob-\njective: Back-translation (4, 9) and Denoising\n(8). Again, we compare the gap to the super-\nvised situation (5, 10).\n4.1 Results\nTable 1 evaluates the base transformer after the\ngiven number of updates on held-out dedupli-\ncated validation splits of In-Domain (ID), Adapted-\nDomain (AD), and the third Out-Of-Domain\n(OOD) data. Note that the results for the BLEU\nscore are properly comparable only within the same\ndomain.\nWe observe that the model trained on a single\ndomain (1, 6) degrades on all other domains. In a\npre-training scenario, domain robustness improves\nwhen incorporating data of adapted domain in any\nobjective. However, in a sequential schedule, we\nobserve catastrophic forgetting towards any most-\nrecent domain of adaptation (2, 3). This is im-\nproved by using the Parallel schedule for a negligi-\nble price of in-domain accuracy (4, 5).\n266\nIn the fine-tuning scenario, we show that incor-\nporating unsupervised Back-translation to AD (7,\n9) improves ID BLEU comparably to supervised\nadaptation (10). Interestingly, Denoising on AD (8)\nimproves in-domain performance but seems less\nefficient than Back-translation.\n4.2 Adapt Or Usage Complexity\nTo give an idea about the relative complexity of\nusing AdaptOr as compared to model-centric frame-\nworks, we compare selected measurable code fea-\ntures of the complexity of our experimental imple-\nmentation to an example implementation using the\nHuggingFace Trainer3. We pick the experiment of\nsupervised pre-training + unsupervised fine-tuning,\nincluding evaluation, in the sequential schedule (2),\nas this can still be addressed using HuggingFace\nTransformers relatively easily; Implementing the\nparallel multi-objective schedule in the Transform-\ners framework would require major customisations\nof selected model and Trainer objects.\nThe training script using HuggingFace Trainer\ncontains 654 lines of code, 135 variable assign-\nments, 186 method calls and the initialisation of\n9 custom objects. Additionally, in the pre-training\n+ fine-tuning framework, this script has to be run\ntwice, initialising the second training from the se-\nlected checkpoint of the first one, with updated\nconfigurations. Back-translated pseudo-labels are\ngenerated by a different script, not included in this\nassessment.\nUsing AdaptOr, we construct an equivalent rou-\ntine from the provided demo script. Our imple-\nmentation contains 124 lines of code, 31 variable\nassignments, 37 method calls and the initialisation\nof 14 custom objects. Despite its brevity, our script\nwraps the whole training process, and hence, to-\ngether with the associated version of AdaptOr or its\nfork, it provides a reproducible fingerprint of the\nexperiment.\n5 Conclusion and Future Work\nThis paper introduces the AdaptOr library, which\nprovides objective-centric training framework well-\nsuitable for multi-task and multi-domain training\nscenarios, and the development of novel objec-\ntives and sampling schedules. We find that even in\nthe conventional single-objective training routines,\nAdaptOr can reduce volumes of custom implemen-\n3For reference, we use run_translation.py example script\non HuggingFace Transformers GitHub, version 4.17.0.\ntation and increases readability and reproducibility.\nHaving used AdaptOr already for several produc-\ntion use cases, we are happy to share it with the\nNLP community.\nOur future work aims to further enhance\nAdaptOr’s user comfort with existing and novel\nunsupervised objectives, dynamic schedules, and\ndemonstrations on novel use cases.\n6 Broader Impact\nThanks to the ubiquity of objective-centric train-\ning, AdaptOr can accelerate the applicability of the\nmost recent research in multi-task and multilingual\nmodeling and enrich the research with the practical\nexperience of the industry.\nWe further identify the benefits of Adapt Or’s\ndefinite training pipelines in saving unnecessary fi-\nnancial and environmental expenses of reproducing\nthe reported results of large language models, oth-\nerwise often including expensive hyperparameter\noptimization over unreported parameters. Due to\nthese aspects, AdaptOr could also ease the spread\nof state-of-the-art language technologies to under-\nresourced languages and more specialized domains\nwith a sufficient amount of unsupervised sources.\nFinally, objective-centric training might help ex-\npose the potential of unsupervised objectives to\nthe generalization and interpretability of models.\nAdaptOr can foster the research in unsupervised\nlearning by lowering the relatively high entry level\nof technical proficiency needed for experimentation\nwith novel language objectives.\nReferences\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientific\nText. In Proc. of the Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Int.\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3615–3620, Hong Kong,\nChina. ACL.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\nShazeer. 2015. Scheduled sampling for sequence\nprediction with recurrent neural networks. In Ad-\nvances in Neural Information Processing Systems,\nvolume 28. Curran Associates, Inc.\nYoshua Bengio, Aaron C. Courville, and Pascal Vincent.\n2013. Representation Learning: A Review and New\nPerspectives. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 35:1798–1828.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum Learning. In\n267\nProceedings of the 26th Annual International Confer-\nence on Machine Learning, ICML ’09, pages 41–48,\nNew York, NY , USA. ACM.\nRonan Le Bras, Swabha Swayamdipta, Chandra Bha-\ngavatula, Rowan Zellers, Matthew E. Peters, Ashish\nSabharwal, and Yejin Choi. 2020. Adversarial filters\nof dataset biases. In ICML.\nRich Caruana. 1997. Multitask learning. Machine\nLearning, 28:41–75.\nA. Chadha and Y . Andreopoulos. 2018. Improving Ad-\nversarial Discriminative Domain Adaptation. CoRR,\nabs/1809.03625v3.\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\nNatural Language Processing (Almost) from Scratch.\nJ. Mach. Learn. Res., 999888:2493–2537.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual Language Model Pretraining. In Proceedings\nof the 33rd International Conference on Neural Infor-\nmation Processing Systems (NIPS), Red Hook, NY ,\nUSA. Curran Associates Inc.\nMichael Crawshaw. 2020. Multi-Task Learning\nwith Deep Neural Networks: A Survey. CoRR,\nabs/2009.09796.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proc. of the 2019 Conference of the\nNorth American Chapter of the ACL: Human Lan-\nguage Technologies, pages 4171–4186, Minneapolis,\nUSA. ACL.\nHady Elsahar and Matthias Gallé. 2019. To Annotate\nor Not? Predicting Performance Drop under Domain\nShift. In Proc. of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2163–\n2173, Hong Kong, China. ACL.\nPierre Foret, Ariel Kleiner, H. Mobahi, and Behnam\nNeyshabur. 2021. Sharpness-Aware Minimization\nfor Efficiently Improving Generalization. CoRR,\nabs/2010.01412v1.\nKaran Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary\nTaschdjian, Mohit Bansal, and Christopher Ré. 2021.\nRobustness gym: Unifying the NLP evaluation land-\nscape. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the ACL: Human Lan-\nguage Technologies: Demonstrations, pages 42–55,\nOnline. ACL.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t Stop Pretraining:\nAdapt Language Models to Domains and Tasks. In\nProc. of the 58th Annual Meeting of the ACL, pages\n8342–8360. ACL.\nGeoffrey E. Hinton. 2002. Training Products of Ex-\nperts by Minimizing Contrastive Divergence. Neural\nComputation, 14(8):1771–1800.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nProceedings of the 36th International Conference\non Machine Learning, volume 97 of Proceedings\nof Machine Learning Research, pages 2790–2799.\nPMLR.\nJing Jiang and ChengXiang Zhai. 2007. Instance\nWeighting for Domain Adaptation in NLP. In Proc.\nof the 45th Annual Meeting of the ACL, pages 264–\n271, Prague, Czech Republic. ACL.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heafield,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, André F. T.\nMartins, and Alexandra Birch. 2018. Marian: Fast\nneural machine translation in C++. In Proceedings of\nACL 2018, System Demonstrations, pages 116–121,\nMelbourne, Australia. ACL.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. In Proc. of the 58th Annual\nMeeting of the ACL, pages 7871–7880.\nWenjie Lu, Leiying Zhou, Gongshen Liu, and Quan-\nhai Zhang. 2020. A mixed learning objective for\nneural machine translation. In Proceedings of the\n19th Chinese National Conference on Computational\nLinguistics, pages 974–983, Haikou, China. Chinese\nInformation Processing Society of China.\nMinh-Thang Luong and Christopher Manning. 2015.\nStanford neural machine translation systems for spo-\nken language domains. In Proceedings of the 12th\nInternational Workshop on Spoken Language Trans-\nlation: Evaluation Campaign, pages 76–79, Da Nang,\nVietnam.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the Wrong Reasons: Diagnosing Syntactic Heuris-\ntics in Natural Language Inference. In Proc. of the\n57th Annual Meeting of the ACL, pages 3428–3448,\nFlorence, Italy. ACL.\nGraham Neubig and Junjie Hu. 2018. Rapid adaptation\nof neural machine translation to new languages. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages 875–\n880, Brussels, Belgium. ACL.\n268\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of NAACL-HLT\n2019: Demonstrations.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020. AdapterHub: A\nframework for adapting transformers. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstra-\ntions, pages 46–54. ACL.\nGeoff Pleiss, Tianyi Zhang 0007, Ethan R. Elenberg,\nand Kilian Q. Weinberger. 2020. Identifying misla-\nbeled data using the area under the margin ranking.\nIn Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving Language Understanding by Generative Pre-\nTraining.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2016. Sequence Level Train-\ning with Recurrent Neural Networks. In 4th Inter-\nnational Conference on Learning Representations,\nICLR 2016, San Juan, Puerto Rico, May 2–4, 2016,\nConference Track Proceedings.\nDanielle Saunders. 2021. Domain Adaptation and\nMulti-Domain Adaptation for Neural Machine Trans-\nlation: A Survey. CoRR, abs/2104.06951.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In Proceedings of the 54th\nAnnual Meeting of the ACL (Volume 1: Long Papers),\npages 86–96, Berlin, Germany. ACL.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9275–9293, Online. ACL.\nJörg Tiedemann. 2012. Parallel Data, Tools and Inter-\nfaces in OPUS. In Proc. of the Eighth International\nConf. LREC, pages 2214–2218, Istanbul, Turkey.\nELRA.\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-\nMT – building open translation services for the world.\nIn Proceedings of the 22nd Annual Conference of\nthe European Association for Machine Translation,\npages 479–480, Lisboa, Portugal. European Associa-\ntion for Machine Translation.\nLifu Tu, Garima Lalwani, Spandana Gella, and He He.\n2020. An Empirical Study on Robustness to Spuri-\nous Correlations using Pre-trained Language Models.\nTransactions of the ACL, 8:621–633.\nPrasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna\nGurevych. 2020. Towards Debiasing NLU Models\nfrom Unknown Biases. In Proc. of the 2020 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 7597–7610, Online. ACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Proc. of the 31st International Con-\nference on Neural Information Processing Systems,\nNIPS’17, pages 6000–6010, Red Hook, NY , USA.\nCurran Associates Inc.\nChaojun Wang and Rico Sennrich. 2020. On exposure\nbias, hallucination and domain shift in neural ma-\nchine translation. In Proceedings of the 58th Annual\nMeeting of the ACL, pages 3544–3552, Online. ACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers:\nState-of-the-Art Natural Language Processing. In\nProc. of the 2020 Conf. EMNLP: System Demonstra-\ntions, pages 38–45. ACL.\nMingzhu Wu, Nafise Sadat Moosavi, Andreas Rücklé,\nand Iryna Gurevych. 2020. Improving QA General-\nization by Concurrent Modeling of Multiple Biases.\narXiv e-prints, page arXiv:2010.03338.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V . Le. 2019. Unsupervised Data Aug-\nmentation. CoRR, abs/1904.12848v1.\nWen Zhang, Yang Feng, Fandong Meng, Di You, and\nQun Liu. 2019. Bridging the gap between training\nand inference for neural machine translation. In Pro-\nceedings of the 57th Annual Meeting of the ACL,\npages 4334–4343, Florence, Italy. ACL.\n269",
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.7147480249404907
    },
    {
      "name": "Computer science",
      "score": 0.7075252532958984
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5050039887428284
    },
    {
      "name": "Software engineering",
      "score": 0.3498263955116272
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3304676413536072
    },
    {
      "name": "Psychology",
      "score": 0.07792961597442627
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21449261",
      "name": "Masaryk University",
      "country": "CZ"
    }
  ]
}