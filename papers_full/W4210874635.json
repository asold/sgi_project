{
    "title": "Protected Health Information Recognition by Fine-Tuning a Pre-training Transformer Model",
    "url": "https://openalex.org/W4210874635",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2281422095",
            "name": "Seo Hyun Oh",
            "affiliations": [
                "Gachon University"
            ]
        },
        {
            "id": "https://openalex.org/A2002975184",
            "name": "Min Kang",
            "affiliations": [
                "Gachon University"
            ]
        },
        {
            "id": "https://openalex.org/A2106970223",
            "name": "Young-Ho Lee",
            "affiliations": [
                "Gachon University"
            ]
        },
        {
            "id": "https://openalex.org/A2281422095",
            "name": "Seo Hyun Oh",
            "affiliations": [
                "Gachon University"
            ]
        },
        {
            "id": "https://openalex.org/A2002975184",
            "name": "Min Kang",
            "affiliations": [
                "Gachon University"
            ]
        },
        {
            "id": "https://openalex.org/A2106970223",
            "name": "Young-Ho Lee",
            "affiliations": [
                "Gachon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2946693031",
        "https://openalex.org/W3095092152",
        "https://openalex.org/W3128211192",
        "https://openalex.org/W2105578014",
        "https://openalex.org/W2150944609",
        "https://openalex.org/W2047749266",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2729101176",
        "https://openalex.org/W2783267968",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2083355945",
        "https://openalex.org/W2793786716",
        "https://openalex.org/W4320013936",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W1034374084",
        "https://openalex.org/W2996428491"
    ],
    "abstract": "Objectives: De-identifying protected health information (PHI) in medical documents is important, and a prerequisite to deidentification is the identification of PHI entity names in clinical documents. This study aimed to compare the performance of three pre-training models that have recently attracted significant attention and to determine which model is more suitable for PHI recognition. Methods: We compared the PHI recognition performance of deep learning models using the i2b2 2014 dataset. We used the three pre-training modelsâ€”namely, bidirectional encoder representations from transformers (BERT), robustly optimized BERT pre-training approach (RoBERTa), and XLNet (model built based on Transformer-XL)â€”to detect PHI. After the dataset was tokenized, it was processed using an inside-outside-beginning tagging scheme and WordPiecetokenized to place it into these models. Further, the PHI recognition performance was investigated using BERT, RoBERTa, and XLNet. Results: Comparing the PHI recognition performance of the three models, it was confirmed that XLNet had a superior F1-score of 96.29%. In addition, when checking PHI entity performance evaluation, RoBERTa and XLNet showed a 30% improvement in performance compared to BERT. Conclusions: Among the pre-training models used in this study, XLNet exhibited superior performance because word embedding was well constructed using the two-stream self-attention method. In addition, compared to BERT, RoBERTa and XLNet showed superior performance, indicating that they were more effective in grasping the context.",
    "full_text": "I. Introduction\nAs analyses based on clinical medical data can be used as \nclinical decision support to help experts make decisions, this \ntechnique has been extensively used by researchers in recent \nstudies [1,2]. In particular, clinical documents, which are \nincluded in Electronic Medical Records, contain important \ndata because these documents are written by clinical experts. \nIn fact, in the field of medical services, clinical documents \nhave been analyzed, and relevant data have been extracted \nand used for important decision-making, such as text sum-\nmarization, automatic question-and-answer systems, dialog \nsystems, and machine translation [3].\n However, these clinical documents are limited in terms of \ntheir secondary use because protected health information \nProtected Health Information Recognition by \nFine-Tuning a Pre-training Transformer Model\nSeo Hyun Oh\n1\n, Min Kang\n1\n, Youngho Lee\n2\n1\nDepartment of IT Convergence Engineering, Gachon University, Seongnam, Korea\n2\nDepartment of Computer Engineering, Gachon University, Seongnam, Korea\nObjectives: De-identifying protected health information (PHI) in medical documents is important, and a prerequisite to de-\nidentification is the identification of PHI entity names in clinical documents. This study aimed to compare the performance \nof three pre-training models that have recently attracted significant attention and to determine which model is more suitable \nfor PHI recognition. Methods: We compared the PHI recognition performance of deep learning models using the i2b2 2014 \ndataset. We used the three pre-training modelsâ€”namely, bidirectional encoder representations from transformers (BERT), \nrobustly optimized BERT pre-training approach (RoBERTa), and XLNet (model built based on Transformer-XL)â€”to detect \nPHI. After the dataset was tokenized, it was processed using an inside-outside-beginning tagging scheme and WordPiece-\ntokenized to place it into these models. Further, the PHI recognition performance was investigated using BERT, RoBERTa, \nand XLNet. Results: Comparing the PHI recognition performance of the three models, it was confirmed that XLNet had a \nsuperior F1-score of 96.29%. In addition, when checking PHI entity performance evaluation, RoBERTa and XLNet showed \na 30% improvement in performance compared to BERT. Conclusions: Among the pre-training models used in this study, \nXLNet exhibited superior performance because word embedding was well constructed using the two-stream self-attention \nmethod. In addition, compared to BERT, RoBERTa and XLNet showed superior performance, indicating that they were more \neffective in grasping the context.\nKeywords: Artificial Intelligence, Big Data, Medical Informatics, Data Anonymization, Deep Learning\nHealthc Inform Res. 2022 January;28(1):16-24. \nhttps://doi.org/10.4258/hir.2022.28.1.16\npISSN 2093-3681  â€¢  eISSN 2093-369X  \nOriginal Article\nSubmitted: May 11, 2021\nRevised: 1st, August 25, 2021; 2nd, October 7, 2021\nAccepted: November 25, 2021\nCorresponding Author \nYoungho Lee\nDepartment of Computer Engineering, Gachon University, 1342, \nSeongnam-daero, Sujeong-gu, Seongnam 13120, Korea. Tel: +82-\n31-750-5011, E-mail: lyh@gachon.ac.kr (https://orcid.org/0000-\n0003-0720-0569)\nThis is an Open Access article distributed under the terms of the Creative Com-\nmons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-\nnc/4.0/) which permits unrestricted non-commercial use, distribution, and reproduc-\ntion in any medium, provided the original work is properly cited.\nâ“’ 2022 The Korean Society of Medical Informatics\n\n17Vol. 28  â€¢  No. 1  â€¢  January 2022\nwww.e-hir.org\nProtected Information Recognition by Transformer\n(PHI) is included in clinical documents, including patientsâ€™ \npersonal and sensitive information. Therefore, to ensure \nconfidentiality, these types of PHI must be de-identified [4]. \nIn particular, the Health Insurance Portability and Account-\nability Act (HIPAA) [5] in the United States defined guide-\nlines for the secondary use of medical records and the de-\nidentification of medical records. Until all types of PHI have \nbeen de-identified, clinical documents are not available to \nthe public. \n Thus, identifying, removing, and de-identifying PHI is cru-\ncial as it allows more researchers to access clinical data and \nencourages the secondary use of these data. In particular, the \nexisting PHI de-identification process has been performed \nonly by humans. A medical records specialist directly identi-\nfies and removes the PHI from clinical documents, but this \nmethod is costly and time-consuming.\n Research on generalized PHI identification algorithms to \nhelp reduce human effort is ongoing. In particular, knowl-\nedge- and rule-based methods have been widely used as \nautomatic identification algorithms. A knowledge-based \nsystem can be implemented using an algorithm that creates \na word dictionary and finds words therein. Meanwhile, a \nrule-based system uses a set rule to recognize formatted text, \nsuch as phone numbers or URLs, mainly using regular ex-\npressions. Thus, Shin et al. [6] proposed a de-identification \nmethod using regular expressions in clinical documents in \nKorea.\n However, a limitation of knowledge- and rule-based sys-\ntems is that they cannot recognize new forms of PHI that \nare different from a set form or rule. Deep learning models \nare more likely to recognize PHI than traditional methods. \nSpecifically, clinical documents involve many abbreviations \nand typos. Furthermore, since the forms to be filled out for \neach clinical expert vary, using prescribed rules or a diction-\nary of words is challenging. Therefore, machine learning-\nbased PHI identification methods have been proposed. In \nparticular, the conditional random field (CRF) method [7] \nreceived attention because it exhibited effective PHI recogni-\ntion performance by directly using the context, consisting \nof the front and back of the words. Lafferty et al. [8] showed \nthat the F1-score of rule-based PHI recognition of individual \nentities in clinical documents was 64.12%, but a substantially \nhigher performance of 81.48% was obtained using CRF . \n As part of machine learning-based methods, deep learning-\nbased methods based on artificial neural networks [9] are \nalso being actively considered. Deep learning can be used \nto construct an advanced model by automatically extracting \nfeatures without the precise feature engineering required by \nmachine learning. In particular, recurrent neural networks \n(RNNs) [10] are used to solve problems through real-time \nrecursive training that can utilize previously acquired infor -\nmation to solve a current problem. In addition, long short-\nterm memory (LSTM) [11] is a model made by improving \nthe RNN and can grasp contexts by considering previous \ndata. In their study that used LSTM, Liu et al. [12] achieved \na high PHI recognition performance.\n Pre-training-based models are being actively investigated \nto effectively perform these tasks as part of natural language \nprocessing. Furthermore, methods such as Word2Vec [13], \nfastText [14], and ELMo [15] are pre-trained with a large \namount of data to pre-configure the embedding vector. \nThese pre-trained models can be applied to new projects to \nimprove their performance. LSTM, which exhibited superior \nperformance in the existing natural language processing \nfield, achieved higher performance than the existing training \nmodel that used Word2Vec [16].\n Specifically, the bidirectional encoder representations \nfrom transformers (BERT) method, developed by Google, \nhas attracted attention because it is trained with contextual \nprinciples and can be fine-tuned in various fields. Unlike the \nexisting model, BERT uses masking, which randomly masks \nand predicts the token that should be predicted. Satisfactory \nperformance was obtained using this method, and various \nderivative models based on the BERT were released.\n Therefore, in this study, a pre-trained model was used to \nimprove the effectiveness of PHI recognition performance. \nThus, we propose a model for recognizing various types of \nPHI contained in clinical documents using BERT, the ro-\nbustly optimized BERT pre-training approach (RoBERTa), \nand XLNet (a model built based on Transformer-XL). We \ntested our methodology using open datasets for performance \ncomparisons. \nII. Methods\nFigure 1 shows a comparison of BERT, RoBERTa, and XLNet \nto verify PHI recognition performance. The letters in the \nclinical document were tokenized and identified by inside-\noutside-beginning (IOB) tagging. Subsequently, the tagged \nletters were tokenized through WordPiece tokenization to \ncreate inputs that can be placed into each pre-training mod-\nel. The resulting data were trained with BERT, RoBERTa, \nand XLNet and evaluated to determine the performance of \nthe models. The pipeline was tested on the i2b2 2014 dataset.\n18\nwww.e-hir.org\nSeo Hyun Oh et al https://doi.org/10.4258/hir.2022.28.1.16\n1. Experimental Data\nThe i2b2 2014 dataset was used in this study [17]. This is \none of the most representative datasets publicly available for \nrecognizing PHI in clinical documents. This dataset con-\nsists of 1,304 anonymized medical records of 296 patients \nwith diabetes. To revitalize de-identification research in \nthe medical field, the Clinical Natural Language Processing \nChallenge was conducted at i2b2 2014, and in this study, a \ndataset that was part of the challenge was used. In total, 790 \ndata records from 188 patients were provided for training, \nand 514 data records from 109 patients were used for testing. \nFurthermore, 17,045 PHI instances for training and 11,462 \nPHI instances for testing were provided as annotated into \nseven large categories and 25 detailed categories. Further, it \ncan be used with permission from the i2b2 homepage [18]. \nThe PHI included in the dataset was directly annotated by a \nmedical records specialist and was annotated based on the \nHIPPA-PHI category and the more detailed i2b2-PHI cat-\negory, as follows:\n â€¢ NAME (types: PATIENT, DOCTOR, USERNAME)\n â€¢ PROFESSION\n â€¢  LOCATION (types: ROOM, DEPARTMENT, HOSPI-\nTAL, ORGANIZATION, STREET, CITY , STATE, COUN-\nTRY , ZIP , OTHER)\n â€¢ AGE\n â€¢ DATE\n â€¢  CONTACT (types: PHONE, FAX, EMAIL, URL, IP AD-\nDRESS)\n â€¢  IDs (types: SOCIAL SECURITY NUMBER, MEDICAL \nRECORD NUMBER, HEALTH PLAN NUMBER, AC-\nCOUNT NUMBER, LICENSE NUMBER, VEHICLE ID, \nDEVICE ID, BIOMETRIC ID, ID NUMBER) \n2. Preprocessing \nAppropriate preprocessing must be performed to input sen-\ntences into the deep learning model. We tokenized words \nand identified them using an IOB tagging scheme. IOB tag-\nging is a method for recognizing entity names in named-\nentity recognition (NER), where â€œBâ€ denotes beginning, â€œIâ€ \nindicates inside, and â€œOâ€ denotes outside. Specifically, â€œBâ€ is \nthe part where the entity name begins, â€œIâ€ is the inner part \nof the entity name, and â€œOâ€ is the part that is not the entity \nname. For example, in the i2b2 dataset, â€œrecordâ€ is a mean-\ningless word, so it is marked with O; â€œFrankâ€ is the begin-\nning of a doctorâ€™s name, so B-DOCTOR; and â€œT. â€ is tagged \nas I-DOCTOR because it denotes the middle of the doctorâ€™s \nname.\n In addition, because BERT has a maximum input size of \n512 words, each sentence is divided into 250 words and used \nas the model input. We set the input size to 250 given the \nmodel training time and computing environment condi-\ntions. For input into the BERT model, we tokenized the data \nagain using the WordPiece tokenizer. WordPiece represents \nwords in subword units until they can be represented. When \ntokenizing WordPiece, the beginning and end of a sentence \nare marked with â€œCLSâ€ and â€œSEPâ€ tags, respectively. To \nmaintain the IOB tagging, it was applied equally to the token \nbefore being divided and then after.\n3. PHI Recognition Model\nIn this study, the performance of the BERT, RoBERTa, and \nXLNet models was compared using pre-training models. All \nmodels used in this study belong to the latest technology and \nare transformer-based models. BERT, a transformer-based \nmodel, has recently shown good performance, and many de-\nrivative models have been released, so we wanted to compare \nthem.\n1) BERT\nBERT [19] is a transformer-based pre-training language \nmodel developed by Google. Pre-training refers to a model \nthat trains data in advance through masking or unsupervised \nlearning. If pre-training is used, a higher performance can \nDataset\nData preprocessing\nRecord data: 2083-10-18 CARDIOLOGY WETZEL\nCOUNTY HOSPITAL reason for visit: ,....\nTokenization\nIOB tagging\nWordPiece\ntokenization\nOutput\nO, HOSPITAL\nHOSPITAL\nO, B-DATE ,O , B-HOSPITAL ,I -,\nI- ,O ,O ,O , ...\nBERT RoBERTa XLNet\nFigure 1.  Pipeline showing the inputs and outputs of deep learn-\ning models: BERT (bidirectional encoder representations \nfrom transformers), RoBERTa (robustly optimized BERT \npre-training approach), and XLNet (a model built based \non Transformer-XL). IOB: inside-outside-beginning.\n19Vol. 28  â€¢  No. 1  â€¢  January 2022\nwww.e-hir.org\nProtected Information Recognition by Transformer\nbe expected because it can be fine-tuned according to the \npurpose. The transformer has an encoder comprising multi-\nhead self-attention that can process information in various \ndimensions. Previous models, such as RNN and LSTM, have \npoor performance as they were mainly trained by consider -\ning all tokens. BERT, which has addressed this problem, is \nbased on a bidirectional transformer and learns by grasping \nall the flow of context through multi-head self-attention. \nIt changes layers and can be applied to various recognition \ntasks [20]. \n In this study, the pre-trained BERT was fine-tuned and \napplied using the i2b2 data. Figure 2 shows a picture of the \nBERT structure and explains the fine-tuned structure of \nthe i2b2 dataset. In this study, among the BERT models, the \nweighting for pre-trained models â€œBERT-based-model (un-\ncased)â€ [19], which is a pre-trained model for English, was \nused.\n2) RoBERTa\nRoBERTa [21] is a model released at Washington University \nand Facebook in July 2019, and it complements the train-\ning process of BERT. The improvements in RoBERTa can \nbe classified into dynamic masking, input format, and large \nbatch training. \n Dynamic masking is a method for transforming the mask \nat every training step, unlike iterative masking in the BERT \nmodel. The mask should be dynamically transformed to \nanalyze it using a large amount of data.\n The weighting for pre-trained models â€œroberta-base-\nsquad2â€ [22] provided by deepset was used for fine-tuning. \nThe Adam optimizer [23] was used to train the model, and \nthe training was repeated for five epochs. \n3) XLNet \nXLNet [24] is a model released by Google that recorded the \nhighest performance among 20 natural language processing \ndatasets at the time of publication. It is a model built based \non Transformer-XL and is a pre-training model based on an \nautoregressive and autoencoding permutation model, which \nis generally known to have satisfactory performance (Figure \n3).\n Transformer-XL solves the shortcomings wherein the sen-\ntence length is fixed and information cannot be transmitted \nbetween segments because the existing transformer only sees \nthe sentences once. Further, it has the advantage of contain-\ning all the information of the segment in a cache and calcu-\nlating it quickly by the segment unit.\n In this study, the weighting for pre-trained models â€œxlnet-\nbase-casedâ€ [25] was used in the experiment. It consists of \n12 layers and 768 hidden layers. The Adam optimizer was \nused for training model, and training was repeated for five \nepochs.\n4. Evaluation \nThe performance of the models was evaluated in terms of \nrecall, precision, and the F1-score. Accuracy was excluded \nbecause the amount of O (no meaning) during IOB tagging \nwas meaningless in clinical documents, which contain a \nlarge amount of unbalanced data.\n Recall recognizes that the â€œtrueâ€ answer is indeed the cor -\nOB-DATE B-HOSPITAL B-DOCTORI -DOCTOR\nT1\nOO\nT3 T4 T5T2 T1 T2\n[MASK] E2E2E1 E3 E4 E5\nRecord date 2083-10-18 CARDIOLOGY WETZELF rank T.\nFigure 2.   Model structure of BERT (bi-\ndirectional encoder repre-\nsentations from transfor -\nmers).\n20\nwww.e-hir.org\nSeo Hyun Oh et al https://doi.org/10.4258/hir.2022.28.1.16\nrect answer, and the formula is as expressed follows:\n \nğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘… =  ğ‘‡ğ‘‡ğ‘‡ğ‘‡\nğ‘‡ğ‘‡ğ‘‡ğ‘‡ + ğ¹ğ¹ğ¹ğ¹. \n   Precision refers to identifying the correct answer among \nthe predictions, and the formula is expressed as follows:\n \nğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ =  ğ‘‡ğ‘‡ğ‘ƒğ‘ƒ\nğ‘‡ğ‘‡ğ‘ƒğ‘ƒ + ğ¹ğ¹ğ‘ƒğ‘ƒ. \n \n  \n The F1-score [26] is the harmonic average of precision and \nrecall, and the formula is expressed as follows:\n \nğ¹ğ¹ğ¹ âˆ’ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ =2  ğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ƒğ‘ƒ Ã— ğ‘…ğ‘…ğ‘ ğ‘ ğ‘ ğ‘ ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…\nğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ƒğ‘ƒ + ğ‘…ğ‘…ğ‘ ğ‘ ğ‘ ğ‘ ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…. \nIn this formula, true positive (TP) denotes a correct answer, \na false negative (FN) is a case in which â€œtrueâ€ is predicted as \nâ€œfalse, â€ and a false positive (FP) is a case in which â€œfalseâ€ is \npredicted as â€œtrue. â€ \nIII. Results\nIn this study, the values of recall, precision, and F1-score of \neach prediction model were checked to compare their per -\nformance.\n1. Performance Comparison of BERT, RoBERTa, and XLNet\nTable 1  shows the average predicted values for precision, \nrecall, and the F1-score of BERT, RoBERTa, and XLNet. As \npresented in Table 1, the overall performance of RoBERTa \nand XLNet was higher than that of the existing BERT. Fur -\nther, XLNet showed the highest performance in the PHI \nrecognition experiment with a recall of 95.82%, precision of \n96.76%, and F1-score of 96.29%. Table 2 shows the training \ntime of BERT, RoBERTa, and XLNet. In BERT, training time \nwas the shortest at 348 seconds, while the longest training \ntime was found for XLNet, at 1,215 seconds.\n2. PHI Entity Tag Performance of BERT, RoBERTa, and XLNet\nTable 3 presents the PHI entity performance for BERT, Ro-\nBERTa, and XLNet. It was observed that the recognition \nperformance of PHI-related words, such as DOCTOR, PA -\nTIENT, and HOSPITAL, was approximately 30% higher in \nRoBERTa and XLNet than in BERT. \nTable 1. Recall, precision, and F1-score of BERT, RoBERTa, and XL-\nNet\nRecall Precision F1-score\nBERT 0.85 0.86 0.85\nRoBERTa 0.92 0.93 0.93\nXLNet 0.95 0.96 0.96\nBERT: bidirectional encoder representations from transformers, \nRoBERTa: robustly optimized BERT pre-training approach, XL-\nNet: a model built based on Transformer-XL.\nTable 2. Training time of BERT, RoBERTa, and XLNet\nTraining time (s)\nBERT 348\nRoBERTa 685\nXLNet 1,215\nBERT: bidirectional encoder representations from transformers, \nRoBERTa: robustly optimized BERT pre-training approach, XL-\nNet: a model built based on Transformer-XL.\nFigure 3.  Permutation model with au-\ntoregressive and autoencod-\ning models.\nRecord date 2083-10-18 WETZEL\nAutoregressive\nAutoregressive\nAutoregressive model\nAutoEncoding model\nPermutation model\nAutoregressive\nRecord date Mask mask\nAutoEncoding\nRecorddate CARDIOLOGY HOSPITAL2083-10-18 WETZEL\nCARDIOLOGY\nCARDIOLOGY\nHOSPITAL\nHOSPITAL\nToken s order changed by Factorization order\n21Vol. 28  â€¢  No. 1  â€¢  January 2022\nwww.e-hir.org\nProtected Information Recognition by Transformer\nIV. Discussion\nSeveral recent studies have focused on the use of â€œdark \ndata, â€ and this need has emerged in the medical field, where \nresearch is being concentrated [27]. The term â€œdark dataâ€ \nrefers to data that have been collected, but only stored and \nare not used for analysis. In the medical field, clinical docu-\nments prepared by clinicians belong to this category. How-\never, with recent advances in NER research, de-identification \nis being performed to ensure that clinical documents, which \nare semi-structured data, can be actively used for analysis. \nAs research is actively conducted, efforts are being made \nto develop a model with better performance and a shorter \ntraining duration.\n We also identified values for FP and FN as part of the per -\nformance evaluation. An example of an FP would be the \nprediction of an entity as DOCTOR when it was actually \nPATIENT. In addition, as shown in Table 4, there were cases \nwhere the prediction was HOSPITAL, but in reality it was \nCITY or COUNTRY . Similarly, for FN results, there was a \nproblem in that it was not possible to distinguish and predict \nsimilar types of data. To address these limitations, future \nstudies will use a pre-trained model with a large amount of \nclinical data.\n The difference between this study and other studies is that \nthe present study adopted a pre-training model and transfer \ntraining method to develop the most effective PHI recogni-\ntion model despite limited data. In particular, in the image \nTable 3. PHI entity performance evaluation of BERT, RoBERTa, and XLNet\nRecall Precision F1-score\nSupport\nBERT RoBERTa XLNet BERT RoBERTa XLNet BERT RoBERTa XLNet\nMEDICALRECORD 0.97 0.98 0.99 0.98 0.95 0.98 0.98 0.96 0.98 1,849\nDATE 0.98 0.99 0.99 0.98 0.99 0.99 0.98 0.99 0.99 13,251\nIDNUM 0.93 0.91 0.93 0.88 0.75 0.88 0.90 0.82 0.90 642\nAGE 0.94 0.97 0.97 0.85 0.95 0.98 0.89 0.96 0.98 628\nPHONE 0.91 0.84 0.95 0.75 0.79 0.97 0.82 0.81 0.96 707\nZIP 0.88 0.95 0.99 0.75 0.96 0.97 0.81 0.95 0.98 377\nSTATE 0.68 0.92 0.94 0.92 0.58 0.88 0.78 0.71 0.91 248\nPATIENT 0.49 0.90 0.97 0.53 0.88 0.96 0.51 0.89 0.97 2,135\nDOCTOR 0.57 0.91 0.95 0.44 0.91 0.96 0.50 0.91 0.96 2,562\nHOSPITAL 0.31 0.85 0.90 0.32 0.73 0.81 0.31 0.78 0.85 1,539\nCITY 0.11 0.80 0.82 0.69 0.58 0.66 0.19 0.67 0.73 390\nSTREET 0.13 0.93 0.95 0.12 0.90 0.89 0.12 0.92 0.92 183\nCOUNTRY 0.00 0.12 0.61 0.00 0.92 0.81 0.00 0.21 0.73 122\nDEVICE 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 29\nLOCATION-OTHER 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 15\nORGANIZATION 0.00 0.07 0.45 0.00 0.40 0.71 0.00 0.12 0.55 92\nPROFESSION 0.00 0.47 0.51 0.00 0.33 0.54 0.00 0.39 0.53 136\nUSERNAME 0.00 0.89 0.95 0.00 0.88 0.79 0.00 0.89 0.86 148\nBERT: bidirectional encoder representations from transformers, RoBERTa: robustly optimized BERT pre-training approach, XLNet: \na model built based on Transformer-XL.\nTable 4. Examples of comparisons of valid and prediction tags in BERT\nInstances\nSentences pain persisted even after returning from blounstown\nValid tags â€˜Oâ€™ â€˜Oâ€™ â€˜Oâ€™ â€˜Oâ€™ â€˜Oâ€™ â€˜Oâ€™ â€˜B-CITYâ€™\nPrediction tags â€˜Oâ€™ â€˜Oâ€™ â€˜Oâ€™ â€˜Oâ€™ â€˜Oâ€™ â€˜Oâ€™ â€˜B-HOSPITAL â€™\nBERT: bidirectional encoder representations from transformers.\n22\nwww.e-hir.org\nSeo Hyun Oh et al https://doi.org/10.4258/hir.2022.28.1.16\nfield, various problems have been solved using pre-trained \nmodels, such as VGG, ResNET, and their weights. We ap-\nplied the transfer training method for a pre-trained model to \nidentify PHI in clinical documents and confirmed the strong \nrecognition performance of the method.\n In addition to BERT, we applied the most representative \npre-training model and the latest derivative models, and we \ndetermined which model had a superior performance. In \nparticular, XLNet showed a 10% performance improvement \ncompared to the BERT-based model. This is because the \ntwo-stream self-attention method, which is a characteristic \nof XLNet, solves the change in the order of tokens due to \nthe permutation model; therefore, it is better composed as a \nword embedding method than BERT.\n When verifying the PHI recognition performance, Ro -\nBERTa and XLNet showed a performance improvement of \napproximately 30% in words related to proper nouns, such \nas DOCTOR, PATIENT, and HOSPITAL. This result im-\nplies that RoBERTa and XLNet are more effective in grasp-\ning the context than the general BERT model. However, \nfor DEVICE, LOCATION-OTHER, ORGANIZATION, \nUSERNAME, and COUNTRY , the recall, precision, and F1-\nscore were 0. The above-mentioned entities have fewer than \n200 classes, which is too small compared to other entities, \nso sufficient data for learning have not been secured. The \nabsolute number of entities also affected the occurrence of \nthis problem, but it is considered that this problem occurred \nbecause the dataset was unbalanced, with a clear difference \ncompared to the other entity counts.\n Oversampling is a method to solve data imbalance. There \nare deep learning methods such as variational autoencod-\ners [28] and generative adversarial networks [29] for overs-\nampling, and if the above methods are applied in future \nresearch, good results can be expected even for entities with \npoor learning rates. \n A limitation of this study is that a single-institution dataset \nwas used and that only one dataset was used. In addition, the \ni2b2 2014 dataset has refined data in which abbreviations \nand uppercase and lowercase letters are not arranged. In \nfuture research, we will conduct research that more closely \nreflects the characteristics of real-world medical documents \nusing multi-institutional medical document datasets.\n In this study, we compared the prediction performance \nof the three models, namely, BERT, RoBERTa, and XLNet, \nwhich are pre-training models that have recently been used. \nThe performance of XLNet was superior among these three \nmodels. A model that had been pre-trained using a general \ncorpus was used; however, in previous studies, the BERT \nmodel was trained to specialize in various documents and \nexhibited satisfactory performance [30]. In future studies, \nbetter performance can be achieved if a model that has been \npre-trained with clinical and biomedical documents, among \nothers, is used.\n Another limitation of this study is the problem of general-\nizing the model. There are few publicly available PHI data -\nsets that are properly annotated, making it difficult to apply \na model to many datasets. Further, the i2b2 dataset, which \nwas used to verify this model, has a single data format from \na single institution. Therefore, future research will focus on \ncollecting multi-center and various types of clinical docu-\nments and presenting a more generalized model.\nConflict of Interest\nNo potential conflict of interest relevant to this article was \nreported.\nAcknowledgments\nThis research was supported by the Ministry of Science and \nICT, Korea, under the Information Technology Research \nCenter support program (No. IITP-2021-2017-0-01630) su-\npervised by the IITP (Institute for Information & communi-\ncations Technology Promotion).\nORCID\nSeo Hyun Oh (https://orcid.org/0000-0001-8047-3032)\nMin Kang (https://orcid.org/0000-0002-0548-170X)\nY oungho Lee (https://orcid.org/0000-0003-0720-0569)\nReferences\n1. Park YT, Kim YS, Yi BK, Kim SM. Clinical decision sup-\nport functions and digitalization of clinical documents \nof electronic medical record systems. Healthc Inform \nRes 2019;25(2):115-23.\n2. Choi YI, Park SJ, Chung JW , Kim KO, Cho JH, et al. \nDevelopment of machine learning model to predict the \n5-year risk of starting biologic agents in patients with \ninflammatory bowel disease (IBD): K-CDM network \nstudy. J Clin Med 2020;9(11):3427.\n3. Seong D, Yi BK. Research trends in clinical natural \nlanguage processing.Â Commun Korean Inst Inf Sci \nEngÂ 2017;35(5):20-6.\n4. Shin SY . Privacy protection and data utilization. Healthc \n23Vol. 28  â€¢  No. 1  â€¢  January 2022\nwww.e-hir.org\nProtected Information Recognition by Transformer\nInform Res 2021;27(1):1-2.\n5. National Committee on Vital and Health Statistics. \nHealth Information Privacy Beyond HIPAA: A 2018 \nEnvironmental Scan of Major Trends and Challenges \n[Internet]. Hyattsville (MD): National Committee on \nVital and Health Statistics; 2017 [cited at 2022 Jan 10]. \nAvailable from: https://ncvhs.hhs.gov/wp-content/\nuploads/2018/02/NCVHS-Beyond-HIPAA_Report-\nFinal-02-08-18.pdf.\n6. Shin SY , Park YR, Shin Y , Choi HJ, Park J, Lyu Y , et al. A \nde-identification method for bilingual clinical texts of \nvarious note types. J Korean Med Sci 2015;30(1):7-15.\n7. Lafferty J, McCallum A, Pereira FC. Conditional ran-\ndom fields: probabilistic models for segmenting and \nlabeling sequence data. Proceedings of the 18th Interna-\ntional Conference on Machine Learning (ICML); 2001 \nJun 28â€“Jul 1; San Francisco, CA. p. 282-9.\n8. Wang Y . Annotating and recognising named entities in \nclinical notes. Proceedings of the ACL-IJCNLP 2009 \nStudent Research Workshop;Â 2009 Aug 4; Suntec, Singa-\npore. p. 18-26\n9. Dreyfus SE. Artificial neural networks, back propaga-\ntion, and the Kelley-Bryson gradient procedure. J Guid \nControl Dyn 1990;13(5):926-8.\n10. Team AI Korea. Recurrent neural network (RNN) tuto-\nrial, Part 1 [Internet]. [place unknow]: Team AI Korea; \n2015 [cited at 2022 Jan 10]. Available from: http://ai-\nkorea.org/blog/rnn-tutorial-1/.\n11. Hochreiter S, Schmidhuber J. Long short-term memo-\nry.Â Neural Comput 1997;9(8):1735-80.\n12. Liu Z, Y ang M, Wang X, Chen Q, Tang B, Wang Z, et al. \nEntity recognition from clinical texts via recurrent neu-\nral network. BMC Med Inform Decis Mak 2017;17(Sup-\npl 2):67.\n13. Mikolov T, Chen K, Corrado G, Dean J. Efficient esti-\nmation of word representations in vector space [Inter -\nnet]. Ithaca (NY):Â arXiv.org; 2013 [cited at 2022 Jan 10]. \nAvailable from: https://arxiv.org/abs/1301.3781.\n14. fastText [Internet]. Menlo Park (CA): Facebook Inc.; \n2020 [cited at 2022 Jan 10]. Available from: https://fast-\ntext.cc/.\n15. Peters ME, Neumann M, Iyyer M, Gardner M, Clark \nC, Lee K, et al. Deep contextualized word representa-\ntions [Internet]. Ithaca (NY):Â arXiv.org; 2018 [cited \nat 2022 Jan 10]. Available from: https://arxiv.org/\nabs/1802.05365.\n16. Kim JM, Lee JH. Text document classification based on \nrecurrent neural network using word2vec. J Korean Inst \nIntell Syst 2017;27(6):560-5.\n17. Stubbs A, Kotfila C, Uzuner O. Automated systems for \nthe de-identification of longitudinal clinical narratives: \noverview of 2014 i2b2/UTHealth shared task Track 1. J \nBiomed Inform 2015;58 Suppl(Suppl):S11-S19.\n18. DBMI Data Portal. n2c2 NLP Research Data Sets [Inter-\nnet]. Boston (MA): Harvard Medical School; 2019 [cited \nat 2022 Jan 10]. Available from: https://portal.dbmi.hms.\nharvard.edu/projects/n2c2-nlp/.\n19. Devlin J, Chang MW , Lee K, Toutanova K. BERT: pre-\ntraining of deep bidirectional transformers for language \nunderstandingÂ [Internet]. Ithaca (NY):Â arXiv.org; 2018 \n[cited at 2022 Jan 10]. Available from: https://arxiv.org/\nabs/1810.04805.\n20. Lan Z, Chen M, Goodman S, Gimpel K, Sharma P , Sori-\ncut R. ALBERT: a lite BERT for self-supervised learn-\ning of language representations.Â Proceedings of the 8th \nInternational Conference on Learning Representations \n(ICLR); 2020 Apr 26â€“30; Addis Ababa, Ethiopia.\n21. Liu Y , Ott M, Goyal N, Du J, Joshi M, Chen D, et al. \nROBERTa: a robustly optimized BERT pretraining ap-\nproach [Internet]. Ithaca (NY):Â arXiv.org; 2019 [cited at \n2022 Jan 10]. Available from: https://arxiv.org/abs/1907. \n11692.\n22. Hugging Face. deepset/roberta-base-squad2 [Internet]. \nNew Y ork (NY): Hugging Face; 2020 [cited at 2022 Jan \n10]. Available from: https://huggingface.co/deepset/\nroberta-base-squad2.\n23. Kingma DP , Ba J. Adam: a method for stochastic optimiza-\ntion [Internet]. Ithaca (NY):Â arXiv.org; 2014 [cited at 2022 \nJan 10]. Available from: https://arxiv.org/abs/1412.6980.\n24. Y ang Z, Dai Z, Y ang Y , Carbonell J, Salakhutdinov RR, \nLe QV . Xlnet: generalized autoregressive pretraining for \nlanguage understanding.Â Adv Neural Inf Process Syst \n2019;32:5754-64.\n25. Hugging Face. xlnet-base-cased [Internet]. New Y ork \n(NY): Hugging Face; 2019 [cited at 2022 Jan 10]. Avail-\nable from: https://huggingface.co/xlnet-base-cased.\n26. Warby SC, Wendt SL, Welinder P , Munk EG, Carrillo O, \nSorensen HB, et al. Sleep-spindle detection: crowdsourc-\ning and evaluating performance of experts, non-experts \nand automated methods. Nat Methods 2014;11(4):385-\n92.\n27. Kim YW , Cho N, Jang HJ. Trends in Research on the se-\ncurity of medical information in Korea: focused on in-\nformation privacy security in hospitals. Healthc Inform \nRes 2018;24(1):61-8.\n28. Kingma DP , Welling M. Auto-encoding variational bayes \n24\nwww.e-hir.org\nSeo Hyun Oh et al https://doi.org/10.4258/hir.2022.28.1.16\n[Internet]. Ithaca (NY):Â arXiv.org; 2013 [cited at 2022 Jan \n10]. Available from: https://arxiv.org/abs/1312.6114.\n29. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-\nFarley D, Ozair S, et al. Generative adversarial nets.Â Adv \nNeural Inf Process Syst 2014;27:2672-80.\n30. Alsentzer E, Murphy JR, Boag W , Weng WH, Jin D, \nNaumann T, et al. Publicly available clinical BERT em-\nbeddings [Internet]. Ithaca (NY):Â arXiv.org; 2019 [cited \nat 2022 Jan 10]. Available from: https://arxiv.org/abs/ \n1904.03323."
}