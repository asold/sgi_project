{
  "title": "Input Combination Strategies for Multi-Source Transformer Decoder",
  "url": "https://openalex.org/W2900094004",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2310900135",
      "name": "Jindřich Libovický",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2973559748",
      "name": "Jindřich Helcl",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A339531207",
      "name": "David Mareček",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2593543827",
    "https://openalex.org/W2807401415",
    "https://openalex.org/W2222949842",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2247931231",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2626792426",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2896234464",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2509282593",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2889903020",
    "https://openalex.org/W2116492146"
  ],
  "abstract": "In multi-source sequence-to-sequence tasks, the attention mechanism can be modeled in several ways. This topic has been thoroughly studied on recurrent architectures. In this paper, we extend the previous work to the encoder-decoder attention in the Transformer architecture. We propose four different input combination strategies for the encoder-decoder attention: serial, parallel, flat, and hierarchical. We evaluate our methods on tasks of multimodal translation and translation with multiple source languages. The experiments show that the models are able to use multiple sources and improve over single source baselines.",
  "full_text": "Proceedings of the Third Conference on Machine Translation (WMT), V olume 1: Research Papers, pages 253–260\nBelgium, Brussels, October 31 - Novermber 1, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/W18-64026\nInput Combination Strategies for Multi-Source Transformer Decoder\nJindˇrich Libovick´y and Jindˇrich Helcl and David Mareˇcek\nCharles University, Faculty of Mathematics and Physics\nInstitute of Formal and Applied Linguistics\nMalostransk´e n´amˇest´ı 25, 118 00 Prague, Czech Republic\n{libovicky, helcl, marecek}@ufal.mff.cuni.cz\nAbstract\nIn multi-source sequence-to-sequence tasks,\nthe attention mechanism can be modeled in\nseveral ways. This topic has been thoroughly\nstudied on recurrent architectures. In this\npaper, we extend the previous work to the\nencoder-decoder attention in the Transformer\narchitecture. We propose four different in-\nput combination strategies for the encoder-\ndecoder attention: serial, parallel, ﬂat, and hi-\nerarchical. We evaluate our methods on tasks\nof multimodal translation and translation with\nmultiple source languages. The experiments\nshow that the models are able to use multiple\nsources and improve over single source base-\nlines.\n1 Introduction\nThe Transformer model (Vaswani et al., 2017) re-\ncently demonstrated superior performance in neu-\nral machine translation (NMT) and other sequence\ngeneration tasks such as text summarization or im-\nage captioning (Kaiser et al., 2017). However, all\nof these setups consider only a single input to the\ndecoder part of the model.\nIn the Transformer architecture, the represen-\ntation of the source sequence is supplied to the\ndecoder through the encoder-decoder attention.\nThis attention sub-layer is applied between the\nself-attention and feed-forward sub-layers in each\nTransformer layer. Such arrangement leaves many\noptions for the incorporation of multiple encoders.\nSo far, attention in sequence-to-sequence learn-\ning with multiple source sequences was mostly\nstudied in the context of recurrent neural networks\n(RNNs). Libovick ´y and Helcl (2017) explicitly\ncapture the distribution over multiple inputs by\nprojecting the input representations to a shared\nvector space and either computing the attention\nover all hidden states at once, or hierarchically, us-\ning another level of attention applied on the con-\ntext vectors. Zoph and Knight (2016) employ a\ngating mechanism for combining the context vec-\ntors. V oita et al. (2018) adapted the gating mech-\nanism for use within the Transformer model for\ncontext-aware MT. The other aproaches are how-\never not directly usable in the Transformer model.\nWe propose a number of strategies of com-\nbining the different sources in the Transformer\nmodel. Some of the strategies described in this\nwork are an adaptation of the strategies previously\nused with recurrent neural networks (Libovick ´y\nand Helcl, 2017), whereas the rest of them is a\nnovel contribution devised for the Transformer ar-\nchitecture. We test these strategies on multimodal\nmachine translation (MMT) and multi-source ma-\nchine translation (MSMT) tasks.\nThis paper is organized as follows. In Sec-\ntion 2, we brieﬂy describe the decoder part of\nthe Transformer model. We propose a number of\ninput combination strategies for the multi-source\nTransformer model in Section 3. Section 4 de-\nscribes the experiments we performed, and Sec-\ntion 5 shows the results of quantitative evaluation.\nAn overview of the related work is given in Sec-\ntion 6. We discuss the results and conclude in\nSection 7.\n2 Transformer Decoder\nThe Transformer architecture is based on the use\nof attention. Attention, as conceptualized by\nVaswani et al. (2017), can be viewed as a soft-\nlookup function operating on an associative mem-\nory. For each query vector in query set Q, the at-\ntention computes a set of weighted sums of values\nV associated with a set of keys K, based on their\nsimilarity to the query.\nThe variant of the attention function used in\nthe Transformer architecture is called multi-head\nscaled dot-product attention. Scaled dot-product\n253\nof queries and keys is used as the similarity mea-\nsure. Given the dimension of the input vectors d,\nthe attention is computed as follows:\nA(Q,K,V ) = softmax\n(QK⊤\n√\nd\n)\nV. (1)\nIn the multi-head variant, the vectors that represent\nthe queries, keys, and values are linearly trans-\nformed to a number of projections (usually with\nsmaller dimension), called attention heads. The\nattention is computed in each head independently\nand the outputs are concatenated and projected\nback to the original dimension:\nAh(Q,K,V ) =\nh∑\ni=1\nCiWO\ni (2)\nwhere WO\ni ∈Rdh×d are trainable parameter ma-\ntrices used as projections of the attention head out-\nputs of dimension dh to the model dimension d,\nand\nCi = A(QWQ\ni ,KW K\ni ,VW V\ni ) (3)\nwhere WQ, WK, and WV ∈Rd×dh, are trainable\nprojection matrices used to project the attention in-\nputs to the attention heads.\nThe model itself consists of a number of lay-\ners, each of which is divided in three sub-layers:\nself-attention, encoder-decoder (or cross) atten-\ntion, and a feed-forward layer. Both of the at-\ntention types use identical sets for keys and val-\nues. The states of the previous layer are used\nas the query set. The self-attention sub-layer at-\ntends to the previous decoder layer (i.e. the sets of\nqueries and keys are identical). Since the decoder\nworks autoregressively from left to right, during\ntraining, the self-attention is masked to prevent\nattending to the future positions in the sequence.\nThe encoder-decoder attention sub-layer attends to\nthe ﬁnal layer of the encoder. The feed-forward\nsub-layer consists of a single non-linear projec-\ntion (usually to a space with larger dimension),\nfollowed by a linear projection back to the vec-\ntor space with the original dimension. The input\nof each sub-layer is summed with the output, cre-\nating a residual connection chain throughout the\nwhole layer stack.\n3 Proposed Strategies\nWe propose four input combination strategies for\nmulti-source variant of the Transformer network,\nself-attention\n⊕\ncross-attention # 1encoder # 1\n⊕\ncross-attention # 2encoder # 2\n⊕\nfeed-forward layer\n⊕\n(a) serial\nself-attention\n⊕\ncross-attention # 1\nencoder # 1\n⊕\ncross-attention # 2\nencoder # 2\nfeed-forward layer\n⊕\n(b) parallel\nself-attention\n⊕\nencoder # 1\nencoder # 2\nconcat\ncross-attention\n⊕\nfeed-forward layer\n⊕\n(c) ﬂat\nself-attention\n⊕\ncross-attention # 1\nencoder # 1\ncross-attention # 2\nencoder # 2\nattention over contexts\n⊕\nfeed-forward layer\n⊕\n(d) hierarchical\nFigure 1 : Schemes of computational steps for\nthe serial, parallel, ﬂat, and hierarchical attention\ncombination in a single layer of the decoder.\n254\nas illustrated in Figure 1. Two of them, serial\nand parallel, model the encoder-decoder attentions\nindependently and are a natural extension of the\nsub-layer scheme in the transformer decoder. The\nother two versions, ﬂat and hierarchical, are in-\nspired by approaches proposed for RNNs by Li-\nbovick´y and Helcl (2017) and model joint distri-\nbutions over the inputs.\nSerial. The serial strategy (Figure 1a) computes\nthe encoder-decoder attention one by one for each\ninput encoder. The query set of the ﬁrst cross-\nattention is the set of the context vectors computed\nby the preceding self-attention. The query set of\neach subsequent cross-attention is the output of the\npreceding sub-layer. All of these sub-layers are in-\nterconnected with residual connections.\nParallel. In the parallel combination strategy\n(Figure 1b), the model attends to each encoder in-\ndependently and then sums up the context vectors.\nEach encoder is attended using the same set of\nqueries, i.e. the output of the self-attention sub-\nlayer. Residual connection link is used between\nthe queries and the summed context vectors from\nthe parallel attention.\nAh\npara(Q,K1:n,V1:n) =\nn∑\ni=1\nAh(Q,Ki,Vi) (4)\nFlat. The encoder-decoder attention in the ﬂat\ncombination strategy (Figure 1c) uses all the states\nof all input encoders as a single set of keys and val-\nues. Thus, the attention models a joint distribution\nover a ﬂattened set of all encoder states. Unlike the\napproach taken in the recurrent setup (Libovick ´y\nand Helcl, 2017), where the ﬂat combination strat-\negy requires an explicit projection of the encoder\nstates to a shared vector space, in the Transformer\nmodels, the vector spaces of all layers are tied with\nresidual connections. Therefore, the intermediate\nprojection of the states of each encoder is not nec-\nessary.\nKﬂat = Vﬂat = concati(Ki) (5)\nAh\nﬂat (Q,K1:n,V1:n) =Ah(Q,Kﬂat ,Vﬂat ) (6)\nHierarchical. In the hierarchical combination\n(Figure 1d), we ﬁrst compute the attention inde-\npendently over each input. The resulting contexts\nare then treated as states of another input and the\nattention is computed once again over these states.\nKhier = Vhier = concati(Ah(Q,Ki,Vi)) (7)\nAh\nhier (Q,K1:n,V1:n) =Ah(Q,Khier ,Vhier ) (8)\n4 Experiments\nWe conduct our experiments on two different\ntasks: multimodal translation and multi-source\nmachine translation. We use Neural Monkey\n(Helcl and Libovick´y, 2017)1 for design, training,\nand evaluation of the experiments.\nIn all experiments, the encoder part of the net-\nwork follows the Transformer architecture as de-\nscribed by Vaswani et al. (2017).\nWe optimize the model parameters using Adam\noptimizer (Kingma and Ba, 2014) with initial\nlearning rate 0.2, and Noam learning rate decay\n(Vaswani et al., 2017) with β1 = 0.9, β2 = 0.98,\nϵ= 10−9, and 4,000 warm-up steps. The size of a\nmini-batch size of 32 for MMT, and 24 for multi-\nsource MT experiments.\nDuring decoding, we use beam search of width\n10 and length normalization of 1.0 (Wu et al.,\n2016).\n4.1 Multimodal Translation\nThe goal of MMT (Specia et al., 2016) is trans-\nlating image captions from one language into an-\nother given both the source and image as the in-\nput. We use Multi30k dataset (Elliott et al., 2016)\ncontaining triplets of images, English captions and\ntheir English translations into German, French and\nCzech. The dataset contains 29k triplets for train-\ning, 1,014 for validation and a test set of 1,000.\nWe experiment with all language pairs available in\nthis dataset.\nWe extract image feature using the last convo-\nlutional layer of the ResNet network (He et al.,\n2016) trained for ImageNet classiﬁcation. We ap-\nply a linear projection into 512 dimensions on the\nimage representation, so it has the same dimen-\nsion as the rest of the model. For each language\npair, we create a shared wordpiece-based vocabu-\nlary of approximately 40k subwords. We share the\nembedding matrices across the languages and we\nuse the transposed embedding matrix as the output\nprojection matrix as proposed by Press and Wolf\n(2017).\nWe use 6 layers in the textual encoder and de-\ncoder, and set the model dimension to 512. We\n1http://github.com/ufal/neuralmonkey\n255\nset the dimension of the hidden layers in the feed-\nforward sub-layers to 4096. We use 16 heads in\nthe attention layers.\nDuring the evaluation, we follow the prepro-\ncessing used in WMT Multimodal Translation\nShared Task (Specia et al., 2016).\nConclusions of previous work show (Elliott and\nK´ad´ar, 2017) that the improved performance of\nthe multimodal models compared to textual mod-\nels can come from improving the input representa-\ntion. In order to test whether it is also the case with\nour models or the models explicitly use the visual\ninput, we perform an adversarial evaluation simi-\nlar to Elliott (2018). We evaluate the model while\nprovidinng a random image and observe how it af-\nfects the score and observe whether their quality\ndrops.\n4.2 Multi-Source MT\nIn this set of experiment, we attempt to generate\na sentence in a target language, given equivalent\nsentences in multiple source languages.\nWe use the Europarl corpus (Tiedemann, 2012)\nfor training and testing the MSMT. We use Span-\nish, French, German, and English as source lan-\nguages and Czech as a target language. We se-\nlected an intersection of the bilingual sub-corpora\nusing English as a pivot language. Our dataset\ncontains 511k 5-tuples of sentences for training,\n1k for validation and another 1k for testing.\nDue of the memory demands of having four en-\ncoders, we use a smaller model than in the previ-\nous experiment. The encoders only have 4 layers\nand the decoder has 6 layers with embeddings size\n256, feed-forward layers dimension 2048, and 8\nattention heads. We use a shared word-piece vo-\ncabulary of 48k subwords. As in the MMT exper-\niments, the transposition of the embedding matrix\nis reused as the parameters of the output projection\nlayer (Press and Wolf, 2017).\nWe use bilingual English-to-Czech translation\nas a single source baseline. The baseline uses vo-\ncabulary of 42k subwords from Czech and English\nonly.\nSimilarly to the MMT, we also perform adver-\nsarial evaluation. To evaluate the importance of\nthe source languages for the translation quality,\nwhen randomizing one of the source languages.\n5 Results\nWe evaluate the results using BLEU (Papineni\net al., 2002) and METEOR (Denkowski and Lavie,\n2011) as implemented in MultEval. 2 The results\nof the MMT task are tabulated in Table 1. The re-\nsults of the multi-source MT are shown in Table 2.\nIn MMT, the input combination signiﬁcantly\nsurpassed the text-only baseline in English-to-\nFrench translation. The performance in other tar-\nget languages is only slightly better than the tex-\ntual baseline.\nThe only worse score was achieved by the ﬂat\ncombination strategy. We hypothesize this might\nbe because the optimization failed to ﬁnd a com-\nmon representation of the input modalities that\ncould be used to compute the joint distribution.\nThe adversarial evaluation with randomly se-\nlected input images shows that all our models rely\non both inputs while generating the target sentence\nand that providing incorrect visual input harms the\nmodel performance. The modality gating in the\nhierarchical attention combination seems to make\nthe models more robust to noisy visual input.\nIn the multi-source translation task, all the pro-\nposed strategies perform better than single-source\ntranslation from English to Czech. Among the\ncombination strategies, the best-scoring is the se-\nrial stacking of the attentions. In multimodal\ntranslation, the ﬂat combination has shown to be\nthe best-performing strategy.\nAnalysis of the attention distribution shows that\nthe serial strategy use information from all source\nlanguages. The parallel strategy almost does not\nuse the Spanish source and the ﬂat strategy prefers\nthe English source. The hierarchical strategy uses\ninformation from all source languages, however\nthe attentions are sometimes more fuzzy than in\nthe previous strategies. Figure 2 shows what\nsource languages were attended on different lay-\ners of the encoder. Other examples of the attention\nvisualization are shown in Appendix A.\nThe adversarial evaluation shows all the models\nused English as a primary source. Providing incor-\nrect English source harms. Introducing noise into\nother languages affects the score in much smaller\nscale.\n256\nMMT: en)de MMT: en)fr MMT: en)cs\nBLEU METEORadv.BLEU BLEU METEORadv.BLEU BLEU METEORadv.BLEU\nbaseline 38.3±.8 56.7±.7 — 59.6±.9 72.7±.7 — 30.9±.8 29.5±.4 —\nserial 38.7±.9 57.2±.6 37.3±.6 60.8±.9 75.1±.6 58.9±.9 31.0±.8 29.9±.4 29.7±.8\nparallel 38.6±.9 57.4±.7 38.2±.8 60.2±.9 74.9±.6 58.9±.9 31.1±.9 30.0±.4 30.4±.8\nﬂat 37.1±.8 56.5±.6 35.7±.8 58.0±.9 73.3±.7 57.0±.9 29.9±.8 29.0±.4 28.2±.8\nhierarchical 38.5±.8 56.5±.6 38.1±.8 60.8±.9 75.1±.6 60.2±.9 31.3±.9 30.0±.4 31.0±.8\nTable 1: Quantitative results of the MMT experiments on the 2016 test set. Column ‘adv. BLEU’ is an\nadversarial evaluation with randomized image input.\nMSMT Adversarial evaluation (BLEU)\nBLEU METEOR en de fr es\nbaseline 16.5±.5 20.5±.3 — — — —\nserial 20.5±.6 23.5±.5 8.1±.4 19.7±.5 19.5±.6 18.4±.5\nparallel 20.5±.6 23.3±.3 1.4±.2 18.7±.5 17.9±.5 20.3±.5\nﬂat 20.4±.6 23.3±.3 0.2±.1 19.9±.6 20.0±.6 19.6±.5\nhierarchical 19.4±.5 22.7±.3 4.2±.3 18.3±.5 18.3±.5 15.3±.5\nTable 2: Quantitative results of the MMT experiment. The adversarial evaluation shows the BLEU score\nwhen one input language was changed randomly.\nes\nfr\nde\nen\nlayer 0\nlayer 1\nlayer 2\nlayer 3\nlayer 4\nlayer 5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 2: Attention over contexts in the hiearchical\nstrategy over the decoder layers.\n6 Related Work\nMMT was so far solved only within the RNN-\nbased architectures. Elliott et al. (2015) report sig-\nniﬁcant improvements with a non-attentive model.\nWith attentive models (Bahdanau et al., 2014), the\nadditional visual information usually did not im-\nprove the models signiﬁcantly (Caglayan et al.,\n2016; Helcl and Libovick ´y, 2017) in terms of\nBLEU score. Our models slightly outperform\nthese models in the single model setup.\n2https://github.com/jhclark/multeval\nExcept for using the image features direct in-\nput to the model, they can be used as an auxil-\niary objective (Elliott and K ´ad´ar, 2017). In this\nsetup, the visually grounded representation, im-\nproves the MMT signiﬁcantly, achieving similar\nresults that our models achieved using only the\nMulti30k dataset.\nTo our knowledge, multi-source MT has also\nbeen studied only using the RNN-based models.\nDabre et al. (2017) use simple concatenation of\nsource sentences in various languages and process\nthem with a single multilingual encoder.\nZoph and Knight (2016) try context concate-\nnation and hierarchical gating method for com-\nbining context vectors in attention models with\nmultiple inputs encoded by separate encoders. In\nall of their experiments, the multi-source meth-\nods signiﬁcantly surpass the single-source base-\nline. Nishimura et al. (2018) extend the former ap-\nproach for situations when of the source languages\nis missing, so that the translation system does not\noverly rely on a single source language like some\nof the models presented in this work.\n7 Conclusions\nWe proposed several input combination strate-\ngies for multi-source sequence-to-sequence learn-\ning using the Transformer model (Vaswani et al.,\n2017). Two of the strategies are a straightfor-\nward extension of cross-attention in the Trans-\n257\nformer model: the cross-attentions are combined\neither serially interleaved by residual connections\nor in parallel. The two remaining strategies are\nan adaptation of the ﬂat and the hierarchical at-\ntention combination strategies introduced by Li-\nbovick´y and Helcl (2017) in context of recurrent\nsequence-to-sequence models.\nThe results on the MMT task show similar\nproperties an in RNN-based models (Caglayan\net al., 2017; Libovick ´y and Helcl, 2017). Adding\nvisual features signiﬁcantly improves translation\ninto French and brings minor improvements on\nother language pairs. All the attention combina-\ntions perform similarly with the exception of the\nﬂat strategy which probably struggles with learn-\ning a shared representation of the input tokens and\nthe image representation.\nEvaluation on multi-source MT shows signif-\nicant improvements over the single-source base-\nline. However, the adversarial evaluation suggests\nthat the model relies heavily on the English input\nand only uses the additional source languages for\nminor modiﬁcations of the output. All attention\ncombinations performed similarly.\nAcknowledgments\nThis research received support from the grant No.\n18-02196S and No. P103/12/G084 of the Grant\nAgency of the Czech Republic, and the grant No.\n976518 of the Grant Agency of the Charles Uni-\nversity. This research was partially supported by\nSVV project number 260 453.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nOzan Caglayan, Walid Aransa, Adrien Bardet, Mer-\ncedes Garc´ıa-Mart´ınez, Fethi Bougares, Lo ¨ıc Bar-\nrault, Marc Masana, Luis Herranz, and Joost van de\nWeijer. 2017. Lium-cvc submissions for wmt17\nmultimodal translation task. In Proceedings of the\nSecond Conference on Machine Translation, Vol-\nume 2: Shared Task Papers, pages 432–439, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nOzan Caglayan, Walid Aransa, Yaxing Wang,\nMarc Masana, Mercedes Garc ´ıa-Mart´ınez, Fethi\nBougares, Lo ¨ıc Barrault, and Joost van de Weijer.\n2016. Does multimodality help human and ma-\nchine for translation and image captioning? In\nProceedings of the First Conference on Machine\nTranslation, pages 627–633, Berlin, Germany.\nAssociation for Computational Linguistics.\nRaj Dabre, Fabien Cromier `es, and Sadao Kurohashi.\n2017. Enabling multi-source neural machine trans-\nlation by concatenating source sentences in multiple\nlanguages. CoRR, abs/1702.06135.\nMichael Denkowski and Alon Lavie. 2011. Meteor\n1.3: Automatic metric for reliable optimization and\nevaluation of machine translation systems. In Pro-\nceedings of the Sixth Workshop on Statistical Ma-\nchine Translation, pages 85–91, Edinburgh, United\nKingdom. Association for Computational Linguis-\ntics.\nDesmond Elliott. 2018. Adversarial evaluation of mul-\ntimodal machine translation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nDesmond Elliott, Stella Frank, and Eva Hasler. 2015.\nMulti-language image description with neural se-\nquence models. CoRR, abs/1510.04709.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30k: Multilingual english-\ngerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language, pages 70–\n74, Berlin, Germany. Association for Computational\nLinguistics.\nDesmond Elliott and ´Akos K ´ad´ar. 2017. Imagination\nimproves multimodal translation. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 130–141, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778. IEEE Computer Society.\nJindˇrich Helcl and Jind ˇrich Libovick ´y. 2017. Neural\nMonkey: An open-source tool for sequence learn-\ning. The Prague Bulletin of Mathematical Linguis-\ntics, 107:5–17.\nJindˇrich Helcl and Jind ˇrich Libovick ´y. 2017. CUNI\nsystem for the WMT17 multimodal translation task.\nIn Proceedings of the Second Conference on Ma-\nchine Translation, Volume 2: Shared Task Papers,\npages 450–457, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nLukasz Kaiser, Aidan N. Gomez, Noam Shazeer,\nAshish Vaswani, Niki Parmar, Llion Jones, and\nJakob Uszkoreit. 2017. One model to learn them\nall. CoRR, abs/1706.05137.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\n258\nJindˇrich Libovick´y and Jindˇrich Helcl. 2017. Attention\nstrategies for multi-source sequence-to-sequence\nlearning. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 2: Short Papers), pages 196–202, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nYuta Nishimura, Katsuhito Sudoh, Graham Neubig,\nand Satoshi Nakamura. 2018. Multi-source neural\nmachine translation with missing data. In The Sec-\nond Workshop on Neural Machine Translation and\nGeneration (WNMT), Melbourne, Australia.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\n40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 157–163. Association\nfor Computational Linguistics.\nLucia Specia, Stella Frank, Khalil Sima’an, and\nDesmond Elliott. 2016. A shared task on multi-\nmodal machine translation and crosslingual image\ndescription. In Proceedings of the First Conference\non Machine Translation, pages 543–553, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In Proceedings of the Eight Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC’12), Istanbul, Turkey. European Lan-\nguage Resources Association (ELRA).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 6000–6010. Curran As-\nsociates, Inc.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers),\npages 1264–1274, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin\nJohnson, Xiaobing Liu, Łukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant\nPatil, Wei Wang, Cliff Young, Jason Smith, Jason\nRiesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2016. Google’s\nneural machine translation system: Bridging the gap\nbetween human and machine translation. CoRR,\nabs/1609.08144.\nBarret Zoph and Kevin Knight. 2016. Multi-source\nneural translation. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 30–34, San Diego, Cali-\nfornia. Association for Computational Linguistics.\n259\nA Attention Visualizations\nWe show cross-attention visualizations for the four\nproposed combination strategies on Multi-source\nMT. The Czech target wordpieces are in rows,\nthe source Spanish, French, German, and En-\nglish wordpieces are concatenated and shown in\ncolumns. These attentions were taken form the\ndecoder’s fourth layer and were averaged across\nthe individual heads. For serial and parallel strat-\negy the cross-attention weights sum to one for\neach language separately, the ﬂat strategy has only\none common cross-attention, and for the hier-\narchical strategy visualization the cross-attention\nweights for individual languages are multiplied by\nthe weights of the attention over contexts.\ntambién_\nla_\nregión_\ndel_\nMar_\nNegro_\ntiene_\nuna_\ngran_\nimportancia_\n._\nla_\nrégion_\nde_\nla_\nmer_\nNoire_\nrevêt_\n,_\nelle_\naussi_\n,_\nune_\ngrande_\nimportance_\n._\nauch_\ndie_\nSchwarzmeerr\negion\n_\nhat_\neine_\ngroße_\nBedeutung_\n._\nthe_\nBlack_\nSea_\nRegion_\n,_\ntoo_\n,_\nis_\nof_\ngreat_\nimportance_\n._\nernomo sk\ný_\nregion_\nmá_\ntaké_\nmimo ádný\n_\nvýznam_\n._\na) serial\ntambién_\nla_\nregión_\ndel_\nMar_\nNegro_\ntiene_\nuna_\ngran_\nimportancia_\n._\nla_\nrégion_\nde_\nla_\nmer_\nNoire_\nrevêt_\n,_\nelle_\naussi_\n,_\nune_\ngrande_\nimportance_\n._\nauch_\ndie_\nSchwarzmeerr\negion\n_\nhat_\neine_\ngroße_\nBedeutung_\n._\nthe_\nBlack_\nSea_\nRegion_\n,_\ntoo_\n,_\nis_\nof_\ngreat_\nimportance_\n._\nernomo sk\ný_\nregion_\nmá_\ntaké_\nmimo ádný\n_\nvýznam_\n._\nb) parallel\ntambién_\nla_\nregión_\ndel_\nMar_\nNegro_\ntiene_\nuna_\ngran_\nimportancia_\n._\nla_\nrégion_\nde_\nla_\nmer_\nNoire_\nrevêt_\n,_\nelle_\naussi_\n,_\nune_\ngrande_\nimportance_\n._\nauch_\ndie_\nSchwarzmeerr\negion\n_\nhat_\neine_\ngroße_\nBedeutung_\n._\nthe_\nBlack_\nSea_\nRegion_\n,_\ntoo_\n,_\nis_\nof_\ngreat_\nimportance_\n._\nernomo sk\ný_\nregion_\nmá_\ntaké_\nmimo ádný\n_\nvýznam_\n._\nc) ﬂat\ntambién_\nla_\nregión_\ndel_\nMar_\nNegro_\ntiene_\nuna_\ngran_\nimportancia_\n._\nla_\nrégion_\nde_\nla_\nmer_\nNoire_\nrevêt_\n,_\nelle_\naussi_\n,_\nune_\ngrande_\nimportance_\n._\nauch_\ndie_\nSchwarzmeerr\negion\n_\nhat_\neine_\ngroße_\nBedeutung_\n._\nthe_\nBlack_\nSea_\nRegion_\n,_\ntoo_\n,_\nis_\nof_\ngreat_\nimportance_\n._\nernomo sk\ný_\nregion_\nmá_\ntaké_\nmimo ádný\n_\nvýznam_\n._\nd) hierarchical\n260",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8359315395355225
    },
    {
      "name": "Transformer",
      "score": 0.810598611831665
    },
    {
      "name": "Encoder",
      "score": 0.7655969858169556
    },
    {
      "name": "Machine translation",
      "score": 0.5211448669433594
    },
    {
      "name": "Architecture",
      "score": 0.4689860939979553
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4120425581932068
    },
    {
      "name": "Speech recognition",
      "score": 0.4051142930984497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3947378993034363
    },
    {
      "name": "Voltage",
      "score": 0.11733919382095337
    },
    {
      "name": "Engineering",
      "score": 0.07051742076873779
    },
    {
      "name": "Electrical engineering",
      "score": 0.06899872422218323
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}