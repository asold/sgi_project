{
  "title": "BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents",
  "url": "https://openalex.org/W3200439183",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2572381778",
      "name": "Teakgyu Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098404811",
      "name": "Dong-Hyun Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2553984227",
      "name": "Mingi Ji",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Kootenay Association for Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112718749",
      "name": "Wonseok Hwang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120174123",
      "name": "Daehyun Nam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2165203449",
      "name": "Sungrae Park",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2572381778",
      "name": "Teakgyu Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098404811",
      "name": "Dong-Hyun Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2553984227",
      "name": "Mingi Ji",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Kootenay Association for Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112718749",
      "name": "Wonseok Hwang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2016234256",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2972985407",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2130723172",
    "https://openalex.org/W3003484198",
    "https://openalex.org/W3003478049",
    "https://openalex.org/W3091226465",
    "https://openalex.org/W2947697927",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W1966382373",
    "https://openalex.org/W3164881641",
    "https://openalex.org/W3108646486",
    "https://openalex.org/W3172229095",
    "https://openalex.org/W2922714365",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3000758063",
    "https://openalex.org/W3104953317",
    "https://openalex.org/W2949370368",
    "https://openalex.org/W2999038738",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3163650427",
    "https://openalex.org/W3194594797",
    "https://openalex.org/W2949861626",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3173306993",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2962772269",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2078777599",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2986619406",
    "https://openalex.org/W3176664887",
    "https://openalex.org/W3034942609",
    "https://openalex.org/W3191581838",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3205981739",
    "https://openalex.org/W2968868378",
    "https://openalex.org/W3202839357",
    "https://openalex.org/W3173325518",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3182680257",
    "https://openalex.org/W4287123554",
    "https://openalex.org/W3132244696",
    "https://openalex.org/W3176851559"
  ],
  "abstract": "Key information extraction (KIE) from document images requires understanding the contextual and spatial semantics of texts in two-dimensional (2D) space. Many recent studies try to solve the task by developing pre-trained language models focusing on combining visual features from document images with texts and their layout. On the other hand, this paper tackles the problem by going back to the basic: effective combination of text and layout. Specifically, we propose a pre-trained language model, named BROS (BERT Relying On Spatiality), that encodes relative positions of texts in 2D space and learns from unlabeled documents with area-masking strategy. With this optimized training scheme for understanding texts in 2D space, BROS shows comparable or better performance compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and SciTSR) without relying on visual features. This paper also reveals two real-world challenges in KIE tasks--(1) minimizing the error from incorrect text ordering and (2) efficient learning from fewer downstream examples--and demonstrates the superiority of BROS over previous methods.",
  "full_text": "BROS: A Pre-trained Language Model Focusing on Text and Layout\nfor Better Key Information Extraction from Documents\nTeakgyu Hong1, Donghyun Kim1, Mingi Ji2, Wonseok Hwang3, Daehyun Nam4, Sungrae Park4\n1NA VER CLOV A,2KAIST, 3LBox, 4Upstage AI Research, Upstage AI\nteakgyu.hong@navercorp.com, dong.hyun@navercorp.com, qwertgfdcvb@kaist.ac.kr,\nwonseok.hwang@lbox.kr, daehyun.nam@upstage.ai, sungrae.park@upstage.ai\nAbstract\nKey information extraction (KIE) from document images re-\nquires understanding the contextual and spatial semantics of\ntexts in two-dimensional (2D) space. Many recent studies try\nto solve the task by developing pre-trained language models\nfocusing on combining visual features from document images\nwith texts and their layout. On the other hand, this paper tack-\nles the problem by going back to the basic: effective combina-\ntion of text and layout. Speciﬁcally, we propose a pre-trained\nlanguage model, named BROS (BERT Relying On Spatial-\nity), that encodes relative positions of texts in 2D space and\nlearns from unlabeled documents with area-masking strat-\negy. With this optimized training scheme for understanding\ntexts in 2D space, BROS shows comparable or better perfor-\nmance compared to previous methods on four KIE bench-\nmarks (FUNSD, SROIE\u0003, CORD, and SciTSR) without rely-\ning on visual features. This paper also reveals two real-world\nchallenges in KIE tasks–(1) minimizing the error from incor-\nrect text ordering and (2) efﬁcient learning from fewer down-\nstream examples–and demonstrates the superiority of BROS\nover previous methods.\nIntroduction\nAutomatic key information extraction (KIE) from industrial\ndocuments is an essential task in robotic process automation\n(RPA). Extracting an ordered item list from receipts (Park\net al. 2019), prices and taxes from invoices (Liu et al. 2019),\nand paired key-values from form-like documents (Jaume,\nEkenel, and Thiran 2019) are representative examples. Since\nthe task requires understanding texts in various layouts, the\ncombination of multiple technical components from both\ncomputer vision and natural language processing is required.\nFigure 1 describes a schematic illustration of pipeline for\nthe document KIE tasks (Hwang et al. 2019; Denk and Reis-\nswig 2019). First, given a document image, optical character\nrecognition (OCR) detects the texts in the image and recog-\nnizes the content to generate a set of text blocks. Next, a\nserializer identiﬁes a reading order of text blocks distributed\nin 2D image space and converts them into text sequence in\n1D text space to apply NLP technology which is developed\nfor 1D text sequence. The most basic form of serializer is\nto arrange text blocks in a top-to-bottom and left-to-right\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nOCR Parsing\nModel\nDate\n9/3/92\nR&D Group\nLicensee...\nSerializer\nFigure 1: Schematic illustrations of document KIE pipeline.\nModel Img # Params (O) (P) (F)\nLayoutLMBASE 113M 78.66 33.89 62.50\nLayoutLMv2BASE ◦ 200M 82.76 40.77 69.92\nBROSBASE 110M 83.05 76.94 72.60\nLayoutLMLARGE 343M 78.95 33.11 61.00\nLayoutLMv2LARGE ◦ 426M 84.20 62.53 72.12\nBROSLARGE 340M 84.52 79.42 74.42\nTable 1: Performance comparison of pre-trained language\nmodels on (O)riginal, (P)ermuted, and (F)ew training sam-\nples FUNSD KIE tasks. In (F), 10 samples are used.\nway (Clausner, Pletschacher, and Antonacopoulos 2013). Fi-\nnally, from the serialized text blocks, key information is ex-\ntracted via the parsing model.\nIn the ﬁrst step, an off-the-shelf OCR tool is often em-\nployed as industrial documents consist of relatively clean\ncharacters compared to general scene text images. On the\nother hand, there are no such off-the-shelf tools for the se-\nrializer even though it is often non-trivial to determine the\nproper reading order of text blocks (Li et al. 2020; Wang\net al. 2021). Representative examples are documents includ-\ning multi-columns or multiple tables. This absence of the\ngeneral-purpose serializer implies the careful design of pars-\ning module is necessary to robustly handle documents with\ncomplex layouts where the reading orders can be often in-\ncomplete.\nIn the early studies of KIE, accurate document parsing\ngreatly depends on the order of text blocks. Once the se-\nrializer identiﬁes an order, the set of the text blocks are\nconverted into a text sequence and processed via a lan-\nguage model such as BERT (Devlin et al. 2019) to identify\nkey information (Hwang et al. 2019; Denk and Reisswig\n2019). The linguistic understanding of the pre-trained lan-\nguage model leads to superior performance than rule-based\nextractions. However, the conversion of texts in 2D space\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10767\ninto a text sequence in 1D space leads to the loss of layout\ninformation that is critical in KIE tasks.\nTo avoid the loss of layout information, a new type of\nlanguage model, LayoutLM (Xu et al. 2020) expands a 1D\npositional encoding of BERT to 2D and is trained over a\nlarge corpus of industrial documents to understand spatial\ndependencies between text blocks. Its ﬁne-tuning has shown\nbreakthrough performances on multiple KIE tasks and be-\ncomes a strong baseline. After the rise of LayoutLM, several\nstudies try to develop pre-trained language models by com-\nbining additional visual features (Xu et al. 2021; Powalski\net al. 2021; Li et al. 2021b; Appalaraju et al. 2021; Li et al.\n2021c) (e.g. image patches identiﬁed by an object detection)\nand show further performance improvements. However, the\nextensions using visual features require additional computa-\ntional costs and they still demand more effective combina-\ntions of texts and their spatial information.\nIn this paper, we introduce a new pre-trained language\nmodel, named BROS, by re-focusing on the combinations\nof texts and their spatial information without relying on\nvisual features. Speciﬁcally, we propose an effective spa-\ntial encoding method by utilizing relative positions between\ntext blocks, while most of previous works employ abso-\nlute 2D positions. Additionally, we introduce a novel self-\nsupervision method, named area-masked language model,\nthat hides texts in an area of a document and supervises\nthe masked texts. With these two approaches for encoding\nof spatial information, BROS shows superior or compara-\nble performances compared to previous methods using ad-\nditional visual features.\nAside from improving KIE performances, BROS also ad-\ndresses two important real-world challenges in KIE tasks:\nminimizing dependency on the order of text blocks and\nlearning from a few training examples of downstream tasks.\nThe ﬁrst challenge indicates the robustness on the serializa-\ntion followed by the OCR process in Figure 1. In real sce-\nnario, document images are usually irregular (i.e. rotated or\ndistorted documents) and the serializer might fail to iden-\ntify a proper order of text blocks. In addition, when se-\nrialization fails, the performance of sequence tagging ap-\nproaches (e.g. BIO tagging), which most previous works\nemploy, drops dramatically. To circumvent the difﬁculty, we\napply SPADE (Hwang et al. 2021) decoder that extracts key\ntext blocks without any order information to the pre-trained\nmodels and evaluates them on the new benchmarks where\nthe order of text blocks are permuted. As a result, BROS\nshows better robustness on the serializers compared to Lay-\noutLM (Xu et al. 2020) and LayoutLMv2 (Xu et al. 2021).\nThe second challenge is related to the required number\nof labeled examples to understand the target key contents.\nSince a single KIE example consists of hundreds of text\nblocks that should be categorized, the annotation is expen-\nsive. Most public benchmarks consist of less than 1,000\nsamples, even though the target documents contain hundreds\nof layouts and diverse contexts. In this paper, we analyze\nKIE performances over the number of training examples\nand compare the pre-trained models. As a result, BROS per-\nforms better on FUNSD KIE tasks, and also BROS only with\n20∼30% of FUNSD examples achieves better performance\nthan LayoutLM with 100% of them. Summarized results for\nthese experiments are shown in Table 1.\nOur contributions can be summarized as follows:\n• We propose an effective spatial layout encoding method\nby accounting for relative positions of text blocks.\n• We also propose a novel area-masking self-supervision\nstrategy that reﬂects 2D natures of text blocks.\n• The proposed model achieves comparable performance\nto the state-of-the-art without relying on visual features.\n• We compare existing pre-trained models on permuted\nKIE datasets that lost the orders of text blocks.\n• We compare the ﬁne-tuning efﬁciency of various pre-\ntrained models under a data-scarce environment.\nRelated Work\nPre-trained Language Models for 2D Text Blocks\nUnlike the pre-trained models for conventional NLP tasks,\nsuch as BERT (Devlin et al. 2019), LayoutLM (Xu et al.\n2020) is ﬁrst proposed to jointly model interaction between\ntext and layout information for the document KIE task. It\nencodes the absolute position of text blocks with axis-wise\nembedding tables and learns a token-level masked language\nmodel that hides tokens randomly and estimates the origins.\nAfter the publication of LayoutLM, several pre-trained mod-\nels have been tried to additionally integrate visual features,\nsuch as visual feature maps from raw images (Xu et al. 2021;\nAppalaraju et al. 2021), image patches identiﬁed by an ob-\nject detection module (Li et al. 2021b), and visual represen-\ntations of text blocks (Powalski et al. 2021; Li et al. 2021c).\nAlthough the extensions imposing multi-modalities of visual\nand textual features provide additional performance gains in\nKIE tasks, they spend additional computations to process\nraw document images. Additionally, an effective combina-\ntion of text and layout is still required as the major compo-\nnent of the multi-modalities.\nAside from incorporating visual features, Struc-\nturalLM (Li et al. 2021a) utilizes cell information, a\ngroup of ordered text blocks, and shows promising per-\nformance improvements. However, the local orders of text\nblocks might not be available depending on the KIE tasks\nand the OCR engines. Therefore, this paper focuses on\nthe original granularity of text blocks identiﬁed by OCR\nengines and improves the combination of text and layout\nby an effective spatial encoding method and an area-based\npre-training strategy.\nParsers for Document Key Information Extraction\nBIO tagger, which is a representative parser for entity ex-\ntraction from the text sequences, extracts key information\nby identifying spans with the beginning (B) and inside (I)\npoints. Though BIO tagger has been used as a conventional\nmethod, it has two limitations for applying to document KIE.\nOne is that the correct order of text blocks is required for\nextracting key information when post-processing each clas-\nsiﬁed token class (i.e. B- and I- classes). For example, if the\ntext blocks are not ordered properly, such as “recognition,\noptical, character”, the correct answer can not be made. The\n10768\nTokens\nBROS Encoder\nPositions\nemgeddings ...\nToken\nemgeddings\n...\n[CLS] if... reports [MASK] the [MASK] [MASK] [SEP]\n+++++ ++++\n...\nPre-training / Fine-tuning\nPositions\n(+2,0) (+6,0)(-8,0)\n(+1,-2)\nRelative spatial encoding\nToken- & area-\nmasking\nFigure 2: An overview of BROS. The tokens in the document image are masked through token- and area-masking strategy.\nThe position difference between text blocks is encoded directly to the attention mechanism in Transformer. The output token\nrepresentations are used in both pre-training and ﬁne-tuning.\nother is that it cannot solve the tasks that require the relation-\nship between tokens as it performs token-level classiﬁcation.\nTo overcome the above two limitations, we adopt a graph-\nbased parser, SPADE (Hwang et al. 2021) decoder, that cre-\nates a directed relation graph of tokens to represent key\nentities and their relationships for KIE tasks. For exam-\nple, SPADE can determine “optical” as a starting word and\n“recognition” as the next word. By directly identifying re-\nlations between tokens, SPADE enables a description of all\nkey information of KIE tasks regardless of the order of text\nblocks. In this paper, we apply the SPADE decoder for en-\ntity linking tasks of KIE benchmarks and also for all tasks\nlost perfect order information of text blocks. Speciﬁcally, we\nslightly modify the SPADE decoder for better application\nwith the pre-trained models.\nBERT Relying on Spatiality (BROS)\nThe main structure of BROS follows LayoutLM (Xu et al.\n2020), but there are two critical advances: (1) a use of spa-\ntial encoding metric that describes spatial relations between\ntext blocks and (2) a use of 2D pre-training objective de-\nsigned for text blocks on 2D space. Figure 2 shows a visual\ndescription of BROS for document KIE tasks.\nEncoding Spatial Information into BERT\nThe way to encode spatial information of text blocks de-\ncides how text blocks be aware of their spatial relations.\nLayoutLM (Xu et al. 2020) simply encodes absolute x- and\ny-axis positions to each text blocks but the speciﬁc-point en-\ncoding is not robust on the minor position changes of text\nblocks. Instead, BROS employs relative positions between\ntext blocks to explicitly encode spatial relations. As shown\nin Figure 3, relative positions provides co-modality of spa-\ntial relations between text blocks regardless of their absolute\nposition. This property can make the model better recognize\nentities which have similar key-value structures.\nFor formal description, we use p = ( x;y) to denote a\npoint on 2D space and a bounding box of a text block con-\n(a) Encodes absolute spatial information.\n(b) Encodes relative spatial information.\nFigure 3: Comparison between absolute and relative posi-\ntions. “Project #” and “Total Sample” have their paired val-\nues, “72-31” and “285”, respectively. In (a), the paired text\nblocks have different modalities based their absolute posi-\ntions. On the other hand, in (b), they can hold co-modality to\nrepresent positions of their semantically coupled text blocks.\nsists of four vertices, such as ptl, ptr, pbr, and pbl, that indi-\ncate top-left, top-right, bottom-right, and bottom-left points,\nrespectively. BROS ﬁrst normalizes all the 2D points of the\ntext blocks using the size of the image. Then, BROS calcu-\nlates relative positions of the vertices from the same vertices\nof the other bounding boxes of text blocks and applies si-\nnusoidal functions as \u0016pi;j = [f sinu(xi −xj); f sinu(yi −yj)].\nHere, f sinu : R → RDs\nindicates a sinusoidal function, which\nis used in Vaswani et al. (2017), Ds is the dimensions of si-\nnusoid embedding, and the semicolon (;) indicates concate-\nnation. Through the calculations, the relative positions ofjth\nbounding box based on the ith bounding box are represented\nwith the four vectors, such as \u0016ptl\ni;j, \u0016ptr\ni;j, \u0016pbr\ni;j, and \u0016pbl\ni;j. Fi-\nnally, BROS combines the four relative positions by apply-\ning a linear transformation,\nbbi;j = W tl \u0016ptl\ni;j + W tr \u0016ptr\ni;j + W br \u0016pbr\ni;j + W bl \u0016pbl\ni;j: (1)\nwhere W tl, W tr, W br, W bl ∈R(H=A)\u00022Ds\nare linear tran-\nsition matrices, H is a hidden size of BERT, and A is the\nnumber of self-attention heads.\n10769\n(a) Random token selection (red) and token masking (gray)\n(b) Random area selection (red) and block masking (gray)\nFigure 4: Illustrations of two masking strategies. The blue\nboxes represent text blocks including masked tokens. In both\nﬁgures, 15% of tokens are masked.\nIn the process of identifying the relative positional vec-\ntor,\nbbi;j, we carefully apply two components: the sinusoidal\nfunction, f sinu, and the shared embeddings to multiple heads\nof the attention module. First, the sinusoidal function can en-\ncode continuous distances more naturally than using a grid\nembedding that split a real-valued space into ﬁnite num-\nber of grids. Second, the multi-head attention modules in\nTransformer share the same relative positional embeddings\nto impose the common spatial relationships between text\nblocks to multiple semantic features identiﬁed by the mul-\ntiple heads.\nBROS directly encodes the spatial relations to the contex-\ntualization of text blocks. In detail, it calculates an attention\nlogit combining both semantic and spatial features as fol-\nlows;\nah\ni;j = (W q\nhti)>(W k\nhtj) + (W q\nhti)>\nbbi;j; (2)\nwhere ti and tj are context representations for ith and jth\ntokens and both W q\nh and W k\nh are linear transition matrices\nfor hth head. The former is the same as the original attention\nmechanism in Transformer (Vaswani et al. 2017). The latter,\nmotivated by Dai et al. (2019), considers the relative spatial\ninformation of the target text block when the source context\nand location are given. As we mentioned above, we have\nshared relative spatial embedding across all of the different\nattention heads for imposing the common spatial relation-\nships.\nCompared to the spatial-aware attention in Xu et al.\n(2021), which utilizes axis-speciﬁc positional difference of\ntext blocks as an attention bias, it has two major differences.\nFirst, our method couples the relative embeddings with the\nsemantic information of tokens for better conjugation be-\ntween texts and their spatial relations. Second, when cal-\nculating the relative spatial information between two text\nblocks, we consider all four vertices of the block. By doing\nthis, our encoding can incorporate not only relative distance\nbut also relative shape and size which play important roles\nin distinguishing key and value in a document. We compare\nour relative encoding method and that of LayoutLMv2’s in\nthe ablation study.\nArea-masked Language Model\nPre-training diverse layouts from unlabeled documents is\na key factor for document KIE tasks. BROS utilizes two\npre-training objectives: one is a token-masked LM (TMLM)\nused in BERT and the other is a novel area-masked LM\n(AMLM) introduced in this paper. The area-masked LM, in-\nspired by SpanBERT (Joshi et al. 2020), captures consecu-\ntive text blocks based on a 2D area in a document.\nTMLM randomly masks tokens while keeping their spa-\ntial information, and then the model predicts the masked\ntokens with the clues of spatial information and the other\nun-masked tokens. The process is identical to MLM of\nBERT and Masked Visual-Language Model (MVLM) of\nLayoutLM. Figure 4 (a) shows how TMLM masks tokens in\na document. Since tokens in a text block can be masked par-\ntially, their estimation can be conducted by referring to other\ntokens in the same block or text blocks near the masked to-\nken.\nAMLM masks all text blocks allocated in a randomly\nchosen area. It can be interpreted as a span masking for\ntext blocks in 2D space. Speciﬁcally, AMLM consists of\nthe following four steps: (1) randomly selects a text block,\n(2) identiﬁes an area by expanding the region of the text\nblock, (3) determines text blocks allocated in the area, and\n(4) masks all tokens of the text blocks and predicts them.\nAt the second step, the degree of expansion is identiﬁed\nby sampling a value from an exponential distribution with\na hyper-parameter, \u0015. The rationale behind using exponen-\ntial distribution is to convert the geometric distribution used\nin SpanBERT for a discrete domain into a distribution for\na continuous domain. Thus, we set \u0015 = −ln(1 −p) where\np= 0:2 used in SpanBERT. Also, we truncated exponential\ndistribution with 1 to prevent an inﬁnity value covering all\nspaces of the document. It should be noted that the masking\narea is expanded from a randomly selected text block since\nthe area should be related to the text sizes and locations to\nrepresent text spans in 2D space. Figure 4 compares token-\nand area-masking on text blocks. Because AMLM hides spa-\ntially close tokens together, their estimation requires more\nclues from text blocks far from the estimation targets.\nFinally, BROS combines two masked LMs, TMLM and\nAMLM, to stimulate the model to learn both individual and\nconsolidated token representations. It ﬁrst masks 15% of to-\nkens for AMLM and then masks 15% of tokens on the left\ntext blocks for TMLM. Similar to BERT (Devlin et al. 2019),\nthe masked tokens are replaced by [MASK] token for 80%,\nrandom token for 10%, and original token for the rest 10%.\nKey Information Extraction Tasks\nWe solve two categories of KIE tasks, entity extraction\n(EE) and entity linking (EL). The EE task identiﬁes se-\nquences of text blocks that represent desired target texts.\nFigure 5 (a) is an example of the EE task: identifying\nheader, question, and answer entities in the form-like doc-\nument. The EL task connects key entities through their hi-\nerarchical or semantic relations. Figure 5 (b) is an example\nof the EL task: grouping menu entities, such as its name,\nunit price, amount, and price. Table 2 lists four KIE bench-\n10770\n(a) An example of FUNSD EE task.\n(b) An example of CORD EL task.\nFigure 5: Examples of EE and EL tasks. In (a), the colored\nblocks represent key entities. In (b), the red arrows show the\nhierarchical relationships between the entities.\nDataset Types Tasks # Images\nFUNSD Forms EE, EL Train\n149, Test 50\nSROIE\u0003y Receipts EE Train\n526, Test 100\nCORD Receipts EE, EL Train\n800, Val 100, Test 100\nSciTSR Tables EL Train\n12,000, Test 3,000\nymodiﬁed v\nersion of SROIE. See details in Appendix.\nTable\n2: Tasks and the number of images for each dataset.\nmark datasets: FUNSD (Jaume, Ekenel, and Thiran 2019),\nSROIE\u0003(Huang et al. 2019), CORD (Park et al. 2019), and\nSciTSR (Chi et al. 2019).\nAlthough these four datasets provide testbeds for the EE\nand EL tasks, they represent the subset of real problems as\nthe order information of text blocks is given. FUNSD pro-\nvides the orders of text blocks related to target classes in\nboth training and testing examples. In SROIE\u0003, CORD, and\nSciTSR, the text blocks are serialized in reading orders. To\nreﬂect the real scenario that does not contain perfect order\ninformation of text blocks, we remove the order information\nof KIE benchmarks by randomly permuting the order of text\nblocks. We denote the permuted datasets as p-FUNSD, p-\nSROIE\u0003, p-CORD, and p-SciTSR.\nExperiments\nExperiment Settings\nFor pre-training, IIT-CDIP Test Collection 1.0 1 (Lewis\net al. 2006), which consists of approximately 11M doc-\nument images, is used but 400K of RVL-CDIP dataset 2\n(Harley, Ufkes, and Derpanis 2015) are excluded following\nLayoutLM. To obtain text blocks from document images,\nCLOV A OCR API3 was applied. We observed no difference\nin performance depending on the OCR engine; LayoutLM\n1https://ir.nist.gov/cdip/\n2https://www.cs.cmu.edu/ aharley/rvl-cdip/\n3https://clova.ai/ocr\ntrained in our experimental setting shows comparable per-\nformances to the published LayoutLM.\nThe main Transformer structure of BROS is the same as\nBERT. We set the hidden size, the number of self-attention\nheads, the feed-forward/ﬁlter size, and the number of Trans-\nformer layers of BROSBASE to 768, 12, 3072, and 12, respec-\ntively and those of BROS LARGE to 1024, 24, 4096, and 24,\nrespectively. The dimensions of sinusoid embedding Ds is\nset to 24 for BROSBASE and 32 for BROSLARGE.\nBROS is trained by using AdamW optimizer (Loshchilov\nand Hutter 2019) with a learning rate of 5e-5 with linear\ndecay. The batch size is set to 64. During pre-training, the\nﬁrst 10% of the total epochs are used for a warm-up learning\nrate. We initialized weights of BROS with those of BERT\nand trained it for 5 epochs on the IIT-CDIP dataset using 8\nNVIDIA Tesla V100 32GB GPUs.\nDuring ﬁne-tuning, the learning rate is set to 5e-5. The\nbatch size is set to 16 for all tasks. The number of training\nepochs or steps is as follows: 100 epochs for FUNSD, 1K\nsteps for SROIE\u0003and CORD, and 7.5 epochs for SciTSR.\nExperiment Results\nTo evaluate the performance of the model, we ﬁrst con-\nduct experiments using the given order of text blocks in the\ndataset. Then, we verify the robustness of the model against\ntwo important challenges in the KIE tasks, which are the de-\npendency about the order of text blocks and learning from a\nfew training examples.\nOver our experiments, we report the scores of LayoutLM\nand LayoutLMv2 using the models published by the au-\nthors4 and denote them as LayoutLM \u0003and LayoutLMv2\u0003.\nWe report the mean (and optionally the standard deviation)\nof the results using the 5 different random seeds.\nWith the Order Information of Text Blocks Table 3\nsummarizes the results for the FUNSD EE task reported by\nprevious approaches. When comparing models only using\ntext and layout, BROS shows remarkable performance im-\nprovements by 2.51 (80.54 → 83.05) for the BASE models\nand 5.57 (78.95 → 84.52) for the LARGE models from the\nprevious best. Interestingly, BROS provides better or similar\nperformances compared to the multi-modal models incor-\nporating additional visual (Image \u0003) or hierarchical (Cell \u0003)\ninformation. In other words, although BROS does not re-\nquire extra computations and parameters to process addi-\ntional features, BROS can achieves better or comparable\nperformances.\nTable 4 shows the F1 scores on three EE and EL tasks with\nthe order of text blocks given in the dataset. F, S, C, and Sci\nrefer to FUNSD, SROIE\u0003, CORD, and SciTSR, respectively.\nFor EE tasks, all models utilize BIO tagger that captures\nspans of text blocks to represent key entities in documents.\nFor EL tasks, SPADE decoder is used to identify relation-\nships between entities not placed sequentially in a series of\ntext blocks. In all cases, BERT performs the worst because\nthose tasks require understanding texts in 2D space, but\nBERT only encodes 1D sequential information. LayoutLM\u0003\n4https://github.com/microsoft/unilm\n10771\nFUNSD EE\nModel Modality Precision Recall\nF1 # P\narams\nBERTB\nASE (Xu et al. 2020) Te\nxt 54.69 67.10\n60.26 110M\nLayoutLMBASE (Xu\net al. 2020) Te\nxt + Layout 75.97 81.55\n78.66 113M\nDocFormerB\nASE (Appalaraju et al. 2021) Te\nxt + Layout 77.63 83.69\n80.54 149M\nBROSB\nASE (Ours) Te\nxt + Layout 81.16\u00060.33 85.02\u00060.32 83.05\u00060.26 110M\nLayoutLMBASE (Xu\net al. 2020) Te\nxt + Layout + Image* 76.77 81.95\n79.27 160M\nLayoutLMv2BASE (Xu\net al. 2021) Te\nxt + Layout + Image* 80.29 85.39\n82.76 200M\nDocFormerB\nASE (Appalaraju et al. 2021) Te\nxt + Layout + Image* 80.76 86.09 83.34 183M\nSelfDoc (Li\net al. 2021b) Te\nxt + Layout + Image* - -\n83.36 137M\nStrucTe\nxT (Li et al. 2021c) Te\nxt + Layout + Image* 85.68 80.97 83.09 107My\nBERTLARGE (Xu\net al. 2020) Te\nxt 61.13 70.85\n65.63 340M\nLayoutLMLARGE (Xu et\nal. 2021) Te\nxt + Layout 75.96 82.19\n78.95 343M\nBROSLARGE (Ours) Te\nxt + Layout 82.81\u00060.35 86.31\u00060.28 84.52\u00060.30 340M\nLayoutLMv2LARGE (Xu et\nal. 2021) Te\nxt + Layout + Image* 83.24 85.19\n84.20 426M\nStructuralLMLARGE (Li et\nal. 2021a) Te\nxt + Layout + Cell* 83.52 86.81 85.14 355M\nyThe number\nof parameters except for ResNet-FPN processing document images.\nTable\n3: Performance comparison on the FUNSD EE task. Bold indicates the best performance among models using only text\nand layout, and underline represents the best one. Image* and Cell* denote additional visual and hierarchical information,\nrespectively. Our methods are repeatedly evaluated ﬁve times and the values of other methods are the reported scores.\nEntity Extraction Entity Linking\nModel F S\nC F C\nSci\nBERTB\nASE 60.92 93.67\n93.13 27.65 92.83\n86.76\nLayoutLM\u0003\nBASE 78.54 95.11\n96.26 45.86 95.21\n99.05\nLayoutLMv2\u0003\nBASE\n81.89 96.09\n96.05 42.91 95.59\n98.19\nBROSBASE 83.05 96.28\n96.50 71.46 95.73\n99.45\nBERTLARGE 64.17 94.25\n94.74 29.11 94.31\n89.23\nLayoutLM\u0003\nLARGE\n79.27 95.36\n96.12 42.83 95.41\n99.33\nLayoutLMv2\u0003\nLARGE\n83.59 96.39\n97.24 70.57 97.29 99.76\nBR\nOSLARGE 84.52 96.62\n97.28 77.01 97.40 99.58\nTable 4: Performance comparisons on three EE and EL tasks\nwith the order information of text blocks.\nand LayoutLMv2 \u0003 show better performance than BERT\nsince they encode layout features as well as text features.\nAnd by combining visual features, LayoutLMv2 \u0003performs\nbetter than LayoutLM\u0003in most tasks. BROS shows the best\nperformance in all tasks except SciTSR. It should be noted\nthat the BROS BASE show better performance than that of\nLayoutLM\u0003\nLARGE, even though it uses three times lower num-\nber of parameters (110M vs 343M). These results indicate\nthat BROS effectively encodes the text and layout features.\nWithout the Order Information of Text BlocksAs we\nmentioned in previous section, we introduce the permuted\nKIE benchmarks lost the orders of text blocks by shufﬂing\nthe provided orders. To solve EE and EL tasks without the\norder information, we employ the SPADE decoder for all\ntasks. Table 5 shows the comparison results. p-F, p-S, p-\nC, and p-Sci refer to p-FUNSD, p-SROIE \u0003, p-CORD, and\np-SciTSR, respectively. BERT, which does not employ any\nspatial information of text blocks, shows the worst results\non the orderless conditions. By being aware of the spatial-\nEntity Extraction Entity Linking\nModel p-F p-S\np-C p-F p-C\np-Sci\nBERTB\nASE 18.85 39.73\n59.71 9.59 27.88\n1.75\nLayoutLM\u0003\nBASE 33.89 66.05\n80.86 22.98 61.51\n97.32\nLayoutLMv2\u0003\nBASE\n40.77 73.56\n80.37 23.25 50.55\n95.86\nBROSBASE 76.94 82.85\n95.86 69.61 87.72\n99.19\nBERTLARGE 18.10 43.19\n57.17 10.81 27.12\n1.93\nLayoutLM\u0003\nLARGE\n33.11 56.84\n82.88 20.72 61.98\n97.64\nLayoutLMv2\u0003\nLARGE\n62.53 84.92\n94.43 50.14 85.80 99.45\nBR\nOSLARGE 79.42 85.14\n96.81 75.61 90.49 99.33\nTable 5: Performance comparisons on three EE and EL tasks\nwithout the order information of text blocks.\nModel p- xy-\nyx- original\nLayoutLM\u0003\nBASE 33.89 34.02\n55.47 78.47\nLayoutLMv2\u0003\nBASE\n40.77 52.08\n62.37 78.16\nBROSBASE 76.94 77.16\n77.42 81.61\nLayoutLM\u0003\nLARGE\n33.11 33.54\n41.45 48.30\nLayoutLMv2\u0003\nLARGE\n62.53 69.14\n75.45 83.00\nBROSLARGE 79.42 79.91\n80.02 83.23\nTable 6: Comparison of FUNSD EE performances according\nto sorting methods.\nity, layout-aware language models show better performances\nthan BERT, and BROS achieves the best except p-SciTSR.\nMore interestingly, BROS shows minor performance drops\ncompared to Table 4, while LayoutLM\u0003and LayoutLMv2\u0003\nsuffer from huge performance degradations by losing the or-\nder information of text blocks.\nTo systematically investigate how the order information\naffects the performance of the models, we construct vari-\n10772\nFigure 6: Performance comparisons according to the amount\nof ﬁne-tuning data. Each point represents the result of ﬁne-\ntuning using from 10% to 100% of training data.\nDataset # Data BERT\nLayoutLM\u0003 LayoutLMv2\u0003 BROS\nFUNSD 5 31.51 48.23\n64.26 68.35\nEE 10 40.46 62.50\n69.92 72.60\nFUNSD 5 14.65 21.38\n7.32 31.11\nEL 10 14.88 21.48\n13.99 39.17\nTable 7: Results of training with 5 and 10 examples.\nants of FUNSD by re-ordering text blocks with two sort-\ning methods based on the top-left points. The text blocks\nof xy-FUNSD are sorted according to the x-axis with as-\ncending order of y-axis and those of yx-FUNSD are sorted\naccording to y-axis with ascending order of x-axis. Ta-\nble 6 shows performance on p-FUNSD, xy-FUNSD, yx-\nFUNSD, and the original FUNSD with the SPADE decoder.\nIn our experiment, LayoutLM \u0003\nLARGE achieves unstable per-\nformance (48.30±12.51), when combined with SPADE de-\ncoder. Interestingly, the performances of LayoutLM \u0003 and\nLayoutLMv2\u0003 are degraded in the order of FUNSD, yx-\nFUNSD, xy-FUNSD, and p-FUNSD as like the order of\nthe reasonable serialization for text on 2D space. On the\nother hand, the performance of BROS is relatively consis-\ntent. These results show the robustness of BROS on multiple\ntypes of serializers.\nLearning from Few Training ExamplesOne of the ad-\nvantages of pre-trained models is that it shows effective\ntransfer learning performance even with a few training ex-\namples (Devlin et al. 2019). Since collecting ﬁne-tuning data\nrequires a lot of resource, achieving high performance with\na small number of training examples is important.\nFigure 6 shows the results of the FUNSD KIE tasks by\nvarying the amount of training examples from 10% to 100%\nduring ﬁne-tuning. In all models, performances tend to in-\ncrease as the ratio of training data increased. In both tasks,\nBROS shows the best performances regardless of the num-\nber of training samples.\nTo further test extreme cases, we conduct experiments us-\ning only 5 and 10 training examples. Table 7 shows the re-\nsults of the FUNSD KIE tasks. We ﬁne-tune models for 100\nepochs with a batch size of 4. In all cases, BROS shows\nthe best performances. The results prove the generalization\nability of BROS even when there are very few training ex-\namples.\nEntity Extraction Entity Linking\nModel F S\nC F C\nSci\nLayoutLMy\nBASE 76.89 94.99\n94.37 44.00 93.60\n99.06\n→pos enc.\nonly 78.84 95.45\n96.36 59.92 94.83\n99.22\n→objectives only 78.44 94.81\n95.95 47.22 94.11\n99.20\n→both (= BROSB\nASE) 80.58 95.72\n96.64 65.24 96.03\n99.28\nTable 8: Performance improvements on EE and EL tasks\nthrough adding components of BROS. At the last line, all\ncomponents are changed from LayoutLM and the model be-\ncomes BROS.\nEntity Extraction Entity Linking\nSE F S\nC F C\nSci\nAbsolute 78.44 94.81\n95.95 47.22 94.11\n99.20\nLayoutLMv2’s 78.93 94.71\n95.82 53.57 95.27 99.28\nOurs 80.58 95.72\n96.64 65.24 96.03\n99.28\nTable 9: Spatial encoding methods from BROS’ setting.\nAblation Study\nWe conduct ablation studies to investigate which component\ncontributes the performance improvement. For the ablation\nstudies, we utilize LayoutLM y that is our own implemen-\ntation of LayoutLM for fair comparisons under the same\nexperimental settings. All models in these studies are pre-\ntrained for 1 epoch.\nTable 8 provides performance changes from adding our\nproposed components. When applying our proposed posi-\ntional encoding to LayoutLM, the performances consistently\nincrease with huge margins of 3.62pp on average over all\ntasks. Independently, our extension on pre-training objec-\ntives solely provides 1.14pp of performance improvement\non average. By utilizing both, BROS BASE provides the best\nperformances with margins of 5.10pp on average. This abla-\ntion study proves that each component of BROS solely con-\ntributes to performance improvements as well as their com-\nbination provides better results.\nTable 9 compares three positional encoding methods:\nabsolute position in LayoutLM, relative position in Lay-\noutLMv2, and ours. Relative position methods perform bet-\nter than absolute one and the performance gap becomes\nlarger in EL tasks. And among them, our method shows the\nbest results.\nConclusion\nWe propose a pre-trained language model, BROS, which\nfocuses on modeling text and layout features for effective\nkey information extraction from documents. By encoding\ntexts in 2D space with their relative positions and pre-\ntraining the model with the area-masking strategy, BROS\nshows superior performance without relying on any addi-\ntional visual features. In addition, under the two real-world\nsettings–imprecise text serialization and small amount of\ntraining examples–BROS shows robust performance while\nother models show signiﬁcant performance degradation.\n10773\nAcknowledgments\nWe thank many colleagues at NA VER CLOV A for their help,\nin particular Yoonsik Kim, Moonbin Yim, Han-cheol Cho,\nBado Lee, Seunghyun Park, and Youngmin Baek for useful\ndiscussions.\nReferences\nAppalaraju, S.; Jasani, B.; Kota, B. U.; Xie, Y .; and\nManmatha, R. 2021. DocFormer: End-to-End Trans-\nformer for Document Understanding. arXiv preprint\narXiv:2106.11539.\nChi, Z.; Huang, H.; Xu, H.-D.; Yu, H.; Yin, W.; and Mao,\nX.-L. 2019. Complicated table structure recognition. arXiv\npreprint arXiv:1908.04729.\nClausner, C.; Pletschacher, S.; and Antonacopoulos, A.\n2013. The signiﬁcance of reading order in document recog-\nnition and its evaluation. In 2013 12th International Con-\nference on Document Analysis and Recognition (ICDAR),\n688–692. IEEE.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J. G.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics (ACL).\nDenk, T. I.; and Reisswig, C. 2019. BERTgrid: Contex-\ntualized Embedding for 2D Document Representation and\nUnderstanding. In Workshop on Document Intelligence at\nNeurIPS 2019.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies (NAACL-HLT), Volume 1 (Long and Short Papers),\n4171–4186.\nHarley, A. W.; Ufkes, A.; and Derpanis, K. G. 2015. Eval-\nuation of deep convolutional nets for document image clas-\nsiﬁcation and retrieval. In Proceedings of the 13th Interna-\ntional Conference on Document Analysis and Recognition\n(ICDAR), 991–995.\nHuang, Z.; Chen, K.; He, J.; Bai, X.; Karatzas, D.; Lu, S.;\nand Jawahar, C. 2019. ICDAR2019 competition on scanned\nreceipt ocr and information extraction. InProceedings of the\n15th International Conference on Document Analysis and\nRecognition (ICDAR), 1516–1520. IEEE.\nHwang, W.; Kim, S.; Seo, M.; Yim, J.; Park, S.; Park, S.;\nLee, J.; Lee, B.; and Lee, H. 2019. Post-OCR parsing: build-\ning simple and robust parser via BIO tagging. In Workshop\non Document Intelligence at NeurIPS 2019.\nHwang, W.; Yim, J.; Park, S.; Yang, S.; and Seo, M. 2021.\nSpatial Dependency Parsing for Semi-Structured Document\nInformation Extraction. In Findings of the Association for\nComputational Linguistics: ACL-IJCNLP 2021, 330–343.\nJaume, G.; Ekenel, H. K.; and Thiran, J.-P. 2019. FUNSD:\nA dataset for form understanding in noisy scanned doc-\numents. In 2019 International Conference on Document\nAnalysis and Recognition Workshops (ICDARW), volume 2,\n1–6. IEEE.\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\nand Levy, O. 2020. SpanBERT: Improving pre-training by\nrepresenting and predicting spans. Transactions of the Asso-\nciation for Computational Linguistics (TACL), 8: 64–77.\nLewis, D.; Agam, G.; Argamon, S.; Frieder, O.; Grossman,\nD.; and Heard, J. 2006. Building a test collection for com-\nplex document information processing. In Proceedings of\nthe 29th annual international ACM SIGIR conference on\nResearch and development in information retrieval (SIGIR),\n665–666.\nLi, C.; Bi, B.; Yan, M.; Wang, W.; Huang, S.; Huang, F.; and\nSi, L. 2021a. StructuralLM: Structural Pre-training for Form\nUnderstanding. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language\nProcessing (ACL-IJCNLP), 6309–6318.\nLi, L.; Gao, F.; Bu, J.; Wang, Y .; Yu, Z.; and Zheng, Q. 2020.\nAn End-to-End OCR Text Re-organization Sequence Learn-\ning for Rich-text Detail Image Comprehension. In Proceed-\nings of the 16th European Conference on Computer Vision\n(ECCV).\nLi, P.; Gu, J.; Kuen, J.; Morariu, V . I.; Zhao, H.; Jain,\nR.; Manjunatha, V .; and Liu, H. 2021b. SelfDoc: Self-\nSupervised Document Representation Learning. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 5652–5660.\nLi, Y .; Qian, Y .; Yu, Y .; Qin, X.; Zhang, C.; Liu, Y .; Yao, K.;\nHan, J.; Liu, J.; and Ding, E. 2021c. StrucTexT: Structured\nText Understanding with Multi-Modal Transformers. arXiv\npreprint arXiv:2108.02923.\nLiu, X.; Gao, F.; Zhang, Q.; and Zhao, H. 2019. Graph Con-\nvolution for Multimodal Information Extraction from Visu-\nally Rich Documents. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies\n(NAACL-HLT), Volume 2 (Industry Papers), 32–39.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In Proceedings of the 7th International\nConference on Learning Representations (ICLR).\nPark, S.; Shin, S.; Lee, B.; Lee, J.; Surh, J.; Seo, M.; and\nLee, H. 2019. CORD: A Consolidated Receipt Dataset for\nPost-OCR Parsing. In Workshop on Document Intelligence\nat NeurIPS 2019.\nPowalski, R.; Borchmann, Ł.; Jurkiewicz, D.; Dwojak, T.;\nPietruszka, M.; and Pałka, G. 2021. Going Full-TILT Boo-\ngie on Document Understanding with Text-Image-Layout\nTransformer. arXiv preprint arXiv:2102.09550.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems 30 (NeurIPS), 5998–6008.\nWang, Z.; Xu, Y .; Cui, L.; Shang, J.; and Wei, F. 2021. Lay-\noutReader: Pre-training of Text and Layout for Reading Or-\nder Detection. arXiv:2108.11591.\n10774\nXu, Y .; Li, M.; Cui, L.; Huang, S.; Wei, F.; and Zhou, M.\n2020. LayoutLM: Pre-training of text and layout for docu-\nment image understanding. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discov-\nery & Data Mining (KDD), 1192–1200.\nXu, Y .; Xu, Y .; Lv, T.; Cui, L.; Wei, F.; Wang, G.; Lu, Y .; Flo-\nrencio, D.; Zhang, C.; Che, W.; et al. 2021. LayoutLMv2:\nMulti-modal Pre-training for Visually-Rich Document Un-\nderstanding. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (ACL-IJCNLP), 2579–2591.\n10775",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8201348781585693
    },
    {
      "name": "Key (lock)",
      "score": 0.6823083758354187
    },
    {
      "name": "Natural language processing",
      "score": 0.5903738141059875
    },
    {
      "name": "Task (project management)",
      "score": 0.568958044052124
    },
    {
      "name": "Space (punctuation)",
      "score": 0.5563197731971741
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5527620315551758
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5389602184295654
    },
    {
      "name": "Information extraction",
      "score": 0.5196320414543152
    },
    {
      "name": "Language model",
      "score": 0.49381765723228455
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.47943514585494995
    },
    {
      "name": "Information retrieval",
      "score": 0.42159950733184814
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}