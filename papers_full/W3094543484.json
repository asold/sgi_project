{
    "title": "Emformer: Efficient Memory Transformer Based Acoustic Model For Low Latency Streaming Speech Recognition",
    "url": "https://openalex.org/W3094543484",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2354921753",
            "name": "Shi, Yangyang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1928402660",
            "name": "Wang Yong-qiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114495019",
            "name": "Wu Chunyang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4214337936",
            "name": "Yeh, Ching-Feng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742524601",
            "name": "Chan, Julian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2269774791",
            "name": "Zhang, Frank",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100217444",
            "name": "Le Duc",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4303495752",
            "name": "Seltzer, Mike",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2972818416",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2981857663",
        "https://openalex.org/W2795138957",
        "https://openalex.org/W1524333225",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2802023636",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2979286696",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3015960524",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W3097558625",
        "https://openalex.org/W3097075707",
        "https://openalex.org/W3021515889",
        "https://openalex.org/W3161873870",
        "https://openalex.org/W2911291251",
        "https://openalex.org/W2982413405",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W3025165719",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2913718171",
        "https://openalex.org/W2998814410",
        "https://openalex.org/W3016010032",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3097973766",
        "https://openalex.org/W2798657914",
        "https://openalex.org/W2407080277",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W3096888553"
    ],
    "abstract": "This paper proposes an efficient memory transformer Emformer for low latency streaming speech recognition. In Emformer, the long-range history context is distilled into an augmented memory bank to reduce self-attention's computation complexity. A cache mechanism saves the computation for the key and value in self-attention for the left context. Emformer applies a parallelized block processing in training to support low latency models. We carry out experiments on benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets WER $2.50\\%$ on test-clean and $5.62\\%$ on test-other. Comparing with a strong baseline augmented memory transformer (AM-TRF), Emformer gets $4.6$ folds training speedup and $18\\%$ relative real-time factor (RTF) reduction in decoding with relative WER reduction $17\\%$ on test-clean and $9\\%$ on test-other. For a low latency scenario with an average latency of 80 ms, Emformer achieves WER $3.01\\%$ on test-clean and $7.09\\%$ on test-other. Comparing with the LSTM baseline with the same latency and model size, Emformer gets relative WER reduction $9\\%$ and $16\\%$ on test-clean and test-other, respectively.",
    "full_text": "EMFORMER: EFFICIENT MEMORY TRANSFORMER BASED ACOUSTIC MODEL FOR\nLOW LATENCY STREAMING SPEECH RECOGNITION\nYangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan,\nFrank Zhang, Duc Le, Mike Seltzer\nFacebook AI\nABSTRACT\nThis paper proposes an efﬁcient memory transformer Emformerfor\nlow latency streaming speech recognition. In Emformer, the long-\nrange history context is distilled into an augmented memory bank to\nreduce self-attention’s computation complexity. A cache mechanism\nsaves the computation for the key and value in self-attention for the\nleft context. Emformer applies a parallelized block processing in\ntraining to support low latency models. We carry out experiments\non benchmark LibriSpeech data. Under average latency of 960 ms,\nEmformer gets WER 2.50% on test-clean and 5.62% on test-other.\nComparing with a strong baseline augmented memory transformer\n(AM-TRF), Emformer gets 4.6 folds training speedup and 18% rel-\native real-time factor (RTF) reduction in decoding with relative WER\nreduction 17% on test-clean and 9% on test-other. For a low latency\nscenario with an average latency of 80 ms, Emformer achieves WER\n3.01% on test-clean and 7.09% on test-other. Comparing with the\nLSTM baseline with the same latency and model size, Emformer\ngets relative WER reduction 9% and 16% on test-clean and test-\nother, respectively.\nIndex Terms— Low Latency, Transformer, Emformer\n1. INTRODUCTION\nTransformers [1] have achieved dominated performance for various\ntasks in natural language processing area [2, 3, 4]. Rather than using\nmemory state to capture long-range dependencies in recurrent neu-\nral networks, the multi-head self-attention method connects arbitrary\npositions in the whole sequence directly in parallel.\nRecently, transformer-based model architectures have also\nbeen successfully applied to automatic speech recognition (ASR)\narea across various modeling paradigms, including sequence-to-\nsequence [5, 6, 7, 8, 9], neural transducer [10, 11, 12], Connectionist\ntemporal classiﬁcation (CTC) [13, 14] and traditional hybrid [15, 16]\nsystems.\nUnlike most natural language processing tasks, many ASR\napplications deal with streaming scenarios challenging for vanilla\ntransformers. The streaming recognizer needs to produce output\ngiven partially available speech utterance rather than entire ut-\nterance. Several methods advance the transformer for streaming\nspeech recognition. The work [15, 10, 17] proposed to constrain\nthe attention computation with a limited length of look-ahead in-\nputs. However, these methods have a signiﬁcant delay due to the\nlook-ahead context leaking issue where essential look-ahead context\ngrows linearly with the number of transformer layers stacking on top\nof one another. A scout network is proposed in [9] to detect the word\nboundary. In scout networks, only the context information before\nthe word boundary is used by the transformer to make predictions.\nHowever, the scout network does not address the heavy self-attention\ncomputation that grows quadratically with the left context length. A\nstreaming transformer with augmented memory (AM-TRF) is pro-\nposed in [18] to reduce latency and the self-attention computation.\nAM-TRF uses a similar block processing method as [19]. The\nblock processing chunks the whole utterance into multiple segments.\nTo reduce the computation in capturing the long-range left context,\nAM-TRF introduces a memory bank. Each vector in the memory\nbank is an abstract embedding from the previous one segment. The\ndirect left context block from the current segment and look-ahead\ncontext block provides context information for current segment\nrecognition in addition to the memory bank. However, AM-TRF\nhas duplicated computations for the direct left context block in both\ntraining and decoding. The memory bank carries over the context\ninformation from previous segments in a similar auto-regression\nway as recurrent neural networks. The inherent auto-regression\ncharacteristic makes AM-TRF challenging to parallelize the block\nprocessing in training.\nIn this paper, we propose the Emformer that improves the AM-\nTRF from the following aspects. First, Emformer removes the du-\nplicated computation from the left context block by caching the key\nand value in previous segments’ self-attention. Second, rather than\npassing the memory bank within the current layer in AM-TRF, in-\nspired by transformer-xl [2] and its applicatin in speech recogni-\ntion [20], Emformer carries over the memory bank from the lower\nlayer. Third, Emformer disables the summary vector’s attention with\nmemory bank to avoid overweighting the most left part of context in-\nformation. Finally, Emformer applies a parallelized block process-\ning training method, which is important to train Emformer for low\nlatency speech recognition.\nTo verify the performance of the proposed method, we carry out\nexperiments on LibriSpeech [21]. More experiments using indus-\ntry dataset with variant scenarios are in [22]. Under the average\nlatency of 640 ms constraint, comparing with AM-TRF, Emformer\ngets relative WER reduction17% on test-clean and 9% on test-other.\nMeanwhile, Emformer reduces the training time by almost 80% and\ndecoding RTF by 18%. For a low latency scenario with an aver-\nage latency of 80 ms, Emformer saves more than 91% computation\nfrom AM-TRF and obtain WER 3.01% on test-clean and 7.09% on\ntest-other. According to our knowledge, this is the ﬁrst work to give\nstreaming transformer results on LibriSpeech with such low latency.\nUnder the average latency of 960 ms and 640 ms constraint, Em-\nformer also gives the best result on LibriSpeech so far.\n2. EMFORMER\nEmformer improves over the AM-TRF. The following subsection\ngives a short introduction to AM-TRF.\n2.1. AM-TRF\nFigure (1a) illustrates the operations in one AM-TRF layer. A\nsequence of input feature vectors are chunked into multiple non-\narXiv:2010.10759v4  [cs.SD]  30 Dec 2020\n(a) AM-TRF\n (b) Emformer\nFig. 1: Comparison of AM-TRF with Emformer\noverlapping segments Cn\n1 , ··· , Cn\nI−1, where the i denotes the index\nof segment, and n the layer’s index. In order to reduce boundary\neffect, left and right contextual blocks,Ln\ni and Rn\ni , are concatenated\nwith Cn\ni to form a contextual segment Xn\ni = [Ln\ni , Cn\ni , Rn\ni ]. At\nthe i-th segment, the n-th AM-TRF layer accepts Xn\ni and a bank of\nmemory vector Mn\ni = [mn\n1 , ··· , mn\ni−1] as the input, and produces\nXn+1\ni = [Ln+1\ni , Cn+1\ni , Rn+1\ni ] and mn\ni as the output, whereas\nXn+1\ni is feed to the next layer and mn\ni is inserted into the mem-\nory bank to generate Mn\ni+1 and carried over to the next segment.\nAfter all the AM-TRF layers, the center blocks {CN−1\ni }I−1\ni=0 are\nconcatenated as the encoder output sequence; the contextual blocks\n{LN−1\ni }I−1\ni=0 and {RN−1\ni }I−1\ni=0 are discarded.\nAt the core of each AM-TRF layer, there is a modiﬁed atten-\ntion mechanism which attends to the memory bank and yields a new\nmemory vector at each segment:\nˆXn\ni =LayerNorm(Xn\ni ) (1)\nKn\ni =Wk[Mn\ni , ˆXn\ni ], (2)\nVn\ni =Wv[Mn\ni , ˆXn\ni ], (3)\n[Zn\nL,i, Zn\nC,i, Zn\nR,i] =Attn(Wq ˆXn\ni , Kn\ni , Vn\ni ) +Xn\ni (4)\nmn\ni =Attn(Wqsn\ni , Kn\ni , Vn\ni ) (5)\nwhereas Zn\nL,i, Zn\nC,i and Zn\nR,i are the attention output forLn\ni , Cn\ni and\nRn\ni respectively; sn\ni is the mean of center block Cn\ni ; Attn(q; k, v)\nis the attention operation deﬁned in [1] with q , k and v being the\nquery, key and value, respectively.\nZn\nL,i, Zn\nC,i, Zn\nR,i are passed to a point-wise feed-forward net-\nwork (FFN) with layer normalization and residual connection to gen-\nerate the output of this AM-TRF layer, i.e.,\nˆXn\ni+1 = FFN(LayerNorm([Zn\nL,i, Zn\nC,i, Zn\nR,i])) (6)\nXn+1\ni = LayerNorm(ˆXn+1\ni + [Zn\nL,i, Zn\nC,i, Zn\nR,i]) (7)\nwhere FNN is a two-layer feed-forward network with Relu non-\nlinearity. The last layer normalization in Eq. (7) is used to prevent a\npath to bypass all the AM-TRF layers.\n2.2. Emformer\nAs shown in [18], given the similar latency constraint, AM-TRF\nhas outperformed previous streaming transformer models. However,\nthere are several issues with AM-TRF. The usage of the left con-\ntext is not efﬁcient. AM-TRF training relies on the sequential block\nprocessing that is not suitable for low latency model training. Hav-\ning observed these limitations, we proposed a new streamable trans-\nformer architecture, namely, Emformer. One layer of Emformer is\ndemonstrated in Figure (1b). The following subsections describe the\nimportant improvements made in Emformer.\n2.2.1. Cache key and value from previous segments\nAs illustrated in Figure (1a), for the i-th segment, the embedding\nof the left context Ln\ni needs to be re-computed for every step, even\nthough Ln\ni is overlapped with Cn\ni−1 (or possibly even more previous\ncenter blocks). Thus we only need to cache the projections from\nthe previous segments. As shown in Figure (1b), Emformer only\ncomputes the key, value projections for the memory bank, center, and\nright context; Emformer saves the computation of query projection\nof left context, as it does not need to give output from the left context\nblock for the next layer. Compared with AM-TRF, the attention part\nin Emformer operates in the following sequence:\n[ ˆCn\ni , ˆRn\ni ] = LayerNorm([Cn\ni , Rn\ni ]) (8)\nKn\ni = [WkMn\ni , Kn\nL,i, WkCn\ni , WkRn\ni ], (9)\nVn\ni = [WvMn\ni , Vn\nL,i, WvCn\ni , WvRn\ni ], (10)\nZn\nC,i = Attn(Wq ˆCn\ni , Kn\ni , Vn\ni ) +Cn\ni (11)\nZn\nR,i = Attn(Wq ˆRn\ni , Kn\ni , Vn\ni ) +Rn\ni (12)\nmn\ni = Attn(Wqsn\ni ; Kn\ni , Vn\ni ) (13)\nwhere Kn\nL,i and Vn\nL,i are the key and value copies from previous\nsegments with no additional computations.\nLet’s assume L, C, R, and M are the lengths for the left context\nblock, the center context, the right context, and the memory bank.\nthe number of heads in the multi-head self-attention is h and per head\ndimension is d. Note the summary vector is the mean of the center\nsegment, of which length is always 1. In practice, the memory bank\nis implemented in ring buffer way with small length, and the model\ndimension, dh, is much larger than any of L, C, R, and M. Emformer\nsaves approximately L\nL+C+R of AM-TRF computation. For low la-\ntency scenario with center context length 80 ms, right context length\n40 ms, and left context length 1280 ms, Emformer reduces more than\n91% computation from AM-TRF.\n2.2.2. Carryover memory vector from previous segments in the\nlower layer\nThe attention output from the summary vector sn\ni is a memory vec-\ntor in the memory bank. The memory bank carries all the previous\ncontext information for future segments. As we can see from Fig-\nure (1a), the memory vector mn\ni from the i-th segment in the n-th\nlayer is a prerequisite for the(i+1)-th segment from the same layer.\nIn training, the auto-regression characteristic of AM-TRF forces the\nblock processing to be in a sequential way that is not suitable for\nGPU computing. Especially for low latency model training, where\nthe center segment is small, sequential block processing chunks the\nwhole utterance computation into a small computation loop, which\nrenders extremely low GPU usage.\nTo support parallelization for block processing training, Em-\nformer takes the memory bank input from previous segments in the\nlower layer rather than the same layer. In this way, for each Em-\nformer layer, the whole sequence is trained in parallel, fully taking\nadvantage of the GPU computing resources.\n2.2.3. Disallow attention between the summary vector with the\nmemory bank\nAccording to Eq. (5), the memory vector is a weighted interpolation\nof values projected from the memory bank, the left context block,\nthe center block, and the right context block. For both AM-TRF and\nEmformer, assigning the attention weight between the summary vec-\ntor and the memory bank to zero stabilizes the training and improves\nrecognition accuracy for long-form speech. Including the memory\nbank information in the current memory vector cause the most left\ncontext information over-weighted. Similar to a recurrent neural net-\nwork, enable the connection of summary vector with the memory\nback could cause gradient vanishing or explosion. For AM-TRF, the\nusage of the weak-attention suppression method [18, 23] partially\naddresses the problem by setting weak-attention weights to zero.\n2.2.4. Deal with look-ahead context leaking\nThe sequential block processing in AM-TRF training chunks the in-\nput sequence physically. The right context size bounds the look-\nahead reception ﬁeld. However, sequentially processing blocks sig-\nniﬁcantly slows the training. Now Emformer processes the input se-\nFig. 2: Illustration of avoiding look-ahead context leaking. The\nchunk size is 4. The right context size is 1.\nquence in a fully parallel manner in the training stage. Like [2, 24],\nEmformer applies attention masks to limit the reception ﬁeld in each\nlayer without physically chunking the input sequence. However, this\nmethod has the risk of a look-ahead of context leaking. The essen-\ntial right context size grows when multiple transformer layers stack\non top of one another. To deal with the look-ahead context leaking\nissue in training, Emformer makes a hard copy of each segment’s\nlook-ahead context and puts the look-ahead context copy at the in-\nput sequence’s beginning as illustrated in the right part of Figure 2.\nFor example, the output at the frame 2 in the ﬁrst chunk only use the\ninformation from the current chunk together with the right context\nframe 4 without right context leaking.\n3. EXPERIMENTS\n3.1. Data and Setup\nWe verify the proposed method on the LibriSpeech corpus [21]. Lib-\nriSpeech has 1000 hours of book reading utterances derived from the\nLibriV ox project. There are two subsets of development data and\nevaluation data in LibriSpeech. The “clean” subsets contain sim-\nple and clean utterances. The “other” subset contains complex and\nnoisy utterances. Based on the WER on the dev data, we select the\nbest model and report its WER on test data. In the experiments, Em-\nformer is used as an encoder for both the hybrid [16, 18, 14] and\ntransducer [10, 11, 12] models.\n3.1.1. Hybrid model\nThe context and positional dependent graphemes are used as output\nunits [25]. We use the standard Kaldi [26] LibriSpeech recipe to\nbuild bootstrap the HMM-GMM system. The 80-dimensional log\nMel ﬁlter bank features at a 10 ms frame rate are used. We also\napply speed perturbation [27] and SpecAugment [28] without time\nwarping to stabilize the training.\nA linear layer maps the 80-dimensional features to 128 dimen-\nsion vectors. Four continuous 128-dimensional vectors are concate-\nnated with stride 4 to form a 512 vector that is the input to Emformer.\nIn Emformer, each layer has eight heads of self-attention. The input\nand output for each layer have 512 nodes. The inner-layer of FFN\nhas dimensionality 2048. Dropout is 0.1 for all layers across all ex-\nperiments. For medium latency, memory bank length is 4. For low\nlatency experiments where the segment size is small, memory bank\ninformation largely overlaps with direct left context. Therefore, we\nset the memory bank length to 0. An auxiliary incremental loss [29]\nwith weight 0.3 is used to overcome the training divergence issue\nfor deep transformer models. All hybrid models are trained with the\nadam optimizer [30] using 180 epochs. The learning rate increases to\n1e-3 in 20K warming-up updates. Then it is ﬁxed until 100 epochs.\nFrom then on, the learning rate shrinks every epoch with factor 0.95.\nAll the models are trained using 32 Nvidia V100 GPUs with fp16\nprecision. We use hosts with Intel Xeon D-2191A 18-core CPUs to\nmeasure real time factors (RTFs). In measuring RTFs, 10 utterances\nare concurrently decoded.\n3.1.2. Transducer model\nThe output units are 1024 sentence pieces [31] with byte pair en-\ncoding (BPE) [32] as the segmentation algorithm. In the predictor,\nthe tokens are ﬁrst represented by 256-dimensional embeddings be-\nfore going through two LSTM layers with 512 hidden nodes, fol-\nlowed by a linear projection to 640-dimensional features before the\njoiner. For the joiner, the combined embeddings from the encoder\nand the predictor ﬁrst go through a Tanh activation and then another\nlinear projection to the target number of sentence pieces. Both the\nLCBLSTM and Emformer encoders are pre-trained from the hybrid\nsystems. Similar to [12], we use a neural network language model\n(NNLM) for shallow fusion during beam search where the weight\nfor NNLM probabilities was 0.3 across experiments. The training\ndata for NNLM is the combined transcripts of the train set and the\n800M text-only set.\n3.2. Results\n3.2.1. Algorithmic latency induced by the encoder (EIL)\nIn block processing based decoding, the latency comes from the cen-\nter block size and the look-ahead context size. For the most left\nframe in the center block, the latency is the center block size plus\nlook-ahead context size. The latency for the most right frame in the\ncenter block is look-ahead context size. Therefore, we use algorith-\nmic latency induced by the encoder (EIL), an average latency of all\nthe frames in the center block, which equals to the look-ahead con-\ntext latency plus center block latency discounted by 0.5.\n3.2.2. From AM-TRF to Emformer\nTable 1 gives a performance comparison of AM-TRF with Emformer\nwith a latency of 960 ms. Caching the key and value computation\nspeeds up the training from 1.14 hours per epoch to 0.5 hours per\nepoch and decoding from RTF (real-time factor) 0.19 to 0.17. The\nleft context caching also reduces the redundant gradient in training\nthat results in some WER reduction 1. Finally, using all improve-\nments, comparing with AM-TRF, Emformer speeds up the training\nby 4.6 folds. Emformer also gets relative WER reduction 17% on\ntest-clean, 9% on test-other and 18% relative RTF reduction in de-\ncoding. For a low latency scenario, Emformer saves up to 91% of\ncomputations from AM-TRF without considering parallel block pro-\ncessing. It is impractical to train AM-TRF for a low latency scenario.\nTherefore we ignore the detailed comparison.\nModel RTF test train hours\nclean other per epoch\nAM-TRF-24L 0.16 3.27 6.66 1.14h\n+ left context caching 0.13 2.88 6.44 0.50h\nEM-24L 0.13 2.72 6.01 0.25h\nTable 1: From AM-TRF to Emformer based on hybrid systems. All\nmodels have 80M parameters. Left context size, center block size\nand right context size are 640 ms, 1280 ms and 320 ms, respectively.\n3.2.3. Results from hybrid systems\nModel LC size Center Size test RTFclean other\nLCBLSTM – 1280 2.90 6.76 0.25\n– 640 2.96 6.97 0.27\nEM-24L\n320\n1280\n2.75 6.08 0.13\n640 2.72 6.01 0.13\n1280 2.59 5.90 0.13\n320\n640\n2.80 6.47 0.13\n640 2.78 6.46 0.13\n1280 2.76 6.59 0.15\nEM-36L 1280 1280 2.58 5.75 0.17\n+smbr 2.50 5.62 0.17\nEM-36L 1280 640 2.69 6.14 0.20\n+smbr 2.62 5.97 0.19\nTable 2: Impact of left context (LC) size (in millisecond) on WER\nand RTF under medium latency constraints for hybrid models. Look-\nahead size is 320 ms, the EIL is 640 ms or 960 ms when center size\nis 640 ms and 1280 ms, respectively. Both LCBLSTM and EM-24L\nhave the similar 80M parameters. EM-36L has 120M parameters.\nTable 2 and Table 3 presents the performance of the Emformer\nbased hybrid systems for medium latency and low latency, respec-\ntively. For both tables, larger left context size gives better WER\nand slightly worse decoding RTF. In Table 2, LCBLSTM consists\nof 5 layers with 800 nodes in each layer each direction. Using a\nsimilar model size and latency constraint, Emformer gets a relative\n48% RTF deduction. Under EIL 1280 ms, Emformer obtained over\nrelative 12% WER reduction over LCBLSTM on both test-clean\nand test-other datasets. Together with sMBR training [33], the Em-\nformer with 120M parameters achieves WER 2.50% on test-clean\n1For large datasets, the caching strategy does not give WER reduction.\nand 5.62% on test-other under EIL 960 ms, and 2.62% on test-clean\nand 5.97% on test-other under EIL 640 ms.\nIn Table 3, the LSTM consists of 7 layers with 1200 nodes in\neach layer. The input to LSTM is a concatenation of the current\nframe with eight look-ahead context frames. Low latency speech\nrecognition gives higher RTF than medium latency speech recog-\nnition. Because medium latency speech recognition chunks an ut-\nterance into fewer larger segments, it speeds up the neural network’s\ncomputation. Using a similar model size and latency constraint, Em-\nformer gets relative WER reduction 9% and 15% on test-clean and\ntest-other, respectively. Together with sMBR training [33], the 36\nlayer Emformer achieves WER 3.01% on test-clean and 7.09% on\ntest-other. According to our knowledge, for low latency 80 ms, Em-\nformer gives the best WER on LibriSpeech data.\nModel LC size latency test RTFin milliseconds clean other\nLSTM – 80 3.75 9.18 0.25\nEM-24L\n320\n80\n3.44 8.37 0.30\n640 3.37 8.05 0.31\n1280 3.41 7.75 0.33\nEM-36L 1280 80 3.32 7.56 0.49\n+smbr 3.01 7.09 0.49\nTable 3: Impact of left context (LC) size (in millisecond) on word\nerror rate and RTF under a low latency constraint for hybrid models.\nThe look-ahead and center context size are 40 ms and 80 ms, respec-\ntively. Latency is deﬁned by encoder induced latency (EIL). Both\nLSTM and EM-24L have the similar 80M parameters. EM-36L has\n120M parameters.\n3.2.4. Results from transducer systems\nTable 4 summarizes the comparison between LCBLSTM and Em-\nformer as encoders in the transducer system. Similar to the previous\nobservations with hybrid systems, we see that given the same EIL\n(640 ms), Emformer consistently outperforms LCBLSTM on WER.\nWith the external NNLM, the transducer systems achieved similar\nWER to those from hybrid systems.\nModel NNLM test\nclean other\nLCBLSTM \u0017 3.04 8.25\n\u0013 2.65 7.26\nEM-24L \u0017 2.78 6.92\n\u0013 2.37 6.07\nTable 4: WER of Emformer with the neural transducers. Both mod-\nels use an EIL 640 ms with center context 640 ms and look-ahead\ncontext 320 ms. Left context size is 1280 ms.\n4. CONCLUSIONS\nThe proposed Emformer applied a cache strategy to remove the du-\nplicated computation in augmented memory transformer (AM-TRF)\nfor the left context. Emformer disabled the summary vector atten-\ntion with a memory bank to stabilize the training. By redeﬁning the\nmemory carryover procedure and avoiding the right context leaking,\nEmformer supported parallelized block processing in training. Com-\nparing with AM-TRF, Emformer got 4.6 folds of training speedup\nand 18% decoding RTF reduction. Experiments on LibriSpeech\nshowed that Emformer outperformed the baselines in both hybrid\nand transducer systems. Under average latency EIL 960 ms, Em-\nformer achieved WER 2.50% on test-clean and 5.62% on test-other\nwith decoding RTF 0.13. Under low latency 80 ms constraint, Em-\nformer achieved WER 3.01% on test-clean and 7.09% on test-other.\n5. REFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all\nyou need,” in NIPS, 2017.\n[2] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Quoc V . Le, and\nR. Salakhutdinov, “Transformer-XL: Attentive language mod-\nels beyond a ﬁxed-length context,”ACL, pp. 2978–2988, 2019.\n[3] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,” NAACL, vol. 1, pp. 4171–4186, 2019.\n[4] C. Raffel, N. Shazeer, A. Roberts, and Others, “Exploring the\nLimits of Transfer Learning with a Uniﬁed Text-to-Text Trans-\nformer,” arXiv preprint arXiv:1910.10683, 2019.\n[5] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-\nrecurrence sequence-to-sequence model for speech recogni-\ntion,” in ICASSP, 2018.\n[6] S. Karita, N. Chen, T. Hayashi, and Others, “A Comparative\nStudy on Transformer vs RNN in Speech Applications,” arXiv\npreprint arXiv:1909.06317, 2019.\n[7] M. Sperber, J. Niehues, G. Neubig, and Others,\n“Self-attentional acoustic models,” arXiv preprint\narXiv:1803.09519, 2018.\n[8] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequence-\nto-sequence speech recognition with the transformer in man-\ndarin Chinese,” arXiv preprint arXiv:1804.10752, 2018.\n[9] C. Wang, Y . Wu, S. Liu, J. Li, et al., “Low Latency End-to-End\nStreaming Speech Recognition with a Scout Network,” arXiv\npreprint arXiv:12003.10369, 2020.\n[10] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo,\nand S. Kumar, “Transformer Transducer: A Streamable Speech\nRecognition Model with Transformer Encoders and RNN-T\nLoss,” ICASSP, vol. 2020-May, pp. 7829–7833, 2020.\n[11] C.-F. Yeh, J. Mahadeokar, and Others, “Transformer-\nTransducer: End-to-End Speech Recognition with Self-\nAttention,” arXiv preprint arXiv:11910.12977, 2019.\n[12] A. Gulati, J. Qin, C.-C. Chiu, et al., “Conformer: Convolution-\naugmented Transformer for Speech Recognition,” arXiv\npreprint arXiv:12005.08100, 2020.\n[13] J. Salazar, K. Kirchhoff, and Z. Huang, “Self-Attention Net-\nworks for Connectionist Temporal Classiﬁcation in Speech\nRecognition,” in Proceedings of ICASSP, 2019.\n[14] F. Zhang, Y . Wang, X. Zhang, C. Liu, et al., “Fast, Simpler\nand More Accurate Hybrid ASR Systems Using Wordpieces,”\nInterSpeech, 2020.\n[15] D. Povey, H. Hadian, P. Ghahremani, and Others, “A time-\nrestricted self-attention layer for asr,” in Proc. ICASSP, 2018,\npp. 5874–5878.\n[16] Y . Wang, A. Mohamed, D. Le, and Others, “Transformer-\nBased Acoustic Modeling for Hybrid Speech Recognition,”\nICASSP, 2019.\n[17] N. Moritz, T. Hori, and J. L. Roux, “Streaming automatic\nspeech recognition with the transformer model,”arXiv preprint\narXiv:2001.02674, 2020.\n[18] C. Wu, Y . Shi, Y . Wang, and C.-F. Yeh, “Streaming\nTransformer-based Acoustic Modeling Using Self-attention\nwith Augmented Memory,” in InterSpeech, 2020.\n[19] L. Dong, F. Wang, and B. Xu, “Self-attention aligner: A\nlatency-control end-to-end model for asr using self-attention\nnetwork and chunk-hopping,” ICASSP, pp. 5656–5660, 2019.\n[20] L. Lu, C. Liu, J. Li, and Y . Gong, “Exploring Transformers for\nLarge-Scale Speech Recognition,” in InterSpeech, 2020.\n[21] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: An ASR corpus based on public domain audio\nbooks,” in ICASSP, 2015.\n[22] Y . Wang, Y . Shi, F. Zhang, C. Wu, and Others, “Transformer\nin action: a comparative study of transformer-based acous-\ntic model for large scale speech recognition applications,”\nhttps://arxiv.org/abs/2010.14665, 2020.\n[23] Y . Shi, Y . Wang, C. Wu, C. Fuegen, et al., “Weak-Attention\nSuppression For Transformer Based Speech Recognition,” in\nInterSpeech, 2020.\n[24] Xie Chen, Yu Wu, Zhenghao Wang, Shujie Liu, and\nJinyu Li, “Developing Real-time Streaming Transformer\nTransducer for Speech Recognition on Large-scale Dataset,”\nhttp://arxiv.org/abs/2010.11395, 2020.\n[25] D. Le, X. Zhang, W. Zheng, and Others, “From Senones\nto Chenones: Tied Context-Dependent Graphemes for Hybrid\nSpeech Recognition,” arXiv preprint arXiv:1910.01493, 2019.\n[26] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, “The Kaldi speech\nrecognition toolkit,” ASRU, pp. 1–4, 2011.\n[27] T. Ko, V . Peddinti, D. Povey, and Others, “Audio augmentation\nfor speech recognition,” in InterSpeech, 2015.\n[28] D S Park, W Chan, Y Zhang, and Others, “Specaugment: A\nsimple data augmentation method for automatic speech recog-\nnition,” arXiv preprint arXiv:1904.08779, 2019.\n[29] A. Tjandra, C. Liu, F. Zhang, and Others, “Deja-vu: Dou-\nble Feature Presentation and Iterated loss in Deep Transformer\nNetworks,” ICASSP, 2020.\n[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” arXiv preprint arXiv:1412.6980, 2014.\n[31] T. Kudo and J. Richardson, “SentencePiece: A simple and\nlanguage independent subword tokenizer and detokenizer for\nneural text processing,” EMNLP, pp. 66–71, 2018.\n[32] R. Sennrich, B. Haddow, and A. Birch, “Neural machine trans-\nlation of rare words with subword units,” ACL, 2016.\n[33] K. Vesely, A. Ghoshal, L. Burget, and D. Povey, “Sequence-\ndiscriminative training of deep neural networks.,” in Inter-\nSpeech, 2013, vol. 2013, pp. 2345–2349."
}