{
  "title": "GLT-T: Global-Local Transformer Voting for 3D Single Object Tracking in Point Clouds",
  "url": "https://openalex.org/W4382464387",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3215896032",
      "name": "Jiahao Nie",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A1996407929",
      "name": "Zhiwei He",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2099045268",
      "name": "Yuxiang Yang",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2121717390",
      "name": "Mingyu Gao",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2087085944",
      "name": "Jing Zhang",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A3215896032",
      "name": "Jiahao Nie",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A1996407929",
      "name": "Zhiwei He",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2099045268",
      "name": "Yuxiang Yang",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2121717390",
      "name": "Mingyu Gao",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2087085944",
      "name": "Jing Zhang",
      "affiliations": [
        "University of Sydney"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6730800597",
    "https://openalex.org/W2435777129",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3093537218",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W2918334248",
    "https://openalex.org/W6682889407",
    "https://openalex.org/W6793697131",
    "https://openalex.org/W6803042736",
    "https://openalex.org/W2902321499",
    "https://openalex.org/W4226495185",
    "https://openalex.org/W2917353338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3042897083",
    "https://openalex.org/W4225292964",
    "https://openalex.org/W4226261750",
    "https://openalex.org/W6629218199",
    "https://openalex.org/W6761849836",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W6778520545",
    "https://openalex.org/W3194700450",
    "https://openalex.org/W2925359305",
    "https://openalex.org/W6811069853",
    "https://openalex.org/W3199838280",
    "https://openalex.org/W6673240527",
    "https://openalex.org/W3175227919",
    "https://openalex.org/W3105084609",
    "https://openalex.org/W4224226832",
    "https://openalex.org/W2991833700",
    "https://openalex.org/W6788305448",
    "https://openalex.org/W3208501016",
    "https://openalex.org/W4200635213",
    "https://openalex.org/W3093995687",
    "https://openalex.org/W3035396860",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W4313136325",
    "https://openalex.org/W3209993199",
    "https://openalex.org/W2089961441",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W2988715931",
    "https://openalex.org/W3094795719",
    "https://openalex.org/W4312257978",
    "https://openalex.org/W4315705623",
    "https://openalex.org/W4280651416",
    "https://openalex.org/W1487560374",
    "https://openalex.org/W4312950653",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3109340983",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W3214072987",
    "https://openalex.org/W2962922818",
    "https://openalex.org/W3209321113",
    "https://openalex.org/W4297780875",
    "https://openalex.org/W3129373298",
    "https://openalex.org/W4224992933",
    "https://openalex.org/W3035641096",
    "https://openalex.org/W2562105614",
    "https://openalex.org/W2964071664",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W2963121255"
  ],
  "abstract": "Current 3D single object tracking methods are typically based on VoteNet, a 3D region proposal network. Despite the success, using a single seed point feature as the cue for offset learning in VoteNet prevents high-quality 3D proposals from being generated. Moreover, seed points with different importance are treated equally in the voting process, aggravating this defect. To address these issues, we propose a novel global-local transformer voting scheme to provide more informative cues and guide the model pay more attention on potential seed points, promoting the generation of high-quality 3D proposals. Technically, a global-local transformer (GLT) module is employed to integrate object- and patch-aware prior into seed point features to effectively form strong feature representation for geometric positions of the seed points, thus providing more robust and accurate cues for offset learning. Subsequently, a simple yet effective training strategy is designed to train the GLT module. We develop an importance prediction branch to learn the potential importance of the seed points and treat the output weights vector as a training constraint term. By incorporating the above components together, we exhibit a superior tracking method GLT-T. Extensive experiments on challenging KITTI and NuScenes benchmarks demonstrate that GLT-T achieves state-of-the-art performance in the 3D single object tracking task. Besides, further ablation studies show the advantages of the proposed global-local transformer voting scheme over the original VoteNet. Code and models will be available at https://github.com/haooozi/GLT-T.",
  "full_text": "GLT-T: Global-Local Transformer Voting for 3D Single Object Tracking\nin Point Clouds\nJiahao Nie1, Zhiwei He1*, Yuxiang Yang1, Mingyu Gao1, Jing Zhang2\n1 School of Electronics and Information, Hangzhou Dianzi University, China\n2 School of Computer Science, The University of Sydney, Australia\n{jhnie, zwhe, yyx, mackgao}@hdu.edu.cn, jing.zhang1@sydney.edu.au\nAbstract\nCurrent 3D single object tracking methods are typically based\non V oteNet, a 3D region proposal network. Despite the suc-\ncess, using a single seed point feature as the cue for off-\nset learning in V oteNet prevents high-quality 3D proposals\nfrom being generated. Moreover, seed points with different\nimportance are treated equally in the voting process, ag-\ngravating this defect. To address these issues, we propose\na novel global-local transformer voting scheme to provide\nmore informative cues and guide the model pay more at-\ntention on potential seed points, promoting the generation of\nhigh-quality 3D proposals. Technically, a global-local trans-\nformer (GLT) module is employed to integrate object- and\npatch-aware prior into seed point features to effectively form\nstrong feature representation for geometric positions of the\nseed points, thus providing more robust and accurate cues\nfor offset learning. Subsequently, a simple yet effective train-\ning strategy is designed to train the GLT module. We de-\nvelop an importance prediction branch to learn the potential\nimportance of the seed points and treat the output weights\nvector as a training constraint term. By incorporating the\nabove components together, we exhibit a superior tracking\nmethod GLT-T. Extensive experiments on challenging KITTI\nand NuScenes benchmarks demonstrate that GLT-T achieves\nstate-of-the-art performance in the 3D single object tracking\ntask. Besides, further ablation studies show the advantages\nof the proposed global-local transformer voting scheme over\nthe original V oteNet. Code and models will be available at\nhttps://github.com/haooozi/GLT-T.\nIntroduction\n3D single object tracking (SOT) in point clouds is a funda-\nmental computer vision task and provides an essential com-\nponent in various practical applications, such as autonomous\ndriving and mobile robotics (Zhang and Tao 2020; Nie et al.\n2022b). Given a 3D bounding box (BBox) in the first frame\nas the target template, the SOT task aims to keep track this\ntarget in a sequence of continuous frames (Kristan et al.\n2021).\nRecently, Siamese architecture has achieved great success\nin 3D SOT. As the pioneer, SC3D (Giancola, Zarzar, and\nGhanem 2019) uses a Kalman filter to generate a series of\n*Corresponding author\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nTemplate\nSearch \nRegion\nFeature\nCorrelation\nGlobal-Local\nTransformer\nPrediction\nHead\nFeature\nExtraction\n3D Proposal and VerificationFeature Augmentation\nSeed Points Proposals 3D BBoxSample Points\nFigure 1: Schematic illustration to show how GTL-T works.\nCompared with the existing 3D Siamese tracking methods,\nwe present a global-local transformer voting scheme to gen-\nerate 3D proposals.\ncandidate 3D BBoxes in current frame, and selects the BBox\nwith the highest similarity to the template as predicted re-\nsult. However, SC3D is not end-to-end trainable and con-\nsumes much computational overhead for matching exhaus-\ntive 3D candidate BBoxes. To solve these issues, an end-\nto-end framework P2B (Qi et al. 2020) is proposed. It in-\ntroduces a 3D region proposal network (RPN), i.e., V oteNet\n(Qi et al. 2019) to generate 3D proposals and predict the\nbest one as tracking BBox. Benefiting from the V oteNet,\nP2B obtains excellent performance in terms of both accu-\nracy and efficiency. Nevertheless, some inherent defects in\nV oteNet prevent high-quality proposals from being gener-\nated, thereby limiting the tracking performance. Different\nfrom the follow-up works (Shan et al. 2021; Wang et al.\n2021b) that have rarely investigated the voting scheme of\nV oteNet, we point out its two defects as follows:\n• The voting scheme uses only a single point feature as the\ncue to infer the offsets of seed points to the target centers,\nmaking it difficult to generate high-quality proposals due\nto the limited representation ability of point feature.\n• Seed points located at different geometric positions in the\ntarget objects contribute differently to learning the off-\nsets. However, existing methods treat them equally in the\nvoting scheme, greatly distracting the model in training\nand leading to sub-optimal proposals.\nIn this paper, we propose a novelglobal-local transformer\nvoting scheme for tracking (GLT-T) to address the above\nchallenges. A schematic illustration of GLT-T is shown in\nFig. 1. To provide more informative cues for offset learn-\ning, we revisit the feature representation requirement in vot-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1957\ning process and suggest a reasonable assumption: modeling\nfeature prior about the geometric position of seed points in\nthe target object can significantly assist in learning the off-\nsets of seed points to the target centers. To achieve this goal,\nwe propose to integrate object- and patch-aware prior into\nthe voting network to infer the position of seed points, pro-\nviding more robust and accurate cues for offset learning.\nMotivated by that point transformer (Zhao et al. 2021) has\npowerful global and local modeling capabilities as well as\nits inherent permutation invariance, we elaborately design\nthe global-local transformer (GLT) module, a cascaded net-\nwork for voting. In contrast to the original point transformer\nthat only encodes the intensity features of point clouds, the\nproposed cascaded transformer structure also effectively en-\ncodes the geometric features. Thus, the GLT module can\nmodel the useful geometric position prior of seed points lo-\ncated in the target object, guiding more accurate offset learn-\ning. Specifically, global-level self-attention and local-level\nself-attention are designed as the core components. On the\none hand, the global-level self-attention encodes the geomet-\nric feature of each seed point by integrating global object in-\nformation. On the other hand, the local-level self-attention\nenhances the geometric feature representation of each seed\npoint by integrating local patch information.\nTo emphasize the different importance of seed points,\nmaking the voting network pay more attention on the seed\npoints that enjoy large potential to offset to the target cen-\nters. We therefore propose to address this challenge by in-\ntroducing a weights vector as the constraint term to train the\nGLT module. The weights vector plays an important role\nin emphasizing seed points, since it describes how difficult\nthe seed points offset to the target centers. Inspired by the\nconcept of centerness (Tian et al. 2019) used in 2D object\ndetection (Zhang et al. 2020; Ma et al. 2020), which repre-\nsent the potential object centers, we propose an importance\nprediction branch to learn this weight vector. Generally, the\nseed points with rich object location information are prone\nto offset to the target centers, and deserve much attention.\nThus, we attach this branch to the global-transformer block\nthat encodes the object-aware geometric prior to learn the\nimportance of seed points.\nOur main contributions can be summarized as follows:\n• We propose a novel GLT-T method for 3D single object\ntracking, alleviating the long-standing unresolved voting\nproblem by a GLT module and a training strategy. To the\nbest of our knowledge, this is the first attempt to improve\nthe voting scheme.\n• GLT module leverages both the global-level and local-\nlevel self-attention to encode object- and patch-aware\nprior and provide informative cues for offset learning,\nthus producing high-quality 3D proposals and signifi-\ncantly enhancing tracking performance.\n• Extensive experiments on KITTI (Geiger, Lenz, and Ur-\ntasun 2012) and NuScenes (Caesar et al. 2020) show that\nGLT-T achieves SOTA performance.\nRelated Work\n3D Single Object Tracking\nEarly 3D SOT methods (Asvadi et al. 2016; Liu et al. 2018;\nPieropan et al. 2015; Bibi, Zhang, and Ghanem 2016; Kart\net al. 2019) mainly track objects in RGB-D domain. Al-\nthough delivering promising results, the RGB-D trackers\nmay fail to track the targets when the RGB-D information\nis degraded, e.g., due to illumination and weather variations.\nSince 3D point cloud data captured by LiDAR sensors is in-\nsensitive to the these variations, point cloud-based SOT has\nattracted great attention recently. SC3D (Giancola, Zarzar,\nand Ghanem 2019) is the first 3D Siamese tracker using pure\npoint clouds. P2B (Qi et al. 2020) introduces a 3D region\nproposal network (RPN) (i.e., V oteNet (Qi et al. 2019)) and\nachieves SOTA performance in terms of both accuracy and\nefficiency. It adopts a tracking-by-detection scheme, which\nemploys a V oteNet to generate 3D proposals and predicts the\nproposal with the highest score for tracking. Inspired by this\nstrong baseline, many follow-up works have been proposed.\nBAT (Zheng et al. 2021) propose a box-aware feature en-\nhancement module to replace the point-wise correlation op-\neration in P2B. Besides, LTTR (Cui et al. 2021) and PTTR\n(Zhou et al. 2022) also propose different correlation opera-\ntions using 3D transformers to promote feature interactions.\nHowever, due to the aforementioned issues of the common\ncomponent V oteNet, the tracking performance is limited by\nthe unsatisfactory proposals.\nTo solve this limitation, MLVSNet (Wang et al. 2021b)\npresents a multi-level voting strategy that uses the multi-\nlayer features of backbone for voting. Similarly, PTT (Shan\net al. 2021) proposes a feature enhancement module with\ntransformer to guide powerful seed point features that par-\nticipate in voting. However, they fail to consider improving\nthe feature representation for offset learning, and give equal\nattention to seed points. In this paper, we deeply investigate\nthe voting procedure and the learning of better feature rep-\nresentation. To this end, we develop a novel voting scheme\nwith a GLT module and a training strategy to guide more\naccurate offset learning and produce high-quality 3D pro-\nposals, thus improving the tracking performance.\nTransformer\nTransformer is originally proposed in the area of natural lan-\nguage processing (Vaswani et al. 2017), showing an excel-\nlent ability in modeling long-range dependency. It becomes\npopular in computer vision recently and has been widely\nused for image classification (Liu et al. 2021; Xu et al. 2021;\nZhang et al. 2022a,b), object detection (Carion et al. 2020;\nWang et al. 2021a, 2022), pose estimation (Xu et al. 2022),\nand object tracking (Lan et al. 2022; Nie et al. 2022a). The\nsuccess motivates researchers to extend it to 3D point cloud\ntasks. Due to the operations such as linear layer, softmax and\nnormalization maintain the permutation invariance of point\nclouds, 3D transformers are well suited for point cloud pro-\ncessing (Zhao et al. 2021; Guo et al. 2021; Lu et al. 2022).\nIn 3D SOT, several transformer tracking methods (Cui\net al. 2021; Zhou et al. 2022; Shan et al. 2021) have been\nproposed. They either use self-attention to further process\n1958\nFinal 3D BBox\nTemplate\nSearch Region\nNt×3\nNs×3\nFeature Augmentation\nShared\nPointNet++PointNet++\nPointNet++PointNet++\nBox-aware\nFeature \nCorrelation\nTemplate Seeds\nSearch Seeds\nMt×(D+3)\nMs×(D+3)\n3D  Proposal and Verification\nSeed \nPoints\nGlobal\nTransformer\nLocal\nTransformer\nImportance Weights\n......\nMs×1\nVoting\nn1×(D+3) \n... ...\nn1×(D+3) \nn1×(D+3) \n... ...p1,    , s1q1 p1,    , s1q1 \npi,    , siqi pi,    , siqi \npK,    , sKqK pK,    , sKqK ... ...p1,    , s1q1 \npi,    , siqi \npK,    , sKqK \nVote Cluster\n Features 3D Proposals\nGlobal-Local Transformer for Voting\nMs×(D+3)\n3D-Center Head\nRot Head\nScore Head\n3D-Center Head\nRot Head\nScore Head\nDecoupled\nPrediction Head\nFigure 2: Overview of our proposed GLT-T. Given a template and search region, we first utilize PointNet++ (Qi et al. 2017b) to\nextract the point features, and fuse them with a box-aware feature correlation (Zheng et al. 2021) to output seed points. We then\napply the proposed global-local transformer voting scheme to generate 3D proposals. Finally, we perform decoupled prediction\nhead to verify the input proposals and output a final 3D BBox.\nfeatures or use cross-attention to interact features from tem-\nplate and search region. In contrast, we elaborate the global-\nlevel and local-level self-attention in GLT module to encode\nobject- and patch-aware prior, allowing to provide informa-\ntive cues for offset learning. Moreover, we emphasize the\nmore important seed points by introducing a novel impor-\ntance prediction branch.\nMethodology\nOverview\nFor 3D object tracking, a template point cloudPt = {pt\ni}Nt\ni=1\ntogether with the 3D BBox Bt = (x t, yt, zt, wt, ht, lt, θt)\nin initial frame are given, where (x, y, z) and (w, h, l) de-\nnotes the center coordinate and size, θ is the rotation an-\ngle around up-axis. The goal is to locate this template tar-\nget in search region Ps = {ps\ni }Ns\ni=1 and output a 3D BBox\nBs = (xs, ys, zs, θs) frame by frame, where Nt and Ns de-\nnote the number of input points for template and search re-\ngion. Notably, since the target size is kept constant in all\nframes, we output only 4 parameters to represent Bs. The\ntracking process can be formulated as:\ntrack : GLT-T(Pt, Ps) → (xs, ys, zs, θs), (1)\nwhere GLT-T is detailed in Fig. 2. It involves two stages to\nperform tracking: 1) Feature Augmentation, and 2) 3D Pro-\nposal and Verification. In the first stage, we employ Point-\nNet++ (Qi et al. 2017b) as the Siamese backbone to extract\npoint features and output the seed points by a box-aware fea-\nture correlation presented in BAT (Zheng et al. 2021). In the\nsecond stage, we propose a global-local-transformer voting\nmethod to generate 3D proposals {(xp, yp, zp)}K\np=1, where\nK denotes the number of proposals. Furthermore, we de-\ncouple the prediction head to predict the confidence scores\nof the 3D proposals and their rotation angles {θp}K\np=1. The\nproposal (xp, yp, zp) with the highest confidence score and\nthe corresponding θp jointly represent the final 3D BBox.\nGlobal-Local Transformer for Voting\nWe devise a global-local transformer based voting scheme to\nlearn accurate offsets. Specifically, the seed points located\non the target surface are used to regress the target center\nand generate accurate 3D proposals {(xp, yp, zp)}K\np=1. As\nshown in Fig. 3, our voting scheme has three essential com-\nponents: a Global Transformer, a Local Transformer and a\nTraining Strategy. Given the seed pointsS = {si}Ms\ni=1, where\nsi = [f i; ci] ∈ RD+3, fi ∈ RD and ci ∈ R3 represent\nthe D-dimension features and 3D coordinate of pointsi. We\nfirst use the global transformer block to encode the object-\naware information for the seed points: [fi; ci] → fg\ni . Af-\nterwards, the local transformer block is used to further en-\ncode the patch-aware information: [fg\ni ; ci] → fgl\ni . With the\ncascaded global-local transformer module, informative cues\nare integrated into the seed points for voting. In addition, an\nimportance prediction branch induced by the global trans-\nformer block is introduced to learn the weights vector, which\nassigns importance weights to the seed points in training.\nGlobal Transformer.The Global Transformer block takes\nthe seed points S = {si}Ms\ni=1 as its inputs. As shown in\nFig. 3, we use a linear layer to embed the original fea-\ntures {fi}Ms\ni=1 into {f∗\ni }Ms\ni=1 to obtain query (Q), key (K)\nand value (V), where f∗\ni ∈ RC. Meanwhile, position en-\ncoding is added. It enables the attention operation to adapt\nto different ranges of information. To model object-aware\nprior and account for computational efficiency, we propose\na sparse sampling strategy to provide global geometric shape\ninformation ciss for each seed point ci. The success of this\nsparse sampling is owing to the feature similarity of neigh-\nboring points.In practice, we first calculate the distance ma-\ntrix {Di,j}Ms,Ms\ni=1,j=1 of the seed points by:\nDi,j =∥ ci − cj ∥2, ∀i, j∈ {1,2, ..., Ms}, (2)\nwhere ∥ · ∥2 denotes L2-norm. Then, each row of the ma-\ntrix {Di,j}Ms,Ms\ni=1,j=1 is ordered from smallest to largest, and\nm seed points are sparsely sampled as the ciss as follows:\nciss =\nMs\nsort\nj=1\n(Di,j)[:: Ms\nm ], ∀i ∈ {1,2, ..., Ms}. (3)\nGiven the global geometric shape informationciss ∈ Rm×3,\nwe consider using the relative position between ci and ciss,\n1959\nGlobal-Level\nSelf-Attention\nFeature\nEmbedding\nPosition\nEncoding\nFFN\nAdd & Norm\nLocal-Level\nSelf-Attention\n Position\nEncoding\nFeature\nEmbedding\nFFN\nAdd & Norm\nVoting\n......\nWeights vector\nImportance Prediction\n...... ......\nTraining Loss\nInput\nTraining \nStrategy\nGlobal\nTransformer\nLocal\nTransformer\nQ K V Q K V\nPP\nFeatures Coordinates\nBackward\nSparse\nSampling\nKNN\nSampling\n...... ......\nFeatures Coordinates\n... ...\nFeatures Coordinates\nFigure 3: Schematic illustration of GLT voting scheme. It\nconsists of three components: a Global Transformer, a Local\nTransformer and a Training Strategy. Given the seed points\nincluding features and coordinates as inputs, the cascaded\nglobal-local transformer encode the object- and patch-aware\ninformation for seed points to perform voting. An impor-\ntance prediction branch is bridged after the global trans-\nformer block, which is used to learn importance weights of\nseed points to constrain the training process.\ni.e., ci − ciss, which captures geometric spatial information,\nto get the position encodings Pos\nF = {pos fi}Ms\ni=1:\npos fi = φg(ci − ciss), (4)\nwhere pos fi ∈ Rm×C and φg(·) is a two-layer MLP net-\nwork with ReLU (Glorot, Bordes, and Bengio 2011).\nAfter getting the feature embedding and position encod-\ning, the global-level self-attention is designed to integrate\nthe features of global sparse points for each seed point. We\nuse a vector attention operator (as shown in Fig. 4) to per-\nform information integration since it can model both channel\nand spatial information interactions, which is more efficient\nthan scalar attention for point cloud transformer. Mathemat-\nically, the global-level self-attention can be formulated as:\n{fglsa\ni }Ms\ni=1 = GLSA(Q, K, V, Pos\nF)\n= Softmax(ϕ(ωq(Q) − ωks(K)\n+ Pos F)) · (ωvs(V ) + Pos F),\n(5)\nwhere ωq(·) is a linear layer, ωks/ωvs(·) is a linear layer\nwith sparse sampling and ϕ(·) is a two-layer MLP network\nwith ReLU. Finally, {fglsa\ni }Ms\ni=1 is fed into a feed-forward\nnetwork, and then added with the original features {fi}Ms\ni=1\nto get the final output {fg\ni }Ms\ni=1:\n{fg\ni }Ms\ni=1 = Norm(FFN({fglsa\ni }Ms\ni=1) + {fi}Ms\ni=1), (6)\nwhere FFN is a linear layer that maps the sizeRC to RD, and\nNorm denotes a layer normalization function to increase the\nfitting ability of the network.\nLinear Linear Linear\nMLP\nSampling Sampling\nQuery Key Value\nSoftmax\nPos_F\nSum\nMs×C\nMs×N×C\nMs×C Ms×C\nMs×C\nMs×C Ms×C\nMs×N×C\nVector Attention\nMs×N×C\nMs×N×C\nMs×N×C\nMs×N×C\nMs×C\nMs×N×C\nMs×N×CMs×N×C\nFigure 4: Illustration of the vector attention. It first projects\nquery, key and value into a latent space and samples the\nglobal / local region features from the key and value (N =\nm and N = n for global-level self-attention and local-level\nself-attention, respectively).Then a MLP and a Softmax op-\nerator are employed to generate the attention matrix. Based\non it, we apply a product dot on the value and obtain the final\noutput by calculating the sum along the attention dimension.\nLocal Transformer.To further provide powerful cues for\nvoting, we use the Local Transformer block to model patch-\naware prior to enhance the geometric feature representation.\nThe local transformer block has a similar structure to the\nglobal transformer block as shown in Fig. 3. Differently,\nwe extract the local region information ciks around ci by\nK-nearest-neighbor (KNN) sampling:\nciks =\nn\nmin\nj=1\n(Di,j), ∀i ∈ {1,2, ..., Ms}, (7)\nwhere ciks ∈ Rn×3, n controls the size of the local region.\nBy encoding the relative position between each seed point\nand the corresponding n nearest neighbor points, the geo-\nmetric feature representation of each seed point is enhanced,\nwhich is beneficial to learn offsets to the target center.\nWith the enhanced seed point features {fgl\ni }Ms\ni=1 that are\nincorporated with both object- and patch-aware prior, a vot-\ning module is employed to learn the offsets:\n[∆fv\ni ; ∆cv\ni ] = Voting([fgl\ni ; ci])\n[fv\ni ; cv\ni ] = [fgl\ni ; ci] + [∆fv\ni ; ∆cv\ni ],\n(8)\nwhere V oting(·) is a three-layer MLP network with batch\nnormalization and ReLU. It learns the feature residuals and\ncoordinate offsets for the seed points participating in the vot-\ning process. At the end, we sample K (K < Ms) 3D pro-\nposals {(xp, yp, zp)}K\np=1 from {cv\ni }Ms\ni=1 using farther point\nsampling (FPS) (Qi et al. 2017a).\nTraining Strategy.Considering that the seed points have\ndifferent impact on learning the offset to the target center,\nas well as only one proposal is predicted as final 3D BBox,\nwe propose a training strategy to distinguish the importance\n1960\nof seed points in the voting process, enabling the network\nto focus on the seed points that are easier to offset to the\ntarget center. To this end, an importance prediction branch\nbridged behind the global transformer block is introduced to\nlearn the weights vector, as shown in Fig. 3. This branch is\nimplemented as a three-layer MLP, and the weights vector\n{Ii}Ms\ni=1 is calculated by:\nIi = MLP(fg\ni ), (9)\nwhere Ii denotes the importance weight of the i-th seed\npoint. To make this weights vector able to represent the im-\nportance of seed points, we use supervised learning to train\nthis branch. The seed points within the target object are de-\nfined as positive samples and the others are defined as neg-\native samples. Then we use a binary cross entropy loss Lbce\nfor {Ii}Ms\ni=1, and the optimization objective is defined as:\nLimp = 1\nMs\nMsX\ni=1\nLbce(Ii, oi), (10)\nwhere oi denotes the label fori-th seed point, i.e.,oi = 1 and\noi = 0 for positive and negative sample, respectively. Using\nthis weights vector, we assign importance weights for seed\npoints in training. In fact, higher weighted points deserve\nmore attention, so we use a modified smooth L1 loss for\noffset learning:\nLoff = 1\nPMs\ni=0 oi\nMsX\ni=0\nsmoothL1(cv\ni , cl) ·(1 +Ii) ·oi, (11)\nwhere c[0] = x, c[1] =y, c[2] = z and cl is the ground truth\nof target center. As illustrated in Eq. 11, the loss for different\nseed points is dynamically adjusted according to {Ii}Ms\ni=1.\nDecoupled Prediction Head\nMost existing trackers use a single MLP as the coupled pre-\ndiction head, which simultaneously predicts the scores, rota-\ntion angles, and refined locations of the proposals. However,\nthe three sub-tasks require different features, so the coupled\nprediction head leads to suboptimal performance. To solve\nthis issue, we decouple the prediction head and use three\nMLP networks to perform the prediction task, in which each\nMLP corresponds to one sub-task. Specifically, as shown in\nFig. 2, the decoupled prediction head consists of three sub-\nprediction heads: 3d-center head, rotation angle head and\nscore head, and each prediction head is composed of a three-\nlayer MLP network with normalization and ReLU. The first\ntwo linear layers keep the feature dimension fixed, while the\nlast layer aligns the output feature dimension according to\nthe different sub-tasks. With a slight increase in model com-\nplexity, the tracking performance is effectively improved by\nthe decoupled prediction head.\nTraining Loss\nThe overall loss arises from two sources, i.e., voting network\nand prediction head. It is defined as:\nL = Loff + λ1Limp + λ2Lscore + λ3Lcenter,rot, (12)\nwhere λ1, λ2 and λ3 are hyper-parameters to balance differ-\nent losses. Details aboutLscore and Lcenter,rot can be found\nin (Qi et al. 2020).\nExperiments\nExperiment Setup\nDatasets. Following previous works, KITTI (Geiger, Lenz,\nand Urtasun 2012) and NuScenes (Caesar et al. 2020)\ndatasets are adopted to train and test the proposed GLT-T\nmethod. KITTI contains 21 training LiDAR sequences and\n29 test LiDAR sequences, which are sampled at 10Hz. Since\nthe test sequence is not open, we follow the common pro-\ntocol and divide the training LiDAR sequences into train-\ning set (0-16), validation set (17-18) and test set (19-20).\nNuScenes is a more challenging dataset, containing 1,000\nscenes and providing the LiDAR annotations at 2Hz. Fol-\nlowing BAT (Zheng et al. 2021), we train our GLT-T model\non training set, and evaluate the performance on validation\nset with key frames.\nEvaluation Metrics. Following the common practice, we\nuse One Pass Evaluation (OPE) (Wu, Lim, and Yang 2013)\nto measure the metrics Success and Precision and report the\nperformance of different trackers. Given a predicted BBox\nand the corresponding ground truth BBox, the Intersection\nOver Union (IoU) between them is defined as overlap and\nthe distance between their centers is defined as error.Success\ndenotes the Area Under the Curve (AUC) with the overlap\nthreshold ranging from 0 to 1, while Precision denotes the\nAUC with the error threshold ranging from 0 to 2 meters.\nComparison With SOTA Methods\nResults on KITTI.We compare our GLT-T with all rele-\nvant SOTA methods. The results of four categories, includ-\ning Car, Pedestrian, Van and Cyclist, as well as their mean\nresults are presented in Table 1. As reported, GLT-T out-\nperforms all comparison methods in overall performance.\nSpecifically, we obtain the best Success or Precision values\nunder all categories. In particular, for Van and Cyclist cate-\ngories with small instances, GLT-T achieves superior perfor-\nmance, implying that the GLT module can still effectively\nform strong object- and patch-aware prior under the con-\ndition of small training samples. Furthermore, compared to\nthe SOTA method BAT, GLT-T exhibits a significant perfor-\nmance gain by 10.1% and 9.4% in terms of mean Success\nand Precision, respectively. Although BAT achieves the im-\npressive performance in Car and Pedestrian categories, our\nmethod GLT-T still outperforms it by a remarkable advan-\ntage. The improvements are attributed to our voting scheme,\nwhich can generate high-quality 3D proposals and thus im-\nproves the tracking accuracy. The voting scheme is further\nanalyzed in the follow-up ablation study.\nResults on NuScenes.To further evaluate GLT-T, compar-\native experiments are conducted on the more challenging\ndataset NuScenes with four different categories, including\nTruck, Trailer, Bus and Motorcycle. We select the track-\ners that have reported performance on the Nuscenes dataset\nas comparisons. As shown in Table 2, GLT-T achieves the\nbest performance and outperforms BAT by 7.8% (Success)\nand 9.6% (Precision). Besides, due to the various outdoor\nscenes involved in this dataset, the promising performance\nof our method proves its generalizability to real-world track-\ning point cloud sequences.\n1961\nCategory Car Pedestrian Van Cyclist Mean\nFrame Number 6424 6088 1248 308 14068\nSC3D (Giancola, Zarzar, and Ghanem 2019) 41.3 / 57.9 18.2 / 37.8 40.4 / 47.0 41.5 / 70.4 31.1 / 48.5\nP2B (Qi et al. 2020) 56.2 / 72.8 28.7 / 49.6 40.8 / 48.4 32.1 / 44.7 42.4 / 60.0\nF-Siamese (Zou et al. 2020) 37.1 / 50.6 16.2 / 32.2 - / - 47.0 / 77.2 - / -\n3D-SiamRPN (Fang et al. 2020) 58.2 / 76.2 35.2 / 56.2 45.6 / 52.8 36.1 / 49.0 46.6 / 64.9\nPTT (Shan et al. 2021) 67.8 / 81.8 44.9 / 72.0 43.6 / 52.5 37.2 / 47.3 55.1 / 74.2\nLTTR (Cui et al. 2021) 65.0 / 77.1 33.2 / 56.8 35.8 / 48.4 66.2 / 89.9 48.7 / 65.8\nMLVSNet (Wang et al. 2021b) 56.0 / 74.0 34.1 / 61.1 52.0 / 61.4 34.3 / 44.5 45.7 / 66.6\nBAT (Zheng et al. 2021) 60.7 / 74.9 42.1 / 70.1 31.5 / 38.9 53.0 / 82.5 50.0 / 69.9\nV2B (Hui et al. 2021) 70.5 / 81.3 48.3 / 73.5 50.1 / 58.0 40.8 / 49.7 58.4 / 75.2\nPTTR (Zhou et al. 2022) 65.2 / 77.4 50.9 / 81.6 52.5 / 61.8 65.1 / 90.5 57.9 / 78.2\nGLT-T (ours) 68.2 / 82.1 52.4 / 78.8 52.6 / 62.9 68.9 / 92.1 60.1 / 79.3\nTable 1: Performance comparison with SOTA methods on the KITTI dataset.Success / Precision are used for evaluation. Bold\nand underline denote the best result and the second-best one, respectively\nCategory Truck Trailer Bus Motorcycle Mean\nFrame Number 13587 3352 2953 2419 22311\nSC3D (Giancola, Zarzar, and Ghanem 2019) 30.7 / 27.7 35.3 / 28.1 29.4 / 24.1 17.2 / 24.8 29.8 / 27.0\nP2B (Qi et al. 2020) 42.9 / 41.6 48.9 / 40.1 32.9 / 27.4 21.3 / 33.4 40.1 / 38.6\nBAT (Zheng et al. 2021) 45.3 / 42.6 52.6 / 44.9 35.4 / 28.0 22.7 / 35.6 42.6 / 40.1\nGLT-T (ours) 52.7 / 51.4 57.6 / 52.0 44.6 / 40.7 34.8 / 47.6 50.4 / 49.7\nTable 2: Performance comparison with SOTA methods on the NuScenes dataset. Success / Precision are used for evaluation.\nBold and underline denote the best result and the second-best one, respectively\nTracking Variations Success Precision\nw/o GLT, w/o TS 61.7 76.1\nw/ GLT, w/o TS 66.4 ↑4.7 80.7↑4.6\nGLT-T: w/ GLT, w/ TS 68.2 ↑6.5 82.1↑6.0\nTable 3: Ablation study on the proposed voting scheme, in-\ncluding the GLT module and training strategy (TS).\nAblation Study\nModel Components. In GLT-T, we propose a novel vot-\ning scheme to generate 3D proposals. It consists of two key\ncomponent: GLT module and training strategy. To verify\nthe effectiveness of the components, a comprehensive abla-\ntion experiment is conducted on the Car category of KITTI\ndataset following (Qi et al. 2020; Zheng et al. 2021). We re-\nport the results in Table 3. By using the GLT module, our\nmethod achieve a performance improvement of 4.7% and\n4.6% in terms of Success and Precision, respectively. When\nusing the proposed training strategy to train this module, the\nperformance improvement increases up to 6.5% and 6.0%.\nTo intuitively show the superiority of our voting scheme\nover the V oteNet (Qi et al. 2019), we visualize their vot-\ning process in Fig. 5. It can be clearly observed that our ap-\nproach generates more 3D proposals that are close to the\ntarget centers. This is owing to the fact that the GLT module\nintegrates both object- and part-aware information for off-\nset learning. Besides, due to the importance scores for seed\npoints in our voting scheme, the seed points within a certain\nregion have diverse offset directions and values, making the\noffsets of seed points with different geometric positions to\nthe target centers more accurate. By contrast, the neighbor-\ning seed points in V oteNet present similar offsets. In the bot-\ntom row, we show the predicted importance scores. The seed\npoints in target objects have higher scores, indicating that the\ntraining strategy emphasizes the more important seed points\nwhile effectively discriminating those far-away ones.\nGLT Module. We further investigate the effectiveness of\neach transformer block, and the ablation results are shown\nin Table 4. The global transformer block encodes the ge-\nometric position features of the seed points by integrating\nobject-aware prior, which allows to infer accurate offsets\nto generate 3D proposals. Therefore, 3.1% (Success) and\n2.9% (Precision) performance improvements are achieved.\nBesides, since the local transformer integrates parch-aware\nprior into each seed point, enhancing the feature representa-\ntion of the geometric position. The tracking performance is\nfurther improved.\nSparse Sampling Size. The sparse sampling size m in\nglobal transformer block is an important hyper-parameter. A\nsmall size will be inefficient to capture global object prior,\nwhile using a large size will cause redundant information.\nTherefore, we conduct an experiment to evaluate the perfor-\n1962\nTimeline (frame)\nT=40 T=60 T=80 T=100 T=120\n[0, 0.5]\n(0.5, 0.75]\n(0.75, 1]\n3D Proposals\nSeed Points\nGround Truths\nSample Points\nFigure 5: Visualization the voting process of V oteNet (Qi et al. 2019) (top row) and our voting scheme (middle row) on a point\ncloud sequence from KITTI. The red boxes denote the ground truth. The sample points are colored by grey, while the seed\npoints are colored by blue and the 3D proposals regressed by seed points are colored by orange. In the bottom row, we show the\npredicted importance scores of seed points in red, blue and green for the ranges of (0.75,1], (0.5,0.75] and [0,0.5], respectively.\nTracking Variations Success Precision\nw/o GT, w/o LT 61.7 76.1\nw/ GT, w/o LT 64.8 ↑3.1 79.0↑2.9\nw/ GT, w/ LT 66.4 ↑4.7 80.7↑4.6\nTable 4: Ablation study on the proposed GLT module. GT\nand LT denote the global transformer block and local trans-\nformer block, respectively.\nmance of using different sparse sampling size m. As shown\nin the top part of Table 5, the best performance is obtained\nwhen m = 16. Although there is a slight performance drop\nin the case of m = 32, the number of floating point opera-\ntions per second (FLOPs) becomes much larger.\nKNN Sampling Size.In addition, we evaluate the perfor-\nmance of using different KNN sampling size n. The results\nare presented in the bottom part of Table 5. When n = 8,\nthe local patch range is too small to effectively integrate in-\nformation from surrounding points, thus limiting the perfor-\nmance. While in the case of n = 24, more noisy informa-\ntion is included, thus degrading the feature representation of\nseed points. When n = 16, it achieves the best performance\nof 68.2 and 82.1 in terms of Success and Precision, respec-\ntively. In this paper, we set m = 16 and n = 16 by default\nin all the experiments if not specified.\nInference Speed\nSimilar to (Qi et al. 2020; Wang et al. 2021b), we calculate\nthe tracking speed by counting the average running time of\nall frames on the Car category of KITTI dataset. GLT-T runs\nat 30 fps on a single NVIDIA 1080Ti GPU, including 8.7 ms\nfor point cloud pre-processing, 24.1 ms for network forward\ncomputation and 0.6 ms for post-processing.\nSize Success Precision FLOPs / G\nSS (m)\n4 61.3 75.9 1.14\n8 64.8 78.6 1.95\n16 68.2 82.1 3.56\n32 67.6 81.9 6.79\nKS (n)\n8 66.3 79.7 1.95\n16 68.2 82.1 3.56\n24 65.7 79.4 5.18\nTable 5: Ablation studies on the sparse sampling (SS) size\nand KNN sampling (KS) size.\nConclusion\nThis work introduces a novel global-local transformer vot-\ning scheme for 3D object tracking (GLT-T) on point clouds.\nGLT-T promotes generating high quality 3D proposals to en-\nhance performance. To learn strong representation for pre-\ndicting offsets, we develop a cascaded global-local trans-\nformer module to integrate both object- and patch-aware\nprior. Moreover, a simple yet effective training strategy is\ndesigned, guiding the model to pay more attention to im-\nportant seed points. Extensive experiments on KITTI and\nNuScenes benchmarks validate the effectiveness of GLT-T.\nWe hope our voting scheme can serve as a basic compo-\nnent in future research to improve the tracking performance.\nLimitation and Discussion. GLT-T follows the Siamese\nnetwork-based appearance matching paradigm and therefore\ninherits its drawbacks, e.g., it is hard to deal with extremely\nsparse point clouds, where there is no sufficient information\nto perform a favorable appearance match and generate reli-\nable seed points. One possible solution is to aggregate tem-\nporal contexts from previous multi-frame point clouds.\n1963\nAcknowledgements\nThis work was supported in part by the Hangzhou Major\nScience and Technology Innovation Project of China under\nGrant 2022AIZD0022, and the Zhejiang Provincial Key Lab\nof Equipment Electronics.\nReferences\nAsvadi, A.; Girao, P.; Peixoto, P.; and Nunes, U. 2016. 3D\nobject tracking using RGB and LIDAR data. In 2016 IEEE\n19th International Conference on Intelligent Transportation\nSystems (ITSC), 1255–1260. IEEE.\nBibi, A.; Zhang, T.; and Ghanem, B. 2016. 3d part-based\nsparse tracker with automatic synchronization and registra-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1439–1448.\nCaesar, H.; Bankiti, V .; Lang, A. H.; V ora, S.; Liong, V . E.;\nXu, Q.; Krishnan, A.; Pan, Y .; Baldan, G.; and Beijbom, O.\n2020. nuscenes: A multimodal dataset for autonomous driv-\ning. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, 11621–11631.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European conference on computer vi-\nsion, 213–229. Springer.\nCui, Y .; Fang, Z.; Shan, J.; Gu, Z.; and Zhou, S. 2021. 3d\nobject tracking with transformer. arXiv:2110.14921.\nFang, Z.; Zhou, S.; Cui, Y .; and Scherer, S. 2020. 3d-\nsiamrpn: An end-to-end learning method for real-time 3d\nsingle object tracking using raw point cloud. IEEE Sensors\nJournal, 21(4): 1019–1026.\nGeiger, A.; Lenz, P.; and Urtasun, R. 2012. Are we ready\nfor autonomous driving? the kitti vision benchmark suite.\nIn 2012 IEEE conference on computer vision and pattern\nrecognition, 3354–3361. IEEE.\nGiancola, S.; Zarzar, J.; and Ghanem, B. 2019. Leveraging\nShape Completion for 3D Siamese Tracking. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n1359–1368.\nGlorot, X.; Bordes, A.; and Bengio, Y . 2011. Deep sparse\nrectifier neural networks. In Proceedings of the fourteenth\ninternational conference on artificial intelligence and statis-\ntics, 315–323. JMLR Workshop and Conference Proceed-\nings.\nGuo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin, R. R.;\nand Hu, S.-M. 2021. Pct: Point cloud transformer. Compu-\ntational Visual Media, 7(2): 187–199.\nHui, L.; Wang, L.; Cheng, M.; Xie, J.; and Yang, J. 2021.\n3D Siamese V oxel-to-BEV Tracker for Sparse Point Clouds.\nIn Advances in Neural Information Processing Systems,\n28714–28727.\nKart, U.; Lukezic, A.; Kristan, M.; Kamarainen, J.-K.; and\nMatas, J. 2019. Object tracking by reconstruction with view-\nspecific discriminative correlation filters. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1339–1348.\nKristan, M.; Matas, J.; Leonardis, A.; Felsberg, M.;\nPflugfelder, R.; K¨am¨ar¨ainen, J.-K.; Chang, H. J.; Danelljan,\nM.; Cehovin, L.; Luke ˇziˇc, A.; et al. 2021. The ninth visual\nobject tracking vot2021 challenge results. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 2711–2738.\nLan, M.; Zhang, J.; He, F.; and Zhang, L. 2022. Siamese\nNetwork with Interactive Transformer for Video Object Seg-\nmentation. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, 1228–1236.\nLiu, Y .; Jing, X.-Y .; Nie, J.; Gao, H.; Liu, J.; and Jiang,\nG.-P. 2018. Context-aware three-dimensional mean-shift\nwith occlusion handling for robust object tracking in RGB-D\nvideos. IEEE Transactions on Multimedia, 21(3): 664–677.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012–10022.\nLu, D.; Xie, Q.; Wei, M.; Xu, L.; and Li, J. 2022. Trans-\nformers in 3D Point Clouds: A Survey. arxiv:2205.07417.\nMa, W.; Tian, T.; Xu, H.; Huang, Y .; and Li, Z. 2020. Aabo:\nAdaptive anchor box optimization for object detection via\nbayesian sub-sampling. In European Conference on Com-\nputer Vision, 560–575. Springer.\nNie, J.; He, Z.; Yang, Y .; Gao, M.; and Dong, Z. 2022a.\nLearning Localization-aware Target Confidence for Siamese\nVisual Tracking. IEEE Transactions on Multimedia.\nNie, J.; Wu, H.; He, Z.; Gao, M.; and Dong, Z. 2022b.\nSpreading Fine-grained Prior Knowledge for Accurate\nTracking. IEEE Transactions on Circuits and Systems for\nVideo Technology.\nPieropan, A.; Bergstr¨om, N.; Ishikawa, M.; and Kjellstr ¨om,\nH. 2015. Robust 3d tracking of unknown objects. In 2015\nIEEE International Conference on Robotics and Automation\n(ICRA), 2410–2417. IEEE.\nQi, C. R.; Litany, O.; He, K.; and Guibas, L. J. 2019. Deep\nhough voting for 3d object detection in point clouds. In\nproceedings of the IEEE/CVF International Conference on\nComputer Vision, 9277–9286.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Pointnet:\nDeep learning on point sets for 3d classification and segmen-\ntation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 652–660.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. Advances in neural information processing\nsystems, 30.\nQi, H.; Feng, C.; Cao, Z.; Zhao, F.; and Xiao, Y . 2020.\nP2B: Point-to-Box Network for 3D Object Tracking in Point\nClouds. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 6328–6337.\nShan, J.; Zhou, S.; Fang, Z.; and Cui, Y . 2021. PTT: Point-\nTrack-Transformer Module for 3D Single Object Tracking\nin Point Clouds. In IEEE/RSJ International Conference on\nIntelligent Robots and Systems, 1310–1316.\n1964\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully\nconvolutional one-stage object detection. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n9627–9636.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, W.; Cao, Y .; Zhang, J.; and Tao, D. 2021a. Fp-detr:\nDetection transformer advanced by fully pre-training. In In-\nternational Conference on Learning Representations.\nWang, W.; Zhang, J.; Cao, Y .; Shen, Y .; and Tao, D. 2022.\nTowards Data-Efficient Detection Transformers. In Euro-\npean conference on computer vision.\nWang, Z.; Xie, Q.; Lai, Y .-K.; Wu, J.; Long, K.; and Wang, J.\n2021b. MLVSNet: Multi-level V oting Siamese Network for\n3D Visual Tracking. InIEEE/CVF International Conference\non Computer Vision, 3081–3090.\nWu, Y .; Lim, J.; and Yang, M.-H. 2013. Online object track-\ning: A benchmark. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 2411–2418.\nXu, Y .; Zhang, J.; Zhang, Q.; and Tao, D. 2022. ViTPose:\nSimple Vision Transformer Baselines for Human Pose Esti-\nmation. arxiv:2204.12484.\nXu, Y .; Zhang, Q.; Zhang, J.; and Tao, D. 2021. Vitae: Vi-\nsion transformer advanced by exploring intrinsic inductive\nbias. Advances in Neural Information Processing Systems,\n34: 28522–28535.\nZhang, J.; and Tao, D. 2020. Empowering things with in-\ntelligence: a survey of the progress, challenges, and oppor-\ntunities in artificial intelligence of things. IEEE Internet of\nThings Journal, 8(10): 7789–7817.\nZhang, Q.; Xu, Y .; Zhang, J.; and Tao, D. 2022a. Vi-\ntaev2: Vision transformer advanced by exploring inductive\nbias for image recognition and beyond. arXiv preprint\narXiv:2202.10108.\nZhang, Q.; Xu, Y .; Zhang, J.; and Tao, D. 2022b. VSA:\nLearning Varied-Size Window Attention in Vision Trans-\nformers. In European conference on computer vision.\nZhang, S.; Chi, C.; Yao, Y .; Lei, Z.; and Li, S. Z. 2020.\nBridging the gap between anchor-based and anchor-free de-\ntection via adaptive training sample selection. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, 9759–9768.\nZhao, H.; Jiang, L.; Jia, J.; Torr, P. H.; and Koltun, V . 2021.\nPoint transformer. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 16259–16268.\nZheng, C.; Yan, X.; Gao, J.; Zhao, W.; Zhang, W.; Li, Z.; and\nCui, S. 2021. Box-Aware Feature Enhancement for Single\nObject Tracking on Point Clouds. In IEEE/CVF Interna-\ntional Conference on Computer Vision, 13179–13188.\nZhou, C.; Luo, Z.; Luo, Y .; Liu, T.; Pan, L.; Cai, Z.; Zhao, H.;\nand Lu, S. 2022. PTTR: Relational 3D Point Cloud Object\nTracking with Transformer. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 8531–8540.\nZou, H.; Cui, J.; Kong, X.; Zhang, C.; Liu, Y .; Wen, F.; and\nLi, W. 2020. F-Siamese Tracker: A Frustum-based Dou-\nble Siamese Network for 3D Single Object Tracking. In\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems, 8133–8139.\n1965",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6780821681022644
    },
    {
      "name": "Voting",
      "score": 0.6531694531440735
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5338913798332214
    },
    {
      "name": "Transformer",
      "score": 0.5318498015403748
    },
    {
      "name": "Offset (computer science)",
      "score": 0.5152361989021301
    },
    {
      "name": "Point cloud",
      "score": 0.4719758927822113
    },
    {
      "name": "Computer vision",
      "score": 0.3492310047149658
    },
    {
      "name": "Engineering",
      "score": 0.13962265849113464
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I50760025",
      "name": "Hangzhou Dianzi University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    }
  ]
}