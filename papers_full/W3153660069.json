{
  "title": "TWEAC: Transformer with Extendable QA Agent Classifiers",
  "url": "https://openalex.org/W3153660069",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5018004671",
      "name": "Gregor Geigle",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110592354",
      "name": "Nils Reimers",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014688346",
      "name": "Andreas Rücklé",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027450194",
      "name": "Iryna Gurevych",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963185067",
    "https://openalex.org/W2890957719",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3101747393",
    "https://openalex.org/W2915836468",
    "https://openalex.org/W2155482025",
    "https://openalex.org/W2971155257",
    "https://openalex.org/W2573462507",
    "https://openalex.org/W2963974889",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2962974452",
    "https://openalex.org/W2963681593",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W2890961898",
    "https://openalex.org/W2970484662",
    "https://openalex.org/W2511149293",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2900338099",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2947008755",
    "https://openalex.org/W2951873305",
    "https://openalex.org/W2971167298",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2915240437",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W3099235767",
    "https://openalex.org/W2946085385",
    "https://openalex.org/W2794859547",
    "https://openalex.org/W2103714277",
    "https://openalex.org/W3099219382"
  ],
  "abstract": "Question answering systems should help users to access knowledge on a broad range of topics and to answer a wide array of different questions. Most systems fall short of this expectation as they are only specialized in one particular setting, e.g., answering factual questions with Wikipedia data. To overcome this limitation, we propose composing multiple QA agents within a meta-QA system. We argue that there exist a wide range of specialized QA agents in literature. Thus, we address the central research question of how to effectively and efficiently identify suitable QA agents for any given question. We study both supervised and unsupervised approaches to address this challenge, showing that TWEAC -- Transformer with Extendable Agent Classifiers -- achieves the best performance overall with 94% accuracy. We provide extensive insights on the scalability of TWEAC, demonstrating that it scales robustly to over 100 QA agents with each providing just 1000 examples of questions they can answer. Our code and data is available: https://github.com/UKPLab/TWEAC-qa-agent-selection",
  "full_text": "TWEAC: Transformer with Extendable QA Agent Classiﬁers\nGregor Geigle, Nils Reimers, Andreas Rücklé, Iryna Gurevych\nUbiquitous Knowledge Processing Lab (UKP-TUDA)\nDepartment of Computer Science, Technical University of Darmstadt\nhttps://www.ukp.tu-darmstadt.de\nAbstract\nQuestion answering systems should help users\nto access knowledge on a broad range of top-\nics and to answer a wide array of different\nquestions. Most systems fall short of this ex-\npectation as they are only specialized in one\nparticular setting, e.g., answering factual ques-\ntions with Wikipedia data. To overcome this\nlimitation, we propose composing multipleQA\nagents within a meta-QA system. We argue\nthat there exist a wide range of specialized QA\nagents in literature. Thus, we address the cen-\ntral research question of how to effectively and\nefﬁciently identify suitable QA agents for any\ngiven question. We study both supervised and\nunsupervised approaches to address this chal-\nlenge, showing that TWEAC—Transformer\nwith Extendable Agent Classiﬁers—achieves\nthe best performance overall with 94% accu-\nracy. We provide extensive insights on the scal-\nability of TWEAC, demonstrating that it scales\nrobustly to over 100 QA agents with each pro-\nviding just 1000 examples of questions they\ncan answer. Our code and data is available.1\n1 Introduction\nExisting question answering systems are limited\nin the types of answers they can provide and in\nthe data sources they can access. For instance, QA\nsystems that rely on a powerful reading compre-\nhension model (e.g., Yang et al., 2019; Chen et al.,\n2017) cannot answer questions that require access\nover structured data, e.g., for providing informa-\ntion about the weather (Will it rain today at 9 am in\nNew York?) or public transportation (When is the\nnext train departing from London to Liverpool?).\nEven though recent work has studied how to bet-\nter generalize QA models across different domains\nor question types (Rücklé et al., 2020; Guo et al.,\n2020; Yang et al., 2020; Talmor and Berant, 2019),\n1https://github.com/UKPLab/\nTWEAC-qa-agent-selection\nMeta-QA\tSystem \nCQA\t \nAgent \nW ikipedia \nRC\tAgent \nKBQA\t \nAgent \nQ1:\tHow\ttall\tis\tthe\tEif fel\ttower? \nQ2:\tWhat\twere\tthe\tbest\tfilms\tset\tin\tIceland? \nQ2 Q1 \nQ1 \nW eather \nReport \nAgent \n... \nFigure 1: The meta-QA system receives a broad range\nof questions and needs to decide which QA agent is\nbest suited for answering a question.\nthey investigate only a small fraction of the broad\nquestions that humans may ask (see Table 1).\nOne approach for answering a much broader\nrange of questions is a meta-QA system that com-\nposes various QA agents, i.e., specialized QA sub-\nsystems. In this work, we address the central re-\nsearch question that arises when developing such\na system: how can we efﬁciently and accurately\nidentify one or multiple QA agents to which we\ncan route the given question (see Figure 1)?\nThis is conceptually similar to skill selection\nin dialog systems (Li et al., 2019b; Kim et al.,\n2018; Burtsev et al., 2018), with two central differ-\nences: (1) Our setting focuses exclusively on QA\nand avoids dealing with personalization and dialog\nhistory; (2) We are not limited in selecting one skill\nto address the user’s intention, but can invoke mul-\ntiple QA agents in parallel—e.g., to return multiple\nanswers or to perform consistency checks.\nTo the best of our knowledge, such an approach\nhas not been studied before. We, therefore, lay the\nfoundational groundwork for meta-QA systems and\nfocus on the identiﬁcation of suitable QA agents.\nImportantly, we treat the QA agents as black\nboxes without further knowledge about their in-\nternal workings. This is advantageous because it\narXiv:2104.07081v2  [cs.CL]  16 Sep 2021\nallows us to study the QA agent selection without\nexternal effects from the QA models themselves—\ne.g., whether they provide a correct answer. We\nthereby identify the following four challenges for\nthe QA agent selection:\n1. Heterogeneous scopes and tasks: We deal with\nimbalanced data, various types of questions, etc.\n2. Extensibility: Adding QA agents seamlessly\nwithin minutes is crucial to make them available\nto the meta-QA system as quickly as possible.\n3. Scalability: We should be able to handle over\none hundred QA agents to cover a broad range\nof question types, data source, and topics,\n4. Small data: Relevant QA agents should be iden-\ntiﬁed even if they provide only a few examples\nof questions that they can answer\nTo address this, we (1) create a realistic scenario\nbased on 10 well-known QA tasks, and (2) con-\nstruct two substantially larger datasets consisting\nof questions from 171 subreddits and 200 StackEx-\nchange forums. In both scenarios, a model receives\na question and determines in which data set this\nquestion is likely to be observed—thus effectively\ndetermining the most appropriate QA agent. While\nour ﬁrst scenario is realistic because it covers a\nwide range of question types that a QA system\nmight receive, our second scenario addresses scala-\nbility with over a hundred different agents.\nWe study two kinds of models that scale well to\na large number of QA agents. (1) similarity-based\nmodels using sentence embeddings and BM25\n(Robertson and Zaragoza, 2009), ﬁnding that they\nachieve a high precision without additional ﬁne-\ntuning; (2) Transformer with Extendable Agent\nClassiﬁers (TWEAC)—a highly scalable trans-\nformer model (Vaswani et al., 2017) that is exten-\nsible with hundreds of agents without full model\nre-training. Our results show that TWEAC is the\nmost effective overall and achieves the best perfor-\nmances in both experimental settings.\nWe ﬁnd that our models are very sample\nefﬁcient—they require only one thousand exam-\nples for each QA agent at most to achieve up to\n94% accuracy for identifying the correct one. Thus,\nthey can be applied to many realistic scenarios in\nwhich we do not have large amounts of training\ndata. Our models can also scale to many agents\nand still correctly identify the relevant agent.\nWe show that our proposed meta-QA system is\nfeasible in realistic and large-scale scenarios even\nwith extending QA agent ensembles and limited\ndata. This opens up an alternative approach to QA\nsystems whereby we can use multiple QA agents\nspecialized on different questions instead of train-\ning a single model to handle all possible questions.\n2 Related Work\nMost of current QA systems are speciﬁc to an\nindividual type of QA, e.g., community QA\n(Rücklé and Gurevych, 2017; Romeo et al., 2018),\nknowledge-base QA (Sorokin and Gurevych,\n2018), or extractive open-domain QA (Yang et al.,\n2019). They implement very narrow QA agents and\nare not applicable across many different scenarios.\nMore recently, research has expanded the scope to\nmulti-domain approaches that can also be applied\nin zero-shot transfer scenarios (Talmor and Berant,\n2019; Guo et al., 2020; Rücklé et al., 2020). Fur-\nther, some approaches fuse different input sources\nsuch as text and knowledge bases (Sun et al., 2018,\n2019; Xu et al., 2016). Singh et al. (2018) pro-\npose an adaptable pipeline approach where sub-\ncomponents in the QA pipeline such as Named\nEntity Recognition can be selected to best suit the\nquestion. However, none of them address combin-\ning fundamentally different QA agents into one\nextensible system and all are limited to one partic-\nular kind of QA such as extracting answer spans\nfrom documents.\nConsolidating different QA agents into one sys-\ntem is conceptually similar to skill systems in chat-\nbots, where utterances can invoke one of many\nspecialized skills. Skill systems are, for instance,\npart of modern chatbot frameworks, e.g., Deep\nPavlov (Burtsev et al., 2018) and ParlAI (Miller\net al., 2017). An important research challenge in\nthose scenarios is to better deal with a large and\nincreasing number of skills in combination with\nuser-speciﬁc preferences, as it is, for instance, the\ncase in Amazon Alexa (Kim et al., 2018; Li et al.,\n2019a). They also focus on extensibility in their\nmodels but their LSTM models require training\nwith hundreds of skills and millions of examples\nbefore it can be extended. Our methods require\nonly hundreds or thousands of available examples.\nIdentifying QA agents is also related to intent\nclassiﬁcation (Liu and Lane, 2016; Kato et al.,\n2017; Gangadharaiah and Narayanaswamy, 2019;\nQin et al., 2019) and question-type classiﬁcation\n(Chernov et al., 2015; Komninos and Manandhar,\n2016). These tasks, however, rely on ﬁxed ques-\ntion and intent taxonomies, which limits the exten-\nsibility of the developed approaches in practical\nscenarios.\nIdentifying QA agents capable of answering a\nquestion is related to human expert identiﬁcation\ntasks such as CQA Expert Routing (Mumtaz et al.,\n2019; Li et al., 2019b) or Reviewer-Paper Matching\n(Zhao et al., 2018; Anjum et al., 2019; Duan et al.,\n2019). They differ from our agent identiﬁcation as\n(1) they do not train models for speciﬁc experts but\ninstead usually learn a shared embedding space for\nexperts and questions, and (2) human expertise is\nmodeled around the topics the humans are proﬁ-\ncient in while our QA agents also differ in the kind\nof questions asked and not just the topic.\n3 QA Agent Identiﬁcation\nOur goal is to retrieve the QA agents that can—\nmost likely—answer a given question. In contrast\nto intent classiﬁcation (e.g., Qin et al., 2019) where\nthere is only one correct intent for any given query,\nwe formulate QA agent identiﬁcation as a ranking\ntask. This is arguably a more realistic approach in\nour case because one question can potentially be\nanswered by multiple agents.\n3.1 Task Deﬁnition\nLet A = {A1, A2, . . . , AN }be a set of N different\nQA agents and let q be a user question. Our goal\nis to rank the agents in A with respect to their\n(anticipated) ability to answer the question q.\nFor each QA agent Ai, we obtain a set Ei =\n{e1, . . . , eni}of example questions that this agent\ncan answer. We use those examples to train the\nsupervised QA agent selection models.\n3.2 Similarity-based Models\nOur central approach is to adapt the k-nearest neigh-\nbor algorithm to rank agents in relation to a ques-\ntion q. Given a function s(q, e) that measures the\nsimilarity between q and an example e from the ex-\nample question set of an agent, we use s to retrieve\nthe top-k most similar examples e1, ..., ek. Using\nthe examples, we score each agent with\nS(Ai) = 1\n|Ei|\nk∑\nj=1\nIEi(ej)s(q, ej) (1)\nwhere IEi is the indicator function indicating if\nthe example ej is in the set of example questions\nEi.2 To address different example set sizes, we\n2IEi (ej) = 1if ej ∈Ei and 0 otherwise\nTransformer\nAgent\nHead\t1\nAgent\nHead\t2 Agent\nHead\tN... \n\"How\ttall\tis\tthe\tEiffel\ttower?\"\n0.9 0.1 0.5... \nFigure 2: Visualization of TWEAC with N agents\nnormalize by dividing with the example set size\n|Ei|of the respective agent.\nWe evaluate two similarity functions for s:\nBM25 (Robertson and Zaragoza, 2009) and the\ndot product between sentence embeddings from\nthe Universal Sentence Encoder QA (USE-QA)\nmodel (Yang et al., 2020), which was trained on\na large question-answer dataset. We found empir-\nically that k = 50 works well regardless of the\nnumber of agents.\nExtending the similarity-based methods with\nnew agents is efﬁciently done by adding the exam-\nple questions from the new agent to the index. How-\never, the approach might lack knowledge about the\nidiosyncrasies of particular QA domains. Next, we\ntherefore also study a supervised transformer-based\napproach.\n3.3 Transformer with Extendable Agent\nClassiﬁers (TWEAC)\nTWEAC consists of a single transformer-based\nmodel with a classiﬁcation head for each agent\n(see Figure 2), which decides whether an agent is\ncapable of answering a question. We experiment\nwith ALBERT (Lan et al., 2020) and RoBERTa\n(Liu et al., 2019).\nEach classiﬁcation head consists of two fully\nconnected layers: The ﬁrst layer has a dimensional-\nity of 256 and uses a GELU activation (Hendrycks\nand Gimpel, 2016). The second layer outputs a\nscalar (with sigmoid activation). Each head, thus,\nrepresents the independent probability of whether\none agent can answer the given question. We can\nthen rank the agents by their probabilities. We\nimplement the classiﬁcation heads using two con-\nvolution layers with grouped convolutions for par-\nallel execution which gives a sub-linear increase in\nrun-time with respect to the number of classiﬁca-\ntion heads, and it is computationally suitable for\nQA Agent Example Question\nCQA Best browser for web application\nKBQA Who is the mayor of Tel Aviv\nSpan RC Who can enforce European Union law\nMultihop RC When was the writer of Seesaw born\nNon-factoid QA How did Beatlemania develop\nReasoning RC Who was ﬁrst, Edward II or Richard I\nBoolean QA Is a yard the same as a meter\nClaim Validation Obama’s birth certiﬁcate is a forgery\nWeather Report What will the weather be in Tontitown\nMovie Screening Find the ﬁlms at Century Theatres\nTable 1: Examples from the 10 QA agents that cor-\nrespond to 10 common QA tasks in the QA-Tasks\ndataset.\nclassifying thousands of agents.3\nWhen adding a new agent to the model, we only\nintroduce a new classiﬁcation head. Afterward, the\nentire model is ﬁne-tuned with both the training\nexamples of the new agent and examples from pre-\nviously added agents. Training with all available\ndata can be infeasible if the model should be ready\nwithin minutes after an agent was added. We there-\nfore also experiment with a sampling strategy that\nonly uses a fraction of all data in §6.\nWe train the model by minimizing the binary\ncross-entropy (BCE) for each head. Each training\nexample is considered a positive example for the\ncorrect agent and as a negative example for all other\nagents. Given the output of the head hi for agent\nAi and the label y = (y1, . . . , yN ) as a one-hot\nencoding of the correct agent, the loss for each\nhead is deﬁned as:\nLi = −[wiyi log(hi) + (1−yi) log(1−hi)] (2)\nWeighting of positive examples according to wi =\n(∑\nj̸ =i |Ej|)/|Ei|is necessary to balance the sig-\nnals from positive and negative examples.\n4 Data and Evaluation\nFor the evaluation of our models, we simulate\nagents through different datasets without explic-\nitly training a QA agent for the questions. This\nway, we can focus on QA agent identiﬁcation in\nan isolated setting without the inﬂuence of errors\nfrom real QA agents. We construct two setups for\nevaluation: QA-Tasks and Many-Agents.\n4.1 QA Tasks\nWe model a realistic QA scenario with distinct\ntypes of questions and minimal overlap between\nthe agents by using questions from different QA\ndatasets from the literature. We construct the\ndataset QA-Tasks with 10 agents for different\nQA tasks.\nCommunity QA (CQA) is a variant of retrieval-\nbased QA and re-uses the large quantities of ques-\ntions and answers that have been discussed in\nquestion-answering forums (Nakov et al., 2017;\nTay et al., 2017; Rücklé et al., 2019b,a). We use\nquestion titles from multiple StackExchange fo-\nrums4 to cover a range of topics. StackExchange\ngroups their forums in ﬁve categories and we select\ntwo forums from each category5. We do not ﬁlter\nthe titles so examples can also include just key-\nwords or sentences that are not proper questions.\nKnowledge Base QA (KBQA) answers factoid\nquestions using a knowledge base (Cui et al., 2017).\nWe use the questions from QALD-7 (Usbeck et al.,\n2017) and WebQSP (Yih et al., 2016) as examples.\nSpan-based Reading Comprehension (Span\nRC) extracts the answer to a question from a cor-\npus of text. The examples for this agent come from\nSQuAD (Rajpurkar et al., 2016)\nMultihop RC differs from Span RC as the agent\nmust now perform a multi-step search over multiple\ndocuments. We use HotPotQA (Yang et al., 2018)\nas a source for the questions.\nNon-factoid QA requires descriptions or expla-\nnations as answers in contrast to factoid questions.\nWe use WikiPassageQA (Cohen et al., 2018) as the\ndataset. Different from CQA, which likewise in-\ncludes non-factoid questions, WikiPassageQA uses\nWikipedia data as answers and the questions have\nbeen crowd-sourced.\nReasoning RC covers a range of questions that\nrequire higher reasoning like arithmetic or compar-\nisons as proposed by Dua et al. (2019). We use\ntheir DROP dataset for example questions.\nBoolean QA is an RC task with questions that\nhave a yes/no answer as proposed by Clark et al.\n3Inference time increases by a factor of 2 when scaling\nfrom 10 to 1000 heads and by a factor of 6 when scaling to\n5000 heads.\n4https://archive.org/download/\nstackexchange\n5Categories are: Technology, Culture/ Recre-\nation, Life/ Arts, Science, and Professional ( https:\n//stackexchange.com/sites). We include Stack-\nOverﬂow, Superuser, Gaming, English, Stats, Math, Cooking,\nPhoto, Writers, and Workplace.\n(2019). We use their BoolQ for the agent. The\nquestions are based on Google queries which have\na Wikipedia passage that answers the question.\nClaim Validation fact-checks if a claim is sup-\nported by evidence. We use the Snopes Corpus cre-\nated by Hanselowski et al. (2019). The difference\nto Boolean QA is that the claims and the evidence\nare from Snopes who include potentially unreliable\nsources including false news sites.\nWeather Report & Movie Screening are two\nagents that we use as examples for highly special-\nized questions which can be seen as skills or in-\ntents in conversational agents. Both agents use\nexamples from the NLU Benchmark by Coucke\net al. (2018) for the intents GetWeather and Search-\nScreeningEvent respectively.\nDataset Splits: We use the splits of the pub-\nlished datasets. If the test set is unavailable, 6 we\nuse the development split as the test set and remove\nthe last 25% of the training data to obtain a new\ndev set. We also obtain a dev set for QALD-7 and\nWebQSP the same way. For the StackExchange\ndata, each forum is split into train, dev, and test set,\nsuch that each split contains all 10 forums.\n4.2 Large-Scale QA Agent Identiﬁcation\nA meta-QA system as we propose can have a large\nnumber of agents, e.g., in a public system where\ndevelopers can publish their agents. Some might be\ngeneral QA agents, while others are highly special-\nized agents able to answer questions for speciﬁc\ndomains or tasks. We simulate such a setup using\n171 forums from StackExchange and 200 randomly\nselected subreddits from reddit7. We consider all\npost titles as questions to obtain a sufﬁcient num-\nber of examples for each agent. We term this setup\nMany-Agents.\nIt is important to note that this setup is artiﬁ-\ncial in that we separate agents primarily by topics\nand less by the types of questions. There is also\na higher overlap between questions of different\nforums: while each StackExchange forum and sub-\nreddit is dedicated to a speciﬁc topic, there might\nbe a certain overlap between related topics, e.g.,En-\nglish and English Language Learners in StackEx-\nchange. Nevertheless, we argue that this approach\nallows us to investigate a larger number of agents\nand how our models can scale to such numbers than\n6BoolQ, HotPotQA, DROP, SQuAD, NLU Benchmark\n7BigQuery ( https://console.cloud.google.\ncom/bigquery/) table fh-bigquery:reddit_posts\nwould be possible when relying on QA research\ndatasets as in §4.1.\n5 Experiments\nWe evaluate the models with respect to their (1) sup-\nport of heterogeneous scopes and tasks, (2) perfor-\nmance with little training data, and (3) scalability.\nOur two setups from §4 are created with (1) in mind\nand our experiments that we introduce in what fol-\nlows consider (2) and (3) as well. Later in §6, we\ninvestigate how to efﬁciently extend TWEAC with\nnew agents. In addition, we provide a compari-\nson of inference speed between our models for the\nagent selection in Appendix §A.1.\n5.1 Experimental Setup\nModels and Hyperparameters. For the\nsimilarity-based models, we empirically ﬁnd\nthat k = 50 works the best and use this in all\nexperiments. As for similarity functions, we\ncompare BM25 and USE-QA with the dot-product.\nFor the transformer based ranker (TWEAC),\nwe use ALBERT base (denoted as TWEAC A-b),\nALBERT large (TWEACA-l) and RoBERTa large\n(TWEACR-l). We train all models for 10 epochs\nwith a batch size of 32, a learning rate of 1e-4 (or\n2e-5 for large transformers) with a linear warm-\nup during the ﬁrst epoch. The hyperparameters\nwere chosen after some preliminary training runs\non QA-Tasks.\nWe report Accuracy@1 and mean reciprocal\nrank (MRR) as performance scores. We make the\nassumption that only the agent from the respective\ndataset from which we draw the test question is\nrelevant, all other agents are irrelevant. This as-\nsumption is a result of our dataset construction.\nSample Efﬁciency. We test how many example\nquestions are needed to achieve appropriate rank-\ning results. We evaluate our models onQA-Tasks\nwith 64 up to 4096 examples per agent. We note\nthat ﬁve agents have less than 4000 examples 8\nwhich is accounted for in our weighting compo-\nnent (see Equation 1 and 2). TWEAC is trained\nwith all 10 agents at the same time. We adjust the\nnumber of epochs accordingly to keep the num-\nber of update steps equal to 10 epochs with 1024\nexamples.\n8Claim Validation (3847), Non-factoid QA (3331), KBQA\n(2398), Weather Report & Movie Screening (226 each)\n64 128 256 512 1024 2048 4096\nExamples per QA agent\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nTWEACR l\nTWEACA l\nTWEACA b\nUSE-QA\nBM25\nFigure 3: Accuracy of our models trained on\nQA-Tasks with increasing number of training exam-\nples per agent.\nScalability. We train models in Many-Agents\nwith an increasing number of agents to evaluate\nthe scalability of the different models. For both\nreddit and StackExchange, we train a model with\n10, 50, 100 and 171 (StackExchange)/ 200 (reddit)\nagents. TWEAC is newly trained for each set of\nagents (using 1024 examples per agent).\n5.2 Results\nSample Efﬁciency. Figure 3 shows the perfor-\nmance of methods in QA-Tasks as a function of\nthe number of training examples per agent.9\nThe TWEAC models achieve with 64 examples\nalready over 80% accuracy and over 90% with a\nthousand examples. Similarity-based models per-\nform noticeably worse—especially with very few\nexamples—but reduce the gap to TWEAC with a\nlarger number of examples. Starting at 1024, more\nexamples have a diminishing return for TWEAC\nand might negatively impact the similarity-based\nmodels. Independent of how large we select the\nexample sets for the agents, we observe the same\nperformance ranks for the ﬁve evaluated systems\nwith TWEACR-l performing best and BM25 per-\nforming worst.\nInspecting the performances of individual agents,\nwe ﬁnd that highly specialized agents such as\nWeather Report and Movie Screening perform very\nwell (≥95% accuracy) with very few examples. QA\nAgents with a wider range of possible questions\nor topics like CQA or Span RC, on the other hand,\nare disproportionately worse with few examples.\nHowever, the need for more examples with such\nagents is often no problem in practice as they likely\n9Precise numbers for these ﬁgures can be found in the\nrespective tables in the appendix.\n10 50 100 200\nNumber of QA agents\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nTWEACR l\nTWEACA l\nTWEACA b\nUSE-QA\nBM25\n(a) reddit\n10 50 100 171\nNumber of QA agents\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nTWEACR l\nTWEACA l\nTWEACA b\nUSE-QA\nBM25\n(b) StackExchange\nFigure 4: Accuracy of models trained with an increas-\ning number of agents from Many-Agents\nrequire more training data for the agent subsystem\nand, thus, have more examples available.\nScalability. We present the results of the large-\nscale experiments with Many-Agents in Fig-\nure 4 as a function of the number of agents. 10 In\ncontrast to the previous experiment, we now ob-\nserve rank changes between our methods when in-\ncreasing the number of agents. In particular, USE-\nQA achieves better results compared to TWEACA-b\nand TWEACA-l, and with a smaller number of 10\nagents it also outperforms TWEACR-l on the Stack-\nExchange dataset. A potential reason for the better\nperformance of USE-QA with Many-Agents as\ncompared to QA-Tasks is that USE-QA has been\ntrained on reddit and StackExchange data.\nWe observe similar performance drops across all\nmodels ranging from 20 to 30 points accuracy and\nMRR when increasing the agents from 10 to the\nmaximum number. This is expected, as the task\ndifﬁculty increases with a larger pool of agents.\nFurther, as we analyze in Section 7, most mistakes\nstem from similar topics, e.g., the agent for the\nMath StackExchange ranked higher than the Statis-\n10Detailed results are given in the appendix.\ntics StackExchange agent for a statistics question.\nEven for a large pool of agents, we still achieve\nhigh Accuracy@1 results compared to a random\nbaseline, and the correct agent is ranked on average\nat least second with all models (except BM25).\nSummary. Our proposed models manage to iden-\ntify the correct agents with high precision in a re-\nalistic setup with 10 QA tasks while also scaling\nwell to hundreds of agents. They can achieve good\nperformances even with as little as 64 training ex-\namples per agent. TWEAC—especially with large\ntransformers—outperforms similarity-based mod-\nels in most of our experiments.\n6 Extending TWEAC\nIn a realistic setting, we may want to add new\nQA agents over time. Similarity-based models\nonly need to index the new examples. Extending\nTWEAC is comparatively harder as it needs to be\nﬁne-tuned for the new agent. Re-training on all\navailable data each time we add a new agent would\nbe prohibitive and it would take hours until a new\nagent is eventually integrated into the system.\nWe consider a half-and-half sampling strategy\nthat uses a constant number of examples regardless\nof the number of agents. It uses all 1024 exam-\nples from the extended agent but randomly selects\n1024/(|agents|− 1) examples from each of the\nother agents where |agents|is the number of agents\nin the model for a total of approximately 2048 ex-\namples in total. Each epoch, a new set of examples\nis selected. We hypothesize that just a few exam-\nples from the previously added agents are necessary\nas (1) the heads were already trained with the ex-\namples in previous iterations and (2) the new head\nrequires just a subset of the negative examples to\ndistinguish them from its own examples.\nThe baseline no sampling uses all available ex-\namples for each agent when extending the model.\nWith full training , we train a new model from\nscratch with all agents and do not extend a pre-\nviously trained model.\nExtending TWEAC Once. We extend the\nmodel from 9 to 10 agents in theQA-Tasks setup.\nWe evaluate this with a leave-one-out approach: We\ntrain TWEAC with 9 of the agents and extend with\nthe remaining one. We repeat this for each agent\nand report the average results. During extension,\nwe train the entire model including the transformer\nand all classiﬁcation heads. We compare against a\nStrategy Accuracy MRR\nFull training 0.9096 0.9462\nNo sampling 0.9105 0.9467\nHalf-and-half 0.8860 0.9311\nTable 2: Average performance of extending a model\nfrom 9 to 10 agents fromQA-Tasks with no sampling\nor half-and-half sampling compared to a model directly\ntrained on all 10 agents (full training).\nmodel that was ﬁne-tuned with examples from all\n10 agents. We present the results in Table 2.\nWe see that extending with no sampling per-\nforms as well as the model trained directly with\nall 10 agents. The model trained with half-and-half\nsampling slightly drops in accuracy by 2 points.\nIterative Extension. We iteratively extend-\ning TWEAK one agent at a time with the\nMany-Agents datasets. We start with 10 or 50\nagents and extend to 100.\nWe plot the accuracy as a function of the number\nof extended agents in Figure 5. 11 After iterative\nextension from 10 to 100 agents,half-and-half sam-\npling is 4 points better for reddit and on-par with\nStackExchange compared to no sampling . The\ngap between the two with the model starting at 50\nagents is 2 points for both datasets. Half-and-half\nsampling can thus be used to train models with\nonly a fraction of all examples without a decrease\nin performance compared to using all examples.\nHowever, models trained directly with all agents\nattain higher scores than iterative extension with\nhalf-and-half sampling. The gap between a model\ntrained with all 100 agents and half-and-half start-\ning at 10 agents is 7/ 4 points but it shrinks to 5/ 2\npoints when starting with 50 agents.\nSummary. Out of the analyzed options, full\ntraining TWEAC from scratch yields the highest\naccuracy. However, the training time increases\nlinearly with the number of QA agents. Using\nhalf-and-half sampling, we can rapidly extend the\nmodel with new QA agents. It has roughly a con-\nstant training time independent of the number of\npreviously existent QA agents. The resulting slight\ndrop in performance can be reduced by periodically\nfull training a model with all agents.\n11Exact numbers can be found in the tables in the appendix.\n20 40 60 80 100\nNumber of extended QA agents\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nAccuracy\nno sampling\nhalf-and-half\nfull training\n(a) reddit\n20 40 60 80 100\nNumber of extended QA agents\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\nno sampling\nhalf-and-half\nfull training\n(b) StackExchange\nFigure 5: Accuracy of Iterative Extension with\nMany-Agents. Solid line starts with 10 agents and\ndashed line starts with 50 agents. Dots show perfor-\nmance of full training .\n7 Analysis\nThe scalability experiments in §5.2 showed a per-\nformance decrease of all models with more agents.\nWe analyze the errors made by TWEACR-l trained\non 200 agents from reddit and 171 agents from\nStackExchange to assess what causes this decrease.\nWe focus on the misclassiﬁcation errors at rank 1\nand count the errors for the different unique mis-\ntakes symmetrical, i.e., errors that mistake agent\nA for agent B and the other way around are both\ncounted towards A-B mistakes.\nThe majority of errors are due to only a few mis-\ntakes. In both datasets, around 0.5% of all unique\nmistakes account for 10% of all errors and every\nsecond error is due to less than 10% of all mistakes\n(detailed numbers in the appendix). The mistakes\nwith the largest number of errors are for Stack-\nExchange english-ell (english language learner),\ncstheory-cs and space-astronomy. For reddit it is\npolitics-Conservative, recipes-food and Autos-cars.\nThese mistakes are due to selecting agents of\nforums with very similar topics, e.g., english and\nell overlap considerably in the topics users address\nin these forums. Mistaking such agents is a rea-\nsonable error to make and we show that humans\nalso fail to correctly identify the correct agent for\nquestions where the model failed: We randomly se-\nlect 50 errors made by the model each from reddit\nand StackExchange and ask three expert annota-\ntors to select the right agent between the true label\nand the (wrong) predicted label. On average, they\nachieve 43% accuracy for reddit and 55% accuracy\nfor StackExchange, indicating that humans are not\nbetter than chance to identify the right forum for\nthese questions.\nIn summary, the lower accuracy with a large\nnumber of agents is mainly caused by overlapping\nagents. The evaluation metrics cannot take the\noverlapping agents into account as we are limited\nto only one right agent. It is therefore important for\nfuture work to design better datasets that take into\naccount that multiple agents can answer a question.\n8 Conclusion\nWe analyzed how to automatically select suitable\nQA agents specializing in different questions, as an\nalternative to a single QA system that tries to cover\nall possible questions. We presented a scalable\nmeta-QA system that allows for a ﬂexible exten-\nsion with different QA agents. For newly posed\nquestions, we rank agents by their ability to answer\nthem and select the most suitable ones.\nTo evaluate the QA agent selection task, we cre-\nated a realistic scenario with QA agents based on\ndifferent QA tasks, and constructed a large-scale\nsetting with hundreds of agents using public fo-\nrums from reddit and StackExchange. We have\nestablished that similarity-based methods and our\nnewly proposed approach TWEAC are scalable,\nextensible, and sample efﬁcient. To allow for a\nfast integration of new QA agents, we presented\na half-and-half sampling strategy, which extends\nTWEAC without full re-training using just a frac-\ntion of all available training data.\nFuture work could further explore the overlap be-\ntween QA agents with respect to the questions they\ncan answer, e.g., by creating dedicated datasets\non QA agent selection. Our current datasets do\nnot account for such overlap, potentially under-\nestimating the models’ capabilities.\nAcknowledgments\nThis work has been support by multiple sources.\n(1) The German Research Foundation through the\nGerman-Israeli Project Cooperation (DIP, grant DA\n1600/1-1 and grant GU 798/17-1), (2) by the Ger-\nman Federal Ministry of Education and Research\nand the Hessian Ministry of Higher Education, Re-\nsearch, Science and the Arts within their joint sup-\nport of the National Research Center for Applied\nCybersecurity ATHENE, (3) by European Regional\nDevelopment Fund (ERDF) and the Hessian State\nChancellery – Hessian Minister of Digital Strategy\nand Development under the promotional reference\n20005482 (TexPrax), (4) the German Research\nFoundation (DFG) as part of the UKP-SQuARE\nproject (grant GU 798/29-1).\nWe thank Jonas Pfeiffer, Tilman Beck and\nLeonardo Ribeiro for their insightful feedback and\nsuggestions on a draft of this paper.\nReferences\nOmer Anjum, Hongyu Gong, Suma Bhat, Wen-Mei\nHwu, and JinJun Xiong. 2019. PaRe: A paper-\nreviewer matching approach using a common topic\nspace. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n518–528, Hong Kong, China. Association for Com-\nputational Linguistics.\nMikhail S. Burtsev, Alexander V . Seliverstov, Rafael\nAirapetyan, Mikhail Arkhipov, Dilyara Baymurz-\nina, Nickolay Bushkov, Olga Gureenkova, Taras\nKhakhulin, Yuri Kuratov, Denis Kuznetsov, Alexey\nLitinsky, Varvara Logacheva, Alexey Lymar,\nValentin Malykh, Maxim Petrov, Vadim Polulyakh,\nLeonid Pugachev, Alexey Sorokin, Maria Vikhreva,\nand Marat Zaynutdinov. 2018. DeepPavlov: Open-\nsource library for dialogue systems. In Proceedings\nof ACL 2018, Melbourne, Australia, July 15-20,\n2018, System Demonstrations , pages 122–127.\nAssociation for Computational Linguistics.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1870–\n1879, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nAlexandr Chernov, V olha Petukhova, and Dietrich\nKlakow. 2015. Linguistically motivated question\nclassiﬁcation. In Proceedings of the 20th Nordic\nConference of Computational Linguistics, NODAL-\nIDA 2015, Institute of the Lithuanian Language,\nVilnius, Lithuania, May 11-13, 2015 , volume 109\nof Linköping Electronic Conference Proceedings ,\npages 51–59. Linköping University Electronic Press\n/ Association for Computational Linguistics.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924–2936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nDaniel Cohen, Liu Yang, and W. Bruce Croft. 2018.\nWikipassageqa: A benchmark collection for re-\nsearch on non-factoid answer passage retrieval. In\nThe 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval ,\nSIGIR ’18, page 1165–1168, New York, NY , USA.\nAssociation for Computing Machinery.\nAlice Coucke, Alaa Saade, Adrien Ball, Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro, Thibault Gisselbrecht, Francesco Calt-\nagirone, Thibaut Lavril, Maël Primet, and Joseph\nDureau. 2018. Snips voice platform: an em-\nbedded spoken language understanding system for\nprivate-by-design voice interfaces. arXiv preprint,\nabs/1805.10190.\nWanyun Cui, Yanghua Xiao, Haixun Wang, Yangqiu\nSong, Seung-won Hwang, and Wei Wang. 2017.\nKBQA: learning question answering over QA cor-\npora and knowledge bases. Proceedings of the\nVLDB Endowment, 10(5):565–576.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nZhen Duan, Shicheng Tan, Shu Zhao, Qianqian Wang,\nJie Chen, and Yanping Zhang. 2019. Reviewer as-\nsignment based on sentence pair modeling. Neuro-\ncomputing, 366:97 – 108.\nRashmi Gangadharaiah and Balakrishnan\nNarayanaswamy. 2019. Joint multiple intent\ndetection and slot labeling for goal-oriented dialog.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n564–569, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMandy Guo, Yinfei Yang, Daniel Cer, Qinlan Shen,\nand Noah Constant. 2020. Multireqa: A cross-\ndomain evaluation for retrieval question answering\nmodels. arXiv preprint, abs/2005.02507.\nAndreas Hanselowski, Christian Stab, Claudia Schulz,\nZile Li, and Iryna Gurevych. 2019. A richly anno-\ntated corpus for different tasks in automated fact-\nchecking. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 493–503, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nDan Hendrycks and Kevin Gimpel. 2016. Bridg-\ning nonlinearities and stochastic regularizers with\ngaussian error linear units. arXiv preprint ,\nabs/1606.08415.\nTsuneo Kato, Atsushi Nagai, Naoki Noda, Ryosuke\nSumitomo, Jianming Wu, and Seiichi Yamamoto.\n2017. Utterance intent classiﬁcation of a spoken di-\nalogue system with efﬁciently untied recursive au-\ntoencoders. In Proceedings of the 18th Annual\nSIGdial Meeting on Discourse and Dialogue, Saar-\nbrücken, Germany, August 15-17, 2017 , pages 60–\n64. Association for Computational Linguistics.\nYoung-Bum Kim, Dongchan Kim, Anjishnu Kumar,\nand Ruhi Sarikaya. 2018. Efﬁcient large-scale neu-\nral domain classiﬁcation with personalized atten-\ntion. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2018, Melbourne, Australia, July 15-20, 2018, Vol-\nume 1: Long Papers, pages 2214–2224. Association\nfor Computational Linguistics.\nAlexandros Komninos and Suresh Manandhar. 2016.\nDependency based embeddings for sentence classi-\nﬁcation tasks. In Proceedings of the 2016 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1490–1500, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nHan Li, Jihwan Lee, Sidharth Mudgal, Ruhi Sarikaya,\nand Young-Bum Kim. 2019a. Continuous learn-\ning for large-scale personalized domain classiﬁca-\ntion. pages 3784–3794.\nZeyu Li, Jyun-Yu Jiang, Yizhou Sun, and Wei Wang.\n2019b. Personalized question routing via hetero-\ngeneous network embedding. In The Thirty-Third\nAAAI Conference on Artiﬁcial Intelligence, AAAI\n2019, The Thirty-First Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2019, The\nNinth AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence, EAAI 2019, Honolulu,\nHawaii, USA, January 27 - February 1, 2019, pages\n192–199. AAAI Press.\nBing Liu and Ian Lane. 2016. Attention-based recur-\nrent neural network models for joint intent detection\nand slot ﬁlling. In Interspeech 2016, 17th Annual\nConference of the International Speech Communica-\ntion Association, San Francisco, CA, USA, Septem-\nber 8-12, 2016, pages 685–689. ISCA.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint, abs/1907.11692.\nAlexander H. Miller, Will Feng, Dhruv Batra, Antoine\nBordes, Adam Fisch, Jiasen Lu, Devi Parikh, and\nJason Weston. 2017. Parlai: A dialog research\nsoftware platform. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2017, Copenhagen, Den-\nmark, September 9-11, 2017 - System Demonstra-\ntions, pages 79–84. Association for Computational\nLinguistics.\nSara Mumtaz, Carlos Rodríguez, and Boualem Bena-\ntallah. 2019. Expert2vec: Experts representation\nin community question answering for question rout-\ning. In Advanced Information Systems Engineering\n- 31st International Conference, CAiSE 2019, Rome,\nItaly, June 3-7, 2019, Proceedings, volume 11483 of\nLecture Notes in Computer Science, pages 213–229.\nSpringer.\nPreslav Nakov, Doris Hoogeveen, Lluís Màrquez,\nAlessandro Moschitti, Hamdy Mubarak, Timothy\nBaldwin, and Karin Verspoor. 2017. Semeval-2017\ntask 3: Community question answering. In Proceed-\nings of the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 27–48.\nLibo Qin, Wanxiang Che, Yangming Li, Haoyang Wen,\nand Ting Liu. 2019. A stack-propagation frame-\nwork with token-level intent detection for spoken\nlanguage understanding. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2078–2087, Hong Kong,\nChina. Association for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. pages 2383–2392.\nStephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3(4):333–389.\nSalvatore Romeo, Giovanni Da San Martino, Alberto\nBarrón-Cedeño, and Alessandro Moschitti. 2018. A\nﬂexible, efﬁcient and accurate framework for com-\nmunity question answering pipelines. In Proceed-\nings of ACL 2018, System Demonstrations , pages\n134–139, Melbourne, Australia. Association for\nComputational Linguistics.\nAndreas Rücklé and Iryna Gurevych. 2017. End-to-\nend non-factoid question answering with an inter-\nactive visualization of neural attention weights. In\nProceedings of ACL 2017, System Demonstrations ,\npages 19–24, Vancouver, Canada. Association for\nComputational Linguistics.\nAndreas Rücklé, Naﬁse Sadat Moosavi, and Iryna\nGurevych. 2019a. Coala: A neural coverage-based\napproach for long answer selection with small data.\nIn Proceedings of the 33rd AAAI Conference on Ar-\ntiﬁcial Intelligence (AAAI 2019), pages 6932–6939.\nAndreas Rücklé, Naﬁse Sadat Moosavi, and Iryna\nGurevych. 2019b. Neural duplicate question detec-\ntion without labeled training data. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1607–1617, Hong Kong,\nChina. Association for Computational Linguistics.\nAndreas Rücklé, Jonas Pfeiffer, and Iryna Gurevych.\n2020. MultiCQA: Zero-shot transfer of self-\nsupervised text matching models on a massive scale.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2471–2486, Online. Association for Computa-\ntional Linguistics.\nKuldeep Singh, Arun Sethupat Radhakrishna, An-\ndreas Both, Saeedeh Shekarpour, Ioanna Lytra, Ri-\ncardo Usbeck, Akhilesh Vyas, Akmal Khikmatul-\nlaev, Dharmen Punjani, Christoph Lange, Maria Es-\nther Vidal, Jens Lehmann, and Sören Auer. 2018.\nWhy Reinvent the Wheel: Let’s Build Question An-\nswering Systems Together, page 1247–1256. Interna-\ntional World Wide Web Conferences Steering Com-\nmittee, Republic and Canton of Geneva, CHE.\nDaniil Sorokin and Iryna Gurevych. 2018. Interac-\ntive instance-based evaluation of knowledge base\nquestion answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations , pages\n114–119, Brussels, Belgium. Association for Com-\nputational Linguistics.\nHaitian Sun, Tania Bedrax-Weiss, and William W. Co-\nhen. 2019. PullNet: Open domain question answer-\ning with iterative retrieval on knowledge bases and\ntext. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019 , pages\n2380–2390. Association for Computational Linguis-\ntics.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William W. Co-\nhen. 2018. Open domain question answering using\nearly fusion of knowledge bases and text. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, Brussels, Bel-\ngium, October 31 - November 4, 2018 , pages 4231–\n4242. Association for Computational Linguistics.\nAlon Talmor and Jonathan Berant. 2019. MultiQA: An\nempirical investigation of generalization and trans-\nfer in reading comprehension. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4911–4921, Florence,\nItaly. Association for Computational Linguistics.\nYi Tay, Minh C. Phan, Anh Tuan Luu, and Siu Che-\nung Hui. 2017. Learning to rank question answer\npairs with holographic dual LSTM architecture. In\nProceedings of the 40th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, Shinjuku, Tokyo, Japan, August 7-\n11, 2017, pages 695–704. ACM.\nRicardo Usbeck, Axel-Cyrille Ngonga Ngomo, Bastian\nHaarmann, Anastasia Krithara, Michael Röder, and\nGiulio Napolitano. 2017. 7th open challenge on\nquestion answering over linked data (QALD-7). In\nSemantic Web Challenges - 4th SemWebEval Chal-\nlenge at ESWC 2017, Portoroz, Slovenia, May 28 -\nJune 1, 2017, Revised Selected Papers, volume 769\nof Communications in Computer and Information\nScience, pages 59–69. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undeﬁne-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’17, page 6000–6010, Red Hook, NY ,\nUSA. Curran Associates Inc.\nKun Xu, Yansong Feng, Songfang Huang, and\nDongyan Zhao. 2016. Hybrid question answering\nover knowledge base and free text. In COLING\n2016, 26th International Conference on Computa-\ntional Linguistics, Proceedings of the Conference:\nTechnical Papers, December 11-16, 2016, Osaka,\nJapan, pages 2397–2407. ACL.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nBERTserini. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics (Demonstra-\ntions), pages 72–77, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy\nGuo, Jax Law, Noah Constant, Gustavo Hernandez\nAbrego, Steve Yuan, Chris Tar, Yun-hsuan Sung,\nBrian Strope, and Ray Kurzweil. 2020. Multilingual\nuniversal sentence encoder for semantic retrieval.\npages 87–94.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\npages 2369–2380.\nWen-tau Yih, Matthew Richardson, Chris Meek, Ming-\nWei Chang, and Jina Suh. 2016. The value of se-\nmantic parse labeling for knowledge base question\nanswering. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers) , pages 201–206, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nShu Zhao, Dong Zhang, Zhen Duan, Jie Chen, Yan-\nPing Zhang, and Jie Tang. 2018. A novel classiﬁ-\ncation method for paper-reviewer recommendation.\nScientometrics, 115(3):1293–1313.\nA Appendix\nA.1 Inference Time Comparison between\nModels\nOur system should be computationally efﬁcient,\ni.e., its latency should be low even with a large\nquantity of QA agents. To estimate the computa-\ntional efﬁciency of our approaches, we measure\nhow many queries per second (qps) our model can\ncomplete with 10 and 200 agents. TWEAC is tested\nwith both large and base-sized transformers. We\nindex 1000 examples per agent for the similarity-\nbased methods. BM25 is implemented using Elas-\nticsearch12 with the default conﬁguration and we\nuse Faiss with a ﬂat index for GPU-based kNN\nlookup with USE-QA. TWEAC and USE-QA pro-\ncess each question separately without batching to\nsimulate single questions coming in from a user.\nGPU computation is done on an NVIDIA Titan-\nRTX GPU.\nQA Agents Name QPS\n10 TWEAC R-l 49.44±0.15\n10 TWEAC A-l 39.44±0.15\n10 TWEAC A-b 76.97±1.15\n10 USE-QA 43.60 ±5.01\n10 BM25 232.29 ±60.19\n200 TWEAC R-l 46.41±2.00\n200 TWEAC A-l 36.39±0.30\n200 TWEAC A-b 70.45±0.76\n200 USE-QA 48.10 ±1.67\n200 BM25 221.19 ±34.52\nTable 3: Queries per second (qps) of the different meth-\nods with 10 and 200 agents. The results are the mean\nand standard deviation over 5000 and 100.000 itera-\ntions for the 10 and 200 agents respectively.\nTable 3 shows the results of the measurement.\nWe observe that Elasticsearch is by far the fastest.\nUSE-QA is similar in speed (±10 qps) to the large\nTWEAC models.\nThe choice of transformer architecture has a\nlarge impact on TWEAC. TWEAC A-b is roughly\ntwice as fast as TWEACA-l which also slower than\nTWEACR-l as well. We note that there is only a\nslight increase in the inference time when using\nTWEAC with 10 and 200 agents.\nIn conclusion, the model choice presents a trade-\noff between speed and accuracy: While Elastic-\nsearch with BM25 is considerably faster and re-\nquires no GPU, it is also vastly less accurate than\nthe other methods as shown by our experiments.\n12https://www.elastic.co\nTWEACA-b is 50% faster than TWEACR-l but the\nlatter has a higher performance in all experiments.\nA.2 Figure Values\nIn this section, we present the results to Figure 3,\n4 and 5 in table format along with MRR, which is\nnot included in the ﬁgures.\nExamples Name Accuracy MRR\n64 TWEAC R-l 0.8572 0.9153\n64 TWEAC A-l 0.8449 0.9028\n64 TWEAC A-b 0.8271 0.8938\n64 USE-QA 0.7143 0.8314\n64 BM25 0.6057 0.7457\n128 TWEAC R-l 0.8934 0.9380\n128 TWEAC A-l 0.8646 0.9174\n128 TWEAC A-b 0.8566 0.9089\n128 USE-QA 0.7562 0.8582\n128 BM25 0.6496 0.7762\n256 TWEAC R-l 0.9158 0.9519\n256 TWEAC A-l 0.8980 0.9392\n256 TWEAC A-b 0.8600 0.9074\n256 USE-QA 0.8006 0.8826\n256 BM25 0.6939 0.8093\n512 TWEAC R-l 0.9303 0.9606\n512 TWEAC A-l 0.9139 0.9497\n512 TWEAC A-b 0.8621 0.9173\n512 USE-QA 0.8189 0.8946\n512 BM25 0.7078 0.8226\n1024 TWEAC R-l 0.9371 0.9638\n1024 TWEAC A-l 0.9275 0.9577\n1024 TWEAC A-b 0.9096 0.9462\n1024 USE-QA 0.8441 0.9092\n1024 BM25 0.7385 0.8407\n2048 TWEAC R-l 0.9428 0.9677\n2048 TWEAC A-l 0.9266 0.9581\n2048 TWEAC A-b 0.8998 0.9409\n2048 USE-QA 0.8520 0.9149\n2048 BM25 0.7609 0.8540\n4096 TWEAC R-l 0.9508 0.9723\n4096 TWEAC A-l 0.9324 0.9606\n4096 TWEAC A-b 0.9031 0.9415\n4096 USE-QA 0.8441 0.9092\n4096 BM25 0.7385 0.8407\nTable 4: Table to Figure 3\nA.3 Misclassiﬁcation Analysis\nIn this section, we present the tables to §7.\nQA Agents Name Accuracy MRR\n10 TWEAC R-l 0.8346 0.8902\n10 TWEAC A-l 0.7746 0.8497\n10 TWEAC A-b 0.7662 0.8436\n10 USE-QA 0.7654 0.8500\n10 BM25 0.6664 0.7682\n50 TWEAC R-l 0.6898 0.7638\n50 TWEAC A-l 0.6042 0.6967\n50 TWEAC A-b 0.5980 0.6874\n50 USE-QA 0.6232 0.7141\n50 BM25 0.4855 0.5824\n100 TWEAC R-l 0.6029 0.6872\n100 TWEAC A-l 0.5146 0.6186\n100 TWEAC A-b 0.5145 0.6049\n100 USE-QA 0.5469 0.6394\n100 BM25 0.4186 0.5080\n200 TWEAC R-l 0.5226 0.6173\n200 TWEAC A-l 0.4463 0.5513\n200 TWEAC A-b 0.4428 0.5385\n200 USE-QA 0.4813 0.5734\n200 BM25 0.3604 0.4437\n(a) reddit\nQA Agents Name Accuracy MRR\n10 TWEAC R-l 0.8648 0.9135\n10 TWEAC A-l 0.8582 0.9072\n10 TWEAC A-b 0.8377 0.8964\n10 USE-QA 0.8812 0.9275\n10 BM25 0.7572 0.8453\n50 TWEAC R-l 0.7339 0.8093\n50 TWEAC A-l 0.6947 0.7768\n50 TWEAC A-b 0.6655 0.7543\n50 USE-QA 0.7046 0.7952\n50 BM25 0.5707 0.6827\n100 TWEAC R-l 0.6830 0.7708\n100 TWEAC A-l 0.6238 0.7239\n100 TWEAC A-b 0.5914 0.6966\n100 USE-QA 0.6321 0.7343\n100 BM25 0.5027 0.6128\n171 TWEAC R-l 0.6285 0.7250\n171 TWEAC A-l 0.5604 0.6705\n171 TWEAC A-b 0.5258 0.6359\n171 USE-QA 0.5636 0.6733\n171 BM25 0.4476 0.5544\n(b) StackExchange\nTable 5: Table to Figure 4.\nQA Agents Name Accuracy MRR\n50 full training 0.5980 0.6874\n50 no sampling (10) 0.5028 0.5664\n50 half-and-half (10) 0.5556 0.6505\n100 full training 0.5145 0.6049\n100 no sampling (10) 0.3971 0.4542\n100 no sampling (50) 0.4363 0.4954\n100 half-and-half (10) 0.4406 0.5421\n100 half-and-half (50) 0.4579 0.5630\n(a) reddit\nQA Agents Name Accuracy MRR\n50 full training 0.6655 0.7543\n50 no sampling 0.6447 0.7296\n50 half-and-half 0.6511 0.7458\n100 full training 0.5914 0.6966\n100 no sampling (10) 0.5541 0.6329\n100 no sampling (50) 0.5900 0.6846\n100 half-and-half (10) 0.5557 0.6738\n100 half-and-half (50) 0.5696 0.6834\n(b) StackExchange\nTable 6: Table to Figure 5 with values for 50 and 100\nagents. The starting number of agents for the iterative\nextension models is indicated in parenthesis.\nreddit\nTotal unique mistakes 4999\nTotal misclassiﬁcation errors 32523\n0.540% of mistakes cause 10% of errors\n8.801% of mistakes cause 50% of errors\n51.45% of mistakes cause 90% of errors\nStackExchanges\nTotal unique mistakes 9120\nTotal misclassiﬁcation errors 48882\n0.394% of mistakes cause 10% of errors\n9.736% of mistakes cause 50% of errors\n59.21% of mistakes cause 90% of errors\nTable 7: Statistics to the misclassiﬁcation errors and the\nunique mistakes between agents in §7\nMistake h of total errors\npolitics ↔Conservative 5.50\nrecipes ↔food 5.09\nAutos ↔cars 4.15\nunitedkingdom ↔ukpolitics 4.09\napple ↔mac 3.72\nbeer ↔Coffee 3.21\nAnarchism ↔socialism 3.15\ndating ↔seduction 3.07\nsnowboarding ↔skiing 2.97\nGames ↔pcgaming 2.95\ntechsupport ↔computers 2.72\nbicycling ↔motorcycles 2.66\nsoccer ↔sports 2.66\natheism ↔Christianity 2.58\napple ↔technology 2.52\nGames ↔videogames 2.50\nBuddhism ↔zen 2.23\nhorror ↔movies 2.17\nMarijuana ↔weed 2.17\ntechsupport ↔windows 2.17\nTotal Sum 62.27\n(a) reddit\nMistake h of total errors\nenglish ↔ell 8.49\ncstheory ↔cs 5.50\nspace ↔astronomy 5.17\nhermeneutics ↔christianity 4.64\nkorean ↔beer 4.52\nsciﬁ ↔movies 4.37\nwindowsphone ↔android 4.00\nlinguistics ↔conlang 3.90\nelementaryos ↔askubuntu 3.87\nstats ↔datascience 3.87\nsuperuser ↔serverfault 3.63\nmonero ↔bitcoin 3.57\nhomebrew ↔beer 3.26\nmatheducators ↔math 3.26\nsecurity ↔crypto 3.26\nunix ↔askubuntu 3.26\nskeptics ↔health 3.14\nvi ↔emacs 2.98\nmovies ↔literature 2.92\nphysics ↔astronomy 2.89\nTotal Sum 80.50\n(b) StackExchange\nTable 8: Twenty largest mistakes in §7",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.834703266620636
    },
    {
      "name": "Question answering",
      "score": 0.7074832320213318
    },
    {
      "name": "Scalability",
      "score": 0.6619189977645874
    },
    {
      "name": "Transformer",
      "score": 0.5157235264778137
    },
    {
      "name": "Labeled data",
      "score": 0.5149934887886047
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4847364127635956
    },
    {
      "name": "Code (set theory)",
      "score": 0.47257769107818604
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.45471546053886414
    },
    {
      "name": "Machine learning",
      "score": 0.44517430663108826
    },
    {
      "name": "Information retrieval",
      "score": 0.4073147773742676
    },
    {
      "name": "Natural language processing",
      "score": 0.336933970451355
    },
    {
      "name": "Data mining",
      "score": 0.32314956188201904
    },
    {
      "name": "Database",
      "score": 0.1796095073223114
    },
    {
      "name": "Programming language",
      "score": 0.16802513599395752
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}