{
  "title": "Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents",
  "url": "https://openalex.org/W3124346429",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3088226223",
      "name": "Bhattacharya, Uttaran",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2755287970",
      "name": "Rewkowski, Nicholas",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2743548578",
      "name": "Banerjee, Abhishek",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A4288016679",
      "name": "Guhan, Pooja",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2743559721",
      "name": "Bera, Aniket",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2743699785",
      "name": "Manocha, Dinesh",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963351004",
    "https://openalex.org/W2968990787",
    "https://openalex.org/W6744658717",
    "https://openalex.org/W2922298118",
    "https://openalex.org/W2024221294",
    "https://openalex.org/W2151215842",
    "https://openalex.org/W4230312136",
    "https://openalex.org/W2758220497",
    "https://openalex.org/W3002310794",
    "https://openalex.org/W6752413241",
    "https://openalex.org/W2738406145",
    "https://openalex.org/W2986708244",
    "https://openalex.org/W2082735203",
    "https://openalex.org/W2007041320",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2538496976",
    "https://openalex.org/W2066806063",
    "https://openalex.org/W2007337857",
    "https://openalex.org/W2739192055",
    "https://openalex.org/W6788464744",
    "https://openalex.org/W3083173864",
    "https://openalex.org/W2967443589",
    "https://openalex.org/W3009042479",
    "https://openalex.org/W6781084731",
    "https://openalex.org/W2962795401",
    "https://openalex.org/W2082981966",
    "https://openalex.org/W2747308422",
    "https://openalex.org/W2604471456",
    "https://openalex.org/W2901872500",
    "https://openalex.org/W2469134594",
    "https://openalex.org/W2611706523",
    "https://openalex.org/W6752114883",
    "https://openalex.org/W2981271350",
    "https://openalex.org/W2963544341",
    "https://openalex.org/W2802076562",
    "https://openalex.org/W2462561572",
    "https://openalex.org/W2974177253",
    "https://openalex.org/W2968566263",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W6762736072",
    "https://openalex.org/W2095527735",
    "https://openalex.org/W1208039178",
    "https://openalex.org/W6719211858",
    "https://openalex.org/W1490440056",
    "https://openalex.org/W2012758188",
    "https://openalex.org/W6676218802",
    "https://openalex.org/W2901466962",
    "https://openalex.org/W2981802563",
    "https://openalex.org/W2078951341",
    "https://openalex.org/W4253285916",
    "https://openalex.org/W2169025255",
    "https://openalex.org/W3090160436",
    "https://openalex.org/W2982294548",
    "https://openalex.org/W1969681536",
    "https://openalex.org/W2991276178",
    "https://openalex.org/W2997258743",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W2798357113",
    "https://openalex.org/W3034520808",
    "https://openalex.org/W2147449317"
  ],
  "abstract": "We present Text2Gestures, a transformer-based learning method to interactively generate emotive full-body gestures for virtual agents aligned with natural language text inputs. Our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions, also known as affective features. We also consider the intended task corresponding to the text and the target virtual agents' intended gender and handedness in our generation pipeline. We train and evaluate our network on the MPI Emotional Body Expressions Database and observe that our network produces state-of-the-art performance in generating gestures for virtual agents aligned with the text for narration or conversation. Our network can generate these gestures at interactive rates on a commodity GPU. We conduct a web-based user study and observe that around 91% of participants indicated our generated gestures to be at least plausible on a five-point Likert Scale. The emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions, with a minimum Pearson coefficient of 0.77 in the valence dimension.",
  "full_text": "Text2Gestures: A Transformer-Based Network for Generating Emotive\nBody Gestures for Virtual Agents\nUttaran Bhattacharya1 Nicholas Rewkowski2 Abhishek Banerjee3 Pooja Guhan4 Aniket Bera5\nDinesh Manocha6\nUniversity of Maryland, College Park, MD 20742, USA\n1uttaranb@umd.edu 2nick1@umd.edu 3abanerj8@terpmail.umd.edu 4pguhan@umd.edu 5bera@umd.edu 6dmanocha@umd.edu\nABSTRACT\nWe present Text2Gestures, a transformer-based learning method to\ninteractively generate emotive full-body gestures for virtual agents\naligned with natural language text inputs. Our method generates\nemotionally expressive gestures by utilizing the relevant biomechan-\nical features for body expressions, also known as affective features.\nWe also consider the intended task corresponding to the text and\nthe target virtual agents’ intended gender and handedness in our\ngeneration pipeline. We train and evaluate our network on the MPI\nEmotional Body Expressions Database and observe that our net-\nwork produces state-of-the-art performance in generating gestures\nfor virtual agents aligned with the text for narration or conversation.\nOur network can generate these gestures at interactive rates on a\ncommodity GPU. We conduct a web-based user study and observe\nthat around 91% of participants indicated our generated gestures\nto be at least plausible on a five-point Likert Scale. The emotions\nperceived by the participants from the gestures are also strongly pos-\nitively correlated with the corresponding intended emotions, with a\nminimum Pearson coefficient of 0.77 in the valence dimension.\nIndex Terms: Computing methodologies—Virtual reality;\nComputing methodologies—Intelligent agents; Computer systems\norganization—Neural networks;\n1 I NTRODUCTION\nAs the world increasingly uses digital and virtual platforms for ev-\neryday communication and interactions, there is a heightened need\nto create highly realistic virtual agents endowed with social and\nemotional intelligence. Interactions between humans and virtual\nagents are being used to augment traditional human-human interac-\ntions in different applications, including online learning [37, 39, 59],\nvirtual interviewing and counseling [6, 16], virtual social interac-\ntions [24, 35, 40, 56], and large-scale virtual worlds [50]. Human-\nhuman interactions rely heavily on a combination of verbal commu-\nnications (the text), inter-personal relationships between the people\ninvolved (the context), and more subtle non-verbal face and body\nexpressions during communication (the subtext) [32, 41]. While\ncontext is often established at the beginning of interactions, virtual\nagents in social VR applications need to align their text with their\nsubtext throughout the interaction, thereby improving the human\nusers’ sense of presence in the virtual environment. Gesticulation\nis an integral component in subtext, where humans use patterns of\nmovement for hands, arms, heads, and torsos to convey a wide range\nof intent, behaviors, and emotions [42]. In this work, we investigate\nthe problem of aligning emotionally expressive gestures with the text\nThis work has been supported in part by ARO Grants W911NF1910069\nand W911NF1910315, and Intel. Code and additional materials available at:\nhttps://gamma.umd.edu/t2g.\nto generate virtual agents’ actions that result in natural interactions\nwith human users.\nCurrent game engines and animation engines can generate human-\nlike movements for virtual agents, including head poses, hand ges-\ntures, and torso movements [2, 61]. However, aligning these move-\nments with a virtual agent’s associated speech or text transcript is\nmore challenging. Traditional approaches such as hand-crafting\nanimations or collecting and transferring context-specific gestures\nthrough rotoscoping or motion capture look natural [49,66], but need\nto be manually designed for every new gesture. However, virtual\nagents performing live social interactions with humans in VR need\nto adapt their gestures to their words and current social context in\nreal-time. As a result, prior approaches based on pre-generated ani-\nmations or motion specifications are limited, and we need interactive\nmethods to generate plausible gestures.\nExisting approaches for interactive speech-aligned gesture gen-\neration learn mappings between speech signals and the generated\ngesture sequences [2,33]. In contrast to these speech-based methods,\nour goal is to align the gestures directly with the natural language\ntext transcripts. This eliminates the need to have speeches pre-\nrecorded by humans or machines, which have a higher production\ncost. Prior works on generating gestures aligned with text [69] have\nleveraged the well-known sequence-to-sequence modeling network,\nwhich is efficient at performing a variety of sequence-to-sequence\nprediction tasks. These methods have only considered arms and head\nmotions and are limited to generating gestures with small variations\nin categorical emotions such as happy, angry, and sad.\nHowever, as evidenced by works on adding emotions through\nfacial and vocal expressions [18, 60, 67], emotional expressiveness\nadds to the realism of virtual agents. Studies in psychology and\naffective computing show that body expressions also contain useful\ncues for perceived emotions [3, 7, 9], and often help disambiguate\nthe emotions perceived from facial and vocal cues [4, 46, 47]. These\nbody expressions are composed of biomechanical features known as\naffective features. Common affective features include, among others,\nthe rate of arm swings, stride lengths, shoulder and spine postures,\nand head jerks [28]. More recent approaches for generating virtual\nagents with gait-based body expressions have leveraged the relevant\ngait-based affective features to improve the perceived naturalness of\nthe animations [7, 54, 55].\nFollowing these works, we aim to generate body gestures for vir-\ntual agents in social VR settings to either narrate text-based content\nto human participants or continue a text-based conversation with\nhuman participants. We use affective features to make the gestures\nemotionally expressive, such that the human participants can per-\nceive appropriate emotions from the virtual agents based on the\nnatural language text.\nMain Results:We present an end-to-end trainable generative net-\nwork that produces emotive body gestures aligned with natural lan-\nguage text. We design our method for interactive applications, where\na virtual agent narrates lines or takes part in a conversation. To this\nend, we make use of the transformer network [64], and extend cur-\nrent approaches to work with gestures for virtual agents in 3D. We\nalso adapt the gestures based on narration or conversation and the in-\narXiv:2101.11101v3  [cs.HC]  22 Nov 2024\ntended gender and handedness (dominance of left-hand or right-hand\nin gesticulation) of the virtual agents. We also make the gestures\nemotionally expressive by utilizing the relevant gesture-based affec-\ntive features of the virtual agents.\nTo summarize, our contributions are four-fold:\n• A transformer-based network that interactively takes in text\none sentence at a time and generates 3D pose sequences for\nvirtual agents corresponding to gestures aligned with that text.\n• Conditioning the generation process to follow the intended\nacting task of narration or conversation and the virtual agents’\nintended gender and handedness.\n• Considering the intended emotion in the text to generate emo-\ntionally expressive gestures.\n• A web study with 600 total responses to evaluate the quality of\nour generated gestures compared to motion-captured sequences\nand the emotional expressiveness of our generated gestures.\nBased on our experiments, we find that our network has state-\nof-the-art performance for generating gestures aligned with text\ncompared to ground-truth sequences in a large-scale motion capture\ndatabase. We can generate these gestures at an interactive rate of\n312.5 fps using an Nvidia GeForce GTX 1080Ti GPU. Based on\nour user study, we also find that the emotions perceived by the\nparticipants from the gestures are strongly positively correlated\nwith the corresponding intended emotions of the gestures, with\na minimum Pearson coefficient of 0.77 in the valence dimension.\nMoreover, around 91% of participants found our generated gestures\nare plausible on a five-point Likert Scale.\n2 R ELATED WORK\nThis section summarizes studies exploring how different emotions\nare perceived from body gestures and how they have been utilized\nto generate emotive virtual agents.\nWe also review prior work on generating human body gestures\nin graphics and VR, particularly those that align the gestures with\nspeech and text content. We focus mostly on data-driven approaches\nhere because we base our work on a similar foundation, and refer\nthe interested reader to Wagner et al.’s extensive survey [66] for the\nmore classical rule-based approaches. The main limitation of such\nrule-based approaches is that their range of gestures is confined to\nthe designed set of gestures. Hence, they require that gestures for\nevery novel speech and text inputs are manually designed.\n2.1 Perceiving Emotions from Body Expressions\nStudies in psychology show that body expressions, including ges-\ntures, are better suited than facial and vocal cues to express and\nperceive emotions varying in arousal and dominance, such as anger,\nrelief, fear, and pride [15, 22]. Body expressions are also useful for\ndisambiguating between pairs of emotions such as fear or anger [43],\nand fear or happiness [63]. Follow-up studies in affective comput-\ning [5, 11, 28, 31] have identified sets of biomechanical features\nfrom body expressions, known as affective features, on which hu-\nman observers focus when perceiving these different emotions from\ngestures. For example, rapid arm swings can indicate anger, an\nexpanded upper body can indicate pride, and slouching shoulders\ncan indicate fear or sadness.\nIn our work, we use such affective features observable from\ngestures to emote our generated virtual agents.\n2.2 Generating Emotive Virtual Agents\nCurrent approaches to endow virtual agents with emotional expres-\nsiveness make use of a number of modalities, including verbal com-\nmunication [13, 60], face movements [18, 29], body gestures [27],\nand gaits [55]. In the context of generating emotional expressions\naligned with speech, Chuah et al. [14] leveraged a dataset of words\nmapped to emotive facial expressions to generate virtual agents\nwith basic emotions automatically. DeVault et al. [16] developed\na full-fledged virtual human counselor, using a pre-built corpus\nof mappings between mental states and body expressions to make\ntheir virtual agent appropriately expressive. In contrast to these\napproaches, we build a generalizable data-driven mapping to body\ngestures from a more diverse range of intended emotions associ-\nated with text transcripts, such that we can generate appropriately\nexpressive gestures for out-of-dataset text sentences.\n2.3 Generating Gestures Aligned with Speech and Text\nThere has been extensive deep-learning-based work on generating\nhuman body gestures that align with speech content in the recent\npast [12]. Levine et al. [36] used a hidden Markov model to learn\nlatent mappings between speech and gestures. Hasegawa et al. [23]\nused recurrent neural networks to predict 3D pose sequences for\ngestures from input speech. More recently, Kucherenko et al. [33]\ntrained autoencoders to learn latent representations for the speech\nand the gesture data and then learned mappings between the two\nto generate gestures that are less sensitive to noise in the training\ndata. By contrast, Alexanderson et al. [2] learned invertible sub\ntransformations between speech and gesture spaces to stochastically\ngenerate a set of best-fitting gestures corresponding to the speech.\nOther approaches have also incorporated individual styles into ges-\ntures [20], added multiple adversarial losses to make the generated\ngestures look more realistic [19], and even added prototypical rule-\nbased behaviors such as head nods and hand waves based on the\ndiscourse [58]. These have culminated into works such as gener-\nating gestures for multiple speakers through style-transfer [1], and\nsemantic-aware gesture generation from speech [34].\nOur approach is complementary to these approaches in that we\nlearn mappings from the text transcripts of speech to gestures. This\neliminates the noise in speech signals and helps us focus only on the\nrelevant content and context. Learning from the text also enables us\nto focus on a broader range of gestures, including iconic, deictic, and\nmetaphoric gestures [42]. Our work is most closely related to that\nof Yoon et al. [69]. They learn upper body gestures as PCA-based,\nlow-dimensional pose features, corresponding to text transcripts\nfrom a dataset of TED-talk videos, then heuristically map these 3D\ngestures to an NAO robot. They have also followed up this work by\ngenerating upper-body gestures aligned with the three modalities of\nspeech, text transcripts, and person identity [68]. On the other hand,\nwe learn to map text transcripts to 3D pose sequences corresponding\nto semantic-aware, full-body gestures of more human-like virtual\nagents using an end-to-end trainable transformer network and blend\nin emotional expressiveness.\n2.4 Generating Stylistic Human Body Motions\nGenerating speech- or text-aligned gestures with emotional expres-\nsiveness can be considered a sub-problem in generating stylistic\nhuman body motions, including facial motions, head motions, and\nlocomotion. Existing approaches on face motions include gener-\nating lip movements and other face-muscle motions aligned with\nspeech, using either recurrent neural networks [62] or convolutional\nnetworks [18]. Methods for generating head motions that convey\nthe pace and intensity of speech have explored neural network ar-\nchitectures based on autoencoders [21] and generative adversarial\nnetworks [57]. Methods to generate stylistic locomotion are based\non convolutional networks [26], parametric phase functions [25],\nand deeply learned phase functions [61] for different styles of walk-\ning. Recent approaches have also incorporated gait-based affective\nfeatures to generate emotionally expressive walking [7, 8, 53]. More-\nover, there has been considerable progress in generating images and\nvideos of body motions based on textual descriptions of moments\nand actions [38, 70].\nIn contrast, we aim to generate emotionally expressive gestures\nat interactive rates that correspond to text sentences. The space of\ngesture motions we explore is also different from the space of mo-\ntions corresponding to locomotion, head motions, or facial muscle\nmotions. Although there is some overlap with the space of head\nFigure 1: Directed pose graph.Our pose graph is a directed tree\nconsisting of 23 joints, with the root joint as the root node of the tree,\nand the end-effector joints (head, wrists, toes) as the leaf nodes of\nthe tree. We manipulate the appropriate joints to generate emotive\ngestures.\nmotions [21,57], the corresponding methods have not been extended\nto deal with full-body motions.\n3 T RANSFORMING TEXT TO GESTURES\nGiven a natural language text sentence associated with an acting\ntask of narration or conversation, an intended emotion, and attributes\nof the virtual agent, including gender and handedness, our goal\nis to generate the virtual agent’s corresponding body gestures. In\nother words, we aim to generate a sequence of relative 3D joint\nrotations Q∗ underlying the poses of a virtual agent, corresponding\nto a sequence of input words W , and subject to the acting task A and\nthe intended emotion E based on the text, and the gender G and the\nhandedness H of the virtual agent. We therefore have\nQ∗ = argmax\nQ\nProb[Q|W ;A, E, G, H]. (1)\n3.1 Representing Text\nFollowing standard practices in NLP tasks, we represent\nthe word at each position s in the input sentence W =\n[w1 . . . ws . . . wTsen ], with Tsen being the maximum sentence\nlength, using word embeddings ws ∈ R300. We obtain the word em-\nbeddings using the GloVe model pre-trained on the Common Crawl\ncorpus [52]. We opt for GloVe based on our preliminary experi-\nments, where it marginally outperformed other similar-dimensional\nembedding models such as Word2Vec [45] and FastText [10], and\nhad similar performance as much higher dimensional embedding\nmodels, e.g., BERT [17]. We demarcate the start and the end of\nsentences using special start of sequence (SoS) and end of sequence\n(EoS) vectors that are pre-defined by GloVe.\n3.2 Representing Gestures\nFollowing prior works on human motion generation [51], we rep-\nresent a gesture as a sequence of poses or configurations of the\n3D body joints. These include body expressions as well as pos-\ntures. We represent each pose with quaternions denoting 3D ro-\ntations of each joint relative to its parent in the directed pose\ngraph (Fig. 1). Specifically, at each time step t in the sequence\nQ =\n\u0002q1 . . . qt . . . qTges\n\u0003\n, with Tges being the maximum ges-\nture length, we represent the pose using flattened vectors of unit\nquaternions qt =\nh\n. . . q⊤\nj,t . . .\ni⊤\n∈ HJ. Each set of 4 entries in\nthe flattened vector qt , represented as qj,t , is the rotation on joint\nj relative to its parent in the directed pose graph, and J is the total\nnumber of joints. We choose quaternions over other representations\nto represent rotations as quaternions are free of the gimbal lock\nproblem [51]. To demarcate the start and the end of each gesture se-\nquence, we define our start-of-sequence (SoS) and end-of-sequence\n(EoS) poses. Both of these are idle sitting poses with decorative\nchanges in the positions of the end-effector joints, the root, wrists\nand the toes.\n3.3 Representing the Agent Attributes\nWe categorize the agent attributes into two types: attributes depend-\ning on the input text and attributes depending on the virtual agent.\n3.3.1 Attributes Depending on Text\nIn this work, we consider two attributes that depend on text, the\nacting task, and the intended emotion.\nActing Task We consider two acting tasks, narration and con-\nversation. In narration, the agent narrates lines from a story to a\nlistener. The gestures, in this case, are generally more exaggerated\nand theatrical. In conversation, the agent uses body gestures to\nsupplement the words spoken in conversation with another agent or\nhuman. The gestures are subtler and more reserved. In our formu-\nlation, we represent the acting task as a two-dimensional one-hot\nvector A ∈ {0, 1}2, to denote either narration or conversation.\nIntended Emotion We consider each text sentence to be as-\nsociated with an intended emotion, given as a categorical emotion\nterm such as joy, anger, sadness, pride, etc. While the same text\nsentence can be associated with multiple emotions in practice, in\nthis work, we limit ourselves to sentences associated with only one\nemotion, owing primarily to the limitations in the dataset available\nfor training. We use the NRC-V AD lexicon [48] to transform these\ncategorical emotions associated with the text to the V AD space.\nThe V AD space [44] is a well-known representation in affective\ncomputing to model emotions. It maps an emotion as a point in a\nthree-dimensional space spanned by valence (V), arousal (A), and\ndominance (D). Valence is a measure of the pleasantness in the\nemotion (e.g., happy vs. sad), arousal is a measure of how active or\nexcited the subject expressing the emotion is (e.g., angry vs. calm),\nand dominance is a measure of how much the subject expressing the\nemotion feels “in control” of their actions (e.g., proud vs. remorse-\nful). Thus, in our formulation, the intended emotion E ∈ [0, 1]3,\nwhere the values are coordinates in the normalized V AD space.\n3.3.2 Attributes Depending on the Agent\nWe consider two attributes that depend on the agent to be animated,\nits gender G, and handedness H. In our work, gender G ∈ {0, 1}2 is\nlimited to a one-hot representation denoting either female or male,\nand handedness H ∈ {0, 1}2 is a one-hot representation indicating\nwhether the agent is left-hand dominant or right-hand dominant.\nMale and female agents typically have differences in body struc-\ntures (e.g., shoulder-to-waist ratio, waist-to-hip ratio). Handedness\ndetermines which hand dominates, especially when gesticulating\nwith one hand (e.g., beat gestures, deictic gestures). Each agent has\nexactly one assigned gender and one assigned handedness.\n3.4 Using the Transformer Network\nModeling the input text and output gestures as sequences shown in\nSecs. 3.1 and 3.2, the optimization in Eq. 1 becomes a sequence\ntransduction problem. We, therefore, approach this problem using\na transformer-based network. We briefly revisit the transformer as\noriginally introduced by Vaswani et al. [64], and describe how we\nmodify it for our transduction problem.\nThe transformer network follows the traditional encoder-decoder\narchitecture for sequence-to-sequence modeling. However, instead\nof using sequential chains of recurrent memory networks, or the\ncomputationally expensive convolutional networks, the transformer\nuses a multi-head self-attention mechanism to model the dependen-\ncies between the elements at different temporal positions in the input\nand target sequences.\nThe attention mechanism is represented as a sum of values from\na dictionary of key-value pairs, where the weight or attention on\neach value is determined by the relevance of the corresponding key\nFigure 2: Text2Gestures Network.Our network takes in sentences\nof natural language text and transforms them to word embeddings\nusing the pre-trained GloVe model [52]. It then uses a transformer\nencoder to transform the word embeddings to latent representations,\nappends the agent attributes to these latent representations, and\ntransforms the combined representations into encoded features. The\ntransformer decoder takes in these encoded features and the past\ngesture history to predict gestures for the subsequent time steps. At\neach time step, we represent the gesture by the set of rotations on all\nthe body joints relative to their respective parents in the pose graph at\nthat time step.\nto a given query. Thus, given a set of m queries Q ∈ Rm×k, a set of\nn keys K ∈ Rn×k, and the corresponding set of n values V ∈ Rn×v\n(for some dimensions k and v), and using the scaled dot-product as a\nmeasure of relevance, we can write,\nAtt(Q, K,V ) =softmax\n \nQK⊤\nk\n!\nV, (2)\nwhere the softmax is used to normalize the weights. In the case of\nself-attention (SA) in the transformer,Q, K, and V all come from the\nsame sequence. In the transformer encoder, the self-attention oper-\nates on the input sequence W . Since the attention mechanism does\nnot respect the relative positions of the elements in the sequence,\nthe transformer network uses a positional encoding scheme to sig-\nnify the position of each element in the sequence, prior to using the\nattention. Also, in order to differentiate between the queries, keys,\nand values, it projects W into a common space using three inde-\npendent fully-connected layers consisting of trainable parameters\nWQ,enc, WK,enc, and WV,enc. Thus, we can write the self-attention in\nthe encoder, SAenc, as\nSAenc (W ) =softmax\n \nW WQW⊤\nK W ⊤\nk\n!\nW WV . (3)\nThe multi-head (MH) mechanism enables the network to jointly\nattend to different projections for different parts in the sequence, i.e.,\nMH(W ) =concat\n\u0000\nSAenc,1 (W ), . . . ,SAenc,h (W )\n\u0001\nWconcat, (4)\nwhere h is the number of heads, Wconcat is the set of trainable pa-\nrameters associated with the concatenated representation, and each\nself-attention i in the concatenation consists of its own set of train-\nable parameters WQ,i, WK,i, and WV,i.\nThe transformer encoder then passes the MH output through two\nfully-connected (FC) layers. It repeats the entire block consisting\nof (SA–MH–FC) N times and uses the residuals around each layer\nin the blocks during backpropagation. We denote the final encoded\nrepresentation of the input sequence W as FW .\nTo meet the given constraints on the acting task A, intended\nemotion E, gender G, and handedness H of the virtual agent, we\nappend these variables to FW and pass the combined representation\nthrough two fully-connected layers with trainable parameters WFC\nto obtain feature representations\n¯FW = FC\n\u0010\u0002\nF⊤\nW A⊤ E⊤ G⊤ H⊤\u0003⊤ ;WFC\n\u0011\n. (5)\nThe transformer decoder operates similarly using the target se-\nquence Q, but with some important differences. First, it uses a\nmasked multi-head (MMH) self-attention on the sequence, such that\nthe attention for each element covers only those elements appearing\nbefore it in the sequence, i.e.,\nMMH(Q) =concat\n\u0000\nSAdec,1 (Q), . . . ,SAdec,h (Q)\n\u0001\nWconcat. (6)\nThis ensures that the attention mechanism is causal and therefore\nusable at test time, when the full target sequence is not known apriori.\nSecond, it uses the output of the MMH operation as the key and\nthe value, and the encoded representation ¯FW as the query, in an\nadditional multi-head self-attention layer without any masking, i.e.,\nMH( ¯FW,Q) =concat\n\nAttdec,1( ¯FW,MMH(Q),MMH(Q)), . . .| {z }\nhentries\n\nWconcat. (7)\nIt then passes the output of this multi-head self-attention through\ntwo fully-connected layers to complete the block. Thus, one block\nof the decoder is (SA–MMH–SA–MH–FC), and the transformer\nnetwork uses N such blocks. It also uses positional encoding of the\ntarget sequence upfront and uses the residuals around each layer in\nthe blocks during backpropagation.\n4 T RAINING THE TRANSFORMER -BASED NETWORK\nFig. 2 shows the overall architecture of our transformer-based net-\nwork. The word embedding layer transforms the words into feature\nvectors using the pre-trained GloVe model. The encoder and the\ndecoder respectively consist of N = 2 blocks of (SA–MH–FC) and\n(SA–MMH–SA–MH–FC). We use h = 2 heads in the multi-head\nattention. The set of FC layers in each of the blocks maps to 200-dim\noutputs. At the output of the decoder, we normalize the predicted\nvalues so that they represent valid rotations. We train our network\nusing the sum of three losses: the angle loss, the pose loss, and\nthe affective loss. We compute these losses between the gesture se-\nquences generated by our network and the original motion-captured\nsequences available as ground-truth in the training dataset.\n4.1 Angle Loss for Smooth Motions\nWe denote the ground-truth relative rotation of each joint j at time\nstep t as the unit quaternion qj,t , and the corresponding rotation\npredicted by the network as ˆqj,t . If needed, we correct ˆqj,t to have\nthe same orientation asqj,t . Then we measure the angle loss between\neach such pair of rotations as the squared difference of their Euler\nangle representations, modulo π. We use Euler angles rather than the\nquaternions in the loss function as it is straightforward to compute\ncloseness between Euler angles using Euclidean distances. To ensure\nthat the motions look smooth and natural, we also consider the\nsquared difference between the derivatives of the ground-truth and\nthe predicted rotations, computed at successive time steps. We write\nthe net angle loss Lang as\nLang =∑\nt\n∑\nj\n\u0000\nEul\n\u0000\nqj,t\n\u0001\n−Eul\n\u0000\nˆqj,t\n\u0001\u00012 +\n\u0000\nEul\n\u0000\nqj,t\n\u0001\n−Eul\n\u0000\nqj,t−1\n\u0001\n−Eul\n\u0000\nˆqj,t\n\u0001\n+Eul\n\u0000\nˆqj,t−1\n\u0001\u00012 .\n(8)\n4.2 Pose Loss for Joint Trajectories\nThe angle loss only penalizes the absolute differences between the\nground-truth and the predicted joint rotations and does not explicitly\nconstrain the resulting poses to follow the same trajectory as the\nFigure 3: Variance in emotive gestures.Emotions with high arousal\n(e.g., amused) generally have rapid limb movements, while emotions\nwith low arousal (e.g., sad) generally have slow and subtle limb move-\nments. Emotions with high dominance ( e.g., proud) generally have\nan expanded upper body and spread arms, while emotions with low\ndominance (e.g., afraid) have a contracted upper body and arms close\nto the body. Our algorithm uses these characteristics to generate the\nappropriate gestures.\nground-truth at all time steps. To this end, we compute the squared\nnorm difference between the ground-truth and the predicted joint\npositions at all time steps. Given the relative joint rotations and the\noffset oj of every joint j from its parent, we can easily compute all\nthe joint positions using forward kinematics (FK). Thus, we write\nthe pose loss Lpose as\nLpose = ∑\nt\n∑\nj\n∥FK\n\u0000\nqj,t , oj\n\u0001\n−FK\n\u0000\nˆqj,t , oj\n\u0001\n∥2. (9)\n4.3 Affective Loss for Emotive Gestures\nTo ensure that the generated gestures are emotionally expressive, we\nalso penalize the loss between the gesture-based affective features of\nthe ground-truth and the predicted poses. Prior studies in affective\ncomputing [11, 22, 28] show that gesture-based affective features\nare good indicators of emotions that vary in arousal and dominance.\nEmotions with high dominance, such as pride, anger, and joy, tend to\nbe expressed with an expanded upper body, spread arms, and upright\nhead positions. Conversely, emotions with low dominance, such\nas fear and sadness, tend to be expressed with a contracted upper\nbody, arms close to the body, and collapsed head positions. Again,\nemotions with high arousal, such as anger and amusement, tend to be\nexpressed with rapid arm swings and head movements. By contrast,\nemotions with low arousal, such as relief and sadness, tend to be\nexpressed with subtle, slow movements. Different valence levels are\nnot generally associated with consistent differences in gestures, and\nhumans often infer from other cues and the context. Fig. 3 shows\nsome gesture snapshots to visualize the variance of these affective\nfeatures for different levels of arousal and dominance.\nWe define scale-independent affective features using angles, dis-\ntance ratios, and area ratios for training our network, following the\nsame rationale as in [7]. Since, in our experiments, the virtual agent\nis sitting down, and only the upper body is expressive during the\ngesture sequences, only the joints at the root, neck, head, shoulders,\nelbows, and wrists move significantly. Therefore, we use these joints\nto compute our affective features. We show the complete list of\naffective features we use in Fig. 4. Denoting the set of affective\nfeatures computed from the ground-truth and the predicted poses at\ntime t as at and ˆat respectively, we write the affective lossLaff as\nLaff = ∑\nt\n∥at − ˆat ∥2. (10)\nFigure 4: Gesture-based affective features.We use a total of 15\nfeatures: 7 angles, A1 through A7, 5 distance ratios, D1\nD4\n, D2\nD4\n, D8\nD5\n, D7\nD5\n,\nand D3\nD6\n, and 3 area ratios, R1\nR2\n, R3\nR4\n, and R5\nR6\n.\nCombining all the individual loss terms, we write our training\nloss functions L as\nL = Lang +Lpose +Laff +λ∥W∥, (11)\nwhere W denotes the set of all trainable parameters in the full net-\nwork, and λ is the regularization factor.\n5 R ESULTS\nThis section elaborates on the database we use to train, validate, and\ntest our method. We also report our training routine, the performance\nof our method compared to the ground-truth, and the current state-\nof-the-art method for generating gestures aligned with text input.\nWe also perform ablation studies to show the benefits of each of the\ncomponents in our loss function: the angle loss, the pose loss, and\nthe affective loss.\n5.1 Data for Training, Validation and Testing\nWe evaluate our method on the MPI emotional body expressions\ndatabase [65]. This database consists of 1,447 motion-captured\nsequences of human participants performing one of three acting\ntasks: narrating a sentence from a story, gesticulating a scenario\ngiven as a sentence, or gesticulating while speaking a line in a con-\nversation. Each sequence corresponds to one text sentence and the\nassociated gestures. For each sequence, the following annotations of\nthe intended emotion E, gender G, and handedness H, are available:\n• E as the V AD representation for one of “afraid”, “amused”,\n“angry”, “ashamed”, “disgusted”, “joyous”, “neutral”, “proud”,\n“relieved”, “sad”, or “surprised”,\n• G is either female or male, and\n• H is either left or right.\nEach sequence is captured at 120 fps and is between 4 and 20\nseconds long. We pad all the sequences with our EoS pose (Sec. 3.2)\nso that all the sequences are of equal length. Since the sequences\nfreeze at the end of the corresponding sentences, padding with the\nEoS pose often introduces small jumps in the joint positions and the\ncorresponding relative rotations when any gesture sequence ends.\nTo this end, we have designed our training loss function (Eq. 11) to\nensure smoothness and generate gestures that transition smoothly to\nthe EoS pose after the end of the sentence.\n5.2 Training and Evaluation Routines\nWe train our network using the Adam optimizer [30] with a learning\nrate of 0.001 and a weight decay of 0.999 at every epoch. We\ntrain our network for 600 epochs, using a stochastic batch size of 16\nwithout replacement in every iteration. We have a total of 26,264,145\ntrainable parameters in our network. We use 80% of the data for\ntraining, validate the performance on 10% of the data, and test on\nTable 1: Mean pose errors.For each listed method, this is the mean\nEuclidean distance of all the joints over all the time steps from all the\nground-truth sequences over the entire test set. The mean error for\neach sequence is computed relative to the mean length of the longest\ndiagonal of the 3D bounding box of the virtual agent in that sequence.\nMethod Mean pose error\nYoon et al. [69] 1.57\nOur method, no angle loss 0.07\nOur method, no pose loss 0.06\nOur method, no affective loss 0.06\nOur method, all losses 0.05\nthe remaining 10% of the data. The total training takes around 8\nhours using an Nvidia GeForce GTX 1080Ti GPU. At the time of\nevaluation, we initialize the transformer decoder withT = 20 (Fig. 2)\ntime steps of the SoS pose and keep using the pastT = 20 time steps\nto generate the gesture at every time step.\n5.3 Comparative Performance\nWe compare the performance of our network with the transformer-\nbased text-to-gesture generation network of Yoon et al. [69] because\nthis method is the closest to our work. To make a fair comparison,\nwe perform the following as per their original paper:\n• use the eight upper body joints (three each on the two arms,\nneck, and head) for their method,\n• use PCA to reduce the eight upper body joints to 10-\ndimensional features,\n• retrain their network on the MPI emotional body expressions\ndatabase [65], using the same data split as in our method, and\nthe hyperparameters provided by the authors,\n• compare the performances only on the eight upper body joints.\nWe report the mean pose error from the ground-truth sequences\nover the entire held-out test set for both Yoon et al. [69] and our\nmethod in Table 1. For each test sequence and each method, we\ncompute the total pose error for all the joints at each time step and\ncalculate the mean of these errors across all time steps. We then\ndivide the mean error by the mean length of the longest diagonal\nof the 3D bounding box of the virtual agent to get the normalized\nmean error. To obtain the mean pose error for the entire test set, we\ncompute the mean of the normalized mean errors for all the test se-\nquences. We also plot the trajectories of the three end-effector joints\nin the upper body, head, left wrist, and right wrist, independently in\nthe three coordinate directions, for two diverse sample sequences\nfrom the test set in Fig. 5. We ensure diversity in the samples by\nchoosing a different combination of the gender, handedness, acting\ntask, and intended emotion of the gesture for each sample.\nWe observe from Table 1 that our method reduces the mean\npose error by around 97% over Yoon et al. [69]. From the plots\nin Fig. 5, we can observe that unlike our method, Yoon et al.’s\nmethod is unable to generate the high amplitude oscillations in\nmotion, leading to larger pose errors. This is because their lower-\ndimensional representation of pose motions does not sufficiently\ncapture the oscillations. Moreover, the gestures generated by Yoon\net al.’s method did not produce any movements in thez-axis. Instead,\nthey confined the movements to a particular z-plane. The step in\ntheir method in the z-axis occurs when the gesture returns to the EoS\nrest pose, which is in a different z-plane.\n5.4 Ablation Studies\nWe compare the performance between different ablated versions\nof our method. We test the contribution of each of the three loss\nterms, angle loss, pose loss, and affective loss, in Eq. 11 by removing\nthem from the total loss one at a time and training our network from\nscratch with the remaining losses. Each of these ablated versions\nhas a higher mean pose error over the entire test set than our actual\nmethod, as we report in Table 1. To visualize the performance\ndifferences, we show in Fig. 5 sample end-effector trajectories in\nFigure 5: End-effector trajectories. The trajectories in the three\ncoordinate directions for the head and two wrists. We show two\nsample sequences from the test set, as generated by all the methods.\nRemoving the angle loss makes the trajectory heavily jerky. Removing\nthe pose loss makes our method unable to follow the desired trajectory.\nRemoving the affective loss reduces the variations corresponding to\nemotional expressiveness. Y oon et al.’s method [69] is unable to\ngenerate large amplitude variations in the trajectories because it\nworks with a dimension-reduced representation of the sequences.\nthe same setup as described in Sec. 5.3. We also show snapshots\nfrom the two sample gesture sequences generated by all the ablated\nversions in Fig. 6. We show the full gesture sequences of these and\nother samples in our supplementary video.\nWe can observe from Fig. 5 that the gestures become heavily jerky\nwithout the angle loss. When we add in the angle loss but remove\nthe pose loss, the gestures become smoother but still have some\njerkiness. This shows that the pose loss also lends some robustness\nto the generation process. The other major drawback in removing\neither the angle or the pose loss is that the network can only change\nthe gesture between time steps within some small bounds, making\nthe overall animation sequence appear rigid and constricted.\nWhen we remove only the affective loss from Eq. 11, the network\ncan generate a wide range of gestures, leading to animations that\nappear fluid and plausible. However, the emotional expressions in\nthe gestures, such as spreading and contracting the arms and shaking\nthe head, are not consistent with the intended emotions.\n5.5 Interfacing the VR Environment\nGiven a sentence of text, we can generate the gesture animation files\nat an interactive rate of 3.2 ms per frame, or 312.5 frames per second,\non average on an Nvidia GeForce GTX 1080Ti GPU.\nWe use gender and handedness to determine the virtual agent’s\nphysical attributes during the generation of gestures. Gender impacts\nthe pose structure. The handedness determines the hand for one-\nhanded or longitudinally asymmetrical gestures. To create the virtual\nagents, we use low-poly humanoid meshes with no textures on the\nface. We use the pre-defined set of male and female skeletons in the\nMPI emotional body motion database [65] for the gesture animations.\nWe assign a different model to each of these skeletons, matching\nFigure 6: Ablation studies.Snapshots of gestures at five time steps\nfrom two sample ground-truth sequences in the test set, and the\ngestures at the same five time steps as generated by our method and\nits different ablated versions. The full sequences of these gestures\nare available in our supplementary video.\ntheir genders. We manually correct any visual distortions caused\nby a shape mismatch between the pre-defined skeletons and the\nlow-poly meshes.\nWe use Blender 2.7 to rig the generated animations to the hu-\nmanoid meshes. To ensure a proper rig, we modify the rest pose\nof the humanoid meshes to match the rest pose of our pre-defined\nskeletons. To make the meshes appear more life-like, we add peri-\nodic blinking and breathing movements to the generated animations\nusing blendshapes in Blender.\nWe prepare our VR environment using Unreal 4.25. We place the\nvirtual agents on a chair in the center of the scene in full focus. The\nusers can interact with the agent in two ways. They can either select\na story that the agent narrates line by line using appropriate body\ngestures or send lines of text as part of a conversation to which the\nagent responds using text and associated body gestures. We show the\nfull demos in our supplementary video. We use synthetic, neutral-\ntoned audio aligned with all our generated gestures to understand\nthe timing of the gestures with the text. However, we do not add\nany facial features or emotions in the audio for the agents since they\nare dominant modalities of emotional expression and make a fair\nevaluation of the emotional expressiveness of the gestures difficult.\nFor example, if the intended emotion is happy, and the agent has a\nsmiling face, observers are more likely to respond favorably to any\ngesture with high valence or arousal.\n6 U SER STUDY\nWe conduct a web-based user study to test two major aspects of\nour method: the correlation between the intended and the perceived\nemotions of and from the gestures, and the quality of the animations\ncompared to the original motion-captured sequences.\n6.1 Procedure\nThe study consisted of two sections and was about ten minutes\nlong. In the first section, we showed the participant six clips of\nvirtual agents sitting on a chair and performing randomly selected\ngesture sequences generated by our method, one after the other. We\nthen asked the participant to report the perceived emotion as one\nof multiple choices. Based on our pilot study, we understood that\nasking participants to choose from one of 11 categorical emotions in\nthe EBEDB dataset [65] was overwhelming, especially since some\nof the emotion terms were close to each other in the V AD space\n(e.g., joyous and amused). Therefore, we opted for fewer choices\nto make it easier for the participants and reduce the probability of\nhaving too many emotion terms with similar V AD values in the\nchoices. For each sequence, we, therefore, provided the participant\nwith four choices for the perceived emotion. One of the choices\nwas the intended emotion, and the remaining three were randomly\nselected. For each animation, randomly choosing three choices can\nTable 2: Likert scale markers to asses quality of gestures. We\nuse the following markers in our five-point Likert scale\nVery Unnatural e.g., broken arms or legs, torso at an impossible angle\nNot Realistic e.g., limbs going inside the body or through the chair\nLooks OK No serious problems, but does not look very appealing\nLooks good No problems and the gestures look natural\nLooks great! The gestures look like they could be from a real person\nunintentionally bias the participant’s response (for instance, if the\nintended emotion is “sad” and the random options are “joyous”,\n“amused” and “proud”). However, the probability of such a set of\nchoices drops exponentially as we consider multiple sequences for\neach participant and multiple participants in the overall study.\nIn the second section, we showed the participant three clips of\nvirtual agents sitting on a chair and performing a randomly selected\noriginal motion-captured sequence and three clips of virtual agents\nperforming a randomly selected generated gesture sequence, one\nafter the other. We showed the participant these six sequences in\nrandom order. We did not tell the participant which sequences\nwere from the original motion-capture and which sequences were\ngenerated by our method. We asked the participant to report the\nnaturalness of the gestures in each of these sequences on a five-point\nLikert scale, consisting of the markers mentioned in Table 2.\nWe had a total of 145 clips of generated gestures and 145 clips of\nthe corresponding motion-captured gestures. For every participant,\nwe chose all the 12 random clips across the two sections without\nreplacement. We did not notify the participant apriori which clips\nhad motion-captured gestures and which clips had our generated\ngestures. Moreover, we ensured that in the second section, none of\nthe three selected generated gestures corresponded to the three se-\nlected motion-captured gestures. Thus, all the clips each participant\nlooked at were distinct. However, we did repeat clips at random\nacross participants to get multiple responses for each clip.\n6.2 Participants\nFifty participants participated in our study, recruited via web ad-\nvertisements. To study the demographic diversity, we asked the\nparticipants to report their gender and age group. Based on the statis-\ntics, we had 16 male and 11 female participants in the age group\nof 18-24, 15 male and seven female participants in the age group\nof 25-34, and one participant older than 35 who preferred not to\ndisclose their gender. However, we did not observe any particular\npattern of responses based on the demographics.\n6.3 Evaluation\nWe analyze the correlation between the intended and the perceived\nemotions from the first section of the user study and the reported\nquality of the animations from the second section. We also summa-\nrize miscellaneous user feedback.\n6.3.1 Correlation between Intended and Perceived Emotions\nEach participant responded to six random sequences in the first\nsection of the study, leading to a total of 300 responses. We convert\nthe categorical emotion terms from these responses to the V AD space\nusing the mapping of NRC-V AD [48]. We show the distribution\nof the valence, arousal, and dominance values of the intended and\nperceived emotions in Fig. 7.\nWe compute the Pearson correlation coefficient between the in-\ntended and perceived values in each of the valence, arousal, and\ndominance dimensions. A Pearson coefficient of 1 indicates maxi-\nmum positive linear correlation, 0 indicates no correlation, and -1\nindicates maximum negative linear correlation. In practice, any co-\nefficient larger than 0.5 indicates a strong positive linear correlation.\nWe hypothesize that intended and the perceived values in all three\ndimensions have such a strong positive correlation.\nWe observe a Pearson coefficient of 0.77, 0.95, and 0.82, re-\nspectively, between the intended and the perceived values in the\nvalence, arousal, and dominance dimensions. Thus, the values in all\nFigure 7: Valence, arousal, and dominance distributions.Dis-\ntribution of values from the intended and perceived emotions in the\nvalence, arousal, and dominance dimensions for gestures in the study.\nAll the distributions indicate strong positive correlation between the\nintended and the perceived values, with the highest correlation in\narousal and the lowest in valence.\nthree dimensions are strongly positively correlated, satisfying our\nhypothesis. The values also indicate that the correlation is stronger\nin the arousal and the dominance dimensions and comparatively\nweaker in the valence dimension. This is in line with prior studies\nin affective computing [22, 28], which show that humans can con-\nsistently perceive arousal and dominance from gesture-based body\nexpressions.\n6.3.2 Quality of Gesture Animations\nEach participant responded to three random motion-captured and\nthree randomly generated sequences in the second section of the\nstudy. Therefore, we have a total of 150 responses on both the\nmotion-captured and the generated sequences. We summarize the\npercentage of responses of each of the five points in the Likert scale\nin Fig. 8. We consider a minimum score of 3 on our Likert scale to in-\ndicate that the participant found the corresponding gesture plausible.\nBy this criterion, we observe that 86.67% of the responses indicated\nthe virtual agents performing the motion-captured sequences have\nplausible gestures and 91.33% of the responses the virtual agents\nperforming the generated sequences have plausible gestures. In fact,\nwe observe that a marginally higher percentage of responses scored\nthe generated gestures 4 and 5 (2.00% and 3.33% respectively),\ncompared to the percentage of responses with the same score for the\nmotion-captured gestures. This, coupled with the fact that partici-\npants did not know apriori which sequences were motion-captured\nand generated, indicates that our generated sequences were perceived\nto be as realistic as the original motion-captured sequences. One\npossible explanation of participants rating our generated gestures\nmarginally more plausible than the motion-captured gestures is that\nour generated poses return smoothly to a rest pose after the end\nof the sentence. The motion-captured gestures, on the other hand,\nfreeze at the end-of-the-sentence pose.\n6.3.3 Miscellaneous Feedback\nOur virtual agents only express emotions through gestures and do\nnot use any other modalities such as faces or voices. Therefore, we\nexpected some participants taking the study to be distracted by the\nlack of emotions on the face or to be unable to determine the emo-\ntions based only on the gestures, without supporting cues from the\nother modalities. Indeed, 14% of the participants reported they were\ndistracted by the lack of facial emotions, 10% were unable to deter-\nmine the emotions based on only the gestures, and 8% experienced\nboth difficulties.\n7 C ONCLUSION\nWe present a novel method that takes in natural language text one\nsentence at a time and generates 3D pose sequences for virtual\nagents corresponding to emotive gestures aligned with that text.\nOur generative method also considers the intended acting task of\nnarration or conversation, the intended emotion based on the text\nand the context, and the intended gender and handedness of the\nvirtual agents to generate plausible gestures. We can generate these\ngestures in a few milliseconds on an Nvidia GeForce GTX 1080Ti\nGPU. We also conducted a web study to evaluate the naturalness and\nFigure 8: Responses on the Quality of Gestures.A small fraction of\nparticipants responded to the few gesture sequences that had some\nstray self-collisions, and therefore found these sequences to not be\nrealistic. The vast majority of the participants found both the motion-\ncaptured and generated gestures to look OK (plausible) on the virtual\nagents. A marginally higher percentage of participants reported that\nour generated gesture sequences looked better on the virtual agents\nthat the original motion-captured gesture sequences.\nemotional expressiveness of our generated gestures. Based on the\n600 total responses from 50 participants, we found a strong positive\ncorrelation between the intended emotions of the virtual agents’\ngestures and the emotions perceived from them by the respondents,\nwith a minimum Pearson coefficient of 0.77 in the valence dimension.\nMoreover, around 91% of the respondents found our generated\ngestures to be at least plausible on a five-point Likert Scale.\n8 L IMITATIONS AND FUTURE WORK\nOur work has some limitations. First, we train our network to learn\nmappings from complete text sentences to gestures. We can improve\nthis by exploring a more granular phrase-level mapping from text to\ngestures to gain insights on how gestures corresponding to parts of\nsentences can be combined to produce gestures for full sentences.\nSecond, our generated gestures return to the EoS pose after every\ngesticulating every sentence. This is because all the samples in\nthe EBEDB dataset [65] start from a rest pose. As a result, we\ncannot exploit any information related to the continuity between\ngestures that correspond to adjacent sentences. A simple method\nto extend this approach is to use the last window of the current\nsentence as an initialization for the next sentence. However, without\nany ground-truth information on the continuity between gesture, it\nis difficult to train or evaluate the transitioning gestures. As part of\nour future work, we plan to explore other techniques to enforce such\ncontinuity. Third, We only consider the V AD representation for the\ncategorical emotion terms associated with the texts. This simplifies\nour network design and the evaluation of the emotions perceived by\nthe participants in our study. In the future, we plan to explore the\ncorrelations between the V AD representations of words in the text\nand the associated categorical emotions. We also plan to study the\ninterrelations of these V AD representations with the gender, age, and\nethnicity of the subjects, to build more sophisticated maps from texts\nto a more diverse range of emotive gestures. We would also like to\nintegrate our emotive gesture generation algorithm with social VR\nsystems and use them for socially-aware conversational agents.\nLastly, we only consider text-based emotive gesture generation,\nbut no facial expression or expressive voice tones. In real-world\nscenarios, facial expressions and voice tones tend to play dominant\nroles in conveying the emotions and may occupy the user’s focus.\nConsequently, in our current video results and studies, we evaluated\nthe effectiveness of our gesture generation approach without any\nusing facial or vocal expressions, which is similar to other methods\nfor evaluating gestures [34, 68]. This way, we ensure that the user\nmainly focuses on emotive gestures. As part of future work, it\nwould be useful to combine our work with varying facial expressions\ncorresponding to different emotions and vary the emotional tone in\nvoices. Furthermore, we would like to evaluate the relative benefits\nof combining different modalities, such as emotive gestures, facial\nexpressions, and voice tones.\nREFERENCES\n[1] C. Ahuja, D. W. Lee, Y . I. Nakano, and L.-P. Morency. Style transfer\nfor co-speech gesture animation: A multi-speaker conditional-mixture\napproach. European Conference on Computer Vision, 2020.\n[2] S. Alexanderson, G. E. Henter, T. Kucherenko, and J. Beskow. Style-\ncontrollable speech-driven gesture synthesis using normalising flows.\nComputer Graphics Forum, 39(2):487–496, 2020. doi: 10.1111/cgf.\n13946\n[3] M. Argyle. Bodily communication. Routledge, 2013.\n[4] H. Aviezer, Y . Trope, and A. Todorov. Body cues, not facial expressions,\ndiscriminate between intense positive and negative emotions. Science,\n338(6111):1225–1229, 2012.\n[5] A. Banerjee, U. Bhattacharya, and A. Bera. Learning unseen emotions\nfrom gestures via semantically-conditioned zero-shot perception with\nadversarial autoencoders. arXiv preprint arXiv:2009.08906, 2020.\n[6] T. Baur, I. Damian, P. Gebhard, K. Porayska-Pomsta, and E. Andr´e. A\njob interview simulation: Social cue-based interaction with a virtual\ncharacter. In 2013 International Conference on Social Computing, pp.\n220–227, 2013.\n[7] U. Bhattacharya, T. Mittal, R. Chandra, T. Randhavane, A. Bera, and\nD. Manocha. Step: Spatial temporal graph convolutional networks for\nemotion perception from gaits. In Proceedings of the Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI’20, p. 1342–1350.\nAAAI Press, 2020.\n[8] U. Bhattacharya, N. Rewkowski, P. Guhan, N. L. Williams, T. Mittal,\nA. Bera, and D. Manocha. Generating emotive gaits for virtual agents\nusing affect-based autoregression. In 2020 IEEE International Sympo-\nsium on Mixed and Augmented Reality (ISMAR), pp. 24–35, 2020. doi:\n10.1109/ISMAR50242.2020.00020\n[9] U. Bhattacharya, C. Roncal, T. Mittal, R. Chandra, K. Kapsaskis,\nK. Gray, A. Bera, and D. Manocha. Take an emotion walk: Perceiving\nemotions from gaits using hierarchical attention pooling and affective\nmapping. In A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, eds.,\nComputer Vision – ECCV 2020, pp. 145–163. Springer International\nPublishing, Cham, 2020.\n[10] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching word\nvectors with subword information. Transactions of the Association for\nComputational Linguistics, 5:135–146, 2017.\n[11] G. Castillo and M. Neff. What do we express without knowing? emo-\ntion in gesture. In Proceedings of the 18th International Conference on\nAutonomous Agents and MultiAgent Systems, AAMAS ’19, p. 702–710.\nInternational Foundation for Autonomous Agents and Multiagent Sys-\ntems, Richland, SC, 2019.\n[12] C.-C. Chiu, L.-P. Morency, and S. Marsella. Predicting co-verbal\ngestures: A deep and temporal modeling approach. In Intelligent\nVirtual Agents, pp. 152–166. Springer International Publishing, Cham,\n2015.\n[13] A. Chowanda, P. Blanchfield, M. Flintham, and M. Valstar. Com-\nputational models of emotion, personality, and social relationships\nfor interactions in games: (extended abstract). In Proceedings of the\n2016 International Conference on Autonomous Agents and Multiagent\nSystems, AAMAS ’16, p. 1343–1344. International Foundation for\nAutonomous Agents and Multiagent Systems, Richland, SC, 2016.\n[14] J. H. Chuah, B. Rossen, and B. Lok. Automated generation of emotive\nvirtual humans. In Intelligent Virtual Agents, pp. 490–491. Springer\nBerlin Heidelberg, Berlin, Heidelberg, 2009.\n[15] B. De Gelder. Towards the neurobiology of emotional body language.\nNature Reviews Neuroscience, 7(3):242–249, 2006.\n[16] D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer,\nK. Georgila, J. Gratch, A. Hartholt, M. Lhommet, et al. Simsen-\nsei kiosk: A virtual human interviewer for healthcare decision support.\nIn Proceedings of the 2014 international conference on Autonomous\nagents and multi-agent systems, pp. 1061–1068, 2014.\n[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-\ntraining of deep bidirectional transformers for language understanding.\nIn Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186. As-\nsociation for Computational Linguistics, Minneapolis, Minnesota, June\n2019. doi: 10.18653/v1/N19-1423\n[18] Y . Ferstl and R. McDonnell. A perceptual study on the manipulation\nof facial features for trait portrayal in virtual agents. In Proceedings of\nthe 18th International Conference on Intelligent Virtual Agents, IV A\n’18, p. 281–288. Association for Computing Machinery, New York,\nNY , USA, 2018. doi: 10.1145/3267851.3267891\n[19] Y . Ferstl, M. Neff, and R. McDonnell. Multi-objective adversarial\ngesture generation. In Motion, Interaction and Games , MIG ’19.\nAssociation for Computing Machinery, New York, NY , USA, 2019.\ndoi: 10.1145/3359566.3360053\n[20] S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens, and J. Malik. Learn-\ning individual styles of conversational gesture. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2019.\n[21] D. Greenwood, S. Laycock, and I. Matthews. Predicting head pose\nfrom speech with a conditional variational autoencoder. ISCA, 2017.\n[22] M. M. Gross, E. A. Crane, and B. L. Fredrickson. Effort-shape and\nkinematic assessment of bodily expression of emotion during gait.\nHuman movement science, 31(1):202–221, 2012.\n[23] D. Hasegawa, N. Kaneko, S. Shirakawa, H. Sakuta, and K. Sumi. Evalu-\nation of speech-to-gesture generation using bi-directional lstm network.\nIn Proceedings of the 18th International Conference on Intelligent Vir-\ntual Agents, IV A ’18, p. 79–86. Association for Computing Machinery,\nNew York, NY , USA, 2018. doi: 10.1145/3267851.3267878\n[24] P. Heidicker, E. Langbehn, and F. Steinicke. Influence of avatar appear-\nance on presence in social vr. In 2017 IEEE Symposium on 3D User\nInterfaces (3DUI), pp. 233–234, 2017.\n[25] D. Holden, T. Komura, and J. Saito. Phase-functioned neural networks\nfor character control. ACM Transactions on Graphics (TOG), 36(4):42,\n2017.\n[26] D. Holden, J. Saito, and T. Komura. A deep learning framework for\ncharacter motion synthesis and editing. ACM Trans. Graph., 35(4),\nJuly 2016. doi: 10.1145/2897824.2925975\n[27] N. Jaques, D. J. McDuff, Y . L. Kim, and R. W. Picard. Understanding\nand predicting bonding in conversations using thin slices of facial\nexpressions and body language. In Intelligent Virtual Agents - 16th\nInternational Conference, IVA 2016, Los Angeles, CA, USA, September\n20-23, 2016, Proceedings, vol. 10011 of Lecture Notes in Computer\nScience, pp. 64–74, 2016. doi: 10.1007/978-3-319-47665-0\n[28] M. Karg, A. Samadani, R. Gorbet, K. K¨uhnlenz, J. Hoey, and D. Kuli´c.\nBody movements for affective expression: A survey of automatic\nrecognition and generation. IEEE Transactions on Affective Computing,\n4(4):341–359, 2013.\n[29] T. Karras, T. Aila, S. Laine, A. Herva, and J. Lehtinen. Audio-driven\nfacial animation by joint end-to-end learning of pose and emotion.\nACM Trans. Graph., 36(4), July 2017. doi: 10.1145/3072959.3073658\n[30] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014.\n[31] A. Kleinsmith and N. Bianchi-Berthouze. Affective body expression\nperception and recognition: A survey. IEEE Transactions on Affective\nComputing, 4(1):15–33, 2013.\n[32] M. L. Knapp, J. A. Hall, and T. G. Horgan. Nonverbal communication\nin human interaction. Cengage Learning, 2013.\n[33] T. Kucherenko, D. Hasegawa, G. E. Henter, N. Kaneko, and H. Kjell-\nstr¨om. Analyzing input and output representations for speech-driven\ngesture generation. In Proceedings of the 19th ACM International\nConference on Intelligent Virtual Agents, IV A ’19, p. 97–104. Associa-\ntion for Computing Machinery, New York, NY , USA, 2019. doi: 10.\n1145/3308532.3329472\n[34] T. Kucherenko, P. Jonell, S. van Waveren, G. E. Henter, S. Alexan-\ndersson, I. Leite, and H. Kjellstr ¨om. Gesticulator: A framework for\nsemantically-aware speech-driven gesture generation. ICMI ’20, p.\n242–250. Association for Computing Machinery, New York, NY , USA,\n2020. doi: 10.1145/3382507.3418815\n[35] M. E. Latoschik, D. Roth, D. Gall, J. Achenbach, T. Waltemate, and\nM. Botsch. The effect of avatar realism in immersive social virtual re-\nalities. In Proceedings of the 23rd ACM Symposium on Virtual Reality\nSoftware and Technology, VRST ’17. Association for Computing Ma-\nchinery, New York, NY , USA, 2017. doi: 10.1145/3139131.3139156\n[36] S. Levine, P. Kr¨ahenb¨uhl, S. Thrun, and V . Koltun. Gesture controllers.\nIn ACM SIGGRAPH 2010 Papers, SIGGRAPH ’10. Association for\nComputing Machinery, New York, NY , USA, 2010. doi: 10.1145/\n1833349.1778861\n[37] J. Li, R. Kizilcec, J. Bailenson, and W. Ju. Social robots and vir-\ntual agents as lecturers for video instruction. Computers in Human\nBehavior, 55:1222 – 1230, 2016. doi: 10.1016/j.chb.2015.04.005\n[38] Y . Li, M. R. Min, D. Shen, D. E. Carlson, and L. Carin. Video genera-\ntion from text. In Proceedings of the Thirty-Second AAAI Conference\non Artificial Intelligence, AAAI’18, pp. 7065–7072. AAAI Press.\n[39] M. Liao, C. Sung, H. Wang, and W. Lin. Virtual classmates: Em-\nbodying historical learners’ messages as learning companions in a vr\nclassroom through comment mapping. In 2019 IEEE Conference on\nVirtual Reality and 3D User Interfaces (VR), pp. 163–171, 2019.\n[40] B. Liebold and P. Ohler. Multimodal emotion expressions of virtual\nagents, mimic and vocal emotion expressions and their effects on\nemotion recognition. In Proceedings of the 2013 Humaine Association\nConference on Affective Computing and Intelligent Interaction, ACII\n’13, p. 405–410. IEEE Computer Society, USA, 2013. doi: 10.1109/\nACII.2013.73\n[41] D. Matsumoto, M. G. Frank, and H. S. Hwang. Nonverbal communica-\ntion: Science and applications. Sage Publications, 2012.\n[42] D. McNeill. Hand and mind: What gestures reveal about thought .\nUniversity of Chicago press, 1992.\n[43] H. K. M. Meeren, C. C. R. J. van Heijnsbergen, and B. de Gelder.\nRapid perceptual integration of facial expression and emotional\nbody language. Proceedings of the National Academy of Sciences ,\n102(45):16518–16523, 2005. doi: 10.1073/pnas.0507650102\n[44] A. Mehrabian and J. A. Russell. An approach to environmental psy-\nchology. the MIT Press, 1974.\n[45] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Dis-\ntributed representations of words and phrases and their composition-\nality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, eds., Advances in Neural Information Processing\nSystems, vol. 26, pp. 3111–3119. Curran Associates, Inc., 2013.\n[46] T. Mittal, U. Bhattacharya, R. Chandra, A. Bera, and D. Manocha.\nM3er: Multiplicative multimodal emotion recognition using facial,\ntextual, and speech cues. In Proceedings of the Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI’20, pp. 1359–1367. AAAI\nPress, 2020.\n[47] T. Mittal, P. Guhan, U. Bhattacharya, R. Chandra, A. Bera, and\nD. Manocha. Emoticon: Context-aware multimodal emotion recogni-\ntion using frege’s principle. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 14234–14243,\n2020.\n[48] S. Mohammad. Obtaining reliable human ratings of valence, arousal,\nand dominance for 20,000 English words. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pp. 174–184. Association for Computational\nLinguistics, Melbourne, Australia, July 2018. doi: 10.18653/v1/P18\n-1017\n[49] M. Neff, M. Kipp, I. Albrecht, and H.-P. Seidel. Gesture modeling and\nanimation based on a probabilistic re-creation of speaker style. ACM\nTrans. Graph., 27(1), Mar. 2008. doi: 10.1145/1330511.1330516\n[50] Oculus. Facebook Horizon, https://www.oculus.com/facebook-\nhorizon/.\n[51] D. Pavllo, D. Grangier, and M. Auli. Quaternet: A quaternion-based re-\ncurrent model for human motion. InBritish Machine Vision Conference\n2018, BMVC 2018, p. 299, 2018.\n[52] J. Pennington, R. Socher, and C. Manning. GloVe: Global vectors\nfor word representation. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , pp.\n1532–1543. Association for Computational Linguistics, Doha, Qatar,\nOct. 2014. doi: 10.3115/v1/D14-1162\n[53] T. Randhavane, A. Bera, K. Kapsaskis, U. Bhattacharya, K. Gray, and\nD. Manocha. Identifying emotions from walking using affective and\ndeep features. arXiv preprint arXiv:1906.11884, 2019.\n[54] T. Randhavane, A. Bera, K. Kapsaskis, K. Gray, and D. Manocha. Fva:\nModeling perceived friendliness of virtual agents using movement char-\nacteristics. IEEE transactions on visualization and computer graphics,\n25(11):3135–3145, 2019.\n[55] T. Randhavane, A. Bera, K. Kapsaskis, R. Sheth, K. Gray, and\nD. Manocha. Eva: Generating emotional behavior of virtual agents\nusing expressive features of gait and gaze. In ACM Symposium on\nApplied Perception 2019, p. 6. ACM, 2019.\n[56] D. Roth, J. Lugrin, D. Galakhov, A. Hofmann, G. Bente, M. E.\nLatoschik, and A. Fuhrmann. Avatar realism and social interaction qual-\nity in virtual reality. In 2016 IEEE Virtual Reality (VR), pp. 277–278,\n2016.\n[57] N. Sadoughi and C. Busso. Novel realizations of speech-driven head\nmovements with generative adversarial networks. In 2018 IEEE In-\nternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 6169–6173, 2018.\n[58] N. Sadoughi and C. Busso. Speech-driven animation with meaningful\nbehaviors. Speech Communication, 110:90 – 100, 2019. doi: 10.1016/j\n.specom.2019.04.005\n[59] A. L. Simeone, M. Speicher, A. Molnar, A. Wilde, and F. Daiber.\nLive: The human role in learning in immersive virtual environments.\nIn Symposium on Spatial User Interaction, SUI ’19. Association for\nComputing Machinery, New York, NY , USA, 2019. doi: 10.1145/\n3357251.3357590\n[60] S. S. Sohn, X. Zhang, F. Geraci, and M. Kapadia. An emotionally\naware embodied conversational agent. In Proceedings of the 17th Inter-\nnational Conference on Autonomous Agents and MultiAgent Systems,\nAAMAS ’18, p. 2250–2252. International Foundation for Autonomous\nAgents and Multiagent Systems, Richland, SC, 2018.\n[61] S. Starke, H. Zhang, T. Komura, and J. Saito. Neural state machine for\ncharacter-scene interactions. ACM Transactions on Graphics (TOG),\n38(6):209, 2019.\n[62] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher-Shlizerman. Syn-\nthesizing obama: Learning lip sync from audio. ACM Trans. Graph.,\n36(4), July 2017. doi: 10.1145/3072959.3073640\n[63] J. Van den Stock, R. Righart, and B. De Gelder. Body expressions\ninfluence recognition of emotions in the face and voice. Emotion,\n7(3):487, 2007.\n[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In\nAdvances in Neural Information Processing Systems 30, pp. 5998–6008.\nCurran Associates, Inc., 2017.\n[65] E. V olkova, S. De La Rosa, H. H. B¨ulthoff, and B. Mohler. The mpi\nemotional body expressions database for narrative scenarios. PloS one,\n9(12):e113647, 2014.\n[66] P. Wagner, Z. Malisz, and S. Kopp. Gesture and speech in interaction:\nAn overview. Speech Communication, 57:209 – 232, 2014. doi: 10.\n1016/j.specom.2013.09.008\n[67] J. Z. Wang, N. Badler, N. Berthouze, R. O. Gilmore, K. L. Johnson,\nA. Lapedriza, X. Lu, and N. Troje. Panel: Bodily expressed emotion\nunderstanding research: A multidisciplinary perspective. In A. Bartoli\nand A. Fusiello, eds., Computer Vision – ECCV 2020 Workshops, pp.\n733–746. Springer International Publishing, Cham, 2020.\n[68] Y . Yoon, B. Cha, J.-H. Lee, M. Jang, J. Lee, J. Kim, and G. Lee. Speech\ngesture generation from the trimodal context of text, audio, and speaker\nidentity. ACM Transactions on Graphics, 39(6), 2020.\n[69] Y . Yoon, W.-R. Ko, M. Jang, J. Lee, J. Kim, and G. Lee. Robots learn\nsocial skills: End-to-end learning of co-speech gesture generation for\nhumanoid robots. In Proc. of The International Conference in Robotics\nand Automation (ICRA), 2019.\n[70] X. Zhou, S. Huang, B. Li, Y . Li, J. Li, and Z. Zhang. Text guided\nperson image synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), June 2019.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I66946132",
      "name": "University of Maryland, College Park",
      "country": "US"
    }
  ],
  "cited_by": 111
}