{
    "title": "Protein language models meet reduced amino acid alphabets",
    "url": "https://openalex.org/W4391517803",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5082343661",
            "name": "Ioan Ieremie",
            "affiliations": [
                "University of Southampton"
            ]
        },
        {
            "id": "https://openalex.org/A5061685303",
            "name": "Rob M. Ewing",
            "affiliations": [
                "University of Southampton"
            ]
        },
        {
            "id": "https://openalex.org/A5033579333",
            "name": "Mahesan Niranjan",
            "affiliations": [
                "University of Southampton"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2980789587",
        "https://openalex.org/W3127426316",
        "https://openalex.org/W2045204781",
        "https://openalex.org/W2900674118",
        "https://openalex.org/W2080646340",
        "https://openalex.org/W3015921770",
        "https://openalex.org/W3203459683",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W3198923619",
        "https://openalex.org/W2028841335",
        "https://openalex.org/W4283795415",
        "https://openalex.org/W2140673705",
        "https://openalex.org/W4281790889",
        "https://openalex.org/W2097892623",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W6763868836",
        "https://openalex.org/W2052376307",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W1978326290",
        "https://openalex.org/W2950954328",
        "https://openalex.org/W2953008890",
        "https://openalex.org/W2076048958",
        "https://openalex.org/W2005120335",
        "https://openalex.org/W1530641120",
        "https://openalex.org/W2483469645",
        "https://openalex.org/W4230276186",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W2971227267",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W4392302569"
    ],
    "abstract": "Abstract Motivation Protein language models (PLMs), which borrowed ideas for modelling and inference from natural language processing, have demonstrated the ability to extract meaningful representations in an unsupervised way. This led to significant performance improvement in several downstream tasks. Clustering amino acids based on their physical–chemical properties to achieve reduced alphabets has been of interest in past research, but their application to PLMs or folding models is unexplored. Results Here, we investigate the efficacy of PLMs trained on reduced amino acid alphabets in capturing evolutionary information, and we explore how the loss of protein sequence information impacts learned representations and downstream task performance. Our empirical work shows that PLMs trained on the full alphabet and a large number of sequences capture fine details that are lost in alphabet reduction methods. We further show the ability of a structure prediction model(ESMFold) to fold CASP14 protein sequences translated using a reduced alphabet. For 10 proteins out of the 50 targets, reduced alphabets improve structural predictions with LDDT-Cα differences of up to 19%. Availability and implementation Trained models and code are available at github.com/Ieremie/reduced-alph-PLM.",
    "full_text": null
}