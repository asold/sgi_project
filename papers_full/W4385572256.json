{
  "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data",
  "url": "https://openalex.org/W4385572256",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2170588646",
      "name": "Xinze Li",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2110335847",
      "name": "Zhenghao Liu",
      "affiliations": [
        "National Engineering Research Center for Information Technology in Agriculture",
        "Northeastern University",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2226924701",
      "name": "Chenyan Xiong",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2099988764",
      "name": "SHI Yu",
      "affiliations": [
        "Tsinghua University",
        "National Engineering Research Center for Information Technology in Agriculture"
      ]
    },
    {
      "id": "https://openalex.org/A2117761656",
      "name": "Yu Gu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Northeastern University",
        "National Engineering Research Center for Information Technology in Agriculture",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A265453316",
      "name": "Ge Yu",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4249573750",
    "https://openalex.org/W4206121183",
    "https://openalex.org/W3125238517",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4287592659",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W2964060837",
    "https://openalex.org/W4394638297",
    "https://openalex.org/W3203288040",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3175362188",
    "https://openalex.org/W4384644324",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W4309083387",
    "https://openalex.org/W4312643954",
    "https://openalex.org/W3099944244",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W4282961889",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3201233724",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3102663935",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4287328196",
    "https://openalex.org/W3197057826",
    "https://openalex.org/W3115295967",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3131870090",
    "https://openalex.org/W4294534134",
    "https://openalex.org/W3157758108",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W3154898636",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4229032688"
  ],
  "abstract": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 11560–11574\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nStructure-Aware Language Model Pretraining Improves Dense Retrieval\non Structured Data\nXinze Li1, Zhenghao Liu1∗, Chenyan Xiong2, Shi Yu3, Yu Gu1, Zhiyuan Liu3 and Ge Yu1\n1Department of Computer Science and Technology, Northeastern University, China\n2Microsoft Research, United States\n3Department of Computer Science and Technology, Institute for AI, Tsinghua University, China\nBeijing National Research Center for Information Science and Technology, China\nAbstract\nThis paper presents Structure Aware DeNse\nReTrievAl (SANTA) model, which encodes\nuser queries and structured data in one uni-\nversal embedding space for retrieving struc-\ntured data. SANTA proposes two pretraining\nmethods to make language models structure-\naware and learn effective representations for\nstructured data: 1) Structured Data Align-\nment, which utilizes the natural alignment re-\nlations between structured data and unstruc-\ntured data for structure-aware pretraining. It\ncontrastively trains language models to repre-\nsent multi-modal text data and teaches models\nto distinguish matched structured data for un-\nstructured texts. 2) Masked Entity Prediction,\nwhich designs an entity-oriented mask strategy\nand asks language models to fill in the masked\nentities. Our experiments show that SANTA\nachieves state-of-the-art on code search and\nproduct search and conducts convincing re-\nsults in the zero-shot setting. SANTA learns\ntailored representations for multi-modal text\ndata by aligning structured and unstructured\ndata pairs and capturing structural semantics\nby masking and predicting entities in the struc-\ntured data. All codes are available at https:\n//github.com/OpenMatch/OpenMatch.\n1 Introduction\nDense retrieval has shown strong effectiveness in\nlots of NLP applications, such as open domain\nquestion answering (Chen et al., 2017), conversa-\ntional search (Qu et al., 2020; Yu et al., 2021), and\nfact verification (Thorne et al., 2018). It employs\npretrained language models (PLMs) to encode un-\nstructured data as high-dimensional embeddings,\nconduct text matching in an embedding space and\nreturn candidates to satisfy user needs (Xiong et al.,\n2021b; Karpukhin et al., 2020).\nBesides unstructured data, structured data, such\nas codes, HTML documents and product descrip-\ntions, is ubiquitous in articles, books, and Web\n∗indicates corresponding author.\nFigure 1: Dense Retrieval Pipeline on Structured Data.\npages, and plays the same important roles in un-\nderstanding text data. Learning the semantics be-\nhind text structures to represent structured data is\ncrucial to building a more self-contained retrieval\nsystem. The structured data modeling stimulates\nresearchers to build several benchmarks to evalu-\nate model performance, such as code search and\nproduct search (Husain et al., 2019; Reddy et al.,\n2022). The structured data retrieval tasks require\nmodels to retrieve structured data according to user\nqueries. Dense retrieval (Karpukhin et al., 2020; Li\net al., 2022) shows a promising way to build a re-\ntrieval system on structured data by encoding user\nqueries and structured data in an embedding space\nand conducting text matching using the embedding\nsimilarity. Nevertheless, without structure-aware\npretraining, most PLMs lack the necessary knowl-\nedge to understand structured data and conduct\neffective representations for retrieval (Feng et al.,\n11560\n2020; Hu et al., 2022; Gururangan et al., 2020).\nLots of structure-aware pretraining methods\nare proposed to continuously train PLMs to be\nstructure-aware and better represent structured\ndata (Wang et al., 2021; Feng et al., 2020). They de-\nsign task-specific masking strategies and pretrain\nPLMs with mask language modeling. Neverthe-\nless, only using mask language modeling may not\nsufficiently train PLMs to conduct effective rep-\nresentations for structured data (Li et al., 2020;\nFang et al., 2020). Some natural alignment sig-\nnals between structured and unstructured data, such\nas code-description documentation and product\ndescription-bullet points, provide an opportunity to\npretrain the structured data representations. Using\nthese alignment signals, PLMs can be contrastively\ntrained (Wu et al., 2020; Karpukhin et al., 2020) to\nmatch the representations of aligned structured and\nunstructured data and understand the semantics of\nstructured data with the help of natural language.\nIn this paper, we propose Structure Aware\nDeNse Re TrievAl (SANTA), a dense retrieval\nmethod on structured data. As shown in Figure 1,\nSANTA encodes queries and structured data in an\nembedding space for retrieval. SANTA designs\ntwo pretraining tasks to continuously train PLMs\nand make PLMs sensitive to structured data. The\nStructured Data Alignment task contrastively trains\nPLMs to align matched structured-unstructured\ndata pairs in the embedding space, which helps\nto represent structured data by bridging the modal-\nity gap between structured and unstructured data.\nThe Masked Entity Prediction task masks entities\nand trains PLMs to fill in the masked parts, which\nhelps to capture semantics from structured data.\nOur experiments show that SANTA achieves\nstate-of-the-art in retrieving structured data, such\nas codes and products. By aligning structured and\nunstructured data, SANTA maps both structured\nand unstructured data in one universal embedding\nspace and learns more tailored embeddings for\nmulti-modal text data matching. The masked entity\nprediction task further guides SANTA to capture\nmore crucial information for retrieval and better\ndistinguish structured and unstructured data. De-\npending on these pretraining methods, SANTA can\neven achieve comparable retrieval results with exist-\ning code retrieval models without finetuning, show-\ning that our structure-aware pretraining can benefit\nstructured data understanding, multi-modal text\ndata representation modeling and text data match-\ning between user queries and structured data.\n2 Related Work\nDense retrieval (Yu et al., 2021; Karpukhin et al.,\n2020; Xiong et al., 2021b; Li et al., 2021) encodes\nqueries and documents using pretrained language\nmodel (PLM) (Devlin et al., 2019; Liu et al., 2019;\nRaffel et al., 2020) and maps them in an embed-\nding space for retrieval. However, during retrieving\ncandidates, the documents can be passages in nat-\nural language (Nguyen et al., 2016; Kwiatkowski\net al., 2019), images (Chen et al., 2015), structured\ndata documents (Lu et al., 2021) or multi-modal\ndocuments (Chang et al., 2021), which challenges\nexisting dense retrieval models to handle different\nkinds of modalities of knowledge sources to build\na self-contained retrieval system.\nExisting work (Guo et al., 2021) also builds\ndense retrievers for retrieving structured data and\nmainly focuses on learning representations for code\ndata. Leaning more effective representations with\nPLMs is crucial for dense retrieval (Gao and Callan,\n2021; Luan et al., 2021), thus several continuous\ntraining models are proposed. They usually em-\nploy mask language modeling to train PLMs on\nstructured data and help to memorize the semantic\nknowledge using model parameters (Wang et al.,\n2021; Feng et al., 2020; Roziere et al., 2021).\nCodeBERT uses replaced token detection (Clark\net al., 2020) and masked language modeling (De-\nvlin et al., 2019) to learn the lexical semantics of\nstructured data (Lu et al., 2021). DOBF (Roziere\net al., 2021) further considers the characteristics of\ncode-related tasks and replaces class, function and\nvariable names with special tokens. CodeT5 (Wang\net al., 2021) not only employs the span mask strat-\negy (Raffel et al., 2020) but also masks the iden-\ntifiers in codes to teach T5 (Raffel et al., 2020)\nto generate these identifiers, which helps better\ndistinguish and comprehend the identifier informa-\ntion in code-related tasks. Nevertheless, the mask\nlanguage modeling (Devlin et al., 2019) may not\nsufficiently train PLMs to represent texts and show\nless effectiveness in text matching tasks (Chen and\nHe, 2021; Gao et al., 2019; Li et al., 2020; Reimers\nand Gurevych, 2019; Li et al., 2020).\nThe recent development of sentence represen-\ntation learning methods has achieved convincing\nresults (Fang et al., 2020; Yan et al., 2021). The\nwork first constructs sentence pairs using back-\ntranslation (Fang et al., 2020), some easy deforma-\n11561\nFigure 2: The Structure-Aware Pretraining Methods of SANTA. We use both Structured Data Alignment (SDA) and\nMasked Entity Prediction (MEP) methods for pretraining.\ntion operations (Wu et al., 2020), original sequence\ncropping (Meng et al., 2021) or adding dropout\nnoise (Gao et al., 2021). Then they contrastively\ntrain PLMs to learn sentence representations that\ncan be used to distinguish the matched sentence\npairs with similar semantics.\n3 Methodology\nIn this section, we introduce our Structure Aware\nDeNse ReTrievAl (SANTA) model. First, we intro-\nduce the preliminary of dense retrieval (Sec. 3.1).\nAnd then we describe our structure-aware pretrain-\ning method (Sec. 3.2).\n3.1 Preliminary of Dense Retrieval\nGiven a query q and a structured data document\nd, dense retriever (Karpukhin et al., 2020; Xiong\net al., 2021a) encodes queries and structured data\ndocuments with pretrained language models (De-\nvlin et al., 2019; Liu et al., 2019) and maps them\nin an embedding space for retrieval.\nFollowing previous work (Ni et al., 2022), we\ncan use T5 (Raffel et al., 2020) to encode the query\nq and structured data document d as low dimen-\nsional representations hq and hd, using the repre-\nsentation of the first token from the decoder:\nhq = T5(q); hd = T5(d). (1)\nThen we can calculate the similarity score f(q, d)\nbetween the representations of query hq and struc-\ntured data document hd:\nf(q, d) =sim(hq, hd), (2)\nwhere sim is the dot product function to calculate\nthe relevance between query q and structured data\ndocument d.\nFinally, we can finetune the representations of\nquery and document by minimizing the loss LDR:\nLDR = −log ef(q,d+)\nef(q,d+) + ∑\nd−∈D− ef(q,d−) , (3)\nwhere d+ is relevant to the given query q. D−\nis the collection of irrelevant structured data doc-\numents, which are sampled from inbatch neg-\natives (Karpukhin et al., 2020) or hard nega-\ntives (Xiong et al., 2021a).\n3.2 Structure Aware Pretraining\nExisting language models are usually pretrained\non unstructured natural languages with masked lan-\nguage modeling (Devlin et al., 2019; Liu et al.,\n2019). Nevertheless, these models struggle to bet-\nter understand the semantics represented by data\nstructures, which limits the effectiveness of lan-\nguage models in representing structured data for\nretrieval (Feng et al., 2020; Wang et al., 2021).\nTo get more effective representations for struc-\ntured data, we come up with structure-aware pre-\ntraining methods, aiming to help language models\nbetter capture the semantics behind the text struc-\ntures. As shown in Figure 2, we continuously fine-\n11562\ntune T5 using two pretraining tasks by minimizing\nthe following loss function L:\nL= LSDA + LMEP, (4)\nwhere LSDA and LMEP are two loss functions from\nstructured data alignment (SDA) (Sec. 3.2.1) and\nmasked entity prediction (MEP) (Sec. 3.2.2), which\nare two subtasks of our structure-aware language\nmodel pretraining method.\n3.2.1 Structured Data Alignment\nThe structured data alignment task teaches lan-\nguage models to optimize the embedding space\nby aligning structured data with unstructured data.\nFor the structured data document d, there are\nusually some natural language passages that share\nthe same semantics with d, e.g. the descriptions\nof codes and bullet points of products. With the\nhelp of these text passages p in natural language,\nwe can enhance the model’s ability in representing\nstructured data by continuously training language\nmodels to align the semantics between structured\nand unstructured data. Through text data alignment,\nthe representations of structured data are benefited\nfrom the intrinsic natural language knowledge of\npretrained language models.\nSpecifically, we can use T5 to encode the text\npassage and structured data document ashp and hd,\nrespectively, calculate the similarity score f(p, d)\nbetween text passage p and structured data docu-\nment d, and then continuously train language mod-\nels using the contrastive loss LSDA:\nLSDA = −log ef(p,d+)\nef(p,d+) + ∑\nd−∈D− ef(p,d−)\n= −f(p, d+) + log(ef(p,d+) +\n∑\nd−∈D−\nef(p,d−)),\n(5)\nwhere D− consists of the irrelevant structured data\nsampled from in-batch negatives.\nAs shown in Eq. 5, the structured data alignment\ntraining task helps to optimize the pretrained lan-\nguage models to assign similar embedding features\nto < p, d+ > pairs and pull d− away from p in the\nembedding space (Wang and Isola, 2020). Such a\ncontrastive training method can bridge the seman-\ntic gap between structured and unstructured data\nand map them in one universal embedding space,\nbenefiting learning representations of multi-modal\ntext data (Liu et al., 2023).\n3.2.2 Masked Entity Prediction\nThe masked entity prediction guides the language\nmodels to better understand the semantics of struc-\ntured data by recovering masked entities. SANTA\nmasks entities for continuous training instead of\nusing the random masking in mask language mod-\neling (Devlin et al., 2019; Raffel et al., 2020).\nAs shown in previous work (Sciavolino et al.,\n2021; Zhang et al., 2019), entity semantics show\nstrong effectiveness in learning text data represen-\ntations during retrieval. Thus, we first recognize\nmentioned entities that appeared in the structured\ndata document Xd = {x1, ent1, x2, ent2, ...,entn}\nand mask them as the input for T5 encoder module:\nXmask\nd = {x1, <mask>1, x2, <mask>2, ..., xn}, (6)\nwhere <mask>i is a special token to denote the i-th\nmasked span. We replace the same entity with the\nsame special token. Then we continuously train T5\nto recover these masked entities using the following\nloss function:\nLMEP =\nk∑\nj=1\n−log P(Yd(tj)|Xmask\nd , Yd(t1,...,j−1)), (7)\nwhere Yd(tj) denotes the j-th to-\nken in the sequence Yd. And Yd =\n{<mask>1, ent1, ...,<mask>n, entn} denotes\nthe ground truth sequence that contains masked\nentities. During training, we optimize the language\nmodel to fill up masked spans and better capture\nentity semantics by picking up the necessary\ninformation from contexts to recover the masked\nentities, understanding the structure semantics\nof text data, and aligning coherent entities in the\nstructured data (Ye et al., 2020).\n4 Experimental Methodology\nIn this section, we describe the datasets, evaluation\nmetrics, baselines, and implementation details in\nour experiments.\nDataset. The datasets in our experiments consist\nof two parts, which are used for continuous training\nand finetuning, respectively.\nContinuous Training. During continuous train-\ning, two datasets, CodeSearchNet (Husain et al.,\n2019) and ESCI (large) (Reddy et al., 2022), are\nemployed to continuously train PLMs to conduct\nstructure-aware text representations for codes and\nshopping products. In our experiments, we regard\ncode documentation descriptions and product bullet\n11563\nSplit Code Search Product Search\nQuery-Code Pair Query Product\nTrain 251,820 18,277 367,946\nDev 9,604 2,611 51,706\nTest 19,210 8,956 181,701\nTable 1: Data Statistics of Model Finetuning.\npoints as unstructured data for aligning structured\ndata, codes and product descriptions, during train-\ning. More details of pretraining data processing are\nshown in Appendix A.2.\nFinetuning. For downstream retrieval tasks on\nstructured data, we use Adv (Lu et al., 2021), and\nESCI (small) (Reddy et al., 2022) to finetune mod-\nels for code search and product search, respectively.\nAll data statistics are shown in Table 1. Each query\nin ESCI (small) has 20 products on average, which\nare annotated with four-class relevance labels: Ex-\nact, Substitute, Complement, and Irrelevant. We\nalso establish a two-class testing scenario by only\nregarding the products that are annotated with the\nExact label as relevant ones.\nEvaluation Metrics. We use MRR@100\nand NDCG@100 to evaluate model performance,\nwhich is the same as the previous work (Lu et al.,\n2021; Reddy et al., 2022; Feng et al., 2020).\nBaselines. We compare SANTA with several\ndense retrieval models on code search and product\nsearch tasks.\nWe first employ three pretrained language mod-\nels to build dense retrievers for structured data\nretrieval, including BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019) and T5 (Raffel et al.,\n2020), which are widely used in existing dense re-\ntrieval models (Karpukhin et al., 2020; Xiong et al.,\n2021a; Ni et al., 2022). All these models are trained\nwith in-batch negatives (Karpukhin et al., 2020).\nFor the code search task, we also compare\nSANTA with three typical and task-specific mod-\nels, CodeBERT (Feng et al., 2020), CodeT5 (Wang\net al., 2021) and CodeRetriever (Li et al., 2022).\nCodeBERT inherits the BERT architecture and is\ntrained on code corpus using both mask language\nmodeling and replaced token detection. CodeT5\nemploys the encoder-decoder architecture for mod-\neling different code-related tasks and teaches the\nmodel to focus more on code identifiers. CodeRe-\ntriever is the state-of-the-art, which continuously\ntrains GraphCodeBERT (Guo et al., 2021) with\nunimodal and bimodal contrastive training losses.\nImplementation Details. This part describes\nthe experiment details of SANTA.\nWe initialize SANTA with T5-base and CodeT5-\nbase for product search and code search. For\nmasked entity prediction, we regard code identi-\nfiers and some noun phrases as entities in codes\nand product descriptions, respectively. More de-\ntails about identifying entities are shown in Ap-\npendix A.3.\nDuring continuous training, we set the learning\nrate as 1e-4 and 5e-5 for product search and code\nsearch, and the training epoch as 6. During finetun-\ning, we conduct experiments by training SANTA\nusing inbatch negatives and hard negatives. we\nset the training epoch to 60 and learning rate to\n5e-5 for product search, while the training epoch\nand learning rate are 6 and 1e-5 for code search.\nAnd we follow ANCE (Xiong et al., 2021a), start\nfrom inbatch finetuned SANTA (Inbatch) model\nand continuously finetune it with hard negatives to\nconduct the SANTA (Hard Negative) model. The\nlearning rates are set to 1e-5 and 1e-6 for product\nsearch and code search. These hard negatives are\nrandomly sampled from the top 100 retrieved nega-\ntive codes/product descriptions from the SANTA\n(Inbatch) model.\nAll models are implemented with PyTorch, Hug-\ngingface transformers (Wolf et al., 2019) and Open-\nMatch (Yu et al., 2023). We use Adam optimizer to\noptimize SANTA, set the batch size to 16 and set\nthe warmup proportion to 0.1 in our experiments.\n5 Evaluation Results\nIn this section, we focus on exploring the per-\nformance of SANTA on code search and product\nsearch tasks, the advantages of SANTA in repre-\nsenting structured data, and the effectiveness of\nproposed pretraining methods.\n5.1 Overall Performance\nThe performance of SANTA on structured data\nretrieval is shown in Table 2.\nSANTA shows strong zero-shot ability by com-\nparing its performance with finetuned models\nand achieving 6.8% improvements over finetuned\nCodeT5 on code search. Such impressive improve-\nments demonstrate that our pretrained strategies\nhave the ability to enable the advantages of PLMs\nin representing structured data without finetuning.\nAfter finetuning, SANTA maintains its advan-\ntages by achieving about 8% and 2% improve-\nments over CodeT5 and T5 on code search and\n11564\nModel\nCode Product\nMRR NDCG\nTwo-C Four-C\nZero-Shot\nBERT (Devlin et al., 2019) 0.20 71.46 72.45\nRoBERTa (Liu et al., 2019) 0.03 71.25 72.24\nCodeBERT (Feng et al., 2020)0.03 - -\nCodeRetriever (Li et al., 2022)34.7 - -\nT5 (Raffel et al., 2020) 0.03 70.21 71.25\nCodeT5 (Wang et al., 2021) 0.03 - -\nSANTA 46.1 76.38 77.14\nFine-Tuning\nBERT (Devlin et al., 2019) 16.7 78.29 79.06\nRoBERTa (Liu et al., 2019) 18.3 79.59 80.29\nCodeBERT (Feng et al., 2020)27.2 - -\nCodeRetriever 43.0 - -\nCodeRetriever (AR2) (Li et al., 2022)46.9 - -\nT5 (Raffel et al., 2020) 23.8 79.77 80.46\nCodeT5 (Wang et al., 2021) 39.3 - -\nSANTA (Inbatch) 47.3 80.76 81.41\nSANTA (Hard Negative) 47.5 82.59 83.15\nTable 2: Retrieval Effectiveness of Different Models\non Structured Data. For product search, there are two\nways to evaluate model performance. Two-C regards\nthe query-product relevance as two classes, Relevant (1)\nand Irrelevant (0). Four-C is consistent with the ESCI\ndataset (Reddy et al., 2022) and sets the relevance labels\nwith the following four classes: Exact (1), Substitute\n(0.1), Complement (0.01), and Irrelevant (0).\nproduct search, respectively. It shows the critical\nrole of structure-aware pretraining, which makes\nlanguage models sensitive to text data structures\nand better represents structured data. On code re-\ntrieval, SANTA outperforms the state-of-the-art\ncode retrieval model CodeRetriever with 4.3% im-\nprovements under the same inbatch training setting.\nSANTA also beats CodeRetriever (AR2), which is\nfinetuned with more sophisticated training strate-\ngies (Zhang et al., 2022) and the larger batch size.\nBesides, we show the retrieval performance of\nSANTA on CodeSearch dataset in Appendix A.4.\n5.2 Ablation Study\nIn this subsection, we conduct ablation studies to\nfurther explore the roles of different components in\nSANTA on retrieving structured data.\nWe start from CodeT5/T5 models and continu-\nously train CodeT5/T5 using two proposed training\ntasks, Masked Entity Prediction (MEP) and Struc-\ntured Data Alignment (SDA) to show their effec-\ntiveness in teaching models to better learn seman-\ntics from structured data. Meanwhile, we compare\nMEP with the random span masking strategy (Raf-\nfel et al., 2020; Wang et al., 2021) to evaluate the\neffectiveness of different masking strategies. The\nModel\nCode Product\nMRR NDCG\nTwo-C Four-C\nZero-Shot\nT5 (Baseline) 0.03 70.21 71.25\nT5 (w/ MEP) 0.03 70.56 71.58\nT5 (w/ SDA) 45.01 76.64 77.40\nSANTA (Span Mask) 35.88 77.37 78.11\nSANTA (Entity Mask) 46.08 76.38 77.14\nFine-Tuning\nT5 (Baseline) 39.30 79.77 80.46\nT5 (w/ MEP) 38.46 79.50 80.29\nT5 (w/ SDA) 46.98 80.42 81.11\nSANTA (Span Mask) 42.11 80.31 80.99\nSANTA (Entity Mask) 47.28 80.76 81.41\nTable 3: The Retrieval Performance of Ablation Models\nof SANTA on Structured Data Retrieval. Masked Entity\nPrediction (MEP) and Structured Data Alignment (SDA)\nare two pretrained tasks that are proposed by SANTA.\nretrieval performance in both zero-shot and finetun-\ning settings is shown in Table 3.\nCompared with our baseline model, MEP and\nSDA show distinct performance in structured data\nretrieval. As expected, MEP shows almost the same\nperformance as the baseline model. It shows that\nonly mask language modeling usually shows less\neffectiveness in learning representations for struc-\ntured data, even using different masking strategies.\nDifferent from MEP, SDA shows significant im-\nprovements in both structured data retrieval tasks,\nespecially the code retrieval task. Our SDA train-\ning method contrastively trains T5 models using\nthe alignment relations between structured data\nand unstructured data, which helps to bridge the\nmodality gap between structured and unstructured\ndata, maps structured and unstructured data in one\nuniversal embedding space, and learns more ef-\nfective representations for retrieval. When adding\nadditional task MEP to T5 (w/ SDA), the retrieval\nperformance of SANTA is consistently improved.\nThis phenomenon shows that mask language mod-\neling is still effective to teach T5 to better capture\nthe structure semantics and conduct more effective\ntext representations for structured data by filling up\nthe masked entities of structured data.\nWe also compare different masking strategies\nthat are used during mask language modeling. Our\nentity masking strategy usually outperforms the\nrandom span masking strategy, showing the crucial\nrole of entities in structured data understanding.\nWith the masked entity prediction task, SANTA\nachieves comparable ranking performance with\nfinetuned models, which illustrates that structure-\n11565\n(a) Ranking Probability of\nMatched Text Data Pairs.\n(b) Embedding Distribution of\nStructured Data.\nFigure 3: Retrieval Effectiveness on Code Search. We\nsample several query-code pairs from the test split of\ncode search data and show the ranking probability dis-\ntribution of query-related codes in Figure 3(a). Then\nFigure 3(b) presents the learned embedding space of\nstructured data of codes.\naware pretraining is starting to benefit downstream\ntasks, such as structured data retrieval. The next\nexperiment further explores how these pretraining\nstrategies guide models to learn representations of\nstructured/unstructured data.\n5.3 Embedding Visualization of Structured\nand Unstructured Data\nThis section further explores the characteristics of\nembedding distributions of structured and unstruc-\ntured data learned by SANTA.\nAs shown in Figure 3, we first conduct experi-\nments to show the retrieval effectiveness of CodeT5\nand SANTA under the zero-shot setting. The rank-\ning probability distribution of relevant query-code\npairs is shown in Figure 3(a). Even though CodeT5\nis pretrained with code text data, it seems that\nCodeT5 learns ineffective representations for struc-\ntured data, assigns a uniform ranking probability\ndistribution for all testing examples and fails to\npick up the related structured data for the given\nqueries. On the contrary, SANTA assigns much\nhigher ranking probabilities to matched structured\ndocuments, demonstrating that our structured data\nalignment task has the ability to guide the model\nto conduct more effective text data representations\nto align queries with its relevant structured docu-\nments. Then we plot the embedding distribution\nof structured data in Figure 3(b). Distinct from\nthe embedding distribution of CodeT5, the embed-\ndings learned by SANTA, are more distinguishable\nand uniform, which are two criteria of learning\nmore effective embedding space under contrastive\ntraining (Li et al., 2021; Wang and Isola, 2020).\nThen we present the embedding distribution of\ndocumentation texts and their corresponding codes\n(a) CodeT5.\n (b) CodeT5 (w/ SDA).\n(c) CodeT5 (w/ MEP).\n (d) SANTA.\nFigure 4: Embedding Visualization of Different Models\nusing T-SNE. We randomly sample 32 codes and 32\ncode documentation texts from the testing set of code\nretrieval and plot their embedding distribution.\nin Figure 4. Overall, depending on our structure-\naware pretraining methods, SANTA conducts a\nmore uniform embedding space than CodeT5 and\nmakes the representations of structured and un-\nstructured data more distinguished in the embed-\nding space. Then we analyze the effectiveness\nof our continuous training methods, Masked En-\ntity Prediction (MEP) and Structured Data Align-\nment (SDA). By comparing Figure 4(b) with Fig-\nure 4(a), our structured data alignment task indeed\nhelps PLMs to align the representations of code\nand documentation, which reduces the distance be-\ntween matched unstructured-structured data pairs\nand mixes the multi-modal embeddings thoroughly\nin the embedding space. After adding the masked\nentity prediction training task to CodeT5 (w/ SDA)\n(from Figure 4(b) to Figure 4(d)), the embedding\ndistributions of code and documentation become\ndistinguished again, demonstrating that masked en-\ntity prediction can help models capture different\nsemantics from different data modalities to repre-\nsent unstructured/structured data. Besides, by com-\nparing Figure 4(d) with Figure 4(c), the structured\ndata alignment task also makes the boundary of\nthe embedding clusters of code and documentation\nclearer. The main reason lies in that these em-\nbeddings are assigned to appropriate positions for\naligning matched code-documentation pairs with\nthe help of our structured data alignment task.\n11566\nModel SANTA CodeT5/T5\nQuery Construct the command to poll the driver status\nRank 1 1\nSnippet ... arg_0 . _connection [ ’master’ ] ] if arg_0\n. _driver_id : arg_1 += [ \"– status\" , arg_0 .\n_driver_id ] else : raise AirflowException ( \"–\nInvalid status: attempted to poll driver ...\ndef Func ( arg_0 ) : return os . path . join (\nget_user_config_dir ( arg_0 . app_name , arg_0\n. app_author ) , arg_0 . filename )\nQuery Attempt to copy path with storage.\nRank 1 1\nSnippet ... if arg_2 in arg_0 . copied_files : return arg_0 .\nlog ( \"Skipping ’%s’ (already –copied earlier)\" %\narg_1 ) if not arg_0 . delete_file ( arg_1 , arg_2 ,\narg_3 ) : return arg_4 = arg_3 . –path ( arg_1 ) ...\n’... arg_0 ) : if arg_0 . _api_arg : arg_1 = str ( arg_0\n._api_arg ) else : arg_1 = arg_0 . _name if arg_0 .\n_parent : return ’/’ . join ( filter ( None , [ arg_0 .\n_parent . Func , arg_1 ] ) ) ...’\nQuery #1 black natural hair dye without ammonia or peroxide\nRank 1 1\nSnippet ... naturcolor Haircolor Hair Dye - Light Burdock,\n4 Ounce (5N) naturcolor 5n light burdock perma-\nnent herbal Ingredients: haircolor gel utilizes herbs\nto cover grey as opposed to chemicals ...\n... Naturtint Permanent Hair Color 5N Light Chest-\nnut Brown (Pack of 1), Ammonia Free, Vegan, Cru-\nelty Free, up to 100% Gray Coverage, Long Lasting\nResults...\nQuery !qscreen fence without holes\nRank 2 2\nSnippet ... Material: HDPE+Brass Color: Green Size(L x\nW): About 6’x50” Package included: Garden fence\nprivacy screen*1 Straps*80 ...\n... Windscreen Cover Fabric Shade Tarp Netting\nMesh Cloth - Commercial Grade 170 GSM - Cable\nZip Ties Included - We Make Custom Size..\nTable 4: Case Studies. We sample four cases from the test datasets of code search and product search to show the\neffectiveness of SANTA. The matched text phrases arehighlighted.\n(a) Code Search.\n(b) Product Search.\nFigure 5: Visualization of Attention Distribution of\nSANTA. The cross attention weight distributions from\nthe decoder module to encoded token embeddings are\nplotted. Darker blue indicates a higher attention weight.\n5.4 Attention Mechanism of SANTA\nThis section presents the attention mechanism of\nSANTA during encoding structured data. In Fig-\nure 5, we randomly sample a small piece of code\nand a text sequence of product descriptions to plot\nthe attention distribution.\nThe attention weight distributions on code search\nare shown in Figure 5(a). Compared with CodeT5,\nCodeT5 (w/ SDA) and SANTA calibrate the atten-\ntion weights from the “if” token to the “>” token.\nThe “>” token is a logical operation, which indi-\ncates the usage of the code. SANTA thrives on the\nstructured data alignment task and captures these\nimportant semantic clues to represent codes. Com-\npared with CodeT5 (w/ SDA), SANTA decreases\nits attention weights on code identifiers, such as\n“x” and “y”, and shares more attention weights to\n“If” and “>”. These identifiers can be replaced with\nattribute ones and are less important than these\nlogical operations to understand code semantics.\nThus, SANTA adjusts its attention weights to logi-\ncal tokens to understand structured data, which is\nbenefited from pretraining with the masked entity\nprediction task.\nFigure 5(b) shows the attention distribution on\nproduct search. T5 (w/ SDA) assigns more atten-\ntion weights to the product attribute “Green” than\nT5, as well as highlights the sequence boundary\ntokens of product attributes. Nevertheless, for the\nproduct “privacy fence screen”, “Large” is a more\nimportant attribute than “Green”. SANTA cap-\ntures such semantic relevance, which confirms that\nour masked entity prediction task indeed helps to\nimprove the semantic understanding ability of lan-\nguage models on structured data.\n11567\n5.5 Case Studies\nFinally, we show several cases in Table 4 to analyze\nthe ranking effectiveness of SANTA.\nIn the first case, SANTA directly matches\nqueries and codes through the text snippet “poll\nthe driver status”. It demonstrates that SANTA has\nthe ability to distinguish the differences between\ncode and documentation and pick up the necessary\ntext clues for matching queries and codes. Then\nthe second case illustrates that SANTA is effec-\ntive in understanding codes by capturing the struc-\nture semantics of codes and matching queries and\ncodes by capturing some keywords in codes, such\nas “copied” and “path”. The last two cases are from\nproduct search and the product description is more\nlike natural language. SANTA also shows its ef-\nfectiveness on identifying some important entities,\nsuch as “Hair Dye” and “fence screen”, to match\nqueries and products.\n6 Conclusion\nThis paper proposes SANTA, which pretrains lan-\nguage models to understand structure semantics of\ntext data and guides language models to map both\nqueries and structured texts in one universal em-\nbedding space for retrieval. SANTA designs both\nstructured text alignment and masked entity predic-\ntion tasks to continuously train pretrained language\nmodels to learn the semantics behind data struc-\ntures. Our experiments show that SANTA achieves\nstate-of-the-art on code and product search by learn-\ning more tailored representations for structured\ndata, capturing semantics from structured data and\nbridging the modality gap between structured and\nunstructured data.\nLimitations\nEven though SANTA shows strong effectiveness\non learning the representation of structured data, it\nheavily depends on the alignment signals between\nstructured and unstructured data. Such alignment\nrelations can be witnessed everywhere, but the qual-\nity of constructed pairs of structured and unstruc-\ntured data directly determines the effectiveness of\nSANTA. Besides, we use the product bullet points\nand code descriptions as the unstructured data in\nour experiments, which is designed for specific\ntasks and limits the model’s generalization abil-\nity. On the other hand, SANTA mainly focuses on\nevaluating the structured data understanding ability\nthrough text data representation and matching. It\nis still unclear whether SANTA outperforms base-\nline models in all downstream tasks, such as code\nsummarization and code generation.\nAcknowledgments\nThis work is supported by the Natural Science\nFoundation of China under Grant No. 62206042,\nNo. 62137001 and No. 62272093, the Fun-\ndamental Research Funds for the Central Uni-\nversities under Grant No. N2216013 and No.\nN2216017, China Postdoctoral Science Founda-\ntion under Grant No. 2022M710022, and National\nScience and Technology Major Project (J2019-IV-\n0002-0069).\nReferences\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2021.\nWebqa: Multihop and multimodal qa. In Proceed-\nings of CVPR.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of ACL , pages\n1870–1879.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. 2015. Microsoft coco captions:\nData collection and evaluation server. CoRR.\nXinlei Chen and Kaiming He. 2021. Exploring simple\nsiamese representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 15750–15758.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In Proceedings of ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171–4186.\nHongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan\nDing, and Pengtao Xie. 2020. Cert: Contrastive self-\nsupervised learning for language understanding.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547.\n11568\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-\nYan Liu. 2019. Representation degeneration problem\nin training natural language generation models. In\nProceedings of ICLR.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of EMNLP, pages 981–993.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence\nembeddings. In Proceedings of EMNLP, pages 6894–\n6910.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu\nTang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\natkovskiy, Shengyu Fu, Michele Tufano, Shao Kun\nDeng, Colin B. Clement, Dawn Drain, Neel Sundare-\nsan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021.\nGraphcodebert: Pre-training code representations\nwith data flow. In Proceedings of ICLR.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL, pages 8342–8360.\nXiaomeng Hu, Shi Yu, Chenyan Xiong, Zhenghao Liu,\nZhiyuan Liu, and Ge Yu. 2022. P3 ranker: Mitigating\nthe gaps between pre-training and ranking fine-tuning\nwith prompt-based learning and pre-finetuning. In\nProceedings of SIGIR, pages 1956–1962.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of semantic\ncode search. CoRR.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof EMNLP, pages 6769–6781.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, pages 452–466.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of EMNLP, pages 9119–9130.\nXiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu,\nHang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang,\nWeizhu Chen, and Nan Duan. 2022. Coderetriever:\nUnimodal and bimodal contrastive learning.\nYizhi Li, Zhenghao Liu, Chenyan Xiong, and Zhiyuan\nLiu. 2021. More robust dense retrieval with con-\ntrastive dual learning. In Proceedings of the 2021\nACM SIGIR International Conference on Theory of\nInformation Retrieval, pages 287–296.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nZhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan\nLiu, and Ge Yu. 2023. Universal vision-language\ndense retrieval: Learning a unified representation\nspace for multi-modal retrieval. In Proceedings of\nICLR.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin B. Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong\nZhou, Linjun Shou, Long Zhou, Michele Tufano,\nMing Gong, Ming Zhou, Nan Duan, Neel Sundare-\nsan, Shao Kun Deng, Shengyu Fu, and Shujie Liu.\n2021. Codexglue: A machine learning benchmark\ndataset for code understanding and generation. In\nProceedings of NeurIPS.\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\nMichael Collins. 2021. Sparse, dense, and attentional\nrepresentations for text retrieval. Transactions of\nthe Association for Computational Linguistics, pages\n329–345.\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti-\nwary, Paul Bennett, Jiawei Han, and Xia Song. 2021.\nCoco-lm: Correcting and contrasting text sequences\nfor language model pretraining. In Proceedings of\nNeurIPS.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. Ms marco: A human-generated machine read-\ning comprehension dataset. In CoCo@ NIPs.\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant,\nJi Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.\nSentence-t5: Scalable sentence encoders from pre-\ntrained text-to-text models. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 1864–1874.\nChen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce\nCroft, and Mohit Iyyer. 2020. Open-retrieval con-\nversational question answering. In Proceedings of\nSIGIR, pages 539–548.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., (140):1–67.\nChandan K. Reddy, Lluís Màrquez, Fran Valero, Nikhil\nRao, Hugo Zaragoza, Sambaran Bandyopadhyay,\nArnab Biswas, Anlu Xing, and Karthik Subbian.\n11569\n2022. Shopping queries dataset: A large-scale ESCI\nbenchmark for improving product search. CoRR.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of EMNLP, pages 3982–\n3992.\nBaptiste Roziere, Marie-Anne Lachaux, Marc\nSzafraniec, and Guillaume Lample. 2021. Dobf: A\ndeobfuscation pre-training objective for program-\nming languages. In Proceedings of NeurIPS.\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers. In Proceedings of\nEMNLP, pages 6138–6148.\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal. 2018.\nThe fact extraction and VERification (FEVER)\nshared task. In Proceedings of the First Workshop\non Fact Extraction and VERification (FEVER), pages\n1–9.\nTongzhou Wang and Phillip Isola. 2020. Understanding\ncontrastive representation learning through alignment\nand uniformity on the hypersphere. In Proceedings\nof ICML, pages 9929–9939.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of EMNLP, pages\n8696–8708.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing.\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa,\nFei Sun, and Hao Ma. 2020. Clear: Contrastive\nlearning for sentence representation.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021a. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In Proceedings of ICLR.\nWenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei\nDu, Patrick S. H. Lewis, William Yang Wang, Yashar\nMehdad, Scott Yih, Sebastian Riedel, Douwe Kiela,\nand Barlas Oguz. 2021b. Answering complex open-\ndomain questions with multi-hop dense retrieval. In\nProceedings of ICLR.\nYuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang,\nWei Wu, and Weiran Xu. 2021. ConSERT: A con-\ntrastive framework for self-supervised sentence rep-\nresentation transfer. In Proceedings of ACL, pages\n5065–5075.\nDeming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng\nLi, Maosong Sun, and Zhiyuan Liu. 2020. Corefer-\nential Reasoning Learning for Language Representa-\ntion. In Proceedings of EMNLP, pages 7170–7186.\nShi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and\nZhiyuan Liu. 2021. Few-shot conversational dense\nretrieval. In Proceedings of SIGIR.\nShi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan\nLiu. 2023. Openmatch-v2: An all-in-one multi-\nmodality plm-based information retrieval toolkit. In\nProceedings of SIGIR.\nHang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv,\nNan Duan, and Weizhu Chen. 2022. Adversarial\nretriever-ranker for dense text retrieval. In Proceed-\nings of ICLR.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of ACL, pages 1441–1451.\n11570\nFigure 6: Examples of Positive and Negative Pairs of Pretraining Data.\nTask Positive Pairs Entities\nPython 429,596 28.6%\nPHP 514,127 17.8%\nGo 317,824 17.1%\nJava 454,433 24.4%\nJavaScript 122,682 15.4%\nRuby 487,90 28.8%\nProduct 331,590 20.1%\nTable 5: Data Statistics of Pretraining Data. “Entities”\ndenotes the proportion of identified entities in the struc-\ntured data.\nA Appendix\nA.1 License\nFor all datasets in our experiments, Adv and Code-\nSearchNet use MIT License, while ESCI uses\nApache License 2.0. All of these licenses and agree-\nments allow their data for academic use.\nA.2 Construction of Pretraining Data\nIn this subsection, we show how to process the pre-\ntraining data and construct structured-unstructured\ndata for code/product search. During pretraining,\nwe use inbatch negatives to optimize SANTA and\nall data statistics are shown in Table 5.\nAs shown in Figure 6, we show some examples\nto show how to construct structured-unstructured\ndata pairs for pretraining. For code retrieval tasks,\ncode snippets have corresponding documentation\ndescriptions, which describe the purpose and func-\ntion of these code snippets. Thus, the code docu-\nmentation and its corresponding code snippet are\nregarded as a positive training pair.\nFor product retrieval tasks, structured product de-\nscriptions usually have corresponding unstructured\nbullet points, which provide key points about the\n(a) Code Search.\n(b) Product Search.\nFigure 7: The Illustration of Identified Entities in Struc-\ntured Data. All entities of different functions are anno-\ntated with different colors.\nproducts. We randomly select one bullet point of\nitems and use its corresponding product description\nto construct a positive training pair.\nA.3 Additional Experimental Details of\nEntities Identification on Structured Data\nWe show some examples of entity identifications\non structured data in Figure 7.\nFor codes, we follow Wang et al. (2021) and re-\ngard code identifiers as entities such as variables,\nfunction names, external libraries and methods.\n11571\nModel CodeSearch AdvRuby Javascript Go Python Java PHP Overall\nZero-Shot\nGraphCodeBERT 1.5 0.4 0.2 0.4 0.7 2.1 0.88 0.5\nCodeRetriever 68.7 63.7 87.6 67.7 69.0 62.8 69.1 34.7\nSANTA 72.6 62.4 88.9 70.0 68.6 62.8 70.9 48.1\nFine-Tuning\nCodeBERT 67.9 62.0 88.2 67.2 67.6 62.8 69.3 27.2\nGraphCodeBERT 70.3 64.4 89.7 69.2 69.1 64.9 71.3 35.2\nCodeT5 71.9 65.5 88.8 69.8 68.6 64.5 71.5 39.3\nCodeRetriever (Inbatch) 75.3 69.5 91.6 73.3 74.0 68.2 75.3 43.0\nCodeRetriever (Hard Negative) 75.1 69.8 92.3 74.0 74.9 69.1 75.9 45.1\nSANTA (Hard Negative) 74.7 68.6 91.8 73.7 73.7 68.6 75.2 48.6\nTable 6: Code Retrieval Evaluations of SANTA. Because of the GPU memory limitation, we set the batch size as\n128 during pretraining and finetuning, which is different with Li et al. (2022). All models are evaluated using MRR.\nLanguage Query DocumentTrain Dev Test\nPython 251,820 13,914 14,918 43,827\nPHP 241,241 12,982 14,014 52,660\nGo 167,288 7,325 8,122 28,120\nJava 164,923 5,183 10,955 40,347\nJavaScript 58,025 3,885 3,291 13,981\nRuby 24,927 1,400 1,261 4,360\nTable 7: Data Statistics of CodeSearch Dataset. The\ndocument collections consist of candidate codes.\nSpecifically, we use BytesIO and tree_sitter 1 to\nidentify entities in Python and other programming\nlanguages, respectively. For product descriptions,\nwe use the NLTK tool2 to identify nouns and proper\nnouns that appear in both product descriptions and\ntitles and regard them as entities.\nIn our experiments, we replace the same enti-\nties with the same special tokens and ask SANTA\nto generate these masked entities (Eq. 7). These\nspecial tokens come from the predefined vocabu-\nlary of T5, such as { <extra_id_0>, <extra_id_1>,\n..., <extra_id_99> }. The proportions of identified\nentities in pretraining data are shown in Table 5.\nA.4 Additional Evaluation Results of SANTA\nIn this experiment, we follow Li et al. (2022), keep\nthe same evaluation settings and evaluate the re-\ntrieval effectiveness of SANTA on CodeSearch\ndataset. The dataset consists of code retrieval tasks\non six programming languages, including Ruby,\nJavascript, Go, Python, Java, and PHP. We show\nthe data statistics of CodeSearch in Table 7. Since\nCodeT5 and CodeRetriever don’t release their data\nprocessing code for pretraining. We can only refer\n1https://github.com/tree-sitter/tree-sitter\n2https://www.nltk.org/\nto the tutorial3 to process data. When we evaluate\nSANTA on CodeSearch, the instances in testing\nand development sets are filtered out from Code-\nSearchNet dataset for pretraining. Some codes that\ncan not be parsed are also filtered out, because the\ndata processing details are not available4.\nDuring continuous pretraining, we set the batch\nsize, learning rate and epoch as 128, 5e-5 and 10,\nrespectively. During finetuning, we set the learn-\ning rate as 2e-5 and 1e-5 for CodeSearch and Adv,\nand set batch size and epoch as 128 and 12. We\nuse inbatch negatives with one hard negative for\nfinetuning and the hard negative is randomly sam-\npled from the top 100 retrieved negative codes by\npretrained SANTA. The warm-up ratio is 0.1.\nThe performance of SANTA on CodeSearch\nand Adv is shown in Table 6. Under the zero-\nshot setting, SANTA still outperforms CodeRe-\ntriever (Li et al., 2022) with about 2% improve-\nments, which shows that the advances of SANTA\ncan be generalized to different structured data re-\ntrieval tasks. Moreover, SANTA also shows strong\nzero-shot ability by achieving comparable per-\nformance with the finetuned CodeBERT, Graph-\nCodeBERT and CodeT5 models. After finetun-\ning, SANTA achieves more than 3.7% improve-\nments over CodeT5 on CodeSearch. All these en-\ncouraged experiment results further demonstrate\nthat our structure-aware pretraining method indeed\nhelps language models to capture the structure se-\nmantics behind the text data. The retrieval perfor-\nmance on Adv dataset illustrates that the retrieval\neffectiveness of SANTA can be further improved\nby increasing the batch size from 16 to 128.\n3https://github.com/github/CodeSearchNet/blob/\nmaster/notebooks/ExploreData.ipynb\n4https://github.com/salesforce/CodeT5/issues/\n64\n11572\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn the section of Limitations.\n□\u0017 A2. Did you discuss any potential risks of your work?\nOur structure-aware language model uses public datasets and pretrained language model, so there\nare no potential risks.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nIn abstract and Section 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nIn Section 4.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nIn Section 4.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nIn Appendix A.1.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nIn Section 4.\nC □\u0013 Did you run computational experiments?\nIn Section 4.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nIn Section 4.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n11573\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nIn Section 4.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nIn Section 4.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nIn Section 4.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n11574",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8605080842971802
    },
    {
      "name": "Natural language processing",
      "score": 0.5729304552078247
    },
    {
      "name": "Unstructured data",
      "score": 0.5640347003936768
    },
    {
      "name": "Semi-structured data",
      "score": 0.5579372644424438
    },
    {
      "name": "Language model",
      "score": 0.5561370849609375
    },
    {
      "name": "Information retrieval",
      "score": 0.5005745887756348
    },
    {
      "name": "Artificial intelligence",
      "score": 0.492047518491745
    },
    {
      "name": "Code (set theory)",
      "score": 0.4697727560997009
    },
    {
      "name": "Data structure",
      "score": 0.4639207124710083
    },
    {
      "name": "Natural language",
      "score": 0.46355050802230835
    },
    {
      "name": "Embedding",
      "score": 0.4627167582511902
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.44359099864959717
    },
    {
      "name": "Modal",
      "score": 0.41917258501052856
    },
    {
      "name": "Data modeling",
      "score": 0.4149395823478699
    },
    {
      "name": "Programming language",
      "score": 0.2427753508090973
    },
    {
      "name": "Relational database",
      "score": 0.2321297824382782
    },
    {
      "name": "Data mining",
      "score": 0.229381263256073
    },
    {
      "name": "Big data",
      "score": 0.1484147608280182
    },
    {
      "name": "Database",
      "score": 0.12304162979125977
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}