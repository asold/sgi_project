{
  "title": "Training Experimental Language Models with Low Resources, for the Hungarian Language",
  "url": "https://openalex.org/W4394831376",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5055490509",
      "name": "Zijian Győző Yang",
      "affiliations": [
        null,
        "Hungarian Research Centre for Linguistics"
      ]
    },
    {
      "id": "https://openalex.org/A5057698911",
      "name": "Tamás Váradi",
      "affiliations": [
        "Hungarian Research Centre for Linguistics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3024794924",
    "https://openalex.org/W2185859000",
    "https://openalex.org/W3097876809",
    "https://openalex.org/W6681279161",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3215499059",
    "https://openalex.org/W3106061119",
    "https://openalex.org/W2920812691",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2609130030",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2124657827",
    "https://openalex.org/W3197349059",
    "https://openalex.org/W6631902868",
    "https://openalex.org/W6795050618",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3127165871",
    "https://openalex.org/W2963706742",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1971415293",
    "https://openalex.org/W3161021427",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W4320013936"
  ],
  "abstract": "In recent years, natural language processing tasks, like sentiment analysis, can be solved with high performance techniques, if a pre-trained language model is fine-tuned.However, in most cases, the pre-training of language models require huge computational resources and training corpora.Our paper addresses the issue of developing deep neural network language models for low resourced languages, such as Hungarian.Pre-training language models like BERT, requires a prohibitive amount of computational power and huge amount of training data.Unfortunately, neither of these prerequisites are commonly available for low resource languages.The question is how well the system can perform with limited resources (both in data and hardware).We focus our research on five transformer models: ELECTRA, ELECTRIC, RoBERTa, BART and GPT-2.To evaluate our models, we fine-tuned the models in six different natural language processing tasks: sentence-level sentiment analysis, named entity recognition, noun phrase chunking, extractive summarization and abstractive summarization.Our results suggest that while our experimental models obviously cannot surpass the performance of the state-of-the-art Hungarian BERT model, they require a smaller carbon footprint, may bring neural network technology to mobile applications and, finally, they may lower the threshold to engaging with neural network technology in low resourced languages, which has been an obstacle so far, in the synergistic co-development of cognitive info-communication systems and its related disciplines.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8377355337142944
    },
    {
      "name": "Automatic summarization",
      "score": 0.7421887516975403
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6044137477874756
    },
    {
      "name": "Language model",
      "score": 0.5675438642501831
    },
    {
      "name": "Natural language processing",
      "score": 0.5493077039718628
    },
    {
      "name": "Chunking (psychology)",
      "score": 0.5186997652053833
    },
    {
      "name": "Obstacle",
      "score": 0.49646812677383423
    },
    {
      "name": "Sentence",
      "score": 0.493415504693985
    },
    {
      "name": "Artificial neural network",
      "score": 0.48747682571411133
    },
    {
      "name": "Transformer",
      "score": 0.4459559917449951
    },
    {
      "name": "Natural language",
      "score": 0.4409017860889435
    },
    {
      "name": "Sentiment analysis",
      "score": 0.410108745098114
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802350943",
      "name": "Hungarian Research Centre for Linguistics",
      "country": "HU"
    }
  ],
  "cited_by": 5
}