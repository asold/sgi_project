{
    "title": "Adaptive Control of Retrieval-Augmented Generation for LLMs Through Reflective Tags",
    "url": "https://openalex.org/W4402116490",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2096808319",
            "name": "Chengyuan Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2027194343",
            "name": "Satoshi Fujita",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4387355694",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W4281250694",
        "https://openalex.org/W4387322982",
        "https://openalex.org/W4319165821",
        "https://openalex.org/W4385570929",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4309417034",
        "https://openalex.org/W4389519118",
        "https://openalex.org/W4321392329",
        "https://openalex.org/W4378474184",
        "https://openalex.org/W3034671305",
        "https://openalex.org/W4388778348",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4386238149",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4319049323",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4385570777"
    ],
    "abstract": "While Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), it also presents challenges that can affect model accuracy and performance. Practical applications show that RAG can mask the intrinsic capabilities of LLMs. Firstly, LLMs may become overly dependent on external retrieval, underutilizing their own knowledge and inference abilities, which can reduce responsiveness. Secondly, RAG techniques might introduce irrelevant or low-quality information, adding noise to the LLM. This can disrupt the normal generation process, leading to inefficient and low-quality content, especially when dealing with complex problems. This paper proposes a RAG framework that uses reflective tags to control retrieval. This framework evaluates retrieved documents in parallel and incorporates the Chain of Thought (CoT) technique for step-by-step content generation. The model selects the highest quality and most accurate content for final generation. The main contributions include: 1) Reducing the hallucination problem by selectively utilizing high-scoring document, 2) Enhancing real-time performance through timely external database retrieval, and 3) Minimizing negative impacts by filtering out irrelevant or unreliable information through parallel content generation and reflective tagging. These advancements aim to optimize the integration of retrieval mechanisms with LLMs, ensuring high-quality and reliable outputs.",
    "full_text": null
}