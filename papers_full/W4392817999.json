{
  "title": "Bidirectional generation of structure and properties through a single molecular foundation model",
  "url": "https://openalex.org/W4392817999",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2107909825",
      "name": "Jinho Chang",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100734166",
      "name": "Jong Chul Ye",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107909825",
      "name": "Jinho Chang",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100734166",
      "name": "Jong Chul Ye",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6680639217",
    "https://openalex.org/W4383955629",
    "https://openalex.org/W3080588469",
    "https://openalex.org/W4366769286",
    "https://openalex.org/W3094492244",
    "https://openalex.org/W4295951229",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2963609389",
    "https://openalex.org/W2947950688",
    "https://openalex.org/W6600339457",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W2962764565",
    "https://openalex.org/W4390193806",
    "https://openalex.org/W4285170409",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W6752934852",
    "https://openalex.org/W4225832925",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W3110846182",
    "https://openalex.org/W4289654392",
    "https://openalex.org/W2901476322",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W4385567824",
    "https://openalex.org/W3165171933",
    "https://openalex.org/W4308255580",
    "https://openalex.org/W6700855459",
    "https://openalex.org/W3097145107",
    "https://openalex.org/W2963028280",
    "https://openalex.org/W4289436753",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W3173178613",
    "https://openalex.org/W4285797776",
    "https://openalex.org/W2804256492",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W6600045627",
    "https://openalex.org/W4200583473",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3088265803",
    "https://openalex.org/W4226159083",
    "https://openalex.org/W4321458910",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W6772452955",
    "https://openalex.org/W4213077304",
    "https://openalex.org/W4313485929",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W4286901673",
    "https://openalex.org/W3181403764",
    "https://openalex.org/W2994678679",
    "https://openalex.org/W3119022334",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W3094771832",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W4390120219",
    "https://openalex.org/W4392817999",
    "https://openalex.org/W2992072991",
    "https://openalex.org/W3104956673"
  ],
  "abstract": "Abstract Recent successes of foundation models in artificial intelligence have prompted the emergence of large-scale chemical pre-trained models. Despite the growing interest in large molecular pre-trained models that provide informative representations for downstream tasks, attempts for multimodal pre-training approaches on the molecule domain were limited. To address this, here we present a multimodal molecular pre-trained model that incorporates the modalities of structure and biochemical properties, drawing inspiration from recent advances in multimodal learning techniques. Our proposed model pipeline of data handling and training objectives aligns the structure/property features in a common embedding space, which enables the model to regard bidirectional information between the molecules’ structure and properties. These contributions emerge synergistic knowledge, allowing us to tackle both multimodal and unimodal downstream tasks through a single model. Through extensive experiments, we demonstrate that our model has the capabilities to solve various meaningful chemical challenges, including conditional molecule generation, property prediction, molecule classification, and reaction prediction.",
  "full_text": "Article https://doi.org/10.1038/s41467-024-46440-3\nBidirectional generation of structure and\nproperties through a single molecular\nfoundation model\nJinho Chang 1 & Jong Chul Ye1\nRecent successes of foundation models in artiﬁcial intelligence have prompted\nthe emergence of large-scale chemical pre-trained models. Despite the grow-\ning interest in large molecular pre-trained models that provide informative\nrepresentations for downstream tasks,attempts for multimodal pre-training\na p p r o a c h e so nt h em o l e c u l ed o m a i nw e r el i m i t e d .T oa d d r e s st h i s ,h e r ew e\npresent a multimodal molecular pre-trained model that incorporates the\nmodalities of structure and biochemical properties, drawing inspiration from\nrecent advances in multimodal learning techniques. Our proposed model\npipeline of data handling and training objectives aligns the structure/property\nfeatures in a common embedding space, which enables the model to regard\nbidirectional information between the molecules’ structure and properties.\nThese contributions emerge synergistic knowledge, allowing us to tackle both\nmultimodal and unimodal downstream tasks through a single model. Through\nextensive experiments, we demonstrate that our model has the capabilities to\nsolve various meaningful chemical challenges, including conditional molecule\ngeneration, property prediction, molecule classiﬁcation, and reaction\nprediction.\nCapturing complex relations between chemical entities and their\nproperties is the essence of numerous chemical challenges. During the\nlast decade, artiﬁcial intelligence has emerged as a promising tool in\nchemistry research for estimating many biochemical properties and\ninteractions between molecules, polymers, and proteins, which are dif-\nﬁcult to obtain experimentally\n1– 3. Various deep learning-based approa-\nches in the chemical domain employed deep neural networks to extract\ndesired characteristics like intrinsic properties, biochemical activities,\nand chemical reactions from raw molecule data\n4– 6. Additionally, de novo\nmolecule design has been extensively studied using recurrent\nnetworks\n7, variational autoencoders8,9,g r a p hn e t w o r k s10,e t c11– 13.M o r e\nrecently, unsupervised learning approaches of learning better repre-\nsentations of the chemical inputs have been suggested\n14– 16 to overcome\nthe limitation of learning separate features for each task in a super- vised\nmanner. These recent approaches are on the same track as the concept\nof the foundation models that are trained with large datasets and are\noften considered as a new paradigm of deep learning\n17,18.\nSpeciﬁcally, a concept of pre-training a neural network in a self-\nsupervised manner for a better feature representation has been\nadapted for various chemicalﬁelds\n14– 16.N - G r a mG r a p h19 and GROVER20\nused a graph neural network and a graph transformer network,\nrespectively, to obtain a pre-trained model from the molecular graph.\nChemBERTa-2\n21 trained a roBERTa model with 77 million molecules to\nbuild a molecular foundation model, by training the model to predict\n200 different chemical property values.\nMeanwhile, in the computer visionﬁeld, multimodal pre-training\nmethods like Vision-Language Pre-training (VLP)\n22 have achieved out-\nstanding performance in downstream tasks that require an under-\nstanding of both image and text. Most of the modern VLP models\nutilize Transformer\n23 architecture and its cross-attention mechanism\nto learn the correlation between different modalities24,25.M o r e o v e r ,\nseveral works introduced contrastive learning, which assimilates fea-\ntures with the same context and distances semantically unrelated\nfeatures, to align image and language features in the common feature\nReceived: 12 February 2023\nAccepted: 27 February 2024\nCheck for updates\n1Graduate School of AI, KAIST, Daejeon, South Korea.e-mail: jong.ye@kaist.ac.kr\nNature Communications|         (2024) 15:2323 1\n1234567890():,;\n1234567890():,;\nspace26– 28. VLP enables various tasks such as visual question\nanswering29, image-text retrieval30, text-driven image generation31,\nimage-driven text generation32, etc., which are not possible using sin-\ngle modality foundation models.\nInspired by the success of multimodal learning, several recent\nworks tried to obtain a better feature of a molecule by leveraging\nknowledge from different data representations. Winter et al. trained a\ntranslation model between Simpliﬁed Molecular-Input Line-Entry Sys-\ntem (SMILES) and International Chemical Identiﬁer (InChI) key to get a\nfeature vector with meaningful information that both molecular\nrepresentations have in common\n33. Zhu et al. used a self-supervised\ntraining method of BYOL34 between different molecule representa-\ntions of SMILES and molecular graphs to build a dual-view model35.\nHowever, these works introduced multimodality only for the\nenhancement of a molecule feature for unimodal tasks, not for the\ninterplay between those different modalities. Furthermore, since\nSMILES, InChI, and graph representations contain almost identical\ninformation about the connection between atoms in a molecule, it is\nunlikely to expect new emergence properties by multimodal learning\nbetween these different molecule representations.\nIn this work, we are interested in the cross-modal comprehension\nbetween molecule structure and the associate properties, which\nfacilitates solving meaningful tasks in many applications like property\npredictions, conditional molecule design\n36,37, etc. Taking a step further\nfrom multi-task learning methods38 which use the prepared properties\nas labels to extract general features21, our approach regards a set of\nproperties as a stand-alone modality that represents the input mole-\ncule and suggests that multimodal learning for molecules with this\nproperty modality can provide much more informative features.\nSpeciﬁcally, we propose a molecule Structure-Property Multi-Modal\nfoundation model (SPMM) that allows various chemistry experiments\nin silico, which is pre-trained with a wide range of molecules’structures\nand a vector of its properties. By employing a Transformer\narchitecture\n23, the intramodal feature extraction and intermodal\nfusion can be done with self-attention and cross-attention mechan-\nisms, respectively.\nOur experimental results show that simultaneous learning of\nstructural features with information from the associate properties\nthrough a single foundation model gives us a better representation\nthat can beﬁne-tuned for various downstream tasks. Speciﬁcally, by\ntreating both structure and property symmetrically, the model can\nperform bidirectional generation and prediction with a single pre-\ntrained model, which was not possible before.\nFigure 1a illustrates the overall model architecture and training\nobjectives for SPMM. The framework of SPMM extends the structure\nof the dual-stream VLP models\n27,28,39. Dual-stream VLP models encode\nthe input for each modality with an unimodal encoder, then use\nanother encoder module to perform cross-attention by using one\nmodality feature as a query and the other modality feature as a key/\nvalue. When a training molecule is given, SPMM takes the molecule’s\nSMILES string and its property vector (PV) as multimodal data inputs as\nshown in Fig.1a. The SMILES and PV are passed through their corre-\nsponding unimodal encoders, which perform self-attention where\nembedded inputs become the key, query, and value. After two unim-\nodal features are obtained, contrastive learning aligns the SMILES and\nPV features into the same embedding space by assimilating the fea-\ntures that contain the same context. This is known to improve the\nmodel performance by making cross-modal encoding easier and\n(a)Pre-training (b)Mul/g415-modaldownstreamtasks\nPV-to-SMILESgenera/g415on SMILES-to-PVgenera/g415on\nForward/retro-\nreac/g415onpredic/g415on\n(c)Single-modaldownstreamtasks\nPropertypredic/g415on\nSMILESProperty Vector\nSMILES-PV\nMatching Loss\nNext Word\nPrediction Loss\nNext Property \nPrediction Loss\nContrastive\nLoss\nPV encoder\nquery query\nSMILES encoder\n[CLS]P\nlogP:1\nMW:79\n…\nlogP:1\nMW:79… c1 c n cc c1 …\n[CLS]S\nc1 c n cc c1 …\nmolecule\nfusion encoder fusion encoder\nprediction head[CLS]S\nauto-regressive\n[CLS]P\nlogP\nMW\n…\nPV encoder\nfusion encoder\nSMILES encoder\nprediction head[CLS]S\nauto-regressive\nfusion encoder\nSMILES encoderSMILES encoder\ntoxicity\nlipophilicity\nbrainpenetra/g415onability\n…\nprediction head\nSMILES encoder\nprediction head[CLS]P\nauto-regressive\nPV encoder\nfusion encoder\nSMILES encoder\n[CLS]P\nlogP\nMW…\nFig. 1 | The overview of the model pipeline of the Structure-Property Multi-\nModal foundation model (SPMM) for its pre-training and downstream tasks.\na The model architecture and pre-training objectives of SPMM. SPMM utilizes the\nmolecule’s Simpliﬁed Molecular-Input Line-Entry System (SMILES) and its Property\nVector (PV). The contrastive loss aligns the output feature of two unimodal enco-\nders into the same embedding space. The fusion encoder learns the relations\nbetween two modalities, trained with Next Word Prediction, Next Property\nPrediction, and SMILES-PV Matching loss. [CLS]S and [CLS]P represent the special\ntoken utilized for SMILES and PV modality, respectively.b Possible downstream\ntask scenarios that require multimodal comprehension, namely PV-to-SMILES\ngeneration and SMILES-to-PV generation.c Possible downstream task scenarios for\nsingle modality inputs, namely property prediction and forward and retro reaction\nprediction.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 2\nguiding the unimodal encoded features to reﬂect more semantics of\nthe input27. Then, the encoded SMILES and PV features are passed\nthrough the fusion encoders, which perform cross-attention between\nSMILES and PV features. This single fusion encoder can perform cross-\nattention with an alternation of its query and key/value input because\nthe contrastive learning aligns the output of the SMILES encoder and\nthe PV encoder into the same feature space\n39. The fusion encoder is\npre-trained with Next Word Prediction (NWP) for SMILES, Next Prop-\nerty Prediction (NPP), and SMILES-PV Matching loss (SPM). Prediction\nof the next component from the given transformer input is a com-\nmonly used self-supervised learning objective, and our NWP and NPP\ntasks make the model learn the contextual relationship between\nSMILES tokens and properties with the aid of the other modality’s\nsemantic feature. Additionally, SPM predicts whether a given pair of\nSMILES and PV represents the same molecule or not.\nOnce trained, SPMM can be used for various bidirectional down-\nstream tasks that require an understanding of both SMILES and\nproperties like property prediction (SMILES-to-properties) and\nproperty-conditioned molecule generation (properties-to-SMILES,\nalso referred to as inverse-QSAR\n37)a ss h o w ni nF i g .1b. Furthermore,\nthe pre-training objectives that we’ve used allow the pre-trained SPMM\nto be applied for single-modality tasks as well, such as molecule clas-\nsiﬁcation and reaction predictions (see Fig.1c). The pre-trained SPMM\nshowed comparable performances to state-of-the-art models in these\nunimodal tasks, which suggests the model’s generalization ability as a\nfoundation model.\nResults\nThe model learns bidirectional comprehension between SMILES\nand properties\nOnce SPMM was pre-trained, we made the model generate SMILES\nwith given PV inputs only, which is a crucial challenge for many che-\nmical tasks such as de novo molecule design. As one of the major\napproaches for drug discovery, various methods have been suggested\nfor generating molecules with desired properties\n9– 11,13. In the approa-\nches presented so far, the maximum number of simultaneously con-\ntrollable properties wasn’t very large. Also, the length of the input\nproperty vector cannot be changed.Whenever the target properties\nchange, the model needs to be trained again for the new wanted\nconditions. In contrast, the pre-trained SPMM can take 53 properties\nused in pre-training as input conditions and generate molecules that\nsatisfy all of them, without separate additional training for each\nproperty combination. Moreover, for the properties that we don’t\nwant to control, we can let the model ignore those conditions by\nreplacing them with the [UNK] token that we used in pre-training. This\nis very useful because controlling all 53 input properties is not a usual\nscenario in practice, and is also not easy since the properties are cor-\nrelated and entangled (e.g.,‘5 atoms & 30 bonds’ or ‘2r i n g s&5a r o -\nmatic rings’ is unlikely to be a valid PV input).\nTo demonstrate the molecule generation capability of SPMM, we\nprepared a number of PV-to-SMILES generation scenarios and let the\npre-trained SPMM autoregressively generate SMILES using the input\nproperties. This process of SPMM is very similar to the sequence-to-\nsequence translation tasks in terms of the model pipeline (see Sup-\nplementary Fig. 1a for details), from the property sentence of PV to the\nmolecular structure sentence of SMILES.\nThe validity, uniqueness, and novelty of the generated molecules\nare the quantitative metrics of SPMM’s molecule generation. Addi-\ntionally, as a qualitative metric to see how the generated SMILES match\nthe property input, we measured the normalized Root Mean Square\nError (normalized RMSE) between the input conditions and the gen-\nerated molecules’ properties. More speciﬁcally, we calculate the\naverage of the RMSE of all controlled properties, after those values are\nnormalized with the corresponding property’s mean and standard\ndeviation in the pre-training dataset. We note that RMSE was calcu-\nlated on the normalized scale of each property because the values of\nthe properties span multiple orders of magnitude.\nFor theﬁrst PV-to-SMILES generation scenario, we prepared 1000\nPVs of SMILES from PubChem\n40 that are not contained in the pre-\ntraining dataset and fed them to the pre-trained SPMM to generate\nappropriate SMILES. Here, the sampling process was done in a deter-\nministic manner: starting from the SMILES [CLS] token ([CLS]\nS), the\nmodel predicts the probability distribution of the next token and\nchooses the option with the highest probability. Theﬁrst row of Table1\nshows its results. Among the output of deterministic PV-to-SMILES\ngeneration for 1000 PVs, 99.5% of the generated output were valid\nSMILES. The mean RMSE of the 53 normalized properties was 0.216,\nwhich implies that the properties of the generated samples agree with\nthe property input.\nApplication ﬁelds like drug discovery often require generating\nmultiple molecules for a single wanted target property condition. This\ncan be done by sampling the next token stochastically from the\nmodeled probability distribution instead of using a token with the\nhighest probability. To verify our model’s ability to generate multiple\nmolecules from a single PV input, we generated 1000 SMILES with\nstochastic sampling on aﬁxed PV. Figure2 shows the property dis-\ntributions of 1000 molecules generated from a single PV input. The\nmode of each property distribution lands on the input property value\n(Fig. 2a). In the situation when only some of the properties are given,\nthe model only regards the known properties while the other masked\nproperties are not restricted (Fig.2b, c). SPMM can generate molecules\neven with no property information at all; when all input properties are\nreplaced with [UNK] token (Fig.2d), the model performs an uncondi-\ntional molecule generation, and the output follows the distribution of\nthe pre-training dataset. The validity, uniqueness, and novelty of the\ngenerated molecules under conditions in Fig.2 are listed in the‘sto-\nchastic’ rows of Table1. The validity, uniqueness, and noveltyﬂuc-\ntuated depending on how feasible or difﬁcult the property input was,\nTable 1 | Quantitative and qualitative results on various scenarios of PV-to-SMILES generation tasks, with the mean value and\nstandard deviations\nSampling input PV Validity Uniqueness Novelty normalized RMSE\ndeterministic 1000 unseen PubChem SMILES ’ PV 0.995 ± 0.001 0.999 ± 0.001 0.961 ± 0.005 0.216 ± 0.004\nstochastic full PV of the molecule 1 0.974 ± 0.005 0.905 ± 0.007 0.998 ± 0.003 0.185 ± 0.004\nMolar mass = 150 0.974 ± 0.007 0.945 ± 0.006 0.872 ± 0.007 0.192 ± 0.010\n#ring = 2, #aromatic ring = 1, TPSA = 30,\nQED = 0.8\n0.998 ± 0.002 0.981 ± 0.006 0.952 ± 0.013 0.257 ± 0.025\nno property control 0.971 ± 0.004 0.991 ± 0.003 0.950 ± 0.003 -\nFor deterministic sampling, we ran the experiment with four different random sets of 1000 unseen Property Vector (PV)s. In the case of stochastic scenarios, four different random seeds were used for\neach experiment;\nTPSA Topological Polar Surface Area,QED Quantitative Estimate of Drug-likeness67, RMSE Root Mean Square Error.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 3\nand it was greater than 0.9 in most cases. Supplementary Table 1 shows\nthat SPMM performed better at generating valid, novel, and desired\nmolecules compared to other benchmark models\n10,41– 43, in both\nunconditional and conditional molecule generation scenarios. More\nexamples of the generated molecule can be found in Supplemen-\ntary Fig. 2.\nThe aforementioned results demonstrate that SPMM can perform\nmolecule generation with arbitrary PV inputs, which enables simple\nmolecule designing and editing. As possible examples of molecular\nediting, Fig.3 contains the output of the SPMM’s stochastic molecule\ngeneration forﬁve PV inputs, which all originated from the PV of the\nmolecule1 but four of them had certain values changed. The generated\nmolecules follow the input modiﬁcation while maintaining unmodiﬁed\nproperties similarly. SPMM is even able to generate molecules with the\nout-of-domain conditions such as‘log P =7 ’ (note that ~5% of the pre-\ntraining dataset has logP >7 ) .\nRegarding the overall molecule generation performance of\nSPMM, we want to emphasize that SPMM can generate suitable SMILES\nfor many property conditions that the model has not seen in its pre-\ntraining. When we trained SPMM without 50% of random property\nmasking with [UNK] token, the model only worked when all 53 prop-\nerties are given since the model has not seen the partially-given\nproperties. However, even with the technique of [UNK] token masking,\nthe model cannot face most of the 2\n53 possible property combination\nduring the pre-training process. The SPMM’s ability to handle arbitrary\nproperty conditions for SMILES generation comes from treating PV as\na ‘language with 53 words’ and focusing on each property separately,\nnot simply considering the entire property input as a single condition.\nFig. 2 | Property distribution of the generated molecules with different Prop-\nerty Vectors (PV) inputs and [UNK] token masking.The gray vertical dotted lines\nare the input property values. For the properties with continuous range, we\nincluded kernel density estimate plots with red or blue solid lines. The controlled\nproperties are colored in red, and the uncontrolled properties are colored in blue.\nOnly 12 out of 53 properties are shown for each scenario. TPSA Topological Polar\nSurface Area, logP octanol-water partition coefﬁcient, MR Molar Refractivity, QED\nQuantitative Estimate of Drug-likeness\n67. Source data are provided as a Source Data\nﬁle. a All 53 properties are controlled with the input PV obtained from the molecule\n1. b Molar mass to 150, and the other property inputs are masked with [UNK] token.\nc #ring, #aromatic ring, TPSA, and QED are controlled to 2, 1, 30, and 0.8. The other\nproperties are masked.d Every property is masked.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 4\nThis innovative approach for conditional molecule generation has\nnever been demonstrated with the existing methods and thus can be\nused for many important chemicalﬁelds.\nWith the same approach as SMILES generation, the pre-trained\nSPMM can also be used to generate a PV with SMILES input only. This\ntask is equivalent to performing 53 property predictions of a given\nSMILES at once. Similar to the PV-to-SMILES generation, properties are\npredicted in an autoregressive manner: the model predicts theﬁrst\nproperty value using only the property [CLS] token ([CLS]\nP), then takes\nall previous outputs again to get the next prediction value, and so on\n(see Supplementary Fig. 1b). Although 53 properties that we’ve used\nc a nb ec a l c u l a t e du s i n gt h eP y t h o nm o d u l e ,t h ep u r p o s eo ft h i s\nexperiment is to verify that the data-driven way of property estimation\ncoincides with the analytic approach.\nSpeciﬁcally, we fed 1000 SMILES from the ZINC15 dataset\n44,w h i c h\nare not contained in the pre-training dataset, to the pre-trained SPMM\nand generated their corresponding PV. Figure4 is the scatter plot of\nthe real property value against the generated output for 12 selected\nproperties out of 53 that we used for pre-training. It is clear that\nSPMM’s predicted property is very close to the actual value, and most\nof the data point lies on they = x line. Although the model virtually has\nnever seen a full-ﬁlled PV in the pre-training due to the 50% of random\nproperty masking, the model could autoregressively predict all 53\nproperties as a whole. The meanr\n2 score of the 53 properties was\n0.924. The full scatter plot for all 53 properties with eachr2 score and\nraw RMSE is in Supplementary Figs. 3 and 4.\nTo provide an interpretation of the pre-trained SPMM’sp e r f o r -\nmance presented so far, we further analyzed the learned cross-modal\ncomprehension between SMILES and property vectors by visualizing\nthe attention scores from the pre-trained SPMM. Transformer-based\nmodels have the beneﬁt of intuitive attention visualization that shows\nhow the model considers the relation between the input queries and\nkeys, by providing cross-attention scores between them. In Fig.5,w e\nplotted the cross-attention score from the last fusion layer of our pre-\ntrained SPMM when SMILES and its property vector inputs were given.\nSince there are multiple heads for the cross-attention, we took the\nmean of their attention scores. It is interesting that the aspect of cross-\nattention scores followed the intuitive relations between chemical\nproperties and molecular fragments. The properties related to\nhydrogen bonding (‘NumHDonors’, ‘NumHAcceptors’)s h o wh i g h\nattention scores for tokens with oxygen and nitrogen atoms. The\nproperty ‘RingCount’ focuses on the tokens that are involved with\nrings while showing weak attentionto side groups, and the property\n‘NumAromaticRings’ only gives high attention score to the compo-\nnents of aromatic rings. When different SMILES tokens played a similar\nr o l ei nt h em o l e c u l es u c ha s‘c1ccccc1)’and ‘c1ccccc1’in the molecule7,\ntheir attention patterns were similar as well. This result demonstrated\nthat SPMM could capture the relations between molecule structures\nand chemical properties without explicitly-given supervision between\nthem. For more statistical analysis, we also observed which tokens\nshow high attention scores for 12 chosen properties, using 1000 ran-\ndomly sampled molecules’ cross-attention map. The result showed\nthat tokens frequently related to certain property tend to show high\nattention score to that property;‘TPSA’ got high attention scores\ntowards tokens with polar atoms like oxygen and halogen atoms,\n‘NumHAcceptors’ got tokens that involve with hydrogen bonding, and\n‘NumAromaticRings’ got the components of aromatic rings. More\ndetailed lists of tokens for each property can be found in Supple-\nmentary Table 2.\nGeneralization ability as a molecular foundation model\nSo far, we have demonstrated that the pre-trained SPMM can be\napplied to tasks that require an understanding of the relationship\nbetween SMILES and properties. However, we can also employ the pre-\ntrained SPMM for challenges that only use SMILES data, such as\nmolecular property prediction. One advantage of having a dual-stream\nVLP model structure is that the SPMM’s multimodal pre-training pro-\ncess includes adjusting the output of one unimodal encoder to contain\ncontextual information from the other modality, by aligning it with the\nother unimodal encoder’s output. This implies that the SMILES\nFig. 3 | Examples of molecule editing, by changing speciﬁcv a l u e sf r o mt h e\noriginal Property Vectors (PV) and performing PV-to-SMILES generation\nwith it.The colored output values correspond to the changed properties from the\noriginal PV. TPSA Topological Polar Surface Area, logP octanol-water partition\ncoefﬁcient, QED Quantitative Estimate of Drug-likeness\n67. (1) The output of the\nsame PV of the source molecule. (2) The output when #aromatic ring is changed to\n0. (3) The output when #ring is changed to 2 and #aromatic ring is changed to 1.\n(4) The output when logP is changed to 7. (5) The output when #rotatable bond is\nchanged to 12. For the generation, the other 41 property conditions are masked by\nthe [UNK] token.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 5\nencoder output is a unimodal representation vector, that not only\nembeds the input molecule’s structural information but it’s also\nenhanced by its property information.\nWe have analyzed if our pre-trained model had learned an infor-\nmative representation that can be readily used for other tasks, even for\na single modality. So we only utilized the SMILES encoder of pre-\ntrained SPMM (see Supplementary Fig. 1c) and made a benchmark\nstudy on nine MoleculeNet\n45 downstream tasks and a Drug-Induced\nLiver Injury (DILI) prediction task. Each MoleculeNet task is a regres-\nsion or classiﬁcation task for pharmaceutical/biochemical applications\nlike solubility, toxicity, and brain penetrability. The DILI classiﬁcation\ntask was done to overcome the potential limitation of open\ndatabases\n46,47 and verify if SPMM could be extended to more complex\nendpoints. The task is to classify whether the given molecule has a risk\nof causing liver injury. Since many proposed DILI machine learning\nmodels have built their dataset rather than using common bench-\nmarks, we took the dataset preparations from a known publication\n48\nand compared the performance with it for a fair evaluation.\nTable 2 contains the performance of SPMM and other models for\nMoleculeNet. Using only 6 BERT encoder layers, SPMM showed com-\nparable performances with state-of-the-art models for all tasks. It\nachieved the best performance forﬁve tasks out of nine, showing its\ncapability as a foundation model. We’ve also observed that the score of\nour model dramatically decreased without pre-training. SPMM also\noutperformed the proposed 5-ensemble models on the DILI\nclassiﬁcation task under the same data preparation as shown in\nTable 3, which was not the case for the naive BERT layers without\nSPMM pre-training.\nWe also trained SPMM for the forward and retro-reaction pre-\ndiction tasks, which require the model to predict the product SMILES\nfrom the reactant SMILES and vice versa. Regarding both tasks as\nsequence-to-sequence generation, the model pipeline for these reac-\ntion prediction tasks is the same as the PV-to-SMILES generation tasks,\nexcept the PV encoder is replaced with the SMILES encoder (see\nSupplementary Fig. 1d). The detailed task deﬁnition and dataset pre-\nparation are described in the Methods section.\nTable 4 shows the performances of SPMM and other benchmark\nmodels on forward and retro-reaction prediction tasks. Although the\nreaction prediction tasks are not the best scenario for the property-\nemergence features to play signiﬁcant roles, SPMM showed the high-\nest top-1 accuracy in the forward-reaction task with a relatively small\npre-training data size (i.e., 50 M molecules, compared to 100 M\nmolecules of Chemformer). SPMM also achieved the second-best top-1\naccuracy among the string-based retro-reaction task models.\nDiscussion\nIn this work, we proposed a transformer-based multimodal chemical\nfoundation model SPMM. The proposed model allows for bidirectional\ngeneration/prediction of molecular structure and properties, as well as\nunimodal tasks like reaction prediction. During the process, we\nFig. 4 | Scatter plots of the 1000 ZINC15 molecules’ real property value against\nthe generated output, for 12 selected properties.The x-axis is the real property\nvalue, and they-axis is the model output. The gray dotted line is they = x line. TPSA\nTopological Polar Surface Area, logP octanol-water partition coefﬁcient. QED\nQuantitative Estimate of Drug-likeness67. Source data are provided as a Source\nData ﬁle.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 6\nintroduced a method of treating property collections as a language so\nthat the model could learn the relationship between SMILES tokens\nand each property independently. We demonstrated that pre-trained\nSPMM showed remarkable performances in problems for interactions\nbetween SMILES and PV domains. And not only for multimodal chal-\nlenges but even its unimodal feature for SMILES, SPMM also provides a\nuseful representation that can be ﬁne-tuned for many molecular\ndownstream tasks. It is important to note that all of these results were\nobtained with a pre-training of 50 million molecules, which is relatively\nsmall compared to other large pre-training approaches and still has\nroom for better performance with more data and parameters. We also\nnote that we’ve gathered our 53 properties to let them cover the widest\nrange possible, rather than paying the best effort to select the most\neffective combination of properties. This implies the proposed\nstructure-property multimodal training can beﬂexibly adopted with\ndifferent property selections, according to the given speci ﬁed\nscenarios.\nOne might consider that treating a PV as tabular data, handling its\nelements without predetermined order, could be a more straightfor-\nward approach. However, the recent theoretical work\n49 showed that\nautoregressive modeling is a universal learner that is not speciﬁct oa n y\ndata type. Moreover, the transformer architecture’s permutational\ninvariance of the positional encoding has been well documented and\nutilized50,51. Additionally, if the order of PVs is permuted in a different\norder, then the learnable positional embedding would learn different\nembedding that takes into account the optimal alignment between the\nproperties and the SMILES embeddings. This makes the output of the\ntransformer’s self-attention and cross-attention mechanism is further\ninvariant to the position of each feature vector. Given all this evidence,\nwe believe that our way of utilizing the PV encoding is theoretically and\nempirically well-supported. In fact, when we modiﬁed the training\nobjectives (more details in the Pre-training objectives section) to pre-\ntrain our model with a purely order-invariant PV, this did not improve\nperformance on the overall downstream tasks (see Supplementary\nTable 3). Moreover, we observed that utilizing different property\norders for constructing PVs does not affect the overall performance of\nSPMM (see Supplementary Table 4).\nDespite the noticeable performances of SPMM, it has several\nchances for improvement. One of those comes from using the SMILES\nnotation. Although SMILES can contain full details about the 2D\nstructure of the molecule, the information on how atoms and bonds\nare connected only exists implicitly. Also, a slight modiﬁcation in\nmolecular structure can be a drastic change in SMILES. Graph format is\nanother widely used modality for molecule representation that con-\ntains the explicit information of the adjacency matrix, which can be an\nalternative for SMILES. Another limitation in our current SPMM is that\nthe 53 properties we used happen to be invariant with the changes in\nthe stereochemistry of the given molecule. It is known that considering\nstereochemistry plays a crucial part in various biochemical tasks.\nHowever, the 53 properties we used cannot provide any knowledge\nabout stereochemical information since their values are unchanged in\ndifferent stereoisomers. This makes the SMILES encoder output of\ndifferent stereoisomers converge since the contrastive loss aligns\nthem to the same PV feature. We believe this is the prominent factor\nO=C1CC(c2ccccc2)OC2=C1C(O)C(O)CC2CCN(C)CCC(=O)C(c1ccccc1)c1ccccc1\n78\nc1ccccc1)\n(a) (b)\nc1ccccc1\n=O)\nC(\nCCC(\nCCN(\nC)\n2\n=C\nc2ccccc2\nOC\nCC(\nO=\nC1\nCC2\nC(O)\n1\n#H bonding donor\nC(O)\n#H bonding acceptor\nMolar mass\n#ring\n#aroma/g415c ring\n#rotatable bond\nTPSA\n#heavy atom\nlogP\nMolar refrac/g415vity\n#nitrogen + #oxygen\nQED\n#H bonding donor\n#H bonding acceptor\nMolar mass\n#ring\n#aroma/g415c ring\n#rotatable bond\nTPSA\n#heavy atom\nlogP\nMolar refrac/g415vity\n#nitrogen + #oxygen\nQED\nFig. 5 | The mean attention score from the attention heads in the Structure-\nProperty Multi-Modal foundation model (SPMM) fusion encoder’s ﬁnal cross-\nattention layer.Two sample molecules (a)a n d(b)w e r eu s e df o rt h i sﬁgure.\nA darker green means a higher attention score. For the attention calculation pro-\ncess, the property features were used as queries, and the SMILES features were used\nas keys and values. The corresponding fragments for each token are indicated with\nivory boxes on the molecular structure, while fragments for duplicated tokens are\ncolor-coded with purple. We have calculated cross-attention scores for all 53\nproperties and SMILES tokens, but only 12 of those properties are shown. TPSA\nTopological Polar Surface Area, logP octanol-water partition coefﬁcient, QED\nQuantitative Estimate of Drug-likeness\n67. Source data are provided as a Source\nData ﬁle.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 7\nthat lowered the performance of SPMM in MoleculeNet tasks, which\ncould be resolved by using more properties that reﬂect the molecule’s\nstereochemistry. Moreover, validation through wet-lab experiments to\nverify the model’s predicted/generated properties is another possible\nfurther study. Overcoming these drawbacks of the current study and\nmaking the model more applicable to other chemical tasks could be\nthe works for the future.\nNevertheless, we believe that our approach can provide a pre-\ntrained model capable of encompassing each input domain and their\nmultimodal domain simultaneously, which has a vast potential utility.\nWe expect this approach to be applied to more various and practical\nchemical situations by using broader and richer molecular modalities,\nand possibly, different biochemical domains like polymers and\nproteins.\nMethods\nHandling SMILES and property values as a language\nMolecules can be represented with various formats such asﬁnger-\nprints, strings like SMILES, InChI, or a molecular graph. Since these\ndifferent notations contain almost the same information about com-\nplete molecular structure, we employed SMILES to describe a molecule\nstructure. SMILES is a sequence of characters that represents the\nconnection structure of the molecule. Many researchers treat SMILES\nas a variant of language data and utilize a concept of language models\nfor chemical tasks on SMILES data\n11,21,52.\nFigure 6a illustrates our embedding procedure for the input\nSMILES. The raw SMILES string is tokenized by the tokenizer and\nembedded by the SMILES encoder with the [CLS]S token and the [SEP]\ntoken. Here, [CLS] token is a special token attached to the beginning of\nevery input sequence\n53. Although the [CLS] token itself doesn’tc o n t a i n\nany meaning, the bidirectional attention mechanism of the model\nallows the [CLS] token to contain contextual information of the entire\ninput. Once the model is pre-trained, the [CLS] token output of the\ngiven sequence can be considered as an input representation vector\na n db eu s e df o rc l a s s iﬁcation/regression downstream tasks, as in many\nBERT variations for images\n54,55 and VLP27.\nIn the SMILES tokenization, our tokenizer tokenizes a given\nSMILES into fragments that are contained in a prepared token dic-\ntionary of 300 subwords. This dictionary was obtained from the pre-\ntraining data SMILES corpus by the BPE algorithm\n56,w h i c hs t a r t sf r o m\na set of simple characters and iteratively appends the most frequent\ntoken pairs as a merged subword. Being widely adopted for various\nlanguage models\n57,58, the BPE algorithm has provided a subword dic-\ntionary containing common functional groups and substructures like\nbenzene rings, carbonyl groups, two- letter atoms, and amino groups.\nCompared to naive character-wise tokenization which considers each\ncharacter as a separate token, the merged subwords help the model’s\nchemical inference for chemical groups and reduce the total number\nof tokens.\nFor this work, we built a PV for each molecule that contains 53\nmolecular properties and considered this as a sentence with a length of\n53. These properties from the RDKit python library\n59 cover a wide\nrange from simple ones, such as the number of rings and molar mass,\nto complex properties like solubility, TPSA, and druggability. The\ntransformer architecture of our model considers each element of PV as\na token to perform the attention mechanism, which is equivalent to\nregarding PV as a semi-sentence of 53 properties. Although the size of\nthe vocabulary is more limited and their order isﬁxed compared to\nnatural language, it provides much more precise and compact infor-\nmation about the 53 properties. One beneﬁt of regarding PV as a lan-\nguage is that we do not have to collect all elements to build a valid PV.\nIn contrast to a simple vector input, some property elements can be\nremoved or masked in our approach.\nFigure6b shows our embedding procedure for the input PV. Each\nproperty element in the PV is a numerical value and normalized with\nTable 2 | Benchmark results on MoleculeNet downstream tasks\nregression[RMSE,↓] classiﬁcation[AUROC in %,↑]\nDataset Delaney ESOL LIPO Freesolv BACE Clearance BBBP BACE Clintox SIDER\n#data 1128 4200 642 1513 837 2039 1513 1478 1427\n#task 1 1 1 1 1 1 1 2 27\nD-MPNN68 1.050 ± 0.008 0.683 ± 0.016 2.082 ± 0.082 2.253 a 49.754 71.0 ± 0.3 80.9 ± 0.6 90.6 ± 0.6 57.0 ± 0.7\nN-GramRF19 1.074 ± 0.107 0.812 ± 0.028 2.688 ± 0.085 1.318 a 52.077a 69.7 ± 0.6 77.9 ± 1.5 77.5 ± 4.0 66.8 ± 0.7\nN-GramXGB19 1.083 ± 0.082 2.072 ± 0.030 5.061 ± 0.744 - - 69.1 ± 0.8 79.1 ± 1.3 87.5 ± 2.7 65.5 ± 0.7\nPretrainGNN69 1.100 ± 0.006 0.739 ± 0.003 2.764 ± 0.002 - - 68.7 ± 1.3 84.5 ± 0.7 72.6 ± 1.5 62.7 ± 0.8\nGROVERlarge\n20 0.895 ± 0.017 0.823 ± 0.010 2.272 ± 0.051 - - 69.5 ± 0.1 81.0 ± 1.4 76.2 ± 3.7 65.4 ± 0.1\nChemRL-GEM70 0.798 ± 0.029 0.660 ± 0.008 1.877 ± 0.094 - - 72.4 ± 0.4 85.6 ± 1.1 90.1 ± 1.3 67.2 ± 0.4\nChemBERTa-2(MTR-77M)\n21 0.889a 0.798a -1 . 3 6 3 a 48.515a 72.8a 79.9a 56.3a -\nMolFormerb71 0.880 ± 0.028 0.700 ± 0.012 2.342 ± 0.052 1.047 ± 0.029 43.175 ± 1.537 73.6 ± 0.8 86.3 ± 0.6 91.2 ± 1.4 65.5 ± 0.2\nSPMM(w/o pre-train) 1.272 ± 0.015 1.009 ± 0.021 3.018 ± 0.179 1.675 ± 0.010 53.544 ± 0.312 66.6 ± 0.3 78.7 ± 2.6 76.3 ± 1.5 57.1 ± 1.6\nSPMM 0.817 ± 0.010 0.681 ± 0.004 1.868 ± 0.041 1.041 ± 0.022 42.607 ± 0.675 75.1 ± 0.9 84.4 ± 0.4 92.7 ± 0.7 66.9 ± 0.9\nFor each task, weﬁne-tuned our model in four random seeds and recorded the mean and the standard deviation of those results. The benchmark model results were taken from ChemRL-GEM and ChemBERTa-2.\nRMSE Root Mean Square Error,AUROC Area Under Receiver Operating Characteristic curve.\naThe standard deviation cannot be found in the source of the benchmark results.\nbUnofﬁcial results, obtained from the ofﬁcial checkpoint under our data preparation.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 8\nTable 3 | The Drug-Induced Liver Injury (DILI) classiﬁcation task performance of Ai et al.48 and the Structure-Property Multi-\nModal foundation model (SPMM)\nmodel Acc in %[ ↑] Selectivity in %[ ↑] Speci ﬁcity in %[↑] AUROC in %[ ↑]\nAi et al.48 (best single model on training set) 81.1 81.0 81.5 89.6\nAi et al. (5-ensemble) 84.3 86.9 75.4 90.4\nSPMM(w/o pre-train) 72.6 70.6 79.2 82.0\nSPMM 84.4 83.9 84.6 92.6\nTable 4 | The performance of the Structure-Property Multi-Modal foundation model (SPMM) and other works on the forward\nand retro-reaction prediction task\nforward prediction molecule modality top-k accuracy in %[ ↑]\nstring-based graph-based k = 1 k = 2 k = 3 k = 5\nMolecular Transformer72 O 88.7 92.1 93.1 94.2\nAugmented Transformer63 O9 0 . 6 9 4 . 4 - 9 6 . 1\nChemformerlarge\n64 O 91.3 - - 93.7\nGraph2SMILES73 O O 90.3 - 94.0 94.8\nMEGAN74 O 86.3 90.3 92.4 94.0\nLocalTransform6 O 90.8 94.8 95.7 96.3\nSPMM O 91.5 93.5 94.6 95.4\nretro-reaction prediction top-k accuracy in %[ ↑]\nk=1 k=5 k=1 0\nSCROP75 43.7 65.2 68.7\nTwo-way Transformer76 47.1 73.1 76.3\nAugmented Transformer63 48.3 73.4 77.4\nChemformerlarge\n64 54.3 62.3 63.0\nSPMM 53.4 67.6 70.3\nFor the retro-reaction prediction task, we only prepared the benchmark results of string-based models. The benchmark model results are from the papero fL o c a l T r a n s f o r m6 and Chemformer64.\nlinear layer\nproperty value encoding\n(convert each property to a feature vector)\n50% property masking \nwith a feature of [UNK] token\nvalue of property #1 #2 #3 #4 #5 #6 #7 #8 …\nvalue o\nf\n propert\nf\n y\n #1 \n#2\n #3 \n#\n4\n #5 \n#\n6 #\n7\n #8 …\ninput molecule\nword embedding\n(convert each token to a feature vector)\nadd posi/g415onal embedding\nSMILES: c1cnccc1\ntokeniza/g415on according to the word dic/g415onary\n[CLS]P\n[CLS]P\n[UNK]\n[UNK]\n[UNK]\n[UNK]\n[CLS]P\n[UNK]\n[UNK]\n[UNK]\n[UNK]\nto PV encoder\n[CLS]S c1 c n cc c1 [SEP]\n[CLS]S\nc1\nc\nn\ncc\nc1\n[SEP]\n[PAD]\n[PAD]\n[PAD]\n[CLS]S\nc1\nc\nn\ncc\nc1\n[SEP]\n[PAD]\n[PAD]\n[PAD]\nto SMILES encoder\n(a)SMILESinputpipeline\nadd posi/g415onal embedding\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n …\n0\n+\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n …\n0\n+\n(b)PVinputpipeline\n53 properties with predetermined order\nn\nc1\ncc\nC\n…\n[CLS]S\n[SEP]\n[PAD]\nc\n…\nFig. 6 | Embedding process for SMILES and the corresponding Property Vec-\ntor(PV). aThe SMILES representation of the input molecule is tokenized with the\nword dictionary, which was obtained with the BPE algorithm. Each token in the\ntokenization result is replaced with its corresponding word embedding, then\npassed to the SMILES encoder with positional embedding added.b Al i s to ft h e\nmolecule’s chemical properties is prepared and passed through a linear layer. The\noutput vectors are randomly replaced with special [UNK] token, and the result is\npassed to the PV encoder with positional embedding.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 9\nthe mean and standard deviation of that property. The order of these\n53 properties is predetermined, as Supplementary Table 4 shows that\nthe performance of SPMM isn’t affected by certain property orders\nused in the PV. Each value in the PV is encoded to a feature vector using\na linear layer as a value encoding. Then we randomly replace 50% of the\nproperty features into the [UNK] token, which is the special token\nutilized to simulate that the property is unknown. This is possible since\nthere is no problem in describing a molecule using only a part of these\nproperties. Random property feature masking prevents the model\nfrom overly dependent on the speciﬁc property, has the effect of data\naugmentation, and improves the model ’s generalization ability.\nAlthough every property we used in this work can be easily and thor-\noughly prepared by the computer, this might not be the case for other\nproperties in real-world situations. SPMM still can be trained when\nsome properties for certain training molecules are not known, by\nreplacing those unknown properties with the [UNK] token. On top of\nthe randomly-masked value encoding, we added a learnable positional\nencoding similar to that in BERT. Since a PV explicitly contains the\nvalues only, this positional embedding provides information about\nwhat property each value corresponds to. Also, because of the pre-\ndeﬁned order of these properties, this position embedding is equiva-\nlent to giving a unique index for each property and adding an\nembedding of that corresponding index. Then we pass theﬁnal result\nto the PV encoder with the [CLS]\nP token.\nPre-training objectives\nContrastive learning aims to learn better unimodal representation by\naligning the features from different modalities into the same feature\nspace\n26. When the encoded features of [CLS] tokens of SMILESS and\nPV P are given asScls and Pcls, we calculate the similarity function\nsim(S, P) and sim(P, S)a s :\nsim S,PðÞ = hS Scls\n/C0/C1/C0/C1 ⊺hP Pcls\n/C0/C1\n,sim P,SðÞ = hP Pcls\n/C0/C1/C0/C1 ⊺hS Scls\n/C0/C1\nð1Þ\nwhere hS and hP are the linear projection + normalization layer for\nSMILES and property vector, respectively. Now, for a given pair ofS\nand P, we calculate the SMILES-to-PV and PV-to-SMILES intermodal\nsimilarities as follows26,27:\nss2p = expðsimðS,PÞ=τÞ\nPN\nn =1\nexpðsimðS,PnÞ=τÞ\n, sp2s = expðsimðP,SÞ=τÞ\nPM\nm =1\nexpðsimðP,SmÞ=τÞ ð2Þ\nwhereM and N are the total numbers of SMILES and PV used in the loss\ncalculation. Here,τ is a learnable temperature parameter, which has a\nsharpening effect by exaggerating the similarity difference. The\nintramodal similarities can be calculated in the same way.\ns\ns2s = expðsimðS,SÞ=τÞ\nPM\nm =1\nexpðsimðS,SmÞ=τÞ\n, sp2p = expðsimðP,PÞ=τÞ\nPN\nn =1\nexpðsimðP,PnÞ=τÞ ð3Þ\nThe overall contrastive loss is deﬁned using the cross-entropy loss\nH and one-hot similarityy, which contains 1 for pairs originating from\nthe same molecule and contains 0 otherwise.\nLcontrastive = 1\n2 ðHðys2p, ss2pÞ + Hðyp2s, sp2sÞ + Hðys2s, ss2sÞ + Hðyp2p, sp2pÞÞ ð4Þ\nFollowing the recent contrastive loss application in VLP60,w e\nbuild the SMILES and PV queues that store thek most recent SMILES\nand PV instances and use them for contrastive loss. We set our queue\nsize k to 24,576.\nNext Word Prediction (NWP) trains the model to predict the\n(n + 1)-th SMILES token when 0∼ n-th tokens and the corresponding PV\nare given. Predicting the next token is a common objective for training\nlanguage models, known for being utilized in the pre-training of GPT\n61.\nThis can be done with a singleﬂow for each SMILES by applying a\ncausal mask in the self- attention of the SMILES encoder and the fusion\nencoder. LetS ={ s0,s 1, …,s n}a n dP denote the input SMILES and the\ncorresponding PV, andpNWP (sn | s0:n−1, P) denote the model’sp r e d i c t e d\nprobability distribution of then-th token with givenP and 0 ~ (n-1)-th\nSMILES tokens. The loss for NWP is deﬁned as follows:\nLNWP =\nXn\ni =1\nHðyNWP\nn ,pNWP ðsnjs0:n/C0 1,PÞÞ ð5Þ\nwhere yn\nNW P is a one-hot label for then-th SMILES tokensn.\nWe applied a similar concept of NWP for the property vector as\nNext Property Prediction (NPP). NPP makes the model predict the next\nproperty value using its corresponding SMILES and the previous\nproperties. Since each property element is a numerical value, we\nreplaced the cross-entropy loss in NWP with mean-square-error loss.\nWhen S and P ={ p\n0,p 1, …,p n} denotes the input SMILES-PV pair and\n^pn(p0:n−1, S) denotes the model’s predicted next property values with\ncausal mask in the PV and the fusion encoder, the loss for NPP is given\nas follows:\nLNPP =\nXn\ni =1\npn /C0 ^pnðp0:n/C0 1,SÞ\n/C0/C1 2\nð6Þ\nIn NPP, the model does not predict the property value if it is\nreplaced with [UNK] token.\nSMILES-PV Matching (SPM) learns if a given SMILES-PV pair (S, P)\nis matched or not. We concatenate the feature of [CLS]S and [CLS]P\ntoken from the fusion encoder output and pass this through a linear-\nlayer SPM head. Whenp\nSPM (S, P) is the output of the SPM head, the\nSPM loss can be deﬁned as\nLSPM = HðySPM , pSPM ðS,PÞÞ ð7Þ\nwhere ySPM is aone-hot vector for a binary label of SMILES-PV matching;\nthe label is 1 ifS and P originated from the same molecule and 0\notherwise. To build negative samples for SPM, we randomly select a\n‘negative’ pair for each SMILES and PV instance from the other\nmodality and match them as negative pairs. This negative pair\nselection was done by hard-negative mining, which gives a higher\nchance of being selected as a negative pair for instances that has a\nhigher similarity of Eq. (2)b u ti s n’t a positive match. This makes the\ntraining more difﬁcult and forces the model to learn how to distinguish\nsimilar instances.\nIn contrastive learning, using a one-hot label could be too strict\nsince it regards all instances that came from other pairs as equally\nnegative instances. However, some PVs might agree with many\nSMILES, not only one SMILES that they’re paired with. Even SMILES can\nbe matched with different PVs since there’s a 50% of masking in a PV\n(e.g.,‘molar mass = [UNK], logP = 2.1, #atom = 12’and ‘molar mass = 78,\nl o g P=2 . 1 , # a t o m=[ U N K ]’ both explain Benzene, even if they came\nfrom different molecules). A similar problem also occurs for NWP.\nSometimes there could be multiple sensible options for being the next\ntoken, but using a one-hot label forground truth might ignore this.\nTo resolve this issue, we built the momentum teacher model\n27,60\nand utilized its output for contrastive learning and NWP. The\nmomentum teacher performs a knowledge distillation by providing a\npseudo-label that reﬂects how the teacher model comprehends. Spe-\nciﬁcally, the label for the contrastive learning and NWP are mixed with\nthe momentum model’so u t p u ts\n∗,momentum(∗ ∈{s2p, p2s, s2s, p2p}) and\npNW P\nmomentum(sn | s0:n−1, P), with an adjusting hyperparameterα.T h e\ndetailed formulas for utilizing the momentum model for contrastive\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 10\nlearning and NWP are described in Eq. (8)∼(9)a n dE q .(10)∼(11).\ney* =1 /C0 αðÞ y* + αs*,momentum * 2 s2p,p2s,s2s,p2p\n/C8/C9/C0/C1\nð8Þ\neLcontrastive = 1\n2 H eys2p,ss2p\n/C16/C17\n+ H eyp2s,sp2s\n/C16/C17\n+ H eys2s,ss2s\n/C0/C1\n+ H eyp2p,sp2p\n/C16/C17/C16/C17\nð9Þ\neyNWP\nn =1 /C0 αðÞ yNWP\nn + αpNWP\nmomentum ðsnjs0:n/C0 1,PÞ ð10Þ\neLNWP =\nXn\ni =1\nH eyNWP\nn ,pNWP ðsnjs0:n/C0 1,PÞ\n/C16/C17\nð11Þ\nAfter the student model’sp a r a m e t e r swmodel are updated for each\nbatch, the parameters of the momentum teacher modelwmomentumare\nupdated by the exponential moving average (EMA) usingwmodel and an\nEMA hyperparameterλ according to Eq. (12).\nwmomentum =1 /C0 λðÞ wmodel + λwmomentum ð12Þ\nThe overall pre-training objective is the combined loss of Con-\ntrastive, NWP, NPP, and SPM loss.\nL = eLcontrastive + eLNWP + LNPP + LSPM ð13Þ\nWe note that when there’sn os p e c iﬁc attention mask (e.g., causal\nattention mask as in GPT61), the output of the transformer’ss e l f -\nattention and cross-attention mechanism is invariant to the position of\neach feature vector. This means the only pre-training objective that the\npredetermined order or the PV’s 53 properties matters is the NPP task,\nand SPMM behaves identically for the other pre-training objectives\nwhen we permute the order of the properties and their corresponded\nposition embedding. If we replace the NPP task to an order-invariant\nobjective (e.g., masked language modeling of BERT\n53), the pre-\ndetermined order of the elements in a PV would not affect the output\nof SPMM at all. Supplementary Table 3 shows the performance of\nSPMM when the NPP task was replaced to masked property modeling,\nand we found that this doesn’t improve the model performance.\nTraining for downstream tasks\nSupplementary Fig. 1 describes how we utilized our pre-trained model\nfor downstream tasks. For PV generation and SMILES generation\n(Supplementary Fig. 1a, b), we don’t need additionalﬁne-tuning since\ntheir training objectives are already included in the pre-training (NWP,\nNPP). For the inference procedure, the model generates PV or SMILES\nwith autoregressive sampling. Speciﬁcally, starting from the [CLS]\ntoken of the modality that we want to generate, the model predicts the\nﬁrst component and repeats taking the previous outputs to predict the\nnext component until it’s done or meets a sign to stop. For PV-to-\nSMILES generation, we used a beam search ofk =2t oh e l pt h em o d e l\ngenerate valid SMILES. The metrics of validity, uniqueness, and novelty\nthat we’ve used for PV-to-SMILES generation are deﬁned as follows:\nvalidity =\n#SMILES with valid syntax\n#generated SMILES ð14Þ\nuniqueness =#non duplicate valid SMILES\n#valid SMILES\nð15Þ\nnovelty =#unique SMILES not in the pretraining data\n#unique SMILES ð16Þ\nFor MoleculeNet downstream tasks and the DILI classiﬁcation task\nthat only provide SMILES data, we utilized only the SMILES encoder\npart of the model (Supplementary Fig. 1c). After the input molecule is\nencoded with the SMILES encoder, we pass the feature of the [CLS]\nS\ntoken through a classiﬁcation/regression head to get an output. The\nclassiﬁcation/regression head consists of MLP with one hidden layer.\nWe ﬁne-tuned our model with the given training set and get a check-\npoint with the lowest loss on the validation set, and recorded that\ncheckpoint’s performance on the test set.\nThe forward reaction prediction task provides a reactant SMILES\n(including multiple reagent molecules) and a product SMILES. We\nencode these two inputs with the SMILES encoder, then feed them into\nthe fusion encoder + prediction head. The model is trained to auto-\nregressively generate the original product SMILES (Supplementary\nFig. 1d). In the inference stage, starting from the [CLS]\nS token, the\nmodel predicts the next token until it generates the [SEP] token.\nSimilar to the SMILES generation, the self-attention of the fusion\nencoder and the reactant SMILES encoder uses a causal mask. The\nretro-reaction prediction task was done in the same way, but the role of\nthe reactant and product SMILES were swapped. Weﬁne-tuned SPMM\nfor the forward reaction prediction task with an approach of‘mixed\ntask’, meaning that the information about the major reactant is not\ngiven to the model. For both forward and retro-reaction tasks, we\nreplaced the input reactants and products with their random non-\ncanonical augmented SMILES\n62 with a probability of 0.5. This SMILES\naugmentation is reported63,64 to increase the accuracy of sequence-\nbased reaction prediction models, and we listed the ablation study\nresults about this in Supplementary Table 5.\nData preparation\nWe obtained 50,000,000 SMILES of general molecules from\nPubChem\n40 for pre-training. All 53 properties we used can be calcu-\nlated with SMILES using the RDKit Python library59.T h ed a t a s e tf o rt h e\nMoleculeNet downstream tasks is provided by the DeepChem65 Python\nlibrary. We split every dataset into train/valid/test sets in a ratio of 8:1:1\nusing a scaffold splitter from DeepChem, which is a more harsh con-\ndition for the model than random splitting. For the reaction prediction\ntask, we used the USPTO-480k dataset which contains 479,035 pairs of\nreactants and the major product of their reaction. The retro-reaction\nprediction task used the USPTO-50k dataset, containing 50,037\nproduct-reactant pairs with corresponding reaction types. Although\nthe USPTO-50k dataset provides tags of reaction type for each reaction\ndata, we didn’t use them, following the previous retro-reaction pre-\ndiction publications.\nImplementation details\nWe employed the architecture of 6 BERTbase encoder layers for our PV\nencoder and SMILES encoder, and 6 BERTbase encoder layers with\ncross-attention layers for our fusion encoder. With givenQ 2 Rlenq × dk ,\nK 2 Rlenk × dk ,a n dV 2 Rlenk × dv as query, key, and value inputs, the self-\nattention and cross-attention layers in BERT compute the output of the\nscaled-dot attention according to the following formula:\nAttention Q,K,VðÞ = Sof tmax\nQKT\nﬃﬃﬃﬃﬃﬃ\ndk\np\n !\nV ð17Þ\nWe pre-trained the model until it converges using a batch size of\n96 and the AdamW optimizer with a weight decay of 0.02. The learning\nrate is warmed up to 10\n−4 and decreased to 10−5 with a cosine scheduler.\nWe used the momentum-adjusting hyperparameterα of 0.4. Since the\npseudo-label from the momentum teacher is not useful in the early\nstages of the training, we linearly increasedα from 0 to 0.4 during the\nﬁrst epoch. The EMA hyperparameterλ was ﬁxed to 0.995, and the size\nof the PV and SMILES queuek was set to 24,576. The momentum\nmodels are not used for downstream tasks. The pre-training was done\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 11\nwith 8 Nvidia A100 GPUs for about 52,000 batch iterations, which took\nroughly 12 h. The full description of training for downstream tasks is in\nSupplementary Table 6.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe pre-training dataset is publicly available in PubChemhttps://\npubchem.ncbi.nlm.nih.gov/. ZINC15 test molecules for SMILES-to-PV\ngeneration are accessible through the ZINC15 website. Scaffold-split\nMoleculeNet datasets are available via DeepChem python module\nhttps://deepchem.io/, and raw databases can be found in the Mole-\nculeNet websitehttps://moleculenet.org/. The DILI training and test\ndata preparation can be found inhttps://pubmed.ncbi.nlm.nih.gov/\n29788510/. The USPTO-480k and USPTO-50k dataset is available at\nhttps://github.com/wengong-jin/nips17-rexgen and https://github.\ncom/MolecularAI/Chemformer/tree/main. Source data are provided\nwith this paper.\nCode availability\nThe source code for SPMM, a list of 53 properties for PV, experimental\ncodes, and datasets are available athttps://github.com/jinhojsk515/\nSPMM/(DOI: 10.5281/zenodo.10567599)\n66 to allow reproducing the\nresults.\nReferences\n1. Ryu, S. & Lee, S. Accurate, reliable and interpretable solubility\nprediction of druglike molecules with attention pooling and Baye-\nsian learning. arXiv preprint arXiv:2210.07145 (2022).\n2. Kuenneth, C. & Ramprasad, R. Polybert: A chemical language\nmodel to enable fully machine-driven ultrafast polymer informatics.\nNat. Commun.14,4 0 9 9( 2 0 2 3 ) .\n3. Moon, S., Zhung, W., Yang, S., Lim, J. & Kim, W. Y. Pignet: A physics-\ninformed deep learning model toward generalized drug– target\ninteraction predictions.Chem. Sci.13, 3661– 3673 (2022).\n4. Xu, C., Wang, Y. & Farimani, A. B. Transpolymer: a transformer-\nbased language model for polymer property predictions.npj\nComp. Mat.9, 64 (2023).\n5. Paul, D. et al. Artiﬁcial intelligence in drug discovery and develop-\nment. Drug Discov. Today26,8 0– 93 (2021).\n6. Chen, S. & Jung, Y. A generalized-template-based graph neural\nnetwork for accurate organic reactivity prediction.Nat. Mach.\nIntelligence4,7 7 2– 780 (2022).\n7 . S e g l e r ,M .H .S . ,K o g e j ,T . ,T y r c h a n ,C .&W a l l e r ,M .P .G e n e r a t i n g\nfocused molecule libraries for drugd i s c o v e r yw i t hrecurrent neural\nnetworks.ACS Central Sci.4,1 2 0– 131 (2017).\n8. Gómez-Bombarelli, R. et al. Automatic chemical design using a\ndata-driven continuous rep- resentation of molecules.ACS Central\nSci. 4,2 6 8– 276 (2018).\n9. Lim, J., Ryu, S., Kim, J. W. & Kim, W. Y. Molecular generative model\nbased on conditional variational autoencoder for de novo mole-\ncular design.J. Cheminform. 10,1 – 9( 2 0 1 8 ) .\n1 0 . L i m ,J . ,H w a n g ,S . - Y . ,M o o n ,S . ,K i m ,S .&K i m ,W .Y .S c a f f o l d - b a s e d\nmolecular design with a graph generative model.Chem. Sci.11,\n1153– 1164 (2020).\n11. Wang, W., Wang, Y., Zhao, H. & Sciabola, S. A transformer-based\ngenerative model for de novo molecular design.arXiv preprint\narXiv:2210.08749 (2022).\n1 2 . O l i v e c r o n a ,M . ,B l a s c h k e ,T . ,E n g k v i s t ,O .&C h e n ,H .M o l e c u l a rd e -\nnovo design through deep reinforcement learning.J. Cheminform.\n9,4 8( 2 0 1 7 ) .\n13. Zhou, Z., Kearnes, S., Li, L., Zare, R. N. & Riley, P. Optimization of\nmolecules via deep reinforcement learning.Sci. rep.9,\n10752 (2019).\n14. Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta: Large-\nscale self-supervised pre-training for molecular property predic-\ntion. InProceedings of Workshop on Neural Information Processing\nSystem (2020).\n15. Zhang, Z. et al. Can pre-trained models really learn better molecular\nrepresentations for ai- aided drug discovery? arXiv:2209.07423\n(2022).\n16. Melnyk, I. et al. Reprogramming large pretrained language models\nfor antibody sequence inﬁlling. InProceedings of the 2023 Inter-\nnational Conference on Machine Learning (ICML)(2023).\n17. Bommasani, R. et al. On the opportunities and risks of foundation\nmodels. arXiv preprint arXiv:2108.07258(2021).\n18. Horawalavithana, S. et al. Foundation models of scientiﬁc knowl-\nedge for chemistry: Oppor- tunities, challenges and lessons\nlearned. InProceedings of BigScience Episode #5– Workshop on\nChallenges & Perspectives in Creating Large Language Models,\n160– 172 (Association for Computational Linguistics, 2022).\n1 9 . L i u ,S . ,D e m i r e l ,M .F .&L i a n g ,Y .N - g r a mg r a p h :S i m p l eu n s u -\npervised representation for graphs, with applications to molecules.\nAdv. Neural. Inf. Process. Syst.32,( 2 0 1 9 ) .\n20. Rong, Y. et al. Self-supervised graph transformer on large-scale\nmolecular data.A d v .N e u r a l .I n f .P r o c e s s .S y s t .S y s t e m s33,\n12559– 12571 (2020).\n21. Ahmad, W., Simon, E., Chithrananda, S., Grand, G. & Ramsundar, B.\nChemberta-2: Towards chemical foundation models. arXiv preprint\narXiv:2209.01712 (2022).\n22. Chen, F. et al. Vlp: A survey on vision-language pre-training.Mach.\nIntelligence Res.20,3 8– 56 (2023).\n23. Vaswani, A. et al. Attention is all you need.A d v .N e u r a l .I n f .P r o c e s s .\nSyst. 30 (2017).\n24. Chen, Y.-C. et al. Uniter: Universal image-text representation\nlearning. InEuropean conference on computer vision,104– 120\n(Springer, 2020).\n25. Li, X. et al. Oscar: Object-semantics aligned pre-training for vision-\nlanguage tasks. InComputer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23– 28, 2020, Proceedings, Part\nXXX 16, 121– 137 (Springer, 2020).\n26. Radford, A. et al. Learning transferable visual models from natural\nlanguage supervision. InInternational conference on machine\nlearning,8 7 4 8– 8763 (PMLR, 2021).\n27. Li, J. et al. Align before fuse: Vision and language representation\nlearning with momentum distillation.Adv. Neural. Inf. Process. Syst.\n34, 9694– 9705 (2021).\n28. Yu, J. et al. Coca: Contrastive captioners are image-text foundation\nmodels. Trans. Mach. Learn. Res. 2022 (2022).\n29. Agrawal, A. et al. Vqa: Visual question answering. InProceedings of\nthe IEEE international conference on computer vision,\n2425– 2433 (2015).\n30. Kaur, P., Pannu, H. S. & Malhi, A. K. Comparative analysis on cross-\nmodal information retrieval: A review.Comput. Sci. Rev.39,\n100336 (2021).\n31. Zhang, C., Zhang, C., Zhang, M. & Kweon, I. S. Text-to-image dif-\nfusion models in generative ai: A survey.arXiv:2303.07909\n[cs] (2023).\n32. Ming, Y. et al. Visuals to text: a comprehensive review on automatic\nimage captioning.IEEE/CAA J. Autom. Sinica9, 1339– 1365 (2022).\n33. Winter, R., Montanari, F., Noé, F. & Clevert, D.-A. Learning con-\ntinuous and data-driven molecular descriptors by translating\nequivalent chemical representations.Chem. Sci.10,\n1692– 1701 (2019).\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 12\n34. Grill, J.-B. et al. Bootstrap your own latent: A new approach to self-\nsupervised learning.Adv. Neural. Inf. Process. Syst.33,\n21271– 21284 (2020).\n35. Zhu, J. et al. Dual-view molecule pre-training. InProceedings of the\n29th ACM SIGKDD Conference on Knowledge Discovery and Data\nMining,3 6 1 5– 3627 (2023).\n36. Meyers, J., Fabian, B. & Brown, N. De novo molecular design and\ngenerative models.Drug Discov. Today26,2 7 0 7– 2715 (2021).\n37. Bort, W. et al. Inverse qsar: reversing descriptor-driven prediction\npipeline using attention- based conditional variational auto-\nencoder.J. Chem. Inform. Model.62,5 4 7 1– 5484 (2022).\n38. Crawshaw, M. Multi-task learning with deep neural networks: A\nsurvey. arXiv preprint arXiv:2009.09796 (2020).\n39. Huh, J., Park, S., Lee, J. E. & Ye, J. C. Improving medical speech-to-\ntext accuracy with vision-languagepre-training model. arXiv pre-\nprint arXiv:2303.00091 (2023).\n40. Kim, S. et al. Pubchem in 2021: new data content and improved web\ninterfaces.Nucleic Acids Res.49,D 1 3 8 8– D1395 (2021).\n41. Li, Y., Zhang, L. & Liu, Z. Multi-objective de novo drug design with\nconditional graph generative model.J. Cheminform.10,3 3( 2 0 1 8 ) .\n42. Simonovsky, M. & Komodakis, N. Graphvae: Towards generation of\nsmall graphs using variational autoencoders. InArtiﬁcial Neural\nNetworks and Machine Learning–ICANN 2018: 27th International\nConference on Artiﬁcial Neural Networks,R h o d e s ,G r e e c e ,O c t o b e r\n4-7, 2018, Proceedings, Part I 27, 412– 422 (Springer, 2018).\n43. De Cao, N. & Kipf, T. Molgan: An implicit generative model for small\nmolecular graphs.arXiv:1805.11973 [cs, stat](2022).\n44. Sterling, T. & Irwin, J. J. Zinc 15– ligand discovery for everyone.J.\nChem. Inform. Model.55,2 3 2 4– 2337 (2015).\n45. Wu, Z. et al. Moleculenet: a benchmark for molecular machine\nlearning.Chem. Sci.9,5 1 3– 530 (2018).\n46. Gogishvili, D., Nittinger, E.,Margreitter, C. & Tyrchan, C. Non-\nadditivity in public and in- house data: implications for drug design.\nJ. Cheminform.13, 47 (2021).\n47. Kwapien, K. et al. Implications of additivity and nonadditivity for\nmachine learning and deep learning models in drug design.ACS\nOmega 7,2 6 5 7 3– 26581 (2022).\n48. Ai, H. et al. Predicting drug-induced liver injury using ensemble\nlearning methods and molecularﬁ\nngerprints.Toxicol. Sci.165,\n100– 107 (2018).\n49. Malach, E. Auto-regressive next-token predictors are universal\nlearners. arXiv preprint arXiv:2309.06979 (2023).\n50. Naseer, M. et al. Intriguing properties of vision transformers.Adv.\nNeural. Inf. Process. Syst.34,2 3 2 9 6– 23308 (2021).\n5 1 . P a r k ,S . ,L e e ,I . - J . ,K i m ,J .W .&Y e ,J .C .S i n g l e - r o u n ds e l f - s u p e r v i s e d\ndistributed learning using vision transformer.arXiv preprint\narXiv:2301.02064(2023).\n52. Lee, K., Jang, J., Seo, S., Lim, J. & Kim, W. Y. Drug-likeness scoring\nbased on unsupervised learning.Chem. Sci.13, 554– 565 (2022).\n53. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTech- nologies, Volume 1 (Long and Short Papers),4 1 7 1– 4186 (2019).\n54. Caron, M. et al. Emerging properties in self-supervised vision\ntransformers. InProceedings of the IEEE/CVF international con-\nference on computer vision,9 6 5 0– 9660 (2021).\n55. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers\nfor image recognition at scale.ICLR (2021).\n56. Gage, P. A new algorithm for data compression.C Users Journal12,\n23– 38 (1994).\n57. Liu, Y. et al. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 (2019).\n58. Raffel, C. et al. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer.J .M a c h .L e a r n .R e s .21,5 4 8 5– 5551 (2020).\n59. Landrum, G. Rdkit: Open-source cheminformatics software.\nhttps://www.rdkit.org(2016).\n60. He, K., Fan, H., Wu, Y., Xie, S. & Girshick, R. Momentum contrast for\nunsupervised visual representation learning. InProceedings of the\nIEEE/CVF conference on computervision and pattern recognition,\n9729– 9738 (2020).\n61. Brown, T. B. et al. Language models are few-shot learners.Adv.\nNeural. Inf. Process. Syst.33,1 8 7 7– 1901 (2020).\n62. Bjerrum, E. Smiles enumeration as data augmentation for neural\nnetwork modeling of molecules. arXiv preprint\narXiv:1703.07076 (2017).\n6 3 . T e t k o ,I .V . ,K a r p o v ,P . ,V a nD e u r s e n ,R .&G o d i n ,G .S t a t e - o f - t h e - a r t\naugmented nlp transformer models for direct and single-step ret-\nrosynthesis.Nat. Commun.11,5 5 7 5( 2 0 2 0 ) .\n64. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: a pre-\ntrained transformer for computational chemistry.Mach. Learn. Sci.\nTechnol.3, 015022 (2022).\n65. Ramsundar, B. et al.\nDeep Learning for the Life Sciences(O’Reilly\nMedia, 2019).\n66. Chang, J. & Ye, J. C. Bidirectional generation of structure and\nproperties through a single molecular foundation model.https://\ngithub.com/jinhojsk515/spmm(2024).\n67. Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S. & Hopkins, A.\nL. Quantifying the chemical beauty of drugs.Nat. Chem.4,\n90– 98 (2012).\n68. Yang, K. et al. Analyzing learned molecular representations for\nproperty prediction.J. Chem. Inform. Model.59,3 3 7 0– 3388 (2019).\n69. Hu, W. et al. Strategies for pre-training graph neural networks. In\nInternational Conference on Learning Representations(2020).\n70. Fang, X. et al. Geometry-enhanced molecular representation\nlearning for property prediction.Nat. Mach. Intell.4,1 2 7– 134 (2022).\n71. Ross, J. et al. Large-scale chemical language representations cap-\nture molecular structure and properties.Nat. Mach. Intell.4,\n1256– 1264 (2022).\n72. Schwaller, P. et al. Molecular transformer: a model for uncertainty-\ncalibrated chemical reac- tion prediction.ACS Central Sci.5,\n1572– 1583 (2019).\n73. Tu, Z. & Coley, C. W. Permutation invariant graph-to-sequence\nmodel for template-free ret- rosynthesis and reaction prediction.J.\nChem. Inform. Model.62,3 5 0 3– 3513 (2022).\n74. Sacha, M. et al. Molecule edit graph attention network: Modeling\nchemical reactions as sequences of graph edits.J. Chem. Inform.\nModel. 61, 3273– 3284 (2021).\n75. Zheng, S., Rao, J., Zhang, Z., Xu, J. & Yang, Y. Predicting retro-\nsynthetic reactions using self-corrected transformer neural net-\nworks. J. Chem. Inf. Model.60,4 7– 55 (2020).\n76. Kim, E., Lee, D.-S., Kwon, Y., Park, M.-S. & Choi, Y.-S. Valid, plausible,\nand diverse ret- rosynthesis using tied two-way transformers with\nlatent variables.J. Chem. Inf. Model.61,1 2 3– 133 (2021).\nAcknowledgements\nThis work was supported by the Institute of Information & communica-\ntions Technology Planning & Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (No.2019-0-00075, Artiﬁcial Intelligence Graduate\nSchool Program (KAIST)), National Research Foundation (NRF) of Korea\ngrant NRF-2020R1A2B5B03001980and RS-2023-00262527, and by the\nKAIST Key Research Institute (Interdisciplinary Research Group) Project.\nAuthor contributions\nJ.C. prepared the code, performed all experiments and analyses, col-\nlected data, and wrote the manuscript. J.C.Y. supervised the project in\nconception and discussion, and prepared the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 13\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-46440-3.\nCorrespondenceand requests for materials should be addressed to\nJong Chul Ye.\nPeer review informationNature Communicationsthanks Junzhou\nHuang, and the other, anonymous, reviewer(s) for their contribution to\nthe peer review of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-46440-3\nNature Communications|         (2024) 15:2323 14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7001268863677979
    },
    {
      "name": "Chemical space",
      "score": 0.6505659222602844
    },
    {
      "name": "Embedding",
      "score": 0.5757007598876953
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5562557578086853
    },
    {
      "name": "Machine learning",
      "score": 0.4591609239578247
    },
    {
      "name": "Property (philosophy)",
      "score": 0.4571073651313782
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.4457700848579407
    },
    {
      "name": "Pipeline (software)",
      "score": 0.4308534860610962
    },
    {
      "name": "Drug discovery",
      "score": 0.12881392240524292
    },
    {
      "name": "Bioinformatics",
      "score": 0.12203264236450195
    },
    {
      "name": "Biology",
      "score": 0.07946854829788208
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ]
}