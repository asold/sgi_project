{
  "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization",
  "url": "https://openalex.org/W3174418826",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222427123",
      "name": "Tay, Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222578401",
      "name": "Tran, Vinh Q.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202039502",
      "name": "Ruder, Sebastian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222427124",
      "name": "Gupta, Jai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747279576",
      "name": "Chung, Hyung Won",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222578404",
      "name": "Bahri, Dara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1902709959",
      "name": "Qin Zhen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4313577911",
      "name": "Baumgartner, Simon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1918232694",
      "name": "Yu Cong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3177605537",
      "name": "Metzler, Donald",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3166790124",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3011279327",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W3126822054",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W2920807444",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2900096133",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3162276117",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3176285434",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1992427404",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W3125507956",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2952125979",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963022149",
    "https://openalex.org/W2540646130",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3034789084",
    "https://openalex.org/W3006439205",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3164045210",
    "https://openalex.org/W2963077280",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3214173179",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2962906592",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W2151035046",
    "https://openalex.org/W3035207248",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W2113459411"
  ],
  "abstract": "State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce Charformer, a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that Charformer outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, Charformer is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28%-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.",
  "full_text": "Published as a conference paper at ICLR 2022\nCHARFORMER : F AST CHARACTER TRANSFORMERS\nVIA GRADIENT -BASED SUBWORD TOKENIZATION\nYi Tay∗, Vinh Q. Tran∗, Sebastian Ruder†, Jai Gupta, Hyung Won Chung, Dara Bahri\nZhen Qin, Simon Baumgartner, Cong Yu, Donald Metzler\nGoogle Research and DeepMind†\nyitay@google.com, vqtran@google.com\nABSTRACT\nState-of-the-art models in natural language processing rely on separate rigid sub-\nword tokenization algorithms, which limit their generalization ability and adap-\ntation to new settings. In this paper, we propose a new model inductive bias that\nlearns a subword tokenization end-to-end as part of the model. To this end, we\nintroduce a soft gradient-based subword tokenization module (GBST) that automat-\nically learns latent subword representations from characters in a data-driven fashion.\nConcretely, GBST enumerates candidate subword blocks and learns to score them\nin a position-wise fashion using a block scoring network. We additionally introduce\nCHARFORMER , a deep Transformer model that integrates GBST and operates on\nthe byte level. Via extensive experiments on English GLUE, multilingual, and noisy\ntext datasets, we show that CHARFORMER outperforms a series of competitive\nbyte-level baselines while generally performing on par and sometimes outperform-\ning subword-based models. Additionally, CHARFORMER is fast, improving the\nspeed of both vanilla byte-level and subword-level Transformers by 28-100% while\nmaintaining competitive quality. We believe this work paves the way for highly\nperformant token-free models that are trained completely end-to-end.\n1 I NTRODUCTION\nNeural networks have achieved tremendous success in natural language processing (NLP) by replacing\nfeature-engineered models with stacks of functions that are learned end-to-end from vast amounts\nof data (Mikolov et al., 2013; Peters et al., 2018; Howard and Ruder, 2018). The single component\nof the traditional NLP pipeline (Manning and Schütze, 1999) that has so far resisted gradient-based\nlearning is tokenization, which is commonly applied as a pre-processing step. State-of-the-art\npre-trained language models (Devlin et al., 2019) generally rely on data-driven subword-based\ntokenization algorithms (Schuster and Nakajima, 2012; Sennrich et al., 2016; Wu et al., 2016; Kudo\nand Richardson, 2018) while expert-crafted segmentation algorithms are still common for languages\nwithout whitespace separation such as Chinese, Thai, and Korean (cf. Lample and Conneau, 2019).\nThis reliance on rigid tokenization methods introduces a bottleneck into current NLP systems that\nlimits their capabilities. Subword segmentation algorithms split tokens into subwords solely based on\nfrequency, without taking into account lexical or semantic similarity. As a result, models are brittle to\nrare words (Gong et al., 2018) and perturbations, both natural and adversarial (Belinkov and Bisk,\n2018; Pruthi et al., 2019; Sun et al., 2020). In multilingual models, tokens in low-resource languages\nare split into many subwords, which impacts performance on those languages and deteriorates cross-\nlingual transfer (Hu et al., 2020; Wang et al., 2021). Finally, a separate tokenization algorithm leads to\na mismatch between the pre-training and downstream distribution of words when adapting pre-trained\nlanguage models to new settings, which requires signiﬁcant engineering effort to overcome.\nThe direct application of character-level modelling into pre-trained language models in turn results in\nseverely increased computational and memory complexity due to an increased sequence length and\ngenerally lower performance.\n∗Equal Contribution\n1\narXiv:2106.12672v3  [cs.CL]  23 Feb 2022\nPublished as a conference paper at ICLR 2022\nTo address this problem, we propose gradient-based\nsubword tokenization (GBST), a new method that\ncombines the compositionality of character-level\nrepresentations with the efﬁciency of subword\ntokenization while enabling end-to-end learning.\nOur method learns latent subword representations\nfrom characters using large amounts of unlabeled\ndata. Speciﬁcally, GBST learns a position-wise\nsoft selection over candidate subword blocks by\nscoring them with a scoring network. In contrast to\nprior tokenization-free methods (Clark et al., 2021),\nGBST learns interpretable latent subwords, which\nenables easy inspection of lexical representations\nand is more efﬁcient than other byte-based models\n(Xue et al., 2021). Given that simply applying a\nstandard Transformer on a sequence of characters\nand bytes is computationally prohibitive, GBST\npaves the way for usable, practical and highly\nperformant character-level models. A high level\noverview of how the GBST module is applied can\nbe found at Figure 1.\nWe furthermore introduce CHARFORMER , a Trans-\nformer encoder-decoder model that uses GBST to\noperate directly on the byte level. In addition, we ex-\nperiment with a re-scaled variant of CHARFORMER ,\nwhich allocates additional capacity to the encoder\nto make up for the lack of discrete subword embed-\ndings.\nFigure 1: High-level differences between tra-\nditional subword Transformer models and\nCharformer which uses gradient-based sub-\nword tokenization.\nWe evaluate our model on a range of standard and non-standard English, and multilingual downstream\ntasks. On English GLUE and long document classiﬁcation tasks, CHARFORMER outperforms strong\nbyte-level baselines and overall achieves performance on par with subword-based models such as\nBERT (Devlin et al., 2019) and T5 (Raffel et al., 2020). On toxicity detection in social media datasets\n(Borkan et al., 2019; Wulczyn et al., 2017), CHARFORMER outperforms byte-level baselines as\nwell as subword-based models, demonstrating robustness to spelling variation and non-standard\nlanguage. Finally, a multilingually pre-trained CHARFORMER performs on par or outperforms strong\nsubword-based multilingual baselines on standard cross-lingual datasets.\nWe additionally demonstrate CHARFORMER is more efﬁcient compared to byte-level and subword-\nbased models with similar numbers of parameters. On a comparable setup, CHARFORMER out-\nperforms a baseline similar to the recent state-of-the-art byte-level model ByT5 (Xue et al., 2021)\nwhile being 2×more memory efﬁcient and 10–93% faster. CHARFORMER also trains 28% faster than\nthe subword-level mT5 model (Xue et al., 2020), has 3×fewer parameters and achieves comparable\nquality on well-established benchmarks. Finally, we demonstrate via visualization that the latent\nsubwords learned by CHARFORMER are interpretable to some extent.\n2 C HARFORMER\nThis section introduces our efﬁcient character-level architecture, CHARFORMER . CHARFORMER\nis comprised of a Gradient-Based Subword Tokenization (GBST) module, followed by deep Trans-\nformer layers. The input to the GBST module is a sequence of characters or bytes1, which is then\ndownsampled to construct latent subwords.\n1We choose bytes rather than characters (Unicode code points) as this allows us to use a vocabulary of\n256 possible byte values for all settings. We note that for languages with a Latin alphabet, many characters\ncorrespond to a single byte. For other languages, each character corresponds to 2–3 bytes in general. For\nsimplicity and to align with prior work, we will generally talk about characters unless stated otherwise.\n2\nPublished as a conference paper at ICLR 2022\n2.1 G RADIENT -BASED SUBWORD TOKENIZATION (GBST)\nThe input to GBST is a tensor of shape X ∈RL×d where Lis the number of input characters and dis\nthe character embedding dimension. The key idea behind GBST is for the model to learn to perform\na latent subword segmentation of the input by selecting the most suitable subword block at every\ncharacter position. A block is a contiguous span of characters Xi:i+b of length bfor 1 ≤i≤L−b.\n2.1.1 C ONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS\nWe ﬁrst enumerate all possible subword blocks of size b up to a maximum block size M. In\norder to learn subword block embeddings, we use a non-parameterized strided pooling function\nF : Rb×d →Rd that projects a subword block consisting of a sequence of character embeddings\nXi:i+b ∈Rb×d to a single subword block representation Xb,i ∈Rd for block size bat position i. We\ncompute subword blocks Xb,i with a stride s:\nXb = [F(Xi:i+b); F(X(i+s):(i+s)+b); ... ] (1)\nIn practice we set s = b, thus Xb ∈R\nL\nb ×d. The construction of latent subword blocks creates a\nshorter overall sequence length by downsampling. We construct Xb for b∈1,...,M , which can be\nseen in Figure 2 for M = 4.\nConsidering Offsets A limitation of a strided implementation is that it is unable to model all\npossible subword windows. For instance, for the character sequence [a,b,c,d ] we would only be able\nto allocate [a,b] and [c,d] as subword blocks of length b= 2and would ignore the subword block\n[b,c]. Offsets can be used to model sliding windows of all possible subword blocks. We consider\nenumerating all possible strided blocks by additionally shifting sequences up until the offsets. As this\nincreases computation, we instead propose to ﬁrst apply a 1D convolution to X, prior to enumerating\nsubword blocks. This effectively “smoothes” over the subword blocks. We use the variant with 1D\nconvolutions in our main experiments and provide additional ablations in §4.4.\nConsidering Intra-block Positions It is important to preserve the ordering of the characters within\nthe block Xi,Xi+1,...,X i+b. E.g., the output of F should differ for the blocks abcand bca. For\ncertain choices of F it may be valuable to add a positional embedding (Vaswani et al., 2017) to\nXi:i+b before applying F. Note that this positional embedding would only be for individual blocks,\nand is not global to the entire input sequence. That is, only positional embedding values for positions\n1,...,b would be used. However, in practice we apply a 1D convolution before the GBST layer and\nuse the mean-pooling function for F. We ﬁnd this to be sufﬁcient to distinguish between same sized\nblocks with different character orders.\n2.1.2 B LOCK SCORING NETWORK\nIn order to allow the model to learn which block to select for every character position, we introduce\na block scoring network. The block scoring network is simply a parameterized function FR(.) that\nproduces a score for each candidate block. Given a subword candidate block Xb,i ∈Rd, we compute\na score pb,i associated with the block using a simple linear transformation FR : Rd →R:\npb,i = FR(Xb,i) (2)\nWe perform ranking of subword blocks with regard to each character position in the original sequence.\nAt every position i, the model learns to select the most suitable subword block Xb,i among all\nblock sizes 1 ≤b≤M. As each sequence of subword blocks Xb is downsampled, we realign the\nrepresentations of the subword blocks by upsampling each Xb to its original sequence length L.\nSpeciﬁcally, for a block size of b, we replicate each block representation Xb,i btimes. We then score\neach candidate block at each position iusing the softmax function:\nPi = softmax([p1,i,p1,i,··· ,pM,i]), (3)\nwhich computes a relative score of each candidate block at each position and Pi ∈RM . We show the\nscoring of realigned blocks in Figure 2.\n3\nPublished as a conference paper at ICLR 2022\n(a) Formation of subword blocks to be scored by FR.\nOffsets and/or pre-GBST convolutions not shown.\n(b) Block scores that have been expanded\nback to length L. Softmax is taken over block\nscores at each position ito form block weights for\nconstructing latent subword representations.\nFigure 2: Illustration of subword block formation and scoring.\n2.1.3 F ORMING LATENT SUBWORDS\nWe then sum the representations of all subword blocks Xb,i at each position imultiplied by their\nlearned probability Pb,i to form a latent subword representation ˆXi ∈Rd:\nˆXi =\nM∑\nb\nPb,iXb,i (4)\nIntuitively, the model learns an ideal subword block for each position. In contrast to standard\ndeterministic subword tokenization algorithms, this selection is soft and can thus consider different\npossible segmentations at every position i. In general, however, this formulation still assumes that\nsubwords are contiguous sequences of characters. While additional context can be considered via the\nconvolutions in §2.1.1, non-concatenative morphology where morphemes are discontinuous may be\nharder for the method to model.2\n2.1.4 P OSITION -WISE SCORE CALIBRATION\nIn the above approach, the scoring of each position is independent of other positions. We hypothesize\nthat it may be beneﬁcial for block scores at each position to be aware of each other. To this end, we\nintroduce an optional module that enables learning a consensus among block scores by calculating\ndot products across the scores Pi across all positions i ∈[1,L]. This can be viewed as a form of\nself-attention across block scores, albeit without any projections for computational efﬁciency. To\nlearn the new scores ˆP ∈RL×M , we compute ˆP = softmax(PP⊤)P.\n2.1.5 D OWNSAMPLING\nAfter learning a candidate block or mixture of blocks for each position, we use a downsampling func-\ntion FD : RL×d →R\nL\nds ×d that downsamples the sequence of latent subwords ˆX = [ˆX1,..., ˆXL] to\n˜X, reducing its sequence length by a factor of ds. We choose FD to be a non-parameterized mean\npooling operation. Notably, such simple stride-based pooling removes potential redundancies caused\nby adjacent positions selecting similar blocks as the mean pool of two identical block embeddings\nproduces the same outcome. Intuitively, as the downsampling operation is ﬁxed, the parameterized\ncomponents preceding it should learn an optimal subword tokenization given the downsampling.\n2.2 T RANSFORMER STACK\nThe remainder of the CHARFORMER model remains identical to a regular Transformer encoder-\ndecoder model. The Transformer stack operates on the downsampled latent subwords ˜X instead of\nsubword embeddings.\nRe-scaling of the Transformer Stack While subword-based models allocate much of their capacity\nto subword embeddings—up to 71% of all parameters for contemporary multilingual models (Chung\n2Future work could explicitly seek to model discontinuous morphological processes by considering skip-\ngrams in addition to character n-grams, although this would increase computational costs.\n4\nPublished as a conference paper at ICLR 2022\net al., 2021)—, the character vocabulary of character-level models is much smaller and thus less\nexpressive. Similar to Xue et al. (2021), we hypothesize that character-level models require deeper\nencoder stacks than subword-based models to make up for their smaller embedding capacity. Conse-\nquently, we explore a scaling variant ofCHARFORMER that puts more parameters at the encoder at the\nexpense of the decoder while preferring a deep narrow model over a larger wide model. Speciﬁcally,\nwe re-conﬁgure the Base model size to be similar to the T5 Small model size, with an expanded\n24 layers in the encoder. The resulting CHARFORMER SBase (Scaled Base) has 134M parameters,\nwhich is about 67% the parameter footprint of the standard base T5 model (200M parameters; Raffel\net al., 2020). Moreover, this particular CHARFORMER model is approximately 50-100% faster than\nthe T5 base model (see §4.1).3 For the re-scaled variant, we also used the GLU variant described in\n(Shazeer, 2020) which is commonly referred to as the V1.1 variant in the T5 library.\nA Note on Comparing Character-level and Subword-based Methods Prior work on efﬁcient\nmethods generally compares models with the same number of parameters (Chung et al., 2021).\nHowever, whereas embedding look-up even with large vocabularies in subword-based methods is\nO(1), re-distributing the subword embedding parameters in character-level models such as ByT5\n(Xue et al., 2021) to dense layers incurs much higher computational costs—a 25% penalty in training\nspeed. We believe that a fair re-scaling of character-level models should not only aim to match the\nnumber of parameters but also the compute and inference costs of subword-based models under the\nassumption that char/byte-level models will require longer sequences (see §4.1 for a comparison).\nSpan-based Pre-training Our pre-training scheme follows T5 quite closely. We maskNcontiguous\ncharacters and train to predict them in a sequence-to-sequence architecture following Xue et al. (2021).\nThe model optimizes the cross-entropy loss and is trained with teacher forcing.\n3 E XPERIMENTS\nWe evaluate our method both in English as well as in a multilingual setting on relevant benchmarks\nand compare against state-of-the-art character-level and subword-based methods.\n3.1 E XPERIMENTS ON MONOLINGUAL ENGLISH DATASETS\nData To showcase the effectiveness of the proposed method, we evaluate on a diverse set of standard\nEnglish tasks from GLUE covering sentiment classiﬁcation (SST-2; Socher et al., 2013), natural\nlanguage inference (MNLI, QNLI; Williams et al., 2018; Rajpurkar et al., 2016), paraphrase detection\n(Dolan and Brockett, 2005, MRPC, QQP) and sentence similarity (Cer et al., 2017). In addition, we\nevaluate on tasks that require dealing with long documents, both for sentiment analysis (IMDb; Maas\net al., 2011) and news classiﬁcation (AGNews; Zhang et al., 2015).\nBaselines We compare CHARFORMER against the following state-of-the-art subword-based models:\nBERT (Devlin et al., 2019), an encoder-only pre-trained masked language model; and T5 (Raffel\net al., 2020), an encoder-decoder model. We also compare against Byte-level T5 (Xue et al., 2021), a\nT5 model that is directly applied to bytes. We additionally evaluate the impact of the downsampling\nin CHARFORMER by comparing it to the downsampling used by the character-level CANINE (Clark\net al., 2021) model in our framework. CANINE downsamples a character sequence using local\nattention and pooling via strided convolutions. As the original CANINE uses an encoder-only model\nand was only trained on multilingual data, we integrate CANINE-style downsampling into Byte-level\nT5, which we refer to as Byte-level T5+LASC (local attention–strided convolution).4 As an ablation\nfor the GBST inductive bias, we compare against Byte-level T5+ConvBase a convolutional baseline\nof Byte-level T5 with a 1D convolution of ﬁlter size 5 placed before the encoder. Note that in all the\nbaselines and for CHARFORMER base models, in the spirit of fair comparison, we compare them at\nan equal parameterization (size). Our scaling experiments are reserved for our SBase models, which\nis intended to only be compared with subword T5 models, and not to unscaled byte-level baselines.\nFinally, we include an SBase scaled version of Byte-level T5 for comparison.\n3The beneﬁts of such re-scaling have also been observed for subword-based encoder-decoder neural machine\ntranslation models (Devlin, 2017; Kasai et al., 2021).\n4Compared to CANINE, Byte-level T5+LASC does not operate on Unicode codepoints and has a decoder. It\nthus forgoes character hash embeddings and upsampling procedures respectively.\n5\nPublished as a conference paper at ICLR 2022\nTable 1: Comparison of CHARFORMER against other subword and character-level models with\ndifferent parameter sizes on diverse standard English datasets.\nModel |θ| SST-2 MNLI QNLI MRPC QQP STSB COLA A VG\nBERTBase,Subword 110M 92.7 84.4/- 88.4 86.7/- - - - -\nT5Base,Subword 220M 92.7 84.2/84.6 90.5 88.9/92.1 91.6/88.7 88.0 53.8 84.3\nByte-level T5Base 200M 91.6 82.5/82.7 88.7 87.3 /91.0 90.9/87.7 84.3 45.1 81.5\nByte-level T5+ConvBase 205M 89.8 81.1/82.5 89.2 83.6/89.2 90.7/87.7 85.0 47.1 81.2\nByte-level T5+LASCBase 205M 90.0 80.0/80.8 87.1 82.8/88.1 89.0/85.4 83.7 25.3 77.0\nCHARFORMERBase 203M 91.6 82.6/82.7 89.0 87.3 /91.1 91.2/88.1 85.3 42.6 81.4\nByte-level T5SBase 133M 91.2 83.9 /83.7 90.9 85.5/89.2 91.1/88.1 85.7 49.3 82.6\nCHARFORMERSBase 134M 91.5 83.7/84.4 91.0 87.5/91.4 91.4/88.5 87.3 51.8 83.6\nSetup We evaluate Base and SBase conﬁgurations of CHARFORMER with 203M and 134M\nparameters respectively. We compare to Base conﬁgurations of BERT and T5 that have a similar\nnumber of parameters. We pre-train all models on the C4 corpus for 1M steps using a batch size\nof 64 and sequence length of 1024. All non-subword models use a vocabulary of 256 bytes.5 Our\npre-training scheme corrupts spans with a mean length of 20 bytes. Each model is pre-trained on 16\nTPU V3 chips. We pre-train our models with the Adafactor optimizer with an inverse square root\nlearning rate. We then ﬁne-tune on each individual task separately using a constant learning rate of\n10−3. More details can be found in the Appendix.\nTable 2: Results on comment classiﬁcation on Civil Com-\nments and Wiki Comments. Metrics are accuracy and\nAUC-PR. T5 baseline results are from (Tay et al., 2021).\nModel Civil Comments Wiki Comments\nT5Base,Subword 81.2 / - 91.5 / -\nByte-level T5Base 82.8 / 78.7 93.2 / 75.4\nByte-level T5+LASCBase 82.9 / 78.2 93.0 / 75.0\nCHARFORMERBase 83.0/ 78.8 92.7 / 79.7\nCHARFORMERSBase 83.0 / 78.9 93.5 / 75.5\nTable 3: Results on text classiﬁcation on\nlong documents.\nModel IMDb News\nT5Base,Subword 94.2 93.5\nByte-level T5Base 91.5 93.6\nByte-level T5+LASCBase 91.1 93.5\nCHARFORMERBase 91.5 94.0\nCHARFORMERSBase 94.4 94.1\nResults For all result tables, we divide the table into three sections: subword baseline(s), un-scaled\nbyte-level baselines, and scaled CHARFORMER results. If a section and task combination has more\nthan one model result, we underline the best result. We show result for GLUE in Table 1. CHAR -\nFORMER outperforms other character-level baselines trained under the same conditions with the same\nnumber of parameters across all tasks, while being considerably faster and requiring less compute\nthan T5-style models that are directly applied to bytes or characters (see §4.1). CHARFORMER SBase\nperforms even better despite having a smaller number of parameters compared to the Base conﬁgu-\nration, demonstrating the usefulness of rescaling the transformer stack for character-level models.\nCHARFORMER SBase furthermore is the only model that performs on par or even outperforms the\nstandard subword-based models on some tasks in standard English. In Table 3 we provide results\nfor text classiﬁcation of long documents. Here, CHARFORMER SBase is the only byte-level model\nto outperform T5Base,Subword on the IMDb classiﬁcation task, and both CHARFORMER models\noutperform byte and subword level baselines on AGNews.\n3.2 E XPERIMENTS ON NON -STANDARD ENGLISH DATASETS\nThe previous set of experiments demonstrated the ability of CHARFORMER to perform well on clean\ndatasets consisting of standard English. However, character-level models are particularly suited to\ndata that is noisy, containing spelling variations, typos, and other non-standard language.\nData To demonstrate CHARFORMER ’s ability to perform well on such data, we evaluate on toxicity\ndetection using the Civil Comments (Borkan et al., 2019) and the Wikipedia Comments (Wulczyn\n5Following Xue et al. (2021) we discard illegal UTF-8 sequences and reuse the ﬁnal 100 byte IDs as sentinel\ntokens.\n6\nPublished as a conference paper at ICLR 2022\nTable 4: Multilingual comparison of CHARFORMER against subword and byte-level models on\nin-language multi-task, translate-train multi-task, and cross-lingual zero-shot (training on English)\nsettings. Model sizes are the same as those in Table 1. mBERT and mT5 baseline results are from\n(Xue et al., 2020).\nIn-Language Translate-Train-All Zero-Shot\nModel |θ| TyDiQA-GoldP XQuAD MLQA XNLI PAWS-X XNLI PAWS-X\nmBERTBase(Subword) 179M 77.6/68.0 -/- -/- - - 65.4 81.9mT5Base(Subword) 582M 80.8 /70.0 75.3/59.7 67.6/48.5 75.9 89.3 75.4 86.4\nByte-level T5Base 200M 75.6/65.4 68.6/54.3 61.8/44.4 69.4 87.1 57.4 80.9Byte-level T5+LASCBase 205M 70.6/59.7 66.8/52.1 58.8/41.1 67.9 84.8 55.2 79.0CHARFORMERBase 203M 75.9 /65.6 70.2/55.9 62.6/44.9 71.1 87.2 57.6 81.6\nCHARFORMERSBase 134M 79.1/68.8 73.6/59.0 66.3/48.5 72.2 88.2 66.6 85.2CHARFORMERSBase,LongPT134M 81.2 /71.3 74.2/59.8 67.2/49.4 72.8 88.6 67.8 83.7\net al., 2017) datasets. Both are standard benchmarks that require estimating the toxicity of user-\ngenerated content. We use the same setup as for the standard English datasets.\nResults We show results in Table 2. Character-level models outperform the subword-based T5\nmodel on both datasets, demonstrating their suitability to deal with such noisy, user-generated data.\nCHARFORMER achieves performs on par or outperforms other character-level methods on both\ndatasets across the different model sizes.\n3.3 M ULTILINGUAL EXPERIMENTS\nData To evaluate the effectiveness of character-level models on multilingual data, we evaluate on\nstandard cross-lingual question answering and classiﬁcation tasks. In particular, we evaluate on the\nquestion answering tasks TyDiQA-GoldP (Clark et al., 2020), XQuAD (Artetxe et al., 2020), and\nMLQA (Lewis et al., 2020) as well as the natural language inference task XNLI (Conneau et al., 2018)\nand the paraphrase detection task PAWS-X (Yang et al., 2019) from XTREME (Hu et al., 2020). We\nevaluate on the in-language multi-task setting for TyDiQA-GoldP (Clark et al., 2020) where models\nare ﬁne-tuned on the combined gold data in all target languages and the translate-train-all setting\nwhere models are ﬁne-tuned on English training data plus translations in all target languages for the\nother datasets. Both are the best-performing settings for the respective tasks in (Hu et al., 2020). In\naddition, we evaluate on zero-shot cross-lingual transfer from English on XNLI and PAWS-X.\nBaselines We compare to strong multilingual subword-based baselines including multilingual BERT\n(Devlin et al., 2019) and multilingual T5 (Xue et al., 2020). In addition, we compare to the byte-level\nmodels from §3.1, which we pre-train on multilingual data.\nSetup We pre-train CHARFORMER as well as the Byte-level T5 and Byte-level T5+LASC baselines\non multilingual mC4 Common Crawl (Xue et al., 2020) in 101 languages. Base size models were\ntrained for 1M steps using a batch size of 64 and sequence length of 2048, with the exception\nof Byte-level T5Base, which was trained with a sequence length of 1024, as training speed was\nprohibitively slow (see Table 10). CHARFORMER SBase and CHARFORMER SBase,LongPT (longer\npre-training) are trained with larger batch sizes for fair comparison with mT5. In particular, CHAR -\nFORMER SBase pre-trains on the same amount of tokens after downsampling as mT5 Base, while\nCHARFORMER SBase,LongPT pre-trains on roughly the same amount of raw text as mT5Base, given\nthat a SentencePiece subword token is about 4.1 bytes on average (Xue et al., 2021); see Table 5\nfor further details. All models were ﬁne-tuned with an input sequence length of 4096 for question-\nanswering tasks and 2048 for inference tasks. Score calibration was not used for these experiments,\nas it did not beneﬁt the model in the multilingual setting. For XNLI and PAWS-X (both translate-train\nand zero-shot settings), we also observed that performance improved if the GBST layer was not\nupdated during ﬁne-tuning; the reported CHARFORMER numbers reﬂect this conﬁguration. Otherwise,\nall other hyper-parameters and model sizes are unchanged from the English experimental setup.\nResults We show in-language multi-task, translate-train, and cross-lingual zero-shot results in\nTable 4. CHARFORMER SBase is competitive with standard subword-based models and CHAR -\nFORMER SBase,LongPT outperforms subword-based models on TyDiQA-GoldP (in-language multi-\ntask). Additionally, in the translate-train setting CHARFORMER SBase,LongPT is on par with subword\nmodels on XQuAD and MLQA, and close to parity on PAWS-X. Furthermore, CHARFORMER\n7\nPublished as a conference paper at ICLR 2022\nTable 5: Comparison of pre-training compute metrics for mT5 (Subword) versus comparable qual-\nity CHARFORMER models on the mC4 dataset. 64 TPUv3 chips were used for this experiment.\nCHARFORMER SBase sees the same number of tokens after downsampling as mT5 Base, while\nCHARFORMER SBase,LongPT roughly sees the same amount of raw text as mT5Base, given that a\nSentencePiece subword token is about 4.1 bytes on average (Xue et al., 2021). CHARFORMER SBase\nis 28% faster than mT5 Base, while using 33% of the FLOPS.\nModel Batch Size L d s |θ| Speed (steps/s) FLOPS\nmT5Base (Subword) 1024 1024 - 582M 1.54 1.3 ×1015\nCHARFORMER SBase 1024 2048 2 134M 1.98 4.3 ×1014\nCHARFORMER SBase,LongPT 2048 2048 2 134M 1.01 4.3 ×1014\nTable 6: Pre-training compute metrics of models at different input lengths, downsampling rates,\nand model sizes on the English C4 dataset. 16 TPUv3 chips were used for this experiment. These\nnumbers reﬂect a batch size of 64. Memory refers to per-device peak memory usage on TPUv3 chips.\nModel L d s |θ| Speed (steps/s) FLOPS Peak Mem.\nT5Base (Subword) 512 - 220M 9.3 1.1 ×1013 -\nByte-level T5Base 1024 1 200M 8.2 2.9 ×1013 3.09GB\nByte-level T5+LASCBase 1024 4 205M 15 9.9 ×1012 1.62GB\nCHARFORMER Base 1024 2 206M 11 1.6 ×1013 1.95GB\nCHARFORMER Base 1024 3 203M 15 1.1 ×1013 1.63GB\nCHARFORMER SBase 1024 2 134M 14 1.3 ×1013 1.73GB\nCHARFORMER SBase 1024 3 134M 20 8.7 ×1012 1.34GB\noutperforms other character-level models in the zero-shot setting. However, we observe that this\nsetting still remains a challenge for token-free models in general. We hypothesize that model size\nmay be a major factor here. Finally, we provide additional comparison between GBST and LASC at\na ﬁxed down-sampling rate in Section 4.3, showing that GBST signiﬁcantly outperforms LASC on\nTyDiQA.\n4 A NALYSES\n4.1 S PEED , MEMORY AND PARAMETERS\nTable 6 reports the speed (global training steps per second), parameter sizes and number of ﬂoat-\ning point operations (FLOPS) for each forward pass of the models used in our experiments. All\nexperiments were run on 16 TPU-v3 chips and speed is benchmarked on English C4 pre-training\nat the 1K input length ( L). CHARFORMER models are generally more efﬁcient both in terms of\nspeed and FLOPS compared to other character-level models at different parameter sizes. With a low\ndown-sampling rate ds for CHARFORMER , Byte-level T5+LASC is more efﬁcient due to using a\nhigher down-sampling rate. Directly consuming the character sequence with a Transformer model\nis slow and requires a large number of FLOPS, which is exacerbated with longer sequence lengths\nwhere Byte-level T5 is more than 2×slower than the fastest CHARFORMER . This difference is even\nlarger at longer input sequence lengths, which we report in the Appendix. CHARFORMER SBase\nachieves better performance (see §3) with fewer parameters but more FLOPS by using a deep thin\nencoder and is twice as fast as the subword-based model with similar performance, T5Base.\n4.2 V ISUALIZING LATENT SUBWORDS\nOne beneﬁt of CHARFORMER compared to other character-level methods is that the subwords it\nlearns are directly interpretable and may give some indications to the behaviour of the underlying\nmodel. We visualize the scores the multilingual CHARFORMER has learned to assign to subword\nblocks of different sizes for the string ‘on subword tokenization’ in Figure 3. We observe that the\nmodel learns to allocate single-character subword blocks predominantly to vowels and whitespace in\nEnglish. Moreover, in English the model allocates larger subword blocks to the beginning and end\n8\nPublished as a conference paper at ICLR 2022\nFigure 3: Visualization of block scores (softmax weights) for every byte position from multilingual\nCHARFORMER SBase on an example English input.\nTable 7: Effect of ds on TyDiQA-GoldP (in-language multi-task).\nModel ds TyDiQA-GoldP F1\nCHARFORMER Small 2 69.6\nCHARFORMER Small 3 68.1\nCHARFORMER Small 4 66.6\nByte-level T5+LASCSmall 4 64.9\nCHARFORMER Base 2 75.8\nCHARFORMER Base 3 74.3\nCHARFORMER Base 4 73.2\nByte-level T5+LASCBase 4 70.6\nconsonants of a subword. Together, we believe this suggests that the model has learned a meaningful\nsegmentation of the input, and that it is able to dynamically mix between byte-level and subword-level\nfeatures. Such behaviour could also parallel the relative importance attributed to consonants for word\nidentiﬁcation observed during reading in humans (Lee et al., 2001; Carreiras et al., 2008).\n4.3 C OMPARING DOWNSAMPLING APPROACHES\nIn Table 9, we compare GBST downsampling with LASC downsampling (Clark et al., 2021) on\nTyDiQA-GoldP. For this experiment we use the same hyperparameters as in Section 3.3, except the\npre-training input length is 1024 instead of 2048. Note that this difference is negligible (0.1 F1) for\nCHARFORMER Base, ds = 2which also appears in Table 4. All hyperparameters are ﬁxed between\nCHARFORMER and Byte-level T5+LASC. Following (Clark et al., 2021) we set ds = 4for LASC,\nand we compare CHARFORMER at the same downsampling rate. We additionally include ds = 2and\nds = 3for CHARFORMER for comparison. With the same hyperparameters and downsampling rate,\nCHARFORMER outperforms Byte-level T5+LASC on TyDiQA-GoldP.\n4.4 A BLATION STUDY\nThis section presents our ablation experiments for both English and multilingual tasks. We analyze the\nimpact of various hyper-parameters and modeling choices such as using offsets vs 1D convolutions.\nAcross experiments, we ﬁnd that pre-GBST convolutions are preferred to enumerating offset blocks,\nas it results in similar (or better) quality but a more efﬁcient implementation. For English tasks, block\nscore calibration (BC) improves performance. We note that in the multilingual setting, block score\ncalibration has little effect. The impact of different downsampling rates varies across tasks and model\nsizes. We also experimented with different convolution ﬁlter sizes in English and found that they\ndid not signiﬁcantly impact performance. Likewise, using a different character span corruption rate\nduring pre-training did not signiﬁcantly impact performance. Adding feed-forward layers to the\nCHARFORMER module in similar fashion to a Transformer block was also not obviously helpful.\n9\nPublished as a conference paper at ICLR 2022\nTable 8: Ablation studies with CHARFORMER Small on English tasks.\nAblation ds Size SST-2 MNLI mm IMDb\nOffsets 2 S 89.11 79.50 90.49\nConv 2 S 89.11 79.65 90.63\nConv + BC 2 S 89.56 80.15 90.60\nConv + Offsets + BC 2 S 89.11 79.68 90.48\nConv 3 S 89.45 80.07 90.15\nConv 4 S 89.11 79.82 90.21\nConv 2 B 90.60 82.92 91.46\nConv 3 B 91.40 82.74 91.46\nConv 4 B 91.40 82.67 92.33\n5 R ELATED WORK\nSubword tokenization Standard algorithms for deterministic subword tokenization are Byte Pair\nEncoding (BPE; Sennrich et al., 2016), Wordpiece (Wu et al., 2016), and SentencePiece (Kudo\nand Richardson, 2018). Prior work has highlighted issues with some of these algorithms (Bostrom\nand Durrett, 2020) and has generally observed that models learned with such rigid tokenization\ndo not cope well with variation in language (Sun et al., 2020). To make a model more robust to\nmorphological and compositional generalization, probabilistic segmentation algorithms such as\nsubword regularization (Kudo, 2018) and BPE-dropout (Provilkov et al., 2020) have been proposed,\nwhich sample different segmentations during training. Recent methods propose to make models\nmore robust for downstream tasks by enforcing prediction consistency between deterministic and\nprobabilistic segmentations (Wang et al., 2021) and propose to update the tokenizer based on the\ndownstream loss under different segmentations (Hiraoka et al., 2020; 2021). He et al. (2020)\nproposed DPE (dynamic programming encoding), a segmentation-based tokenization algorithm based\non dynamic programming. Such methods, however, incur large computation costs due multiple\nforward passes needing to be performed for each segmentation of an example or due to the expensive\nDP computation, which make them unsuitable for pre-training.\nCharacter-level models For recurrent neural networks, pure character-level models that take a\nsequence of characters as input (Graves, 2013; Zhang et al., 2015; Hwang and Sung, 2017) have\nmostly been superseded by character-aware methods that compute a token-level representation using\na CNN over characters (Kim et al., 2016; Jozefowicz et al., 2016; Peters et al., 2018) due to poor\nperformance when learning directly from characters. Such character-aware representations have\nlately been applied to deep Transformer models (El Boukkouri et al., 2020; Ma et al., 2020). These\nmethods, however, still require tokenization for pre-processing and cannot be directly applied to\nlanguages without whitespace separation. Prior work also learned segmentation as part of the model\nbut did not scale very well (Wang et al., 2017; Kreutzer and Sokolov, 2018; Kawakami et al., 2019).\nOne notable exception is (Lee et al., 2017), which enabled fully character-level neural machine\ntranslation, using stacked convolutions, max pooling, and highway networks. Building on this, recent\ntokenization-free approaches such as CANINE (Clark et al., 2021) revisit the original character-level\nsetting in the context of large pre-trained language models with a focus on multilingual models.\nOur method outperforms CANINE-style downsampling (local attention, strided convolutions) and\nalso leads to improvements in the monolingual setting, while using less compute and parameters to\ndown-sample than both Lee et al. (2017) and Clark et al. (2021). Recently, ByT5 (Xue et al., 2021)\nset new start-of-the-art results for tokenization-free models, by operating on the byte-level. This work\nperforms on par with or outperforms ByT5, with signiﬁcant gains in speed and compute efﬁciency.\nMultilingual models Current multilingual models are generally analogues to successful monolingual\nTransformer models (Ruder et al., 2021). Consequently, models such as multilingual BERT (Devlin\net al., 2019) and XLM-R (Conneau et al., 2020) employ the same subword tokenization algorithms as\nmonolingual models, now applied to a massively multilingual corpus. In the multilingual setting, the\nproblems of subword-based tokenization are exacerbated as tokens in languages with few data are\nover-segmented while high-frequency tokens are under-segmented, which limits cross-lingual transfer\n(Wang et al., 2021). This motivates our work as well as recent work on character-level models.\n10\nPublished as a conference paper at ICLR 2022\nEfﬁcient Transformers Moving from subwords to characters signiﬁcantly increases the sequence\nlength, which is an issue for Transformers due to the quadratic complexity of self-attention. Many ef-\nﬁcient self-attention models have been proposed (Choromanski et al., 2020; Wang et al., 2020; Zaheer\net al., 2020) to tackle this problem; see (Tay et al., 2020b;a) for a comprehensive overview. Notably,\nthe CANINE model uses local attention (Parmar et al., 2018), which could also be swapped with\nanother efﬁcient Transformer variant. We note that the problem of efﬁciency is important but not the\nonly challenge towards developing performant tokenization-free models. While applying an efﬁcient\nattention mechanism might solve the fundamental computational costs of employing character-level\nmodels, there is no guarantee that these models will learn locally meaningful compositions.\n6 C ONCLUSION\nWe have proposed CHARFORMER , a re-scaled Transformer architecture that integrates gradient-based\nsubword tokenization, a novel lightweight tokenization method that enables efﬁcient end-to-end\nlearning of latent subwords directly from characters. We have demonstrated that English and\nmultilingual variants of CHARFORMER outperform strong character-level baselines across various\ndatasets while being more efﬁcient. CHARFORMER achieves performance on par with subword-based\nmodels on standard English tasks and outperforms subword-based models on noisy social media\ndata. On multilingual data, CHARFORMER generally performs on par with subword-based models,\nwhile being faster than both byte-level and subword-level baselines. Finally, we provide a method to\ninspect the inner workings of the GBST module. Overall, we believe that the strong results presented\nin this paper pave the way for highly effective and powerful token-free models.\nETHICS STATEMENT\nStandard subword tokenization algorithms produce segmentations that do not equally represents\nwords and phrases in different languages. Instead, they are biased towards languages that already\nhave many resources available, which leads to multilingual models performing worse on under-\nrepresented languages (Wang et al., 2021). Tokenization-free approaches such as the one proposed in\nthis paper may help to ameliorate this to some extent. Another challenge to using large multilingual\nmodels in practice is their relative computational inefﬁciency, which makes them unsuitable in\nresource-constrained settings common in scenarios where under-represented languages are spoken.\nCHARFORMER trains 28% faster than mT5 and has 3×fewer parameters, so may be a more suitable\nchoice in such settings compared to state-of-the-art multilingual models.\nREPRODUCIBILITY STATEMENT\nAll code to train the core byte-level Transformer encoder-decoder for CHARFORMER its variants\nis already open-sourced as a part of the Mesh Tensorﬂow6 (Shazeer et al., 2018), T57 (Raffel et al.,\n2020), and ByT58 (Xue et al., 2021) libraries. Additionally, an implementation of Charformer GBST\ncompatible with existing open-source models has been open-sourced9. All detailed experiment and\nhyperparameter settings required to reproduce our experiments can be found in Section 7.1 of the\nAppendix.\nREFERENCES\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the Cross-lingual Transferability of\nMonolingual Representations. In Proceedings of ACL 2020, 2020. URL http://arxiv.org/\nabs/1910.11856.\n6https://github.com/tensorflow/mesh\n7https://github.com/google-research/text-to-text-transfer-transformer\n8https://github.com/google-research/byt5\n9https://github.com/google-research/google-research/tree/master/\ncharformer\n11\nPublished as a conference paper at ICLR 2022\nYonatan Belinkov and Yonatan Bisk. Synthetic and Natural Noise Both Break Neural Machine\nTranslation. In Proceedings of ICLR 2018 , 2018. URL http://arxiv.org/abs/1711.\n02173.\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced\nmetrics for measuring unintended bias with real data for text classiﬁcation. CoRR, abs/1903.04561,\n2019. URL http://arxiv.org/abs/1903.04561.\nKaj Bostrom and Greg Durrett. Byte Pair Encoding is Suboptimal for Language Model Pretraining.\nIn Findings of EMNLP 2020, pages 4617–4624, 2020. doi: 10.18653/v1/2020.ﬁndings-emnlp.414.\nManuel Carreiras, Margaret Gillon-Dowens, Marta Vergara, and Manuel Perea. Are vowels and\nconsonants processed differently? event-related potential evidence with a delayed letter paradigm.\nJournal of Cognitive Neuroscience, 21(2):275–288, 2008.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055, 2017.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794, 2020.\nHyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, and Sebastian Ruder. Rethinking\nEmbedding Coupling in Pre-trained Language Models. In Proceedings of ICLR 2021, 2021.\nJon Clark, Tom Kwiatkowski, Jennimaria Palomaki, Michael Collins, and Dan Garrette. TyDi QA: A\nBenchmark for Information-Seeking Question Answering in Typologically Diverse Languages. In\nTransactions of the ACL, 2020.\nJonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efﬁcient\ntokenization-free encoder for language representation. arXiv preprint arXiv:2103.06874, 2021.\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger\nSchwenk, and Veselin Stoyanov. XNLI: Evaluating Cross-lingual Sentence Representations. In\nProceedings of EMNLP 2018, 2018. URL http://arxiv.org/abs/1809.05053.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-\nsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , pages 8440–8451, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL\nhttps://www.aclweb.org/anthology/2020.acl-main.747.\nJacob Devlin. Sharp models on dull hardware: Fast and accurate neural machine translation decoding\non the cpu. arXiv preprint arXiv:1705.01991, 2017.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In Proceedings of NAACL 2019, 2019.\nURL http://arxiv.org/abs/1810.04805.\nWilliam B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.\nHicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Hiroshi Noji, Pierre Zweigenbaum, and\nJun’ichi Tsujii. CharacterBERT: Reconciling ELMo and BERT for word-level open-vocabulary\nrepresentations from characters. In Proceedings of the 28th International Conference on Com-\nputational Linguistics, pages 6903–6915, Barcelona, Spain (Online), December 2020. Interna-\ntional Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.609. URL\nhttps://www.aclweb.org/anthology/2020.coling-main.609.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. FRAGE: Frequency-\nAgnostic Word Representation. In Proceedings of NIPS 2018, 2018.\n12\nPublished as a conference paper at ICLR 2022\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,\n2013.\nXuanli He, Gholamreza Haffari, and Mohammad Norouzi. Dynamic programming encoding for\nsubword segmentation in neural machine translation. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages 3042–3051, Online, July 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.acl-main.275. URL https://www.\naclweb.org/anthology/2020.acl-main.275.\nTatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki, and Naoaki Okazaki. Optimiz-\ning word segmentation for downstream task. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020 , pages 1341–1351, Online, November 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.120. URL https:\n//www.aclweb.org/anthology/2020.findings-emnlp.120.\nTatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki, and Naoaki Okazaki. Joint Optimization\nof Tokenization and Downstream Model. In Findings of ACL-IJCNLP 2021, 2021. URL http:\n//arxiv.org/abs/2105.12410.\nJeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classiﬁcation.\nIn Proceedings of ACL 2018, 2018. URL http://arxiv.org/abs/1801.06146.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson.\nXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual General-\nization. In Proceedings of ICML 2020, 2020.\nKyuyeon Hwang and Wonyong Sung. Character-level language modeling with hierarchical recurrent\nneural networks. In 2017 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5720–5724. IEEE, 2017.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah A. Smith. Deep Encoder, Shallow\nDecoder: Reevaluating Non-autoregressive Machine Translation. In Proceedings of ICLR 2021,\n2021. ISBN 0080437516.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom. Learning to discover, ground and use words with\nsegmental neural language models. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 6429–6441, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1645. URL https://www.aclweb.org/\nanthology/P19-1645.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander Rush. Character-aware neural language\nmodels. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 30, 2016.\nJulia Kreutzer and Artem Sokolov. Learning to segment inputs for nmt favors character-level\nprocessing, 2018.\nTaku Kudo. Subword regularization: Improving neural network translation models with mul-\ntiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) , pages 66–75, Melbourne, Australia,\nJuly 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1007. URL\nhttps://www.aclweb.org/anthology/P18-1007.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71,\nBrussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/\nD18-2012. URL https://www.aclweb.org/anthology/D18-2012.\nGuillaume Lample and Alexis Conneau. Cross-lingual Language Model Pretraining. In Proceedings\nof NeurIPS 2019, 2019. URL https://github.com/google-research/bert.\n13\nPublished as a conference paper at ICLR 2022\nHye-Won Lee, Keith Rayner, and Alexander Pollatsek. The relative contribution of consonants and\nvowels to word identiﬁcation during reading. Journal of Memory and Language, 44(2):189–205,\n2001.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine transla-\ntion without explicit segmentation. Transactions of the Association for Computational Linguis-\ntics, 5:365–378, 2017. doi: 10.1162/tacl_a_00067. URL https://aclanthology.org/\nQ17-1026.\nPatrick Lewis, Barlas O˘guz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. MLQA: Evaluating\nCross-lingual Extractive Question Answering. In Proceedings of ACL 2020, 2020. URL http:\n//arxiv.org/abs/1910.07475.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang, and Guoping Hu. CharBERT: Character-\naware pre-trained language model. In Proceedings of the 28th International Conference on\nComputational Linguistics, pages 39–50, Barcelona, Spain (Online), December 2020. International\nCommittee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.4. URL https:\n//www.aclweb.org/anthology/2020.coling-main.4.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the\nassociation for computational linguistics: Human language technologies, pages 142–150, 2011.\nChristopher Manning and Hinrich Schütze. Foundations of statistical natural language processing.\nMIT press, 1999.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words\nand Phrases and their Compositionality. In Advances in Neural Information Processing Systems,\n2013.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In International Conference on Machine Learning , pages\n4055–4064. PMLR, 2018.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-HLT\n2018, 2018.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita. BPE-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 1882–1892, Online, July 2020. Association for Computational Linguistics. doi:\n10.18653/v1/2020.acl-main.170. URL https://www.aclweb.org/anthology/2020.\nacl-main.170.\nDanish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. Combating adversarial misspellings\nwith robust word recognition. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 5582–5591, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1561. URL https://www.aclweb.org/\nanthology/P19-1561.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uniﬁed\nText-to-Text Transformer. Journal of Machine Learning Research , 21, 2020. URL http:\n//arxiv.org/abs/1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016. Association\nfor Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.\norg/anthology/D16-1264.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu,\nJunjie Hu, Graham Neubig, and Melvin Johnson. Xtreme-r: Towards more challenging and\nnuanced multilingual evaluation. arXiv preprint arXiv:2104.07412, 2021.\n14\nPublished as a conference paper at ICLR 2022\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149–5152. IEEE, 2012.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) , pages 1715–1725, Berlin, Germany, Au-\ngust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL\nhttps://www.aclweb.org/anthology/P16-1162.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep\nlearning for supercomputers. arXiv preprint arXiv:1811.02084, 2018.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/D13-1170.\nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, and Caiming Xiong.\nAdv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert. arXiv\npreprint arXiv:2003.04985, 2020.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient\ntransformers. arXiv preprint arXiv:2011.04006, 2020a.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv\npreprint arXiv:2009.06732, 2020b.\nYi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are\npre-trained convolutions better than pre-trained transformers? arXiv preprint arXiv:2105.03322,\n2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\neditors, Advances in Neural Information Processing Systems , volume 30. Curran Asso-\nciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nChong Wang, Yining Wang, Po-Sen Huang, Abdelrahman Mohamed, Dengyong Zhou, and Li Deng.\nSequence modeling via segmentations. In International Conference on Machine Learning, pages\n3674–3683. PMLR, 2017.\nSinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\nXinyi Wang, Sebastian Ruder, and Graham Neubig. Multi-view Subword Regularization. In\nProceedings of NAACL 2021, 2021. URL http://arxiv.org/abs/2103.08490.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers) , pages 1112–1122, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL\nhttps://www.aclweb.org/anthology/N18-1101.\n15\nPublished as a conference paper at ICLR 2022\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson,\nXiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex\nRudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s Neural\nMachine Translation System: Bridging the Gap between Human and Machine Translation. arXiv\npreprint arXiv:1609.08144, 2016.\nEllery Wulczyn, Nithum Thain, and Lucas Dixon. Ex machina: Personal attacks seen at scale. In\nProceedings of the 26th International Conference on World Wide Web, WWW ’17, pages 1391–\n1399, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences\nSteering Committee. ISBN 9781450349130. doi: 10.1145/3038912.3052591. URL https:\n//doi.org/10.1145/3038912.3052591.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer, 2020.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam\nRoberts, and Colin Raffel. ByT5: Towards a token-free future with pre-trained byte-to-byte models.\narXiv preprint arXiv:2105.13626, 2021. URL http://arxiv.org/abs/2105.13626.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A Cross-lingual Adversarial\nDataset for Paraphrase Identiﬁcation. In Proceedings of EMNLP 2019 , 2019. URL http:\n//arxiv.org/abs/1908.11828.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer\nsequences. arXiv preprint arXiv:2007.14062, 2020.\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional Networks for Text\nClassiﬁcation. Advances in Neural Information Processing Systems, pages 649–657, 2015. URL\nhttp://arxiv.org/abs/1509.01626#.\n16\nPublished as a conference paper at ICLR 2022\n7 A PPENDIX\n7.1 H YPERPARAMETERS\nThis section describes the hyperparameters that we use in our experiments.\nMonolingual English Datasets Our small model follows the T5 small model size with 6 encoder\nlayers and 6 decoder layers, hidden size dmodel of 512, 8 heads, dkv of 32 and dff of 2048. This\ncorresponds to bi_v1_small.gin in the T5 codebase. The base model (corresponding to bi_v1.gin) has\n12 encoder layers, 12 decoder layers, dmodel of 768, dff of 3072 and 12 heads. The SBase model has\n24 encoder layers and 6 decoder layers, while the remainder of its hyperparameters remain identical\nto the small model. All Transformer stacks use relative attention over positional encodings as per\n(Raffel et al., 2020). For pre-training, we run our models for 1M steps on C4 with a batch size of\n64. The maximum sequence length for all tasks is set to 1024. TPU packing is not activated for\nCharformer. For Charformer, the ﬁlter size of the pre-GBST convolution is set to 5 by default. For\nCHARFORMER , the downsampling rate is tuned in the range of {2,3,4}. For smaller models, the rate\nof 2 seems to work consistently the best. For base models, the best models used a downsampling rate\nof either 2 or 3. For the SBase models, the optimal downsampling rate was often 3.\nMultilingual Datasets Hyperparameters are kept constant between English and multilingual tasks\nexcept for the following differences. For pre-training, we run our models for 1M steps with a\nbatch size of 64, except for CHARFORMER SBase which uses a batch size of 1024 and CHAR -\nFORMER SBase,LongPT which usees a batch size of 2048. Models were pre-trained with a maximum\nsequence length of 2048 and ﬁne-tuned with a maximum sequence length of 4096 for TyDiQA,\nXQuAD, and MLQA, and 2048 for XNLI and PAWS-X. Byte-level T5Base was the only model to\nbe pre-trained with a maximum sequence length of 1024, as it was prohibitively slow, see Table 10.\nFine-tuning and inference for this model, however still used 4096 and 2048 input lengths identical to\nother models. For all tasks, CHARFORMER models used a downsampling rate of 2, while Byte-level\nT5+LASC models used a downsampling rate of 4 (Clark et al., 2021). The downsampling rate of 2\nwas picked by ablating the downsampling rate on the TyDiQA-GoldP validation set. CHARFORMER\nmodels for XNLI and PAWS-X additionally did not back-propagate into the GBST layer during\nﬁne-tuning. Checkpoints were picked based on the dev set metrics, and then evaluated on test set.\nReported metrics represent the macro-average of all languages in the task.\n7.2 L ARGE -SCALE EXPERIMENTS\nIn this section we report preliminary results for scaling Charformer using the same number of\nparameters as mT5Large and ByT5Large (1.23B). We follow a model scaling conﬁguration identical\nto ByT5 in these experiments, and use the same hyperparameter settings as our main multilingual\nresults.\nTable 9: Comparison on TyDiQA at 1.23B parameters. *Due to resource constraints, the Charformer\nresult below uses ∼100K less pretraining steps than ByT5 and mT5.\nModel TyDiQA-GoldP F1 / EM\nmT5Large 85.3 / 75.3\nByT5Large 87.7 / 79.2\nCHARFORMER * 86.3 / 77.3\nResults The CHARFORMER model under the same scaling as ByT5Large was able to outperform\nmT5Large, a very strong baseline. Our preliminary results at this scale shows that CHARFORMER is\ncompetitive with, but is 1.4 F1 behind ByT5Large. However, we point out two important notes. First,\nthe CHARFORMER result is undertrained compared to ByT5Large since 10% of the pretraining has\nnot ﬁnished. Second, the CHARFORMER model is also twice as fast as ByT5, as seen from Table 10.\n7.3 M ULTILINGUAL EXPERIMENTS\nThis section contains detailed results for our multilingual experiments.\n17\nPublished as a conference paper at ICLR 2022\nTable 10: Compute metrics of base models at longer (2K) input length on the mC4 pre-training\ncorpus, using a batch size of 64 on 16 TPU-v3 chips.\nModel L d s |θ| Speed (steps/s) FLOPS\nByte-level T5Base 2048 1 200M 2.7 2.0 ×1013\nByte-level T5+LASCBase 2048 4 205M 11 5.5 ×1012\nCHARFORMER Base 2048 2 203M 6.1 9.5 ×1012\nCHARFORMER Base 2048 3 203M 10 6.5 ×1012\nCHARFORMER SBase 2048 2 134M 6.1 9.2 ×1012\nTable 11: Per-language breakdown of in-language multi-task TyDiQA-GoldP results.\nModel |θ| ar bn en ﬁ id ko ru sw te Avg.\nmBERTBase(Subword) 179M -/- -/- -/- -/- -/- -/- -/- -/- -/- 77.6/68.0mT5Base(Subword) 582M 84.2/71.8 80.0/69.0 76.6/65.2 80.1/69.3 85.5/75.0 70.3/61.6 77.5/64.4 83.6/74.9 88.2/78.0 80.8 / 70.0\nByte-level T5Base 200M 81.4/67.0 66.8/56.6 69.8/59.5 75.6/63.0 81.6/72.4 64.6/58.7 74.1/60.8 81.8/74.3 85.0/76.1 75.6/65.4Byte-level T5+LASCBase 205M 78.1/62.3 61.1/50.4 66.7/55.2 72.5/60.4 79.9/68.3 51.5/43.5 70.4/58.7 74.7/67.5 80.2/71.2 70.6/59.7CHARFORMERBase 203M 81.8/67.9 69.1/60.2 71.4/60.5 76.3/64.2 83.0/73.1 62.7/54.3 74.7/61.7 80.2/73.3 83.6/75.0 75.9/65.6\nCHARFORMERSBase 134M 82.4/68.1 78.1/67.3 75.4/64.3 79.5/68.2 85.0/75.9 66.6/58.0 77.0/64.3 81.5/74.1 86.5/78.6 79.1/68.8CHARFORMERSBase,LongPT134M 85.7/74.5 78.7/67.3 76.8/65.9 81.9/70.6 86.7/79.1 69.4/61.6 79.2/67.1 83.7/75.2 88.8/80.6 81.2/71.3\nTable 12: Per-language breakdown of translate-train-all XQuAD results.\nModel |θ| ar de el en es hi ru th tr vi zh Avg.\nmT5Base(Subword) 582M 72.4/55.2 76.9/59.7 76.8/58.8 83.1/70.3 79.0/61.2 71.4/53.4 76.1/58.5 67.9/62.0 72.5/51.4 75.9/56.3 76.9/69.7 75.3/59.7\nByte-level T5Base 200M 64.8/47.9 74.3/58.3 69.2/51.8 81.5/70.4 77.2/60.4 67.0/51.5 72.3/55.5 48.3/41.9 69.6/51.7 73.3/54.4 57.3/53.3 68.6/54.3Byte-level T5+LASCBase 205M 62.9/45.5 70.6/54.2 68.3/52.3 80.1/68.4 74.8/57.9 63.1/46.2 68.2/52.2 50.0/43.4 67.1/48.2 71.7/51.8 57.7/52.7 66.8/52.1CHARFORMERBase 203M 65.7/49.8 74.2/58.0 71.1/53.1 82.2/70.5 77.8/61.0 67.0/51.3 73.4/57.6 54.3/48.0 70.3/53.0 74.6/55.6 62.0/56.6 70.2/55.9\nCHARFORMERSBase 134M 70.3/53.7 78.6/61.4 74.4/55.1 85.1/73.7 79.8/63.6 69.1/52.7 76.7/61.3 57.6/51.2 73.9/55.8 76.8/57.6 67.4/62.4 73.6/59.0CHARFORMERSBase,LongPT134M 72.6/55.0 79.0/62.3 74.9/56.1 85.4/74.5 80.4/63.4 70.6/56.1 77.8/62.2 56.1/49.2 76.1/58.2 77.7/59.4 66.0/61.8 74.2/59.8\nTable 13: Per-language breakdown of translate-train-all MLQA results.\nModel |θ| ar de en es hi vi zh Avg.\nmT5Base(Subword) 582M 61.1/40.7 65.5/49.2 80.7/66.3 70.7/52.1 63.6/44.3 68.0/47.6 63.5/39.4 67.6/48.5\nByte-level T5Base 200M 52.6/34.2 60.5/46.1 77.7/64.8 67.1/49.2 52.9/36.5 63.6/43.8 58.3/36.4 61.8/44.4Byte-level T5+LASCBase 205M 50.8/32.0 58.1/43.5 75.8/62.2 64.7/46.7 49.2/32.6 60.4/40.4 52.6/30.6 58.8/41.1CHARFORMERBase 203M 53.5/34.5 61.3/46.8 78.5/65.4 67.2/49.3 54.5/37.6 64.3/43.9 58.8/36.6 62.6/44.9\nCHARFORMERSBase 134M 58.3/39.1 65.7/50.5 81.8/68.7 71.0/53.1 57.7/40.8 67.3/46.8 62.7/40.8 66.3/48.5CHARFORMERSBase,LongPT134M 59.6/40.0 66.6/51.3 82.2/69.0 72.1/54.5 59.7/42.9 68.2/47.4 62.4/40.7 67.2/49.4\nTable 14: Per-language breakdown of translate-train-all and cross-lingual zero-shot XNLI results.\nModel |θ| ar bg de el en es fr hi ru sw th tr ur vi zh Avg.\nTranslate-Train-All\nmT5Base(Subword) 582M 74.4 78.5 77.7 78.1 82.0 79.1 77.9 72.2 76.5 71.5 75.0 74.8 70.4 74.5 76.0 75.9\nByte-level T5Base 200M 67.1 72.0 71.0 70.6 76.9 74.0 73.4 63.7 69.2 66.2 65.7 69.4 62.8 69.6 69.0 69.4Byte-level T5+LASCBase 205M 65.6 72.1 70.5 67.9 75.6 73.4 72.2 63.5 68.6 65.4 64.5 67.4 62.4 68.3 61.0 67.9CHARFORMERBase 203M 69.5 72.9 72.7 72.6 78.2 74.5 73.6 67.0 71.7 67.9 68.1 70.8 65.0 70.7 71.5 71.1\nCHARFORMERSBase 134M 70.8 75.7 75.9 73.1 80.9 76.9 76.8 65.6 74.7 65.7 67.7 72.0 63.1 72.9 71.5 72.2CHARFORMERSBase,LongPT134M 71.1 75.9 73.6 74.2 80.8 76.6 76.8 69.2 72.2 68.2 71.0 71.2 65.7 72.9 73.0 72.8\nCross-Lingual Zero-Shot\nmBERTBase(Subword) 179M 64.3 68.0 70.0 65.3 80.8 73.5 73.4 58.9 67.8 49.7 54.1 60.9 57.2 69.3 67.8 65.4mT5Base(Subword) 582M 73.3 78.6 77.4 77.1 84.7 80.3 79.1 70.8 77.1 69.4 73.2 72.8 68.3 74.2 74.1 75.4\nByte-level T5Base 200M 56.7 61.2 63.0 60.9 79.2 70.1 65.3 43.9 61.0 45.5 43.5 52.0 44.3 58.3 55.6 57.4Byte-level T5+LASCBase 205M 53.3 58.8 62.2 54.9 77.1 68.6 65.4 44.7 58.4 46.1 43.6 50.4 42.8 55.9 46.1 55.2CHARFORMERBase 203M 55.7 61.1 64.8 60.1 77.3 69.9 67.9 44.4 60.2 45.3 47.9 54.0 43.5 59.1 53.4 57.6\nCHARFORMERSBase 134M 66.4 71.0 72.7 68.6 82.4 77.1 75.4 57.6 70.6 48.7 61.4 61.8 54.1 68.9 62.8 66.6CHARFORMERSBase,LongPT134M 68.4 70.9 74.3 70.2 82.4 77.0 76.6 59.9 71.0 42.6 64.0 65.5 56.5 71.2 66.0 67.8\n18\nPublished as a conference paper at ICLR 2022\nTable 15: Per-language breakdown of translate-train-all and cross-lingual zero-shot PAWS-X results.\nModel |θ| de en es fr ja ko zh Avg.\nTranslate-Train-All\nmT5Base(Subword) 582M 90.9 95.5 91.4 92.5 83.6 84.8 86.4 89.3\nByte-level T5Base 200M 89.3 94.6 90.1 90.3 81.4 81.1 82.3 87.0\nByte-level T5+LASCBase 205M 87.3 93.1 89.2 89.2 81.0 72.9 80.8 84.8\nCHARFORMERBase 203M 89.9 94.6 89.8 91.4 82.7 78.4 83.3 87.2\nCHARFORMERSBase 134M 89.9 95.9 91.8 92.2 83.9 78.9 84.4 88.2\nCHARFORMERSBase,LongPT134M 90.7 95.1 92.2 92.2 84.1 81.6 84.6 88.6\nCross-Lingual Zero-Shot\nmBERTBase(Subword) 179M 85.7 94.0 87.4 87.0 73.0 69.6 77.0 81.9\nmT5Base(Subword) 582M 89.4 95.4 89.6 91.2 79.8 78.5 81.1 86.4\nByte-level T5Base 200M 84.7 93.8 85.8 86.4 72.2 67.9 75.2 80.9\nByte-level T5+LASCBase 205M 83.2 93.2 84.1 85.0 67.9 66.4 73.4 79.0\nCHARFORMERBase 203M 86.1 94.8 87.2 88.0 70.1 69.7 75.5 81.6\nCHARFORMERSBase 134M 89.6 95.2 90.7 90.7 77.1 74.4 78.9 85.2\nCHARFORMERSBase,LongPT134M 89.8 95.3 88.7 89.7 74.5 68.9 78.9 83.7\nTable 16: Effect of freezing the GBST layer for XNLI and PAWS-X.\nModel ds Freeze GBST XNLI (Zero) XNLI (Translate) PAWS-X (Zero) PAWS-X (Translate)\nCHARFORMERSmall 2 No 44.5 62.7 27.9 37.5\nCHARFORMERSmall 2 Yes 50.9 68.7 77.1 84.8\nCHARFORMERSmall 3 No 47.9 67.9 29.5 36.8\nCHARFORMERSmall 3 Yes 43.2 68.6 77.8 83.7\nCHARFORMERSmall 4 No 47.5 47.5 30.9 36.9\nCHARFORMERSmall 4 Yes 43.6 43.6 77.9 83.5\n7.4 E XAMPLE IMPLEMENTATION\nFor additional clarity, we include a simpliﬁed implementation of the GBST module in Tensorﬂow\nbelow. Default hyper-parameters here match those used in the paper.\nfrom typing import Optional\nimport tensorflow as tf\nkeras_layers = tf.keras.layers\nclass GBSTLayer(keras_layers.Layer):\n\"\"\"Performs Charformer GBST on a sequence.\nAttributes:\ninput_shape: Shape [len, embedding_size] of input tensor in future calls,\nwithout batch dimension.\ndownsample_rate: Integer of how much to downsample by.\nmax_subword_block_width: Integer of max block size to use for enumeration.\nblock_attention: Hhether to use block score calibration.\nblock_scoring_network: module for parameterized block scoring.\nconv_kernel_size: Integer of the size of the pre-GBST convolution kernel.\n\"\"\"\ndef __init__(self,\ninput_shape: tf.Tensor,\ndownsample_rate: int = 2,\nmax_subword_block_width: int = 4,\nblock_attention: bool = False,\nconv_kernel_size: Optional[int] = 5):\nsuper(GBSTLayer, self).__init__()\nself.downsample_rate = downsample_rate\nself.max_subword_block_width = max_subword_block_width\nself.conv_kernel_size = conv_kernel_size\nself.conv_layer = keras_layers.Conv1D(\ninput_shape[-1], self.conv_kernel_size, input_shape=input_shape)\nself.block_attention = block_attention\nself.block_scoring_network = keras_layers.Dense(1, use_bias=False)\ndef call(self, inputs):\n\"\"\"Performs downsampling on the character-scale input representation.\nArgs:\ninputs: float Tensor of shape [batch_size, seq_length,\n19\nPublished as a conference paper at ICLR 2022\nembedding_size].\nReturns:\n<float>[batch_size, seq_length / downsample_rate , embedding_size].\nDownsampled sequences.\n\"\"\"\nlength = inputs.shape[1]\nif self.conv_kernel_size:\ninputs = self.conv_layer(inputs)\nall_block_scores = []\nall_sequences = []\nfor subword_len in range(1, self.max_subword_block_width):\npadded_input = inputs\n# Pad the sequence length if needed.\nif length % subword_len != 0:\npad_amt = subword_len - int(length % subword_len)\npadding = tf.constant([[0, 0], [0, pad_amt], [0, 0]])\npadded_input = tf.pad(inputs, padding)\n# For this block size, form candidate block embeddings and scores.\n# candidates shape: [batch, seq_len/subword_len, dim]\n# block_scores shape: [batch, seq_len/subword_len, 1]\ncandidates = tf.nn.avg_pool(\npadded_input, [subword_len], strides=[subword_len], padding=\"VALID\")\nblock_scores = self.block_scoring_network(candidates)\n# Upsample it back to the original sequence length.\nretiled_seq = tf.repeat(candidates, subword_len, axis=1)\nretiled_block_scores = tf.repeat(block_scores, subword_len, axis=1)\n# Repad the upsampled sequence if needed.\nif retiled_block_scores.shape[1] < length:\nrepad_amt = length - retiled_block_scores.shape[1]\nrepadding = tf.constant([[0, 0], [0, repad_amt], [0, 0]])\nretiled_seq = tf.pad(retiled_seq, repadding)\nretiled_block_scores = tf.pad(retiled_block_scores, repadding)\n# Make sure everything is the right length and add new dimension to concat\n# candidate blocks on.\nretiled_block_scores = retiled_block_scores[:, :length, :, None]\nretiled_seq = retiled_seq[:, :length, :, None]\nall_block_scores.append(retiled_block_scores)\nall_sequences.append(retiled_seq)\nblock_scores = tf.concat(all_block_scores, axis=-1)\nblock_scores = tf.nn.softmax(block_scores, axis=-1)\ncandidates = tf.concat(all_sequences, axis=-1)\n# TODO: Block score calibration / block-by-block attention is omitted in this implementation.\n# batch_size x num_candidates x length x dim\ncandidates = candidates * block_scores\noutput = tf.reduce_sum(candidates, axis=-1) # bsz x length x dim\n# Downsample by mean pooling.\nif self.downsample_rate > 1:\noutput = tf.nn.avg_pool(\noutput, (self.downsample_rate,),\nstrides=(self.downsample_rate,),\npadding=\"VALID\")\nreturn output\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8633939027786255
    },
    {
      "name": "Lexical analysis",
      "score": 0.8420295715332031
    },
    {
      "name": "Transformer",
      "score": 0.6865946054458618
    },
    {
      "name": "Security token",
      "score": 0.5958811640739441
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5812796354293823
    },
    {
      "name": "Byte",
      "score": 0.5267733335494995
    },
    {
      "name": "Language model",
      "score": 0.5168983936309814
    },
    {
      "name": "Natural language processing",
      "score": 0.41653239727020264
    },
    {
      "name": "Speech recognition",
      "score": 0.34939074516296387
    },
    {
      "name": "Programming language",
      "score": 0.16063624620437622
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}