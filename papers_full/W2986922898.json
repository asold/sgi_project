{
  "title": "Compressive Transformers for Long-Range Sequence Modelling",
  "url": "https://openalex.org/W2986922898",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225639001",
      "name": "Rae, Jack W.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288879230",
      "name": "Potapenko, Anna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287304911",
      "name": "Jayakumar, Siddhant M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287272415",
      "name": "Lillicrap, Timothy P.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2540404261",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963702144",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W2960955628",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963985863",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2795285343",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2963870701",
    "https://openalex.org/W2896528354",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W2963782041",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2894175714",
    "https://openalex.org/W2540419089",
    "https://openalex.org/W2722019070",
    "https://openalex.org/W179875071"
  ],
  "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",
  "full_text": "COMPRESSIVE TRANSFORMERS FOR LONG -RANGE\nSEQUENCE MODELLING\nJack W. Rae∗∗†‡ Anna Potapenko*† Siddhant M. Jayakumar† Chloe Hillier†\nTimothy P. Lillicrap†‡\nABSTRACT\nWe present the Compressive Transformer, an attentive sequence model which\ncompresses past memories for long-range sequence learning. We ﬁnd the Com-\npressive Transformer obtains state-of-the-art language modelling results in the\nWikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respec-\ntively. We also ﬁnd it can model high-frequency speech effectively and can be\nused as a memory mechanism for RL, demonstrated on an object matching task.\nTo promote the domain of long-range sequence learning, we propose a new open-\nvocabulary language modelling benchmark derived from books, PG-19.\n1 I NTRODUCTION\nHumans have a remarkable ability to remember information over long time horizons. When reading\na book, we build up a compressed representation of the past narrative, such as the characters and\nevents that have built up the story so far. We can do this even if they are separated by thousands\nof words from the current text, or long stretches of time between readings. During daily life, we\nmake use of memories at varying time-scales: from locating the car keys, placed in the morning,\nto recalling the name of an old friend from decades ago. These feats of memorisation are not\nachieved by storing every sensory glimpse throughout one’s lifetime, but via lossy compression. We\naggressively select, ﬁlter, or integrate input stimuli based on factors of surprise, perceived danger,\nor repetition — amongst other signals (Richards and Frankland, 2017).\nMemory systems in artiﬁcial neural networks began with very compact representations of the past.\nRecurrent neural networks (RNNs, Rumelhart et al. (1986)) learn to represent the history of obser-\nvations in a compressed state vector. The state is compressed because it uses far less space than the\nhistory of observations — the model only preserving information that is pertinent to the optimization\nof the loss. The LSTM (Hochreiter and Schmidhuber, 1997) is perhaps the most ubiquitous RNN\nvariant; it uses learned gates on its state vector to determine what information is stored or forgotten\nfrom memory.\nHowever since the LSTM, there has been great beneﬁt discovered in not bottlenecking all histori-\ncal information in the state, but instead in keeping past activations around in an external memory\nand attending to them. The Transformer (Vaswani et al., 2017) is a sequence model which stores\nthe hidden activation of every time-step, and integrates this information using an attention operator\n(Bahdanau et al., 2014). The Transformer will thus represent the past with a tensor (depth ×mem-\nory size ×dimension) of past observations that is, in practice, an order of magnitude larger than an\nLSTM’s hidden state. With this granular memory, the Transformer has brought about a step-change\nin state-of-the-art performance, within machine translation (Vaswani et al., 2017), language mod-\nelling (Dai et al., 2019; Shoeybi et al., 2019), video captioning (Zhou et al., 2018), and a multitude\nof language understanding benchmarks (Devlin et al., 2018; Yang et al., 2019) amongst others.\nOne drawback in storing everything is the computational cost of attending to every time-step and\nthe storage cost of preserving this large memory. Several works have focused on reducing the\ncomputational cost of attention with sparse access mechanisms (Rae et al., 2016; Child et al., 2019;\n∗Authors contributed equally, †DeepMind, London, UK. ‡CoMPLEX, Computer Science, University Col-\nlege London, UK. Please direct correspondence to {jwrae, apotapenko}@google.com.\n1\narXiv:1911.05507v1  [cs.LG]  13 Nov 2019\nSukhbaatar et al., 2019; Lample et al., 2019). However sparse attention does not solve the storage\nproblem, and often requires custom sparse kernels for efﬁcient implementation. Instead we look\nback to the notion of compactly representing the past. We show this can be built with simple dense\nlinear-algebra components, such as convolutions, and can reduce both the space and compute cost\nof our models.\nWe propose the Compressive Transformer, a simple extension to the Transformer which maps past\nhidden activations (memories) to a smaller set of compressed representations (compressed memo-\nries). The Compressive Transformer uses the same attention mechanism over its set of memories\nand compressed memories, learning to query both its short-term granular memory and longer-term\ncoarse memory. We observe this improves the modelling of text, achieving state-of-the-art results\nin character-based language modelling — 0.97 bpc on Enwik8 from the Hutter Prize (Hutter, 2012)\n— and word-level language modelling — 17.1 perplexity on WikiText-103 (Merity et al., 2016).\nSpeciﬁcally, we see the Compressive Transformer improves the modelling of rare words.\nWe show the Compressive Transformer works not only for language, but can also model the\nwaveform of high-frequency speech with a trend of lower likelihood than the TransformerXL and\nWavenet (Oord et al., 2016) when trained over 400,000 steps. We also show the Compressive Trans-\nformer can be used as a memory component within an RL agent, IMPALA (Espeholt et al., 2018),\nand can successfully compress and make use of past observations.\nFurthermore we present a new book-level language-modelling benchmark PG-19, extracted from\ntexts in Project Gutenberg 1, to further promote the direction of long-context sequence modelling.\nThis is over double the size of existing LM benchmarks and contains text with much longer contexts.\n2 R ELATED WORK\nThere have been a variety of recent attempts to extend the range of attention, particularly in the\nTransformer, or to replace the attention operation with something less expensive. Wu et al. (2019)\nshow that a convolution-like operator that runs in linear time can actually exceed the performance\nof the quadratic-time self-attention layer in the Transformer at sentence-to-sentence translation and\nsentence-level language modelling. However such a mechanism inhibits the ﬂow of information\nacross a large number of time-steps for a given layer, and has not shown to be beneﬁcial for long-\nrange sequence modelling.\nDai et al. (2019) propose the TransformerXL, which keeps past activations around in memory. They\nalso propose a novel relative positional embedding scheme which they see outperforms the Trans-\nformer’s original absolute positional system. Our model incorporates both of these ideas, the use of\na memory to preserve prior activations and their relative positional embedding scheme.\nThe Sparse Transformer (Child et al., 2019) uses ﬁxed sparse attention masks to attend to roughly√nlocations in memory. This approach still requires keeping all memories around during training,\nhowever with careful re-materialization of activations and custom kernels, the authors are able to\ntrain the model with a reasonable budget of memory and compute. When run on Enwik8, the much\nlarger attention window of 8,000 improves model performance, but overall it does not signiﬁcantly\noutperform a simpler TransformerXL with a much smaller attention window.\nThe use of dynamic attention spans is explored in Sukhbaatar et al. (2019). Different attention heads\ncan learn to have shorter or longer spans of attention — and they observe this achieves state-of-\nthe-art in character-based language modelling. This idea could easily be combined with our contri-\nbution — a compressive memory. However an efﬁcient implementation is not possible on current\ndense-linear-algebra accelerators, such as Google’s TPUs, due to the need for dynamic and sparse\ncomputation. Our approach builds on simple dense linear algebra components, such as convolutions.\n3 M ODEL\nWe present the Compressive Transformer, a long-range sequence model which compacts past acti-\nvations into a compressed memory. The Compressive Transformer is a variant of the Transformer\n1https://www.gutenberg.org/\n2\nCompressed  Memory Memory Sequence\nfc\n(3)\nt\nfc\n(2)\nfc\n(1)\nFigure 1: The Compressive Transformer keeps a ﬁne-grained memory of past activations, which are\nthen compressed into coarser compressed memories. The above model has three layers, a sequence\nlength ns = 3, memory sizenm = 6, compressed memory sizencm = 6. The highlighted memories\nare compacted, with a compression function fc per layer, to a single compressed memory — instead\nof being discarded at the next sequence. In this example, the rate of compression c= 3.\n(Vaswani et al., 2017), a deep residual network which only uses attention to propagate information\nover time (namely multi-head attention). We build on the ideas of the TransformerXL (Dai et al.,\n2019) which maintains a memory of past activations at each layer to preserve a longer history of con-\ntext. The TransformerXL discards past activations when they become sufﬁciently old (controlled by\nthe size of the memory). The key principle of the Compressive Transformer is to compress these old\nmemories, instead of discarding them, and store them in an additional compressed memory.\n3.1 D ESCRIPTION\nWe deﬁne nm and ncm to be the number of respective memory and compressive memory slots in the\nmodel per layer. The overall input sequenceS= x1,x2,...,x |s|represents input observations (e.g.\ntokens from a book). These are split into ﬁxed-size windows of size ns for the model to process in\nparallel. The model observes x = xt,...,x t+ns at time t, which we refer to as thesequence (e.g. in\nFigure 1). As the model moves to the next sequence, itsns hidden activations are pushed into a ﬁxed-\nsized FIFO memory (like the TransformerXL). The oldestns activations in memory are evicted, but\nunlike the TransformerXL we do not discard them. Instead we apply a compression operation,\nfc : Rns×d →R⌊ns\nc ⌋×d, mapping the ns oldest memories to ⌊ns\nc ⌋compressed memories which we\nthen store in a secondary FIFO compressed memory. ddenotes the hidden size of activations and c\nrefers to the compression rate, a higher value indicates more coarse-grained compressed memories.\nThe full architecture is described in Algorithm 1.\nAlgorithm 1Compressive Transformer\nAt time zero\n1: m0 ←0 // Initialize memory to zeros (l×nm ×d)\n2: cm0 ←0 // Initialize compressed memory to zeros (l×ncm ×d)\nAt time t\n3: h(1) ←xWemb // Embed input sequence(ns ×d)\n4: for layer i= 1,2,...,l do\n5: mem(i) ←concat(cm(i)\nt ,m(i)\nt ) // ((ncm + nm) ×d)\n6: ˜ a(i) ←multihead attention(i)(h(i),mem(i)\nt ) // MHA over both mem types ( ns ×d)\n7: a(i) ←layer norm(˜ a(i) + h(i)) // Regular skip + layernorm (ncm ×d)\n8: old mem(i) ←m(i)\nt [: ns] // Oldest memories to be forgotten (ns ×d)\n9: new cm(i) ←f(i)\nc (old mem(i)) // Compress oldest memories by factor c(⌊ns\nc ⌋×d)\n10: m(i)\nt+1 ←concat(m(i)\nt ,h(i))[−nm :] // Update memory (nm ×d)\n11: cm(i)\nt ←concat(cm(i)\nt ,new cm(i))[−ncm :] // Update compressed memory (ncm ×d)\n12: h(i+1) ←layer norm(mlp(i)(a(i)) +a(i)) // Mixing MLP ( ns ×d)\n3\nAlgorithm 2Attention-Reconstruction Loss\n1: Lattn ←0\n2: for layer i= 1,2,...,l do\n3: h(i) ←stop gradient(h(i)) // Stop compression grads from passing...\n4: old mem(i) ←stop gradient(old mem(i)) // ...into transformer network.\n5: Q,K,V ←stop gradient(attention params at layer i) // Re-use attention weight matrices.\n6: def attn(h,m) ←σ((hQ) (mK))(mV) // Use content-based attention (no relative).\n7: new cm(i) ←f(i)\nc (old mem(i)) // Compression network (to be optimized).\n8: Lattn ←Lattn + ||attn(h(i),old mem(i)) −attn(h(i),new cm(i))||2\n3.2 C OMPRESSION FUNCTIONS AND LOSSES\nFor choices of compression functions fc we consider (1) max/mean pooling, where the kernel and\nstride is set to the compression rate c; (2) 1D convolutionalso with kernel & stride set to c;\n(3) dilated convolutions; (4) most-usedwhere the memories are sorted by their average attention\n(usage) and the most-used are preserved. The pooling is used as a fast and simple baseline. Themost-\nused compression scheme is inspired from the garbage collection mechanism in the Differentiable\nNeural Computer (Graves et al., 2016) where low-usage memories are erased. The convolutional\ncompression functions contain parameters which require training.\nOne can train the compression network using gradients from the loss; however for very old memories\nthis requires backpropagating-through-time ( BPTT) over long unrolls. As such we also consider\nsome local auxiliary compression losses. We consider an auto-encoding loss where we reconstruct\nthe original memories from the compressed memories Lae = ||old mem(i) −g(new cm(i))||2,\nwhere g : R\nns\nc ×d →Rns×d is learned. This is a lossless compression objective — it attempts\nto retain all information in memory. We also consider an attention-reconstruction loss described\nin Algorithm 2 which reconstructs the content-based attention over memory, with content-based\nattention over the compressed memories. This is a lossy objective, as information that is no longer\nattended to can be discarded, and we found this worked best. We stop compression loss gradients\nfrom passing into the main network as this prevents learning. Instead the Transformer optimizes\nthe task objective and the compression network optimizes the compression objective conditioned on\ntask-relevant representations; there is no need to mix the losses with a tuning constant.\n3.3 T EMPORAL RANGE\nThe TransformerXL with a memory of size n has a maximum temporal range of l ×n with an\nattention cost of O(n2\ns + nsn) (see Dai et al. (2019) for a detailed discussion). The Compressive\nTransformer now has a maximum temporal range of l×(nm + c∗ncm) with an attention cost of\nO(n2\ns + ns(nm + ncm)). For example, setting ncm = nm = n/2 and c= 3we obtain a maximum\ntemporal range that is two times greater than the TransformerXL with an identical attention cost.\nThus if we can learn in the c > 1 compressed setting, the temporal range of the model can be\nsigniﬁcantly increased.\n4 PG-19 B ENCHMARK\nAs models begin to incorporate longer-range memories, it is important to train and benchmark them\non data containing larger contexts. Natural language in the form of text provides us with a vast\nrepository of data containing long-range dependencies, that is easily accessible. We propose a new\nlanguage modelling benchmark, PG-19, using text from books extracted from Project Gutenberg 2.\nWe select Project Gutenberg books which were published over 100 years old, i.e. before 1919\n(hence the name PG-19) to avoid complications with international copyright, and remove short texts.\nThe dataset contains 28,752 books, or 11GB of text — which makes it over double the size of\nBookCorpus and Billion Word Benchmark.\n2The authors intend to release the PG-19 dataset along with the split into train, validation and test subsets.\n4\nTable 1: Comparison to existing popular language modelling benchmarks.\nAvg. length (words) Train Size Vocab Type\n1B Word 27 4.15GB 793K News (sentences)\nPenn Treebank 355 5.1MB 10K News (articles)\nWikiText-103 3.6K 515MB 267K Wikipedia (articles)\nPG-19 69K 10.9GB (open) Books\n4.1 R ELATED DATASETS\nThe two most benchmarked word-level language modelling datasets either stress the modelling of\nstand-alone sentences (Billion Word Benchmark from Chelba et al. (2013)) or the modelling of a\nsmall selection of short news articles (Penn Treebank processed by Mikolov et al. (2010)). Merity\net al. (2016) proposed the WikiText-103 dataset, which contains text from a high quality subset of\nEnglish-language wikipedia articles. These articles are on average 3,600 words long. This dataset\nhas been a popular recent LM benchmark due to the potential to exploit longer-range dependencies\n(Grave et al., 2016; Rae et al., 2018; Bai et al., 2018b). However recent Transformer models, such\nas the TransformerXL (Dai et al., 2019) appear to be able to exploit temporal dependencies on the\norder of several thousand words. This motivates a larger dataset with longer contexts.\nBooks are a natural choice of long-form text, and provide us with stylistically rich and varied natural\nlanguage. Texts extracted from books have been used for prior NLP benchmarks; such as the Chil-\ndren’s Book Test (Hill et al., 2015) and LAMBADA (Paperno et al., 2016). These benchmarks use\ntext from Project Gutenberg, an online repository of books with expired US copyright, and Book-\nCorpus (Zhu et al., 2015), a prior dataset of 11K unpublished (at time of authorship) books. CBT\nand LAMBADA contain extracts from books, with a speciﬁc task of predicting held-out words. In\nthe case of LAMBADA the held-out word is speciﬁcally designed to be predictable for humans with\naccess to the full textual context — but difﬁcult to guess with only a local context.\nCBT and LAMBADA are useful for probing the linguistic intelligence of models, but are not ideal\nfor training long-range language models from scratch as they truncate text extracts to at most a\ncouple of paragraphs, and discard a lot of the books’ text. There has been prior work on training\nmodels on book data using BookCorpus directly (e.g. BERT from Devlin et al. (2018)) however\nBookCorpus is no longer distributed due to licensing issues, and the source of data is dynamically\nchanging — which makes exact benchmarking difﬁcult over time.\nThe NarrativeQA Book Comprehension Task (Ko ˇcisk`y et al., 2018) uses Project Gutenberg texts\npaired with Wikipedia articles, which can be used as summaries. Due to the requirement of needing\na corresponding summary, NarrativeQA contains a smaller selection of books: 1,527 versus the\n28,752 books in PG-19. However it is reasonable that PG-19 may be useful for pre-training book\nsummarisation models.\n4.2 S TATISTICS\nA brief comparison of PG-19 to other LM datasets can be found in Table 1. We intentionally do not\nlimit the vocabulary by unk-ing rare words, and release the dataset as an open-vocabulary bench-\nmark. To compare models we propose to continue measuring the word-level perplexity. This can\nstill be computed for any chosen character-based, byte-based or subword-based scheme. To do this,\none calculates the total cross-entropy loss L = −∑\nt log(pt|p<t) over the given validation or test\nsubset using a chosen tokenization scheme, and then one normalizes this value by the number of\nwords: L/nwords where nwords is the total number of words in the given subset, taken from Table\n2. The word-level perplexity is thus eL/nwords . For sake of model comparisons, it is important to\nuse the exact number of words computed in Table 2 as the normalisation constant.\nAlongside quantitative analyses, we build an LDA topic model (Blei et al., 2003) for a qualitative\ninspection of the text. We present key words for several topics in the Supplementary Table 10. These\ntopics include art, education, naval exploration, geographical description, war, ancient civilisations,\nand more poetic topics concerning the human condition — love, society, religion, virtue etc. This\ncontrasts to the more objective domains of Wikipedia and news corpora.\n5\nTable 2: PG-19 statistics split by subsets.\nTrain Valid. Test\n# books 28,602 50 100\n# words 1,973,136,207 3,007,061 6,966,499\nTable 3: Eval. perplexities on PG-19.\nValid. Test\n36L TransformerXL 45.5 36.3\n36L Compressive Transf. 43.4 33.6\nTable 4: State-of-the-art results on Enwik8.\nModel BPC\n7L LSTM (Graves, 2013) 1.67\nLN HyperNetworks Ha et al. (2016) 1.34\nLN HM-LSTM Chung et al. (2016) 1.32\nByteNet (Kalchbrenner et al., 2016) 1.31\nRHN Zilly et al. (2017) 1.27\nmLSTM Krause et al. (2016) 1.24\n64L Transf. Al-Rfou et al. (2019) 1.06\n24L TXL (Dai et al., 2019) 0.99\nSparse Transf. (Child et al., 2019) 0.991\nAdaptive Transf. (Sukhbaatar et al., 2019) 0.98\n24L TXL (ours) 0.98\n24L Compressive Transformer 0.97\nTable 5: Compression approaches on Enwik8.\nCompression fn Compression loss BPC\nConv BPTT 0.996\nMax Pooling N/A 0.986\nConv Auto-encoding 0.984\nMean Pooling N/A 0.982\nMost-used N/A 0.980\nDilated conv Attention 0.977\nConv Attention 0.973\n5 E XPERIMENTS\nWe optimised all models with Adam (Kingma and Ba, 2014). We used a learning rate schedule\nwith a linear warmup from 1e-6 to 3e-4 and a cosine decay back down to 1e-n6. For character-\nbased LM we used 4,000 warmup steps with 100,000 decay steps, and for word-based LM we used\n16,000 warmup steps with 500,000 decay steps. We found that decreasing the optimisation update\nfrequency helped (see Section 5.5.1), namely we only applied parameter updates every 4 steps after\n60,000 iterations. However we found the models would optimise well for a range of warmup/warm-\ndown values. We clipped the gradients to have a norm of at most0.1, which was crucial to successful\noptimisation.\n5.1 PG-19\nWe benchmark the Compressive Transformer against the TransformerXL on the newly proposed PG-\n19 books dataset. Because it is open-vocabulary, we train a subword vocabulary of size 32000 with\nSubwordTextEncoder from the tfds package in TensorFlow and use the dataset statistics to compute\nword-level perplexity, as described in Section 4.2. We train a 36 layer Compressive Transformer with\na window size of512, both memory and compressed memory size of512, and compression rateC =\n2. We compare this to a 36 layer TransformerXL trained with window size512 and attention window\n1024. The model was trained on 256 TPUv3 cores with a total batch size of512 and converged after\nprocessing around 100 billion subword tokens. We display the results in Table 3 where we see the\nCompressive Transformer obtains a test perplexity of33.6 versus the TransformerXL’s36.3. Despite\nthe dataset size, it is clearly a challenging domain. This can suit as a ﬁrst baseline on the proposed\nlong-range language modelling benchmark. We show samples from this model in Supplementary\nSection E. The model is able to generate long-form narrative of varying styles: from character\ndialogue, ﬁrst person diary entries, to descriptive third-person text.\n5.2 E NWIK 8\nWe compare the TransformerXL and the Compressive Transformer on the standard character-level\nlanguage modelling benchmark Enwiki8 taken from the Hutter Prize (Hutter, 2012), which contains\n100M bytes of unprocessed Wikipedia text. We select the ﬁrst 90MB for training, 5MB for valida-\ntion, and the latter 5MB for testing — as per convention. We train 24-layer models with a sequence\nwindow size of 768. During training, we set the TransformerXL’s memory size to 2304, and for\nthe Compressive Transformer we use memory of size 768 and compressed memory of size 1152\n6\nwith compression rate C = 3. During evaluation, we increased the TransformerXL memory size\nto 4096 and the compressed memory in our model to 3072 (after sweeping over the validation set),\nobtaining the numbers reported in Table 4. We show the effect of scaling the compressed memory\nsize and evaluation performance in Supplementary Section B. The proposed model achieves the new\nstate-of-the-art on this dataset with 0.97 bits-per-character.\nWe compare compression functions and the use of auxiliary losses in Table 5. We sweep over\ncompression rates of 2, 3, and 4 and report results with the best performing value for each row.\nBPTT signiﬁes that no auxiliary compression loss was used to train the network other than the\noverall training loss. To feed gradients into the compression function we unrolled the model over\ndouble the sequence length and halved the batch size to ﬁt the larger unroll into memory.\n5.3 W IKITEXT -103\nWe train an eighteen-layered Compressive Transformer on the closed-vocabulary word-level lan-\nguage modelling benchmark WikiText-103, which contains articles from Wikipedia. We train the\nmodel with a compressed memory size, memory size, and a sequence window size all equal to 512.\nWe trained the model over 64 Tensor Processing Units (TPU) v3 with a batch size of 2 per core —\nmaking for a total batch size of 128. The model converged in a little over 12 hours. We found the\nsingle-layer convolution worked best, with a compression rate of c = 4. This model obtained 17.6\nperplexity on the test set. By tuning the memory size over the validation set — setting the memory\nsize to 500, and compressed memory size to 1,500 — we obtain 17.1 perplexity. This is 1.2 per-\nplexity points over prior state of the art, and means the model places a ≈5% higher probability on\nthe correct word over the prior SotA TransformerXL.\nIt is worth noting that in Table 6 we do not list methods that use additional training data, or that make\nuse of test-time labels to continue training the model on the test set (known as dynamic evaluation\n(Graves, 2013)). If we incorporate a very naive dynamic evaluation approach of loading a model\ncheckpoint and continuing training over one epoch of the test set, then we obtain a test perplexity\nof 16.1. This is slightly better than the published 16.4 from Krause et al. (2019) — which uses a\nmore sophisticated dynamic evaluation approach on top of the TransformerXL. However in most\nsettings, one does not have access to test-time labels — and thus we do not focus on this setting.\nFurthermore there has been great progress in showing that more data equates to much better lan-\nguage modelling; Shoeybi et al. (2019) ﬁnd a large transformer 8B-parameter transformer trained\non 170GB of text obtains 10.7 word-level perplexity on WikiText-103. However it is not clear to\nwhat extent the WikiText-103 test set may be leaked inside these larger training corpora. For clarity\nof model comparisons, we compare to published results trained on the WikiText-103 training set.\nCertainly the direction of larger scale and more data appear to bring immediate gains to the quality\nof existing language models. Both data scale and quality alongside intelligent model design are\ncomplementary lines of research towards better sequence modelling.\nWe break perplexity down by word frequency in Table 7 and see the Compressive Trans-\nformer makes only a small modelling improvement for frequent words ( 2.6% over the Trans-\nformerXL baseline) but obtains a much larger improvement of ≈20% for infrequent words. Fur-\nthermore, we see 10X improvement in modelling rare words over the prior state-of-the-art LSTM\nlanguage model published in 2018 — which demonstrates the rate of progress in this area.\n5.4 C OMPRESSIBILITY OF LAYERS\nWe can use compression to better understand the model’s mode of operation. We inspect how\ncompressible Transformer’s activations are as they progress through higher layers in the network.\nOne may expect representations to become more difﬁcult to compress at higher layers, if more\nsemantic information is represented there. We monitor the compression loss at each layer of our\nbest-performing Compressive Transformer models trained on Enwik8 and WikiText-103 and display\nthese in Supplementary Section A Figure 6. We note that the compression loss is about one order of\nmagnitude higher for word-level language modelling (WikiText-103) over character-level langauge\nmodelling (Enwik8). Furthermore the ﬁrst layer of the Transformer is highly compressible. However\nthere is not a clear trend of compression cost increasing with layer depth.\n7\nTable 6: Validation and test perplexities on WikiText-103.\nValid. Test\nLSTM (Graves et al., 2014) - 48.7\nTemporal CNN (Bai et al., 2018a) - 45.2\nGCNN-14 (Dauphin et al., 2016) - 37.2\nQuasi-RNN Bradbury et al. (2016) 32 33\nRMC (Santoro et al., 2018) 30.8 31.9\nLSTM+Hebb. (Rae et al., 2018) 29.0 29.2\nTransformer (Baevski and Auli, 2019) - 18.7\n18L TransformerXL, M=384 (Dai et al., 2019) - 18.3\n18L TransformerXL, M=1024 (ours) - 18.1\n18L Compressive Transformer, M=1024 16.0 17.1\nTable 7: WikiText-103 test perplexity broken down by word frequency buckets. The most frequent\nbucket is words which appear in the training set more than 10,000 times, displayed on the left. For\nreference, a uniform model would have perplexity |V|= 2.6e5 for all frequency buckets. *LSTM\ncomparison from Rae et al. (2018)\n> 10K 1K−10K 100 −1K < 100 All\nLSTM* 12.1 219 1,197 9,725 36.4\nTransformerXL (ours) 7.8 61.2 188 1,123 18.1\nCompressive Transformer 7.6 55.9 158 937 17.1\nRelative gain over TXL 2.6% 9.5% 21% 19.9% 5.8%\n5.5 A TTENTION\nWe inspect where the network is attending to on average, to determine whether it is using its com-\npressed memory. We average the attention weight over a sample of20,000 sequences from a trained\nmodel on Enwik8. We aggregate the attention into eighteen buckets, six for each of the compressed\nmemory, memory, and sequence respectively. We set the size of the sequence, memory and com-\npressed memory all to be 768. We plot this average attention weight per bucket in Figure 2 with a\n1σ standard error. We see most of the attention is placed on the current sequence; with a greater\nweight placed on earlier elements of the sequence due to the causal self-attention mechanism which\nmasks future attention weights. We also observe there is an increase in attention from the oldest\nactivations stored in the regular memory, to the activations stored in the compressed memory. This\ngoes against the trend of older memories being accessed less frequently — and gives evidence\nthat the network is learning to preserve salient information.\n5.5.1 O PTIMISATION SCHEDULE\nWe make an observation about an interesting but undesirable meta-learning phenomenon during\nlong-context training. When the learning rate is tuned to be much smaller (or set to zero) during\ntraining, performance degrades drastically both for the TransformerXL and the Compressive Trans-\nformer. This is displayed in Figure 3.\nUsually we consider distributional shift from the training data to the test data, but we can also\nobserve a shift in the model when transferring from a training to evaluation mode (even when the\nmodel is evaluated on the training data). In this case, this is due to the online updating of parameters\nwhilst processing long contiguous articles. We would like the model to generalise well to scenarios\nwhere it is not continuously optimised. Updating the parameters only at article boundaries (and then\nresetting the state) could be one solution for long-range memory models, but this would slow down\nlearning signiﬁcantly.\nInstead, we propose reducing the frequency of optimisation updates during training. We ﬁnd this\nallows for the best of both worlds — fast initial learning with frequent updates, and better gen-\neralisation near the end of training with less frequent updates (e.g. every 4 steps). Reducing the\noptimisation frequency increases the effective batch size, which has also been shown to be prefer-\n8\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\nCompressed Memory           Memory                 Sequence           \n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14Average attention weight\nFigure 2: Attention weight on Enwik8. Av-\nerage attention weight from the sequence over\nthe compressed memory (oldest), memory, and\nsequence (newest) respectively. The sequence\nself-attention is causally masked, so more at-\ntention is placed on earlier elements in the se-\nquence. There is an increase in attention at the\ntransition from memory to compressed memory.\n5000 10000 15000 20000 25000\nTraining iterations\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2Training BPC\nChange learning rate\nduring training\nLearning Rate\n0.0\n1e-9\n1e-7\n3e-4\n3e-4 update period=2\nFigure 3: Learning rate analysis. Reducing the\nlearning rate (e.g. to zero) during training (on\nEnwik8) harms training performance. Reduc-\ning the frequency of optimisation updates (ef-\nfectively increasing the batch size) is preferable.\nable to learning rate decay in image modelling (Smith et al., 2018). We observed a ﬁnal performance\nimprovement in our TransformerXL baseline on Enwik8, from0.995 — which approximately repli-\ncates the published result — to 0.984 — which matches the most recent SotA architecture. We note,\nthe additional space and compute cost of accumulating gradients is negligible across iterations, so\nthere was no performance regression in using this scheme.\n5.6 S PEECH\nWe train the Compressive Transformer on the waveform of speech to assess its performance on\ndifferent modalities. Speech is interesting because it is sampled at an incredibly high frequency, but\nwe know it contains a lot of information on the level of phonemes and entire phrases.\nTo encourage long-term reasoning, we refrain from conditioning the model on speaker identity or\ntext features, but focus on unconditional speech modelling. We train the model on 24.6 hours of\n24kHz North American speech data. We chunk the sequences into windows of size 3840, roughly\n80ms of audio, and compare a 20-layer Compressive Transformer to a 20-layer TransformerXL\nand a 30-layer WaveNet model (Oord et al., 2016) — a state-of-the-art audio generative model\nused to serve production speech synthesis applications at Google (Oord et al., 2018). All networks\nhave approximately 40M parameters, as WaveNet is more parameter-efﬁcient per layer. We train\neach network with 32 V100 GPUs, and a batch size of 1 per core (total batch size of 32) using\nsynchronous training.\nWaveNet processes an entire chunk in parallel, however the TransformerXL and Compressive Trans-\nformer are trained with a window size of 768 and a total memory size of 1,568 (for the Compres-\nsive Transformer we use 768 memory + 768 compressed). We thus unroll the model over the se-\nquence. Despite this sequential unroll, the attention-based models train at only half the speed of\nWaveNet. We see the test-set negative-log-likelihood in Figure 4, and observe that a Compressive\nTransformer with a compression rate of 4 is able to outperform the TransformerXL and maintain\na slim advantage over WaveNet. However we only trained models for at most one week (with\n32GPUs) and it would be advantageous to continue training until full convergence — before deﬁni-\ntive conclusions are made.\n5.7 R EINFORCEMENT LEARNING\nCompression is a good ﬁt for video input sequences because subsequent frames have high mutual\ninformation. Here we do not test out the Compressive Transformer on video, but progress straight to\na reinforcement learning agent task that receives a video stream of visual observations — but must\nultimately learn to use its memory to reason over a policy.\n9\n0 50000 100000 150000 200000 250000 300000 350000 400000\nTraining Iterations\n1.80\n1.82\n1.84\n1.86\n1.88Test NLL\nCompressive Transformer 20L C=4\nTransformerXL 20L\nWavenet 30L\nFigure 4: Speech Modelling.We see the Com-\npressive Transformer is able to obtain competi-\ntive results against the state-of-the-art WaveNet\nin the modelling of raw speech sampled at\n24kHz.\n0.0 0.2 0.4 0.6 0.8 1.0\nFrames 1e9\n0\n20\n40\n60\n80\n100Human Normalised Score\nCompression Rate\n1\n2\n4\n8\nFigure 5: Vision and RL. We see the Com-\npressive Transformer integrates visual informa-\ntion across time within an IMPALA RL agent,\ntrained on an object matching task.\nWe test the Compressive Transformer as a drop-in replacement to an LSTM in the IMPALA setup\n(Espeholt et al., 2018). Otherwise, we use the same training framework and agent architecture as\ndescribed in the original work with a ﬁxed learning rate of 1.5e-5 and entropy cost coefﬁcient of\n2e-3. We test the Compressive Transformer on a challenging memory task within the DMLab-30\n(Beattie et al., 2016) domain, rooms select nonmatching object. This requires the agent to explore\na room in a visually rich 3D environment and remember the object present. The agent can then\nadvance to a second room where it must select the object not present in the original room. This\nnecessitates that the agent both remember events far in the past, and also learn to efﬁciently reason\nabout them.\nWe ﬁx both the memory and compressed memory sizes to 64. In Figure 5, we present results for a\nrange of compression rates, averaged over 3 seeds. We see that the best performing agents endowed\nwith the Compressive Transformer are able to solve the task to human-level. We note that the model\nwith compression rate 1 is unable to learn the task to the same proﬁciency. The speed of learning\nand stability seem to increase proportionally with higher rates of compression (up to a limit) – i.e.\nthe effective memory window of the agent – and we ﬁnd compression rate 4 to once again be the\nbest performing. We see this as a promising sign that the architecture is able to efﬁciently learn,\nand suitably use, compressed representations of its visual input and hope to test this more widely in\nfuture work.\n6 C ONCLUSION\nIn this paper we explore the notion of compression as a means of extending the temporal receptive\nﬁeld of Transformer-based sequence models. We see a beneﬁt to this approach in the domain of\ntext, with the Compressive Transformer outperforming existing architectures at long-range language\nmodelling. To continue innovation in this area, we also propose a new book-level LM benchmark,\nPG-19. This may be used to compare long-range language models, or to pre-train on other long-\nrange reasoning language tasks, such as NarrativeQA (Koˇcisk`y et al., 2018).\nWe see the idea of compressive memories is applicable not only to the modality of text, but also\naudio, in the form of modelling the waveform of speech, and vision, within a reinforcement-learning\nagent trained on a maze-like memory task. In both cases, we compare to very strong baselines\n(Wavenet (Oord et al., 2016) and IMPALA (Espeholt et al., 2018)).\nThe main limitation of this work is additional complexity, if the task one wishes to solve does not\ncontain long-range reasoning then the Compressive Transformer is unlikely to provide additional\nbeneﬁt. However as a means of scaling memory and attention, we do think compression is a simpler\napproach to dynamic or sparse attention — which often requires custom kernels to make efﬁcient.\nOne can build effective compression modules from simple neural network components, such as\nconvolutions. The compression components are immediately efﬁcient to run on GPUs and TPUs.\nMemory systems for neural networks began as compressed state representations within RNNs. The\nrecent wave of progress using attention-based models with deep and granular memories shows us\n10\nthat it is beneﬁcial to refrain from immediately compressing the past. However we hypothesise that\nmore powerful models will contain a mixture of granular recent memories and coarser compressed\nmemories. Future directions could include the investigation of adaptive compression rates by layer,\nthe use of long-range shallow memory layers together with deep short-range memory, and even the\nuse of RNNs as compressors. Compressive memories should not be forgotten about just yet.\nACKNOWLEDGEMENTS\nWe thank Chris Dyer, Felix Gimeno, and Koray Kavukcuoglu for reviewing the manuscript. We\nthank Peter Dayan, Adam Santoro, Jacob Menick, Emilio Parisotto, Hyunjik Kim, Simon Osin-\ndero, Sergey Bartunov, David Raposo, and Daan Wierstra for ideas regarding model design. We\nthank Yazhe Li and Aaron Van de Oord for their help and advice in instrumenting speech modelling\nexperiments. Finally, we thank our wider DeepMind colleagues for supporting this project with\nstimulating discussions, engineering infrastructure, and positive reinforcement signals.\nAUTHOR CONTRIBUTIONS\nModel and Experiment design: JR, TL, AP, SJ\nDataset creation: AP, JR, CH\nText experiments: JR, AP\nRL experiments: SJ\nSpeech experiments: JR\nFUNDING\nThis research was funded by DeepMind.\nCOMPETING INTERESTS\nThe authors declare no competing ﬁnancial interests.\n11\nREFERENCES\nR. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones. Character-level language modeling with\ndeeper self-attention. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33,\npages 3159–3166, 2019.\nA. Baevski and M. Auli. Adaptive input representations for neural language modeling. arXiv\npreprint arXiv:1809.10853, 2019.\nD. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473, 2014.\nS. Bai, J. Z. Kolter, and V . Koltun. Convolutional sequence modeling revisited, 2018a. URL\nhttps://openreview.net/forum?id=rk8wKk-R-.\nS. Bai, J. Z. Kolter, and V . Koltun. Trellis networks for sequence modeling. arXiv preprint\narXiv:1810.06682, 2018b.\nC. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. K¨uttler, A. Lefrancq, S. Green,\nV . Vald´es, A. Sadik, J. Schrittwieser, K. Anderson, S. York, M. Cant, A. Cain, A. Bolton, S. Gaffney,\nH. King, D. Hassabis, S. Legg, and S. Petersen. Deepmind lab. CoRR, abs/1612.03801, 2016. URL\nhttp://arxiv.org/abs/1612.03801.\nD. M. Blei, A. Y . Ng, and M. I. Jordan. Latent dirichlet allocation.J. Mach. Learn. Res., 3:993–1022,\nMar. 2003. ISSN 1532-4435.\nJ. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. arXiv preprint\narXiv:1611.01576, 2016.\nC. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One bil-\nlion word benchmark for measuring progress in statistical language modeling. arXiv preprint\narXiv:1312.3005, 2013.\nR. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers.\narXiv preprint arXiv:1904.10509, 2019.\nJ. Chung, S. Ahn, and Y . Bengio. Hierarchical multiscale recurrent neural networks.arXiv preprint\narXiv:1609.01704, 2016.\nZ. Dai, Z. Yang, Y . Yang, W. W. Cohen, J. Carbonell, Q. V . Le, and R. Salakhutdinov. Transformer-\nxl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860 ,\n2019.\nY . N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional\nnetworks. arXiv preprint arXiv:1612.08083, 2016.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nL. Espeholt, H. Soyer, R. Munos, K. Simonyan, V . Mnih, T. Ward, Y . Doron, V . Firoiu, T. Harley,\nI. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner ar-\nchitectures. In International Conference on Machine Learning, pages 1406–1415, 2018.\nE. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache.\narXiv preprint arXiv:1612.04426, 2016.\nA. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,\n2013.\nA. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401,\n2014.\nA. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwi ´nska, S. G. Col-\nmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou, et al. Hybrid computing using a neural network\nwith dynamic external memory. Nature, 538(7626):471, 2016.\nD. Ha, A. Dai, and Q. V . Le. Hypernetworks.arXiv preprint arXiv:1609.09106, 2016.\nF. Hill, A. Bordes, S. Chopra, and J. Weston. The goldilocks principle: Reading children’s books\nwith explicit memory representations. arXiv preprint arXiv:1511.02301, 2015.\n12\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\nA. Holtzman, J. Buys, M. Forbes, and Y . Choi. The curious case of neural text degeneration. arXiv\npreprint arXiv:1904.09751, 2019.\nM. Hutter. The human knowledge compression contest. URL http://prize. hutter1. net, 6, 2012.\nN. Kalchbrenner, L. Espeholt, K. Simonyan, A. v. d. Oord, A. Graves, and K. Kavukcuoglu. Neural\nmachine translation in linear time. arXiv preprint arXiv:1610.10099, 2016.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nT. Koˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis, and E. Grefenstette. The\nnarrativeqa reading comprehension challenge. Transactions of the Association for Computational\nLinguistics, 6:317–328, 2018.\nB. Krause, L. Lu, I. Murray, and S. Renals. Multiplicative lstm for sequence modelling. arXiv\npreprint arXiv:1609.07959, 2016.\nB. Krause, E. Kahembwe, I. Murray, and S. Renals. Dynamic evaluation of transformer language\nmodels. CoRR, abs/1904.08378, 2019. URL http://arxiv.org/abs/1904.08378.\nG. Lample, A. Sablayrolles, M. Ranzato, L. Denoyer, and H. J ´egou. Large memory layers with\nproduct keys. arXiv preprint arXiv:1907.05242, 2019.\nS. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. arXiv preprint\narXiv:1609.07843, 2016.\nT. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and S. Khudanpur. Recurrent neural network\nbased language model. In Eleventh Annual Conference of the International Speech Communication\nAssociation, 2010.\nA. Oord, Y . Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu, G. Driessche, E. Lockhart,\nL. Cobo, F. Stimberg, et al. Parallel wavenet: Fast high-ﬁdelity speech synthesis. In International\nConference on Machine Learning, pages 3915–3923, 2018.\nA. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,\nand K. Kavukcuoglu. Wavenet: A generative model for raw audio.arXiv preprint arXiv:1609.03499,\n2016.\nD. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,\nR. Fern ´andez, K. Erk, et al. The lambada dataset: Word prediction requiring a broad discourse\ncontext. Association for Computational Linguistics, 2016.\nJ. Rae, J. J. Hunt, I. Danihelka, T. Harley, A. W. Senior, G. Wayne, A. Graves, and T. Lillicrap.\nScaling memory-augmented neural networks with sparse reads and writes. In Advances in Neural\nInformation Processing Systems, pages 3621–3629, 2016.\nJ. W. Rae, C. Dyer, P. Dayan, and T. P. Lillicrap. Fast parametric learning with activation memo-\nrization. arXiv preprint arXiv:1803.10049, 2018.\nB. A. Richards and P. W. Frankland. The persistence and transience of memory. Neuron, 94(6):\n1071–1084, 2017.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating\nerrors. Nature, 323(6088):533, 1986.\nA. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, D. Wierstra, O. Vinyals,\nR. Pascanu, and T. Lillicrap. Relational recurrent neural networks. In Advances in Neural Informa-\ntion Processing Systems, pages 7299–7310, 2018.\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training\nmulti-billion parameter language models using model parallelism, 2019.\nS. Smith, P. jan Kindermans, C. Ying, and Q. V . Le. Don’t decay the learning rate, increase the batch\nsize. 2018. URL https://openreview.net/pdf?id=B1Yy1BxCZ.\n13\nS. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin. Adaptive attention span in transformers.\narXiv preprint arXiv:1905.07799, 2019.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In Advances in neural information processing systems , pages\n5998–6008, 2017.\nF. Wu, A. Fan, A. Baevski, Y . N. Dauphin, and M. Auli. Pay less attention with lightweight and\ndynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\nZ. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized autore-\ngressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\nL. Zhou, Y . Zhou, J. J. Corso, R. Socher, and C. Xiong. End-to-end dense video captioning with\nmasked transformer. InProceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 8739–8748, 2018.\nY . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\nbooks and movies: Towards story-like visual explanations by watching movies and reading books.\nIn Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015.\nJ. G. Zilly, R. K. Srivastava, J. Koutn ´ık, and J. Schmidhuber. Recurrent highway networks. In\nProceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 4189–\n4198. JMLR. org, 2017.\n14\nSUPPLEMENTARY MATERIALS\nA C OMPRESSION ACROSS LAYERS\nWe inspect the compression loss broken down by the layer index, to investigate whether there is\na trend in network depth with how compressible the representations are. The compression loss\nhere refers to the attention-reconstruction attention loss. We plot this for a 24 layer trained model\non Enwik8, and an 18 layer model trained on WikiText-103. The compression loss for character-\nbased language modelling is about one order of magnitude lower than that of word-level language\nmodelling. The ﬁrst layer’s representations are highly compressible, however from then on there is\nno ﬁxed trend. Some non-contiguous layers have a very similar compression loss (e.g. 4 & 6, 5 &\n7) which suggests information is being routed from these layer pairs via the skip connection.\n1 2 3 4 5 6 7 8 9 101112131415161718192021222324\nLayer\n0\n4e-4\n8e-4\n1.2e-3\n1.6e-3Compression Loss\nEnwik8\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\nLayer\n0\n4e-3\n8e-3\n1.2e-2\n1.6e-2\nWikiText-103\nFigure 6: Model analysis.Compression loss broken down by layer.\nB C OMPARISON OF COMPRESSED MEMORY SIZES\nWe compare the best test perplexity obtained for the Compressive Transformer trained on WikiText-\n103 and Enwik8 across a range of compressed memory sizes. For both models, the best model used\na 1D convolution compression network with a compression rate of3. The Enwik8 model was trained\nwith an embedding size of1024, 8 attention heads, 24 layers, an mlp hidden size of3072, a sequence\nwindow size of 768, and a memory size of 768. We see the best compressed memory size is 3,072\nin this sweep, facilitating a total attention window of 3840. The WikiText-103 model was trained\nwith an embedding size of 1024, adaptive inputs using the same parameters as (Sukhbaatar et al.,\n2019), 16 attention heads, 18 layers, an mlp hidden size of 4096, a sequence window of size 512\nand a memory of size 512. The best compressed memory size is 1536 resulting in a total attention\nwindow of c. 2048.\nCompressed Memory Size 512 1024 2048 3072 4096\nEnwik8 BPC 1.01 0.99 0.98 0.97 1.00\nTable 8: Compressed memory size vs test performance for Enwik8\nCompressed Memory Size 256 512 1024 1536 2048\nWikiText-103 Perplexity 18.2 17.9 17.6 17.1 17.7\nTable 9: Compressed memory size vs test performance for WikiText-103\nC PG-19 P REPROCESSING\nThe raw texts from the Gutenberg project were minimally pre-processed by removing boilerplate\nlicense text. We then also replaced discriminatory words with a unique ⟨DWx⟩token using the\nOfcom list of discriminatory words 3.\n3https://www.ofcom.org.uk/__data/assets/pdf_file/0023/91625/\nOfcomQRG-AOC.pdf\n15\nD PG-19 T OPICS\nWe present top-words for some of the topics on the PG-19 corpus. These were generated with LDA\ntopic model (Blei et al., 2003).\nTable 10: Examples of top topics on PG-19 corpus.\nGeography War Civilisations Human Condition Naval Education Art\nwater people roman love island work poet\nriver emperor rome religion ship school music\nfeet war greek religious sea life one\nmiles army city life men children poetry\nnorth death gods moral captain may work\nsouth battle king human coast social literature\nmountains city ﬁrst society land child art\nsea soldiers caesar man great education great\nlake power great virtue found conditions poem\nrock thousand romans nature islands well written\nmountain arms athens marriage shore study english\ncountry empire greece women voyage best author\nvalley upon temple christian vessels years play\nice country son age time possible genius\nwest time egypt law english class style\nE PG-19 S AMPLES\nWe show a few different samples from the Compressive Transformer trained on PG-19. We use\nNucleus Sampling with p = 0.98 (Holtzman et al., 2019). We choose extracts of books from the\ntest set as preﬁxes. We see the model is able to continue in the style of the text, creating artiﬁcial\ndialogue or descriptive text, and remembering the names of characters over hundreds of words.\nAs the Compressive Transformer is trained without state resetting, it is actually slightly out of sample\nwhen provided with the (relatively) short contexts. This is because its memory and compressed\nmemory may be still empty (whereas they are always full during training). However we see a trend\nof the samples usually improving towards the end.\n16\nContext from Baby Mine by Maragaret Mayo:\nCHAPTER I\nEven in college Alfred Hardy was a young man of ﬁxed ideas and high ideals and proud of it.\nHis friend, Jimmy Jinks, had few ideas and no ideals, and was glad of it, and before half of their\nﬁrst college term had passed, Jimmy had ridded himself of all such worries as making up his\nown mind or directing his own morals. Alfred did all these things so much better, argued Jimmy,\nfurthermore, Alfred LIKED to do them–Jimmy owed it to his friend to give him that pleasure.\nCompressive Transformer:\nHe did not like doing them in the most elegant of all possible marble walks–where freshmen\nand sophomores stand and talk and laugh and chatter till evening falls and look cool and still\nand cool and beautifully white in spite of the hot sun, but had to go there for a month before\ngoing home. It happened one day that he and Jimmy were strolling together in the green shade\nof St. Martin’s, and Jimmy naturally asked why he was so silent. He was very talkative as\nmost boys are who have once got the habit of it on shore, so the need for answering was soon\nsupplied.\n”I am wondering,” he said, ”why my old friend and chum, Jimmy Jinks, is living a single life\nand so has no boys. Have you not noticed that many of his old friends had their own boys to\nfollow in their footsteps and help, and often helped successfully too?”\n”Oh, yes,” said Alfred, ”it was plain enough then that he liked to live by himself and got none\nof his present pleasures or entertainments because of the other fellow’s giving it to him.”\n”Then it IS sad,” said Jimmy, ”that an old friend like the friend of Big Lights, Jimmy Reynolds,\nshould be living the single life and having no boys.”\n”And it is pretty true, too,” said Alfred, ”yet I believe it is the best thing that could happen even\nnow.”\n”And so do I,” said Jimmy, ”and I also believe that it is time someone did something to make\nthe things worse for the other’s partner.”\n”And so do I,” said Alfred, ”and so do I, and so do I.”\n”What can we do?” asked Jimmy.\n”I don’t know,” said Alfred, ”but if someone will come down to the valley and go through a\nregular mill with the old forces at work, doing the work of the two, and doing it better than any\nother Miller he ever saw or ever hoped to see, I think there will be a good deal of change in the\nthings and things to be cared for for and for driven and counter-driven.”\n”I will come and do that mill,” said Jimmy.\n17\nContext from The Diary of Samuel Pepys:\nJune 1st. Having taken our leaves of Sir W. Batten and my Lady, who are gone this morn-\ning to keep their Whitsuntide, Sir W. Pen and I and Mr. Gauden by water to Woolwich, and\nthere went from ship to ship to give order for and take notice of their forwardness to go forth,\nand then to Deptford and did the like, having dined at Woolwich with Captain Poole at the\ntavern there. From Deptford we walked to Redriffe, calling at the half-way house, and there\ncome into a room where there was inﬁnite of new cakes placed that are made against Whit-\nsuntide, and there we were very merry. By water home, and there did businesses of the ofﬁce.\nAmong others got my Lord’s imprest of L1000 and Mr. Creed’s of L10,000 against this voyage\ntheir bills signed. Having wrote letters into the country and read some things I went to bed.\nCompressive Transformer:\nSo I rose and did some business at the ofﬁce, and then to dinner, where great abundance of\ngood things of the kitchen I only being partaker in the feast. After dinner I went to look at the\nyard and found it in a very bad condition, so I went home and there ﬁnding no delight in seeing\nthings I should make shift to get in my way and so I will leave off.\nSo I took leave of my great friend, and the next day being Sunday, my wife and I and a\ngreat number of friends went by water to Deptford, and there took the oaths of allegiance and\nsupremacy and promised such obedience to the Lord as should hereafter turn to the advantage\nof our State. Being come back we found the shops and warehouses in a pitiable condition, so\nmy wife and I and all the rest of the friends went home and there did the like and by Saturday\nday I being alone in my ofﬁce I found myself in a very bad situation, for my Lord, and Mr.\nCreed, and Pett were all standing ready to receive and receive the oaths of allegiance and right\nto make use of the ten days’ licence I had allowed for this purpose by virtue of the law that was\nsuch a help to the servant. Having looked at the ships, I told my wife what I had seen and that\nI was greatly troubled at my situation and, I hope that will mend it.\nSo that is the end of a long and patient seeking after those things which in my opinion I deserve\nand may get if I can only by fair means get them.\n18\nContext from The Patrol of the Sun Dance Trail by Ralph Connor:\nHigh up on the hillside in the midst of a rugged group of jack pines the Union Jack shook\nout its folds gallantly in the breeze that swept down the Kicking Horse Pass. That gallant ﬂag\nmarked the headquarters of Superintendent Strong, of the North West Mounted Police, whose\nspecial duty it was to preserve law and order along the construction line of the Canadian Paciﬁc\nRailway Company, now pushed west some scores of miles.\nAlong the tote-road, which ran parallel to the steel, a man, dark of skin, slight but wiry,\ncame running, his hard panting, his streaming face, his open mouth proclaiming his ex-\nhaustion. At a little trail that led to the left he paused, noted its course toward the ﬂaunt-\ning ﬂag, turned into it, then struggled up the rocky hillside till he came to the wooden\nshack, with a deep porch running round it, and surrounded by a rustic fence which en-\nclosed a garden whose neatness illustrated a characteristic of the British soldier. The run-\nner passed in through the gate and up the little gravel walk and began to ascend the steps.\nCompressive Transformer:\nAs though by mutual consent, all the three members of the Superintendent Force were standing\non the veranda watching his progress, a man came strolling slowly down the path, stopping\noften to gaze curiously about him, then stopping once more when he saw the four standing on\nthe veranda. His face was dark, his eyes prominent, and the lines that furrowed his forehead\nshowed marks of many woes, yet his step was alert and he began to mount the steps with steady\nstrides.\nThe man’s attention was diverted by the unexpected appearance of the two policemen; a third\nman came slipping up, grumbling under his breath, and stopped long enough to look doubtfully\naround him, then slipped away again. In the meantime, the four on the veranda continued to\nwatch the approach of the new arrival. In spite of himself, the Superintendent Force became\nconscious of a curious feeling–uneasiness that was almost fear–in the presence of that strange\nman. He would not allow himself to admit the fact, yet he felt it in his bones. But to the\nwatchers, the plain, everyday features of that stranger and his coming, seemed only just what\nthe Seven White Shee owed him–their weight, their hurry, their blast.\nHad a chariot been made for the good of the three horses, and had the driver been given orders\nthat he should speed them that he might win, they would have been heartening things in the\nsight of the veteran and the victor. To you they would have been unintelligible to the root of\nyour understanding. When you gaze up in the faces of those four gray horses, you can see\nclearly through the clouds of dust that rise from their hoofs, and discern plainly where the\nbanker is and where the hobo. Then you will understand why you shall not press the bitter\ngrapes and why you shall not spurn the generous doctrines. You will understand why you shall\nnot praise the lash or the spur, for you will know where the true would be and where the false\nwould be. Then you will understand why you, a man with reason and heart, need not tear your\nhair over-bitter and why you need not laugh over the blunders of an ignorant man.\nAbout nine o’clock that morning, two buggies, drawn by powerful horses, crossed the Rubicon\nand turned the railroad from Sandhurst into the Hollow of the Mountains. And though the char-\nioteers stood at their horses’ heads, and their drivers cried at their loudest, there was not a man\nin the four teams who did not feel that his day was worth all the toil and all the peril that he\nhad undergone. And if there were a man in them who did not know that–who did not feel that\nthe road through the Hollow of the Mountains is made easy by the arrival of travelers and by\nthe coming of government, there was one who did not at that moment care whether his day’s\nwork were worth all the toil and all the danger that he had had to endure or whether it were not\nworth more than all.\n19",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8318943977355957
    },
    {
      "name": "Computer science",
      "score": 0.6372523307800293
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.540073812007904
    },
    {
      "name": "Sequence learning",
      "score": 0.5326805710792542
    },
    {
      "name": "Vocabulary",
      "score": 0.4952322542667389
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4833618998527527
    },
    {
      "name": "Language model",
      "score": 0.4784415066242218
    },
    {
      "name": "Sequence (biology)",
      "score": 0.45956000685691833
    },
    {
      "name": "Sequence labeling",
      "score": 0.45034050941467285
    },
    {
      "name": "Speech recognition",
      "score": 0.3531891703605652
    },
    {
      "name": "Natural language processing",
      "score": 0.35068172216415405
    },
    {
      "name": "Task (project management)",
      "score": 0.3176325559616089
    },
    {
      "name": "Engineering",
      "score": 0.17320901155471802
    },
    {
      "name": "Electrical engineering",
      "score": 0.12658601999282837
    },
    {
      "name": "Voltage",
      "score": 0.0971095860004425
    },
    {
      "name": "Linguistics",
      "score": 0.07726436853408813
    },
    {
      "name": "Geology",
      "score": 0.06083884835243225
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 49
}