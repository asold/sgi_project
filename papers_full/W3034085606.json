{
  "title": "Language Models as Fact Checkers?",
  "url": "https://openalex.org/W3034085606",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2120619077",
      "name": "Nayeon Lee",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2161526898",
      "name": "Belinda Li",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2166097142",
      "name": "Sinong Wang",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A4259474567",
      "name": "Wen-tau Yih",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2097317532",
      "name": "Hao Ma",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A296516693",
      "name": "Madian Khabsa",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2955520273",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2971455676",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2964039914",
    "https://openalex.org/W2964068236",
    "https://openalex.org/W2789566302",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2890662817",
    "https://openalex.org/W2955753753",
    "https://openalex.org/W2980847631",
    "https://openalex.org/W2902421513",
    "https://openalex.org/W2793978524"
  ],
  "abstract": "Recent work has suggested that language models (LMs) store both common-sense and factual knowledge learned from pre-training data. In this paper, we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components. While previous work on extracting knowledge from LMs have focused on the task of open-domain question answering, to the best of our knowledge, this is the first work to examine the use of language models as fact checkers. In a closed-book setting, we show that our zero-shot LM approach outperforms a random baseline on the standard FEVER task, and that our fine-tuned LM compares favorably with standard baselines. Though we do not ultimately outperform methods which use explicit knowledge bases, we believe our exploration shows that this method is viable and has much room for exploration.",
  "full_text": "Proceedings of the Third Workshop on Fact Extraction and VERiﬁcation (FEVER), pages 36–41\nJuly 9, 2020.c⃝2020 Association for Computational Linguistics\n36\nLanguage Models as Fact Checkers?\nNayeon Lee1∗ Belinda Z. Li2 Sinong Wang2\nWen-Tau Yih2 Hao Ma2 Madian Khabsa2\n1Hong Kong University of Science and Technology 2Facebook AI\nnayeon.lee@connect.ust.hk\n{belindali,sinongwang,scottyih,haom,mkhabsa}@fb.com\nAbstract\nRecent work has suggested that language mod-\nels (LMs) store both common-sense and fac-\ntual knowledge learned from pre-training data.\nIn this paper, we leverage this implicit knowl-\nedge to create an effective end-to-end fact\nchecker using a solely a language model, with-\nout any external knowledge or explicit re-\ntrieval components. While previous work on\nextracting knowledge from LMs have focused\non the task of open-domain question answer-\ning, to the best of our knowledge, this is the\nﬁrst work to examine the use of language mod-\nels as fact checkers. In a closed-book set-\nting, we show that our zero-shot LM approach\noutperforms a random baseline on the stan-\ndard FEVER task, and that our ﬁnetuned LM\ncompares favorably with standard baselines.\nThough we do not ultimately outperform meth-\nods which use explicit knowledge bases, we\nbelieve our exploration shows that this method\nis viable and has much room for exploration.\n1 Introduction\nPre-trained language models have recently lead to\nsigniﬁcant advancements in wide variety of NLP\ntasks, including question-answering, commonsense\nreasoning, and semantic relatedness (Devlin et al.,\n2018; Radford et al., 2019; Peters et al., 2018;\nRadford et al., 2018). These models are typi-\ncally trained on documents mined from Wikipedia\n(among other websites). Recently, a number of\nworks have found that LMs store a surprising\namount of world knowledge, focusing particularly\non the task of open-domain question answering\n(Petroni et al., 2019; Roberts et al., 2020). In this\npaper, we explore whether we can leverage the\nknowledge in LMs for fact checking.\nWe propose an approach (Fig. 1b) that replaces\nthe document retriever and evidence selector mod-\nels in traditional fact-checking (Fig. 1a) with a\n∗Work done while at Facebook AI.\n(a) Traditional fact-checking\npipeline.\n(b) Our new fact-\nchecking pipeline.\nFigure 1: Traditional fact-checking pipeline (left) vs.\nOur LM-based pipeline (right)\nsingle language model that generates masked to-\nkens. This offers a number of advantages over the\ntraditional approach: ﬁrst, the procedure is over-\nall simpler, requiring fewer resources and compu-\ntation – we do not need to maintain an explicit\nknowledge base external to our LM, and we do\nnot need an explicit retrieval step. The latter in\nparticular can lead to a huge speedup in the sys-\ntem, since we can skip the time-consuming step\nof searching over a potentially massive space of\ndocuments. Second, LMs are widely-available and\nare currently attracting signiﬁcant research effort.\nThus, research in language-modeling, particularly\nin improving LMs ability to memorizing knowl-\nedge, may also improve the overall effectiveness\nof our fact-checking pipeline. Lastly, our system\nfurther shifts the paradigm towards “one model for\nall” — LMs have been used for a wide variety of\ntasks, and now also for fact checking.\nIn order to determine the feasibility of our\napproach, we start with a human review study\nwhere participants are given a claim from FEVER\n(Thorne et al., 2018a), and are asked to validate\nthe claim using only a BERT language model. We\nfound that users had reasonable success in deter-\nmining claim validity. Empowered by the results,\n37\nwe design an end-to-end neural approach for utiliz-\ning BERT as a fact checker (see Figure 1b). At a\nhigh level, we ﬁrst generate an evidence sentence\nby masking the claim and using BERT to “ﬁll in”\nthe mask. We then feed the generated sentence,\nalongside the original claim, to a veriﬁcation clas-\nsiﬁer model that classiﬁes whether the claim is sup-\nported, refuted, or the information is insufﬁcient to\nmake a call.\nThe rest of the paper is organized as such: Sec-\ntion 2 gives an overview of the problem space.\nSection 3 describes our preliminary experiments.\nSections 4 and 5 highlights our main methods (i.e.\nend-to-end model, experimental setup), and 6 re-\nports our main results. Sections 7 and 8 conclude\nour paper with a discussion and future works.\n2 Background\nTask The main goal of fact-checking is to vali-\ndate the truthfulness of a given claim. Each claim\nis assigned one of three labels: support, refute, or\nnot enough information (NEI) to verify.\nDataset We use FEVER (Thorne et al., 2018a), a\nlarge-scale fact-checking dataset with around 5.4M\nWikipedia documents. Claims were generated by\nextracting sentences from Wikipedia (with possible\nmutations), and were annotated by humans with\ntheir veriﬁcation label and/or evidence sentences\nfrom Wikipedia.\nTraditional pipeline Traditional fact-checking\nsystems (Fig. 1a) access knowledge within an ex-\nternal knowledge base (i.e. Wikipedia) to validate\na claim. They use a multi-step, pipelined approach,\nwhich involve IR-modules, such as document re-\ntrievers and evidence selectors, for retrieving the\nappropriate evidence, and veriﬁcation modules that\ntake in {claim, [evidences]}pairs and pre-\ndict a ﬁnal veriﬁcation label\nOur pipeline As shown in Fig.1b, our proposed\npipeline replaces both the external knowledge base\nas well as the IR modules with a pretrained lan-\nguage model. In the remainder of this paper, we\nutilize BERT. Future work can explore other lan-\nguage models.\nQuerying the Language Model In Petroni et al.\n(2019), language models were used as knowledge\nbase to answer open-domain questions. To do this,\nthe authors devised a probe known as “LAMA”,\nwhich generates ﬁll-in-the-blank cloze-style state-\nments from questions. For example, in order to\nanswer the question ‘Where is Microsoft’s head-\nquarter?’, the question would be rewritten as as\n‘Microsoft’s headquarter is in [MASK]’ and fed\ninto a language model for the answer.\nInspired by LAMA (Petroni et al., 2019), we also\ngenerate evidences from language models through\nﬁll-in-the-blank style tasks.\n3 Exploratory Experiments\nIn order to determine the feasibility of our ap-\nproach, we began by conducting a human review\nstudy on 50 random-selected claims from FEVER\n(Thorne et al., 2018a). Participants were asked to\nvalidate each claim with only a language model, by\nfollowing these steps:\n1. Mask a token from the claim, depending on\ncomponent of the claim we wish to verify:\nThomas Jefferson founded the University\nof Virginia after retiring →Thomas Jeffer-\nson founded the University of [MASK] af-\nter retiring.\nIn this example, the user is verifying which\nuniversity was founded by Thomas Jefferson.\nNote that the user could alternatively choose\nto mask Thomas Jeffersonin order to verify\nthe founder of University of Virginia.\n2. Get the top-1 predicted token from the LM.\nTop-1 predicted token = Virginia.\n3. If predicted token matches the masked token,\nthe claim is supported, otherwise it is refuted.\nVirginia ≡Virginia →SUPPORTS\nIn other words, we asked participants to serve as\nthe “masking” and “veriﬁcation classiﬁer” compo-\nnents of our fact-checking pipeline in Fig. 1b.\nTwo participants examined the 50 claims, and\neventually achieved an average accuracy of 55%. 1\nWe also conducted this zero-shot study on a\nlarger scale and in a more systematic way, by taking\nall claims in the full FEVER dataset, and always\nmasking the last token.2 Otherwise, we preserve\nsteps 2 and 3 from above. Even with this na ¨ıve\n1Both participants had NLP background, and both were\nfamiliar with FEVER and the fact-checking task. We also as-\nsumed both participants were capable of selecting the optimal\nposition to mask.\n2We omit examples for which the masked token is not in\nBERT’s vocab.\n38\ntoken-matching approach, we were able to obtain\nprecision 56% and F1 59% for the positive label\n(SUPPORT).\nOur preliminary experiments’ results illustrate\nthat, with a good masking mechanism and veriﬁca-\ntion model, language models can indeed feasibly\nbe used for fact-checking.\n4 End-to-End Fact-Checking Model\nEnlightened by results from our preliminary exper-\niments, we devise an end-to-end model that auto-\nmates and improve upon the masking and veriﬁca-\ntion steps that were conducted by humans. Speciﬁ-\ncally, we resolve two limitations: 1. manual mask-\ning of claims, and 2. na ¨ıve validation of the pre-\ndicted token that fails to deal with synonyms and\nother semantic variants of the answer.\nAutomatic Masking We mask the last named\nentity in the claim, which we identify using an off-\nthe-shelf Named-Entity-Recognition (NER) model\nfrom spaCy Honnibal and Montani (2017). In par-\nticular, we choose to mask named entities in order\nto better ensure that the token we mask actually\nmakes use of the knowledge encoded in language\nmodels. (Otherwise, we may mask tokens that only\nmake use of the LM’s ability to recover linguis-\ntic structures and syntax – for instance, masking\nstopwords). This hinges on the observation that,\nfor most claims, its factuality hinges upon the cor-\nrectness of its entities (and the possible relations\nbetween them), and not on how speciﬁcally the\nclaim is phrased.\nVeriﬁcation using Entailment To move beyond\nna¨ıvely matching predicted and gold tokens, we\nleverage a textual entailment model from Al-\nlenNLP (Gardner et al., 2018) to validate our LM\npredictions. Note that textual entailment models\npredict the directional truth relation between a text\npair (i.e. “sentence t entails h” if, typically, a hu-\nman reading t would infer that h is most likely\ntrue).\nFull-pipeline steps Detailed steps for our end-to-\nend model (Fig. 2) are as follows:\n1. Masked the last named entity found by the\nNER model.\n2. Get the top-1 predicted token from the LM,\nand ﬁll in the [MASK] accordingly to create\nthe “evidence” sentence.\nFigure 2: Detailed illustration of our pipeline\n3. Using the claim and generated “evidence” sen-\ntence, obtain entailment “features” using out-\nputs from the last layer of the pretrained en-\ntailment model (before the softmax).\n4. Input the entailment features into a multi-layer\nperceptron (MLP) for ﬁnal fact-veriﬁcation\nprediction.\n5 Experiments\n5.1 Experiment setup\nWe conduct our experiments on the FEVER claim\nveriﬁcation dataset (Thorne et al., 2018a) using the\nstandard provided splits. We use the publicly avail-\nable 24-layer BERT-Large as our language model,\nwhich was pre-trained on Wikipedia in 2018.3\nThe MLP was optimized using Adam, and\ntrained with a mini-batch size of 32. The learn-\ning rate was set to 0.001 with max epoch size 200\nand epoch patience of 30. The embedding size of\nthe entailment features (from the pre-trained entail-\nment model) was 400, and our MLP classiﬁer had\nhidden size of 100.\n5.2 Evaluation Metric\nThe traditional pipeline was evaluated using\nFEVER scoring, which is a stricter form of scoring\nthat treats predictions to be correct only when cor-\nrect evidences were retrieved. Since our pipeline\n3It’s possible the model was trained on a later Wikipedia\ndump than what’s released as part of FEVER, but pre-training\nBERT from scratch is beyond the scope of this paper.\n39\nModel Label prec recall f1 accuracy macro prec macro recall macro f1\nBERTfreeze\nREFUTES 0.36 0.69 0.47\n0.38 0.39 0.38 0.33SUPPORTS 0.43 0.09 0.15\nNEI 0.39 0.35 0.37\nBERTfinetune\nREFUTES 0.62 0.55 0.58\n0.57 0.57 0.57 0.57SUPPORTS 0.54 0.67 0.59\nNEI 0.57 0.49 0.53\nBERTasKB\nREFUTES 0.76 0.38 0.51\n0.49 0.59 0.49 0.44SUPPORTS 0.41 0.92 0.57\nNEI 0.58 0.15 0.24\nSoTA (Thorne et al., 2018b) * - - - - 0.68 - - -\nTable 1: Performance comparison between BERT-as-encoder models (BERTfreeze , BERTfinetune ) and BERT-\nas-LM model ( BERTasKB ) (*We report fact-checking label accuracy, not FEVER score - a stricter form of\nscoring\ndoes not utilize an external knowledge base, and\ndoes not have an evidence retriever, we only ex-\namine the correctness of the ﬁnal veriﬁcation step\nusing precision, recall, F1 and accuracy. We leave\ngenerating evidences with language models for fu-\nture work.\n5.3 Baselines\nWe introduce two language model baselines for\ncomparison. The ﬁrst baseline, BERTfreeze , uses\nan MLP layer on top of a frozen BERT encoder to\nmake predictions (gradients backpropagate to the\nMLP layer only). In this baseline, we aim toextract\nthe already stored knowledge within BERT model\nas an embedding vector, and avoid ﬁnetuning the in-\nternal layers, in order to disentangle BERT’s knowl-\nedge from it’s ability to serve as a high-capacity\nclassiﬁer.\nThe second baseline, BERTfinetune , allows all\nthe model layers to be updated based on the fact-\nveriﬁcation loss from the MLP layer. This baseline\ncaptures BERT’s ability asboth a language model,\nand a high-capacity text encoder.\nNote that the dataset is evenly distributed among\nthe three classes, therefore a random baseline\nwould yield an accuracy of 33%. Also note that the\nFever-baseline model introduced by the task orga-\nnizers achieves accuracy score of 48.8% (Thorne\net al., 2018b).\n6 Results and Discussion\nThe results of the three models are reported in Ta-\nble 1. We observe that our proposed approach\n(BERTasKB ) outperforms BERTfreeze on all\nmetrics suggesting that querying language models\nin QA style is a better approach for extracting their\nencoded knowledge. Similarly, BERTasKB\nmodel achieves an accuracy score of 49% which\nis comparable to Fever-baseline at 48.8%, except\nwithout the need for explicit document retrieval\nand evidence selection. This suggests that lan-\nguage models, used as sources of knowledge for\nfact checking, are at least as effective as standard\nbaselines. However, there is still much room for\nfuture research, as the state-of-the-art model on the\nFever shared task achieves an accuracy score of\n68.21% (Thorne et al., 2018b).\nOn the other hand, we ﬁnd that BERTasKB\nlags behind BERTfinetune , as expected, on most\nmetrics. We hypothesize this is due to the high\ncapacity of the model, in comparison, and to the\neffectiveness of BERT models in text classiﬁca-\ntion. Upon examining the results of these two mod-\nels closely, we ﬁnd that BERTasKB struggles\nmightily with the NEI category (F1 score of 0.24\nvs 0.53) indicating that our current approach might\nneed speciﬁc modules to better tackle that cate-\ngory. As both models seem to be equally adept in\nidentifying the support class (0.57 vs 0.59 F1),\nindicating that BERTasKB is unable to distin-\nguish between refute and NEI classes. Future\nwork can further investigate techniques to identify\nthese two categories.\nInterestingly, the BERTfreeze achieves an accu-\nracy score of 38% which is slightly better than a\nrandom baseline which achieves 33%.\n7 Analysis of Token Prediction Results\nIn this section, we provide some examples of to-\nkens predicted from BERT to understand the per-\nformance of “evidence generation”.\nFirst two examples in Table 2 ( a, b) are exam-\nples with correct fact-check labels from zeroshot\nsetting. When a claim has enough context, and con-\ntains rather rare names such as “Sarawak”, BERT\nmanages to predict correct tokens.\n40\nID Claim Masked Token Predicted Token Label\na Kuching is the capital of [MASK]. Sarawak Sarawak SUPPORTS\nb The Beach’s director was Danny [MASK]. Boyle Boyle SUPPORTS\nc Tim Roth was born in [MASK] 1961 London SUPPORTS\nd Chile is a [MASK]. country democracy SUPPORTS\ne Seohyun [MASK]. sings Park SUPPORTS\nTable 2: Examples of token predictions from BERT in zeroshot setting. a, b are correctly fact-checked examples,\nand c, d, f are wrongly fact-checked examples.\nWe also provide detailed analysis on the error\ncases to facilitate future work in making further\nimprovements:\n•One common form of errors is that, the en-\ntity type of token prediction is biased towards\nthe way how the training data was written.\nFor example, sentence c from Table 2 illus-\ntrates a common claim structure in FEVER\ndataset which talks about the birth-year of a\nperson (e.g., Tim Roth). However, 100% of\nour test samples with such structure always\npredict city/country (e.g., London). The rea-\nson is, in Wikipedia, the birth-years are al-\nways written in the following structure “PER-\nSON (born DATE)” (e.g., “Tim Roth (born 14\nMay 1961)”), and birth city/country written in\n“PERSON was born in city/country” structure\n(e.g., “Roth was born in Dulwich, London”).\nTherefore, to obtain birth-year, the claim had\nto be written as Tim Roth (born [MASK]) to\npredict correctly.\n•Sentence d is another example that the entity\ntype of token prediction is hard to control. “is\na...” is a very general preﬁx phrase, making\nit hard for BERT model to correctly predict\ncorrect entity type.\n•There are lots of short claims in FEVER test\nset (approx. 1100 samples) which has less\nthan 5 tokens (e.g. sentence e). Since there is\nvery little context, BERT struggles to predict\ncorrectly.\nOne of the the main insight we get from these\nanalysis is that, the way the language model is\ninitially pre-trained, greatly determines the way it\nshould be “queried”.\n8 Conclusions & Future Work\nIn this paper, we explored a new fact-checking\npipeline that use language models as knowledge\nbases. Unlike previous pipelines that required dedi-\ncated components for document retrieval and sen-\ntence scoring, our approach simply translates a\ngiven claim into a ﬁll-in-the-blank type query and\nrelies on a BERT language model to generate the\n“evidence”. Our experiment shows that this ap-\nproach is comparable to the standard baselines on\nthe FEVER dataset, though not enough to beat the\nstate-of-the-art using the traditional pipeline. How-\never, we believe our approach has strong potential\nfor improvement, and future work can explore us-\ning stronger models for generating evidences, or\nimproving the way how we mask claims.\nIn the future, we will investigate sequence-to-\nsequence language models such as BART (Lewis\net al., 2019) or T5 (Raffel et al., 2019), that have re-\ncently shown to be effective on generative question-\nanswering (Roberts et al., 2020). Similarly, our\nproposed approach seem to struggle with correctly\nidentifying NEI cases, and we plan to investigate\nadding speciﬁc modules to deal with NEI. Lastly,\nwe plan to explore new ways of pre-training lan-\nguage models to better store and encode knowl-\nedge.\nAcknowledgements\nWe would like to thank Fabio Petroni for the helpful\ndiscussion and inspiration.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllennlp: A deep semantic natural language process-\ning platform. arXiv preprint arXiv:1803.07640.\nMatthew Honnibal and Ines Montani. 2017. spacy 2:\nNatural language understanding with bloom embed-\n41\ndings, convolutional neural networks and incremen-\ntal parsing. To appear, 7(1).\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nFabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018a.\nFever: a large-scale dataset for fact extraction and\nveriﬁcation. arXiv preprint arXiv:1803.05355.\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal.\n2018b. The fact extraction and veriﬁcation (fever)\nshared task. arXiv preprint arXiv:1811.10971.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8193483948707581
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.8128376007080078
    },
    {
      "name": "Language model",
      "score": 0.7396628856658936
    },
    {
      "name": "Task (project management)",
      "score": 0.6268219947814941
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6251824498176575
    },
    {
      "name": "Question answering",
      "score": 0.5828602313995361
    },
    {
      "name": "Domain knowledge",
      "score": 0.5444245934486389
    },
    {
      "name": "Natural language processing",
      "score": 0.47830870747566223
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47006306052207947
    },
    {
      "name": "Machine learning",
      "score": 0.3390439450740814
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ],
  "cited_by": 3
}