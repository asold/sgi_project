{
  "title": "Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data",
  "url": "https://openalex.org/W3094508337",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A247404666",
      "name": "Kong, Lingkai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2371030016",
      "name": "Jiang, Haoming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231315164",
      "name": "Zhuang, Yuchen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2587638869",
      "name": "Lyu Jie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1949403856",
      "name": "Zhao Tuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1851812986",
      "name": "Zhang Chao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2921861056",
    "https://openalex.org/W2115791615",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W2996564870",
    "https://openalex.org/W2963756980",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2759474451",
    "https://openalex.org/W2963501948",
    "https://openalex.org/W2098824882",
    "https://openalex.org/W2597787948",
    "https://openalex.org/W2950300355",
    "https://openalex.org/W2951266961",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2964031043",
    "https://openalex.org/W2108281845",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2254249950",
    "https://openalex.org/W2964159205",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2970121940",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2164411961",
    "https://openalex.org/W2898681588",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2804697534",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2592505114",
    "https://openalex.org/W1857789879",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2970316625",
    "https://openalex.org/W2970206392",
    "https://openalex.org/W2964282813",
    "https://openalex.org/W2531327146"
  ],
  "abstract": "Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.",
  "full_text": "Calibrated Language Model Fine-Tuning for In- and\nOut-of-Distribution Data\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, Chao Zhang∗\nAbstract\nFine-tuned pre-trained language models can su ﬀer from severe miscalibration for both\nin-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate\nthis issue, we propose a regularized ﬁne-tuning method. Our method introduces two types\nof regularization for better calibration: (1) On-manifold regularization, which generates pseudo\non-manifold samples through interpolation within the data manifold. Augmented training with\nthese pseudo samples imposes a smoothness regularization to improve in-distribution calibra-\ntion. (2) Oﬀ-manifold regularization, which encourages the model to output uniform distributions\nfor pseudo o ﬀ-manifold samples to address the over-conﬁdence issue for OOD data. Our ex-\nperiments demonstrate that the proposed method outperforms existing calibration methods\nfor text classiﬁcation in terms of expectation calibration error, misclassiﬁcation detection, and\nOOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/\nCalibrated-BERT-Fine-Tuning .\n1 Introduction\nPre-trained language models have recently brought the natural language processing (NLP) commu-\nnity into the transfer learning era. The transfer learning framework consists of two stages, where\nwe ﬁrst pre-train a large-scale language model, ( e.g., BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), ALBERT (Lan et al., 2020) and T5 (Raﬀel et al., 2019)) on a large text corpus and then\nﬁne-tune it on downstream tasks. Such a ﬁne-tuning approach has achieved SOTA performance in\nmany NLP benchmarks (Wang et al., 2018, 2019).\nMany applications, however, require trustworthy predictions that need to be not only accurate\nbut also well calibrated. In particular, a well-calibrated model should produce reliable conﬁdent\nestimates for both in-distribution and out-of-distribution (OOD) data: (1) For in-distribution data,\na model should produce predictive probabilities close to the true likelihood for each class, i.e.,\nconﬁdence ≈true likelihood. (2) For OOD data, which do not belong to any class of the training\ndata, the model output should produce high uncertainty to say ‘I don’t know’,i.e., conﬁdence ≈\nrandom guess, instead of producing absurdly wrong yet wildly conﬁdent predictions. Providing\nsuch calibrated output probabilities can help us to achieve better model robustness (Lee et al.,\n∗All authors are a ﬃliated with Georgia Institute of Technology. Emails: {lkkong,jianghm,\nyczhuang,jie.lyu,tourzhao,chaozhang}@gatech.edu.\n1\narXiv:2010.11506v1  [cs.CL]  22 Oct 2020\nFigure 1: The reliability diagrams on in-distribution data (the ﬁrst row) and the histograms of\nthe model conﬁdence on out-of-distribution (OOD) data (the second row) of CNN (Kim, 2014)\nand ﬁne-tuned BERT-MLP classiﬁer (Devlin et al., 2019). Though BERT improves classiﬁcation\naccuracy, it makes over-conﬁdent predictions for both in-distribution and OOD data.\n2018), model fairness (Chouldechova, 2017) and improve label eﬃciency via uncertainty driven\nlearning (Gal et al., 2017; Siddhant and Lipton, 2018; Shen et al., 2018).\nUnfortunately, Guo et al. (2017) have shown that due to over-parameterization, deep convolu-\ntional neural networks are often miscalibrated. Our experimental investigation further corroborates\nthat ﬁne-tuned language models can suﬀer from miscalibration even more for NLP tasks. As shown\nin Figure 1, we present the calibration of a BERT-MLP model for a text classiﬁcation task on the\n20NG dataset. Speciﬁcally, we train a TextCNN (Kim, 2014) and a BERT-MLP using 20NG15 (the\nﬁrst 15 categories of 20NG) and then evaluate them on both in-distribution and OOD data. The\nﬁrst row plots their reliability diagrams (Niculescu-Mizil and Caruana, 2005) on the test set of\n20NG15. Though BERT improves the classiﬁcation accuracy from 83 .9% to 87.4%, it also increases\nthe expected calibration error (ECE, see more details in Section 2) from 4.0% to 9.5%. This indicates\nthat BERT-MLP is much more miscalibrated for in-distribution data. The second row plots the\nhistograms of the model conﬁdence, i.e., the maximum output probability, on the test set of 20NG5\n(the unseen 5 categories of 20NG). While it is desirable to produce low probabilities for these\nunseen classes, BERT-MLP produces wrong yet over-conﬁdent predictions for such OOD data.\nSuch an aggravation of miscalibration is due to the even more signiﬁcant over-parameterization\nof these language models. At the pre-training stage, they are trained on a huge amount of unlabeled\ndata in an unsupervised manner, e.g., T5 is pre-trained on 745 GB text. To capture rich semantic\nand syntactic information from such a large corpus, the language models are designed to have\nenormous capacity, e.g., T5 has about 11 billion parameters. At the ﬁne-tuning stage, however, only\n2\nlimited labeled data are available in the downstream tasks. With the extremely high capacity, these\nmodels can easily overﬁt training data likelihood and be over-conﬁdent in their predictions.\nTo ﬁght against miscalibration, a natural option is to apply a calibration method such as\ntemperature scaling (Guo et al., 2017) in a post-processing step. However, temperature scaling only\nlearns a single parameter to rescale all the logits, which is not ﬂexible and insuﬃcient. Moreover, it\ncannot improve out-of-distribution calibration. A second option is to mitigate miscalibration during\ntraining using regularization. For example, Pereyra et al. (2017) propose an entropy regularizer\nto prevent over-conﬁdence, but it can needlessly hurt legitimate high conﬁdent predictions. A\nthird option is to use Bayesian neural networks (Blundell et al., 2015; Louizos and Welling, 2017),\nwhich treat model parameters as probability distributions to represent model uncertainty explicitly.\nHowever, these Bayesian approaches are often prohibitive, as the priors of the model parameters are\ndiﬃcult to specify, and exact inference is intractable, which can also lead to unreliable uncertainty\nestimates.\nWe propose a regularization approach to addressing miscalibration for ﬁne-tuning pre-trained\nlanguage models from a data augmentation perspective. We propose two new regularizers using\npseudo samples both on and oﬀ the data manifold to mitigate data scarcity and prevent over-\nconﬁdent predictions. Speciﬁcally, our method imposes two types of regularization for better\ncalibration during ﬁne-tuning: (1) On-manifold regularization: We ﬁrst generate on-manifold\nsamples by interpolating the training data and their corresponding labels along the direction\nlearned from hidden feature space; training over such augmented on-manifold data introduces a\nsmoothness constraint within the data manifold to improve the model calibration for in-distribution\ndata. (2) Oﬀ-manifold regularization: We generate oﬀ-manifold samples by adding relatively large\nperturbations along the directions that point outward the data manifold; we penalize the negative\nentropy of the output distribution for such oﬀ-manifold samples to address the over-conﬁdence\nissue for OOD data.\nWe evaluate our proposed model calibration method on six text classiﬁcation datasets. For\nin-distribution data, we measure ECE and the performance of misclassiﬁcation detection. For\nout-of-distribution data, we measure the performance of OOD detection. Our experiments show\nthat our method outperforms existing state-of-the-art methods in both settings, and meanwhile\nmaintains competitive classiﬁcation accuracy.\nWe summarize our contribution as follows: (1) We propose a general calibration framework,\nwhich can be applied to pre-trained language model ﬁne-tuning, as well as other deep neural\nnetwork-based prediction problems. (2) The proposed method adopts on- and oﬀ-manifold regu-\nlarization from a data augmentation perspective to improve calibration for both in-distribution\nand OOD data. (3) We conduct comprehensive experiments showing that our method outperforms\nexisting calibration methods in terms of ECE, miscalssiﬁcation detection and OOD detection on six\ntext classiﬁcation datasets.\n2 Preliminaries\nWe describe model calibration for both in-distribution and out-of-distribution data.\n3\nCalibration for In-distribution Data: For in-distribution data, a well-calibrated model is expected\nto output prediction conﬁdence comparable to its classiﬁcation accuracy. For example, given 100\ndata points with their prediction conﬁdence 0.6, we expect 60 of them to be correctly classiﬁed.\nMore precisely, for a data point X, we denote by Y(X) the ground truth label, ˆY(X) the label\npredicted by the model, and ˆP(X) the output probability associated with the predicted label. The\ncalibration error of the predictive model for a given conﬁdence p∈(0,1) is deﬁned as:\nEp = |P(ˆY(X) = Y(X)|ˆP(X) = p) −p|. (1)\nAs (1) involves population quantities, we usually adopt empirical approximations (Guo et al., 2017)\nto estimate the calibration error. Speciﬁcally, we partition all data points into M bins of equal\nsize according to their prediction conﬁdences. Let Bm denote the bin with prediction conﬁdences\nbounded between ℓm and um. Then, for any p∈[ℓm,um), we deﬁne the empirical calibration error\nas:\nˆEp = ˆEm = 1\n|Bm|\n⏐⏐⏐⏐\n∑\ni∈Bm\n[\n1(ˆyi = yi) −ˆpi\n]⏐⏐⏐⏐, (2)\nwhere yi, ˆyi and ˆpi are the true label, predicted label and conﬁdence for sample i.\nTo evaluate the overall calibration error of the predictive model, we can futher take a weighted\naverage of the calibration errors of all bins, which is also known as the expected calibration error\n(ECE) (Naeini et al., 2015) deﬁned as:\nECE =\nM∑\nm=1\n|Bm|\nn\nˆEm, (3)\nwhere nis the sample size.\nWe remark that the goal of calibration is to minimize the calibration error without signiﬁcantly\nsacriﬁcing prediction accuracy. Otherwise, a random guess classiﬁer can achieve zero calibration\nerror.\nCalibration for Out-of-distribution Data: In real applications, a model can encounter test data\nthat signiﬁcantly diﬀer from the training data. For example, they come from other unseen classes,\nor they are potential outliers. A well-calibrated model is expected to produce an output with high\nuncertainty for such out-of-distribution (OOD) data, formally,\nP(Y = j) = 1/K ∀j= 1,...,K,\nwhere Kis the number of classes of the training data. As such, we can detect OOD data by setting\nup an uncertainty threshold.\n3 Calibrated Fine-Tuning via Manifold Smoothing\nWe consider N data points of the target task S= {(xi,yi)}N\ni=1, where xi’s denote the input embedding\nof the sentence and yi’s are the associated one-hot labels. Let f(·) denote the feature extraction\n4\nx\n<latexit sha1_base64=\"kO36Fwqnz69PGe5U1YsdTl42T9o=\">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZkq2GXBjcsK9oFtKZk004ZmMkNyRyxD/8KNC0Xc+jfu/Bsz7Sy09UDgcM695Nzjx1IYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJRoxpsskpHu+NRwKRRvokDJO7HmNPQlb/uTm8xvP3JtRKTucRrzfkhHSgSCUbTSQy+kOPaD9Gk2KJXdijsHWSVeTsqQozEoffWGEUtCrpBJakzXc2Psp1SjYJLPir3E8JiyCR3xrqWKhtz003niGTm3ypAEkbZPIZmrvzdSGhozDX07mSU0y14m/ud1Ewxq/VSoOEGu2OKjIJEEI5KdT4ZCc4ZyagllWtishI2ppgxtSUVbgrd88ippVSveZaV6d1Wu1/I6CnAKZ3ABHlxDHW6hAU1goOAZXuHNMc6L8+58LEbXnHznBP7A+fwB+4WRFg==</latexit>\n˜x\n<latexit sha1_base64=\"ptYS2YtzpEm19IJM5Kr9s0kFMHA=\">AAAB+3icbVDLSsNAFJ3UV62vWJduBovgqiRVsMuCG5cV7AOaUCaTSTt08mDmRlpCfsWNC0Xc+iPu/BsnbRbaemDgcM693DPHSwRXYFnfRmVre2d3r7pfOzg8Oj4xT+t9FaeSsh6NRSyHHlFM8Ij1gINgw0QyEnqCDbzZXeEPnphUPI4eYZEwNySTiAecEtDS2Kw7wIXPMickMPWCbJ7nY7NhNa0l8CaxS9JAJbpj88vxY5qGLAIqiFIj20rAzYgETgXLa06qWELojEzYSNOIhEy52TJ7ji+14uMglvpFgJfq742MhEotQk9PFhHVuleI/3mjFIK2m/EoSYFFdHUoSAWGGBdFYJ9LRkEsNCFUcp0V0ymRhIKuq6ZLsNe/vEn6raZ93Ww93DQ67bKOKjpHF+gK2egWddA96qIeomiOntErejNy48V4Nz5WoxWj3DlDf2B8/gDvQpT9</latexit>\n\u0000 on\n<latexit sha1_base64=\"CZChaCfdAh9O/ewyV96FqPYve1U=\">AAAB+HicbVDLSsNAFJ34rPXRqEs3g0VwVZIq2GXBjcsK9gFNCJPJpB06jzAzEWrol7hxoYhbP8Wdf+O0zUJbD1w4nHMv994TZ4xq43nfzsbm1vbObmWvun9weFRzj096WuYKky6WTKpBjDRhVJCuoYaRQaYI4jEj/XhyO/f7j0RpKsWDmWYk5GgkaEoxMlaK3FqQEGZQVASKQylmkVv3Gt4CcJ34JamDEp3I/QoSiXNOhMEMaT30vcyEBVKGYkZm1SDXJEN4gkZkaKlAnOiwWBw+gxdWSWAqlS1h4EL9PVEgrvWUx7aTIzPWq95c/M8b5iZthQUVWW6IwMtFac6gkXCeAkyoItiwqSUIK2pvhXiMFMLGZlW1IfirL6+TXrPhXzWa99f1dquMowLOwDm4BD64AW1wBzqgCzDIwTN4BW/Ok/PivDsfy9YNp5w5BX/gfP4A3QWTMA==</latexit>\nTraining data\nOn-manifold sample \nOﬀ-manifold sample Data manifold\nx\n<latexit sha1_base64=\"kO36Fwqnz69PGe5U1YsdTl42T9o=\">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZkq2GXBjcsK9oFtKZk004ZmMkNyRyxD/8KNC0Xc+jfu/Bsz7Sy09UDgcM695Nzjx1IYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJRoxpsskpHu+NRwKRRvokDJO7HmNPQlb/uTm8xvP3JtRKTucRrzfkhHSgSCUbTSQy+kOPaD9Gk2KJXdijsHWSVeTsqQozEoffWGEUtCrpBJakzXc2Psp1SjYJLPir3E8JiyCR3xrqWKhtz003niGTm3ypAEkbZPIZmrvzdSGhozDX07mSU0y14m/ud1Ewxq/VSoOEGu2OKjIJEEI5KdT4ZCc4ZyagllWtishI2ppgxtSUVbgrd88ippVSveZaV6d1Wu1/I6CnAKZ3ABHlxDHW6hAU1goOAZXuHNMc6L8+58LEbXnHznBP7A+fwB+4WRFg==</latexit>\n˜x\n<latexit sha1_base64=\"ptYS2YtzpEm19IJM5Kr9s0kFMHA=\">AAAB+3icbVDLSsNAFJ3UV62vWJduBovgqiRVsMuCG5cV7AOaUCaTSTt08mDmRlpCfsWNC0Xc+iPu/BsnbRbaemDgcM693DPHSwRXYFnfRmVre2d3r7pfOzg8Oj4xT+t9FaeSsh6NRSyHHlFM8Ij1gINgw0QyEnqCDbzZXeEPnphUPI4eYZEwNySTiAecEtDS2Kw7wIXPMickMPWCbJ7nY7NhNa0l8CaxS9JAJbpj88vxY5qGLAIqiFIj20rAzYgETgXLa06qWELojEzYSNOIhEy52TJ7ji+14uMglvpFgJfq742MhEotQk9PFhHVuleI/3mjFIK2m/EoSYFFdHUoSAWGGBdFYJ9LRkEsNCFUcp0V0ymRhIKuq6ZLsNe/vEn6raZ93Ww93DQ67bKOKjpHF+gK2egWddA96qIeomiOntErejNy48V4Nz5WoxWj3DlDf2B8/gDvQpT9</latexit>\n\u0000 o ↵\n<latexit sha1_base64=\"XfdbJCoJT3eMoz4Sp59lm8GkcII=\">AAAB+XicbVDLSsNAFJ3UV62vqEs3g0VwVZIq2GXBjcsK9gFNCJPJpB06jzAzKZTQP3HjQhG3/ok7/8Zpm4W2HrhwOOde7r0nzhjVxvO+ncrW9s7uXnW/dnB4dHzinp71tMwVJl0smVSDGGnCqCBdQw0jg0wRxGNG+vHkfuH3p0RpKsWTmWUk5GgkaEoxMlaKXDdICDMoKgLFoUzTeeTWvYa3BNwkfknqoEQncr+CROKcE2EwQ1oPfS8zYYGUoZiReS3INckQnqARGVoqECc6LJaXz+GVVRKYSmVLGLhUf08UiGs947Ht5MiM9bq3EP/zhrlJW2FBRZYbIvBqUZozaCRcxAATqgg2bGYJworaWyEeI4WwsWHVbAj++subpNds+DeN5uNtvd0q46iCC3AJroEP7kAbPIAO6AIMpuAZvII3p3BenHfnY9VaccqZc/AHzucPlnmTmA==</latexit>\nMixup sampleInterpolation path\nFigure 2: The on-manifold and o ﬀ-manifold samples generated by our calibration procedure.\nMixup adopts a coarse linear interpolation and the generated data point may deviate from the data\nmanifold.\nlayers (e.g., BERT); let g(·) denote the task-speciﬁc layer; and let θdenote all parameters of f and g.\nWe propose to optimize the following objective at the ﬁne-tuning stage:\nmin\nθ\nF(θ) = Ex,y∼Sℓ(g◦f(x),y) +λonRon(g◦f) +λoﬀRoﬀ(g◦f), (4)\nwhere ℓis the cross entropy loss, and λon,λoﬀ are two hyper-parameters. The regularizers Ron and\nRoﬀ are for on- and oﬀ-manifold calibration, respectively.\n3.1 On-manifold Regularization\nThe on-manifold regularizerRon exploits the interpolation of training data within the data manifold\nto improve the in-distribution calibration. Speciﬁcally, given two training samples (x,y) and (˜x,˜y)\nand the feature extraction layers f, we generate an on-manifold pseudo sample (x′,y′) as follows:\nx′∗= argmin\nx′∈B(x,δon)\nDx(f(x′),f(˜x)), (5)\ny′= (1 −δy)y+ δy˜y, (6)\nwhere δon and δy are small interpolation parameters for data and label, and Dx is a proper distance\nfor features extracted by f such as cosine distance, i.e., Dx(a,b) = ⟨a/∥a∥2,b/∥b∥2⟩, and B(x,δon)\ndenotes an ℓ∞ball centered at x with a radius δon, i.e.,\nB(x,δon) = {x′|∥x′−x∥∞≤δon}.\nAs can be seen, x′∗ is essentially interpolating between x and ˜x on the data manifold, and\nDx(f(·),f(·)) can be viewed as a metric over such a manifold. However, as f(·) is learnt from ﬁnite\ntraining data, it can recover the actual data manifold only up to a certain statistical error. Therefore,\nwe constrain x′∗to stay in a small neighborhood of x, which ensures x′∗to stay close to the actual\ndata manifold.\n5\nAlgorithm 1 Our Proposed Eﬃcient Stochastic Optimization Algorithm for Solving (4). dis the\ndimension of features.\nfor # training iterations do\nSample a mini-batch B= {xi,yi}from S.\n// Generate on-manifold samples:\nFor each xi ∈B, randomly select{˜xi,˜yi}from B, initialize x′\ni ←xi+vi with vi ∼UNIF[−δon,δon]d\n∆′\ni ←sign(∇x′\ni\nDx(f(x′\ni),f(˜xi)))\nx′\ni ←Π∥x′\ni−xi∥∞≤δon (x′\ni−δon∆′\ni)\ny′←(1 −δy)yi+ δy˜yi\n// Generate oﬀ-manifold samples:\nFor each xi ∈B, initialize x′′\ni ←xi+ v′\ni with v′\ni ∼UNIF[−δoﬀ,δoﬀ]d\n∆′′\ni ←sign(∇x′′\ni\nℓ(g◦f(x′′\ni ),y)\nx′′\ni ←Π∥x′′\ni −xi∥∞=δoﬀ(x′′\ni + δoﬀ∆′′\ni )\nUpdate θusing ADAM\nend for\nThis is diﬀerent from existing interpolation methods such as Mixup (Zhang et al., 2018; Verma\net al., 2019). These methods adopt coarse linear interpolations either in the input space or latent\nfeature space, and the generated data may signiﬁcantly deviate from the data manifold.\nNote that our method not only interpolates x but also y. This can yield a soft label for x′∗, when\nx and ˜x belong to diﬀerent classes. Such an interpolation is analogous to semi-supervised learning,\nwhere soft pseudo labels are generated for the unlabelled data. These soft-labelled data essentially\ninduce a smoothing eﬀect, and prevent the model from making overconﬁdent predictions toward\none single class.\nWe remark that our method is more adaptive than the label smoothing method (M¨uller et al.,\n2019). As each interpolated data point involves at most two classes, it is unnecessary to distribute\nprobability mass to other classes in the soft label. In contrast, label smoothing is more rigid and\nenforces all classes to have equally nonzero probability mass in the soft label.\nWe then deﬁne the on-manifold regularizer as\nRon(g◦f) = E(x′,y′)∼Son DKL(y′,g ◦f(x′)),\nwhere Son denotes the set of all pseudo labelled data generated by our interpolation method, and\nDKL denotes the KL-divergence between two probability simplices.\n3.2 O ﬀ-manifold Regularization\nThe oﬀ-manifold regularizer, R2, encourages the model to yield low conﬁdence outputs for samples\noutside the data manifold, and thus mitigates the over-conﬁdence issue for out-of-distribution\n(OOD) data. Speciﬁcally, given a training sample (x,y), we generate an oﬀ-manifold pseudo sample\nx\n′′∗by:\nx\n′′∗= max\nx′′∈S(x,δoﬀ)\nℓ(g◦f(x′′),y), (7)\n6\nwhere S(x,δoﬀ) denotes an ℓ∞sphere centered at x with a radius δoﬀ.\nSince we expect x′′∗to mimic OOD data, we ﬁrst need to choose a relatively large δoﬀ such that\nthe sphere S(x,δoﬀ) can reach outside the data manifold. Then, we generate the pseudo oﬀ-manifold\nsample from the sphere along the adversarial direction. Existing literature (Stutz et al., 2019;\nGilmer et al., 2018) has shown that such an adversarial direction points outward the data manifold.\nBy penalizing the prediction conﬁdence for these oﬀ-manifold samples, we are able to encourage\nlow prediction conﬁdence for OOD data. Hence, we deﬁne the o ﬀ-manifold regularizer as\nRoﬀ(g◦f) = Ex′′∼Soﬀ−H(g◦f(x′′)), (8)\nwhere Soﬀ denotes the set of all generated oﬀ-manifold samples, and H(·) denotes the entropy of\nthe probability simplex.\n3.3 Model Training\nWe can adopt stochastic gradient-type algorithms such as ADAM (Kingma and Ba, 2014) to optimize\n(4). At each iteration, we need to ﬁrst solve two inner optimization problems in (5) and (7), and\nthen plug x′and x′′into (4) to compute the stochastic gradient. The two inner problems can be\nsolved using the projected sign gradient update for multiple steps. In practice, we observe that one\nsingle update step with random initialization is already suﬃcient to eﬃciently optimize θ. Such\na phenomenon has also been observed in existing literature on adversarial training (Wong et al.,\n2019). We summarize the overall training procedure in Algorithm 1.\n4 Experiments\nTo evaluate calibration performance for in-distribution data, we measure the expected calibration\nerror (ECE) and the misclassiﬁcation detection score. For out-of-distribution data, we measure the\nOOD detection score.\nWe detect the misclassiﬁed and OOD samples by model conﬁdence, which is the output prob-\nability associated with the predicted label ˆP(X). Speciﬁcally, we setup a conﬁdence threshold\nτ∈[0,1], and take the samples with conﬁdence below the threshold, i.e., ˆP(X) <τ, as the misclas-\nsiﬁed or OOD samples. We can compute the detection F1 score for every τ: F1(τ), and obtain a\ncalibration curve (F1(τ) vs. τ). Then, we set τupper as the upper bound of the conﬁdence threshold,\nsince a well calibrated model should provide probabilities that reﬂect the true likelihood and it is\nnot reasonable to use a large τ to detect them. We use the empirical Normalized Bounded Area\nUnder the Calibration Curve (NBAUCC) as the overall detection score:\nNBAUCCτupper = 1\nM\nM∑\ni=1\nF1\n(τupper\nM i\n)\n,\nwhere Mis the number of sub-intervals for the numerical integration. We set M= 50 throughout\nthe following experiments. Note that the traditional binary classiﬁcation metrics, e.g., AUROC\nand AUPR, cannot measure the true calibration because the model can still achieve high scores\n7\neven though it has high conﬁdences for the misclassiﬁed and OOD samples. We provide more\nexplanations of the metrics in Appendix C. We report the performance when τupper = 0.5 here and\nthe results when τupper = 0.7 and 1 in Appendix D.\n4.1 Datasets\nFor each dataset, we construct an in-distribution training set, an in-distribution testing set, and an\nOOD testing set. Speciﬁcally, we use the following datasets:\n20NG1. The 20 Newsgroups dataset (20NG) contains news articles with 20 categories. We use\nStanford Sentiment Treebank (SST-2) (Socher et al., 2012) as the OOD data.\n20NG15. We take the ﬁrst 15 categories of 20NG as the in-distribution data and the other 5\ncategories (20NG5) as the OOD data.\nWOS (Kowsari et al., 2017). Web of Science (WOS) dataset contains scientiﬁc articles with 134\ncategories. We use AGnews (Zhang et al., 2015) as the OOD data.\nWOS100. We use the ﬁrst 100 classes of WOS as the in-distribution data and the other 34 classes\n(WOS34) as the OOD data.\nYahoo (Chang et al., 2008). This dataset contains questions with 10 categories posted to ‘Yahoo!\nAnswers’. We randomly draw 2000 from 140,000 samples for each category as the training set. We\nuse Yelp (Zhang et al., 2015) as the OOD data.\nYahoo8. We use the ﬁrst 8 classes of Yahoo as the in-distribution data and the other 2 classes\n(Yahoo2) as the OOD data.\nThe testing set of OOD detection consists of the in-distribution testing set and the OOD data.\nMore dataset details can be found in Appendix A. We remark that 20NG15, WOS100, and Yahoo8\nare included to make OOD detection more challenging, as the OOD data and the training data\ncome from similar data sources.\n4.2 Baselines\nWe consider the following baselines:\n•BERT (Devlin et al., 2019) is a pre-trained base BERT model stacked with one linear layer.\n•Temperature Scaling (TS) (Guo et al., 2017) is a post-processing calibration method that learns\na single parameter to rescale the logits on the development set after the model is ﬁne-tuned.\n•Monte Carlo Dropout (MCDP) (Gal and Ghahramani, 2016) applies dropout at testing time for\nmultiple times and then averages the outputs.\n•Label Smoothing (LS) (M¨uller et al., 2019) smoothes the one-hot label by distributing a certain\nprobability mass to other non ground-truth classes.\n•Entropy Regularized Loss (ERL) (Pereyra et al., 2017) adds a entropy penalty term to prevent\nDNNs from being over-conﬁdent.\n•Virtual Adversarial Training (V AT)(Miyato et al., 2018) introduces a smoothness-inducing\nadversarial regularizer to encourage the local Lipschitz continuity of DNNs.\n1We use the 20 Newsgroups dataset from: http://qwone.com/~jason/20Newsgroups/\n8\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013\n/uni00000037/uni0000004b/uni00000055/uni00000048/uni00000056/uni0000004b/uni00000052/uni0000004f/uni00000047\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a/uni00000029/uni00000014/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013\n/uni00000037/uni0000004b/uni00000055/uni00000048/uni00000056/uni0000004b/uni00000052/uni0000004f/uni00000047\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000032/uni00000032/uni00000027/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000025/uni00000028/uni00000035/uni00000037/uni00000007\n/uni00000030/uni00000026/uni00000027/uni00000033\n/uni00000037/uni00000036\n/uni00000028/uni00000035/uni0000002f\n/uni0000002f/uni00000036\n/uni00000030/uni0000004c/uni0000005b/uni00000058/uni00000053\n/uni00000030/uni00000010/uni00000050/uni0000004c/uni0000005b/uni00000058/uni00000053\n/uni00000039/uni00000024/uni00000037\n/uni00000032/uni00000058/uni00000055/uni00000056\nFigure 3: Calibration curves of OOD detection and misclassiﬁcation detection on WOS. Our\nmethod can achieve high F1 scores starting from a small threshold which indicates that it indeed\nprovides low conﬁdences for misclassiﬁed and OOD samples; the F1 scores of the baselines peak at\nhigh thresholds which indicates that they are poorly calibrated.\n•Mixup (Zhang et al., 2018; Thulasidasan et al., 2019) augments training data by linearly interpo-\nlating training samples in the input space.\n•Manifold-mixup (M-mixup) (Verma et al., 2019) is an extension of Mixup, which interpolates\ntraining samples in the hidden feature space.\n4.3 Implementation Details\nWe use ADAM (Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.999 as the optimizer. For our\nmethod, we simply setλon = λoﬀ = 1,δon = 10−4,δoﬀ = 10−3, and δy = 0.1 for all the experiments. We\nalso conduct an extensive hyper-parameter search for the baselines. See more details in Appendix B.\n4.4 Main Results\nOur main results are summarized as follows:\nExpected Calibration Error: Table 1 reports the ECE and predictive accuracy of all the methods.\nOur method outperforms all the baselines on all the datasets in terms of ECE except for Yahoo,\nwhere only ERL is slightly better. Meanwhile, our method does not sacriﬁce the predictive accuracy.\nMisclassiﬁcation Detection: Table 2 compares the NBAUCC0.5 on misclassiﬁcation detection of\ndiﬀerent methods. As shown, our method outperforms all the baselines on all the six datasets.\nOut-of-distribution Detection: Table 2 reports the NBAUCC0.5 on OOD detection of di ﬀerent\nmethods. Again, our method achieves the best performance on all the six datasets. The improve-\nment is particularly remarkable on the 20NG dataset, where NBAUCC0.5 increases from 47.00 to\n63.92 compared with the strongest baseline. We also ﬁnd that detecting the unseen classes from\nthe original dataset is much more challenging than detecting OOD samples from a totally diﬀerent\ndataset.\nSigniﬁcance Test: We perform the Wilcoxon signed rank test (Wilcoxon, 1992) for signiﬁcance\ntest. For each dataset, we conduct experiments using 5 diﬀerent random seeds with signiﬁcance\n9\nModel ECE Accuracy\n20NG15 20NG WOS100 WOS Yahoo8 Yahoo 20NG15 20NG WOS100 WOS Yahoo8 Yahoo\nBERT 9 .24 11 .61 6 .81 6 .74 10 .11 10 .54 87.42 84 .55 81 .94 79 .40 73 .58 71 .89\nTS 4 .42 8 .17 3 .63 4 .43 5 .18 4 .24 87.42 84 .55 81 .94 79 .40 73 .58 71 .89\nMCDP 6 .88 9 .17 4 .00 3 .55 6 .54 6 .72 87.45 84 .55 82 .09 79 .67 73 .67 71 .99\nLS 4 .35 6 .15 4 .35 4 .67 4 .89 3 .61 87.54 85 .02 81 .95 79 .47 73 .66 71 .54\nERL 7 .16 6 .10 3 .74 3 .35 3 .42 2.96 87.67 84 .83 81 .96 79 .48 73 .63 72 .01\nV AT 9 .07 11 .28 7 .27 6 .76 10 .96 7 .92 87.61 85 .20 81 .65 79 .71 73 .71 72 .08\nMixup 5 .98 9 .02 4 .72 4 .21 4 .60 5 .18 87.49 84 .86 81 .97 79 .51 73 .88 71 .82\nM-mixup 5 .04 7 .78 6 .48 6 .68 7 .01 6 .07 87.40 84 .45 81 .77 79 .57 73 .67 72 .03\nOurs 3.69 4.43 3.24 3.04 3.03 3.42 87.44 84 .53 81 .59 79 .06 73 .71 72 .17\nTable 1: ECE and accuracy (in percentage). We report the average performance of 5 random\ninitializations.\nlevel α= 0.5. We ﬁnd that our model outperforms other baselines on all the datasets signiﬁcantly,\nwith only exceptions of ERL in ECE on Yahoo and ERL in misclassiﬁcation detection on 20NG.\nMisclassiﬁcation Detection OOD Detection\nData 20NG15 20NG WOS100 WOS Yahoo8 Yahoo 20NG15 20NG WOS100 WOS Yahoo 8 Yahoo\n( OOD ) 20NG5 SST-2 WOS34 AGnews Yahoo2 Yelp\nBERT 2 .30 2 .86 16 .53 20 .52 7 .47 8 .43 2.66 21 .65 23 .12 49 .84 8 .35 13 .88\nTS 6 .08 5 .74 21 .20 23 .76 10 .48 12 .74 6.62 32 .64 28 .12 53 .32 11 .55 20 .27\nMCDP 4 .37 5 .28 20 .44 24 .16 10 .12 10 .75 3.99 25 .10 27 .28 53 .52 9 .98 15 .93\nLS 4 .72 6 .75 20 .37 23 .56 11 .19 16 .15 5.70 41 .08 27 .12 58 .48 12 .02 19 .81\nERL 8 .54 10 .35 20 .49 25 .13 12 .89 15 .47 8.78 47 .00 27 .73 56 .67 13 .78 23 .47\nV AT 2 .52 3 .36 18 .70 19 .96 6 .54 10 .37 2.96 29 .62 23 .41 54 .60 7 .42 17 .65\nMixup 4 .99 4 .51 20 .65 24 .80 10 .75 11 .29 5.86 31 .84 26 .77 58 .02 11 .62 19 .84\nM-mixup 2 .16 3 .16 16 .94 19 .39 9 .09 11 .79 2.36 26 .08 24 .08 51 .39 10 .08 22 .41\nOurs 9.10 10.76 26.93 30.80 14.34 17.88 9.69 63.92 35.60 71.13 14.94 29.40\nTable 2: NBAUCC0.5 on misclassiﬁcation detection and OOD detection. We report the average\nperformance of 5 random initializations.\n4.5 Parameter Study\nWe investigate the eﬀects of the interpolation parameters for on-manifold data, i.e., δon and δy, and\nthe perturbation size for oﬀ-manifold samples, i.e., δoﬀ. The default values areδon = 10−4,δoﬀ = 10−3\nand δy = 0.1. Figure 4 shows the reuslts on 20NG15, 20NG, WOS100, and WOS datasets. Our results\nare summarized as follows:\n•The performance of all metrics versus δon is stable within a large range from 10−5 to 10−2. When\n10\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n /uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n /uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n /uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\nFigure 4: Parameter study of δon, δoﬀ and δy.\nδon is larger than 10−1, the predictive accuracy begins to drop.\n•The performance versus δoﬀ is more sensitive: (1) when δoﬀ is too small, ECE increases dramati-\ncally becasue the generated oﬀ-manifold samples are too close to the manifold and make the model\nunder-conﬁdent. (2) when δoﬀ is too large, the oﬀ-manifold regularization is too weak and OOD\ndetection performance drops.\n•In general, δon should be small to let x′stay on the data manifold while δoﬀ should be large to let\nx′′leave the data manifold. However, the regularization eﬀect of Ron (Roﬀ) depends on both λon\n(λoﬀ) and δon (δoﬀ). Therefore, it is not necessary to let δon be smaller than δoﬀ. We can also tune\nλon and λoﬀ to achieve better performance.\n•The performance versus δy is relatively stable except for the metric of ECE. When δy is larger\nthan 0.2, ECE begins to increase.\n4.6 Ablation Study\nWe investigate the eﬀectiveness of the on-manifold regularizerRon and the oﬀ-manifold regularizer\nRoﬀ via ablation studies. Table 3 shows the results on the 20NG15 and 20NG datasets.\n•As expected, removing either component in our method would result in a performance drop.\n11\nIt demonstrates that these two components complement each other. All the ablation models\noutperform the BERT baseline model, which demonstrates the eﬀectiveness of each module.\n•We observe that the optimal δon is diﬀerent when using only Ron. This indicates that the\nhyperparameters of Ron and Roﬀ should be jointly tuned, due to the joint eﬀect of both components.\n•By removing Roﬀ, we observe a severe OOD performance degradation on the 20NG dataset (from\n63.92 to 43.87). This indicates that Roﬀ is vital to out-of-distribution calibration. Meanwhile, the\nperformance degradation is less severe on 20NG15 (from 9.69 to 7.94). It is because Ron can also\nhelp detect the OOD samples from similar data sources. (20NG5).\n•By removing Ron, the in-distribution calibration performance drops as expected.\nDataset 20NG15 20NG\nModel δon Accuracy ECE OOD Mis Accuracy ECE OOD Mis\nBERT - 87.42 9 .24 2 .66 2 .30 84.55 11 .61 21 .65 2 .86\nw/ Roﬀ - 86.48 6 .51 6 .22 6 .09 83.90 7 .98 55 .40 7 .12\nw/ Ron 10−2 88.73 2 .77 7 .94 8 .08 85.60 5 .00 35 .80 8 .66\nw/ Ron 10−3 88.29 3 .52 7 .39 6 .83 85.69 4 .43 38 .00 9 .01\nw/ Ron 10−4 87.93 4 .48 5 .33 4 .83 85.12 6 .76 43 .87 5 .95\nw/ Ron 10−5 87.61 4 .69 3 .83 4 .73 85.39 6 .35 35 .70 5 .30\nw/ Both 10 −4 87.44 3 .69 9 .69 9 .10 84.53 4 .43 63 .92 10 .76\nTable 3: Ablation study on the 20NG15 and 20NG datasets. For OOD detection and misclassiﬁcation\ndetection, we report BAUCC0.5. We set δy = 0.1 and δoﬀ = 10−3.\n5 Related Works and Discussion\nOther Related Works: Lakshminarayanan et al. (2017) propose a model ensembling approach\nto improve model calibration. They ﬁrst train multiple models with diﬀerent initializations and\nthen average their predictions. However, ﬁne-tuning multiple language models requires extremely\nintensive computing resources.\nKumar et al. (2018) propose a diﬀerentiable surrogate for the expected calibration error, called\nmaximum mean calibration error (MMCE), using kernel embedding. However, such a kernel\nembedding method is computationally expensive and not scalable to the large pre-trained language\nmodels.\nAccelerating Optimization: To further improve the calibration performance of our method, we can\nleverage some recent minimax optimization techniques to better solve the two inner optimization\nproblems in (5) and (7) without increasing the computational complexity. For example, Zhang et al.\n(2019) propose an eﬃcient approximation algorithm based on Pontryagin’s Maximal Principle to\nreplace the multi-step projected gradient update for the inner optimization problem. Another\noption is the learning-to-learn framework (Jiang et al., 2018), where the inner problem is solved by\na learnt optimizer. These techniques can help us obtain x′and x′′more eﬃciently.\n12\nConnection to Robustness: The interpolated training samples can naturally promote the local\nLipschitz continuity of our model. Such a local smoothness property has several advantages: (1) It\nmakes the model more robust to the inherent noise in the data,e.g., noisy labels; (2) it is particularly\nhelpful to prevent overﬁtting and improve generalization, especially for low-resource tasks.\nExtensions: Our method is quite general and can be applied to other deep neural network-based\nproblems besides language model ﬁne-tuning.\n6 Conclusion\nWe have proposed a regularization method to mitigate miscalibration of ﬁne-tuned language\nmodels from a data augmentation perspective. Our method imposes two new regularizers using\ngenerated on- and oﬀ- manifold samples to improve both in-distribution and out-of-distribution\ncalibration. Extensive experiments on six datasets demonstrate that our method outperforms state-\nof-the-art calibration methods in terms of expected calibration error, misclassiﬁcation detection\nand OOD detection.\nAcknowledgement\nThis work was supported in part by the National Science Foundation award III-2008334, Amazon\nFaculty Award, and Google Faculty Award.\nReferences\nBlundell, C., Cornebise, J., Kavukcuoglu, K. and Wierstra, D. (2015). Weight uncertainty in\nneural network. In International Conference on Machine Learning.\nChang, M.-W., Ratinov, L., Roth, D. and Srikumar, V. (2008). Importance of semantic represen-\ntation: Dataless classiﬁcation. In Proceedings of the Twenty-Third AAAI Conference on Artiﬁcial\nIntelligence.\nChouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism\nprediction instruments. Big data, 5 153–163.\nDevlin, J., Chang, M.-W., Lee, K. and Toutanova, K.(2019). Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers).\nGal, Y. and Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model\nuncertainty in deep learning. In International Conference on Machine Learning.\nGal, Y., Islam, R. and Ghahramani, Z. (2017). Deep bayesian active learning with image data. In\nInternational Conference on Machine Learning.\n13\nGilmer, J., Metz, L., Faghri, F ., Schoenholz, S. S., Raghu, M., Wattenberg, M., Goodfellow, I.\nand Brain, G. (2018). The relationship between high-dimensional geometry and adversarial\nexamples. arXiv preprint arXiv:1801.02774.\nGuo, C., Pleiss, G., Sun, Y. and Weinberger, K. Q. (2017). On calibration of modern neural\nnetworks. In International Conference on Machine Learning.\nHendrycks, D. and Gimpel, K. (2016). A baseline for detecting misclassiﬁed and out-of-distribution\nexamples in neural networks. In International Conference on Learning Representations.\nJiang, H., Chen, Z., Shi, Y., Dai, B. and Zhao, T. (2018). Learning to defense by learning to attack.\narXiv preprint arXiv:1811.01213.\nJiang, H., He, P ., Chen, W., Liu, X., Gao, J. and Zhao, T. (2020). SMART: Robust and eﬃcient ﬁne-\ntuning for pre-trained natural language models through principled regularized optimization. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\nKim, Y. (2014). Convolutional neural networks for sentence classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).\nKingma, D. P .and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKowsari, K., Brown, D. E., Heidarysafa, M., Jafari Meimandi, K., , Gerber, M. S. and Barnes,\nL. E. (2017). Hdltex: Hierarchical deep learning for text classiﬁcation. In IEEE International\nConference on Machine Learning and Applications (ICMLA).\nKumar, A., Sarawagi, S. and Jain, U. (2018). Trainable calibration measures for neural networks\nfrom kernel mean embeddings. In International Conference on Machine Learning.\nLakshminarayanan, B., Pritzel, A. and Blundell, C. (2017). Simple and scalable predictive\nuncertainty estimation using deep ensembles. In Advances in Neural Information Processing\nSystems.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P .and Soricut, R. (2020). Albert: A lite bert\nfor self-supervised learning of language representations. In International Conference on Learning\nRepresentations.\nhttps://openreview.net/forum?id=H1eA7AEtvS\nLee, K. , Lee, K. , Lee, H. and Shin, J. (2018). A simple uniﬁed framework for detecting out-\nof-distribution samples and adversarial attacks. In Advances in Neural Information Processing\nSystems.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and\nStoyanov, V.(2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint\narXiv:1907.11692.\n14\nLouizos, C. and Welling, M. (2017). Multiplicative normalizing ﬂows for variational Bayesian\nneural networks. In International Conference on Machine Learning.\nMiyato, T., Maeda, S.-i., Koyama, M. and Ishii, S. (2018). Virtual adversarial training: a regulariza-\ntion method for supervised and semi-supervised learning. IEEE transactions on pattern analysis\nand machine intelligence, 41 1979–1993.\nM¨uller, R., Kornblith, S. and Hinton, G. E. (2019). When does label smoothing help? In Advances\nin Neural Information Processing Systems.\nNaeini, M. P ., Cooper, G. F .and Hauskrecht, M. (2015). Obtaining well calibrated probabili-\nties using bayesian binning. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial\nIntelligence.\nNiculescu-Mizil, A. and Caruana, R. (2005). Predicting good probabilities with supervised\nlearning. In International Conference on Machine Learning.\nPereyra, G., Tucker, G., Chorowski, J., Kaiser, Ł. and Hinton, G. (2017). Regularizing neural\nnetworks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P . J.\n(2019). Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\nShen, Y., Yun, H., Lipton, Z. C., Kronrod, Y. and Anandkumar, A. (2018). Deep active learning for\nnamed entity recognition. In International Conference on Learning Representations.\nSiddhant, A. and Lipton, Z. C. (2018). Deep bayesian active learning for natural language\nprocessing: Results of a large-scale empirical study. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing.\nSocher, R., Bengio, Y. and Manning, C. D. (2012). Deep learning for nlp (without magic). In\nTutorial Abstracts of ACL 2012.\nStutz, D., Hein, M. and Schiele, B. (2019). Disentangling adversarial robustness and generalization.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\nThulasidasan, S., Chennupati, G., Bilmes, J. A., Bhattacharya, T.and Michalak, S. (2019). On\nmixup training: Improved calibration and predictive uncertainty for deep neural networks. In\nAdvances in Neural Information Processing Systems.\nVerma, V., Lamb, A. , Beckham, C. , Najafi, A. , Mitliagkas, I. , Lopez-Paz, D. and Bengio, Y.\n(2019). Manifold mixup: Better representations by interpolating hidden states. In International\nConference on Machine Learning.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F ., Levy, O. and Bowman, S.\n(2019). Superglue: A stickier benchmark for general-purpose language understanding systems.\nIn Advances in Neural Information Processing Systems.\n15\nWang, A., Singh, A., Michael, J., Hill, F ., Levy, O. and Bowman, S. R. (2018). Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. InInternational Conference\non Learning Representations.\nWilcoxon, F .(1992). Individual comparisons by ranking methods. In Breakthroughs in statistics.\nSpringer, 196–202.\nWong, E., Rice, L. and Kolter, J. Z. (2019). Fast is better than free: Revisiting adversarial training.\nIn International Conference on Learning Representations.\nZhang, D., Zhang, T., Lu, Y., Zhu, Z. and Dong, B. (2019). You only propagate once: Accelerating\nadversarial training via maximal principle. In Advances in Neural Information Processing Systems.\nZhang, H., Cisse, M., Dauphin, Y. N. and Lopez-Paz, D. (2018). mixup: Beyond empirical risk\nminimization. In International Conference on Learning Representations.\nZhang, X., Zhao, J. and LeCun, Y. (2015). Character-level convolutional networks for text classiﬁ-\ncation. In Advances in neural information processing systems.\n16\nA Dataset Details\n#Train #Dev #Test #Label\n20NG15 7010 1753 5833 15\n20NG5 - - 1699 5\n20NG 9051 2263 7532 20\nSST-2 - - 1822 2\nWOS100 16794 4191 13970 100\nWOS34 - - 4824 34\nWOS 22552 5639 18794 134\nAGnews - - 7600 4\nYahoo8 16000 4000 48000 8\nYahoo2 - - 12000 2\nYahoo 20000 5000 60000 10\nYelp - - 38000 2\nTable 4: Dataset statistics and dataset split. ’-’ denotes that this part is not used. The original Yahoo\ndataset contains 140,000 training samples for each class which is too large; we randomly draw\n2,000 and 500 samples for each class as our training and development set.\nAll the data are publicly available. We also oﬀer the links to the data as follows:\n1. 20NG: http://qwone.com/~jason/20Newsgroups/.\n2. SST-2: https://nlp.stanford.edu/sentiment/index.html.\n3. WOS: https://data.mendeley.com/datasets/9rw3vkcfy4/2.\n4. AGnews: https://github.com/yumeng5/WeSTClass.\n5. Yahoo: https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset.\n6. Yelp: https://github.com/yumeng5/WeSTClass.\nB Experiment Details\nWe use ADAM (Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.999 as the optimizer in all the\ndatasets. We use the learning rate of 5×10−5 and batch size 32 except 1 ×10−5 and 16 for Yahoo8\nand Yahoo. We set the maximum number of epochs to 5 in Yahoo8 and Yahoo and 10 in the other\ndatasets. We use the dropout rate of 0.1 as in (Devlin et al., 2019). The documents are tokenized\nusing wordpieces and are chopped to spans no longer than 150 tokens on 20NG15 and 20NG and\n256 on other datasets..\nHyper-parameters: For our method, we use λon = λoﬀ = 1, δon = 10−4, δoﬀ = 10−3 and δy = 0.1 for\nall the datasets. We then conduct an extensive hyper-parameter search for the baselines: for label\n17\nsmoothing, we search the smoothing parameter from {0.05,0.1}as in (M¨uller et al., 2019); for ERL,\nthe penalty weight is chosen from {0.05,0.1,0.25,0.5,1,2.5,5}; for V AT, we search the perturbation\nsize in {10−3,10−4,10−5}as in (Jiang et al., 2020); for Mixup, we search the interpolation parameter\nfrom {0.1,0.2,0.3,0.4}as suggested in (Zhang et al., 2018; Thulasidasan et al., 2019); for Manifold-\nmixup, we search from {0.2,0.4,1,2,4}. We perform 10 stochastic forward passes for MCDP at test\ntime. For hyper-parameter tuning, we run all the methods 5 times and then take the average. The\nhyper-parameters are selected to get the best ECE on the development set of each dataset. The\ninterpolation of Mixup is performed on the input embeddings obtained from the ﬁrst layer of the\nlanguage model; the interpolation of Manifold-mixup is performed on the features obtained from\nthe last layer of the language model.\nC Metrics of Misclassiﬁcation and Out-of-distribution detection\nExisting works on out-of-distribution (OOD) detection and misclassiﬁcation detection (Hendrycks\nand Gimpel, 2016) use traditional binary classiﬁcation metrics, e.g., AUPR and AUROC. As we\ndiscussed in Section 1 and 2, the output probability of a calibrated model should reﬂect the true\nlikelihood. However, AUROC and AUPR cannot reﬂect true model calibration because the model\ncan still achieve high scores even though it has high conﬁdences for misclassiﬁed and OOD samples.\nWe argue that it is more reasonable to use the Normalized Bounded Area Under the Calibration\nCurve (NBAUCC) deﬁned as in Section 4.\nModel Conﬁdence Optimal τ AUPR AUROC NBAUCC1 NBAUCC0.5xin,1 xin,2 xout,1 xout,2\nh1 (Miscalibrated) 0.9 0.95 0.8 0.85 (0.85,0.9) 0.417 1 0.145 0\nh2 (Well-calibraterd) 0.9 0.95 0.1 0.15 (0.15,0.9) 0.417 1 0.845 0.773\nTable 5: NBAUCC vs. AUROC/AUPR\nTable 5 shows an illustrative example. As can be seen, h1 is better calibrated than h2, since h1\ncan detect OOD samples under a wide range of threshold (0 .15 < τ <0.9) while h2 requires\nan absurdly large threshold (0 .85 < τ <0.9). However, if we use the traditional AUPR and\nAUROC metrics, we will conclude that h1 is as well calibrated as h2 since AUPRh1 = AUPRh2\n= 0.417 and AUROCh1 = AUROCh2 = 1. On the other hand, if we use NBAUCC, we will have\nNBAUCCh1\n1 = 0.845 >NBAUCCh1\n1 = 0.145, or NBAUCCh1\n0.5 = 0.773 >NBAUCCh1\n0.5 = 0 which can\nreﬂect the true calibration of the two classiﬁers.\nWe remark that it is more appropriate to use NBAUCC0.5 than NBAUCC1 since a calibrated\nmodel should provide low conﬁdences for the misclassiﬁed and OOD samples and it is unreasonable\nto use a large threshold to detect them.\nD Additional Results\nTable 6 and 7 report the NBAUCCs of all the methods on misclassiﬁcation and OOD detection when\nτupper = 0.7 and τupper = 1. Table 8 and 9 report the ablation study results of all the methods when\n18\nτupper = 0.7 and τupper = 1. Figure 5 and 6 report the parameter study results of all the methods\nwhen τupper = 0.7 and τupper = 1.\nMisclassiﬁcation Detection OOD Detection\nData 20NG15 20NG WOS100 WOS Yahoo8 Yahoo 20NG15 20NG WOS100 WOS Yahoo 8 Yahoo\n( OOD ) 20NG5 SST-2 WOS34 AGnews Yahoo2 Yelp\nBERT 17.86 18.48 35.84 39.08 28.83 29.67 13.52 42.86 40.04 59.42 26.63 38.30\nTS 23.74 23.58 38.34 40.76 31.10 32.63 19.74 50.00 42.96 60.70 28.30 42.07\nMCDP 23.58 24.58 38.54 41.20 31.43 32.57 16.82 44.96 42.74 60.72 27.47 39.83\nLS 21.22 23.24 37.22 40.12 30.93 34.30 18.76 55.24 42.54 63.62 27.87 40.77\nERL 24.04 25.68 37.87 41.17 32.27 33.90 22.10 54.20 42.67 62.10 28.73 43.37\nV AT 17.80 17.50 35.90 38.80 27.87 31.13 13.00 49.00 40.30 62.50 25.80 40.63\nMixup 21.42 21.86 37.72 40.92 30.97 32.97 16.70 50.94 42.13 62.98 28.00 44.57\nM-mixup 17.86 19.24 36.48 38.33 29.67 31.50 14.06 44.56 41.51 61.30 27.43 44.20\nOurs 26.50 28.10 40.93 43.70 33.07 35.13 23.20 66.36 46.73 68.10 29.70 46.43\nTable 6: NBAUCC1 on misclassiﬁcation detection and OOD detection. We report the average\nperformance of 5 random initializations.\nMisclassiﬁcation Detection OOD Detection\nData 20NG15 20NG WOS100 WOS Yahoo8 Yahoo 20NG15 20NG WOS100 WOS Yahoo 8 Yahoo\n( OOD ) 20NG5 SST-2 WOS34 AGnews Yahoo2 Yelp\nBERT 8.26 8.70 26.95 31.18 18.52 19.46 7.05 33.24 32.97 57.45 18.86 27.68\nTS 14.60 13.72 31.73 33.89 22.32 24.61 12.91 43.55 37.84 59.86 22.17 34.03\nMCDP 13.14 14.21 31.05 34.74 21.41 22.62 9.85 36.96 36.97 60.06 19.99 29.45\nLS 12.45 14.24 30.92 33.51 22.94 27.52 11.63 49.60 36.04 65.28 22.38 33.00\nERL 17.92 20.04 30.83 35.26 25.07 27.34 15.43 55.69 36.69 61.93 24.07 36.74\nV AT 8.44 9.66 29.39 30.57 17.23 21.74 7.26 41.35 32.56 60.81 17.64 31.17\nMixup 13.33 11.87 31.71 35.24 22.62 22.80 11.50 43.60 37.09 65.51 22.19 33.66\nM-mixup 8.67 9.89 27.33 29.61 20.33 23.05 7.18 37.10 33.57 58.13 20.66 36.42\nOurs 18.35 20.18 36.63 40.01 25.94 29.15 16.55 68.72 43.40 72.62 25.03 41.11\nTable 7: NBAUCC0.7 on misclassiﬁcation detection and OOD detection. We report the average\nperformance of 5 random initializations.\n19\nDataset 20NG15 20NG\nModel δon Accuracy ECE OOD Mis Accuracy ECE OOD Mis\nBERT - 87.42 9.24 13.52 17.86 84.55 11.61 42.86 18.48\nw/ Roﬀ - 86.48 6.51 18.10 24.53 83.90 7.98 63.73 25.40\nw/ Ron 10−2 88.73 2.77 22.83 27.40 85.60 5.00 51.53 27.40\nw/ Ron 10−3 88.29 3.52 21.03 24.13 85.69 4.43 53.87 26.30\nw/ Ron 10−4 87.93 4.48 17.43 21.63 85.12 6.76 57.47 21.93\nw/ Ron 10−5 87.61 4.69 15.73 21.43 85.39 6.35 52.07 21.63\nw/ Both 10 −4 87.44 3.69 23.20 26.50 84.53 4.43 66.36 28.10\nTable 8: Ablation study on the 20NG15 and 20NG datasets. For OOD detection and misclassiﬁcation\ndetection, we report NBAUCC1. We set δy = 0.1 and δoﬀ = 10−3.\nDataset 20NG15 20NG\nModel δon Accuracy ECE OOD Mis Accuracy ECE OOD Mis\nBERT - 87.42 9.24 7.05 8.26 84.55 11.61 33.24 8.70\nw/ Roﬀ - 86.48 6.51 11.75 14.79 83.90 7.98 62.67 15.42\nw/ Ron 10−2 88.73 2.77 15.27 18.35 85.60 5.00 46.67 18.39\nw/ Ron 10−3 88.29 3.52 13.86 15.66 85.69 4.43 50.07 18.17\nw/ Ron 10−4 87.93 4.48 10.61 12.59 85.12 6.76 53.64 13.18\nw/ Ron 10−5 87.61 4.69 8.71 12.25 85.39 6.35 46.24 12.20\nw/ Both 10 −4 87.44 3.69 16.55 18.35 84.53 4.43 68.72 20.18\nTable 9: Ablation study on the 20NG15 and 20NG datasets. For OOD detection and misclassiﬁcation\ndetection, we report NBAUCC0.7. We set δy = 0.1 and δoﬀ = 10−3.\n20\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018\n/uni00000018/uni00000013/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\nFigure 5: Parameter study of δon, δoﬀ and δy. We use NBAUCC1 for OOD and misclassiﬁcation\ndetection.\n21\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000014/uni00000013\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000014\non\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000014/uni00000013\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n /uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000014/uni00000013\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\nFigure 6: Parameter study of δon, δoﬀ and δy. We use NBAUCC0.7 for OOD and misclassiﬁcation\ndetection.\n22",
  "topic": "Regularization (linguistics)",
  "concepts": [
    {
      "name": "Regularization (linguistics)",
      "score": 0.7930406928062439
    },
    {
      "name": "Manifold (fluid mechanics)",
      "score": 0.6744319200515747
    },
    {
      "name": "Calibration",
      "score": 0.6357154846191406
    },
    {
      "name": "Computer science",
      "score": 0.6249278783798218
    },
    {
      "name": "Interpolation (computer graphics)",
      "score": 0.549224853515625
    },
    {
      "name": "Smoothness",
      "score": 0.5153098106384277
    },
    {
      "name": "Distribution (mathematics)",
      "score": 0.44779491424560547
    },
    {
      "name": "Manifold alignment",
      "score": 0.4256078004837036
    },
    {
      "name": "Data point",
      "score": 0.41398096084594727
    },
    {
      "name": "Algorithm",
      "score": 0.38736507296562195
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3689528703689575
    },
    {
      "name": "Nonlinear dimensionality reduction",
      "score": 0.25749000906944275
    },
    {
      "name": "Mathematics",
      "score": 0.23491773009300232
    },
    {
      "name": "Statistics",
      "score": 0.1950359046459198
    },
    {
      "name": "Mathematical analysis",
      "score": 0.06001269817352295
    },
    {
      "name": "Dimensionality reduction",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Motion (physics)",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}