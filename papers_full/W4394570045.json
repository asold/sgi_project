{
    "title": "TrajectoFormer: Transformer-Based Trajectory Prediction of Autonomous Vehicles with Spatio-temporal Neighborhood Considerations",
    "url": "https://openalex.org/W4394570045",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5005199035",
            "name": "Farhana Amin",
            "affiliations": [
                "Rajshahi University of Engineering and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5014702176",
            "name": "Kanchon Gharami",
            "affiliations": [
                "Rajshahi University of Engineering and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5088943920",
            "name": "Barshon Sen",
            "affiliations": [
                "Rajshahi University of Engineering and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2803184913",
        "https://openalex.org/W2424778531",
        "https://openalex.org/W2891058410",
        "https://openalex.org/W2142606050",
        "https://openalex.org/W2580495915",
        "https://openalex.org/W4221140095",
        "https://openalex.org/W2963906196",
        "https://openalex.org/W4205222881",
        "https://openalex.org/W2097545165",
        "https://openalex.org/W2600255062",
        "https://openalex.org/W4313478129",
        "https://openalex.org/W6681158678",
        "https://openalex.org/W4246464108",
        "https://openalex.org/W2016289761",
        "https://openalex.org/W3147254695",
        "https://openalex.org/W3136121014",
        "https://openalex.org/W2607296803",
        "https://openalex.org/W2964193755",
        "https://openalex.org/W2897179324",
        "https://openalex.org/W2784715585",
        "https://openalex.org/W6600751047",
        "https://openalex.org/W3207981904",
        "https://openalex.org/W4205342285",
        "https://openalex.org/W4385517014",
        "https://openalex.org/W4220865846",
        "https://openalex.org/W2975767248",
        "https://openalex.org/W3160582233",
        "https://openalex.org/W3105115779",
        "https://openalex.org/W3104946437"
    ],
    "abstract": "Abstract Accurate trajectory prediction of autonomous vehicles is crucial for ensuring road safety. Predicting precise and accurate trajectories is still considered a challenging problem because of the intricate spatio-temporal dependencies among the vehicles. Our study primarily focuses on resolving this issue by introducing a comprehensive system called ‚ÄúTrajectoFormer‚Äù, which can effectively represent the spatio-temporal dependency between vehicles. In this system, we have conducted preprocessing on the NGSIM dataset by constructing an 8-neighborhood for each vehicle that represents the spatio-temporal dependency between vehicles effectively. Second, we have deployed a transformer network that captures dependencies between the target vehicle and its neighbor from the constructed neighborhood and predicts future trajectories for the target vehicle with notably reduced training times and significant accuracy compared to existing methods. Experiments on both NGSIM US-101 and US-I80 show that our proposed approach outperforms the other benchmarks in terms of showing low RMSE value for the 5-s prediction horizon of trajectory prediction. Our conducted ablation study also underscores the effectiveness of each component of our proposed TrajectoFormer model relative to traditional time-series prediction models.",
    "full_text": "Vol.:(0123456789)\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \nhttps://doi.org/10.1007/s44196-024-00410-1\nRESEARCH ARTICLE\nTrajectoFormer: Transformer‚ÄëBased Trajectory Prediction \nof¬†Autonomous Vehicles with¬†Spatio‚Äëtemporal Neighborhood \nConsiderations\nFarhana¬†Amin1 ¬†¬∑ Kanchon¬†Gharami1¬†¬∑ Barshon¬†Sen1\nReceived: 14 October 2023 / Accepted: 9 January 2024 \n¬© The Author(s) 2024\nAbstract\nAccurate trajectory prediction of autonomous vehicles is crucial for ensuring road safety. Predicting precise and accurate tra-\njectories is still considered a challenging problem because of the intricate spatio-temporal dependencies among the vehicles. \nOur study primarily focuses on resolving this issue by introducing a comprehensive system called ‚ÄúTrajectoFormer‚Äù, which \ncan effectively represent the spatio-temporal dependency between vehicles. In this system, we have conducted preprocessing \non the NGSIM dataset by constructing an 8-neighborhood for each vehicle that represents the spatio-temporal dependency \nbetween vehicles effectively. Second, we have deployed a transformer network that captures dependencies between the target \nvehicle and its neighbor from the constructed neighborhood and predicts future trajectories for the target vehicle with nota-\nbly reduced training times and significant accuracy compared to existing methods. Experiments on both NGSIM US-101 \nand US-I80 show that our proposed approach outperforms the other benchmarks in terms of showing low RMSE value for \nthe 5-s prediction horizon of trajectory prediction. Our conducted ablation study also underscores the effectiveness of each \ncomponent of our proposed TrajectoFormer model relative to traditional time-series prediction models.\nKeywords Autonomous vehicle¬†¬∑ Transformer¬†¬∑ Trajectory prediction¬†¬∑ Automobile¬†¬∑ NGSIM¬†¬∑ Spatio-temporal dependency\n1 Introduction\nArtificial intelligence has enabled autonomous vehicles \n(AVs) to navigate between destinations without the assis-\ntance of a human driver. Though numerous studies on vari-\nous fields of autonomous driving have been conducted over \nthe past few years, without high situational awareness and a \ncomprehensive understanding of the environment, it will be \ndifficult to adopt autonomous driving completely. Observing \nthe trajectory of the nearby vehicles can help to offer an \nanticipatory evaluation of the driving conditions around the \nego vehicle (the car that is under the autonomous driving \nsystem‚Äôs control is the ego vehicle), which can help in the \nprecise trajectory prediction of autonomous vehicles and \nprevent potential hazards to safe driving.\nIn broad terms, trajectory prediction refers to forecasting \na target vehicle‚Äôs future motion based on the target vehicle‚Äôs \nand its nearby vehicles‚Äôprevious motions. In general, antici-\npating the target vehicle‚Äôs future coordinate values involves \nlooking at its and its nearby vehicles‚Äô historical coordinate \nvalues. The goal of trajectory prediction is to create a future \nroute by considering the past route of the ego vehicle and its \nsurrounding vehicles. To be precise, the challenge of fore-\ncasting the short-term (1‚Äì3¬†s) and long-term (3‚Äì5¬†s) spa-\ntial coordinates of different road agents, such as vehicles, \nbuses, pedestrians, rickshaws, and other vehicles, is known \nas trajectory prediction. Our study mainly concentrated on \ntrajectory prediction for autonomous vehicles.\nOne crucial characteristic of AVs is the capacity to pre-\ndict the vehicles‚Äô future paths effectively in terms of tra -\njectory prediction. Since traffic agents may influence one \nFarhana Amin and Kanchon Gharami have contributed equally to \nthis work.\n * Farhana Amin \n 400fjemin@gmail.com\n Kanchon Gharami \n kanchon2199@gmail.com\n Barshon Sen \n barshon.sen@cse.ruet.ac.bd\n1 Department of¬†Computer Science and¬†Engineering, \nRajshahi University of¬†Engineering and¬†Technology, Kazla, \nRajshahi¬†6204, Bangladesh\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 2 of 20\nanother‚Äôs behavior, particularly in highly interactive driving \nscenarios, it is necessary for the prediction model to capture \nthe social interaction among agents in the scene to produce \nsocially acceptable and accurate trajectory predictions. The \ndifficulty of trajectory prediction arises from the fact that \ndriving is a complex interaction activity in which a vehicle‚Äôs \ntrajectory is influenced by its driving style and surroundings. \nAlso, the number of vehicles present in the surroundings \nmight vary depending on the traffic conditions. Autono-\nmous vehicles‚Äô ability to accurately anticipate other traffic \nparticipants‚Äô behavior is crucial for their safe and efficient \nnavigation through complicated traffic conditions. Although \nadvanced research has focused on interaction-aware trajec-\ntory prediction considering the effect of many nearby vehi-\ncles, most autonomous vehicles can not detect the motion \nstatus of other vehicles at a distance without communication \ntechnologies in a mixed traffic scenario. Thus, our work con-\ncentrates on reflecting the traffic interaction between the ego \nvehicle and its surrounding vehicles to predict computation-\nally efficient and accurate trajectories.\nVehicle trajectory prediction has received a lot of atten-\ntion from researchers so far. Various approaches have been \nused in literature for the task of predicting trajectories. \nLSTM [1]-based model [2], social LSTM [3] were, respec-\ntively, introduced for predicting the trajectory of autonomous \nvehicles and pedestrians using past locations of the target \nand its neighbors. Deep neural network [4 ]-based learn-\nable end-to-end models, [5, 6] were also proposed to reason \nabout both high-level behavior and long-term trajectories. \nThese end-to-end trajectory prediction models employ a \nsequence-to-sequence CNN [7 ] architecture where a fully \nconnected layer is used for embedding trajectory histories, \nand to consistently learn about temporal dependencies, \nstacked convolutional layers are employed. By combining \nexisting techniques with the Markov [8] model and genera-\ntive adversarial learning [9 ], trajectory prediction models \nhave recently advanced. The inter-vehicular dependencies, \nhowever, have not been seriously considered in these mod-\nels, and there is much scope for improvement in terms of \nprediction accuracy.\nIntelligent vehicles‚Äô decision-making and path-plan-\nning depend heavily on precisely modeling interactions \nand forecasting the trajectories of nearby vehicles. Real-\nizing the importance of vehicle interaction dependency, \n[10] developed a unique ensemble learning-based frame-\nwork to enhance the accuracy of trajectory predictions \nin interactive settings. [11] also suggested an LSTM \nencoder‚Äìdecoder model for learning interdependencies in \nvehicle motion that leverages convolutional social pool-\ning for trajectory prediction tasks. Though these studies \nconsider interaction dependencies, questions remain, such \nas what the distance range is for considering interaction \ndependencies, which neighboring vehicles are affecting \nthe ego vehicle‚Äôs movement, etc. Therefore, more insights \nare needed to capture spatio-temporal dependencies and \nincrease autonomous vehicles‚Äô long- and short-term pre-\ndiction accuracy.\nCapturing inter-vehicular dependency is a crucial \ntask to improve the prediction accuracy of autonomous \nvehicles. The task is also harder because the ego vehicle \nis spatially and temporally dependent on its neighbors, \nindicating that its movements are intricately linked to its \nneighbors‚Äô movements in terms of its relative positions \n(spatial) and its interactions with them over time.\nInspired by the above research gaps, a comprehen-\nsive transformer-based model incorporate with custom \ndesigned preprocessing strategy, at a whole referred \nas‚ÄúTrajectoFormer‚Äù system, is proposed for long-term \ntrajectory prediction of autonomous vehicles. Our model \nforecasts a vehicle‚Äôs trajectory over a fixed horizon utiliz-\ning the target vehicle‚Äôs interaction with its neighboring \nvehicles. Our study aims at predicting a vehicle‚Äôs trajec -\ntory over a time period of 5¬†s using only 3¬†s of its past \nhistorical trajectory data. Regardless of whether other fac-\ntors influence the predicted vehicle‚Äôs behavior, its histori-\ncal motion trajectory always exists objectively, and hence, \nin this study, we have considered the historical data of \nboth the target vehicle and its surrounding vehicles. Also, \nto consider interaction dependency, a neighborhood of 8 \nneighbors has been considered for each ego vehicle. The \nmain contributions of this study are briefly summarized \nas follows:\n1. As vehicles do not operate in isolation, the behavior \nof neighboring entities deeply influences their movements. \nConsequently, a distinguishing feature of the traffic situation \nis the intricate spatio-temporal relationships between entities \ndepicted in Fig.¬† 1. Our prediction model accounts for the \nprecise consideration of these spatio-temporal dependencies \nby constructing a neighborhood for each target vehicle to \nproduce reliable predictions.\n2. Our trajectory prediction system TrajectoFormer‚Äôs cor-\nnerstone lies in its ability to forecast a vehicle‚Äôs trajectory \nover a fixed horizon. More formally, given the past trajectory \nTpast that spans a duration of 3.2¬†s, the model is trained for \npredicting the future trajectory Tfuture  for the following 4.8¬†s.\nThe remainder of this paper is organized as follows: \nSect.¬† 3 presents an overview of the system architecture, \nSect.¬†4 depicts the problem formulation for our trajectory \nprediction task, Sect.¬† 5 demonstrates the detailed preproc-\nessing stage, and Sect.¬† 6 describes the steps of the method \ndevelopment for TrajectoFormer. In Sect.¬† 7, an ablation \nresearch is conducted to evaluate the model‚Äôs engineering \nindependently in comparison to others that undergo the same \npreprocessing. Experimental analysis and comprehensive \nstudies with benchmarks are discussed in Sect.¬† 8, and con-\ncluding remarks are drawn in Sect.¬†9.\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 3 of 20    87 \n2  Related Works\nDifferent kinds of classification strategies for autonomous \nvehicle‚Äôs trajectory prediction have been followed through-\nout the literature of AV [12]. The present methodologies, \nprimarily used in this study field, may be divided into \nmodel-based and data-driven approaches [13, 14]. Most \nmodel-based approaches detect typical driving actions, \nsuch as switching lanes, making a left or right turn, estimat-\ning the vehicle‚Äôs maximum turning speed, etc [15]. On the \nother hand, data-driven approaches often include using a \nsignificant amount of training data to educate a black box \nmodel (typically a neural network-based model) [16]. Once \nthe model has been trained, it may be used to predict future \nbehavior based on historical data. The models developed \nusing these approaches can be sectioned into physics-based, \nmaneuver-based, and interaction-aware motion models.\nUsing inputs like steering angle, acceleration rate, vehi-\ncle mass, and even the coefficient of friction between the \ntires and the road surface, physical-based motion models \nuse physical principles to assess a vehicle‚Äôs trajectory and \nanticipate its final location and direction. Kalman filters [17] \nand Monte Carlo sampling [18] are two examples of physics-\nbased techniques. Manuever-based motion models attempt to \npredict the sequence of actions that vehicles perform while \nmoving. But, they make the assumption that each vehicle \nmakes decisions independent of the others on the road. \nThese models attempt to identify such motions as soon as \nthey occur, project their continuance into the near future, and \nthen forecast the associated trajectories. The most compre-\nhensive group of models is interaction-aware motion models, \nwhich imply that a vehicle‚Äôs movements are influenced by \nadjacent road users. Dynamic Bayesian networks [19] or \nprototype trajectories are used in these models. Rule-based \nsystems [20], dynamically connected hidden Markov mod-\nels [21], and coupled hidden Markov models are examples \nof interaction-based models. Inter-agent dependencies are \nconsidered in these models, increasing comprehension of \nthe scenario. Interaction-aware models provide longer-term \nforecasts than physics-based models and are more reliable \nthan maneuver-based models as they frequently have to cal-\nculate all possible pathways [22].\nFurthermore, in terms of methodologies primarily applied \nin these models, the current methodologies can be classi-\nfied into three categories: methods relying on neural net-\nworks, methods relying on stochastic processes, and mixed \nmethods. Many trajectory prediction frameworks are built \non deep neural networks, including convolutional neural net-\nworks (CNNs), recurrent neural networks (RNNs) [23], long \nshort-term memory (LSTM), networks, or a mix of them, \nfor neural network-based prediction. One of the earliest effi-\ncient neural network models DESIRE [24], is a generative \nsystem with the purpose of predicting the future positions of \nmany interacting actors in dynamic (driving) settings. Using \nmulti-modal prediction, it can predict many outcomes from \nthe same inputs. It also contains scene atmosphere and traf-\nfic participant interactions employing a computationally \nefficient end-to-end neural network model. [25] proposed \nan effective data-driven trajectory prediction framework \nbased on an LSTM that uses a tremendous amount of tra-\njectory data to train the vehicles‚Äô complex behavior. Based \non the coordinates and speeds of the adjacent automobiles \nas inputs, the LSTM creates probabilistic information about \nthe future placements of the traffic participants on an occu-\npancy grid map.\nMany methods have been introduced relying on stochastic \nprocesses. [26] describes a stochastic process for trajectory \nprediction of autonomous vehicles in a congested city. New \nYork and San Francisco automobile movements were stud-\nied for 1000¬†h. Comparing a vehicle‚Äôs current location to \na massive database of surrounding movements, its future \nlocation was predicted. Additional samples improved this \nnon-parametric technique, removing the need for complex \nmodels. Another stochastic framework proposed by [27] \nincludes three interconnected modules: a hidden Markov \nmodel (HMM) maneuver recognition module, a trajectory \nprediction module using motion-based interaction models \nand maneuver-specific variational Gaussian mixture models, \nand a vehicle interaction module that manages scene context \nand minimizes energy for final predictions.\nAlso, there are a few methods combining different \nmethods for trajectory prediction. Some literature [28, \n29] demonstrated how deep learning and mixture models \nmay be used to predict trajectories. These models used a \nFig. 1  The illustration of inter-\ndependency between vehicle‚Äôs \nmotion. The yellow ego vehicle \nis an AV, and the rest are the \nsurrounding vehicles whose \nposition impacts the ego vehi-\ncle‚Äôs movement\n\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 4 of 20\nmixture density network (MDN) in the neural model and \nunivariate Gaussian distributions in the mixture model \nfor multi-task learning. Also, literature [30] employed a \nmethodology that relied on vehicle kinematics and mini-\nmizing a cost function. The Monte Carlo-based sampling \ntechnique and Langevin sampling were utilized to produce \nbetter forecasts with more stability, and neural networks \nwere used to expand the cost functions in order to combine \nthe benefits of model-based and model-free learning.\nIn contrast to these methods, we propose a spatio-tem-\nporal neighborhood-aware model that considers the tar -\nget vehicle‚Äôs interaction with its neighborhood to learn \nabout the ego vehicle‚Äôs motion dynamics. Instead of \nsimply combining motion encoding and implicitly allow -\ning the decoder to learn their relationship, we employ a \ntransformer-based network to capture complex temporal \ninteractions and spatial relationships between vehicles. \nAfter that, positional encoding is added to the embedding \nto make sequence order more significant for sequential \nprocessing. Attention-based encoder‚Äìdecoder Blocks with \nmulti-head attention mechanisms and feed-forward net-\nworks captured complicated vehicle dynamics relation -\nships and patterns. By constructing a neighborhood in the \npreprocessing stage for spatio-temporal data extraction \nand using a transformer for capturing interactive motion \ndynamics, we have built our TrajectoFormer system effi-\nciently for trajectory prediction.\n3  System Overview\nExisting approaches for predicting vehicle trajectories are \ndiverse, but they fail to account for all of the key aspects that \nimpact the ultimate accuracy of the prediction. This research \nwas inspired to make AVs more aware of their surround-\nings in a busy traffic environment, which is a key aspect \nof trajectory prediction. For example, observing the past \ntrajectories of the target vehicle and neighboring vehicles \nhelps to determine the next trajectory position of the ego \nvehicle and executes trajectory planning. In this study, we \nconclude that in a non-virtually connected environment, the \nsignificant factors responsible for trajectory prediction tasks \ncan be split into the trajectory history of the target vehicle \nand the trajectory history of its nearby vehicles.\nOur research proposes an architecture that incorporates \nthese aspects, allowing AVs to forecast the long-term 5-s \ntrajectory by examining the target vehicle‚Äôs and surround-\ning vehicles‚Äô 3-s history trajectory. As a result, in the early \nstages of our study, an effective neighborhood was con-\nstructed for each ego vehicle that would influence the ego \nvehicle‚Äôs future dynamics.\nThe architectural overview in Fig.¬† 2 comprises of four \nsections: problem formulation, dataset preprocessing, trans-\nformer-based model, and results evaluation. The initial stage \nof problem formulation was performed to provide clarity of \nthe task and to ensure that the trajectory prediction model \nRMSE Analysis\nTrajectory Prediction Model\nInput\nEmbedding\nPredicted\nTrajectory\nOutput\nEmbedding\nAttention\nBased\nEncoder\nAttention\nBased\nDecoder\nActual vs Predicted Trajectory\nPreprocessed Data\nMissing Value Handeling\nWindow Creation\nTpast = {P1,P2,......,P3}\nTfuture = {P4,P5,......,P8}\nf (Tpast) ‚Üí Tfuture\nProblem Formulation\nHistorical Trajectory of Vehicle 8 Neighbors ExtractionNGSIM DataSet\nFig. 2  Overview of TrajectoFormer: our proposed trajectory prediction system\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 5 of 20    87 \naligns with the real-world requirements and constraints typi-\ncal of dense traffic scenarios. This section delineates the \nmathematical framework and underlying objectives that \nsteer our prediction model. More formally, given the past \ntrajectory /u1D413/u1D429/u1D41A/u1D42C/u1D42D seconds, the model is tasked with predicting \nthe future trajectory /u1D413/u1D41F/u1D42E/u1D42D/u1D42E/u1D42B/u1D41E.\nThe primary objective of the data preprocessing stage was \nto build a neighborhood that detects the vehicles in close \nproximity to a certain vehicle in a given data frame. The \ninformation appears to be in relation to moving automo-\nbiles in various lane locations. In the next stage, Transformer \nmodel was trained upon a preprocessed dataset for long-term \ntrajectory prediction of the target or ego vehicle. After train-\ning the model with two datasets, it was tested with an inde-\npendent test set. Later, a comparison with existing bench-\nmarks was performed to evaluate TrajectoFormer system \nfor the trajectory prediction task. This study introduces the \nTrajectoFormer, which combines an specifically developed \npreprocessing method with a standard transformer-based \nmodel together for accurate long-term vehicle trajectory \nprediction.\n4  Problem Formulation\nWhen regarded from the perspective of an autonomous \nvehicle traversing a busy metropolitan area, the capacity of \nan autonomous vehicle to precisely anticipate the probable \nmotions of surrounding entities is a question of safety. The \npaths of autonomous vehicles are influenced by a variety \nof factors, ranging from vehicle traits and limitations to the \nbehaviors and intents of neighboring entities. The complex \nspatio-temporal interactions between vehicles are one of the \ntraffic datasets‚Äô defining qualities, as multiple vehicles are \nconstantly moving on the road, and nearby entities‚Äô actions \nsignificantly impact their motions. Consequently, to provide \ncredible predictions, our prediction model must account for \nthese spatio-temporal correlations.\nThe key feature of our trajectory prediction model is the \nability to anticipate a vehicle‚Äôs trajectory across a specified \nhorizon. Given the past trajectory /u1D413/u1D429/u1D41A/u1D42C/u1D42D , the model is tasked \nwith predicting the future trajectory /u1D413/u1D41F/u1D42E/u1D42D/u1D42E/u1D42B/u1D41E . Let us represent \nour past and future trajectories as sequences of positions in \na 2D space, such that:\nwhere each p i is a vector containing the spatial coordinates \nat the ith time step. Our prediction function f  can be defined \nas:\n(1)Tpast ={ p1 ,p2 ,‚Ä¶ ,p9 },\n(2)Tfuture ={ p10 ,p32 ,‚Ä¶ ,p21 },\nOur historical trajectory /u1D413/u1D429/u1D41A/u1D42C/u1D42D will have 9 positional data \npoints for 9 timestamps during 3.2-s trajectory observation, \nand our future trajectory /u1D413/u1D41F/u1D42E/u1D42D/u1D42E/u1D42B/u1D41E will contain 12 forecasted \ndata points for 12 timestamps during 4.8-s trajectory predic-\ntion, assuming that our data updates position every 0.4¬†s. \nThis level of specificity guarantees that our model‚Äôs fore-\ncasts are accurate and timely, capturing subtle changes in \nthe vehicle‚Äôs trajectory.\n5  Data Preprocessing\nWe have utilized the NGSIM dataset for our study. In the \nearly 2000s, the Next Generation Simulation (NGSIM) pro-\nject was launched to gather empirical microscopic traffic \ndata [31], and the most extraordinary collection of empiri-\ncal microscopic traffic data currently made accessible to the \nscholarly community is the NGSIM data. ITS DataHub and \nthe Federal Highway Administration (FHWA)‚ÄîNGSIM \nprogram offers raw and processed video footage, exact \nvehicle trajectory data, and data files. Researchers for the \nNGSIM program collected these data on southbound US 101 \nand Lankershim Boulevard in Los Angeles, eastbound I-80 \nin Emeryville, and southbound Peachtree Street in Atlanta \n[32].\nData were collected via synchronized digital video cam-\neras. NGVIDEO, an NGSIM-specific tool, transcribed vehi-\ncle trajectories. These vehicle trajectories located every \nvehicle in the research region every tenth of a second, pro-\nviding their lane positions and relative positions [33]. Each \nNGSIM dataset‚Äôs zip file contains location-specific data \nand vehicle trajectories. Following are the time bounds for \nNGSIM data sets, two from freeways (I-80 and US-101). \nBoth US-101 and US-I-80 datasets have been preprocessed \nto train our TrajectoFormer system (Table¬†1).\nIn the NGSIM dataset, vehicles are classified into several \ntypes. Following shows the vehicle distribution for US-101 \ndataset in Table¬†2 [31]:\nThe NGSIM data collection efforts have resulted in an \nextensive dataset containing trajectory data for four loca-\ntions. This dataset comprises of over 11.8 million rows \n(3)f (/u1D413/u1D429/u1D41A/u1D42C/u1D42D) ‚Üí /u1D413/u1D41F/u1D42E/u1D42D/u1D42E/u1D42B/u1D41E.\nTable 1  NGSIM dataset time \nbounds Dataset Time bound\nUS-101 7:50 am‚Äì8:05 am\n8:05 am‚Äì8:20 am\n8:20 am‚Äì8:35 am\nI-80 4:00 am‚Äì4:15 am\n5:00 am‚Äì5:15 am\n5:15 am‚Äì5:30 am\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 6 of 20\nand 25 columns of valuable information [33]. The dataset \nincludes a wide variety of variables that accurately depict \nthe motion and actions of automobiles, trucks, and buses. \nSome of the more prominent features of the NGSIM dataset \nare described here that have been considered in most of the \nliterature.\n‚Ä¢ Vehicle ID: A vehicle ID is a special number assigned to \neach vehicle in the collection.\n‚Ä¢ Frame ID: A number that is used to track certain time \nintervals. Typically, the dataset is sampled once every \ntenth of a second, making each frame a frozen moment \nin time.\n‚Ä¢ Total Frames: A vehicle‚Äôs total frames is the sum of all \nthe frames in which it appears.\n‚Ä¢ Time (Global Time): Each frame of an audio or video \nrecording will include a timestamp that specifies how \nmuch time has elapsed since the recording began, \nexpressed in global time.\n‚Ä¢ Local X, Local Y: The vehicle‚Äôs local X and Y coordinates \nin meters. Position ‚ÄúLocal X ‚Äù (across lanes) and ‚ÄúLocal \nY‚Äù (along a direction) are denoted respectively (along the \nlength of the road).\n‚Ä¢ Global X, Global Y: The vehicle‚Äôs geographical position \nis indicated by its global X and Y coordinates.\n‚Ä¢ Vehicle Length and Width: The length and width of the \nvehicle are its dimensions.\n‚Ä¢ Vehicle Type: Classifies the vehicle as either a car, \nmotorbike, bus, or truck, among other categories.\n‚Ä¢ Vehicle Velocity: The vehicle‚Äôs speed, expressed in \nmeters per second; abbreviated v.\n‚Ä¢ Vehicle‚Äôs Acceleration: Acceleration is the rate at which \na vehicle‚Äôs speed increases or decreases, expressed in \nmeters per second squared.\n‚Ä¢ Lane ID: The vehicle‚Äôs lane ID shows the lane it is in.\n‚Ä¢ Preceding Vehicle, Following Vehicle:Identity of the \nvehicle immediately before and following the relevant \nvehicle.\n‚Ä¢ Space Headway: The distance between the focus vehicle \nand the car in front of it, also known as the ‚Äúspace head-\nway‚Äù.\n‚Ä¢ Time Headway: Time Headway is the amount of time it \nwould take for the target vehicle to advance one place \nrelative to the stationary vehicle in front of it.\n‚Ä¢ Vehicle Class: Classification of vehicles according to the \nnumber of their wheels.\n‚Ä¢ Gap: If both cars keep going at their present speeds, the \ngap is the time until they collide with the one in front of \nthem.\nThe abundance of these data enables a wide range of ana-\nlytics, such as modeling traffic flow, comprehending lane-\nchanging behavior, forecasting traffic congestion, and much \nmore. However, for our study, we have taken into considera-\ntion the following features: ‚ÄòLocal X ‚Äô, ‚ÄòLocal Y ‚Äô, ‚Äòvehicle \nLength‚Äô, ‚Äòvehicle Width‚Äô, ‚Äòvehicle Class‚Äô, ‚Äòvehicle Velocity‚Äô, \n‚ÄòSpace Headway‚Äô, ‚ÄòTime Headway‚Äô, ‚Äòvehicle Acceleration‚Äô, \nand ‚ÄòLane ID‚Äô.\n5.1  Neighborhood Selection and¬†8‚ÄëNeighbor \nConstruction\nComplex and dynamic driving makes predicting trajectories \nchallenging owing to changing interactions. Ego vehicles‚Äô \nand neighboring vehicles‚Äô observed trajectories assist in \nestimating an autonomous vehicle‚Äôs future trajectory. Thus, \nthe ego vehicle‚Äôs history and interactions with surrounding \nvehicles must be examined. Many studies show that a tar -\nget vehicle‚Äôs past movement and neighbor interaction affect \nforecast accuracy [34]. Our study was initiated by develop-\ning an effective neighborhood for each vehicle that affects its \ndynamics. Therefore, during the data preprocessing stage, a \nneighborhood was constructed for each existing vehicle ID \nin the NGSIM dataset.\nAccording to studies [34], eight neighboring vehicles \nhad a higher impact on a target vehicle‚Äôs behavior than the \nother traffic agents presented in a complex traffic condition. \nOur study used the eight-neighbor notion of an image pixel, \nand each ego vehicle‚Äôs eight nearest neighbors were located \nat the left, right, front, behind, left front, left behind, right \nfront, and right behind positions. The neighborhood was cre-\nated to detect automobiles near a vehicle in a data frame. \nThe neighborhood was built in the following way: \n(i) In the provided frame, choose the target vehicle The \nwhole row from the dataset was retrieved for a certain \nvehicle and provided frame.\n(ii) Obtaining the vehicle IDs of the cars in front of and \nbehind the target vehicle Vehicle Ids were retrieved \nfrom the preceding and following attributes of the data-\nset to find the front and behind neighbors.\n(iii) Determine the target vehicle‚Äôs upper and lower bound-\naries (y-axis) The following equations were used to \ndetermine the vehicle‚Äôs boundaries along the Y-axis. \n(4)Upper bound = LocalY + vehicle length‚àï2,\nTable 2  NGSIM dataset US-101 vehicle types\nVehicle type Vehicles Percentage (%)\nMotorcycle 30 1.4\nAutomobile 2086 96.2\nTruck and buses 53 2.4\nSum 2169 100.0\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 7 of 20    87 \n(iv) Determining the vehicles in the left lane The vehicles \npositioned at left, left front, and left behind of the ego \nvehicle are determined. As vehicles are positioned \nbetween Lane ID (0‚Äì8) in the NGSIM dataset, no \nvehicles are set for the left lane if the ego vehicle‚Äôs \nlane is zero. Otherwise, the three left neighbors are \nconstructed in the following manner: Left: The vehi-\ncle positioned at lane ID one less than the ego vehi-\ncle‚Äôs lane Id. Left Front: The vehicle in the left lane \nthat is in front of the ego vehicle. Left Bottom: The \nvehicle in the left lane that is behind the ego vehicle.\n(v) Determine the vehicles in the right lane The approach \nis similar to the one taken for the left lane, but it \n(5)Lower bound = LocalY ‚àí vehicle length‚àï2. checks for the lane to the right. At first, it is checked \nif the right lane is a valid lane (less than 8 or equal \nto 8). If not valid, all right-lane vehicles are set to 0. \nOtherwise, the three right neighbors are constructed \nin the following manner: Right: The vehicle posi-\ntioned at lane ID one more than the ego vehicle‚Äôs lane \nId. Right Front: The vehicle in the right lane that is in \nfront of the ego vehicle. Right Bottom: The vehicle \nin the right lane that is behind the ego vehicle.\nAn algorithm for extracting the eight neighbors of each tar-\nget vehicle has been demonstrated below. This algorithm \ncan be divided into five sections according to the data pre-\nprocessing steps. These steps show how each neighbor is \nextracted gradually, and each step has been commented on \nin the described algorithm.\nAlgorithm¬†1  Detecting Neighborhood of a Target Vehicle\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 8 of 20\n5.2  Window Creation\nTime-series predictions need the creation of temporal win-\ndows in sequential data, particularly in the context of auton-\nomous driving. Recording significant vehicle contacts with \nneighbors over a lengthy period of time became crucial for \nour study. In light of this, an 8-s window is determined. Win-\ndowSize refers to this 8-s window in algorithm¬†2. An inten-\ntional distinction inside this 8-s window is created to ensure \nthat training and prediction were appropriately taken care \nof. During the first 3.2¬†s of the window, referred to as Train-\ningWindow in algorithm¬†2, the model is trained on historical \ndata to identify the patterns and behaviors of the target and \nneighboring vehicles. Then, using previously discovered pat-\nterns, the following 4.8¬†s, referred to as PredictionWindow in \nalgorithm¬†2, were used to forecast vehicle trajectories. Fur -\nthermore, adding eight separate features (‚ÄòLocalX ‚Äô, ‚Äò LocalY ‚Äô, \n‚Äò vLength ‚Äô, ‚Äò vWidth ‚Äô, ‚Äò vClass ‚Äô, ‚Äò vVel ‚Äô, ‚Äò vAcc ‚Äô, ‚Äò LaneID ‚Äô) with each \nframe, referred to as FeatureSetSize, improved each temporal \nframe in our TrainingWindow. Thus, the TrainingWindow is \nstructured as a 9 √ó 8 matrix; where 9 refers to the 9 sequen-\ntial time-frames (3.2 s with 0.4 s frame interval, including \nthe last frame) and 8 refers to the eight predefined features. \nThis feature set provided the vehicle with a full view of its \nsurroundings and increased the model‚Äôs ability to forecast \naccurately. A formal visual notation for this process has been \ndepicted in Fig.¬†3. Also, an algorithm describing the window \ncreation process in detail has been depicted below with two \nloops.\nAlgorithm¬†2  Creating Temporal Windows for Sequential Data in Autonomous Driving\nFig. 3  Window creation\nwindow 1\nwindow 2\nwindow n\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 9 of 20    87 \nThis windowing strategy also helped to draw a compari-\nson between how well it worked for training and testing sce-\nnarios. This ensured that our model TrajectoFormer remains \nbalanced under varying conditions.\n5.3  Dealing with¬†Missing Values\nOne issue that arose during the preprocessing phase of \nthe NGSIM dataset was border vehicles with missing \ncontextual information. Some vehicles, in particular, had \nto proceed or follow neighbors who were not present in \nthe dataset. This absence could create irregularity in the \ndataset, reducing the overall dependability and accuracy \nof any model trained on this data. An elimination tech-\nnique was devised to address this issue. Vehicles with \nno preceding or following counterparts were identified. \nThese vehicles were then designated unfit for training \nbut not removed from the dataset. This method ensured \nthat all vehicles in the dataset, including preceding and \nfollowing vehicles, had comprehensive contextual data. \nAlso, the preprocessing process incorporated with han-\ndling missing values, improved the dataset‚Äôs consistency \nand integrity, making it suitable for accurate model train-\ning and testing.\n6  Methodology\nThe core of our trajectory prediction model TrajectoFor -\nmer system is a transformer-based network that is tuned \nspecifically to capture complex temporal interactions and \nspatial relationships between vehicles. The model archi-\ntecture can be broadly divided into some main component \nblocks, as illustrated in Fig.¬† 4. The model architecture \nstarted with the input‚Äìoutput embedding layers, which \nencoded the observed trajectories over 9 sequential time-\nframes and 8 vehicle attributes into high-dimensional \nrepresentations. A combination of slicing, flattening, and \nrepeat vectors was also used to capture the last frame of \nthe target vehicle and to perfectly attach it to the out-\nput embedding layer as its input. After that, the embed-\nding is enriched with positional encoding to encapsulate \nthe sequence order, making them more meaningful for \nsequential processing. Following this, attention-based \nencoder‚Äìdecoder blocks served as the computational back -\nbone, wherein multi-head attention mechanisms and feed-\nforward networks operated to capture complex depend-\nencies and patterns in the vehicle dynamics. Finally, a \ndense layer acted as a mapper to map the output into our \nprediction space as a 12 √ó 3 feature vector, representing \nFig. 4  Transformer-based trajectory prediction model architecture\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 10 of 20\nthree prediction features across 12 sequential time-frames, \nyielding the estimated future trajectories. Together, these \ncomponent blocks formed a fine-tuned network to predict \nthe target vehicle trajectory. In the following subsections, \ndetails of each component block have been described.\n6.1  Input‚ÄìOutput Embedding\nThe input‚Äìoutput embedding layer is the initial stage of the \nTransformer-based model. These layers were significant for \ntwo reasons: first, the obtained data from the neighborhood \nconstruction were transformed into a more useful form that \nhelped the model to recognize patterns more effectively. \nSecond, the data for the next stages of the model (the atten-\ntion mechanisms) were prepared by embedding. This trans-\nformation was important because the model was able to \nunderstand the complex ways that vehicles move and inter-\nact because of embedding, which could not be easily done \nfrom the raw data alone.\n6.1.1  Input Embedding\nThe raw vehicular data from neighborhood construction \nwere transformed into continuous vector representations by \nthe input embedding layer in the transformer-based network. \nThis representation is also known as latent space, where each \ndimension captures some aspect of the data‚Äôs original mean-\ning. In our architecture, this embedding is performed via two \nconnected dense layers, the outputs of which are denoted as \nE1 and E2.\nLet X ‚àà ‚Ñù9√ó72 represent the input feature matrix. The \ndense layers perform the transformation:\nwhere W1 ,W2 are the weight matrices, b1 ,b2 are the bias \nvectors, and /u1D70E is the activation function.\nThe reason behind employing two dense layers for input \nembedding is that in complex interactions like vehicular traf-\nfic, one layer might be insufficient to capture the nuances. \nBoth spatial (‚ÄòLocalX ‚Äô, ‚Äò LocalY ‚Äô) and temporal (‚ÄòvVel ‚Äô, ‚Äò vAcc ‚Äô) \nfeatures were contained in the 9 √ó 8 input vector for each \nvehicle of the scenario. Individual relationships between \nthese features were captured by the first dense layer by pro-\nducing distinct linear combinations. Subsequently, the sec-\nond dense layer was involved in refining the track by com-\nbining the output of the first layer in a manner that allows \nfor more complex relationships between spatial and temporal \nvariables. From a mathematical perspective, a two-tier hier-\narchical parsing of the input features was efficiently executed \n(6)E1 = /u1D70E(W 1 ‚ãÖ X + b1 )‚àà‚Ñù 9√ó72 ,\n(7)E2 = /u1D70E(W 2 ‚ãÖ E1 + b2)‚àà‚Ñù 9√ó64 ,\nby this structure, enriching the model‚Äôs understanding of \nboth space and time aspects.\n6.1.2  Output Embedding\nThe output embedding layer is held responsible for produc-\ning a feature space for the prediction problem. The core of \nthis embedding layer consists of 2 connected dense layers, but \nthe input of this layer is customized to mitigate the computa-\ntional complexities associated with error growth. In a typi-\ncal transformer network, the output embedding block takes \nthe concatenated result of the full sequence and the actual \noutput sequence. However, in our regression-type trajectory \nprediction problem, tokenization or normalization is avoided \nto achieve better results while requiring less training time. \nAs a result, the input system of this output embedding block \nis customized to accept only the last frame data. With this \nmodification,accurate trajectory is predicted without having \nto worry about the error growing exponentially over time-\nframe. Mathematically, it can be said that we operated on \n( Traj_target_vechicle ) to produce Vrepeat which served as the \noutput embedding. The operations can be described as follows:\nAs previously mentioned, considering the full sequence for \nembedding, the model attempted to learn a complex map-\nping f ‚à∂ E full ‚Üí Y  , where Y is the output sequence. In this \nsetting, small errors in estimating E full got amplified through \nover all time-frames, leading to an exponential growth in the \nerror term, denoted as /u1D716:\nTo counter this, the last frame Vlast was focused on by \nour architecture, reducing the function‚Äôs complexity to \nfÔøΩ ‚à∂ V last ‚Üí Y  , thereby mitigating error growth:\nThis adjustment in output embedding also aligns with the \nMarkov property [35] by utilizing Vlast as the pivotal state \nfor future predictions. According to the Markov property, \nthe future state is dependent only on the current state, math-\nematically expressed as\n(8)V last = Slice(Traj_target_vechicle,t = 9)‚àà ‚Ñù1√ó8 ,\n(9)VÔ¨Çat = Flatten(Vlast)‚àà ‚Ñù8 ,\n(10)Vrepeat = RepeatVector(VÔ¨Çat,T = 12)‚àà‚Ñù 12√ó8 .\n(11)ùúñ ‚àù eùõº‚ãÖ‚ÄñEfull‚àíÃÇEfull‚Äñ, where ùõº> 0.\n(12)ùúñreduced ‚àù eùõº‚ãÖ‚ÄñVlast‚àí ÃÇVlast‚Äñ.\n(13)P(Y t+1 /uni007C.varYt)=P(Y t+1 /uni007C.varYt, Yt‚àí1 , ‚Ä¶ , Y1 ).\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 11 of 20    87 \nThe sliced vector Vlast ‚àà ‚Ñù8  is replicated T = 12 times \nthrough the RepeatVector operation. This resulted in Vrepeat , \nwhich served as a constant initial condition that adhered to \nthe Markov property and minimized error propagation.\nTwo justifications were provided by the adoption of utilizing \nV repeat as the constant initial input:minimizing error while \nsatisfying the Markov property and providing a mathemati-\ncally concrete foundation for trajectory prediction.\n6.1.3  Positional Encoding\nPositional encoding in transformer-based architectures is \nused to provide information about the relative or absolute \npositions of tokens in a sequence [36]. Since trajectory pre-\ndiction problems do not have a tokenized form of input but \nstill demand positional information to distinguish the order \nof vehicle location over time, both sine and cosine func-\ntions were employed based on positional encoding to learn \ntime-dependent patterns. Mathematically, for a sequence of \nlength T and feature dimension D , the positional encoding \nPE is defined as:\n(14)Vrepeat =[ VÔ¨Çat ,VÔ¨Çat ,‚Ä¶ ,VÔ¨Çat ]1√óT .\n(15)PE (i,2j) = sin\n/parenleft.s2i\n10000 2j‚àïD\n/parenright.s2\n,\nThe model effectively learned the temporal patterns of com-\nplex vehicular traffic by this chosen formula [37]. A damp-\ning factor of 100002j‚àïD  ensures that the function has vary -\ning wavelengths, enabling the model to capture long-range \ndependencies in vehicular data.\n6.2  Attention‚ÄëBased Encoder‚ÄìDecoder Mechanism\nAttention-based encoder‚Äìdecoder [38] pair works at the core \nof the transformer network inside our proposed TrajectoFor-\nmer system. The attention mechanism works by assigning \ndifferent ‚Äúattention scores‚Äù to different parts of the input. \nThis allows the model to focus on specific parts of the input \nsequence rather than using fixed-size data, making the archi-\ntecture more flexible and able to handle longer sequences \nwith complex dependencies. In our study, multi-branch \narchitecture is employed for encoder‚Äìdecoder models, con-\nsisting of 2-way parallel encoder and decoder chains. Here, \neach chain focused on various aspects of vehicular traffic \nby forming more latent features, providing a more accu -\nrate prediction. The block diagram of the attention-based \nencoder‚Äìdecoder mechanism is depicted in Fig.¬† 5.\n6.2.1  Encoder Block\nTo encode a sequence of observations into a latent feature \nspace, a two-layer encoder block is employed in the Trans-\nformer network. Both short and long-term spatial dependen-\ncies are captured by this feature space. A multi-head atten-\ntion mechanism and a point-wise feed-forward network are \nentrenched in the configurations of each encoder layer in our \nmodel. These parts are arranged hierarchically to capture \nthe complicated spatio-temporal patterns found in the data \nefficiently.\nLet X ‚àà ‚Ñùn√ód denote the concatenated input feature ten-\nsor for post-positional encoding, where n is the sequence \nlength and d is the feature dimension. The model could focus \non a variety of input features since this tensor is initially \ntreated to multi-headed attention with 10 attention heads \nand a 16-dimensional key. Specifically, for each head h , the \nattention is computed as:\nwhere Attention_Scoreh is computed as QK‚ä§\n‚àö\ndk\n in line with our \ncustom-defined approach, contrasting the typical softmax-\nbased methods.\nThe result of concatenating and linearly transforming the \noutput from all the heads is represented by:\n(16)PE (i,2j+1) = cos\n/parenleft.s2i\n10000 2j‚àïD\n/parenright.s2\n.\n(17)Attentionh(Q,K,V)=V √ó Attention_Scoreh(Q,K),\n2x\n2x\nFig. 5  Attention-based encoder‚Äìdecoder block [38]\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 12 of 20\nwhere WO is the weight matrix for the output projection. \nThe output Oi for each encoder layer i is then computed as:\nEach encoder layer incorporated a feed-forward neural net-\nwork with two dense layers post-attention. The first layer \nincreased the feature dimension to 128 and used a ReLU acti-\nvation, which is formally denoted as Dense128(ReLU (Oi )) . \nThe dimension is reduced to 64 by the second dense \nlayer, which acts as a bottleneck. It is represented by the \nDense64(H) , where H is the result of the first dense layer.\nThe encoder in our system consisted of two linked lay -\ners, each with a distinct mathematical formalism and func-\ntion. Considering each encoder layer to be a function, T  , \nthat transfers an input feature space X to an output feature \nspace O.\nEncoding the data‚Äôs local spatio-temporal relationships \nwas focused on by the first encoder layer. The mathemati-\ncal definition of the transformation in the first layer is as \nfollows:\nwhere W1 and b1 are the weight matrix and bias vector, \nrespectively. The keys, queries, and values‚Äô K1 ,Q1 , and V1\n‚Äôare calculated from input trajectory‚Äôs local segments.\nThe second encoder layer can be mathematically dem-\nonstrated as follows for capturing long-term relationships:\nHere, Q2 ,K2  , and V2 are calculated from more extensive \ntrajectories and even from the complete data set, allowing \nthe model to comprehend global broad patterns.\nFinally, the output feature space O of the entire encoder \nblock is a non-linear composition of these transformations:\nWith this arrangement, it is guaranteed that both the local \nand global spatio-temporal properties of the trajectory are \naccurately captured by O . By organizing the encoder this \nway, it was ensured that the decoded output trajectories dis-\nplay contextual precision, reflecting spatial dependencies \nspanning short- and long-term horizons.\n6.2.2  Decoder Block\nThe decoder block is a pivotal component in the archi-\ntecture, responsible for synthesizing the future trajectory \nbased on the high-level representations derived from the \n(18)MultiHead(Q,K,V)=Concat(head 1,‚Ä¶ , headH)WO,\n(19)Oi = LayerNorm (X+ MultiHead(Q,K,V)).\n(20)T1 (X)= W 1 √ó Attentionlocal(Q1 ,K1 ,V1 )+ b1 ,\n(21)T2 (X)=W 2 √ó Attentionglobal(Q2 ,K2 ,V2 )+b 2 .\n(22)O = T2 (T1 (X)).\nencoder. Specifically, it ingested an aggregated feature set \nO ‚àà ‚ÑùT√óD and forecasted trajectory Y ‚àà ‚ÑùTÔøΩ√óDÔøΩ\n.\nThe decoder‚Äôs operations were initiated with a causal \nmulti-head attention layer, ensuring the temporal integrity \nof the sequence by applying a causal mask. This procedure \nallowed the model to condition each time step‚Äôs prediction \nsolely on prior time steps. Mathematically, this is repre-\nsented as:\nThe output is then stabilized through an addition and a layer \nnormalization operation:\nFollowing this, another multi-head attention layer is \nemployed to cross-reference the decoder‚Äôs inputs with the \nencoder‚Äôs outputs. The operation for this attention mecha-\nnism can be formulated as follows:\nLikewise before, addition and layer normalization was \napplied on output:\nSubsequently, a feed-forward neural network (FFN) is incor-\nporated into each decoder block. Comprising of two dense \nlayers, this FFN refined the feature set by adding another \nlayer of abstraction. The mathematical representation of this \nFFN is:\nAn addition and a layer normalization were applied again \nfor final time:\nTo adequately capture the spatio-temporal dynamics in the \ndata, the architecture employed two-chained decoder blocks. \nShort-term pattern extraction was focused on by the first \none while understanding the more extended temporal rela -\ntionships was aimed at by the second one. This hierarchi -\ncal approach ensured the model‚Äôs proficiency in generating \nfuture trajectories that are both locally precise and globally \ncoherent.\n6.3  Final Mapper\nThe architecture is completed with a dense layer that effi-\nciently maps the rich feature vectors generated by the \ndecoder into accurate trajectory predictions. A dense neural \n(23)\nAmask = Attention(Qmask ,Kmask ,Vmask ,\nuse_causal_mask=True ).\n(24)Xmask = LayerNorm(Amask + O).\n(25)Across = Attention (Qcross ,Kenc ,Venc ).\n(26)Xcross= LayerNorm (Across+ Xmask ).\n(27)F= /u1D70E2 (W 2 /u1D70E1 (W 1 Xcross + b1 )+ b2 ).\n(28)XFFN = LayerNorm (F+ Xcross).\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 13 of 20    87 \nnetwork generated a 12 √ó 3 matrix in this layer which rep-\nresented the three prediction features Localx , Localy , and \nLane_ID over 12 sequential time steps.\nMathematically, the input to the mapping layer can be \ndenoted as Z ‚àà ‚Ñù12√ó72 , which is the concatenation of two \nmatrices: the decoder output D ‚àà ‚Ñù12√ó64 and a repeated vec-\ntor R ‚àà ‚Ñù12√ó8 of the target vehicle‚Äôs last frame data. The \ndense mapping layer then applied a linear transformation \nto Z to yield the predicted output Y ‚àà ‚Ñù12√ó3 ; with a weight \nmatrix W ‚àà ‚Ñù3√ó72 and a bias vector b ‚àà ‚Ñù3 . This linear \nmapping is designed to convert the abstract feature space \ninto concrete, meaningful outputs that directly correspond \nto target prediction features.\nThis trajectory blueprint in O captured our model‚Äôs ability \nto discern and predict the future path. Specifically, each row \nin this matrix provided a precise snapshot of the predicted \nlocation and lane status. In illustration, ÃÑLocalX , ÃÑLocalY , and \nÃÑLocal are represented by matrix‚Äôs columns, respectively.\nIn summation, the final mapper completed our model by \nseamlessly transitioning the intricacies of historical data to \npotential future paths, ensuring an informed and accurate \nprediction.\n7  Ablation Study\nThis study involves conducting an ablation analysis to assess \nthe efficacy of each component in our proposed Trajecto-\nFormer system, including the custom preprocessing and the \nadopted Transformer neural network model, for predict-\ning trajectories in autonomous vehicles. First, we attempt \nOutput=\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éù\nÃÑLocalX ÃÑLocalY ÃÑLaneID\n0.4s ‚àí‚àí ‚àí‚àí ‚àí‚àí\n0.8s ‚àí‚àí ‚àí‚àí ‚àí‚àí\n‚àí‚àí ‚àí‚àí ‚àí‚àí ‚àí‚àí\n‚àí‚àí ‚àí‚àí ‚àí‚àí ‚àí‚àí\n4.4s ‚àí‚àí ‚àí‚àí ‚àí‚àí\n4.8s ‚àí‚àí ‚àí‚àí ‚àí‚àí\n‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†\nFig. 6  LSTM-based model architecture designed to incorporate with proposed preprocessing strategy\nFig. 7  CS-LSTM model well-suited with traditional preprocessing\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 14 of 20\nto evaluate the effectiveness of the adopted Transformer \nmodel by contrast a long short-term memory (LSTM)-\nbased model‚Äîa traditional choice for time-series analysis. \nThis comparison, utilizing the same dataset, preprocessing \nstrategy and evaluation metrics, aims to highlight the Trans-\nformer model‚Äôs superior ability to capture spatio-temporal \ndependencies crucial for predicting vehicle trajectories.\nThe selection of the LSTM model for the first abla-\ntion experiment is motivated by its specific strengths and \nestablished track record in the domain of time-series data, \nincluding the temporal dependencies. As depicted in Fig.¬†6, \nthe model architecture incorporates multiple input layers \nfor individual vehicle(target and its eight-neighbor) trajec-\ntories, feeding into corresponding LSTM layers designed \nto capture dynamic temporal behavior. Each LSTM layer \nis configured with 64 units, ensuring sufficient capacity to \nlearn complex temporal patterns. The outputs of these layers \nare concatenated and processed through dense layers with \nLeakyReLU activation, promoting non-linear learning. The \nmodel culminates in a final LSTM layer, outputting the pre-\ndicted future trajectory.\nSecond, we intend to analyze and compare the efficacy \nof our customized preprocessing strategy in terms of pat-\ntern recognition and overall trajectory prediction. This \nexperiment demands replacing the preprocessing method \nof TrajectoFormer with a preexisting one from previous \nscholarly works. However, this task is not straightforward \nbecause every aspect of the neural network layers architec-\nture relies on how we input the data, particularly up to the \noutput of the preprocessing stage. Since the existing pre-\nprocessing methods(from previous literature) are not specifi-\ncally tailored for the Transformer model, it is not feasible to \nincorporate them with Transformer model. So, we employ \na straightforward preprocessing technique with an LSTM-\nbased model, which appears to be effective when compared \nto the previous ablation experiment (Transformer vs LSTM \nwith same preprocessing). The LSTM architecture must be \nmodified based on the preprocessing requirements. There-\nfore, we have designed a combined preprocessing + LSTM \nmodel setup inspired directly from the CS-LSTM model \nproposed by Deo et¬†al. [11], as illustrated in Fig.¬†7.\nIn the experimental setup, both the LSTM and Trans-\nformer models were trained and evaluated on the preproc -\nessed NGSIM data, and CS-LSTM with traditional pre-\nprocessing technique as proposed by Deo et¬†al. [11]. For \nconsistency in evaluation, all of these three setups were \nconfigured to process the same past 3¬†s of trajectory data to \npredict the next 5¬†s, mirroring real-world autonomous driv-\ning scenarios. The LSTM, CS-LSTM model, as previously \ndescribed, and the Transformer model were both trained \nunder identical conditions: utilizing the Adam optimizer \nwith a learning rate of 0.001, and the models‚Äô performance \nwas measured using mean squared error (MSE) as the loss \nfunction.\nTo provide a comprehensive comparison, the evalua-\ntion focused on key metrics: Root Mean Squared Error \n(RMSE), the total number of weights in each network, and \nthe required training time. Table¬† 3 presents a direct com-\nparison of the RMSE values for both models, indicating \ntheir accuracy in predicting vehicle trajectories. In terms \nof model complexity, the LSTM model comprises a total \nof 2,51,847 weights, while the Transformer model consists \nof 3,21,755 weights, highlighting the differences in their \narchitectural designs. Although, proposed TrajectoFormer \nsystem being far heavier than the LSTM-based model in \nterms of overall weights in the network, it achieves a stable \nTable 3  RMSE comparison for \nablation study Model 1¬†s 2¬†s 3¬†s 4¬†s 5¬†s\nUS-101 CS-LSTM-M + Traditional preprocessing 0.63 1.35 2.18 3.26 4.59\nLSTM + Proposed preprocessing 0.58 1.02 1.84 2.63 3.19\nTrajectoFormer 0.47 0.84 1.33 1.97 2.56\nI-80 CS-LSTM-M + Traditional preprocessing 0.62 1.29 2.13 3.20 4.52\nLSTM + Proposed preprocessing 0.59 0.98 1.78 2.65 3.21\nTrajectoFormer 0.52 0.85 1.34 1.93 2.47\nTable 4  Constructed \nneighborhood for each target \nvehicle\nVid Fid Up Down Left Up_left Down_left Right Up_right Down_right\n100 230 90 0 99 79 0 51 33 0\n100 231 90 0 99 79 0 51 33 0\n100 232 90 0 99 79 0 51 42 132\n100 233 90 0 99 79 0 51 42 132\n100 234 90 109 99 79 0 51 42 132\n100 235 90 109 99 79 0 51 42 132\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 15 of 20    87 \nMSE loss value (where we can consider to stop training) \nin only 15‚Äì20 epochs on average. In contrast, the LSTM \nmodel requires 30‚Äì35 epochs and CS-LSTM require 100 \nepochs (with higher training time for each epochs) during \nour experiment. So, in terms of training time, the Trajec-\ntoFormer model outperforms the LSTM and CS-LSTM \nmodel both and simultaneously achieves a high level of \naccuracy based on RMSE analysis. These comparative \nanalyses underscore the relative strengths of Transformer \nmodels in capturing the complex spatio-temporal pat-\nterns within the NGSIM dataset, thereby offering critical \ninsights into their suitability for real-world autonomous \nvehicle trajectory prediction.\nThe error analysis results derived from the ablation study, \nas presented in Table¬† 3, reveal two noteworthy observa-\ntions. First of all, considering all three of these methods, it \nbecomes apparent that there is a significant variance in per-\nformance between typical preprocessing (using CS-LSTM) \nand our proposed preprocessing (utilizing both LSTM and \nTransformer). This result unambiguously demonstrates the \nsuperiority of our proposed preprocessing model in terms of \nfacilitating input for pattern recognition. Second, in between \nthe two model using our proposed preprocessing (LSTM and \nTransformer), RMSE values across both US-101 and I-80 \ndatasets consistently show that the Transformer model out-\nperforms the LSTM model, particularly in predicting longer-\nterm future trajectories. Which indicates a more accurate and \nreliable prediction capability, that is crucial for the safety \nand efficiency of autonomous vehicle navigation. In terms \nof model complexity, although the Transformer model has \na higher number of weights, its enhanced performance justi-\nfies the increased complexity, showcasing its ability to cap-\nture more nuanced spatio-temporal patterns in the data. Fur-\nthermore, the training time comparison reveals that despite \nthere potential increased in total number of weights in the \nTransformer model, it require significantly low training time \nand epoch number compare to LSTM model. These findings \nunderscore the effectiveness of the Transformer architecture \nin handling the intricacies of real-world driving scenarios, \nalso demonstrate the superiority of our proposed preprocess-\ning strategy in terms of making pattern identification easier. \nAltogether, the TrajectoFormer system, which consist of pro-\nposed preprocessing +Transformer model, is more feasible \nchoice for complex autonomous vehicle trajectory prediction \ntasks.\n8  Result Analysis and¬†Evaluation\nIn this section, the data preprocessing outcome and the \nresults and evaluation of our implemented model are \ndescribed. Additionally, a comparison is drawn with the \nexisting benchmarks.\n8.1  Data Preprocessing Outcome\nAccording to studies, a target vehicle‚Äôs behavior is more \naffected by eight nearby vehicles than by a large number of \nvehicles surrounding the ego vehicle. Table¬† 4 displays the \nresults of the neighborhood construction that we performed. \nFor the sake of our study, each ego vehicle and its eight near-\nest neighbors were taken into consideration. A number of \nrecords of neighbors changing or remaining the same over \nthe frame can be seen here for a particular vehicle (with a \nspecific vehicle ID). This representation helped us to create \na window frame and precisely capture patterns by the model.\n8.2  Implementation Details\nFor our study, we have used NGSIM US-101 and US-I-80 \ndatasets. The prediction model is trained independently on \nthe preprocessed training set of each dataset. Initially, data \nwere divided into 80‚Äì20 for train and test split. The pre-\nliminary step in our training process was the division of our \ndataset. Ensuring a robust evaluation is paramount, so we \nopted for an 80‚Äì20 split. This 80% training data were again \nsplit into an 80‚Äì20 ratio for the training and validation set. \nAllocating 80% of the training data allowed the model to \nlearn from a substantial dataset, ensuring it grasps intricate \npatterns. Meanwhile, the reserved 20% served as a valida-\ntion benchmark, helping detect and mitigate overfitting while \ngauging the model‚Äôs generalization capability. Afterward, \nthe model was tested on initially divided independent test \nsets. Like [39, 40], three seconds of observation were used \nto predict the next 5 s. Twenty frames per second (FPS) were \nused for both observation and prediction windows.\nThe mean squared error (MSE) was selected as the major \nloss function for both the training and validation phases, as \nusing MSE as a loss function provides insight into the mod-\nel‚Äôs prediction accuracy and convergence during training. \nMean squared error (MSE) is a popular regression statistic \nthat estimates the average squared discrepancies between \npredicted and actual (ground truth) values. It is computed by \nadding the squared differences between each anticipated and \nactual value and then dividing by the total number of data \npoints [ 41]. MSE can be calculated mathematically using \nthe following formula:\nwhere\n‚Ä¢ N is the total number of data points.\n‚Ä¢ yactual,i is the actual value of the ith data point.\n‚Ä¢ ypredicted, i is the predicted value of the ith data point.\n(29)MSE = 1\nN\nN/uni2211.s1\ni=1\n(yactual,i ‚àí ypredicted,i)2 ,\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 16 of 20\nMSE measures the average squared difference between \nexpected and actual values. Squaring the differences empha-\nsizes larger errors, and outliers impact the metric more. As \na result, a lower MSE suggests that the model‚Äôs predictions \nare more accurate, signifying more accuracy. A greater MSE \nvalue, on the other hand, indicates that the model‚Äôs predic-\ntions differ more from the actual values.\nOur model‚Äôs parameters are iteratively changed during \ntraining to minimize the MSE. This statistic indicates the \nmodel‚Äôs generalization capabilities, offering insight into how \nwell it works on unknown input. MSE, as a loss function, \nhas several advantages, including a focus on greater errors \nand differentiability, which is essential for gradient-based \noptimization techniques.\nThe optimization landscape of deep learning models is \nintricate. To navigate this, we employed the Adam opti-\nmizer with an initial learning rate of 0.001. With its adaptive \nlearning rates, Adam caters to both rapid convergence and \nstability‚Äôa crucial balance in ensuring consistent training \ndynamics. Furthermore, by adopting the mean squared error \n(MSE) as our loss function, we ensured that the model is \naptly penalized for significant deviations from true trajec-\ntory values, pushing it toward greater accuracy. The model \nwas trained for 25 epochs with a batch size of 25 to cap off  \nour training procedure. This batch size was chosen to strike \na balance between memory consumption and the granular -\nity of weight updates. Evaluating model performance on the \nvalidation set at the end of each epoch served as a crucial \nfeedback loop, guiding subsequent training phases.\n8.3  Experimental Result\nThe root mean square error is the key evaluation metric used \nin this study (RMSE). The root mean square error (RMSE) \nis a widely used metric in statistics and machine learning to \nestimate the accuracy of predictions in regression assign-\nments. It computes the average magnitude of the discrepan-\ncies between anticipated and actual (observed) values in a \ndataset [42]. A lower RMSE number shows that the forecasts \nare closer to the actual values on average, indicating superior \npredictive accuracy. A greater RMSE value, on the other \nTable 5  RMSE over 5-s \nprediction horizon for US-101 \nand US-I-80 dataset\nSeconds US-101 US-I-80\n1¬†s 0.47 0.52\n2¬†s 0.84 0.85\n3¬†s 1.33 1.34\n4¬†s 1.97 1.93\n5¬†s 2.56 2.47\nFig. 8  Error analysis of trajec-\ntory prediction in autonomous \nvehicle movement\n(a) Longitudinal Position Error (b) LateralPosition Error\nFig. 9  Predicted vs actual trajectory comparison over US-101 dataset\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 17 of 20    87 \nhand, indicates that the forecasts differ more from the actual \nvalues. The RMSE equation can be stated as follows:\nHere, n is the size of the test set, (x‚àói\ntp\n, y‚àói\ntp\n) and (xi\ntp\n, yi\ntp\n) are the \npredicted position of the target vehicle in data i at time tp and \nthe corresponding ground truth, respectively. RMSE is cal-\nculated for each predictive time step tp  for future twelve \ntimesteps.\nThe results of our model‚Äôs predictions are provided in \nthis section using both visual representation and a predic-\ntion horizon. Most of the literature observes a 3-s history \ntrajectory for predicting a 5-s future trajectory. For more \naccurate prediction, we divided the total 8-s timestamp into \nthe history trajectory interval (0‚Äì3.2¬†s) and the predicted tra-\njectory interval (3.2‚Äì8¬†s). We took into account timestamps \nat a 0.4-s gap within each interval. In the history interval, we \ntracked the target vehicle‚Äôs trajectory over 9 timestamps, and \nin the prediction interval, we anticipated its trajectory over \n12 timestamps. The historical data served as the input for our \nprediction model, providing context for predicting its future \n(30)RMSE (tp )=\n/uni221A.t/uni221A.x/uni221A.x/uni221A.s41\nn\nn/uni2211.s1\ni=1\n((x‚àói\ntp\n‚àí xi\ntp\n)2 +( y‚àói\ntp\n‚àí yi\ntp\n)2).\ntrajectory, and we used our trained model to forecast the \ntarget vehicle‚Äôs trajectory for future timestamps. A dynamic \npicture of the vehicle‚Äôs movement over time was produced as \na result of the predictions, which were made at intervals of \n0.4¬†s. However, to make a fair comparison with comparing \nwith other existing models, we have predicted trajectory for \na 5-s horizon by observing 3-s history data that have been \nillustrated in Table¬†5.\nBesides RMSE as a base analysis, we further assess the \naverage displacement error (ADE) [43] of our TrajectoFor -\nmer model. The measure of ADE is defined as follows:\nwhere N is the number of data points, xpred\ni  and ypred\ni  are the \npredicted longitudinal and lateral positions, respectively, and \nxtrue\ni  and ytrue\ni  are the actual longitudinal and lateral positions \nof the vehicle.\nWe analyze the errors in two scenarios: lane-keeping and \nlane-changing maneuvers. The prediction errors are catego-\nrized into longitudinal and lateral position errors. The error \nlevel are depicted in Fig.¬†8a, b, which are bar charts with the \nADE represented by bars and the range of error limits from \n(31)ADE = 1\nN\nN/uni2211.s1\ni=1\n/uni221A.s1\n(xpred\ni ‚àí xtrue\ni )2 +( ypred\ni ‚àí ytrue\ni )2,\nFig. 10  Predicted vs actual trajectory comparison over I-80 dataset\nTable 6  Comparison with state \nof the arts in terms of RMSE Model 1¬†s 2¬†s 3¬†s 4¬†s 5¬†s\nUS-101 C-VGMM 0.66 1.56 2.75 4.24 5.99\nGAIL-GRU 0.69 1.51 2.55 3.65 4.71\nIETP-CS 0.52 1.16 1.94 2.95 4.24\nDSCAN 0.58 1.26 2.03 2.98 4.13\nSIT-ID 0.58 1.23 1.99 2.96 4.05\nOurs 0.47 0.84 1.33 1.97 2.56\nI-80 C-VGMM 0.72 1.69 2.60 4.32 6.15\nGAIL-GRU 0.67 1.63 2.72 3.74 4.75\nIETP-CS 0.50 1.09 2.14 2.87 4.18\nDSCAN 0.67 1.44 2.19 2.98 4.22\nSIT-ID 0.49 1.46 2.35 3.65 4.86\nOurs 0.52 0.85 1.34 1.93 2.47\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 18 of 20\nfirst quartile (25%) to third quartile (75%) values, indicating \nthe variation in prediction accuracy.\nWe showed a visual representation of the target vehicle‚Äôs \ntrajectory in the presence of surrounding neighbors to give \na simple and intuitive comprehension of the predictions \nmade by our study. The visual representations further dem-\nonstrated the alignment between the ground truth and real \ntrajectories, showing our method‚Äôs effectiveness. Figure¬† 9 \ncompares the predicted trajectory to the actual trajectory for \nthe NGSIM US-101 dataset, while Fig.¬†10 does the same for \nthe NGSIM I-80 dataset. This comparison illustrates that our \nmethod successfully captured the movement patterns of the \nvehicle. The projections from our model are very close to \nthe actual trajectory, though some deviations are unavoid-\nable due to the inherent uncertainty of forecasting real-world \nvehicle trajectories.\n8.4  Comparison with¬†Benchmark\nTable¬†6 reports the performance of our implemented tra-\njectory prediction system TrajectoFormer on both NGSIM \ndatasets (US-101 and I-80) against the state-of-the-art tra-\njectory prediction approaches. The analysis focuses on the \nroot mean squared error (RMSE) measure, which offers a \nquantitative evaluation of the precision of our trajectory \nforecasts in contrast to various existing methods. These \nprediction models are those from the literature that report \nRMSE on any highway trajectory dataset over a 5-s predic-\ntion horizon. These models include the Gaussian mixture-\nbased approaches like C-VGMM+VIM [27], the genera-\ntive adversarial imitation learning model like GAIL-GRU \n[9], and IETP-CS [10], DSCAN [44] and SIT-ID [36]. Our \nmodel reports the least RMSE for a 5-s prediction horizon. \nThe results show that our model outperforms baseline mod-\nels in the 5-s prediction horizon and that our proposed model \nhas a competitive RMSE value when compared to the estab-\nlished benchmarks. It can be said that our eight-neighbor \nconstruction strategy to extract spatio-temporal dependency \nplayed a significant role in achieving low RMSE value as \neven when used different models than our TrajectoFormer \nsystem in ablation study the obtained result shown in table¬†3 \nsurpass most other existing benchmarks. And, finally, our \nTransformer-based TrajectoFormer system attains lowest \nRMSE value and predicts future trajectories that are closely \naligned to ground truth.\nAs an alternative approach to RMSE analysis, we also \nconduct final displacement error(FDE) [43] to evaluate and \ncompare our model with existing benchmarks. While FDE \nmay not be as widely recognized or as impactful as RMSE, \nit is nevertheless sufficiently significant to be considered as \na secondary option for model evaluation, as well as useful \nfor visualize the error level compare to others. The Fig.¬† 11 \ndisplays the outcome of FDE analysis for a 5-s prediction of \na 3-s observed trajectory in terms of both longitudinal and \nlateral positional error analysis, using the identical experi-\nmental conditions as previous; For this circumstance we \nconduct the FDE analysis only over NGSIM I-80 dataset.\nThis line graph clearly demonstrates the enhanced perfor-\nmance of our proposed TrajectoFormer system compared to \nother existing benchmarks.\n9  Conclusion\nIn this paper, we proposed a novel transformer-based \ntrajectory prediction framework by constructing a neigh-\nborhood for each target vehicle that considers spatio-\ntemporal dependency between the vehicles. The precise \nintegration of eight-neighbor concepts for each target \nvehicle is a noteworthy contribution to our research. This \n(a) Longitudinal position error (b) Lateral position error\nFig. 11  Final displacement error of trajectory prediction\nInternational Journal of Computational Intelligence Systems           (2024) 17:87  \n Page 19 of 20    87 \nneighborhood captures the deep interdependencies that \nexist between the ego vehicle and its near surroundings \nand results in a trajectory prediction model that better \nmimics the dynamics of real-world traffic events. Employ -\ning a transformer-based model with positional encoding \nalso enables our model to learn the latent features of the \nspatio-temporal data, providing better prediction accuracy. \nThe performance evaluation with known state-of-the-art \nmodels indicates that neighborhood construction in the \npreprocessing stage enhanced the prediction accuracy for \nour proposed model. Our suggested model surpasses most \nof the benchmarks in terms of RMSE value over a 5-s \nprediction horizon. It demonstrates our model‚Äôs candidacy \nin the long-term trajectory prediction of autonomous vehi-\ncles; with considering neighborhood interactions. Though \nour model is capable of navigating a collision-less future \npath for autonomous vehicles, in the future, we want to \nincorporate a couple of additions to our model. Our future \nundertakings will focus on creating an advanced collision \ndetection system. Our goal will be to create a reliable \nsystem for spotting probable collisions in real time rather \nthan just forecasting collision-less future paths.Different \ndriving circumstances result in a wide range of driving \nbehaviors. Recognizing and adjusting to these actions is \nan important component of self-driving. Our future work \nwill also focus on expanding the model to account for the \nmultimodality of driving behaviors, providing a more \ncomplete picture of on-road events.\nAuthor Contributions Both FA and KG contributed equally to this \nwork. FA analyzed and interpreted existing benchmarks in this field, \ndid the literature review, defined the problem statement and conducted \nthe data preprocessing stage. KG developed and implemented the Tra-\njectoFormer framework of this study and also performed the result \nevaluation portions. Both KG and FA took responsibility for writing the \nmanuscript. BS suggested model architecture, supervised this research \nwork, and helped with his valuable guidance and review. All authors \nhave read and approved the final manuscript.\nFunding No financial support/funding was provided to complete this \nresearch work.\nAvailability of Data and Materials Data and materials can be available \non request by mailing the corresponding author.\nDeclarations \nConflict of interest No conflict of interest.\n Ethics approval The submitted work is original and has not been pub-\nlished elsewhere in any form or language.\n Consent to participate All authors have approved this manuscript and \nagreed with its submission.\n Consent for publication All authors have agreed with the publication \nprocess.\nCode availability Code will be available on request by mailing the cor-\nresponding author.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article‚Äôs Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article‚Äôs Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Staudemeyer, R.C., Morris, E.R.: Understanding lstm‚Äîa tutorial \ninto long short-term memory recurrent neural networks (2019). \narXiv preprint arXiv: 1909. 09586\n 2. Deo, N., Trivedi, M.M.: Multi-modal trajectory prediction of \nsurrounding vehicles with maneuver based lstms. In: 2018 IEEE \nIntelligent Vehicles Symposium (IV), pp. 1179‚Äì1184. IEEE \n(2018)\n 3. Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., \nSavarese, S.: Social lstm: Human trajectory prediction in crowded \nspaces. In: Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, pp. 961‚Äì971 (2016)\n 4. Canziani, A., Paszke, A., Culurciello, E.: An analysis of deep \nneural network models for practical applications (2016). arXiv \npreprint arXiv: 1605. 07678\n 5. Casas, S., Luo, W., Urtasun, R.: Intentnet: learning to predict \nintention from raw sensor data. In: Conference on Robot Learn-\ning, pp. 947‚Äì956. PMLR (2018)\n 6. Nikhil, N., Tran¬†Morris, B.: Convolutional neural network for \ntrajectory prediction. In: Proceedings of the European Conference \non Computer Vision (ECCV) Workshops (2018)\n 7. O‚ÄôShea, K., Nash, R.: An introduction to convolutional neural \nnetworks (2015). arXiv preprint arXiv: 1511. 08458\n 8. Kaplan, D.: An overview of markov chain methods for the study \nof stage-sequential developmental processes. Dev. Psychol. 44(2), \n457 (2008)\n 9. Kuefler, A., Morton, J., Wheeler, T., Kochenderfer, M.: Imitating \ndriver behavior with generative adversarial networks. In: 2017 \nIEEE Intelligent Vehicles Symposium (IV), pp. 204‚Äì211. IEEE \n(2017)\n 10. Li, Z., Lin, Y., Gong, C., Wang, X., Liu, Q., Gong, J., Lu, C.: An \nensemble learning framework for vehicle trajectory prediction in \ninteractive scenarios. In: 2022 IEEE Intelligent Vehicles Sympo-\nsium (IV), pp. 51‚Äì57. IEEE (2022)\n 11. Deo, N., Trivedi, M.M.: Convolutional social pooling for vehicle \ntrajectory prediction. In: Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition Workshops, pp. 1468‚Äì\n1476 (2018)\n 12. De Iaco, R., Smith, S.L., Czarnecki, K.: Universally safe swerve \nmaneuvers for autonomous driving. IEEE Open J. Intell. Transp. \nSyst. 2, 482‚Äì494 (2021)\n 13. Fortunato, S.: Community detection in graphs. Phys. Rep.-Rev. \nSec. Phys. Lett. 486, 75‚Äì174 (2010)\n 14. Lef√®vre, S., Vasquez, D., Laugier, C.: A survey on motion predic-\ntion and risk assessment for intelligent vehicles. ROBOMECH J. \n1(1), 1‚Äì14 (2014)\n International Journal of Computational Intelligence Systems           (2024) 17:87 \n   87  Page 20 of 20\n 15. Cao, H., Song, X., Zhao, S., Bao, S., Huang, Z.: An optimal \nmodel-based trajectory following architecture synthesising the lat-\neral adaptive preview strategy and longitudinal velocity planning \nfor highly automated vehicle. Veh. Syst. Dyn. 55(8), 1143‚Äì1188 \n(2017)\n 16. Hu, Y., Fu, J., Wen, G.: Safe reinforcement learning for model-\nreference trajectory tracking of uncertain autonomous vehicles \nwith model-based acceleration. IEEE Trans. Intell. Veh. (2023)\n 17. Welch, G., Bishop, G., et¬†al.: An introduction to the Kalman filter \n(1995)\n 18. Hastings, W.K.: Monte carlo sampling methods using markov \nchains and their applications (1970)\n 19. Mihajlovic, V., Petkovic, M.: Dynamic Bayesian networks: a state \nof the art. University of Twente Document Repository (2001)\n 20. Hayes-Roth, F.: Rule-based systems. Commun. ACM 28(9), 921‚Äì\n932 (1985)\n 21. Eddy, S.R.: Hidden Markov models. Curr. Opin. Struct. Biol. 6(3), \n361‚Äì365 (1996)\n 22. Leon, F., Gavrilescu, M.: A review of tracking and trajectory pre-\ndiction methods for autonomous driving. Mathematics 9 (6), 660 \n(2021)\n 23. Medsker, L.R., Jain, L.: Recurrent neural networks. Des. Appl. \n5(64‚Äì67), 2 (2001)\n 24. Lee, N., Choi, W., Vernaza, P., Choy, C.B., Torr, P.H., Chan-\ndraker, M.: Desire: Distant future prediction in dynamic scenes \nwith interacting agents. In: Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition, pp. 336‚Äì345 (2017)\n 25. Kim, B., Kang, C.M., Kim, J., Lee, S.H., Chung, C.C., Choi, J.W.: \nProbabilistic vehicle trajectory prediction over occupancy grid \nmap via recurrent neural network. In: 2017 IEEE 20Th Interna-\ntional Conference on Intelligent Transportation Systems (ITSC), \npp. 399‚Äì404. IEEE (2017)\n 26. Suraj, M., Grimmett, H., Platinsk·ª≥, L., Ondruska, P.: Predicting \ntrajectories of vehicles using large-scale motion priors. In: 2018 \nIEEE Intelligent Vehicles Symposium (IV), pp. 1639‚Äì1644. IEEE \n(2018)\n 27. Deo, N., Rangesh, A., Trivedi, M.M.: How would surround vehi-\ncles move? a unified framework for maneuver classification and \nmotion prediction. IEEE Trans. Intell. Veh. 3(2), 129‚Äì140 (2018)\n 28. Andersson, J.: Predicting vehicle motion and driver intent using \ndeep learning (2018)\n 29. Baheri, A.: Safe reinforcement learning with mixture density \nnetwork, with application to autonomous driving. Res. Control \nOptim. 6, 100095 (2022)\n 30. Xu, Y., Zhao, T., Baker, C., Zhao, Y., Wu, Y.N.: Learning trajec-\ntory prediction with continuous inverse optimal control via Lan-\ngevin sampling of energy-based models (2019). arXiv preprint \narXiv: 1904. 05453\n 31. Federal Highway Administration‚Äôs¬†(FHWA), U.: Traffic Analysis \nTools: Next Generation Simulation‚ÄîFHWA Operations. https:// \nops. fhwa. dot. gov/ traffi  cana lysis tools/ ngsim. htm. Accessed 9 Oct \n2023\n 32. Kovvali, V.G., Alexiadis, V., Zhang¬†PE, L.: Video-based vehicle \ntrajectory data collection. Technical report (2007)\n 33. ITS DataHub: Next Generation Simulation (NGSIM) Open Data. \nhttps:// datah ub. trans porta tion. gov/ stori es/s/ Next- Gener ation- \nSimul ation- NGSIM- Open- Data/ i5zb- xe34/. Accessed 16 Sep \n2023\n 34. Mo, X., Xing, Y., Lv, C.: Graph and recurrent neural network-\nbased vehicle trajectory prediction for highway driving. In: 2021 \nIEEE International Intelligent Transportation Systems Conference \n(ITSC), pp. 1934‚Äì1939. IEEE (2021)\n 35. Norris, J.R.: Markov Chains. Cambridge Series in Statistical and \nProbabilistic Mathematics. Cambridge University Press, Cam-\nbridge (1998)\n 36. Li, X., Xia, J., Chen, X., Tan, Y., Chen, J.: Sit: a spatial interac-\ntion-aware transformer-based model for freeway trajectory predic-\ntion. ISPRS Int. J. Geo Inf. 11(2), 79 (2022)\n 37. Kazemnejad, A.: Transformer Architecture: The Positional Encod-\ning - Amirhossein Kazemnejad‚Äôs Blog. https:// kazem nejad. com/ \nblog/ trans former_ archi tectu re_ posit ional_ encod ing/. Accessed 17 \nSep 2023\n 38. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., \nGomez, A.N., Kaiser, ≈Å., Polosukhin, I.: Attention is all you need. \nAdv. Neural Inf. Process. Syst. 30 (2017)\n 39. Tang, C., Salakhutdinov, R.R.: Multiple futures prediction. Adv. \nNeural Inf. Process. Syst. 32 (2019)\n 40. Mozaffari, S., Sormoli, M.A., Koufos, K., Dianati, M.: Multi-\nmodal manoeuvre and trajectory prediction for automated driving \non highways using transformer networks. IEEE Robot. Automat. \nLett. (2023)\n 41. Liu, J., Luo, Y., Zhong, Z., Li, K., Huang, H., Xiong, H.: A proba-\nbilistic architecture of long-term vehicle trajectory prediction for \nautonomous driving. Engineering 19, 228‚Äì239 (2022)\n 42. c3.ai: Root Mean Square Error (RMSE). https:// c3. ai/ gloss ary/ \ndata- scien ce/ root- mean- square- error- rmse/. Accessed 17 Sep \n2023\n 43. Hou, L., Xin, L., Li, S.E., Cheng, B., Wang, W.: Interactive trajec-\ntory prediction of surrounding road users for autonomous driving \nusing structural-lstm network. IEEE Trans. Intell. Transp. Syst. \n21(11), 4615‚Äì4625 (2019)\n 44. Yu, J., Zhou, M., Wang, X., Pu, G., Cheng, C., Chen, B.: A \ndynamic and static context-aware attention network for trajectory \nprediction. ISPRS Int. J. Geo Inf. 10(5), 336 (2021)\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}