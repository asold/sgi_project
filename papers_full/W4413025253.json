{
  "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining",
  "url": "https://openalex.org/W4413025253",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Deyu Cao",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2129154511",
      "name": "Samin Aref",
      "affiliations": [
        "University of New Brunswick",
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Deyu Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129154511",
      "name": "Samin Aref",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4402683778",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W4402667003",
    "https://openalex.org/W4393147132",
    "https://openalex.org/W4392181806",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6608836677",
    "https://openalex.org/W4402683730",
    "https://openalex.org/W4399837121",
    "https://openalex.org/W4404783494",
    "https://openalex.org/W4402684305",
    "https://openalex.org/W6853804809",
    "https://openalex.org/W4408146288",
    "https://openalex.org/W4393147284",
    "https://openalex.org/W4393152626",
    "https://openalex.org/W4387686975",
    "https://openalex.org/W4404782328",
    "https://openalex.org/W4406650295",
    "https://openalex.org/W4402670692",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W4391709676",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3194676777",
    "https://openalex.org/W4405011791",
    "https://openalex.org/W4389520768",
    "https://openalex.org/W4402897324",
    "https://openalex.org/W4393146966",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4402684165",
    "https://openalex.org/W4404918643",
    "https://openalex.org/W4404133979"
  ],
  "abstract": null,
  "full_text": "Enhancing Ultra-Low-Bit Quantization of Large\nLanguage Models Through Saliency-Aware Partial\nRetraining\nDeyu Cao1[0009−0003−8563−3842] and Samin Aref2∗[0000−0002−5870−9253]\n1 The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8654, Japan\n2 Department of Mechanical and Industrial Engineering, University of Toronto,\n5 King’s College Rd, Toronto, ON M5S 3G8, Canada\n*Corresponding author’s email:s.aref@utoronto.ca\nAbstract. The growing use of large language models has raised environmental and\neconomic concerns about their intensity of resource usage during inference. Serv-\ning these models to each user requires substantial energy and water for cooling.\nModel compression techniques like quantization can shrink large language mod-\nels and make them more resource efficient at the cost of potential performance\ndegradation. Quantization methods compress model size through replacing their\nhigh-precision parameters by quantized values of lower precision. Among existing\nmethods, the ApiQ method achieves superior accuracy preservation at minimal\nmemory and time overhead. We investigate two ideas to extend performance in\nultra-low-bit quantization beyond ApiQ’s level. First, we look into combining ex-\nisting quantization-aware training techniques with ApiQ’s partial training. We\nshow that this does not outperform the baseline ApiQ method with limited train-\ning data and frozen weights. This leads to two key insights: (1) The substantial\nrepresentational capacity that is gained through full retraining is unlikely to be\nfeasible through partial training. (2) This gain may depend on using a large and\ndiverse dataset in quantization-aware training. Second, through a novel approach\ninformed by the two insights, we propose an ultra-low-bit quantization method\nthat builds upon ApiQ and extends its performance without the need for full re-\ntraining. This publicly available method relies on a saliency-aware regularization\nterm that prioritizes preserving the most impactful parameters during quantiza-\ntion. Our experiments on LLaMA 7B and 13B benchmarks demonstrate that our\nmethod reduces the ApiQ’s accuracy degradation by 10.85% and 7.54% respec-\ntively. A Python implementation of the proposed quantization method is publicly\navailable on GitHubhttps://github.com/TokuyuSou/ULB-SAPR.\nThis is a post-peer-review accepted manuscript from the proceedings of the 22nd International Confer-\nence on Modeling Decisions for Artificial Intelligence (MDAI’25). The publisher authenticated version\n(versionofrecord)andfullcitationdetailsareavailableonSpringer’swebsite(LectureNotesinArtificial\nIntelligence 15957). DOI:https://doi.org/10.1007/978-3-032-00891-6_28\nKeywords: Model compression · Low-rank adaptation · Large language mod-\nels · Quantization-aware training · Post-training quantization · Saliency-aware\nregularization\narXiv:2504.13932v3  [cs.LG]  30 Jul 2025\n2 D. Cao and S. Aref\n1 Introduction\nLarge Language Models (LLMs) have substantially advanced the field of artificial intelli-\ngence and specifically the area of natural language processing. However, their substantial\nmemory and computational requirements have led to economic and environmental concerns\nraised about their intensive use of resources. Moreover, there are technical challenges for\ndeploying LLMs on resource-constrained devices. Model compression techniques, including\nquantization, shrink LLMs in different ways and allow a more resource-efficient deployment\n[43]. Quantization refers to methods that reduce model size by decreasing the bit precision of\nmodel parameters [34,20]. The reduced model size, in turn, improves inference speed, which\nis especially useful for deploying models on edge devices [19,14]. The application of different\nquantization methods on LLMs has been widely studied in recent review papers [19,14].\nQuantization involves mapping high-precision values (typically in float32 or float16) to\nlower-precision representations (such as int8, int4, or even fewer bits). For instance, when\nquantizing to 8-bit integers, each parameter can take one of28 possible discrete values. The\nspecific formulation of a quantization method determines how this mapping is performed.\nIn Appendix C, we present the mathematical formulation and a numerical example of the\nsimplest quantization method, known as Round-To-Nearest quantization to provide technical\ndetails on concepts such as scaling factors, clipping thresholds, and zero points.\nCurrent quantization techniques for LLMs mainly fall into quantization-aware training\n(QAT) [16] and post-training quantization (PTQ) frameworks [10,42,43].\nQuantization-Aware Training (QAT):QAT involves simulating the quantization ef-\nfect during the training process itself [16]. This is done to mitigate the performance degra-\ndation introduced by the quantization of parameters. Through QAT, the model can learn to\nadjust its weights to accommodate the lower precision, potentially preserving more accuracy\n(compared to PTQ) at the cost of substantially more intensive computations. QAT usually\nrequires fine-tuning the model with quantization-aware layers that mimic the behavior of\nquantization during forward passes but use full precision in backward passes for gradient\ncalculations.\nPost-Training Quantization (PTQ):In PTQ, quantization is directly applied to the\nexisting pre-trained model without the need for retraining [10,21]. Some PTQ methods re-\nquire a small calibration dataset to calculate the activation magnitudes or identifysalient\nweights [40]. Salient weights are those that have a large effect on the output of either the\nlayer, block, or the final output of the model. PTQ-based methods are generally faster and\nmore resource-efficient compared to QAT-based methods [9]. However, without retraining,\nthese methods must rely on heuristics in the quantization process, such as the choice of the\nscaling factors and the design of the outlier retention [10,21]. While they have lower computa-\ntional costs, PTQ methods often result in larger performance degradation compared to QAT\nmethods. Particularly, it is suggested that the performance of models that are quantized to\nfewer than 3 bits by most PTQ methods degrades substantially [39].\nRepresentative approaches for PTQ and QAT are reviewed in Appendix D.1, and tech-\nniques for alleviating QAT’s computational burden are detailed in Appendix D.2.\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 3\nCombining QAT with Parameter-Efficient Fine-Tuning\nA recent line of work [6,29,13] combines QAT retraining with Parameter-Efficient Fine-\nTuning (PEFT) methods, such as Low-Rank Adaptation (LoRA) [17]. PEFT can serve two\npurposes: (1) recovering the performance loss caused by quantization and (2) fine-tuning the\nquantized model for better adaptation to downstream tasks.\nQLoRA [6] is the pioneering work in this direction. This approach integrates quantization\nand PEFT in a straightforward manner. The model is first quantized using a standard PTQ\nmethod. Then, the quantized LLM undergoes PEFT, where the quantized weights remain\nfixed, and only the LoRA parameters are fine-tuned to adapt to the specific task. Although\nQLoRA substantially reduces GPU memory and fine-tuning time while maintaining strong\nperformance at the 4-bit level, it suffers from considerable performance degradation below 4\nbits.\nApiQ (Activation-preserved initialization of Quantized LLMs) [23] aims to preserve the\nactivations of the full-precision model by jointly optimizing the quantization parameters and\nthe initial values of LoRA weights, rather than simply setting the initial values to zeros. This\noptimization is performed layer- or block-wise, proceeding sequentially from the lower layers\nto ensure that the output of each layer (or block) closely aligns with the original output.\nThis prevents the accumulation of quantization errors across layers (or blocks). The layer-\nor block-wise training approach also reduces the memory requirements for fine-tuning. We\nprovide more technical details on ApiQ in Section 2.2.\n2 Technical background\n2.1 Toward ultra-low-bit quantization\nBoth QAT- and PTQ-based methods have succeeded in nearly preserving the performance\nof full-precision models at 4-bit precision [38]. However, there remains substantial room for\nimprovement as the bit-width is reduced to 3, 2, or 1 bit (ultra-low bit). In the ultra-low-\nbit quantization regime, a major challenge is managing the salient weights, as they can\nunnecessarily extend the quantization range [32]. Several primary strategies are commonly\nemployed to address this challenge. The first approach involves retaining salient weights\nin higher precision, which results in a mixed-precision model [32,12]. The second approach\nis to increase the expressiveness of the quantization formulation to better accommodate\nsalient weights [2,39,15]. Here, some of the prominent approaches tackling the ultra-low-bit\nquantization problem are reviewed.\nPB-LLM [32] was the first work on 1-bit quantization for LLMs. It first filters out salient\nweights based on their magnitude (or their Hessian matrix) and then quantizes the remaining\nnon-salient weights using standard PTQ methods such as GPTQ [8]. It also explores a QAT-\nbased approach where the salient weights are frozen, and only the non-salient weights are\ntrained. However, to achieve competitive performance, this method requires storing up to\n30% of all weights as salient weights, effectively increasing the bit-width and necessitating\nunstructured mixed-precision operations.\nDB-LLM [2] explores the second perspective of increasing the expressiveness of the quan-\ntization formulation, specifically targeting the 2-bit regime. This method introduces a flexible\ndual binarization by representing 2-bit quantized weights as the scaled sum of two binary\n4 D. Cao and S. Aref\nsets. By optimizing the scaling factors for each binary set, it enables flexible non-uniform\nquantization. During inference, this approach benefits from the efficiency of binary compu-\ntations and the sparsity of binary representations.\nOneBit [39] also addresses this issue by increasing the expressiveness of the quantiza-\ntion formulation. It incorporates scaling factors for both the input and output dimensions,\nthereby achieving a more flexible representation of weights than RTN. OneBit approach elim-\ninates the need for storing separate information about the salient weights, reducing memory\noverhead and simplifying the inference process.\nBuilding on OneBit [39], BinaryMoS [15] draws inspiration from a mixture-of-experts\napproach by introducing multiple scaling experts and arouter3 that dynamically combines\nthem based on the input token. The BinaryMoS method showed that adding multiple scaling\nfactors only requires minimal memory overhead while substantially enhancing the represen-\ntational capacity of the scaling factors [15].\n2.2 The ApiQ method as a foundation\nResults on the perplexity of the reviewed quantization methods are provided in Table 7\n(in the Appendices) based on two LLMs: LLaMA-2-7B and LLaMA-2-13B. As shown in\nTable 7, the LoRA weights used in ApiQ can effectively mitigate quantization errors in low-\nbit regimes (down to 3 bits and 2 bits). ApiQ has been shown to outperform strong PTQ\nbaselines such as OmniQuant [33] and AWQ [25], along with preceding quantization methods\nintegrating PEFT, such as QLoRA [6] and LoftQ [22]. The ApiQ method is therefore chosen\nas a foundation for our proposed method. ApiQ comprises two stages: (1) a post-training\nquantization stage, and (2) a fine-tuning stage.\nApiQ was motivated by the observation that under PEFT, preserving the starting point\nand mitigating the propagation of quantization error from shallower layers to deeper layers\nare crucial for achieving better accuracy [24]. Given that LoRA [11] is arguably one of the\nmost widely used PEFT methods , we will limit the scope of our discussion to LoRA. In\ntraditional PEFT without quantization, LoRA weights initialized to zeros inherently preserve\nthe initial state. However, through quantization, the original weights undergo substantial\nmodifications, particularly in ultra-low-bit settings. This requires finding a more effective\ninitialization for LoRA weights to ensure that the combined weights remain as close as\npossible to the original full-precision weights.\nTo achieve the aforementioned objectives while conserving memory and computational\nresources during training, ApiQ employs block-wise training. In this approach, for each block\n(e.g., each transformer block), the quantization parameters (such as weight clippings) and\nLoRA weights of all layers within the block are jointly optimized to minimize the output\nerror, as defined in Equation 1.\narg min\nQs,As,Bs\n∥F(Ws, X) − F(Qs, As, Bs, Xq)∥ (1)\nIn Equation 1,F denotes the mapping function of a transformer block,Ws represents\nall the weights of the linear layers within this block,X is the input to this block,Qs are the\nquantized versions ofWs, As and Bs are all low-rank matrices within this block, andXq\n3 A router is a linear layer that outputs a weight for each scaling expert based on the input token.\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 5\nis the input to the quantized block, which is also the output from the preceding quantized\nblock.\nApiQ is also compared to previous methods such as LoftQ [22] in Table 7 of Appendices.\nLoftQ, which optimizes quantization parameters for each layer independently with the ob-\njective of preserving the original weights. Compared to LoftQ, ApiQ offers the advantage of\nmitigating the propagation of quantization errors. This is achieved by allowing errors intro-\nduced in one layer to be further minimized through optimization in the subsequent layer, at\nthe expense of requiring additional time for training.\nIn our usage of ApiQ as a foundation, we will focus on the first stage of ApiQ, viewing it\nas a PTQ method. Specifically, we optimize only the initialized PEFT parameters (i.e. LoRA\nweights) via the block-wise training procedure described, without conducting additional fine-\ntuning on downstream tasks.\n3 Investigation of preliminary ideas\nAs mentioned in Section 2.2, ApiQ has demonstrated strong performance under 2-bit quan-\ntization through its novel approach. However, ApiQ employs a relatively simplistic quantiza-\ntion framework, where the clipping ranges are the sole learnable parameters. This limitation\nsuggests the potential for performance improvement by adopting more expressive quantiza-\ntion formulations including those proposed in other existing works [15,2].\nIt is noteworthy that several high-performing methods for ultra-low-bit quantization rely\non quantization-aware training (QAT) [15,2], which entails substantial computational and\nmemory overhead. By leveraging the strong representational capacity of quantization algo-\nrithms from these methods and combining it with the low training cost of ApiQ, it may be\npossible to strike a balance between performance and computational cost in a new quanti-\nzation method.\nTo investigate this hypothesis, we incorporated quantization formulations from two re-\ncently proposed and high-performing methods: BinaryMoS [15] and DB-LLM [2]. Although\nthese investigations ultimately proved ineffective as shown in Appendix F, they provided us\nimportant insights and paved the way for our proposed method (discussed in Section 4).\n3.1 Combining ApiQ with BinaryMoS\nBinaryMoS [15] has outperformed all its competitors in terms of accuracy for 1-bit quantiza-\ntion. It is inspired by themixture of expertsapproach, incorporating multiple scaling experts\nand a router that dynamically combines them based on the input token.\nHowever, as a QAT-based approach, BinaryMoS incurs relatively high computational and\nmemory costs. Therefore, we hypothesize that leveraging LoRA weights to compensate for\naccuracy loss, instead of retraining the original weights, could potentially reduce training\ntime and/or memory consumption.\nTo investigate this idea, we replaced the quantized linear layer in the ApiQ codebase\nwith its counterpart from BinaryMoS and trained the quantization parameters alongside\nLoRA weights in a block-wise manner, following the same approach as ApiQ (described in\nSection 2.2). The results for this investigation are provided in Appendix F.1. Contrary to our\ninitial hypothesis, our combined method exhibited substantially poorer performance than the\n6 D. Cao and S. Aref\noriginal BinaryMoS approach. This result suggests an inherent limitation in the ability of\nLoRA weights to counteract the accuracy loss induced by quantization, especially under the\nstringent constraint of 1-bit quantization.\n3.2 Combining ApiQ with DB-LLM\nDB-LLM [2] is a method specifically designed for 2-bit quantization and has some of the\nbest 2-bit performance measures among the techniques reviewed in Section 1. This method\nintroduces flexible dual binarization, which represents 2-bit quantized weights as a scaled\nsum of two binary sets. By optimizing the scaling factors for each binary set, it enables\nflexible and non-uniform quantization.\nFollowing the same approach that we had with BinaryMoS, we replaced the quantized\nlinear layer in ApiQ with its counterpart from DB-LLM and applied the same training\nprocedure as used in ApiQ. The results for this approach are provided in Appendix F.2. We\nfound that applying the DB-LLM quantization formulation augments the model’s expressive\npower.However,italsoheightensitssusceptibilitytooverfitting,causingthequantizedmodel\nto perform worse when trained on a dataset of the same size.\n4 Proposed method\nLike most methods that rely on a calibration dataset, ApiQ is prone to overfitting to the\ncalibration dataset. A sign of overfitting can be observed in the perplexity gap between the\nWikiText-2 and C4 datasets, which is shown in Fig. 3 of Appendices. For the model quantized\nwith ApiQ, this gap becomes larger than that of the original full-precision model. We focused\non addressing this issue while preserving its advantage of mitigating output errors.\nA widely used approach to addressing the overfitting problem is the addition of a regular-\nization term. Since this method does not require additional data or introduce new trainable\nparameters, it should incur minimal memory and computational overhead, making it a suit-\nable component for our proposed approach.\n4.1 Preservation of original weights\nWe first investigated a naive regularization approach that involves preserving the original\nweights by incorporating the L2 distance between the original and quantized weights into\nthe loss function. EasyQuant [35] adopted this regularization term as the only term in loss\nfunction and demonstrated promising results for 4-bit quantization. Notably, this approach\neliminates the risk of overfitting, as it does not depend on values from the calibration dataset.\nMoreover, it is intuitively reasonable that preserving the original weights as much as possi-\nble contributes to maintaining the original performance. We evaluated two variants of this\nregularization strategy.\nIn the first variant, the penalty term is defined as the difference between the original\nweights and the quantized weights before adding the LoRA component (hereafter referred\nto as \"BeforeLoRA\"). In the second variant, the penalty term is defined as the difference\nbetween the original weights and the final weights after the LoRA adjustments have been\napplied (hereafter referred to as \"AfterLoRA\"). The results are provided in Appendix G.\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 7\nIn short, even with hyperparameter tuning, this naive regularization does not exceed the\nperformance of the original ApiQ method, motivating our saliency-aware approach.\n4.2 Saliency-aware preservation of original weights\nThe previous naive weight preservation method assigns equal importance to all parameters\nwithin a weight matrix (or a subset thereof). However, as demonstrated in prior studies such\nas SqueezeLLM [18], certain parameters (the salient parameters) have a greater impact on\nthe model’s output. At this stage, we hypothesize that prioritizing the preservation of these\nsalient weights leads to improved model performance. To investigate this idea, we use the\nregularization term in Eq. (2), wherewi refers to each parameter in the weight matrix,Q\ndenotes the quantization function, andαi represents the saliency of the parameter, which is\nused as its weight in the regularization term.\nNX\ni=1\nαi(wi − Q(wi))2. (2)\nWe calculatedαi in the same manner as SqueezeLLM [18], which approximates the Hessian\nmatrix using squared gradients. The gradients were calculated using a small calibration set\nof 100 samples, each with a sequence length of 512, drawn from either the WikiText-2 or C4\ndataset. The results are shown in Section 6.1.\n4.3 Training framework\nThe training framework is largely consistent with the approach described in Section 2.2, with\nthe only difference being the addition of a regularization term to the loss function. Schematic\noverviews of the training framework are presented in Figs. 1a–1b. Fig. 1a illustrates the\nprocess of obtaining the output weights (i.e., the weights used in the quantized model).\nIn Figs. 1a–1b, rectangles with a red background indicate values that are updated during\ntraining, while those with a light blue background indicate values that remain fixed. The\nsame color code is used in Fig. 1b that depicts the flow of block-wise training. This procedure\nis applied sequentially from the first block to the final block (the 32nd block in the case of\nthe LLaMA-2-7B model), using the output of the preceding block as the input for the next.\n5 Experimental setup\n5.1 Implementation details\nThe ApiQ method requires a calibration dataset to optimize both the quantization param-\neters and the LoRA adapter weights. For this purpose, we randomly sampled a subset of\nsentences from the training set of WikiText-2 [28]. Given its superior accuracy and time ef-\nficiency compared to layer-wise optimization[23], we conducted our experiments exclusively\nusing block-wise optimization. In all experiments, we used the hyperparameter configuration\nlisted in Table 3 of Appendices, unless stated otherwise.\n8 D. Cao and S. Aref\nQuantized Weights\nFull-PrecisionWeights\nQuantization\nQuantizationParameters\nLoRA Weights (BA)\nOutput Weights\nFrozen Parameters\nTrainable Parameters\n(a) The process for obtaining output\nweights\nTransformer Block 1\nOutput Weights 1\nFull-\nPrecision\nInput\nTransformer Block 1\nFull-Precision\nWeights 1\nTransformer Block 2\nFull-Precision\nWeights 2\nTransformer Block 2\nOutput Weights 2\nMSE Weighted\nby Saliency\nMSE\nOutput of\nQuantized Block 1\nOutput of Full-\nPrecision Block 1 Final Loss 1\nInput of\nQuantized Block\n2\nInput of Full-\nPrecision Block\n2\nBlock 1\nMSE Weighted\nby Saliency\nMSE\nOutput of\nQuantized Block 2\nOutput of Full-\nPrecision Block 2 Final Loss 2\nInput of\nQuantized Block\n3\nInput of Full-\nPrecision Block\n3\nSaliency values 1\nSaliency values 2\nBlock 2 (b) Block-wise training flow\nFig.1: Schematic overviews of the training framework. These high-resolution figures can be\nmagnified on screen for the details.\n5.2 Target language models\nThe majority of our evaluations were conducted on the LLaMA-2-7B model. However, to\nconfirm the consistency of our proposed method, additional experiments were performed on\ntheLLaMA-2-13B model,as presented inpart (b)ofTable1. OurLLaMA-2-13Bexperiments\nwere mainly centred on evaluating the saliency-aware weight preservation term, which—as\nshown in Section 6.1—demonstrated promising results on the 7B model.\n5.3 Metrics and datasets used\nTo assess the effectiveness of quantization, namely the extent to which the accuracy of the\noriginal full-precision model is preserved, we used perplexity (PPL) as the evaluation metric.\nTo align with previous research, we used the test set of WikiText-2 [28] and validation set\nof C4 [30]. Also, consistent with the literature, we assessed the zero-shot accuracy on five\ncommon-sensereasoningtasks,namelyWinoGrande[31],PIQA[1],HellaSwag[41],Arc-Easy\n[5], and Arc-Challenge [5].\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 9\n6 Results\n6.1 Evaluation of the proposed method\nInthissection,weprovidetheresultsofthesaliency-awareweightpreservationtermproposed\nin Section 4.2. The results of the naive weight preservation term (introduced in Section 4.1)\nare provided in Appendix G.\nTable 1: Performance of ApiQ with saliency-aware weight preservation regularization on\nLLaMA-2 models\n(a) 7B model\nLoRA Variant| Calibration Dataset| Coefficient PPL↓ Accuracy (%)↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model 5.47 6.97 69.22 57.16 43.52 76.26 78.07 64.85\nOriginal ApiQ (No regularization) 7.56 10.42 62.83 46.53 31.57 66.54 72.09 55.91\nAfterLoRA WikiText-2 1 7.54 10.18 64.09 47.08 32.0867.55 72.63 56.69\nAfterLoRA C4 1 7.53 10.34 63.22 46.80 32.42 66.67 72.25 56.27\nBeforeLoRA WikiText-2 1 7.58 10.65 63.46 46.35 32.25 67.13 71.71 56.18\nBeforeLoRA C4 1 7.57 10.38 62.59 46.12 32.8567.17 72.20 56.18\n(b) 13B model\nLoRA Variant| Calibration Dataset| CoefficientPPL↓ Accuracy (%)↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model 4.88 6.47 72.22 60.07 48.29 79.42 79.05 67.81\nOriginal ApiQ (No regularization) 6.46 8.93 67.72 51.70 37.29 73.02 74.81 60.91\nAfterLoRA WikiText-2 2 6.45 8.90 67.64 51.98 38.23 73.48 75.7361.41\nAfterLoRA C4 2 6.43 9.66 67.96 52.01 38.65 73.06 75.46 61.43\nWe tested using different coefficients for the regularization term and evaluated the two\nvariants (Before LoRA and After LoRA described in Section 4.1). Additionally, we exper-\nimented with different calibration datasets (C4 and WikiText-2). The results are provided\nin Tables 1–2. Table 1 presents the comparison between the two variants and different cali-\nbration datasets. Table 2 shows the results when varying the regularization term coefficient,\nwith the After-LoRA variant and WikiText-2 calibration dataset fixed.\nIn Table 2, the coefficient multiplier column indicates the factor by which the regulariza-\ntion term coefficient is scaled for each layer4.\nAcrossawiderangeofhyperparameters,theinclusionofthisregularizationtermenhances\nthe average zero-shot accuracy of the quantized model by up to 0.97 % in absolute terms\nfor LLaMA-2-7B model as shown in Tables 1–2. This improvement covers 10.85% of the\naccuracy gap between the ApiQ method and the full-precision model of LLaMA-2-7B.\nSimilarly, for LLaMA-2-13B model, we observed consistent improvements across all few-\nshot tasks, with an average zero-shot accuracy increase of 0.52% in absolute terms. This\n4 This adjustment is intended to account for the increasing quantization loss in deeper layers, ensuring that the\nregularization term is aligned with the loss.\n10 D. Cao and S. Aref\nTable 2: Performance of ApiQ with saliency-aware weight preservation regularization on\nLlaMA-2-7B model (with different coefficients for the regularization term)\nPPL↓ Accuracy (%)↑\nInitial Coefficient| Coefficient MultiplierWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model 5.47 6.97 69.22 57.16 43.52 76.26 78.07 64.85\nOriginal ApiQ (No regularization) 7.56 10.42 62.83 46.53 31.57 66.54 72.09 55.91\n1.0 1.0 7.54 10.18 64.09 47.08 32.08 67.55 72.63 56.69\n2.0 1.0 7.57 10.26 64.80 47.11 32.34 67.21 72.96 56.88\n10.0 1.0 7.72 10.75 63.38 44.99 31.31 66.04 71.24 55.40\n1e-3 1.3 7.53 10.41 64.25 46.82 33.0266.04 72.63 56.55\n1e-2 1.3 7.54 10.36 62.98 46.90 32.00 65.61 72.52 56.00\n1e-1 1.3 7.62 10.43 62.51 46.03 32.68 67.51 72.58 56.26\ncorresponds to 7.54% of the accuracy gap, further confirming the consistency and effective-\nness of our approach.\n7 Discussions\nWe pursued two primary lines of investigation: (1) examining whether existing QAT-based\nquantization methods (specifically BinaryMoS [15] and DB-LLM [2]) could improve an ApiQ-\nbased quantization method [23] under constrained retraining; and (2) exploring saliency-\naware weight preservation regularization to enhance ApiQ’s accuracy by mitigating over-\nfitting. The results and implications from the first line of investigation in Appendix F. In\nsummary, we confirmed that maintaining high accuracy in QAT-based methods requires re-\ntraining the original model weights to ensure sufficient representational capacity, and lever-\naging large, diverse datasets to prevent overfitting.\nOur second investigation was centered on the trade-off between two common training\nobjectives—weight preservation and output preservation—within the framework of ApiQ.\nDespite the effectiveness of naive weight preservation under 4-bit quantization [22], ApiQ\nrevealed that this approach performs poorly under 2-bit quantization, with perplexity of\nquantized LLaMA-2-7B model exceeding 500 on WikiText-2 and C4 datasets. This can be\nattributed to the accelerated accumulation of output errors in deeper layers. While ApiQ\nachieved substantially better performance in the 2-bit setting, it exhibited noticeable over-\nfitting. We argue that the overfitting of ApiQ can be attributed to its exclusive reliance on\noutput preservation, which is highly dependent on the quality and size of the calibration\ndata. To address this drawback of ApiQ, our proposed method incorporates both weight and\noutput preservation objectives.\nThe results in Appendix G show that introducing a naive weight preservation objec-\ntive leads to degraded performance. This degradation in performance is consistent with the\nobserved ineffectiveness of the LoftQ method under 2-bit quantization [22].\nOn the other hand, experimental results in Section 6.1 showed that incorporating our\nproposed saliency-aware regularization term yielded modest yet consistent improvements in\nperplexity and zero-shot accuracy— recapturing 7.54% and 10.85% of the performance gap\nbetween baseline ApiQ and the original model of LLaMA-2-7B and LLaMA-2-13B respec-\ntively. This performance boost verifies that using the preservation of salient weights as a\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 11\ntraining target can be effective in reducing the accumulation of output errors and alleviating\noverfitting to the training data at the same time.\nTheimprovementachievedbysaliency-awareweightpreservationovernaiveweightpreser-\nvation is aligned with the insights from the SqueezeLLM model [18]. In particular, our results\nreaffirm the results from [18] highlighting the benefits of assigning greater importance to\nsalient weights in non-uniform quantization, particularly under 4- bit and 3-bit compression\nregimes.\nFurthermore, the overhead in memory was negligible in our proposed method tested on\nthe quantization of LLaMA-2-7B and LLaMA-2-13B models. Unlike the memory overhead,\nruntime was observed to increase in our proposed method due to additional gradient compu-\ntations required. Taken together, our findings indicate that focusing on salient weights helps\npreserve crucial representational capacity under severe bit-width constraints, strengthening\nthe potential of partial retraining strategies such as ApiQ.\n8 Conclusions\nIn this study, we conducted two lines of experiments focusing on design considerations for\nimproving an existing frontier quantization method in the ultra-low-bit regimes.\nAs discussed in Appendix F.3, through our first set of experiments, we were able to gain\ninsight into the underlying factors contributing to the effectiveness of QAT methods. First,\nby integrating BinaryMoS into ApiQ, we observed that under 1-bit quantization, retraining\nthe model weights offers a representational capacity that cannot be achieved by training\nLoRA adapters alone. This retraining — despite being memory and compute intensive —\nis critical for preserving model performance. Second, by integrating DB-LLM into ApiQ, we\nreaffirmed findings from EfficientQAT [3] that highlight the importance of using large and\ndiverse datasets to mitigate overfitting and maintain accuracy.\nIn our second line of investigation, we examined the trade-off between output preservation\nand weight preservation—two major training objectives used in both PTQ and QAT. Under\n2-bit quantization, we found that incorporating a naive weight preservation regularization\nterm had a negative effect on the accuracy of the quantized model. Informed by this finding,\nour proposed saliency-aware weight preservation term assigns greater importance to weights\nmost influential on the model’s output. Applying our proposed method on several LLMs, we\nobserved consistent improvement in zero-shot accuracies across tasks.\nAlthough we demonstrated the practicality of our proposed saliency-aware weight preser-\nvation regularization term in the specific context of ApiQ, our approach is generalizable to\nany quantization method that involves some form of training such as LLM-QAT [26] and\nOneBit [39].\nOne potential direction for future work is to explore different orders of applying the two\ntraining objectives (output preservation and saliency-aware weight preservation), and more\nflexible combinations of them. For example, one could first obtain a suitable initialization\nby training with the saliency-aware weight preservation objective, and then fine-tune the\nquantization parameters using output preservation as the training objective. When combined\nwith the two training paradigms (block-wise training and end-to-end training as outlined in\nEfficientQAT [3]), these objectives are expected to offer additional flexibility for exploration.\nFor instance, the saliency-aware weight preservation term can be applied exclusively during\n12 D. Cao and S. Aref\nblock-wise training, exclusively during end-to-end training, or during both stages. Further\nresearch is needed to determine which of these approaches will tighten the accuracy gap more\neffectively.\nAnother avenue for future work is to explore alternative definitions of saliency. In our\nstudy, saliency was defined as the degree to which each weight influences the model’s final\noutput. However, an alternative definition could consider the impact of each weight on the\noutput of its corresponding layer or block [25]. Future research can investigate if alternative\ndefinitions of saliency enable a more fine-grained control during optimization and become\nmore effective for a desired outcome.\nAcknowledgments\nThe authors thank the three anonymous reviewers of the MDAI’25 for their valuable com-\nments and Baohao Liao for helpful insights on the ApiQ method.\nReferences\n1. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al.: PIQA: Reasoning about physical commonsense in natural language.\nIn: Proceedings of the AAAI conference on artificial intelligence. vol. 34, pp. 7432–7439 (2020)\n2. Chen, H., Lv, C., Ding, L., Qin, H., Zhou, X., Ding, Y., Liu, X., Zhang, M., Guo, J., Liu, X., et al.: DB-LLM:\nAccurate dual-binarization for efficient LLMs. In: Findings of the Association for Computational Linguistics ACL\n2024. pp. 8719–8730 (2024)\n3. Chen, M., Shao, W., Xu, P., Wang, J., Gao, P., Zhang, K., Luo, P.: EfficientQAT: Efficient quantization-aware\ntraining for large language models. arXiv preprint arXiv:2407.11062 (2024)\n4. Clark, C., Lee, K., Chang, M.W., Kwiatkowski, T., Collins, M., Toutanova, K.: BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In: Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies. vol. 1, pp. 2924–2936 (2019)\n5. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think you have solved\nquestion answering? try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018)\n6. Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: QLoRA: Efficient finetuning of quantized LLMs.\nAdvances in neural information processing systems36, 10088–10115 (2023)\n7. Du, D., Zhang, Y., Cao, S., Guo, J., Cao, T., Chu, X., Xu, N.: BitDistiller: Unleashing the potential of sub-4-bit\nLLMs via self-distillation. In: Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). pp. 102–116 (2024)\n8. Frantar, E., Ashkboos, S., Hoefler, T., Alistarh, D.: GPTQ: Accurate post-training quantization for generative\npre-trained transformers. arXiv preprint arXiv:2210.17323 (2022)\n9. Gong, Z., Liu, J., Wang, J., Cai, X., Zhao, D., Yan, R.: What makes quantization for large language model\nhard? an empirical study from the lens of perturbation. In: Proceedings of the AAAI Conference on Artificial\nIntelligence. vol. 38, pp. 18082–18089 (2024)\n10. Guan, Z., Huang, H., Su, Y., Huang, H., Wong, N., Yu, H.: APTQ: Attention-aware post-training mixed-precision\nquantization for large language models. In: Proceedings of the 61st ACM/IEEE Design Automation Conference.\npp. 1–6 (2024)\n11. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank\nadaptation of large language models. In: International Conference on Learning Representations (2022)\n12. Huang, W., Liu, Y., Qin, H., Li, Y., Zhang, S., Liu, X., Magno, M., Qi, X.: BiLLM: Pushing the limit of\npost-training quantization for LLMs. arXiv preprint arXiv:2402.04291 (2024)\n13. Jeon, H., Kim, Y., Kim, J.j.: L4q: Parameter efficient quantization-aware fine-tuning on large language models.\narXiv preprint arXiv:2402.04902 (2024)\n14. Jin, R., Du, J., Huang, W., Liu, W., Luan, J., Wang, B., Xiong, D.: A comprehensive evaluation of quantization\nstrategies for large language models. In: Findings of the Association for Computational Linguistics ACL 2024.\npp. 12186–12215 (2024)\n15. Jo, D., Kim, T., Kim, Y., et al.: Mixture of scales: Memory-efficient token-adaptive binarization for large language\nmodels. Advances in Neural Information Processing Systems37, 137474–137494 (2024)\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 13\n16. Ke, W., Li, Z., Li, D., Tian, L., Barsoum, E.: DL-QAT: Weight-decomposed low-rank quantization-aware training\nfor large language models. In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing: Industry Track. pp. 113–119 (2024)\n17. Kim, M., Lee, S., Sung, W., Choi, J.: RA-LoRA: Rank-adaptive parameter-efficient fine-tuning for accurate 2-bit\nquantized large language models. In: Findings of the Association for Computational Linguistics ACL 2024. pp.\n15773–15786 (2024)\n18. Kim,S.,Hooper,C.,Gholami,A.,Dong,Z.,Li,X.,Shen,S.,Mahoney,M.W.,Keutzer,K.:Squeezellm:dense-and-\nsparse quantization. In: Proceedings of the 41st International Conference on Machine Learning. pp. 23901–23923\n(2024)\n19. Lang, J., Guo, Z., Huang, S.: A comprehensive study on quantization techniques for large language models.\nIn: 2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC). pp.\n224–231. IEEE (2024)\n20. Lee, C., Jin, J., Kim, T., Kim, H., Park, E.: OWQ: Outlier-aware weight quantization for efficient fine-tuning and\ninference of large language models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 38,\npp. 13355–13364 (2024)\n21. Li, L., Li, Q., Zhang, B., Chu, X.: Norm tweaking: High-performance low-bit quantization of large language\nmodels. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 38, pp. 18536–18544 (2024)\n22. Li, Y., Yu, Y., Liang, C., He, P., Karampatziakis, N., Chen, W., Zhao, T.: LoftQ: LoRA-fine-tuning-aware\nquantization for large language models. In: International Conference on Learning Representations (2024)\n23. Liao, B., Herold, C., Khadivi, S., Monz, C.: ApiQ: Finetuning of 2-bit quantized large language model. In:\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. pp. 20996–21020\n(2024)\n24. Liao, B., Tan, S., Monz, C.: Make pre-trained model reversible: From parameter to memory efficient fine-tuning.\nAdvances in Neural Information Processing Systems36, 15186–15209 (2023)\n25. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.M., Wang, W.C., Xiao, G., Dang, X., Gan, C., Han, S.: AWQ:\nActivation-aware weight quantization for on-device LLM compression and acceleration. Proceedings of Machine\nLearning and Systems6, 87–100 (2024)\n26. Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., Chandra, V.:\nLLM-QAT: Data-free quantization aware training for large language models. In: Findings of the Association for\nComputational Linguistics ACL 2024. pp. 467–484 (2024)\n27. Marcus, M.P., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated corpus of English: The Penn\nTreebank. Computational Linguistics19(2), 313–330 (1993),https://aclanthology.org/J93-2004/\n28. Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer sentinel mixture models. In: International Conference on\nLearning Representations (2017)\n29. Qin, H., Ma, X., Zheng, X., Li, X., Zhang, Y., Liu, S., Luo, J., Liu, X., Magno, M.: Accurate LoRA-finetuning\nquantization of LLMs via information retention. In: Proceedings of the 41st International Conference on Machine\nLearning. pp. 41498–41516 (2024)\n30. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the\nlimits of transfer learning with a unified text-to-text transformer. Journal of machine learning research21(140),\n1–67 (2020)\n31. Sakaguchi, K., Bras, R.L., Bhagavatula, C., Choi, Y.: WinoGrande: An adversarial WinoGrad schema challenge\nat scale. Communications of the ACM64(9), 99–106 (2021)\n32. Shang, Y., Yuan, Z., Wu, Q., Dong, Z.: PB-LLM: Partially binarized large language models. arXiv preprint\narXiv:2310.00034 (2023), the Twelfth International Conference on Learning Representations\n33. Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., Luo, P.: OmniQuant:\nOmnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137 (2024),\nthe Twelfth International Conference on Learning Representations\n34. Shen, A., Lai, Z., Li, D.: Exploring quantization techniques for large-scale language models: Methods, challenges\nandfuturedirections.In:Proceedingsofthe20249thInternationalConferenceonCyberSecurityandInformation\nEngineering. pp. 783–790 (2024)\n35. Tang, H., Sun, Y., Wu, D., Liu, K., Zhu, J., Kang, Z.: EasyQuant: An efficient data-free quantization algorithm\nfor LLMs. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp.\n9119–9128 (2023)\n36. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro,\nE., Azhar, F., et al.: LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971\n(2023)\n37. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava,\nP., Bhosale, S., et al.: LLaMA 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288\n(2023)\n14 D. Cao and S. Aref\n38. Wang, J., Liu, H., Feng, D., Ding, J., Ding, B.: FP4-quantization: Lossless 4bit quantization for large language\nmodels. In: 2024 IEEE International Conference on Joint Cloud Computing (JCC). pp. 61–67. IEEE (2024)\n39. Xu, Y., Han, X., Yang, Z., Wang, S., Zhu, Q., Liu, Z., Liu, W., Che, W.: OneBit: Towards extremely low-bit\nlarge language models. arXiv preprint arXiv:2402.11295 (2025), the Thirty-eighth Annual Conference on Neural\nInformation Processing Systems\n40. Yao, Z., Wu, X., Li, C., Youn, S., He, Y.: Exploring post-training quantization in llms from comprehensive\nstudy to low rank compensation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 38, pp.\n19377–19385 (2024)\n41. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., Choi, Y.: Hellaswag: Can a machine really finish your sentence?\nIn: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4791–4800\n(2019)\n42. Zhao, J., Zhang, M., Zeng, C., Wang, M., Liu, X., Nie, L.: LRQuant: Learnable and robust post-training quantiza-\ntion for large language models. In: Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). pp. 2240–2255 (2024)\n43. Zhu,X.,Li,J.,Liu,Y.,Ma,C.,Wang,W.:Asurveyonmodelcompressionforlargelanguagemodels.Transactions\nof the Association for Computational Linguistics12, 1556–1577 (2024)\nAppendices\nA Experimental details\nIn Table 3, we present the complete hyperparameter configurations used throughout our\nexperiments unless otherwise noted.\nTable 3: Hyperparameter configuration\nHyperparameter Value\nGranularity and Bit-width\nOptimization granularity block-wise\nBit-width 2\nLoRA Settings\nLoRA rank 64\nOptimizer and Training Settings\nOptimizer AdamW\nWeight decay for quantization parameters 0.1\nLearning rate for quantization parameters 0.005\nWeight decay for LoRA weights 0.1\nLearning rate for LoRA weights 0.0005\nBatch size 1\nEpochs (per layer) 20\nCalibration Dataset Settings\nSequence length for calibration 2048\nNumber of calibration samples 128\nB Computing resources\nThe majority of experiments were conducted on a single NVIDIA A100 GPU with 40GB of\nmemory.However,forcomputingthesquaredgradientsrequiredbytheLLaMA-2-13Bmodel,\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 15\nTable 4: The duration and peak GPU memory used for quantizing LLaMA-2 models\n(a) 7B\nMethod Duration Peak GPU\nmemory\nApiQ (Original) 1.5h 16GB\nApiQ With Saliency-Aware Weight\nPreservation Regularization (After LoRA) 2.0h 16GB\nApiQ With Saliency-Aware Weight\nPreservation Regularization (Before LoRA) 1.8h 16GB\nApiQ + DB-LLM 1.6h 16GB\nApiQ + DB-LLM With Saliency-Aware\nWeight Preservation Regularization (After\nLoRA)\n2.2h 16GB\n(b) 13B\nMethod Duration Peak GPU\nmemory\nApiQ (Original) 2.7h 29GB\nApiQ With Saliency-Aware Weight\nPreservation Regularization (After LoRA) 3.8h 29GB\nwe employed a single NVIDIA H100 GPU with 80GB of memory due to the substantial\nmemory demands of this computation.\nThe memory usage and training time of some of the experiments are provided in Table 4.\nThe proposed regularization methods introduce negligible GPU memory overhead; however,\nthe training time increases by approximately 40% due to the computation of the regulariza-\ntion terms. This could be reduced through further optimization. It is important to note that\nquantization is usually performed only once, whereas inference is carried out repeatedly in\ndeployment. Consequently, the modest extra time required for quantization is quickly amor-\ntized over a limited number of inference runs, and the resulting boost in inference accuracy\ngenerally far outweighs this one-off overhead.\nOurworkmodifiesonlyApiQ’strainingprocedureandleavesinference unchanged.There-\nfore, we omit inference time and memory measurements. One may refer to the original ApiQ\npaper [23] for their details on inference time and memory measurements.\nC Overview of round-to-nearest (RTN) quantization\nHere, we illustrate the concept of quantization using the simplest approach—Round-To-\nNearest (RTN) quantization. Equation 3 describes the first part of quantization, which maps\neach original weight to an integer value between 0 and2N−1, where N is the bit width of the\nquantized value.\nWint = clamp\n\u0012\u0016W\ns\n\u0019\n+ z, 0, 2N − 1\n\u0013\n(3)\n16 D. Cao and S. Aref\nHere, ⌊·⌉ represents the rounding operation.Wint and W denote the quantized integer and\nfull-precision weight matrices, respectively.s is the scaling factor andz is the zero point.\nIn the simplest case, these are computed using Equations 4 and 5, whereWmax and Wmin\nrepresent the maximum and minimum values in the weight matrix, respectively.\ns = Wmax − Wmin\n2N − 1 (4)\nz = −round\n\u0012Wmin\ns\n\u0013\n(5)\nIn quantization, the scaling factor and zero point are typically shared among a group of\nweights—such as an entire weight matrix, a single channel within a matrix, or a subset of a\nchannel. This grouping determines the granularity of quantization.\nDuring forward propagation, the quantized weights are converted back to full-precision\nvalues by applying the corresponding scaling factors and zero points—a process known as\ndequantization, as shown in Equation 6.\nˆW = (Wint − z) · s (6)\n-0.8 -0.2 0.1\n0.5 0.7 1.0\n1.5 2.0 3.0\n \n0 1 1\n1 2 2\n2 3 3 \n-1.27 0.0 0.0\n0.0 1.27 1.27\n1.27 2.53 2.53\nFig.2: Numerical example of 2-bit quantization. The color intensity in the rightmost matrix\ncorresponds to the deviation of the quantized weight from the original weight.\nIn Eq. 6, ˆW refers to the reconstructed weights used in the forward computation. Since\nour focus is on the reconstructed weights and how their deviation from the original weights\naffects model accuracy, we refer to the entire process of obtaining reconstructed weightsˆW\nfrom the original weightsW as quantization. The whole process of quantization is illustrated\nin Fig. 2 using a numerical example.\nD A review of other related works\nD.1 Recent developments in PTQ and QAT\nThis section provides a review of recent developments in the area of PTQ [18,25] followed\nby notable works from the QAT category [26,7,39].\nSqueezeLLM [18] achieves near-lossless compression down to 3 bits by weighting parame-\nters according to their impact on model output (sensitivity-weighted non-uniform quantiza-\ntion) and isolating outlier weights in a sparse matrix (dense and sparse decomposition). This\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 17\ncombination preserves the representation of highly sensitive and outlier weights, effectively\nmaintaining accuracy under aggressive quantization.\nAWQ [25] builds on the observation that retaining outlier weights in full precision can\nsignificantly improve performance. However, to avoid the hardware inefficiencies associated\nwith mixed-precision formats, AWQ first identifies the output channels in each weight matrix\nwith the largest activation magnitudes—referred to as salient channels—and then amplifies\nthem. This approach leverages on quantization loss reduction through scaling up specific\nweights which was proven effective in comparisons provided against benchmarks [25].\nLLM-QAT [26] is argued to be the first to apply a QAT framework to LLMs. Given\nthe challenge of obtaining suitable fine-tuning datasets for retraining, LLM-QAT leverages\ngenerated data from the full-precision LLM itself and trains the quantized LLM to align with\nthe output distribution of the full-precision model. This process is referred to as a knowledge\ndistillation framework.\nBitDistiller [7] adopts the same knowledge distillation framework as LLM-QAT [26] but\nenhances the naive symmetric quantization approach by using asymmetric quantization and\ncustomized clipping technique to preserve the fidelity of quantized weights. Specifically, be-\nfore the main training, they use a small calibration set to optimize the asymmetric clipping\nthresholds for weights to minimize the output discrepancy caused by quantization. Addition-\nally, it proposes a new self-distillation loss function5, which takes into account the confidence\nof the full-precision model about each prediction. With these advancements, BitDistiller was\nshown to achieve substantial performance gains over existing methods in 3-bit and 2-bit\nquantization [7].\nOneBit [39] was the first to explore a QAT-based approach for 1-bit quantization, also\nknownasbinarization.MotivatedbytheobservationthatthenaiveRound-To-Nearest(RTN)\nquantization method used in prior QAT works, such as LLM-QAT [26], leads to substantial\nperformance degradation at the 1-bit level, this approach introduced scaling factors for both\ninput and output dimensions to mitigate the precision loss caused by binarization. These\nscaling factors, along with the binarized weight matrix, are initialized using a singular value\ndecomposition and are optimized through knowledge distillation in the same manner as\nLLM-QAT [26].\nWhile these QAT methods effectively mitigate the accuracy degradation caused by quan-\ntization, they are computationally expensive, as they require updating all the weights in\nLLMs. This challenge encouraged researchers to develop new methods aimed at preserv-\ning the beneficial adjustments from retraining while reducing the time, memory, and data\nrequirements of the retraining process. Some of the methods which aim to achieve such a\ntradeoff are reviewed in Appendix D.2.\nD.2 Improving the efficiency of QAT methods\nA natural approach to bridging the gap between QAT and PTQ is to update only a small\nset of auxiliary parameters instead of retraining all the original weights. This strategy aims\nto retain the high accuracy of QAT while benefiting from the speed, memory efficiency, and\nreducedoverfittingrisksofPTQwhenworkingwithlimiteddata.Apromisingchoiceforthese\nauxiliary parameters is the set of quantization parameters such as scaling factors, clipping\n5 The loss function is named Confidence-Aware Kullback-Leibler Divergence [7].\n18 D. Cao and S. Aref\nthresholds, and, zero points (if applicable). These quantization parameters (explained in\nAppendix C) are typically predetermined using heuristics in most PTQ methods.\nOmniQuant [33] freezes the original full-precision weights and only trains a few learnable\nquantization parameters which determine the clipping thresholds. Additionally, it quantizes\nthe activations and the key-value cache by jointly learning a transformation that shifts the\nquantization difficulty from activations to weights. As a result, this method was able to\nshorten the training time from 90 hours in LLM-QAT [26] to only 1.6 hours for LLaMA-7B\non a single NVIDIA A100 GPU. Also, it only used 128 segments for retraining but still\noutperformed existing methods by a large margin.\nEfficientQAT [3] aims to combine the efficiency of partial training with the accuracy\nbenefits of full retraining. To achieve this, it introduces a two-stage framework: (1) block-\nwise training of all parameters and (2) end-to-end fine-tuning of quantization parameters.\nIn the first stage, both model and quantization parameters are trained block-by-block using\nreconstruction loss, enabling accurate calibration with reduced memory usage. In the second\nstage, the quantized weights are fixed, and only the step sizes are fine-tuned on target\ndatasets to improve inter-block interaction. This two-stage design makes EfficientQAT a\nmemory-efficient QAT method, as demonstrated by benchmark comparisons [3].\nE Comparison and replication of existing methods\nE.1 Comparison of existing methods\nIn this section, we compare the performance of quantized models using the methods reviewed\ninSection1.Ourfocusissolelyonhowwelleachmethodpreservestheaccuracyoftheoriginal\nfull-precision model. Comparisons involving memory usage and inference speed are beyond\nthe scope of this study.\nAs noted in Section 2.1, in this study, we specifically focus on their performance under\n4-bit and lower precisions. All results in Tables 5 to Table 8 are gathered from the original\npapers and presented in a comparable manner.\nIn Tables 5 – 8, g128 indicates that the group size is 128, while W4A4 signifies that\nboth weights and activation values are quantized to 4-bit precision. The results are sorted\naccording to the performance on the 7B model evaluated on Wiki-Text2 dataset. The results\nfor the models listed in Table 5–8 are taken directly from the original paper (where each\nmodel is discussed), and therefore the models used and the evaluation metrics vary across\nstudies. As a result, Tables 5 to 8 include different sets of methods, depending on what\nperformance measures were reported in each paper.\nIn Table 5, we present a comparison of the performance of several methods discussed in\nSection 1, evaluated on two models from LLaMA-1 [36]. The evaluation is based on perplexity\nmeasured on two commonly used datasets: WikiText-2 [28] and C4 [30]. Perplexity is a\ncommonly used metric to evaluate the performance of a language model. It is defined as\nthe exponentiated average negative log-likelihood of a given token sequence. Formally, for a\nsequence of tokensX = (x1, x2, . . . , xN ), the perplexity is defined as:\nPPL(X) = exp\n \n− 1\nN\nNX\ni=1\nlog P(xi | x1, x2, . . . , xi−1)\n!\n(7)\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 19\nHere, P(xi | x1, x2, . . . , xi−1) denotes the probability assigned by the language model to\nthe tokenxi, given its preceding context. Lower perplexity indicates that the model predicts\nthe sequence more accurately.\nIn Table 6, we use the same two Llama-1 models to present a comparison of some of the\nultra-low-bit quantization methods based on various common sense reasoning tasks such as\nBoolQ [4], WinoGrande [31], PIQA [1], HellaSwag [41], Arc-Easy [5], and Arc-Challenge [5].\nThese datasets consist of multiple-choice questions from various domains. For each dataset,\nzero-shot accuracy, which is the accuracy without task-specific fine-tuning is reported.\nThe perplexity comparison for two models from the LLaMA-2 family [37] are provided\nin Table 7. Table 7 shows that the accuracy drop for 3-bit quantized models has become less\npronounced. However, for 2-bit and 1-bit quantization, even QAT-based methods continue\nto exhibit a relatively large performance gap compared to the original model. Note that large\nperformance degradation is observed for models such as DB-LLM [2], BinaryMoS [15], and\nOneBit [39], that are specifically designed for low-bit quantization.\nThe accuracy comparison for LLaMA-2-7B and LLaMA-2-13B models is provided in\nTable 8. It indicates that there remains an accuracy gap of approximately 5% to 10% to be\npotentially bridged by better models.\n20 D. Cao and S. Aref\nTable 5: Perplexity comparison of different quantization methods evaluated on LLaMA-1-7B\nand LLaMA-1-13B\nMethod Bit Additional Info LLaMA-1-7B LLaMA-1-13B\nWikiText2 C4 WikiText2 C4\nOriginal 16 5.68 7.08 5.09 6.61\nSqueezeLLM 3 0.45% outlier retention 6.13 7.56 5.45 6.92\nSqueezeLLM 3 no outlier retention 6.32 7.75 5.60 7.08\nAWQ 3 g128 6.46 7.92 5.51 7.07\nGPTQ 3 g128 6.55 7.85 5.62 7.10\nRTN 3 g128 7.01 8.62 5.88 7.49\nDB-LLM 2 g64 7.59 9.74 6.35 8.42\nBinaryMoS 1 7.97 9.72 7.16 8.81\nGPTQ 3 c 8.06 9.49 6.76 8.16\nOmniQuant 2 g64 8.90 11.78 7.34 9.75\nOmniQuant 2 g128 9.72 12.97 7.93 10.36\nOneBit 1 10.38 11.56 9.18 10.25\nOmniQuant W4A4 11.26 14.51 10.87 13.78\nAWQ 3 c 11.88 13.26 7.45 9.13\nOmniQuant 2 c 15.47 24.89 13.21 18.31\nGPTQ 2 g64 22.10 17.71 10.06 11.70\nSmoothQuant W4A4 25.25 32.32 40.05 47.18\nRTN 3 c 25.73 28.26 11.39 13.22\nBiLLM 1 41.66 48.15 14.56 16.67\nGPTQ 2 g128 44.01 27.71 15.60 15.29\nRTN 2 g64 188.32 151.43 101.87 76.00\nPB-LLM 1 10% salient weights retained 198.37 157.35 35.83 39.79\nRTN 2 g128 1.9e+03 1.0e+03 781.20 447.64\nGPTQ 2 c 2.1e+03 689.13 5.5e+03 2.5e+03\nRTN 2 c 1.1e+05 1.3e+05 6.8e+04 5.6e+04\nAWQ 2 g64 2.5e+05 2.8e+05 2.7e+05 2.2e+05\nAWQ 2 g128 2.6e+05 1.9e+05 2.8e+05 2.3e+05\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 21\nTable 6: Zero-shot accuracy comparison of different quantization methods evaluated on\nLLaMA-1-7B and LLaMA-1-13B\n(a) LLaMA-1-7B\nMethod Bit Additional info WinoG Hellaswag PIQA BoolQ ARC-e ARC-c\nOriginal 16 66.85 72.99 77.37 73.21 52.53 41.38\nDB-LLM 2 g64 61.01 60.71 72.14 44.70 33.62\nOneBit 1 60.30 50.73 67.46 62.51 41.71 29.61\nBinaryMoS 1 58.88 58.18 71.82 64.59 42.09 31.31\nBiLLM 1 51.14 34.64 58.65 62.23 33.08 25.68\nPB-LLM 1 10% salient weights retained 49.17 27.23 53.53 60.51 27.48 26.02\n(b) LLaMA-1-13B\nMethod Bit Additional info WinoG Hellaswag PIQA BoolQ ARC-e ARC-c\nOriginal 16 70.17 76.24 79.05 68.47 59.85 44.54\nDB-LLM 2 g64 64.72 68.29 74.16 51.18 37.54\nOneBit 1 62.90 56.78 70.67 64.16 44.53 32.10\nBinaryMoS 1 60.93 64.05 73.88 63.82 44.28 33.11\nBiLLM 1 59.43 52.24 68.17 62.53 41.91 29.94\nPB-LLM 1 10% salient weights retained 52.17 33.97 58.70 62.17 31.86 23.63\n22 D. Cao and S. Aref\nTable 7: Perplexity comparison of different quantization methods evaluated on LLaMA-2-7B\nand LLaMA-2-13B\nMethod Bit Additional Info LLaMA-2-7B LLaMA-2-13B\nWikiText2 C4 WikiText2 C4\nOriginal 16 5.47 6.97 4.88 6.46\nApiQ 3 block-wise 5.77 7.48 5.12 6.83\nEfficientQAT 3 g128 5.81 7.34 5.12 6.73\nSqueezeLLM 3 0.45% outlier retention 5.96 7.51 5.23 6.82\nSqueezeLLM 3 no outlier retention 6.18 7.72 5.36 6.97\nAWQ 3 g128 6.24 7.84 5.32 6.94\nGPTQ 3 g128 6.29 7.89 5.42 7.00\nRTN 3 g128 6.66 8.40 5.51 7.18\nEfficientQAT 2 g64 6.86 8.50 5.96 7.59\nEfficientQAT 2 g128 7.19 8.79 6.08 7.75\nDB-LLM 2 g64 7.23 9.62 6.19 8.38\nApiQ 2 block-wise 7.59 10.56 6.44 8.93\nBinaryMoS 1 7.88 9.75 7.08 8.91\nGPTQ 3 c 8.37 9.81 6.44 8.02\nOmniQuant 2 g64 9.62 12.72 7.56 10.05\nOneBit 1 9.73 11.11 8.76 10.15\nLoftQ 3 10.72 12.79 6.89 8.72\nOmniQuant 2 g128 11.06 15.02 8.26 11.05\nOmniQuant W4A4 14.26 18.02 12.30 14.55\nGPTQ 2 g64 20.85 19.40 22.44 12.48\nAWQ 3 c 24.00 23.85 10.45 13.07\nBiLLM 1 27.72 36.34 20.71 27.19\nGPTQ 2 g128 36.77 33.70 28.14 20.97\nOmniQuant 2 c 37.37 90.64 17.21 26.76\nPB-LLM 1 10% salient weights retained 76.75 85.92 155.25 151.15\nSmoothQuant W4A4 83.12 77.27 35.88 43.19\nRTN 2 g64 431.97 475.35 26.22 28.69\nRTN 3 c 539.48 402.35 10.68 12.51\nLoftQ 2 1.0e+03 670.00 59.94 72.64\nRTN 2 g128 4.2e+03 4.9e+03 122.08 139.65\nGPTQ 2 c 7.7e+03 2.1e+03 323.12\nRTN 2 c 3.8e+04 4.8e+04 5.6e+04 7.2e+04\nAWQ 2 g64 2.1e+05 1.6e+05 1.2e+05 9.5e+04\nAWQ 2 g128 2.2e+05 1.7e+05 1.2e+05 9.4e+04\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 23\nTable 8: Zero-shot accuracy comparison of different quantization methods evaluated on\nLLaMA-2-7B and LLaMA-2-13B\n(a) LLaMA-2-7B\nMethod Bit Additional info WinoG Hellaswag PIQA BoolQ ARC-e ARC-c\nOriginal 16 67.09 72.94 76.88 71.10 53.58 40.61\nDB-LLM 2 g64 61.72 61.98 73.18 45.20 33.53\nOneBit 1 58.41 52.58 68.12 63.06 41.58 29.61\nBinaryMoS 1 56.18 59.41 71.55 65.02 41.84 30.03\nBiLLM 1 53.11 35.18 59.19 62.14 34.22 26.54\nPB-LLM 1 10% salient weights retained 50.11 26.87 52.82 62.17 26.89 24.31\n(b) LLaMA-2-13B\nMethod Bit Additional info WinoG Hellaswag PIQA BoolQ ARC-e ARC-c\nOriginal 16 69.77 76.62 79.05 68.99 57.95 44.20\nDB-LLM 2 g64 64.09 68.04 75.14 51.64 38.14\nOneBit 1 61.72 56.43 70.13 65.20 43.10 33.62\nBinaryMoS 1 58.98 63.80 73.12 66.12 45.71 33.19\nBiLLM 1 56.35 38.05 62.51 62.20 4.69 27.73\nPB-LLM 1 10% salient weights retained 49.48 28.89 53.26 37.82 28.28 23.72\n24 D. Cao and S. Aref\nE.2 Replication study of ApiQ\nWe replicated the results from ApiQ [23] so that we have a baseline for measuring the\neffects of our further modifications. We used the same hyperparameter sets as described in\ntheir paper and outlined in Table 3 and ran quantization on LLaMA-2-7B model. The main\nperformance results are presented in Fig. 3. Although minor differences were observed, the\nresults we obtained were largely consistent with those reported in the original paper [23].\nApiQ Paper Our Result Full-Precision Model0\n2\n4\n6\n8\n10\n12Perplexity on WikiT ext-2\n7.59 7.56\n5.47\n(a) WikiText-2\nApiQ Paper Our Result Full-Precision Model0\n2\n4\n6\n8\n10\n12Perplexity on C4\n10.56 10.42\n6.97 (b) C4\nFig.3: Perplexity comparison between our reproduced results and the original results re-\nported for ApiQ (for 2-bit quantization of LLaMA-2-7B model). Performance of the full-\nprecision model is also added for reference.\nF Evaluation of preliminary ideas\nF.1 Performance of ApiQ + BinaryMoS\nIn this experiment, we evaluated the approach described in Section 3.1. The hyperparameters\nused were identical to those listed in Table 3, except for the learning rate of the quantization\nparameters, which was adjusted to 5e-4 as it yielded better results. We tested different num-\nbers of scaling factors (i.e. experts) and the results are shown in Fig. 4. Despite extensive\nhyperparameter tuning, our approach did not yield results superior to those presented in the\nthird row of Fig. 4, which are substantially worse than the results reported in the original\nBinaryMoS paper. The key distinction between our configuration and their original setup lies\nin the treatment of trainable parameters. While the original paper trained both weights and\nquantization parameters jointly, our approach froze the weights, introduced LoRA adapters,\nand trained only the quantization parameters and LoRA weights. This observation highlights\na fundamental limitation in the capacity of LoRA weights to mitigate the accuracy degrada-\ntion caused by quantization, particularly under the stringent condition of 1-bit quantization.\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 25\n5 10 15 20 25 30\nNumber of Experts\n0\n20\n40\n60\n80\n100\n120Perplexity (PPL)\n37.6734.50\n30.13\n36.21\n106.45\n92.46\n77.42\n99.06\nWikiT ext-2 (PPL)\nC4 (PPL)\nFig.4: Performance of ApiQ+BinaryMoS on LLaMA-2-7B model with different number of\nexperts\nF.2 Performance of ApiQ + DB-LLM\nIn this experiment, we evaluated the approach described in Section 3.2. We initially evaluated\nourproposedmethodusingthesamesetofhyperparametersasApiQ(asspecifiedinTable3),\nexceptfor thelearningrate ofthequantizationparameters. Forthis,weadopteda value of1e-\n4, as our hyperparameter tuning experiments indicated that it leads to more stable training.\nSince this naive integration led to substantial overfitting and degraded performance, we\nalso tested the saliency-aware weight preservation regularization proposed in Section 4. We\nexclusively evaluated the after-LoRA variant, as it demonstrated superior performance when\ntested using the original ApiQ method. The results are presented in Tables 9 – 10. While\nthe inclusion of the regularization term substantially improved performance, our method did\nnot surpass the original ApiQ approach when we used the same amount of training data as\nthe original ApiQ. However, as the third line of Table 10 indicates, using more training data\ncan improve the performance decently, surpassing that of the original ApiQ method.\nWe observed that directly using the quantization formulation of DB-LLM, which was\noriginally proposed as a QAT method, can reduce quantization loss. However, without large\nand diverse training data, addressing the issue of overfitting remains challenging.\nTable 9: Performance of ApiQ + DB-LLM on LLaMA-2-7B model\nMethod PPL ↓ Accuracy (%) ↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model 5.47 6.97 69.22 57.16 43.52 76.26 78.07 64.85\nOriginal ApiQ 7.56 10.42 62.83 46.53 31.57 66.54 72.09 55.91\nApiQ + DB-LLM 8.26 12.95 60.22 44.64 20.90 37.37 59.98 44.54\n26 D. Cao and S. Aref\nTable 10: Performance of ApiQ + DB-LLM with saliency-aware weight preservation regular-\nization on LLaMA-2-7B (After-LoRA only). Dataset: training set; Coefficient: regularization\nweight; Samples: number of training examples; Epochs: training epochs per layer.\nDataset Coefficient Samples Epochs PPL↓ Accuracy (%)↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nWikiText-2 10 128 20 7.52 10.38 62.35 46.26 31.40 66.37 71.55 55.58\nC4 10 128 20 8.39 10.75 62.83 46.85 31.31 66.25 71.44 55.73\nC4 10 512 10 7.46 10.43 62.35 46.72 33.45 68.14 72.09 56.55\nF.3 Lessons learned from combining QAT methods with ApiQ\nWe investigated the idea of replacing ApiQ’s quantized linear layers with those from Binary-\nMoS and DB-LLM. Each approach originally achieves strong performance when full model\nweights are retrained on ample data, but we hypothesized that LoRA-based partial finetun-\ning might be sufficient to close the gap between a compressed model and its full-precision\ncounterpart. However, the results indicated otherwise: freezing the original weights while\ntraining only LoRA adapters and quantization parameters did not yield accuracy improve-\nments beyond baseline ApiQ. For instance, BinaryMoS’s multi-expert binarization, effective\nunder standard QAT, still struggled when only LoRA weights were permitted to be up-\ndated. On the other hand, DB-LLM’s flexible dual binarization helped reduce quantization\nloss but showed substantial overfitting to the limited calibration data, preventing it from\noutperforming ApiQ’s simpler design.\nOur result of integrating BinaryMoS into ApiQ suggests that the effectiveness of QAT-\nbased strategies in the 1-bit regime is highly dependent on the ability to retrain a substantial\nportion of the model’s weights. While ApiQ demonstrated that LoRA adapters can effectively\ncompensate for quantization errors in the 2-bit setting, this compensatory effect does not\nextend to 1-bit scenarios, where the representational capacity of quantized weights is more\nseverely constrained. Notably, our results with BinaryMoS underperformed not only Bina-\nryMoS’s original implementation but also its predecessor, OneBit [39], which uses a simpler\nand thus less expressive quantization scheme with only a single pair of scaling factors. This\ncomparison underscores a key insight: increasing the expressiveness of quantization alone is\ninsufficient when the main weights remain frozen. Instead, retraining the original weights\nremains essential for recovering performance under 1-bit quantization.\nIn contrast, our results of integrating DB-LLM into ApiQ suggest a different implication.\nIts quantization formulation is sufficiently expressive—evidenced by its tendency to overfit\nthelimitedcalibrationdata—indicatingthattheavailabilityofalargeanddiversetrainingset\nis equally critical. This observation aligns with findings from EfficientQAT [3], whose ablation\nstudies showed that when both weights and quantization parameters (scaling factors and zero\npoints) are trained in a block-wise manner, at least 4096 samples are required to mitigate\noverfitting. Although our method freezes the original weights and trains fewer parameters\nby introducing LoRA adapters, our results similarly indicate that 128 samples (the number\nwe used) are not enough to prevent overfitting. The third row of Table 10 demonstrates that\nincreasing the volume of calibration data consistently improves performance beyond the\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 27\nApiQ baseline. This highlights both the risk of overfitting and the imperative of employing\nlarger datasets when using more expressive quantization schemes.\nG Additional results on the naive regularizer\nWe evaluated the effectiveness of the naive weight preservation regularizer introduced in\nSection 4.1.\nTable 11: Performance of ApiQ with weight preservation regularization on LLaMA-2-7B\nmodel\nLoRA Variant| Coefficient PPL↓ Accuracy (%)↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model 5.47 6.97 69.22 57.16 43.52 76.26 78.07 64.85\nOriginal ApiQ (No regularization) 7.56 10.42 62.83 46.53 31.57 66.54 72.09 55.91\nAfterLoRA 1e-3 8.77 11.5 57.62 45.97 27.82 59.09 68.93 51.88\nAfterLoRA 1e-4 8.12 10.83 60.83 46.68 30.72 64.27 72.09 54.92\nAfterLoRA 1e-5 7.83 10.84 60.93 45.84 31.74 66.25 71.38 55.21\nAfterLoRA 1e-6 8.82 21.91 60.06 43.15 30.12 64.48 70.73 53.71\nBeforeLoRA 1e-3 8.21 12.16 61.64 45.03 30.46 62.29 70.35 53.95\nBeforeLoRA 1e-4 8.12 12.77 61.64 41.19 32.59 66.41 70.78 54.52\nBeforeLoRA 1e-5 7.85 16.77 61.40 43.91 31.91 66.79 71.06 55.01\nBeforeLoRA 1e-6 7.69 12.72 61.40 45.22 31.06 66.20 72.03 55.18\nWe tested using different coefficients for the regularization term and also evaluated the\ntwo variants (before / after LoRA) mentioned in Section 4.1. The results are shown on Table\n11. None of the results presented in this table surpassed the performance of the original ApiQ\nmethod, suggesting that the naive weight preservation approach is ineffective for regularizing\nthe loss function.\nH Ablation studies\nH.1 Applying proposed regularization term under 3-bit quantization\nTo validate the robustness of the saliency-aware weight preservation term introduced in\nSection 4.2, we extended its use to 3-bit quantization. The training settings used here follow\nthe settings reported in ApiQ [23], which are provided in Table 12. We used the AfterLoRA\nvariant and used the gradients calculated from WikiText-2 dataset to calculate the saliency\nvalues. Through hyperparameter tuning, we found that regularization stronger than the 2-bit\ncase improved performance at 3 bits. We also observed that a coefficient of 10 yielded the\nbest results, as indicated in Table 13 in comparison with the baselines. With the proposed\nregularization, we were able to achieve an increase of 14 basis points in the average accuracy\n(the differece between second and the third rows of the rightmost column in Table 13). This\nincrease accounts for 7.91 % relative improvement in the accuracy degradation incurred by\nthe ApiQ method.\n28 D. Cao and S. Aref\nTable 12: Training settings for 3-bit quantization of LLaMA-2-7B model. Other hyperpa-\nrameters are the same as Table 3.\nHyperparameter Value\nOptimizer and Training Settings\nOptimizer AdamW\nWeight decay for quantization parameters 0.1\nLearning rate for quantization parameters 0.001\nWeight decay for LoRA weights 0.0\nLearning rate for LoRA weights 0.0005\nBatch size 1\nEpochs (per layer) 20\nTable 13: Performance of ApiQ with saliency-aware weight preservation regularization under\n3-bit quantization evaluated on LlaMA-2-7B model\nMethod PPL↓ Accuracy (%)↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model 5.47 6.97 69.22 57.16 43.52 76.26 78.07 64.85\nOriginal ApiQ 5.98 7.45 68.22 55.09 40.44 73.65 77.37 63.08\nApiQ with Proposed Regularization 5.78 7.46 68.19 55.36 41.21 73.99 77.37 63.22\nH.2 Investigating the effects of different calibration datasets\nTo evaluate the sensitivity of our proposed method to the calibration dataset and its size,\nwe carried out the same experiment with different calibration datasets. We conducted exper-\niments on the Penn Treebank (PTB) dataset [27] as well as on a composite corpus formed\nby equally mixing WikiText-2 [28], C4 [30], and PTB. To ensure comparability with the\nresults in Section 6.1, we sampled 128 examples from each dataset, applied a regularization\ncoefficient of 2, and evaluated the AfterLoRA variant. All other hyperparameters remain as\nlisted in Table 3. The results for PTB dataset and the mixed dataset are provided in Table\n14 and Table 15 respectively.\nWhen calibrated on PTB, our method achieved a gain of 115 basis points in average ac-\ncuracy (corresponding to the difference between the second and third rows of the rightmost\ncolumn in Table 14). This gain accounts for a relative improvement of 11.83 % in the accu-\nracy degradation incurred by ApiQ. Using the mixed dataset, the proposed regularization\ndelivered an accuracy improvement of 203 basis points (the difference between the second\nand third rows of the rightmost column in Table 15). This accounts for 17.29 % relative im-\nprovement in accuracy degradation incurred by ApiQ. These results indicate that, although\nthe baseline ApiQ’s performance depends on the calibration data, our approach consistently\nenhances accuracy irrespective of the dataset chosen. Furthermore, we observe that the im-\nprovement is particularly pronounced on datasets where the accuracy of the original ApiQ\nmethod is low, yielding greater relative gains in such challenging cases.\nH.3 Investigating the effect of calibration dataset size\nTo investigate the effect of calibration dataset size on the accuracy of the quantized model,\nwe used WikiText-2 as the calibration dataset. We increased the number of training samples\nSaliency-Aware Partial Retraining for Ultra-Low-Bit Quantization 29\nTable 14: Performance of ApiQ with Saliency-Aware Weight Preservation Regularization on\nthe LLaMA-2-7B Model Calibrated Using the PTB Dataset\nMethod PPL↓ Accuracy (%)↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model 5.47 6.97 69.22 57.16 43.52 76.26 78.07 64.85\nOriginal ApiQ 9.22 NaN 62.19 46.46 31.06 64.31 71.60 55.13\nApiQ with Proposed Regularization 8.40 10.47 63.61 47.20 31.57 66.67 72.36 56.28\nTable 15: Performance of ApiQ with saliency-aware weight preservation regularization on\nthe LLaMA-2-7B model calibrated using the mixed dataset\nMethod PPL↓ Accuracy (%)↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model 5.47 6.97 69.22 57.16 43.52 76.26 78.07 64.85\nOriginal ApiQ NaN NaN 60.62 44.40 29.86 59.89 70.78 53.11\nApiQ with Proposed Regularization 8.41 11.01 63.93 45.61 30.12 64.39 71.65 55.14\nfrom 128 to 512 and trained each layer for five epochs, which proved more effective with the\nlarger dataset. We set the regularization coefficient to 2, which yielded the best performance\nas shown in Table 2. All other hyperparameters remained the same as those listed in Table\n3.\nTable 16: Performance comparison of our proposed method when trained with different\ndataset sizes. Samples means the number of training samples used.\nMethod Samples PPL↓ Accuracy (%)↑\nWikiText2 C4 WinoGrande HellaSwag ArcC ArcE PiQA Average\nFull-Precision Model - 5.47 6.97 69.22 57.16 43.52 76.26 78.07 64.85\nOriginal ApiQ 128 7.56 10.42 62.83 46.53 31.57 66.54 72.09 55.91\nApiQ with Proposed Regularization128 7.57 10.26 64.80 47.11 32.34 67.21 72.96 56.88\nOriginal ApiQ 512 7.58 10.41 61.48 46.60 32.85 67.05 72.09 56.01\nApiQ with Proposed Regularization512 7.55 10.30 64.25 46.89 33.28 67.72 73.01 57.03\nThe results are provided in Table 16. It can be seen that our method shows consistent\nimprovement when the dataset size is increased and thus the performance of the baseline\nApiQ is improved as well.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8387752175331116
    },
    {
      "name": "Retraining",
      "score": 0.8070931434631348
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.7404425740242004
    },
    {
      "name": "Bit (key)",
      "score": 0.6247966885566711
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42936161160469055
    },
    {
      "name": "Speech recognition",
      "score": 0.3393637239933014
    },
    {
      "name": "Computer vision",
      "score": 0.27953147888183594
    },
    {
      "name": "Computer network",
      "score": 0.11027359962463379
    },
    {
      "name": "International trade",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}