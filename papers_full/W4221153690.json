{
    "title": "LinkBERT: Pretraining Language Models with Document Links",
    "url": "https://openalex.org/W4221153690",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2656961764",
            "name": "Michihiro Yasunaga",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A1878631932",
            "name": "Jure Leskovec",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2171686691",
            "name": "Percy Liang",
            "affiliations": [
                "Stanford University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2557764419",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W2963339397",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2180377648",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2964179635",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2972111982",
        "https://openalex.org/W2787905871",
        "https://openalex.org/W1983416950",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W2735784619",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W3030932098",
        "https://openalex.org/W2154142897",
        "https://openalex.org/W3119619013",
        "https://openalex.org/W2174775663",
        "https://openalex.org/W3005296017",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2949894546",
        "https://openalex.org/W2160992478",
        "https://openalex.org/W4385572953",
        "https://openalex.org/W2964144561",
        "https://openalex.org/W3172335055",
        "https://openalex.org/W174342562",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W131533222",
        "https://openalex.org/W4313908941",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2991612931",
        "https://openalex.org/W2988421999",
        "https://openalex.org/W3174370755",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3162922479",
        "https://openalex.org/W3100452049",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3182696977",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3099635335",
        "https://openalex.org/W3167292670",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W3194310511",
        "https://openalex.org/W4253067820",
        "https://openalex.org/W1752822397",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W4286907499",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2047782770",
        "https://openalex.org/W1533230146",
        "https://openalex.org/W4226281578",
        "https://openalex.org/W3166445280",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2609826708",
        "https://openalex.org/W3035324702",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W3114916066",
        "https://openalex.org/W3035763680",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W3117679003",
        "https://openalex.org/W2990928880",
        "https://openalex.org/W3005552578",
        "https://openalex.org/W2170189740"
    ],
    "abstract": "Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8003 - 8016\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nLinkBERT: Pretraining Language Models with Document Links\nMichihiro Yasunaga Jure Leskovec ∗ Percy Liang∗\nStanford University\n{myasu,jure,pliang}@cs.stanford.edu\nAbstract\nLanguage model (LM) pretraining captures vari-\nous knowledge from text corpora, helping down-\nstream NLP tasks. However, existing methods\nsuch as BERT model a single document, failing\nto capture document dependencies and knowl-\nedge that spans across documents. In this work,\nwe proposeLinkBERT, an effective LM pretrain-\ning method that incorporates document links,\nsuch as hyperlinks. Given a pretraining corpus,\nwe view it as a graph of documents, and create\nLM inputs by placing linked documents in the\nsame context. We then train the LM with two\njoint self-supervised tasks: masked language\nmodeling and our newly proposed task, docu-\nment relation prediction. We study LinkBERT\nin two domains: general domain (pretrained on\nWikipedia with hyperlinks) and biomedical do-\nmain (pretrained on PubMed with citation links).\nLinkBERT outperforms BERT on various down-\nstream tasks in both domains. It is especially\neffective for multi-hop reasoning and few-shot\nQA (+5% absolute improvement on HotpotQA\nand TriviaQA), and our biomedical LinkBERT\nattains new state-of-the-art on various BioNLP\ntasks (+7% on BioASQ and USMLE). We\nrelease the pretrained models, LinkBERT and\nBioLinkBERT, as well as code and data.1\n1 Introduction\nPretrained language models (LMs), like BERT and\nGPTs (Devlin et al., 2019; Brown et al., 2020), have\nshown remarkable performance on many natural\nlanguage processing (NLP) tasks, such as text\nclassification and question answering (Raffel et al.,\n2020), becoming the foundation of modern NLP\nsystems. By performing self-supervised learning\non text, such as masked language modeling (Devlin\net al., 2019), LMs learn to encode various knowl-\nedge from text corpora and produce informative\nlanguage representations for downstream tasks\n(Bosselut et al., 2019; Bommasani et al., 2021).\n∗ Equal senior authorship.\n1Available at https://github.com/michiyasunaga/\nLinkBERT.\nLanguage Model\n[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] \nSegment A Segment B\nJapanese cherry\nMasked language modeling \n(MLM)\nDocument relation prediction \n(DRP)\n●Linked ●Random ●Contiguous \nCorpus with Document Links \n(e.g. hyperlink, reference)\nDoc 2\nDoc 5\nDoc 4 Doc 6\nDoc 1 Doc 3\nCreate\nLM inputs\nTrain \nLMs\nSegment ASegment B\nDoc 1.1 Doc 1.2\nDoc 1.1 Doc 5.2\nDoc 1.1 Doc 3.1\nContiguous\nRandom\nLinked\nor\nor\n[Tidal Basin, Washington D.C.]\nThe Tidal Basin is a man-made \nreservoir located between the \nPotomac River and the \nWashington Channel in \nWashington, D.C. It is part of \nWest Potomac Park, is near the \nNational Mall and is a focal point \nof the National Cherry Blossom \nFestival held each spring. The \nJeﬀerson Memorial, the Martin \nLuther King Jr. Memorial, the \nFranklin Delano Roosevelt \nMemorial, and the George Mason \nMemorial are situated adjacent \nto the Tidal Basin. \n[The National Cherry Blossom \nFestival] ... It is a spring \ncelebration commemorating the \nMarch 27, 1912, gift of Japanese \ncherry trees from Mayor of \nTokyo City Yukio Ozaki to the city \nof Washington, D.C. Mayor Ozaki \ngifted the trees to enhance the \ngrowing friendship between the \nUnited States and Japan. ... Of \nthe initial gift of 12 varieties of \n3,020 trees, the Yoshino Cherry \n(70% of total) and Kwanzan \nCherry (13% of total) now \ndominate. ...\nDocument Linked document(e.g. hyperlink, reference)\n[Tidal Basin, Washington D.C.]\nThe Tidal Basin is a man-made \nreservoir located between the \nPotomac River and the \nWashington Channel in \nWashington, D.C. It is part of \nWest Potomac Park, is near the \nNational Mall and is a focal point \nof the National Cherry Blossom \nFestival held each spring. The \nJeﬀerson Memorial, the Martin \nLuther King Jr. Memorial, the \nFranklin Delano Roosevelt \nMemorial, and the George Mason \nMemorial are situated adjacent \nto the Tidal Basin. \n[The National Cherry Blossom \nFestival] ... It is a spring \ncelebration commemorating the \nMarch 27, 1912, gift of Japanese \ncherry trees from Mayor of \nTokyo City Yukio Ozaki to the city \nof Washington, D.C. Mayor Ozaki \ngifted the trees to enhance the \ngrowing friendship between the \nUnited States and Japan. ... Of \nthe initial gift of 12 varieties of \n3,020 trees, the Yoshino Cherry \n(70% of total) and Kwanzan \nCherry (13% of total) now \ndominate. ...\nDocument Linked document(e.g. hyperlink, reference)\nFigure 1: Document links (e.g. hyperlinks) can provide salient\nmulti-hop knowledge. For instance, the Wikipedia article\n“Tidal Basin” (left) describes that the basin hosts “National\nCherry Blossom Festival”. The hyperlinked article (right)\nreveals that the festival celebrates “Japanese cherry trees”.\nTaken together, the link offers new knowledge not available\nin a single document (e.g. “Tidal Basin has Japanese cherry\ntrees”), which can be useful for various applications, including\nanswering a question “What trees can you see at Tidal Basin?”.\nWe aim to leverage document links to incorporate more\nknowledge into language model pretraining.\nHowever, existing LM pretraining methods typ-\nically consider a single document in each input con-\ntext (Liu et al., 2019; Joshi et al., 2020), and do not\nmodel links between documents. This can pose lim-\nitations because documents often have rich depen-\ndencies with each other (e.g. hyperlinks, references),\nand knowledge can span across documents. As a\nsimple example, in Figure 1, the Wikipedia article\n“Tidal Basin, Washington D.C.” (left) describes that\nthe basin hosts “National Cherry Blossom Festival”,\nand the hyperlinked article (right) reveals the back-\nground that the festival celebrates “Japanese cherry\ntrees”. Taken together, the hyperlink offers new,\nmulti-hop knowledge “Tidal Basin has Japanese\ncherry trees”, which is not available in the single ar-\nticle “Tidal Basin” alone. Acquiring such multi-hop\nknowledge in pretraining could be useful for various\napplications including question answering. In fact,\ndocument links like hyperlinks and references are\nubiquitous (e.g. web, books, scientific literature),\nand guide how we humans acquire knowledge and\n8003\n\r\u001c)\"0\u001c\" \u0001\u000e*\u001f '\nȤ\u001dc¦ȥȽ³ĴĒȽ ³ĹČèŏȽ\u001cèƓĹŚȽ Ȥ¦*ȥȽ Ȥm\u0001¦`ȥȽȤm\u0001¦`ȥȽƟƉĒĒƓССС\u0001 ССС\u0001 Ȥ¦*ȥȽ\n\u0014 \"( )/\u0001\u0002 \u0014 \"( )/\u0001\u0003\n^èƆèŚĒƓĒȽąĴĒƉƉǅ\n\u000e\u001c.& \u001f\u0001'\u001c)\"0\u001c\" \u0001(*\u001f '$)\"\u0001\nт\u000e\r\u000eу\n\u0005*\u001e0( )/\u0001- '\u001c/$*)\u0001\n+- \u001f$\u001e/$*)\u0001т\u0005\u0013\u0011у\nƔcĹŚŌĒČȽƔèŚČţŘȽƔ\u001dţŚƟĹĭƧţƧƓȽ\n\u0004*-+0.\u0001*!\u0001'$)& \u001f\u0001\u001f*\u001e0( )/.\n$ţąȽǛ\n$ţąȽǞ\n$ţąȽǝ $ţąȽǟ\n$ţąȽǚ $ţąȽǜ\n\u0014 \"( )/\u0001\u0002 \u0014 \"( )/\u0001\u0003\n$ţąȽǚȽƓĒĭȽƆɝǚ\n\u001dţŚƟĹĭƧţƧƓ\nèŚČţŘ\ncĹŚŌĒČ\nţƉ\nţƉ\n$ţąȽǚȽƓĒĭȽƆ\n$ţąȽǞȽƓĒĭȽƈ$ţąȽǚȽƓĒĭȽƆ\n$ţąȽǜȽƓĒĭȽƈ$ţąȽǚȽƓĒĭȽƆ\n\u0004- \u001c/ \u0001\r\u000e\u0001$)+0/. \u0011- /-\u001c$)\u0001/# \u0001\r\u000e\n#4+ -'$)&Т\u0001\n- ! - )\u001e Т\u0001 /\u001eС\nFigure 2: Overview of our approach, LinkBERT. Given a pretraining corpus, we view it as a graph of documents, with links\nsuch as hyperlinks (§4.1). To incorporate the document link knowledge into LM pretraining, we create LM inputs by placing a pair\nof linked documents in the same context (linked), besides the existing options of placing a single document (contiguous) or a pair\nof random documents (random) as in BERT. We then train the LM with two self-supervised objectives: masked language modeling\n(MLM), which predicts masked tokens in the input, and document relation prediction (DRP), which classifies the relation of\nthe two text segments in the input (contiguous, random, or linked) (§4.2).\neven make discoveries too (Margolis et al., 1999).\nIn this work, we proposeLinkBERT, an effective\nlanguage model pretraining method that incor-\nporates document link knowledge. Given a text\ncorpus, we obtain links between documents such as\nhyperlinks, and create LM inputs by placing linked\ndocuments in the same context window, besides\nthe existing option of placing a single document or\nrandom documents as in BERT. Specifically, as in\nFigure 2, after sampling an anchor text segment, we\nplace either (1) the contiguous segment from the\nsame document, (2) a random document, or (3) a\ndocument linked from anchor segment, as the next\nsegment in the input. We then train the LM with two\njoint objectives: We use masked language modeling\n(MLM) to encourage learning multi-hop knowledge\nof concepts brought into the same context by docu-\nment links (e.g. “Tidal Basin” and “Japanese cherry”\nin Figure 1). Simultaneously, we propose a Doc-\nument Relation Prediction (DRP) objective, which\nclassifies the relation of the second segment to the\nfirst segment (contiguous, random, or linked). DRP\nencourages learning the relevance and bridging con-\ncepts (e.g. “National Cherry Blossom Festival”) be-\ntween documents, beyond the ability learned in the\nvanilla next sentence prediction objective in BERT.\nViewing the pretraining corpus as a graph\nof documents, LinkBERT is also motivated as\nself-supervised learning on the graph, where DRP\nand MLM correspond to link prediction and node\nfeature prediction in graph machine learning (Yang\net al., 2015; Hu et al., 2020). Our modeling approach\nthus provides a natural fusion of language-based\nand graph-based self-supervised learning.\nWe train LinkBERT on two domains: the general\ndomain, using Wikipedia articles with hyperlinks\n(§4), and the biomedical domain, using PubMed\narticles with citation links (§6). We then evaluate\nthe pretrained models on a wide range of down-\nstream tasks including question answering, in both\ndomains. LinkBERT consistently improves on base-\nline LMs across domains and tasks. For the general\ndomain, LinkBERT outperforms BERT on MRQA\nbenchmark (+4% absolute in F1-score) as well as\nGLUE benchmark. For the biomedical domain,\nLinkBERT exceeds PubmedBERT (Gu et al., 2020)\nand attains new state-of-the-art on BLURB biomed-\nical NLP benchmark (+3% absolute in BLURB\nscore) and MedQA-USMLE reasoning task (+7%\nabsolute in accuracy). Overall, LinkBERT attains\nnotably large gains for multi-hop reasoning, multi-\ndocument understanding, and few-shot question\nanswering, suggesting that LinkBERT internalizes\nsignificantly more knowledge than existing LMs\nby pretraining with document link information.\n2 Related work\nRetrieval-augmented LMs. Several works\n(Lewis et al., 2020b; Karpukhin et al., 2020;\nOguz et al., 2020; Xie et al., 2022) introduce a\nretrieval module for LMs, where given an anchor\ntext (e.g. question), retrieved text is added to the\nsame LM context to improve model inference\n(e.g. answer prediction). These works show the\npromise of placing related documents in the same\nLM context at inference time, but they do not study\npretraining. Guu et al. (2020) pretrain an LM with\na retriever that learns to retrieve text for answering\nmasked tokens in the anchor text. In contrast,\nour focus is not on retrieval, but on pretraining a\ngeneral-purpose LM that internalizes knowledge\nthat spans across documents, which is orthogonal\nto the above works (e.g., our pretrained LM could\nbe used to initialize the LM component of these\nworks). Additionally, we focus on incorporating\ndocument links such as hyperlinks, which can offer\nsalient knowledge that common lexical retrieval\nmethods may not provide (Asai et al., 2020).\nPretrain LMs with related documents. Several\nconcurrent works use multiple related documents\n8004\nto pretrain LMs. Caciularu et al. (2021) place doc-\numents (news articles) about the same topic into the\nsame LM context, and Levine et al. (2021) place sen-\ntences of high lexical similarity into the same con-\ntext. Our work provides a general method to incor-\nporate document links into LM pretraining, where\nlexical or topical similarity can be one instance of\ndocument links, besides hyperlinks. We focus on hy-\nperlinks in this work, because we find they can bring\nin salient knowledge that may not be obvious via\nlexical similarity, and yield a more performant LM\n(§5.5). Additionally, we propose the DRP objective,\nwhich improves modeling multiple documents and\nrelations between them in LMs (§5.5).\nHyperlinks and citation links for NLP. Hyper-\nlinks are often used to learn better retrieval models.\nChang et al. (2020); Asai et al. (2020); Seonwoo\net al. (2021) use Wikipedia hyperlinks to train\nretrievers for open-domain question answering.\nMa et al. (2021) study various hyperlink-aware\npretraining tasks for retrieval. While these works\nuse hyperlinks to learn retrievers, we focus on using\nhyperlinks to create better context for learning\ngeneral-purpose LMs. Separately, Calixto et al.\n(2021) use Wikipedia hyperlinks to learn multilin-\ngual LMs. Citation links are often used to improve\nsummarization and recommendation of academic\npapers (Qazvinian and Radev, 2008; Yasunaga et al.,\n2019; Bhagavatula et al., 2018; Khadka et al., 2020;\nCohan et al., 2020). Here we leverage citation net-\nworks to improve pretraining general-purpose LMs.\nGraph-augmented LMs. Several works aug-\nment LMs with graphs, typically, knowledge graphs\n(KGs) where the nodes capture entities and edges\ntheir relations. Zhang et al. (2019); He et al. (2020);\nWang et al. (2021b) combine LM training with\nKG embeddings. Sun et al. (2020); Yasunaga et al.\n(2021); Zhang et al. (2022) combine LMs and graph\nneural networks (GNNs) to jointly train on text and\nKGs. Different from KGs, we use document graphs\nto learn knowledge that spans across documents.\n3 Preliminaries\nA language model (LM) can be pretrained from\na corpus of documents, X = {X(i)}. An LM is\na composition of two functions, fhead(fenc(X)),\nwhere the encoderfenc takes in a sequence of tokens\nX = (x1,x2,...,xn) and produces a contextualized\nvector representation for each token,(h1,h2,...,hn).\nThe head fhead typically uses these representations\nto perform self-supervised tasks in the pretraining\nstep, and perform particular downstream tasks in\nthe fine-tuning step. We build on BERT (Devlin\net al., 2019), which pretrains an LM with the\nfollowing two self-supervised tasks.\nMasked language modeling (MLM). Given a\nsequence of tokens X, a subset of tokens Y ⊆ X\nis masked, and the task is to predict the original\ntokens from the modified input. Y accounts for\n15% of the tokens inX; of those, 80% are replaced\nwith [MASK], 10% with a random token, and 10%\nare kept unchanged.\nNext sentence prediction (NSP). The NSP task\ntakes two text segments2 (XA,XB) as input, and\npredicts whether XB is the direct continuation of\nXA. Specifically, BERT first samplesXA from the\ncorpus, and then either (1) takes the next segment\nXB from the same document, or (2) samples XB\nfrom a random document in the corpus. The two\nsegments are joined via special tokens to form\nan input instance, [CLS] XA [SEP] XB [SEP],\nwhere the prediction target of[CLS] is whether XB\nindeed followsXA (contiguous or random).\nIn this work, we will further incorporate docu-\nment link information into LM pretraining. Our\napproach (§4) will build on MLM and NSP.\n4 LinkBERT\nWe present LinkBERT, a self-supervised pretraining\napproach that aims to internalize more knowledge\ninto LMs using document link information.\nSpecifically, as shown in Figure 2, instead of\nviewing the pretraining corpus as a set of documents\nX = {X(i)}, we view it as a graph of documents,\nG = (X, E), where E = {(X(i), X(j))} denotes\nlinks between documents (§4.1). The links can\nbe existing hyperlinks, or could be built by other\nmethods that capture document relevance. We\nthen consider pretraining tasks for learning from\ndocument links (§4.2): We create LM inputs by\nplacing linked documents in the same context\nwindow, besides the existing options of a single\ndocument or random documents. We use the MLM\ntask to learn concepts brought together in the con-\ntext by document links, and we also introduce the\nDocument Relation Prediction (DRP) task to learn\nrelations between documents. Finally, we discuss\nstrategies for obtaining informative pairs of linked\ndocuments to feed into LM pretraining (§4.3).\n4.1 Document graph\nGiven a pretraining corpus, we link related docu-\nments so that the links can bring together knowledge\nthat is not available in single documents. We focus\n2A segment is typically a sentence or a paragraph.\n8005\non hyperlinks, e.g., hyperlinks of Wikipedia articles\n(§5) and citation links of academic articles (§6). Hy-\nperlinks have a number of advantages. They provide\nbackground knowledge about concepts that the doc-\nument writers deemed useful—the links are likely\nto have high precision of relevance, and can also\nbring in relevant documents that may not be obvious\nvia lexical similarity alone (e.g., in Figure 1, while\nthe hyperlinked article mentions “Japanese” and\n“Yoshino” cherry trees, these words do not appear in\nthe anchor article). Hyperlinks are also ubiquitous\non the web and easily gathered at scale (Aghajanyan\net al., 2021). To construct the document graph, we\nsimply make a directed edge(X(i),X(j)) if there is\na hyperlink from documentX(i) to document X(j).\nFor comparison, we also experiment with a docu-\nment graph built by lexical similarity between docu-\nments. For each documentX(i), we use the common\nTF-IDF cosine similarity metric (Chen et al., 2017;\nYasunaga et al., 2017) to obtain top-k documents\nX(j)’s and make edges(X(i),X(j)). We usek=5.\n4.2 Pretraining tasks\nCreating input instances. Several works (Gao\net al., 2021; Levine et al., 2021) find that LMs can\nlearn stronger dependencies between words that\nwere shown together in the same context during\ntraining, than words that were not. To effectively\nlearn knowledge that spans across documents, we\ncreate LM inputs by placing linked documents in\nthe same context window, besides the existing op-\ntion of a single document or random documents.\nSpecifically, we first sample an anchor text segment\nfrom the corpus (Segment A;XA ⊆X(i)). For the\nnext segment (Segment B;XB), we either (1) use\nthe contiguous segment from the same document\n(XB ⊆X(i)), (2) sample a segment from a random\ndocument (XB ⊆X(j) where j ̸=i), or (3) sample a\nsegment from one of the documents linked from Seg-\nment A (XB ⊆ X(j) where (X(i),X(j)) ∈ E). We\nthen join the two segments via special tokens to form\nan input instance: [CLS] XA [SEP] XB [SEP].\nTraining objectives. To train the LM, we use\ntwo objectives. We apply the MLM objective to\nencourage the LM to learn multi-hop knowledge\nof concepts brought together in the same context\nby document links. We also propose a Document\nRelation Prediction (DPR) objective, which clas-\nsifies the relationr of segment XB to segment XA\n(r∈{contiguous,random,linked}). By distinguish-\ning linked from contiguous and random, DRP en-\ncourages the LM to learn the relevance and existence\nof bridging concepts between documents, besides\nthe capability learned in the vanilla NSP objective.\nTo predict r, we use the representation of [CLS]\ntoken, as in NSP. Taken together, we optimize:\nL=LMLM +LDRP (1)\n=−\nX\ni\nlogp(xi |hi)−logp(r|h[CLS]) (2)\nwhere xi is each token of the input instance,[CLS]\nXA [SEP] XB [SEP], and hi is its representation.\nGraph machine learning perspective. Our\ntwo pretraining tasks, MLM and DRP, are also\nmotivated as graph self-supervised learning on the\ndocument graph. In graph self-supervised learning,\ntwo types of tasks, node feature prediction and\nlink prediction, are commonly used to learn the\ncontent and structure of a graph. In node feature\nprediction (Hu et al., 2020), some features of a node\nare masked, and the task is to predict them using\nneighbor nodes. This corresponds to our MLM\ntask, where masked tokens in Segment A can be\npredicted using Segment B (a linked document\non the graph), and vice versa. In link prediction\n(Bordes et al., 2013; Wang et al., 2021a), the task is\nto predict the existence or type of an edge between\ntwo nodes. This corresponds to our DRP task,\nwhere we predict if the given pair of text segments\nare linked (edge), contiguous (self-loop edge), or\nrandom (no edge). Our approach can be viewed as\na natural fusion of language-based (e.g. BERT) and\ngraph-based self-supervised learning.\n4.3 Strategy to obtain linked documents\nAs described in §4.1, §4.2, our methodbuilds links\nbetween documents, and for each anchor segment,\nsamples a linked document to put together in the LM\ninput. Here we discuss three key axes to consider\nto obtain useful linked documents in this process.\nRelevance. Semantic relevance is a requisite\nwhen building links between documents. If links\nwere randomly built without relevance, LinkBERT\nwould be same as BERT, with simply two options of\nLM inputs (contiguous or random). Relevance can\nbe achieved by using hyperlinks or lexical similarity\nmetrics, and both methods yield substantially better\nperformance than using random links (§5.5).\nSalience. Besides relevance, another factor to con-\nsider (salience) is whether the linked document can\noffer new, useful knowledge that may not be obvious\nto the current LM. Hyperlinks are potentially more\nadvantageous than lexical similarity links in this\nregard: LMs are shown to be good at recognizing\nlexical similarity (Zhang et al., 2020), and hyper-\nlinks can bring in useful background knowledge that\n8006\nmay not be obvious via lexical similarity alone (Asai\net al., 2020). Indeed, we empirically find that using\nhyperlinks yields a more performant LM (§5.5).\nDiversity. In the document graph, some docu-\nments may have a very high in-degree (e.g., many\nincoming hyperlinks, like the “United States” page\nof Wikipedia), and others a low in-degree. If we uni-\nformly sample from the linked documents for each\nanchor segment, we may include documents of high\nin-degree too often in the overall training data, los-\ning diversity. To adjust so that all documents appear\nwith a similar frequency in training, we sample a\nlinked document with probability inversely propor-\ntional to its in-degree, as done in graph data mining\nliterature (Henzinger et al., 2000). We find that this\ntechnique yields a better LM performance (§5.5).\n5 Experiments\nWe experiment with our proposed approach in the\ngeneral domain first, where we pretrain LinkBERT\non Wikipedia articles with hyperlinks (§5.1) and\nevaluate on a suite of downstream tasks (§5.2). We\ncompare with BERT (Devlin et al., 2019) as our base-\nline. We experiment in the biomedical domain in §6.\n5.1 Pretraining setup\nData. We use the same pretraining corpus used\nby BERT: Wikipedia and BookCorpus (Zhu et al.,\n2015). For Wikipedia, we use the WikiExtractor3 to\nextract hyperlinks between Wiki articles. We then\ncreate training instances by sampling contiguous,\nrandom, or linked segments as described in §4, with\nthe three options appearing uniformly (33%, 33%,\n33%). For BookCorpus, we create training instance\nby sampling contiguous or random segments (50%,\n50%) as in BERT. We then combine the training\ninstances from Wikipedia and BookCorpus to train\nLinkBERT. In summary, our pretraining data is\nthe same as BERT, except that we have hyperlinks\nbetween Wikipedia articles.\nImplementation. We pretrain LinkBERT of\nthree sizes, -tiny, -base and -large, following the\nconfigurations of BERT tiny (4.4M parameters),\nBERTbase (110M params), and BERTlarge (340M\nparams) (Devlin et al., 2019; Turc et al., 2019). We\nuse -tiny mainly for ablation studies.\nFor -tiny, we pretrain from scratch with ran-\ndom weight initialization. We use the AdamW\n(Loshchilov and Hutter, 2019) optimizer with\n(β1,β2) = (0.9,0.98), warm up the learning rate\nfor the first 5,000 steps and then linearly decay it.\n3https://github.com/attardi/wikiextractor\nWe train for 10,000 steps with a peak learning rate\n5e-3, weight decay 0.01, and batch size of 2,048\nsequences with 512 tokens. Training takes 1 day\non two GeForce RTX 2080 Ti GPUs with fp16.\nFor -base, we initialize LinkBERT with the\nBERTbase checkpoint released by Devlin et al.\n(2019) and continue pretraining. We use a peak\nlearning rate 3e-4 and train for 40,000 steps. Other\ntraining hyperparameters are the same as -tiny.\nTraining takes 4 days on four A100 GPUs with fp16.\nFor -large, we follow the same procedure as -base,\nexcept that we use a peak learning rate of 2e-4. Train-\ning takes 7 days on eight A100 GPUs with fp16.\nBaselines. We compare LinkBERT with BERT.\nSpecifically, for the -tiny scale, we compare with\nBERTtiny, which we pretrain from scratch with the\nsame hyperparameters as LinkBERTtiny. The only\ndifference is that LinkBERT uses document links\nto create LM inputs, while BERT does not.\nFor -base scale, we compare with BERTbase, for\nwhich we take the BERTbase release by Devlin et al.\n(2019) and continue pretraining it with the vanilla\nBERT objectives on the same corpus for the same\nnumber of steps as LinkBERTbase.\nFor -large, we follow the same procedure as -base.\n5.2 Evaluation tasks\nWe fine-tune and evaluate LinkBERT on a suite of\ndownstream tasks.\nExtractive question answering (QA). Given a\ndocument (or set of documents) and a question as\ninput, the task is to identify an answer span from\nthe document. We evaluate on six popular datasets\nfrom the MRQA shared task (Fisch et al., 2019):\nHotpotQA (Yang et al., 2018), TriviaQA (Joshi\net al., 2017),NaturalQ (Kwiatkowski et al., 2019),\nSearchQA (Dunn et al., 2017),NewsQA (Trischler\net al., 2017), and SQuAD (Rajpurkar et al., 2016).\nAs the MRQA shared task does not have a public\ntest set, we split the dev set in half to make new\ndev and test sets. We follow the fine-tuning method\nBERT (Devlin et al., 2019) uses for extractive QA.\nMore details are provided in Appendix B.\nGLUE. The General Language Understanding\nEvaluation (GLUE) benchmark (Wang et al., 2018)\nis a popular suite of sentence-level classification\ntasks. Following BERT, we evaluate on CoLA\n(Warstadt et al., 2019),SST-2(Socher et al., 2013),\nMRPC (Dolan and Brockett, 2005), QQP, STS-B\n(Cer et al., 2017), MNLI (Williams et al., 2017),\nQNLI (Rajpurkar et al., 2016), and RTE (Dagan\net al., 2005; Haim et al., 2006; Giampiccolo\n8007\nHotpotQA TriviaQA SearchQA NaturalQ NewsQA SQuAD Avg.\nBERTtiny 49.8 43.4 50.2 58.9 41.3 56.6 50.0\nLinkBERTtiny 54.6 50.0 58.6 60.3 42.8 58.0 54.1\nBERTbase 76.0 70.3 74.2 76.5 65.7 88.7 75.2\nLinkBERTbase 78.2 73.9 76.8 78.3 69.3 90.1 77.8\nBERTlarge 78.1 73.7 78.3 79.0 70.9 91.1 78.5\nLinkBERTlarge 80.8 78.2 80.5 81.0 72.6 92.7 81.0\nTable 1: Performance (F1) on MRQA question answering datasets. LinkBERT\nconsistently outperforms BERT on all datasets across the -tiny, -base, and -large scales.\nThe gain is especially large on datasets that require reasoning with multiple documents\nin the context, such as HotpotQA, TriviaQA, SearchQA.\nGLUE score\nBERTtiny 64.3\nLinkBERTtiny 64.6\nBERTbase 79.2\nLinkBERTbase 79.6\nBERTlarge 80.7\nLinkBERTlarge 81.1\nTable 2: Performance on the\nGLUE benchmark. LinkBERT\nattains comparable or moderately\nimproved performance.\nSQuAD SQuAD distract\nBERTbase 88.7 85.9\nLinkBERTbase 90.1 89.6\nTable 3: Performance (F1) on SQuAD when distracting\ndocuments are added to the context. While BERT incurs a\nlarge drop in F1, LinkBERT does not, suggesting its robustness\nin understanding document relations.\nHotpotQA TriviaQA NaturalQ SQuAD\nBERTbase 64.8 59.2 64.8 79.6\nLinkBERTbase 70.5 66.0 70.2 82.8\nTable 4: Few-shot QA performance (F1) when 10% of fine-\ntuning data is used. LinkBERT attains large gains, suggesting\nthat it internalizes more knowledge than BERT in pretraining.\nHotpotQA TriviaQA NaturalQ SQuAD\nLinkBERTtiny 54.6 50.0 60.3 58.0\nNo diversity 53.5 48.0 60.0 57.8\nChange hyperlink to TF-IDF 50.0 48.2 59.6 57.6\nChange hyperlink to random 49.8 43.4 58.9 56.6\nTable 5: Ablation study on what linked documents to feed\ninto LM pretraining (§4.3).\nHotpotQA TriviaQA NaturalQ SQuADSQuADdistract\nLinkBERTbase 78.2 73.9 78.3 90.1 89.6\nNo DRP 76.5 72.5 77.0 89.3 87.0\nTable 6:Ablation study on the document relation prediction\n(DRP) objective in LM pretraining (§4.2).\net al., 2007), and report the average score. More\nfine-tuning details are provided in Appendix B.\n5.3 Results\nTable 1 shows the performance (F1 score) on\nMRQA datasets. LinkBERT substantially outper-\nforms BERT on all datasets. On average, the gain is\n+4.1% absolute for the BERTtiny scale, +2.6% for\nthe BERTbase scale, and +2.5% for the BERTlarge\nscale. Table 2 shows the results on GLUE, where\nLinkBERT performs moderately better than BERT.\nThese results suggest that LinkBERT is especially\neffective at learning knowledge useful for QA tasks\n(e.g. world knowledge), while keeping performance\non sentence-level language understanding too.\n5.4 Analysis\nWe further study when LinkBERT is especially\nuseful in downstream tasks.\nImproved multi-hop reasoning. In Table 1,\nwe find that LinkBERT obtains notably large\ngains on QA datasets that require reasoning with\nmultiple documents, such as HotpotQA (+5% over\nBERTtiny), TriviaQA (+6%) and SearchQA (+8%),\nas opposed to SQuAD (+1.4%) which just has\na single document per question. To further gain\nqualitative insights, we studied in what QA exam-\nples LinkBERT succeeds but BERT fails. Figure\n3 shows a representative example from HotpotQA.\nAnswering the question needs 2-hop reasoning:\nidentify “Roden Brothers were taken over by Birks\nGroup” from the first document, and then “Birks\nGroup is headquartered in Montreal” from the sec-\nond document. While BERT tends to simply predict\nan entity near the question entity (“Toronto” in the\nfirst document, which is just 1-hop), LinkBERT\ncorrectly predicts the answer in the second docu-\nment (“Montreal”). Our intuition is that because\nLinkBERT is pretrained with pairs of linked docu-\nments rather than purely single documents, it better\nlearns how to flow information (e.g., do attention)\nacross tokens when multiple related documents\nare given in the context. In summary, these results\nsuggest that pretraining with linked documents\nhelps for multi-hop reasoning on downstream tasks.\nImproved understanding of document rela-\ntions. While the MRQA datasets typically use\nground-truth documents as context for answering\nquestions, in open-domain QA, QA systems need to\nuse documents obtained by a retriever, which may\ninclude noisy documents besides gold ones (Chen\net al., 2017; Dunn et al., 2017). In such cases, QA\nsystems need to understand the document relations\nto perform well (Yang et al., 2018). To simulate this\nsetting, we modify the SQuAD dataset such that\nwe prepend or append 1–2 distracting documents\nto the original document given to each question.\nTable 3 shows the result. While BERT incurs a large\nperformance drop (-2.8%), LinkBERT is robust to\ndistracting documents (-0.5%). This result suggests\nthat pretraining with document links improves\nthe ability to understand document relations and\n8008\nThree days after undergoing a laparoscopic Whipple's procedure, a \n43-year-old woman has swelling of her right leg. ... She was diagnosed \nwith pancreatic cancer 1 month ago. ... Her temperature is 38°C (100.4°\nF), pulse is 90/min, and blood pressure is 118/78 mm Hg. Examination \nshows mild swelling of the right thigh to the ankle; there is no \nerythema or pitting edema. ... Which of the following is the most \nappropriate next step in management?\n(A)  CT pulmonary angiography     (B)  Compression ultrasonography\n(C)  D-dimer level                                 (D)  2 sets of blood cultures\nLinkBERT predicts: B (✓)    PubmedBERT predicts: D (✗)\nLeg swelling, pancreatic cancer(symptom) \nDeep vein thrombosis(possible cause)\nCompression ultrasonography(next step for diagnosis)\nDoc A: ... Pancreatic cancer can induce deep \nvein thrombosis in leg ...      (e.g. Ansari et al. 2015)\nDoc B: ... Deep vein thrombosis is tested by \ncompression ultrasonography ... \n(e.g. Piovella et al. 2002)\n[Tidal Basin, Washington D.C.]\nThe Tidal Basin is a man-made \nreservoir located between the \nPotomac River and the \nWashington Channel in \nWashington, D.C. It is part of \nWest Potomac Park, is near the \nNational Mall and is a focal point \nof the National Cherry Blossom \nFestival held each spring. The \nJeﬀerson Memorial, the Martin \nLuther King Jr. Memorial, the \nFranklin Delano Roosevelt \nMemorial, and the George Mason \nMemorial are situated adjacent \nto the Tidal Basin. \nMedQA-USMLE example\nNeed multi-hop reasoning\n[The National Cherry Blossom Festival] … \nIt is a spring celebration commemorating \nthe March 27, 1912, gift of Japanese cherry \ntrees from Mayor of Tokyo City to the city of \nWashington, D.C. ... Of the initial gift of 12 \nvarieties of 3,020 trees, the Yoshino Cherry \nnow dominates. ...\nKnowledge learned via document links\nReference\nQuestion: Roden Brothers were taken over in 1953 by a group \nheadquartered in which Canadian city?\nDoc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, \nCanada by Thomas and Frank Roden.  In the 1910s the firm became \nknown as Roden Bros.  Ltd. and were later taken over by Henry Birks \nand Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, \n\"Rich Cut Glass\" with Clock House Publications in Peterborough, \nOntario, which was a reprint of the 1917 edition published by Roden \nBros., Toronto. \nDoc B: Birks Group (formerly Birks & Mayors) is a designer, \nmanufacturer and retailer of jewellery, timepieces, silverware and gifts, \nwith stores and manufacturing facilities located in Canada and the \nUnited States.  As of June 30, 2015, it operates stores under three \ndiﬀerent retail banners: … The company is headquartered in Montreal, \nQuebec, with American corporate oﬀices located in Tamarac, Florida.\nLinkBERT prediction: “Montreal” (✓)                                \n \nBERT prediction: “Toronto” (✗)\nHotpotQA example\nQuestion: Roden Brothers were taken over in 1953 by a group \nheadquartered in which Canadian city?\nDoc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, \nCanada by Thomas and Frank Roden.  In the 1910s the firm became known \nas Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons \nin 1953.  ... In 1974 Roden Bros.  Ltd. published the book, \"Rich Cut \nGlass\" with Clock House Publications in Peterborough, Ontario, which was \na reprint of the 1917 edition published by Roden Bros., Toronto. \nDoc B: Birks Group (formerly Birks & Mayors) is a designer, \nmanufacturer and retailer of jewellery, timepieces, silverware and gifts, \nwith stores and manufacturing facilities located in Canada and the United \nStates.  As of June 30, 2015, it operates stores under three different retail \nbanners: ... The company is headquartered in Montreal, Quebec, with \nAmerican corporate offices located in Tamarac, Florida.\nLinkBERT prediction: “Montreal” (✓)     BERT prediction: “Toronto” \n(✗)\nHotpotQA example\nLinkBERT predicts: “Montreal” (✓)      BERT predicts: “Toronto” (✗)\nFigure 3: Case study of multi-hop reasoning on HotpotQA.\nAnswering the question needs to identify “Roden Brothers\nwere taken over by Birks Group” from the first document,\nand then “Birks Group is headquartered in Montreal” from\nthe second document. While BERT tends to simply predict\nan entity near the question entity (“Toronto” in the first\ndocument), LinkBERT correctly predicts the answer in the\nsecond document (“Montreal”).\nrelevance. In particular, our intuition is that the\nDRP objective helps the LM to better recognize\ndocument relations like (anchor document, linked\ndocument) in pretraining, which helps to recognize\nrelations like (question, right document) in down-\nstream QA tasks. We indeed find that ablating the\nDRP objective from LinkBERT hurts performance\n(§5.5). The strength of understanding document\nrelations also suggests the promise of applying\nLinkBERT to various retrieval-augmented methods\nand tasks (e.g. Lewis et al. 2020b), either as the\nmain LM or the dense retriever component.\nImproved few-shot QA performance. We also\nfind that LinkBERT is notably good at few-shot\nlearning. Concretely, for each MRQA dataset, we\nfine-tune with only 10% of the available training\ndata, and report the performance in Table 4. In this\nfew-shot regime, LinkBERT attains more signifi-\ncant gains over BERT, compared to the full-resource\nregime in Table 1 (on NaturalQ, 5.4% vs 1.8% abso-\nlute in F1, or 15% vs 7% in relative error reduction).\nThis result suggests that LinkBERT internalizes\nmore knowledge than BERT during pretraining,\nwhich supports our core idea that document links\ncan bring in new, useful knowledge for LMs.\n5.5 Ablation studies\nWe conduct ablation studies on the key design\nchoices of LinkBERT.\nWhat linked documents to feed into LMs? We\nstudy the strategies discussed in §4.3 for obtaining\nlinked documents: relevance, salience, and diversity.\nTable 5 shows the ablation result on MRQA datasets.\nFirst, if we ignore relevance and use random doc-\nument links instead of hyperlinks, we get the same\nperformance as BERT (-4.1% on average; “random”\nin Table 5). Second, using lexical similarity links\ninstead of hyperlinks leads to 1.8% performance\ndrop (“TF-IDF”). Our intuition is that hyperlinks\ncan provide more salient knowledge that may not be\nobvious via lexical similarity alone. Nevertheless,\nusing lexical similarity links is substantially better\nthan BERT (+2.3%), confirming the efficacy of\nplacing relevant documents together in the input\nfor LM pretraining. Finally, removing the diversity\nadjustment in document sampling leads to 1% per-\nformance drop (“No diversity”). In summary, our\ninsight is that to create informative inputs for LM\npretraining, the linked documents must be seman-\ntically relevant and ideally be salient and diverse.\nEffect of the DRP objective. Table 6 shows the\nablation result on the DRP objective (§4.2). Re-\nmoving DRP in pretraining hurts downstream QA\nperformance. The drop is large on tasks with multi-\nple documents (HotpotQA, TriviaQA, and SQuAD\nwith distracting documents). This suggests that\nDRP facilitates LMs to learn document relations.\n6 Biomedical LinkBERT ( BioLinkBERT)\nPretraining LMs on biomedical text is shown\nto boost performance on biomedical NLP tasks\n(Beltagy et al., 2019; Lee et al., 2020; Lewis\net al., 2020a; Gu et al., 2020). Biomedical LMs\nare typically trained on PubMed, which contains\nabstracts and citations of biomedical papers. While\nprior works only use their raw text for pretraining,\nacademic papers have rich dependencies with each\nother via citations (references). We hypothesize\nthat incorporating citation links can help LMs learn\ndependencies between papers and knowledge that\nspans across them.\nWith this motivation, we pretrain LinkBERT on\nPubMed with citation links (§6.1), which we term\nBioLinkBERT, and evaluate on biomedical down-\nstream tasks (§6.2). As our baseline, we follow and\ncompare with the state-of-the-art biomedical LM,\nPubmedBERT (Gu et al., 2020), which has the same\narchitecture as BERT and is trained on PubMed.\n6.1 Pretraining setup\nData. We use the same pretraining corpus used\nby PubmedBERT: PubMed abstracts (21GB).4 We\n4https://pubmed.ncbi.nlm.nih.gov. We use papers\npublished before Feb. 2020 as in PubmedBERT.\n8009\nuse the Pubmed Parser5 to extract citation links be-\ntween articles. We then create training instances by\nsampling contiguous, random, or linked segments\nas described in §4, with the three options appearing\nuniformly (33%, 33%, 33%). In summary, our pre-\ntraining data is the same as PubmedBERT, except\nthat we have citation links between PubMed articles.\nImplementation. We pretrain BioLinkBERT of\n-base size (110M params) from scratch, following\nthe same hyperparamters as the PubmedBERTbase\n(Gu et al., 2020). Specifically, we use a peak learn-\ning rate 6e-4, batch size 8,192, and train for 62,500\nsteps. We warm up the learning rate in the first 10%\nof steps and then linearly decay it. Training takes\n7 days on eight A100 GPUs with fp16.\nAdditionally, while the original PubmedBERT\nrelease did not include the -large size, we pretrain\nBioLinkBERT of the -large size (340M params)\nfrom scratch, following the same procedure as\n-base, except that we use a peak learning rate of 4e-4\nand warm up steps of 20%. Training takes 21 days\non eight A100 GPUs with fp16.\nBaselines. We compare BioLinkBERT with\nPubmedBERT released by Gu et al. (2020).\n6.2 Evaluation tasks\nFor downstream tasks, we evaluate on the BLURB\nbenchmark (Gu et al., 2020), a diverse set of biomed-\nical NLP datasets, and MedQA-USMLE (Jin et al.,\n2021), a challenging biomedical QA dataset.\nBLURB consists of five named entity recog-\nnition tasks, a PICO (population, intervention,\ncomparison, and outcome) extraction task, three\nrelation extraction tasks, a sentence similarity task,\na document classification task, and two question\nanswering tasks, as summarized in Table 7. We\nfollow the same fine-tuning method and evaluation\nmetric used by PubmedBERT (Gu et al., 2020).\nMedQA-USMLE is a 4-way multi-choice QA\ntask that tests biomedical and clinical knowledge.\nThe questions are from practice tests for the US\nMedical License Exams (USMLE). The questions\ntypically require multi-hop reasoning, e.g., given\npatient symptoms, infer the likely cause, and then\nanswer the appropriate diagnosis procedure (Figure\n4). We follow the fine-tuning method in Jin et al.\n(2021). More details are provided in Appendix B.\nMMLU-professional medicine is a multi-choice\nQA task that tests biomedical knowledge and reason-\ning, and is part of the popular MMLU benchmark\n5https://github.com/titipata/pubmed_parser\nPubMed-\nBERTbase\nBioLink-\nBERTbase\nBioLink-\nBERTlarge\nNamed entity recognition\nBC5-chem(Li et al., 2016) 93.33 93.75 94.04\nBC5-disease(Li et al., 2016) 85.62 86.10 86.39\nNCBI-disease(Do˘gan et al., 2014)87.82 88.18 88.76\nBC2GM(Smith et al., 2008) 84.52 84.90 85.18\nJNLPBA(Kim et al., 2004) 80.06 79.03 80.06\nPICO extraction\nEBM PICO(Nye et al., 2018)73.38 73.97 74.19\nRelation extraction\nChemProt(Krallinger et al., 2017)77.24 77.57 79.98\nDDI(Herrero-Zazo et al., 2013)82.36 82.72 83.35\nGAD(Bravo et al., 2015) 82.34 84.39 84.90\nSentence similarity\nBIOSSES(So˘gancıo˘glu et al., 2017)92.30 93.25 93.63\nDocument classification\nHoC(Baker et al., 2016) 82.32 84.35 84.87\nQuestion answering\nPubMedQA(Jin et al., 2019) 55.84 70.20 72.18\nBioASQ(Nentidis et al., 2019)87.56 91.43 94.82\nBLURB score 81.10 83.39 84.30\nTable 7: Performance on BLURB benchmark. BioLinkBERT\nattains improvement on all tasks, establishing new state of\nthe art on BLURB. Gains are notably large on document-level\ntasks such as PubMedQA and BioASQ.\nMethods Acc. (%)\nBioBERTlarge (Lee et al., 2020) 36.7\nQAGNN(Yasunaga et al., 2021) 38.0\nGreaseLM(Zhang et al., 2022) 38.5\nPubmedBERTbase (Gu et al., 2020) 38.1\nBioLinkBERTbase(Ours) 40.0\nBioLinkBERTlarge(Ours) 44.6\nTable 8: Performance on MedQA-USMLE. BioLinkBERT\noutperforms all previous biomedical LMs.\nMethods Acc. (%)\nGPT-3 (175B params)(Brown et al., 2020) 38.7\nUnifiedQA (11B params)(Khashabi et al., 2020) 43.2\nBioLinkBERTlarge(Ours) 50.7\nTable 9: Performance on MMLU-professional medicine.\nBioLinkBERT significantly outperforms the largest general-\ndomain LM or QA model, despite having just 340M parameters.\n(Hendrycks et al., 2021) that is used to evaluate mas-\nsive language models. We take the BioLinkBERT\nfine-tuned on the above MedQA-USMLE task, and\nevaluate on this task without further adaptation.\n6.3 Results\nBLURB. Table 7 shows the results on BLURB.\nBioLinkBERTbase outperforms PubmedBERTbase\non all task categories, attaining a performance\nboost of +2% absolute on average. Moreover,\nBioLinkBERTlarge provides a further boost of +1%.\nIn total, BioLinkBERT outperforms the previous\nbest by +3% absolute, establishing new state-of-\nthe-art on the BLURB leaderboard. We see a trend\nthat gains are notably large on document-level tasks\nsuch as question answering (+7% on BioASQ and\n8010\nThree days after undergoing a laparoscopic Whipple's procedure, a \n43-year-old woman has swelling of her right leg. ... She was diagnosed \nwith pancreatic cancer 1 month ago. ... Her temperature is 38°C (100.4°\nF), pulse is 90/min, and blood pressure is 118/78 mm Hg. Examination \nshows mild swelling of the right thigh to the ankle; there is no \nerythema or pitting edema. ... Which of the following is the most \nappropriate next step in management?\n(A)  CT pulmonary angiography     (B)  Compression ultrasonography\n(C)  D-dimer level                                 (D)  2 sets of blood cultures\nLinkBERT predicts: B (✓)    PubmedBERT predicts: D (✗)\nLeg swelling, pancreatic cancer(symptom) \nDeep vein thrombosis(possible cause)\nCompression ultrasonography(next step for diagnosis)\nDoc A: ... Pancreatic cancer can induce deep \nvein thrombosis in leg ...      (e.g. Ansari et al. 2015)\nDoc B: ... Deep vein thrombosis is tested by \ncompression ultrasonography ... \n(e.g. Piovella et al. 2002)\n[Tidal Basin, Washington D.C.]\nThe Tidal Basin is a man-made \nreservoir located between the \nPotomac River and the \nWashington Channel in \nWashington, D.C. It is part of \nWest Potomac Park, is near the \nNational Mall and is a focal point \nof the National Cherry Blossom \nFestival held each spring. The \nJeﬀerson Memorial, the Martin \nLuther King Jr. Memorial, the \nFranklin Delano Roosevelt \nMemorial, and the George Mason \nMemorial are situated adjacent \nto the Tidal Basin. \nMedQA-USMLE example\nNeed multi-hop reasoning\n[The National Cherry Blossom Festival] … \nIt is a spring celebration commemorating \nthe March 27, 1912, gift of Japanese cherry \ntrees from Mayor of Tokyo City to the city of \nWashington, D.C. ... Of the initial gift of 12 \nvarieties of 3,020 trees, the Yoshino Cherry \nnow dominates. ...\nKnowledge learned via document links\nReference\nQuestion: Roden Brothers were taken over in 1953 by a group \nheadquartered in which Canadian city?\nDoc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, \nCanada by Thomas and Frank Roden.  In the 1910s the firm became \nknown as Roden Bros.  Ltd. and were later taken over by Henry Birks \nand Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, \n\"Rich Cut Glass\" with Clock House Publications in Peterborough, \nOntario, which was a reprint of the 1917 edition published by Roden \nBros., Toronto. \nDoc B: Birks Group (formerly Birks & Mayors) is a designer, \nmanufacturer and retailer of jewellery, timepieces, silverware and gifts, \nwith stores and manufacturing facilities located in Canada and the \nUnited States.  As of June 30, 2015, it operates stores under three \ndiﬀerent retail banners: … The company is headquartered in Montreal, \nQuebec, with American corporate oﬀices located in Tamarac, Florida.\nLinkBERT prediction: “Montreal” (✓)                                \n \nBERT prediction: “Toronto” (✗)\nHotpotQA example\nQuestion: Roden Brothers were taken over in 1953 by a group \nheadquartered in which Canadian city?\nDoc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, \nCanada by Thomas and Frank Roden.  In the 1910s the firm became known \nas Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons \nin 1953.  ... In 1974 Roden Bros.  Ltd. published the book, \"Rich Cut \nGlass\" with Clock House Publications in Peterborough, Ontario, which was \na reprint of the 1917 edition published by Roden Bros., Toronto. \nDoc B: Birks Group (formerly Birks & Mayors) is a designer, \nmanufacturer and retailer of jewellery, timepieces, silverware and gifts, \nwith stores and manufacturing facilities located in Canada and the United \nStates.  As of June 30, 2015, it operates stores under three different retail \nbanners: ... The company is headquartered in Montreal, Quebec, with \nAmerican corporate offices located in Tamarac, Florida.\nLinkBERT prediction: “Montreal” (✓)     BERT prediction: “Toronto” \n(✗)\nHotpotQA example\nLinkBERT predicts: “Montreal” (✓)       BERT predicts: “Toronto” \n(✗)\nFigure 4: Case study of multi-hop reasoning on MedQA-USMLE. Answering the question (left) needs 2-hop reasoning (center):\nfrom the patient symptoms described in the question (leg swelling, pancreatic cancer), infer the cause (deep vein thrombosis),\nand then infer the appropriate diagnosis procedure (compression ultrasonography). While the existing PubmedBERT tends to\nsimply predict a choice that contains a word appearing in the question (“blood” for choice D), BioLinkBERT correctly predicts\nthe answer (B). Our intuition is that citation links bring relevant documents together in the same context in pretraining (right),\nwhich readily provides the multi-hop knowledge needed for the reasoning (center).\nPubMedQA). This result is consistent with the\ngeneral domain (§5.3) and confirms that LinkBERT\nhelps to learn document dependencies better.\nMedQA-USMLE. Table 8 shows the results.\nBioLinkBERTbase obtains a 2% accuracy boost\nover PubmedBERTbase, and BioLinkBERT large\nprovides an additional +5% boost. In total, Bi-\noLinkBERT outperforms the previous best by +7%\nabsolute, attaining new state-of-the-art. To further\ngain qualitative insights, we studied in what QA\nexamples BioLinkBERT succeeds but the baseline\nPubmedBERT fails. Figure 4 shows a representative\nexample. Answering the question (left) needs 2-hop\nreasoning (center): from the patient symptoms\ndescribed in the question (leg swelling, pancreatic\ncancer), infer the cause ( deep vein thrombosis ),\nand then infer the appropriate diagnosis procedure\n(compression ultrasonography). We find that while\nthe existing PubmedBERT tends to simply predict\na choice that contains a word appearing in the\nquestion (“blood” for choice D), BioLinkBERT\ncorrectly predicts the answer (B). Our intuition is\nthat citation links bring relevant documents and\nconcepts together in the same context in pretraining\n(right),6 which readily provides the multi-hop\nknowledge needed for the reasoning (center). Com-\nbined with the analysis on HotpotQA (§5.4), our\nresults suggest that pretraining with document links\nconsistently helps for multi-hop reasoning across\ndomains (e.g., general documents with hyperlinks\nand biomedical articles with citation links).\nMMLU-professional medicine. Table 9 shows\nthe performance. Despite having just 340M parame-\n6For instance, as in Figure 4 (right), Ansari et al. (2015) in\nPubMed mention thatpancreatic cancer can induce deep vein\nthrombosis in leg, and it cites a paper in PubMed, Piovella et al.\n(2002), which mention thatdeep vein thrombosis is tested by\ncompression ultrasonography. Placing these two documents\nin the same context yields the complete multi-hop knowledge\nneeded to answer the question (“pancreatic cancer” →“deep\nvein thrombosis” →“compression ultrasonography”).\nters, BioLinkBERTlarge achieves 50% accuracy on\nthis QA task, significantly outperforming the largest\ngeneral-domain LM or QA models such as GPT-3\n175B params (39% accuracy) and UnifiedQA 11B\nparams (43% accuracy). This result shows that\nwith an effective pretraining approach, a small\ndomain-specialized LM can outperform orders of\nmagnitude larger language models on QA tasks.\n7 Conclusion\nWe presented LinkBERT, a new language model\n(LM) pretraining method that incorporates docu-\nment link knowledge such as hyperlinks. In both\nthe general domain (pretrained on Wikipedia with\nhyperlinks) and biomedical domain (pretrained on\nPubMed with citation links), LinkBERT outper-\nforms previous BERT models across a wide range\nof downstream tasks. The gains are notably large\nfor multi-hop reasoning, multi-document under-\nstanding and few-shot question answering, suggest-\ning that LinkBERT effectively internalizes salient\nknowledge through document links. Our results sug-\ngest that LinkBERT can be a strong pretrained LM\nto be applied to various knowledge-intensive tasks.\nReproducibility\nPretrained models, code and data are available at\nhttps://github.com/michiyasunaga/\nLinkBERT.\nExperiments are available at\nhttps://worksheets.\ncodalab.org/worksheets/\n0x7a6ab9c8d06a41d191335b270da2902e.\nAcknowledgment\nWe thank Siddharth Karamcheti, members of the\nStanford P-Lambda, SNAP and NLP groups, as well\nas our anonymous reviewers for valuable feedback.\nWe gratefully acknowledge the support of NSF\nCAREER Award IIS-1552635; DARPA under Nos.\n8011\nHR00112190039 (TAMI), N660011924033 (MCS);\nFunai Foundation Fellowship; Microsoft Research\nPhD Fellowship; ARO under Nos. W911NF-16-1-\n0342 (MURI), W911NF-16-1-0171 (DURIP); NSF\nunder Nos. OAC-1835598 (CINES), OAC-1934578\n(HDR), CCF-1918940 (Expeditions), IIS-2030477\n(RAPID), NIH under No. R56LM013365; Stanford\nData Science Initiative, Wu Tsai Neurosciences\nInstitute, Chan Zuckerberg Biohub, Amazon,\nJPMorgan Chase, Docomo, Hitachi, Intel, KDDI,\nToshiba, NEC, Juniper, and UnitedHealth Group.\nReferences\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke\nZettlemoyer. 2021. Htlm: Hyper-text pre-training\nand prompting of language models. arXiv preprint\narXiv:2107.06955.\nDavid Ansari, Daniel Ansari, Roland Andersson, and\nÅke Andrén-Sandberg. 2015. Pancreatic cancer and\nthromboembolic disease, 150 years after trousseau.\nHepatobiliary surgery and nutrition, 4(5):325.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning\nto retrieve reasoning paths over wikipedia graph for\nquestion answering. In International Conference on\nLearning Representations (ICLR).\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali,\nJohan Högberg, Ulla Stenius, and Anna Korhonen.\n2016. Automatic semantic classification of scientific\nliterature according to the hallmarks of cancer.\nBioinformatics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nPretrained language model for scientific text. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nChandra Bhagavatula, Sergey Feldman, Russell Power,\nand Waleed Ammar. 2018. Content-based citation\nrecommendation. In North American Chapter of the\nAssociation for Computational Linguistics (NAACL).\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri Chatterji, Annie\nChen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,\nRohith Kuditipudi, Ananya Kumar, Faisal Ladhak,\nMina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,\nXiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik,\nChristopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika\nNarayan, Deepak Narayanan, Ben Newman, Allen\nNie, Juan Carlos Niebles, Hamed Nilforoshan, Julian\nNyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou,\nJoon Sung Park, Chris Piech, Eva Portelance, Christo-\npher Potts, Aditi Raghunathan, Rob Reich, Hongyu\nRen, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack\nRyan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa,\nKeshav Santhanam, Andy Shih, Krishnan Srinivasan,\nAlex Tamkin, Rohan Taori, Armin W. Thomas,\nFlorian Tramèr, Rose E. Wang, William Wang, Bohan\nWu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie,\nMichihiro Yasunaga, Jiaxuan You, Matei Zaharia,\nMichael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui\nZhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.\n2021. On the opportunities and risks of foundation\nmodels. arXiv preprint arXiv:2108.07258.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,\nJason Weston, and Oksana Yakhnenko. 2013.\nTranslating embeddings for modeling multi-relational\ndata. In Advances in Neural Information Processing\nSystems (NeurIPS).\nAntoine Bosselut, Hannah Rashkin, Maarten Sap,\nChaitanya Malaviya, Asli Çelikyilmaz, and Yejin\nChoi. 2019. Comet: Commonsense transformers\nfor automatic knowledge graph construction. In\nAssociation for Computational Linguistics (ACL).\nÀlex Bravo, Janet Piñero, Núria Queralt-Rosinach,\nMichael Rautschka, and Laura I Furlong. 2015.\nExtraction of relations between genes and diseases\nfrom text and large-scale data analysis: implications\nfor translational research. BMC bioinformatics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nAvi Caciularu, Arman Cohan, Iz Beltagy, Matthew E\nPeters, Arie Cattan, and Ido Dagan. 2021. Cross-\ndocument language modeling. arXiv preprint\narXiv:2101.00406.\nIacer Calixto, Alessandro Raganato, and Tommaso\nPasini. 2021. Wikipedia entities as rendezvous\nacross languages: Grounding multilingual language\nmodels by predicting wikipedia hyperlinks. In\nNorth American Chapter of the Association for\nComputational Linguistics (NAACL).\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. In International\nWorkshop on Semantic Evaluation (SemEval).\n8012\nWei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming\nYang, and Sanjiv Kumar. 2020. Pre-training tasks\nfor embedding-based large-scale retrieval. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Association for Computational\nLinguistics (ACL).\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel S Weld. 2020. Specter:\nDocument-level representation learning using\ncitation-informed transformers. In Association for\nComputational Linguistics (ACL).\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2005.\nThe pascal recognising textual entailment challenge.\nIn Machine Learning Challenges Workshop.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In North American Chapter of the\nAssociation for Computational Linguistics (NAACL).\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for disease\nname recognition and concept normalization.Journal\nof biomedical informatics.\nWilliam B Dolan and Chris Brockett. 2005. Automat-\nically constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur\nGuney, V olkan Cirik, and Kyunghyun Cho. 2017.\nSearchqa: A new q&a dataset augmented with\ncontext from a search engine. arXiv preprint\narXiv:1704.05179.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo,\nEunsol Choi, and Danqi Chen. 2019. Mrqa 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Workshop on Machine Reading\nfor Question Answering.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-\nshot learners. In Association for Computational\nLinguistics (ACL).\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nWilliam B Dolan. 2007. The third pascal recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and\nparaphrasing.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2020. Domain-specific\nlanguage model pretraining for biomedical natural lan-\nguage processing. arXiv preprint arXiv:2007.15779.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. InInterna-\ntional Conference on Machine Learning (ICML).\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual\nentailment challenge. In Proceedings of the Second\nPASCAL Challenges Workshop on Recognising\nTextual Entailment.\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. 2020. Integrating\ngraph contextualized knowledge into pre-trained\nlanguage models. In Findings of EMNLP.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. 2021. Measuring massive multitask\nlanguage understanding. In International Conference\non Learning Representations (ICLR).\nMonika R Henzinger, Allan Heydon, Michael Mitzen-\nmacher, and Marc Najork. 2000. On near-uniform\nurl sampling. Computer Networks.\nMaría Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMartínez, and Thierry Declerck. 2013. The ddi\ncorpus: An annotated corpus with pharmacological\nsubstances and drug–drug interactions. Journal of\nbiomedical informatics.\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka\nZitnik, Percy Liang, Vijay Pande, and Jure Leskovec.\n2020. Strategies for pre-training graph neural\nnetworks. In International Conference on Learning\nRepresentations (ICLR).\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams.\nApplied Sciences.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanbert: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and\nLuke Zettlemoyer. 2017. Triviaqa: A large scale\ndistantly supervised challenge dataset for reading\ncomprehension. In Association for Computational\nLinguistics (ACL).\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval\n8013\nfor open-domain question answering. In Empirical\nMethods in Natural Language Processing (EMNLP).\nAnita Khadka, Ivan Cantador, and Miriam Fernandez.\n2020. Exploiting citation knowledge in personalised\nrecommendation of recent scientific publications.\nIn Language Resources and Evaluation Conference\n(LREC).\nDaniel Khashabi, Tushar Khot, Ashish Sabharwal,\nOyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi.\n2020. Unifiedqa: Crossing format boundaries with\na single qa system. InFindings of EMNLP.\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,\nYuka Tateisi, and Nigel Collier. 2004. Introduction to\nthe bio-entity recognition task at jnlpba. InProceed-\nings of the international joint workshop on natural lan-\nguage processing in biomedicine and its applications.\nMartin Krallinger, Obdulia Rabal, Saber A Akhondi,\nMartın Pérez Pérez, Jesús Santamaría, Gael Pérez\nRodríguez, Georgios Tsatsaronis, and Ander In-\ntxaurrondo. 2017. Overview of the biocreative vi\nchemical-protein interaction track. In Proceedings of\nthe sixth BioCreative challenge evaluation workshop.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics (TACL).\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics.\nYoav Levine, Noam Wies, Daniel Jannai, Dan Navon,\nYedid Hoshen, and Amnon Shashua. 2021. The\ninductive bias of in-context learning: Rethink-\ning pretraining example design. arXiv preprint\narXiv:2110.04541.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin\nStoyanov. 2020a. Pretrained language models for\nbiomedical and clinical tasks: Understanding and\nextending the state-of-the-art. InProceedings of the\n3rd Clinical Natural Language Processing Workshop.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\net al. 2020b. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. InAdvances in Neural\nInformation Processing Systems (NeurIPS).\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky,\nChih-Hsuan Wei, Robert Leaman, Allan Peter Davis,\nCarolyn J Mattingly, Thomas C Wiegers, and Zhiyong\nLu. 2016. Biocreative v cdr task corpus: a resource\nfor chemical disease relation extraction.Database.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International\nConference on Learning Representations (ICLR).\nZhengyi Ma, Zhicheng Dou, Wei Xu, Xinyu Zhang,\nHao Jiang, Zhao Cao, and Ji-Rong Wen. 2021.\nPre-training for ad-hoc retrieval: Hyperlink is\nalso you need. In Conference on Information and\nKnowledge Management (CIKM).\nEric Margolis, Stephen Laurence, et al. 1999.Concepts:\ncore readings. Mit Press.\nAnastasios Nentidis, Konstantinos Bougiatiotis, Anas-\ntasia Krithara, and Georgios Paliouras. 2019. Results\nof the seventh edition of the bioasq challenge. In\nJoint European Conference on Machine Learning and\nKnowledge Discovery in Databases.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei\nYang, Iain J Marshall, Ani Nenkova, and Byron C\nWallace. 2018. A corpus with multi-level annotations\nof patients, interventions and outcomes to support\nlanguage processing for medical literature. In\nProceedings of the conference. Association for\nComputational Linguistics. Meeting.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan\nPeshterliev, Dmytro Okhonko, Michael Schlichtkrull,\nSonal Gupta, Yashar Mehdad, and Scott Yih. 2020.\nUnified open-domain question answering with\nstructured and unstructured knowledge. arXiv\npreprint arXiv:2012.14610.\nFranco Piovella, Luciano Crippa, Marisa Barone,\nS Vigano D’Angelo, Silvia Serafini, Laura Galli,\nChiara Beltrametti, and Armando D’Angelo. 2002.\nNormalization rates of compression ultrasonography\nin patients with a first episode of deep vein thrombosis\nof the lower limbs: association with recurrence and\nnew thrombosis. Haematologica, 87(5):515–522.\nVahed Qazvinian and Dragomir R Radev. 2008.\nScientific paper summarization using citation\nsummary networks. In International Conference on\nComputational Linguistics (COLING).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. In Empirical\nMethods in Natural Language Processing (EMNLP).\n8014\nYeon Seonwoo, Sang-Woo Lee, Ji-Hoon Kim, Jung-Woo\nHa, and Alice Oh. 2021. Weakly supervised pre-\ntraining for multi-hop retriever. InFindings of ACL.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2020. Towards controllable biases\nin language generation. In the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP)-Findings, long.\nLarry Smith, Lorraine K Tanabe, Rie Johnson nee Ando,\nCheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi\nLin, Roman Klinger, Christoph M Friedrich, Kuzman\nGanchev, et al. 2008. Overview of biocreative ii gene\nmention recognition. Genome biology.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment\ntreebank. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nGizem So ˘gancıo˘glu, Hakime Öztürk, and Arzucan\nÖzgür. 2017. Biosses: a semantic sentence similarity\nestimation system for the biomedical domain.\nBioinformatics.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng\nGuo, Yaru Hu, Xuan-Jing Huang, and Zheng\nZhang. 2020. Colake: Contextualized language and\nknowledge embedding. In International Conference\non Computational Linguistics (COLING).\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris,\nAlessandro Sordoni, Philip Bachman, and Kaheer\nSuleman. 2017. Newsqa: A machine comprehension\ndataset. In Workshop on Representation Learning for\nNLP.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP.\nHongwei Wang, Hongyu Ren, and Jure Leskovec. 2021a.\nRelational message passing for knowledge graph com-\npletion. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021b. Kepler: A unified model for knowledge\nembedding and pre-trained language representation.\nTransactions of the Association for Computational\nLinguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R\nBowman. 2019. Neural network acceptability\njudgments. Transactions of the Association for\nComputational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n2017. A broad-coverage challenge corpus for\nsentence understanding through inference. In\nNorth American Chapter of the Association for\nComputational Linguistics (NAACL).\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. Unifiedskg:\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models.arXiv\npreprint arXiv:2201.05966.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2015. Embedding entities and\nrelations for learning and inference in knowledge\nbases. In International Conference on Learning\nRepresentations (ICLR).\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nMichihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexan-\nder R Fabbri, Irene Li, Dan Friedman, and Dragomir R\nRadev. 2019. Scisummnet: A large annotated corpus\nand content-impact models for scientific paper sum-\nmarization with citation networks. In Proceedings\nof the AAAI Conference on Artificial Intelligence.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nReasoning with language models and knowledge\ngraphs for question answering. In North American\nChapter of the Association for Computational\nLinguistics (NAACL).\nMichihiro Yasunaga, Rui Zhang, Kshitijh Meelu,\nAyush Pareek, Krishnan Srinivasan, and Dragomir\nRadev. 2017. Graph-based neural multi-document\nsummarization. In Conference on Computational\nNatural Language Learning (CoNLL).\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2020. Bertscore:\nEvaluating text generation with bert. InInternational\nConference on Learning Representations (ICLR).\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\nHongyu Ren, Percy Liang, Christopher D Man-\nning, and Jure Leskovec. 2022. Greaselm: Graph\nreasoning enhanced language models for question\nanswering. In International Conference on Learning\nRepresentations (ICLR).\n8015\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities.\narXiv preprint arXiv:1905.07129.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In International Conference on\nComputer Vision (ICCV).\nA Ethics, limitations and risks\nWe outline potential ethical issues with our work\nbelow. First, LinkBERT is trained on the same\ntext corpora (e.g., Wikipedia, Books, PubMed)\nas in existing language models. Consequently,\nLinkBERT could reflect the same biases and toxic\nbehaviors exhibited by language models, such as\nbiases about race, gender, and other demographic\nattributes (Sheng et al., 2020).\nAnother source of ethical concern is the use of\nthe MedQA-USMLE evaluation (Jin et al., 2021).\nWhile we find this clinical reasoning task to be an\ninteresting testbed for LinkBERT and for multi-hop\nreasoning in general, we do not encourage users\nto use the current models for real world clinical\nprediction.\nB Fine-tuning details\nWe apply the following fine-tuning hyperparameters\nto all models, including the baselines.\nMRQA. For all the extractive question answering\ndatasets, we use max_seq_length = 384 and a\nsliding window of size128 if the lengths are longer\nthan max_seq_length.\nFor the -tiny scale (BERTtiny, LinkBERTtiny),\nwe choose learning rates from {5e-5, 1e-4, 3e-4},\nbatch sizes from {16, 32, 64}, and fine-tuning\nepochs from {5, 10}.\nFor -base (BERT base, LinkBERT base), we\nchoose learning rates from {2e-5, 3e-5}, batch sizes\nfrom {12, 24}, and fine-tuning epochs from {2, 4}.\nFor -large (BERT large, LinkBERT large), we\nchoose learning rates from {1e-5, 2e-5}, batch sizes\nfrom {16, 32}, and fine-tuning epochs from {2, 4}.\nGLUE. We usemax_seq_length = 128.\nFor the -tiny scale (BERTtiny, LinkBERTtiny),\nwe choose learning rates from {5e-5, 1e-4, 3e-4},\nbatch sizes from {16, 32, 64}, and fine-tuning\nepochs from {5, 10}.\nFor -base and -large (BERTbase, LinkBERTbase,\nBERTlarge, LinkBERTlarge), we choose learning\nrates from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5}, batch sizes\nfrom {16, 32, 64} and fine-tuning epochs from 3–10.\nBLURB. We use max_seq_length = 512 and\nchoose learning rates from {1e-5, 2e-5, 3e-5, 5e-5,\n6e-5}, batch sizes from {16, 32, 64} and fine-tuning\nepochs from 1–120.\nMedQA-USMLE. We use max_seq_length\n= 512 and choose learning rates from {1e-5, 2e-5,\n3e-5}, batch sizes from {16, 32, 64} and fine-tuning\nepochs from 1–6.\n8016"
}