{
  "title": "Identification of Adjective-Noun Neologisms using Pretrained Language Models",
  "url": "https://openalex.org/W2970128736",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A4212104882",
      "name": "John Philip McCrae",
      "affiliations": [
        "Ollscoil na Gaillimhe – University of Galway"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2143017621",
    "https://openalex.org/W2137607259",
    "https://openalex.org/W2250962466",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2131598923",
    "https://openalex.org/W2245085406",
    "https://openalex.org/W1889268436",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2250320835",
    "https://openalex.org/W2786146541",
    "https://openalex.org/W941635066",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2617565638",
    "https://openalex.org/W179719743",
    "https://openalex.org/W1608322251",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2057399676",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W3173650417",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2514891156",
    "https://openalex.org/W2249806877",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W1897587124",
    "https://openalex.org/W53004240",
    "https://openalex.org/W2182620654",
    "https://openalex.org/W2739929300",
    "https://openalex.org/W2241566302"
  ],
  "abstract": "&lt;p&gt;Neologism detection is a key task in the constructing of lexical resources and has wider implications for NLP, however the identification of multiword neologisms has received little attention. In this paper, we show that we can effectively identify the distinction between compositional and non-compositional adjective-noun pairs by using pretrained language models and comparing this with individual word embeddings. Our results show that the use of these models significantly improves over baseline linguistic features, however the combination with linguistic features still further improves the results, suggesting the strength of a hybrid approach.&lt;/p&gt;",
  "full_text": "Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 135–141\nFlorence, Italy, August 2, 2019.c⃝2019 Association for Computational Linguistics\n135\nIdentiﬁcation of Adjective-Noun Neologisms using Pretrained Language\nModels\nJohn P. McCrae\nData Science Institute/Insight Centre for Data Analytics\nNational University of Ireland Galway\nGalway, Ireland\njohn@mccr.ae\nAbstract\nNeologism detection is a key task in the con-\nstructing of lexical resources and has wider\nimplications for NLP, however the identiﬁca-\ntion of multiword neologisms has received lit-\ntle attention. In this paper, we show that\nwe can effectively identify the distinction be-\ntween compositional and non-compositional\nadjective-noun pairs by using pretrained lan-\nguage models and comparing this with indi-\nvidual word embeddings. Our results show\nthat the use of these models signiﬁcantly im-\nproves over baseline linguistic features, how-\never the combination with linguistic features\nstill further improves the results, suggesting\nthe strength of a hybrid approach.\n1 Introduction\nIn the context of the construction of lexical re-\nsources, such as WordNet (Miller, 1995; Fell-\nbaum, 2012), a key task is the identiﬁcations of\nterms that would be of relevance for inclusion in\nthe resource and this task is called ‘neologism\ndetection.’ Detection of single word neologisms\ncan be principally accomplished by means of fre-\nquency statistics (McCrae et al., 2017) and even\nnew senses of words can be identiﬁed by means\nof topic models (Lau et al., 2012). However, this\ntask is much harder when we consider multiword\nexpressions as a multiword expression may con-\nsist of two or more words that are already in the\ndictionary but whose combination may give extra\nmeaning that could not be understood from just\nthe words that compose this multiword expression.\nFor example a ‘common viper’ is not merely a\nviper that is ‘common’, but in fact refers toVipera\nberus a speciﬁc species of snake. In contrast, a\n‘dangerous viper’ is simply a viper that is also\ndangerous and as such most lexicographers would\nprefer not to include the term in their resources.\nIn this work, we focus on a particular kind\nof construction of neologisms, that is neologisms\nwhere the term consists of a single adjective and\na noun. The reason for this focus is driven by the\nidea that the semantics of adjectives is complex in\nterms of their semantic compositionality (McCrae\net al., 2014) and this can be broadly broken down\ninto three categories, intersective, subsective and\nprivative adjectives (Partee, 2003; Bouillon and\nViegas, 1999; Morzycki, 2015). We use WordNet\nas the principle background knowledge and thus\nrely on the judgement of the WordNet lexicogra-\nphers in order to deduce if a particular adjective-\nnoun combination is a neologism.\nOur approach for detecting whether adjective-\nnoun pairs are likely to be neological is based\non the recent breakthroughs regarding pretrained\nlanguage models, such as ELMo (Peters et al.,\n2018) and BERT (Devlin et al., 2018), which have\nshown to be effective for solving a wide variety\nof tasks (Radford et al., 2018). For this partic-\nular problem of neologism detection, it is clear\nthat there is signiﬁcant value in the use of these\npretrained models as they easily create a vec-\ntor that represents the adjective-noun combina-\ntion and this can be compared with a word-based\nmodel such as GloVe (Pennington et al., 2014), to\ndeduce if an adjective-noun pair is compositional\nor neological.\nThe paper is structured as follows, ﬁrst in\nSection 2 we will describe some of the related\nwork in the identiﬁcation of neologisms, termi-\nnology and semantic compositionality. We will\nthen, in Section 3, describe how we created a\ndataset for noun-adjective neologisms and in\nparticular how we constructed a weak negative\nset for evaluation. We then describe our baseline\nmethodologies and how we used pretrained lan-\nguage models in order to identify adjective-noun\nneologism with increased accuracy. The results\n136\nof these experiments are presented in Section 4\nbefore we conclude in Section 5. The code and\ndatasets used in these experiments are avail-\nable at https://github.com/jmccrae/\nadj-noun-neologism-identification.\n2 Related Work\nNeologism identiﬁcation is a task that is a ba-\nsic task as part of the construction of a lexicon\nand as the task of lexicography is being increas-\ningly automated (Kosem et al., 2013) in the con-\ntext of infrastructures such as ELEXIS (Krek et al.,\n2018), and as such it is of increasing importance.\nHowever, while the task has received some at-\ntention, most approaches so far have signiﬁcant\nweaknesses, even though it is a major area of work\nfor publishers in lexicography (O’Donovan and\nO’Neill, 2008). Some semi-automated approaches\nhave relied on the extraction of features and the\nuse of classiﬁers such as SVMs (Falk et al., 2014)\nor on language-speciﬁc features (Breen, 2010).\nOf close relationship to this task is automatic\nterm recognition, where new terms are recog-\nnized based on their occurrence in a corpus. In\nthese works, a number of metric for assessing ‘ter-\nmhood’ (Spasi´c et al., 2013; Cram and Daille,\n2016) have been introduced and these are often\ndeveloped to work in speciﬁc domains (Buitelaar\net al., 2013). It has been shown that combinations\nof many metrics can effectively learn terms (As-\ntrakhantsev, 2014). However, previous work (Mc-\nCrae et al., 2017) as well as the results in this paper\nshow that these metrics perform poorly at identi-\nfying semantic compositionality.\nThe semantics of adjectives have been studied\nnot only from a logical perspective but as in terms\nof vector space models and word embeddings and\nin the context of analysis of semantic composi-\ntionality (Mitchell and Lapata, 2008). Most works\nstart from Mitchell and Lapata in representing\nthe compositional vector of an adjective-noun pair\nwith the following equation\np = αu + βv\nWhere p is the vector of compound, u and v\nare vectors for the individual words and α,β are\nlearned weights. This has been extended by re-\nplacing the scalar values, α and β with matri-\nces (Boleda et al., 2013):\np = Au + Bv\nDataset Positives Negatives\nTraining 9,474 84,934\nDevelopment 1,000 1,000\nTest 1,000 1,000\nTotal 11,474 86,934\nTable 1: The number of positive and (weak) negative\nexamples of adjective-nouns used in this study\nFurther, it has been suggested that adjectives\nthemselves should be matrices (Baroni and Zam-\nparelli, 2010), such that\np = Auv\nHowever, learning a matrix to represent each\nword can be quite difﬁcult. This has been fur-\nther extended to an approach where each word has\na matrix to give a general approach to semantic\ncompositionality (Socher et al., 2012). Moreover,\nit was shown that simpler models such as bidi-\nrectional LSTMs produce better results (Tai et al.,\n2015). This has lead to the development of pre-\ntrained models (Devlin et al., 2018; Peters et al.,\n2018), which can be trained on truly massive cor-\npora and then still be effectively applied to tasks\nwith relatively little training data.\n3 Methodology\n3.1 Data Preparation\nIn order to develop a classiﬁer to determine if a\nparticular adjective-noun pair is a neologism. We\nﬁrst need to develop a set of pairs that we know to\nbe neological and a set that we can assume is likely\nnot to be. For the development of the positive\nset, we simply took all the two-word expressions\nwithin Princeton WordNet 3.1, and deduced the\nlikely part-of-speech tagging using NLTK (Loper\nand Bird, 2002) and selected only those that were\ntagged as “JJ NN” or “JJ NNS”. This yielded as\nset of 11,474 terms that we could use as a positive\nset.\nDeveloping a negative set is much harder, as we\nwould need to ask an expert lexicographer to man-\nually evaluate a large number of adjective-noun\ncombinations and verify that they were not neol-\nogisms that could be put into a dictionary. As\nsuch, we rely on a weakly supervised dataset that\nwas constructed from Wikipedia. In particular,\nwe randomly chose from Wikipedia articles a list\n137\nof unique adjective-noun pairs, which again were\nidentiﬁed by part-of-speech tagging with NLTK,\nand then ﬁltered out all those pairs, which are\nalready in Wordnet. As this negative set is still\nlikely to contain some true neologisms, we per-\nformed a quick manual analysis of 100 of these\nterms showed that 5 of them were certainly wor-\nthy of inclusion in a dictionary (e.g., ‘special ed-\nucation’, ‘safe position’) as they have meanings\nthat are not deducible from the two words that\ncompose the phrase. In contrast, most of the ex-\namples in the set were clearly compositional, e.g.,\n‘British soldiers’, ‘much teamwork’, ‘new congre-\ngation’. One example was unclear ‘Korean lan-\nguage’, which does not occur in WordNet, while\nother similar terms, such as ‘English language’\nand ‘German language’ do. As such we estimate\nthat our weak negative set is about 94-95% nega-\ntive. We acknowledge that this is a weakness of\nour approach however it would be very expensive\nto construct a true gold standard and our experi-\nments and analysis below show that the system is\ncapable of effectively learning this task in spite of\nthe noisy training data.\nIn this way, we constructed a set of weak neg-\native examples that was roughly ten times larger\nthan the positive set, as our intuition was that there\nare many more negative examples in text than oc-\ncur naturally. We reserved two sets of 1,000 pos-\nitive and negative examples for test and develop-\nment as shown in Table 1.\n3.2 Baseline Models\nA natural approach for determining whether an\nadjective-noun pair is compositional would be to\ncompare the frequency with which the adjective-\nnoun occurs in comparison to the adjective and\nnoun’s total frequency. This can be achieved by\nmeans of Probabilistic Mutual Information as fol-\nlows:\nPMI (uv) =p(uv) log\n( p(uv)\np(u)p(v)\n)\nWhere p(uv) represents the probability of the\nadjective-noun pair, uv, occurring in our corpus,\ni.e., the total frequency divided by the length of the\ncorpus, and p(u) and p(v) representing the proba-\nbility of the adjective, uand the noun v. For cor-\npora we used a recent dump1 of Wikipedia and we\n1This corpus was compiled in December 2015\ndeveloped this into a simple classiﬁer by learning\na threshold, β from the development dataset ac-\ncepting a pair as a neologism if\nPMI (uv) >β\nThe results from this (in line with our previous\nexperience in this task) were little better than a ma-\njority class baseline and as such we developed a\nclassiﬁer that looked only at the words that are in\nthe compound and deduced whether they were ne-\nological based on the words themselves. The prin-\ncipal reason for this is that we are attempting to\ndistinguish between collocations and phrases rep-\nresenting novel concepts and it the frequency of\nthese are very similar, meaning that PMI does a\nvery poor job in distinguishing these two simi-\nlar but distinct linguistic phenomena. In this case\nwe used a na¨ıve Bayes classiﬁer which predicts\nif a word pair is a neologism based on whether\np(Neologism|uv) >p(¬Neologism|uv) where:\np(Neologism|uv) ∝\np(u|Neologism)p(v|Neologism)p(Neologism)\nThe relevant probabilities p(u|Neologism) was\nsimply deduced by the frequency with which a\ngiven adjective or noun occurred in our posi-\ntive or negative training set. The resulting Na ¨ıve\nBayes classiﬁer provided (surprisingly) strong re-\nsults and so we continued to use it as a feature\nwithin our complete model.\n3.3 Using Pretrained Models\nWe used three pretrained models for computing a\nsingle representation of adjective-nouns:\nUSE Universal sentence encoders (Cer et al.,\n2018) were introduced to provide a way to\nmake embeddings of whole sentences. As\nsuch, they directly model semantic composi-\ntionality and we apply them by considering\nour term as a sentence and generating an 512-\ndimensional embedding of the term.\nELMo ELMo is a pretrained language model that\nprovides a deep contextual representation of\na sentence. We used the ‘small’ model which\ngenerates a representation of 1,024 dimen-\nsions.\nBERT BERT has further innovated on the pre-\ntrained model by training in both direction.\nWe use the ﬁnal sentence encoding of our\n138\nAdjective Noun\nPretrained Language\nModel\nGloVe \nVector \nGloVe \nVector Freq Freq \nSoftmax\nFigure 1: The architecture of the neural network used to identify adjective-noun neologisms\nnoun-adjective pair, which is a vector of di-\nmensionality 768.\nIn order to deduce whether there was a sig-\nniﬁcant improvement in the compositional repre-\nsentation that was learnt by these models in con-\ntrast to the individual words, we also used a pre-\ntrained model for the individual words, namely\nGloVe (Pennington et al., 2014), which we chose\nat is has been shown to have good performance\nacross a wide number of tasks. We developed\na single vector to represent the noun-adjective\nby concatenating the two vectors we have from\nGloVe:\nguv =\n( gu\ngv\n)\nAs we discovered that the Na¨ıve Bayes baseline\nmodel was very strong we also calculated for each\nof the examples the following feature vector:\nfuv =\n\n\nlog(p(u|Neologism))\nlog(p(u|¬Neologism))\nlog(p(v|Neologism))\nlog(p(v|¬Neologism))\n\n\nWe combined all these vectors as follows:\nx = Apuv + Bguv + Cfuv (1)\nWhere x ∈ R2 and we then used a single dense\nlayer taking x as input to compare the pretrained\nrepresentation, puv with the GloVe representation,\nguv. This model is depicted in Figure 1. The\nerror function for the network was cross-entropy\nover the softmax of the values for x. The soft-\nmax was chosen to output two values which rep-\nresent the probability of a term being neological\nand not being neological respectively. All models\nwere trained with the Adam optimizer (Kingma\nand Ba, 2014) for a total of 200 epochs with a\nlearning rate of 0.01 and at the end of each epoch\nthe accuracy on the development set was evaluated\nand the ﬁnal model selected for evaluation on the\ntest set was the model with highest development\naccuracy. In general, this model occurred within\nthe ﬁrst 100 epochs so we do not expect that more\ntraining would lead to better accuracy.\n4 Results\nWe evaluated the model given in Equation 1 in a\nnumber of settings, by varying the inclusion of the\nfeatures from the model. Firstly we considered\nthe model without the use of pretrained language\nmodels and only the GloVe vectors which we term\nthe “feed forward” model, this can be considered\nas ﬁxing the corresponding matrix ( A) to zero.\nWe used the GloVe vectors trained on the 6 billion\nword corpus which comes in four dimensions, 50,\n100, 200, 300. We evaluated on all of these set-\ntings and in addition the case where we did not use\nany vectors of GloVe which we labelled as “n/a”.\nAs such the setting “feed forward (n/a)” could be\nconsidered as another baseline that does not use\nany features from deep neural networks. We then\n139\nModel GloVe\nDimensions Accuracy Precision Recall F-Measure\nPMI (Baseline) n/a 0.491 0.495 0.979 0.658\nNa¨ıve Bayes (Baseline) n/a 0.800 0.735 0.937 0.824\nFeed Forward n/a 0.834 0.850 0.810 0.829\nFeed Forward 50 0.846 0.857 0.831 0.844 ∗\nFeed Forward 100 0.846 0.818 0.889 0.852 †\nFeed Forward 200 0.835 0.852 0.810 0.830\nFeed Forward 300 0.846 0.854 0.833 0.844 ∗\nUSE n/a 0.833 0.869 0.784 0.824\nUSE 50 0.861 0.852 0.872 0.862 †\nUSE 100 0.873 0.861 0.888 0.874 †\nUSE 200 0.859 0.849 0.872 0.860 †\nUSE 300 0.862 0.844 0.887 0.865 †\nELMo n/a 0.853 0.865 0.836 0.850 †\nELMo 50 0.858 0.848 0.872 0.860 †\nELMo 100 0.860 0.873 0.841 0.857 †\nELMo 200 0.866 0.853 0.884 0.868 †\nELMo 300 0.860 0.881 0.832 0.856 †\nBERT n/a 0.830 0.808 0.866 0.835\nBERT 50 0.862 0.839 0.894 0.866 †\nBERT 100 0.882 0.895 0.866 0.880†\nBERT 200 0.854 0.872 0.830 0.850 †\nBERT 300 0.848 0.828 0.879 0.853 †\nBERT (No Freq) 100 0.846 0.834 0.863 0.848 †\nTable 2: Result for the detection of neological adjective-noun terms using our models.∗and †denote a statistically\nsigniﬁcant improvement over the Na¨ıve Bayes baseline atp= 0.05,0.01 respectively.\nevaluated all these settings on the 3 pretrained lan-\nguage models, USE, ELMo and BERT and the re-\nsults are presented in Table 2. Statistical signiﬁ-\ncance was calculated at two levels (Yeh, 2000).\nThe strongest result in accuracy, precision and\nF-Measure is the BERT model with GloVe vec-\ntors of dimensionality 100, although the USE and\nELMo methods present a similar result with GloVe\ndimensionality of 100 or 200, suggesting that the\nuse of pretrained models in general is helpful\nfor the identiﬁcation of neological adjective-noun\nphrases. The difference in performance between\nthe choice of models was however not statistically\nsigniﬁcant. Furthermore, we also observe that the\nlarger GloVe vectors are not helpful and obser-\nvations of the test set accuracy as well as pre-\nliminary experiments in more complex neural net-\nwork architectures have suggested that over-ﬁtting\nis likely the cause of this given the comparatively\nsmall training set.\nWe found that the inclusion of the frequency\nfeature remained helpful and to evaluate this we\nrerun our best scoring model with the frequency\nfeatures and presented them on the bottom row\nof Table 2, we see that the results without fre-\nquency features is still signiﬁcantly better than the\nbaseline, however the inclusion of these features\ndoes give a sizeable increase in the performance\nof the system. As such, this suggests that there is\nstill a role for traditional feature engineering ap-\nproaches alongside deep learning methodologies\nfor this task.\nFurther, we applied a qualitative analysis of the\nerrors made by the system, and we show an exam-\nple of some of the errors generated by the ELMo-\nbased system in Table 3. For most results it is\nhard to see why the system made an error, however\nthere are a few patterns, in that many of the false\nnegatives seem to contain low-frequency adjec-\ntives such as ‘antigenic’ or ‘Sullian’. In the false\n140\npositives, as expected we see some that should\nnot be counted as errors, in particular ‘alpha in-\nterferon’, and this is due to the weaknesses in\nour methodology that we have previously noted.\nWe also see many cases that would also be hard\nfor a human to decide if they are truly composi-\ntional such as ‘natural world’, ‘Korean language’\nor ‘constitutional law’, conﬁrming our results that\nthe system is producing near-human results for\nthis task.\n5 Conclusion\nWe have presented a method for identifying\nadjective-noun pairs as neologisms and have\nshown that the usage of pretrained language mod-\nels improves signiﬁcantly over other baselines.\nThis is particularly interesting as the systems pre-\nsented in this paper do not require the usage of a\nlarge corpus and as such can be robustly and easily\napplied to a large number of domains. However,\nwe discovered that simple frequency features are\nstill important and this suggests that the combina-\ntion of linguistically motivated features as well as\ndeep learning models is likely to provide the best\nresults.\nAcknowledgments\nThis publication has emanated from research sup-\nported in part by a research grant from Sci-\nence Foundation Ireland (SFI) under Grant Num-\nber SFI/12/RC/2289, co-funded by the European\nRegional Development Fund, and the European\nUnions Horizon 2020 research and innovation\nprogramme under grant agreement No 731015,\nELEXIS - European Lexical Infrastructure.\nReferences\nNikita Astrakhantsev. 2014. Automatic term acquisi-\ntion from domain-speciﬁc text collection by using\nWikipedia. Proceedings of the institute for system\nprogramming, 26(4):7–20.\nMarco Baroni and Roberto Zamparelli. 2010. Nouns\nare vectors, adjectives are matrices: Representing\nadjective-noun constructions in semantic space. In\nProceedings of the 2010 Conference on Empirical\nMethods in Natural Language Processing , pages\n1183–1193. Association for Computational Linguis-\ntics.\nGemma Boleda, Marco Baroni, Louise McNally, et al.\n2013. Intensionality was only alleged: On adjective-\nnoun composition in distributional semantics. In\nProceedings of the 10th International Conference on\nComputational Semantics (IWCS 2013)–Long Pa-\npers, pages 35–46.\nPierrette Bouillon and Evelyne Viegas. 1999. The\ndescription of adjectives for natural language pro-\ncessing: Theoretical and applied perspectives. In\nProceedings of Description des Adjectifs pour les\nTraitements Informatiques. Traitement Automatique\ndes Langues Naturelles, pages 20–30.\nJames Breen. 2010. Identiﬁcation of neologisms in\nJapanese by corpus analysis. E-lexicography in the\n21st Century: New Challenges, New Applications:\nProceedings of ELex 2009, Louvain-la Neuve, pages\n13–21.\nPaul Buitelaar, Georgeta Bordea, and Tamara Polajnar.\n2013. Domain-independent term extraction through\ndomain modelling. In The 10th international con-\nference on terminology and artiﬁcial intelligence\n(TIA 2013), Paris, France . 10th International Con-\nference on Terminology and Artiﬁcial Intelligence.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nDamien Cram and B´eatrice Daille. 2016. Terminology\nextraction with term variant detection. Proceedings\nof ACL-2016 System Demonstrations, pages 13–18.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nIngrid Falk, Delphine Bernhard, and Christophe\nG´erard. 2014. From non word to new word: Au-\ntomatically identifying neologisms in French news-\npapers. In LREC-The 9th edition of the Language\nResources and Evaluation Conference.\nChristiane Fellbaum. 2012. Wordnet. The Encyclope-\ndia of Applied Linguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nIztok Kosem, Polona Gantar, and Simon Krek. 2013.\nAutomation of lexicographic work: an opportunity\nfor both lexicographers and crowd-sourcing. Elec-\ntronic Lexicography in the 21st Century: Thinking\nOutside the Paper. Proceedings of the eLex , pages\n17–19.\nSimon Krek, John McCrae, Iztok Kosem, Tanja Wis-\nsek, Carole Tiberius, Roberto Navigli, and Bo-\nlette Sandford Pedersen. 2018. European Lexico-\ngraphic Infrastructure (ELEXIS). In Proceedings of\nthe XVIII EURALEX International Congress on Lex-\nicography in Global Contexts, pages 881–892.\n141\nFalse Negatives False positives\nSuillus albivelatus uniform button\ncritical mass natural world\nNorwegian elkhound constitutional law\nfree people single tube\nevolutionary trend religious knowledge\nﬁnancial backing transitional phase\ntotal depravity pilot error\nﬂuorescent ﬁxture Korean language\nright hand alpha interferon\nantigenic determinant regulatory region\nTable 3: Some examples of false negatives and false positives generated by the system\nJey Han Lau, Paul Cook, Diana McCarthy, David New-\nman, and Timothy Baldwin. 2012. Word sense in-\nduction for novel sense detection. In Proceedings\nof the 13th Conference of the European Chapter\nof the Association for Computational Linguistics ,\npages 591–601. Association for Computational Lin-\nguistics.\nEdward Loper and Steven Bird. 2002. NLTK: the nat-\nural language toolkit. arXiv preprint cs/0205028.\nJohn P. McCrae, Christina Unger, Francesca Quattri,\nand Philipp Cimiano. 2014. Modelling the Seman-\ntics of Adjectives in the Ontology-Lexicon Interface.\nIn Proceedings of 4th Workshop on Cognitive As-\npects of the Lexicon.\nJohn P. McCrae, Ian Wood, and Amanda Hicks.\n2017. The Colloquial WordNet: Extending Prince-\nton WordNet with Neologisms. In Proceedings of\nthe First Conference on Language, Data and Knowl-\nedge (LDK2017), pages 194–202.\nGeorge A Miller. 1995. WordNet: a lexical\ndatabase for English. Communications of the ACM,\n38(11):39–41.\nJeff Mitchell and Mirella Lapata. 2008. Vector-based\nmodels of semantic composition. proceedings of\nACL-08: HLT, pages 236–244.\nMarcin Morzycki. 2015. The lexical semantics of ad-\njectives: more than just scales , Key Topics in Se-\nmantics and Pragmatics, pages 13–87. Cambridge\nUniversity Press.\nRuth O’Donovan and Mary O’Neill. 2008. A sys-\ntematic approach to the selection of neologisms\nfor inclusion in a large monolingual dictionary.\nIn Proceedings of the 13th Euralex International\nCongress, pages 571–579.\nBarbara H Partee. 2003. Are there privative adjectives.\nIn Conference on the Philosophy of Terry Parsons,\nUniversity of Massachusetts, Amherst.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532–1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Self-published.\nRichard Socher, Brody Huval, Christopher D Manning,\nand Andrew Y Ng. 2012. Semantic compositional-\nity through recursive matrix-vector spaces. In Pro-\nceedings of the 2012 joint conference on empirical\nmethods in natural language processing and com-\nputational natural language learning , pages 1201–\n1211. Association for Computational Linguistics.\nIrena Spasi ´c, Mark Greenwood, Alun Preece, Nick\nFrancis, and Glyn Elwyn. 2013. Flexiterm: a ﬂexi-\nble term recognition method. Journal of biomedical\nsemantics, 4(1):27.\nKai Sheng Tai, Richard Socher, and Christopher D\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. arXiv preprint arXiv:1503.00075.\nAlexander Yeh. 2000. More accurate tests for the sta-\ntistical signiﬁcance of result differences. In Pro-\nceedings of the 18th conference on Computational\nlinguistics-Volume 2, pages 947–953. Association\nfor Computational Linguistics.",
  "topic": "Adjective",
  "concepts": [
    {
      "name": "Adjective",
      "score": 0.8824692964553833
    },
    {
      "name": "Computer science",
      "score": 0.704778790473938
    },
    {
      "name": "Neologism",
      "score": 0.7030521631240845
    },
    {
      "name": "Natural language processing",
      "score": 0.6269685626029968
    },
    {
      "name": "Identification (biology)",
      "score": 0.6229860782623291
    },
    {
      "name": "Noun",
      "score": 0.6000345349311829
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5765140056610107
    },
    {
      "name": "Linguistics",
      "score": 0.5118838548660278
    },
    {
      "name": "Philosophy",
      "score": 0.09325456619262695
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I188760350",
      "name": "Ollscoil na Gaillimhe – University of Galway",
      "country": "IE"
    }
  ],
  "cited_by": 6
}