{
  "title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning",
  "url": "https://openalex.org/W4385571308",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2144895168",
      "name": "Tiannan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2952165275",
      "name": "Wangchunshu Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101445476",
      "name": "Yan Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141857187",
      "name": "Xinsong Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3184784418",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W4377164416",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W3110662498",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W3214685499",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W4389665280",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3106784008",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W4385567126",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W4287240280",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4285202066",
    "https://openalex.org/W4303414778",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W2548228487",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4206634569",
    "https://openalex.org/W4298038525",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3197901717",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W3204191992",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W4212804468",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W2963828549",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W3035497460",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3176824248",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W4226498390",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W4226126941",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W2963530300"
  ],
  "abstract": "Pre-trained vision-language models (VLMs) have achieved impressive results in a range of vision-language tasks. However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and latency constraints. In this work, we introduce a distilling then pruning framework to compress large vision-language models into smaller, faster, and more accurate ones. We first shrink the size ofa pre-trained large VLM and apply knowledge distillation in the vision-language pre-training stage to obtain a task-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm to automatically infer the importance of vision and language modalities for different downstream tasks and adaptively remove redundant structures and neurons in different encoders with controllable target sparsity. We apply our framework to train EfficientVLM, a fast and accurate vision-language model consisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers, accounting for only 93 million parameters in total, which is 44.3% of the teacher model. EfficientVLM retains 98.4% performance of the teacher model and accelerates its inference speed by 2.2×. EfficientVLM achieves a large absolute improvement over previous SoTA efficient VLMs of similar sizes by a large margin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2 (+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation (CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 13899–13913\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nEfficientVLM: Fast and Accurate Vision-Language Models via\nKnowledge Distillation and Modal-adaptive Pruning\nTiannan Wang∗\nBeihang University\ntiannanwang@buaa.edu.cn\nWangchunshu Zhou*†\nETH Zurich\nwangchunshu.zhou@inf.ethz.ch\nYan Zeng\nBytedance AI Lab\nzengyan.yanne@bytedance.com\nXinsong Zhang\nBytedance AI Lab\nzhangxinsong.0320@bytedance.com\nAbstract\nPre-trained vision-language models (VLMs)\nhave achieved impressive results in a range\nof vision-language tasks. However, popular\nVLMs usually consist of hundreds of millions\nof parameters which brings challenges for fine-\ntuning and deployment in real-world applica-\ntions due to space, memory, and latency con-\nstraints. In this work, we introduce a distill-\ning then pruning framework to compress large\nvision-language models into smaller, faster, and\nmore accurate ones. We first shrink the size of\na pre-trained large VLM and apply knowledge\ndistillation in the vision-language pre-training\nstage to obtain a task-agnostic compact VLM.\nThen we propose a modal-adaptive pruning al-\ngorithm to automatically infer the importance\nof vision and language modalities for differ-\nent downstream tasks and adaptively remove\nredundant structures and neurons in different\nencoders with controllable target sparsity.\nWe apply our framework to train EfficientVLM,\na fast and accurate vision-language model con-\nsisting of 6 vision layers, 3 text layers, and\n3 cross-modal fusion layers, accounting for\nonly 93 million parameters in total, which is\n44.3% of the teacher model. EfficientVLM re-\ntains 98.4% performance of the teacher model\nand accelerates its inference speed by 2.2×.\nEfficientVLM achieves a large absolute im-\nprovement over previous SoTA efficient VLMs\nof similar sizes by a large margin on vari-\nous vision-language tasks, including VQAv2\n(+4.9%), NLVR2 (+5.6%), ITR (R@1 on TR\n+17.2%, on IR + 15.6% ) and COCO caption\ngeneration (CIDEr +6.5), demonstrating a large\npotential on training lightweight VLMs. 1\n∗Equal contribution, work done during internship at\nBytedance AI Lab\n†Correspondence to: wangchunshu.zhou@inf.ethz.ch\n1Our code and pretrained checkpoints are available at\nhttps://github.com/swaggy-TN/EfficientVLM.\n1 Introduction\nInspired by the success of large pre-trained lan-\nguage models (Devlin et al., 2019; Radford et al.,\n2018) in the field of natural language processing\n(NLP), recent studies (Su et al., 2019; Li et al.,\n2020a; Radford et al., 2021a; Kim et al., 2021;\nLi et al., 2021b) in vision-language pretraining\n(VLP) have advanced the state-of-the-art on vari-\nous vision-language tasks such as image captioning,\nvisual question answering, and image-text retrieval.\nHowever, in both NLP and vision-language do-\nmains, large Transformer-based pre-trained mod-\nels often consist of hundreds of millions, if not\nbillions, of parameters, bringing various practi-\ncal challenges for deployment. As summarized\nin Schwartz et al. (2020a) and Xu et al. (2021d),\nlarge pre-trained models require large amounts of\nspace (in terms of GPU memory and disk storage)\nand heavy computing for fine-tuning and inference,\nwhich is both costly and may lead to negative en-\nvironmental impact. Furthermore, large models\ninevitably lead to low latency, which poses a chal-\nlenge for the production environment.\nRecent literature revealed that BERT (Devlin\net al., 2019), a popular Transformer-based pre-\ntrained language model, can be effectively com-\npressed and accelerated via knowledge distilla-\ntion (Sanh et al., 2019; Jiao et al., 2019; Xu\net al., 2020; Wang et al., 2020b). However, only\na few prior works investigated building efficient\nVLMs. For instance, Wang et al. (2020a) intro-\nduced MiniVLM which combines a lighter ob-\nject detector with MiniLM (Wang et al., 2020b).\nFang et al. (2021) further proposed DistilVLM,\nwhich uses knowledge distillation to pre-train a\ncompact VLM with the guidance of a large pre-\ntrained VLM. However, their approach is limited\nto object-feature-based VLMs. As such, the vision\nfeature extractor cannot be distilled together with\nthe Transformer model in an end-to-end manner,\n13899\nwhich limits the potential of knowledge distillation.\nAs a result, existing compact VLMs are generally\nfalling short compared to regular-size VLMs.\nIn this work, we investigate strategies for VLM\ncompression and introduce a distilling then prun-\ning framework for compressing fully Transformer-\nbased VLMs. Specifically, in the first stage, we use\nknowledge distillation for task-agnostic compres-\nsion of a pre-trained VLM by aligning the logits,\nattention distribution, and hidden representations\nbetween the student model and the teacher model.\nThis results in a task-agnostic compact VLM that\nachieves competitive results on many downstream\nvision-language tasks by simply fine-tuning. The\ngeneral distillation stage reduces the size of all\nmodules (i.e., vision encoder, text encoder, cross-\nmodal encoder) equally so that the compressed\nmodel can be versatile to different downstream\ntasks. However, our preliminary study, which is\ndescribed in detail in section 3.3, shows that not all\nmodules are created equal in a VLM and their im-\nportance drastically varies on different downstream\nvision-language tasks requiring different levels of\nunderstanding on either vision and text modalities.\nThis indicates that compressing a VLM requires\nmodal- and task-specific designs. Therefore, in\nthe second stage, we propose to prune the compact\nVLM when fine-tuning on different downstream\ntasks to flexibly adjust the model size/latency ac-\ncording to modal importance. Concretely, we pro-\npose a modal-adaptive pruning strategy that regular-\nizes the model with a differentiable approximation\nto the L0-norm regularization (Louizos et al., 2017)\nto automatically infer the importance of vision and\nlanguage modalities with controllable target spar-\nsity. In this way, our method can adaptively prune\ndifferent modules in the VLM in the fine-tuning\nstage according to the relative importance of vision-\nlanguage modalities on different downstream tasks.\nWe apply our framework to compress X-\nVLM (Zeng et al., 2021), a recent Transformer-\nbased VLM and train EfficientVLM, a fast and\naccurate vision-language model. EfficientVLM\nconsists of 6 vision layers, 3 text layers, and 3 cross-\nmodal fusion layers, accounting for only 93 million\nparameters in total, which is 44.3% of the X-VLM\nmodel. EfficientVLM recovers 98.4% performance\nof X-VLM and accelerates its inference speed by\n2.2×. Experimental results show that despite being\ntrained with fewer image-text pairs, EfficientVLM\nachieves a large absolute improvement over Distil-\nVLM, the previous best-performing efficient VLM\nwith similar size and inference speed, on various\nvision-language tasks, including VQAv2 (Goyal\net al., 2017) (+6.7%), NLVR2 (Suhr et al., 2018)\n(+7.8%), ITR-COCO (Lin et al., 2014) (R@1 on\nTR +19.9%, R@1 on IR + 15.6% ) and COCO\ncaption generation (Chen et al., 2015) (CIDEr\n+6.5), demonstrating a large potential on training\nlightweight VLMs.\nTo the best of our knowledge, our work is the\nfirst attempt to (1) compress a fully Transformer-\nbased vision-language model, and (2) combine\nknowledge distillation with (modal-adaptive) prun-\ning for vision-language model compression.\n2 Related Work\nVision-Language Pre-training The existing\nwork on vision language pre-training typically falls\ninto two categories. Most methods rely on object\ndetection (Tan and Bansal, 2019; Lu et al., 2019;\nLi et al., 2019; Su et al., 2019; Li et al., 2020a;\nChen et al., 2020; Li et al., 2020b; Gan et al., 2020;\nLi et al., 2021b; Xu et al., 2021c; Liu et al., 2021;\nLi et al., 2022; Zhou et al., 2022b), where an im-\nage is represented by dozens of object-centric fea-\ntures. However, the object detection process re-\nquires high-resolution images as model input and\nis very time-consuming. Moreover, most works\nunder this category utilize pre-trained object detec-\ntors (Ren et al., 2015; Anderson et al., 2018), and\ndo not optimize the model in an end-to-end man-\nner, yielding sub-optimal performance. Therefore,\nrecent works turn to encoding images by convo-\nlutional network (Jiang et al., 2020; Huang et al.,\n2020, 2021; Wang et al., 2022) or vision trans-\nformer (Kim et al., 2021; Li et al., 2021a), largely\nimproving the inference speed. Nevertheless, some\nrecent work (Zhang et al., 2021; Zeng et al., 2021,\n2022) shows that understanding fine-grained vision\nlanguage alignments (e.g. object-level) is critical\nfor some downstream tasks such as visual reason-\ning and visual grounding.\nPre-trained Model Compression Prior work has\nshown that BERT (Devlin et al., 2019), a popu-\nlar encoder-only pre-trained Transformer (Vaswani\net al., 2017), can be effectively compressed and ac-\ncelerated. As summarized in Xu et al. (2021d) and\nXu et al. (2021a), popular BERT compression tech-\nniques include knowledge distillation (Hinton et al.,\n2015; Sanh et al., 2019; Sun et al., 2019; Jiao et al.,\n2019; Wang et al., 2020b; Zhou et al., 2022a; Xu\n13900\nTeacher<-> Student\nLayer hidden statesAttention map\nMLPHead\nLogits\n(a) pre-training with knowledge distillation\nCross-ModalEncoderVisionEncoderTextEncoderimagetext\nLogitsVison Language Model \nMLPHead Text Encoder\n(b) finetune with modal-adaptive pruning \nSelf AttentionFFNSelf AttentionFFN\n……Self AttentionFFNSelf AttentionFFN\nVision Encoder Cross-Modal EncoderSelf Attention\nFFNSelf Attention\nFFN\nCross Attention\nCross Attention…\nFigure 1: The distilling then pruning framework for training EfficientVLM. In the pre-training stage, we apply\nknowledge distillation with a pre-trained X-VLM model as the teacher. During fine-tuning, we use a modal-adaptive\npruning method to adaptively prune encoders of different modalities.\net al., 2021b) which trains a compact student net-\nwork to mimic the behavior of the original teacher\nmodel, pruning (LeCun et al., 1989; Michel et al.,\n2019; Gordon et al., 2020; Sanh et al., 2020; Lagu-\nnas et al., 2021; Wang et al., 2019; Xia et al., 2022)\nwhich prunes redundant neurons or structures in\nthe original model, module replacing (Xu et al.,\n2020) which train compact successor sub-modules\nto replace that in the original model, and quanti-\nzation (Shen et al., 2020; Zafrir et al., 2019) that\ncompresses a neural network by reducing the num-\nber of bits used to represent its parameters. On the\nother hand, a number of work also investigated ef-\nficient inference with BERT-like models with early\nexit (Teerapittayanon et al., 2016; Xin et al., 2020;\nLiu et al., 2020; Schwartz et al., 2020b; Zhou et al.,\n2020) or adaptive computation time (Graves, 2016;\nEyzaguirre et al., 2021; Zhou et al., 2023).\nIn contrast, only a few prior works investigated\nmethods to compress a pre-trained vision-language\nmodel. Fang et al. (2021) explored distilling a pre-\ntrained vision-language model into a more compact\nstudent model and proposed a teacher adaptation\nmethod that aligns object feature proposal. How-\never, their approach is limited to the use of ob-\nject detection based vision-language model, which\nmakes end-to-end distillation infeasible and results\nin unsatisfactory performance compared to the re-\ncent state-of-the-art. Wang et al. (2021) explored\ndistilling a vision-language model with a cross-\nmodal fusion module to a dual-encoder model for\nefficient retrieval. Moreover, Gan et al. (2021) ex-\nplored the lottery ticket hypothesis (Frankle and\nCarbin, 2018) in vision-language models and find\nthat sparse winning tickets exist in pre-trained\nVLMs. However, the process of finding and re-\ntraining winning tickets is less efficient compared\nto other compression methods.\n3 EfficientVLM\nIn this section, we present EfficientVLM, a fast and\naccurate vision-language model trained with our\ndistilling then pruning framework. We choose X-\nVLM (Zeng et al., 2021), one of the state-of-the-art\nvision-language models, as the teacher model. 2\n3.1 Model Overview\nEfficientVLM is a compressed version of X-VLM,\na fully Transformer-based VLM. X-VLM has the\nsame architecture as ALBEF (Li et al., 2021a),\nwhich consists of an image encoder, a text encoder,\nand a cross-modal encoder. The image encoder\ncontains 12 transformer layers, while the text en-\ncoder and the cross-modal encoder each consist\nof 6 transformer layers. The cross-modal encoder\nfuses the vision features with the text features by\ncross-attention at each layer. EfficientVLM shrinks\nthe size of X-VLM by half, thus consisting of 6 vi-\nsion layers, 3 text layers, and 3 cross-modal layers,\naccounting for only 92 million parameters in total,\nwhich is 43.6% of the X-VLM model.\nThe teacher model is optimized by: 1) aligning\nthe texts and visual concepts, where the alignments\nare in multi-granularity using a contrastive loss\nLITC, a matching loss LITM, and a masked lan-\nguage modeling loss LMLM; 2) in the meantime\n2In practice, our proposed method suits any VLMs that are\nequipped with modal-specific modules such as VLMo (Bao\net al., 2022) or ALBEF (Li et al., 2021a).\n13901\nlocating visual concepts in the image given the cor-\nresponding texts by bounding box prediction loss\nLBBOX. Overall, the vision language pre-training\nloss is:\nLVLP = LITC + LITM + LMLM + LBBOX (1)\n3.2 Pre-training with Knowledge Distillation\nWe initialize EfficientVLM with a pre-trained X-\nVLM and shrink its size by half by only retain-\ning the even-numbered layers. Then we pre-train\nEfficientVLM on image-text pairs with both the\noriginal vision-language pre-training objectives of\nX-VLM and knowledge distillation objective with\nthe pre-trained X-VLM as the teacher model. The\nknowledge distillation objective consists of atten-\ntion distillation, hidden states distillation, and logits\ndistillation.\nAttention Distillation Prior work (Jiao et al.,\n2019) on BERT distillation has shown the effec-\ntiveness of transferring the latent knowledge in\nself-attention matrices:\nA = softmax( Q ·K/\n√\ndk). (2)\nwhere Q and K denote the query and key matrix in\nthe attention layer of a transformer block. dk is the\ndimension of the key matrix as a scaling factor. We\nformulate attention distillation loss by minimizing\nthe mean square error between the self-attention\nmatrices of the teacher and the student:\nLattn = 1\nh\n∑ L\nj=1\n∑ h\ni=1\nMSE(AS\ni,j,AT\ni,2j) (3)\nwhere Ldenotes the number of layers in each en-\ncoder of the student, his the number of attention\nheads, Ai refers to the normalized attention matrix\ncorresponding to the i-th head in j-th layer of the\nstudent and in 2j-th layer of the teacher. The atten-\ntion matrix is in the shape of A∈R l×p. land p\nare the length of query and key, respectively3.\nHidden States Distillation Following Trans-\nformer distillation in TinyBERT (Jiao et al., 2019),\nwe also adopt the hidden states distillation to better\nutilize the information from the teacher model. The\nloss function is defined as follows:\nLhid =\n∑ L\ni=1\nMSE(HS\ni ,HT\n2i), (4)\nHS ∈R l×d′\nand HT ∈R l×d refer to the hid-\nden states of student and teacher networks in the\ncorresponding layer.\n3In the cross-attention module of cross-modal encoder,\np represents the length of patch sequence of vision encoder\notherwise l and p are equal\n0 5 10 15 20 25 30 35 40 45\nPruned Percentage\n55\n58\n61\n64\n67\n70\n73\n76\n79\n82Performance (R@1)\nITR-COCO T ext Retrieval\nvision\ntext\ncross\n0 5 10 15 20 25 30 35 40 45\nPruned Percentage\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84Performance (Accuracy)\nNLVR2\nvision\ntext\ncross\nFigure 2: Empirical study of modal-encoders impor-\ntance on NLVR2 and ITR-COCO tasks.\nLogits Distillation In addition to imitating the be-\nhaviors of intermediate layers, we also use knowl-\nedge distillation to fit the predictions of teacher\nmodel as in (Hinton et al., 2015). We adopt KL\ndivergence as the optimization objective:\nPre-training We formulate the final loss by\ncombing the original vision-language pre-training\nloss with general distillation loss.\nLKD = αLattn + βLhid + γLlogits\nLpretrain = λLVLP + (1 −λ)LKD\nwhere α, β, γ and λ are the weights of the loss\nterms. We only adjust the weights to scale the\nlosses to similar values so that the optimization\nprocess can perform more robustly.\n3.3 Fine-tuning with Pruning\nTo flexibly adjust the efficiency-performance trade-\noff of EfficientVLM on different downstream tasks\naccording to varying resource constraints, we pro-\npose a modal-adaptive pruning method to further\ncompress EfficientVLM to a desired size in the\nfine-tuning stage.\nAre All Modalities Created Equal in VLMs?\nUnlike prior work (Lagunas et al., 2021) on BERT\npruning where there is only one Transformer en-\ncoder, pruning VLMs are more challenging be-\ncause the importance of vision and language clues\nmay not be equally important (Cao et al., 2020).\nThis is also verified by our preliminary experiments\nwhere we prune 40% attention heads in each en-\ncoder and find that the performance drops drasti-\ncally, which is contrary to prior findings on pruning\nBERT (Michel et al., 2019).\nTo this end, we conduct an empirical study to\ninvestigate whether encoders for vision/language\nmodalities have similar importance across differ-\nent vision-language tasks. We prune each encoder\n13902\nin a fine-tuned teacher model at one time while\nleaving other encoders untouched. From Figure\n2, we observe that: (1) the encoders of different\nmodalities have different sensitivity with respect to\nhead pruning, and (2) the difference in sensitivity\nvaries on different downstream tasks. Specifically,\non the ITR-COCO task, pruning 40% heads in the\ntext encoder and the cross-modal encoder does not\nsignificantly impact performance while pruning the\nvision encoder causes a large performance drop.\nHowever, the results on NLVR2 show that the text\nencoder is as important as the image encoder in\nthis task while cross-modal encoders are not very\nsensitive to head pruning. These results suggest\nthat encoders of different modalities are not cre-\nated equal in a vision-language model, motivating\nus to explore modal-specific pruning methods for\nVLMs.\nModal-adaptive pruning A naive way to\nachieve modal-specific pruning is to manually ad-\njust the pruning percentage of different encoders\nbased on the prior observation. Specifically, we\nconsider a baseline that prunes 30% parameters\nout of each encoder as the baseline. Then for ITR-\nCOCO, we prune 10% parameters in the vision en-\ncoder while pruning 40% parameters in the text and\nthe cross-modal encoder. For NLVR2, we set this\npercentage to 10%, 10%, and 60% for image, text,\nand cross-modal encoders, respectively. These per-\ncentages are heuristically adjusted according to the\nprevious findings and the empirical performance.\nMoreover, the relative sparsity is set to ensure the\noverall sparsity of the model is similar.\nsparsity Text Retrieval Image RetrievalNLVR2\nR@1 R@5 R@10R@1 R@5 R@10val test\n.3/.3/.376.4 93.4 96.8 58.6 83.2 90.0 78.9 77.9\n.1/.4/.478.1 94.2 97.1 60.2 84.1 90.5 - -\n.1/.1/.6 - - - - - - 80.9 80.9\nTable 1: Modal-specific pruning results on NLVR2 and\nITR-COCO. All models are trained with pruning and\nknowledge distillation.\nThe results are shown in Table 4. We find that\nmanually specifying sparsity levels for different en-\ncoders according to their \"importance\" leads to sub-\nstantial improvements, demonstrating the effective-\nness of modal-specific pruning. However, manu-\nally determining the sparsity for different encoders\ncould be laborious and sub-optimal. Therefore, we\npropose modal-adaptive pruning, an end-to-end\npruning algorithm using a differentiable approxi-\nmation of L0 regularization (Louizos et al., 2017)\nto automatically infer the importance of vision and\nlanguage modalities and adaptively remove redun-\ndant structures and neurons in different encoders\nwith controllable target sparsity.\nConsider a given neural network model f(·; θ)\nparameterized by θ= {θj}n\nj=1, where each θj rep-\nresents an individual parameter weight or a block\nof weights (e.g. a column of a weight matrix) and\nndenotes the number of blocks. By introducing\nadditional binary variables z = {zj}n\nj=1 such that\nzj ∈ {0,1}, we can formulate the optimization\nobjective as below\nmin E z\n[\n1\nD\nD∑\ni=1\nL\n(\nxi,yi; ˜θ\n)\n+ λ∥˜θ∥0\n]\n(5)\nwhere ˜θ = {˜θj}denotes the set of model pa-\nrameters after pruning and its L0 norm, ∥˜θ∥0 =∑n\nj=1 zj, measures the effective size of the pruned\nmodel. {xi,yi}D\ni=1 are training examples, Lis the\ntraining loss function andλ> 0 is a constant hyper-\nparameter. The masking variables z are learned\nduring training as real numbers in the range [0,\n1]. In contrast, during inference, all the variables\nthat are below a threshold are set to 0 so that our\npruned model can achieve the expected sparsity.\nSee Appendix A for more details.\nWe also adopt knowledge distillation at fine-\ntuning with pruning stage to help the student model\nbetter preserve capacity on downstream tasks. The\nfinal training objective is as follows:\nLft = λLVL + (1 −λ)LKD + LLgr (6)\nwhere LVL represents the task-specific fine-tuning\nloss brought by the re-parameterized student model,\nthe LKD is the task-specific knowledge distillation\nloss and LLgr infers to the lagrangian loss.\n4 Experiments\n4.1 Baselines\nWe mainly compare EfficientVLM with two base-\nlines: MiniVLM (Wang et al., 2020a), a com-\npact VLM consists of a lightweight object de-\ntection model and a compact Transformers-based\nvision-language encoder, which is initialized by\nMiniLM (Wang et al., 2020b), a compressed pre-\ntrained language model; and DistillVLM(Fang\net al., 2021), which adopts the same model archi-\ntecture with MiniVLM and apply knowledge dis-\ntillation for further boosting model’s performance.\n13903\nMethod Input Length End-to-End Vision Module Text(and)Fusion Module\nimage/text Time(ms) Para(M) Time(ms) FLOPs(B) Para(M) Time(ms) FLOPs(B)\nX-VLMclip 196/35 17.8 86.1 9 .0 18 .9 123 8 .8(14.2) 4.2\n- CPU - 395.5 - 355.1 - - 40.4(56.8) -\nOSCARB 50/35 135.2 63.8 121 .9 767 .0 109 13 .3 8 .2\n- CPU - 12347.1 - 12300 - - 47.1 -\nMiniVLM 50/35 23.6 7.5 12 .2 4 .4 34.5 11 .4 2 .3\n- CPU - 418.2 - 393.9 - - 24.3 -\nViLT 200/40 21 2.4 0 .7 0 .6 109 20 .3 22 .8\n- CPU - 69.1 - 0.8 - - 68.3 -\nEfficientVLM 196/35 9.7 42.0 5 .0 8 .3 50.3 8 .5 1 .3\n- CPU - 180.8 - 171.5 - - 17.1 -\nTable 2: Model size and actual inference time for visual feature extractor and vision-language fusion model of\ncompared models. DistilVLM is of the same size and speed as MiniVLM. Actual Inference time is reported on both\nGPU and CPU.\nFor reference, we also include the performance of\nDistilDualEnc (Wang et al., 2021), ViLT (Kim\net al., 2021) and X-VLM small in our comparison.\nDistillDualEnc is a dual-encoder VLM distilled\nfrom a fusion-based VLM. ViLT is a single-stream\nVLM that feeds vision features without using re-\ngion features nor deep convolutional visual embed-\nders and X-VLM small use the same initialization\nas EfficientVLM but trained without knowledge\ndistillation or pruning.\nTo better illustrate our comparison, Table 2\nshows the model size and inference speed of the\nmodels compared. We test model inference time4\non both GPU and CPU devices which are Nvidia\nTesla V100 GPU and Intel(R) Xeon(R) Platinum\n8260 CPU @2.40GHz, respectively. Since the num-\nber of FLOPs is affected by the input sequence\nlength, we show the input image token length and\naverage text length of each model in their set-\ntings in the table. We can see that despite the\nfully Transformer-based visual feature extractor\nbeing heavier on model size, it consumes much\nless time during inference than MiniVLM. As for\nthe Transformer-based text/fusion module, Effi-\ncientVLM is slightly larger than MiniVLM and\nDistilVLM while much faster thanks to the parallel\nnature of image and text encoders in its architecture.\nDespite the extremely efficient vision module of\nViLT, it consumes more time because of its heavy\ntext and fusion encoder. Specifically, when compar-\ning with their corresponding teacher model, Distil-\nVLM only reduces the inference time of the Trans-\nformer encoder by around 15% on GPU, while\nEfficientVLM achieves a speed-up ratio of 1.9×on\n4In practice, the text encoder can be run in parallel with\nthe image encoder while being much faster. Therefore, the\ninference time of text encoders does not actually contribute to\nthe overall actual inference time of the model.\nGPU and 2.2×on CPU.\n4.2 Datasets and Tasks\nPre-training datasets We construct our pre-\ntraining dataset following (Zeng et al., 2021) 4M-\nsetting using two in-domain datasets, COCO (Lin\net al., 2014) and Visual Genome (VG) (Krishna\net al., 2017), and two out-of-domain datasets, SBU\nCaptions (Ordonez et al., 2011) and Conceptual\nCaptions (CC) (Sharma et al., 2018). Note that we\nhave cleaned the pre-training datasets to avoid data\nleaks since downstream V+L tasks have overlaps\nin images with COCO and Visual Genome. The\nstatistics of our pre-training dataset are presented\nin Appendix B.\nImage-Text RetrievalThere are two subtasks: text\nretrieval (TR) and image retrieval (IR). We evalu-\nate X-VLM on MSCOCO datasets. We adopt the\nwidely used Karpathy split (Karpathy and Li, 2015)\ndatasets. Following ALBEF and X-VLM, we op-\ntimize LITC and LITM and fine-tune the model\nfor 10 epochs. During inference, we first com-\npute s(I,T ) for all images and texts, and then\ntake the top-kcandidates and calculate pmatch(I,T )\nfor ranking. kis set to 256 for MSCOCO follow-\ning Zeng et al. (2021).\nVisual Question Answering (VQA 2.0) (Goyal\net al., 2017) It requires the model to predict an an-\nswer given an image and a question. Following\nALBEF and X-VLM, we use a three-layer Trans-\nformer decoder initialized by the cross-modal en-\ncoder of EfficientVLM to generate answers based\non the outputs of the cross-modal encoder. We fine-\ntune the model for 10 epochs. During inference,\nwe constrain the decoder to only generate from\nthe 3,129 candidate answers following Zeng et al.\n(2021); Li et al. (2021a).\nNatural Language for Visual Reasoning\n13904\nMethod ITR-TR ITR-IR NLVR2 VQA 2.0 COCO-Caption\nR@1 R@5 R@10 R@1 R@5 R@10 val test test-dev test-stdB@4 M C S\nX-VLMclip 79.0 94.5 97.9 61.5 84.6 90.8 83.15 83.48 76.92 77.02 39.4 30.5 131.0 23.6\n–98% 77.4 92.6 95.9 60.3 82.9 89.0 81.49 81.81 75.38 75.48 38.6 29.9 128.4 23.1\nOSCARB 70.0 91.1 95.5 54.0 80.8 88.5 78.07 78.36 73.4 73.2 36.5 30.3 123.7 23.1\n–98% 68.6 89.3 93.6 52.9 79.2 86.7 76.51 76.79 71.93 71.74 35.8 29.7 121.2 22.6\nDistilDualEnc - - - - - - 74.16 74.30 68.05 - - - - -\nViLT 61.5 86.3 92.7 42.7 72.9 83.1 75.7 76.1 71.3 - - - - -\nMiniVLM 58.8 85.1 91.7 45.0 74.1 84.0 73.71 73.93 69.1 69.4 35.6 28.6 119.8 21.6\nDistillVLM 58.3 84.1 91.3 43.9 73.7 83.3 - - 69.8 69.6 35.6 28.7 120.8 22.1\nX-VLMsmall 74.5 92.3 96.0 56.1 81.6 88.7 79.34 79.26 73.7 73.93 37.2 29.4 123.4 22.4\nEfficientVLM78.7 94.5 97.5 60.6 84.4 90.5 81.83 81.72 76.2 76.28 38.1 30.1 127.3 23.1\nTable 3: Main results on various downstream vision-language tasks. The top groups are teacher models and\ntheir 98% performance, which is used for reference. The bottom group contains previous efficient VLMs and the\nX-VLMsmall baseline.\n(NLVR2 (Suhr et al., 2018)) The task prescribes\nthe model to predict whether a text describes the\nrelations between two images. Following ALBEF\nand X-VLM, we extend the cross-modal encoder\nto enable reasoning over two images and perform a\ndomain pre-training step for two epochs. We then\nfine-tune the model for 10 epochs.\nImage Captioning The task requires a model\nto generate textual descriptions of input images.\nWe evaluate X-VLM on the COCO Captioning\ndataset (Chen et al., 2015). We report BLEU-\n4 (Papineni et al., 2002), METEOR (Denkowski\nand Lavie, 2014), SPICE (Anderson et al., 2016)\nand CIDEr (Vedantam et al., 2015) scores on the\nKarparthy test split. Following Zeng et al. (2021),\nwe simply adapt EfficientVLM to a multi-modal\ndecoder for caption generation. We train Effi-\ncientVLM with language modeling loss for two\nepochs on 4M data. Then, we fine-tune it on the\nCOCO Captioning dataset for 10 epochs.\n4.3 Experiment Setup\nTeacher ModelsWe initialized the teacher X-VLM\nmodel with a pre-trained CLIP ViT (Radford et al.,\n2021b) and a pre-trained BERT. We pre-train the\nX-VLM on 4 million image-text pairs for 200k\nsteps. Then we fine-tune the teacher model on\ndownstream tasks following Zeng et al. (2021).\nPre-training We pre-train EfficientVLM on the\naforementioned 4 million image-text pairs for\n400k steps with 16 ×V100 32G GPU. We adopt\nAdamW (Loshchilov and Hutter, 2019) optimizer\nand set the learning rate and weight decay as 1e-4\nand 0.01 respectively. The batch size is set to 1024.\nFine-tuning We combine the modal-adaptive prun-\ning algorithm with knowledge distillation from the\nfine-tuned teacher models. We set pruning sparsity\nat 25%. Other fine-tuning hyper-parameters are\npresented in the Appendix C.\n4.4 Experimental Results\n4.4.1 Main Results\nWe present the main results in Table 3. The top\ngroup of models denotes the base-size VLMs used\nas the teacher model for different compact VLMs.\nWe also list the 98% performance of these models\nfor better comparison. Specifically, X-VLM clip 5\nis the teacher of EfficientVLM while OSCARB is\nthe teacher of DistillVLM. In the bottom group,\nwe compare EfficientVLM with other efficient\nvision-language models as well as the X-VLMsmall\nbaseline. We can see that EfficientVLM substan-\ntially outperforms all compared models by a large\nmargin despite DistilVLM and MiniVLM being\ntrained with 7 million image-text pairs while Effi-\ncientVLM is only trained with 4 million image-text\npairs. Specifically, EfficientVLM achieves a R@1\nof 78.7% and 60.6% on Image Retrieval and Text\nRetrieval respectively, accounting for a large abso-\nlute improvement of 17.2% and 15.6% compared\nto the previous compact SoTA VLMs. We also\nachieve 81.83% and 81.72% accuracy on the val-\nidation set and test-P set of NLVR2, respectively,\nsurpassing prior efficient VLMs by a large mar-\ngin. Similar observation can also be found on VQA\n2.0 and COCO Captioning, where EfficientVLM\nachieves 76.2% accuracy and 76.28 on the test-dev\nset and test-std set, and 127.3 CIDEr score, respec-\ntively. EfficientVLM also consistently outperforms\nX-VLMsmall by a large margin on all datasets de-\n5We adopted the first version of X-VLM model as teacher\ninstead of the latest one that uses Swin-Transformer as\nits vision encoder because the model architecture of Swin-\nTransformer makes the general distillation more difficult.\n13905\nMethod ITR-TR ITR-IR NLVR2 VQA COCO-Caption\nR@1 R@5 R@10R@1 R@5 R@10 val test test-devB@4 M C S\nAblation Study Results on Pre-train Distilltion Objectives\nX-VLMsmall 73.0 91.8 96.0 55.3 81.1 88.6 78.68 78.39 73.39 35.7 29.0 117.9 21.8\n+ Logits 76.6 93.4 96.8 58.7 82.9 89.4 81.16 80.97 74.91 36.4 29.5 121.5 22.2\n+ Hidden 76.7 93.6 96.8 59.1 83.0 89.7 80.74 81.13 75.12 36.9 29.8 126.2 22.9\n+ Attn 76.5 94.1 97.0 59.0 83.0 89.6 81.06 81.01 75.22 37.9 29.8 126.2 22.9\nAblation Study Results on Fine-tuning Objectives\nEfficientVLM78.7 94.5 97.5 60.6 84.4 90.5 81.83 81.72 76.2 38.1 30.1 127.3 23.1\n- KD only 78.2 94.4 97.2 60.4 84.2 90.5 82.73 81.92 76.48 38.2 30.1 127.7 23.1\n- Pruning only77.9 94.3 97.3 59.7 83.8 90.1 80.71 80.47 74.87 6.9 10.9 8.2 3.5\n- Fine-tune only77.5 94.2 97.4 59.2 83.5 89.9 81.56 81.47 75.65 37.7 29.9 126.8 22.9\nTable 4: Ablation study results. The top group shows the effects of gradually adding different distilled knowledge at\npre-training stage. We take checkpoints at 10w training steps for evaluation. The bottom group presents ablation\nexperiments of pruning and knowledge distillation at fine-tuning stage.\nspite being more compact and efficient, demon-\nstrating the effectiveness of the proposed distilling\nthen pruning framework. Moreover, we find that\nEfficientVLM surpasses 98% performance of the\nteacher model on most datasets. In contrast, Distil-\nlVLM underperforms the 98% OSCARB baseline\nby a large margin. Actually, EfficientVLM recov-\ners 98.4% performance of X-VLMclip on average,\nwhile DistilVLM only retains 89.3% performance\nof OSCARB on average. This further confirms the\neffectiveness of our method.\n4.4.2 Ablation Study\nWe also conduct a series of ablation studies to better\nunderstand the effectiveness of EfficientVLM.\nImpact of Knowledge Distillation We first inves-\ntigate the impact of different distillation objectives\nby gradually adding logits distillation, hidden states\ndistillation, and attention distillation starting with\nX-VLMsmall. The results are shown in the top group\nof Table 4. We find that adding each component\nimproves the overall performance, demonstrating\nthe effectiveness of combing these components for\npre-train distillation.\nImpact of Fine-tuning Objectives We then study\nthe effect of modal-adaptive pruning and knowl-\nedge distillation in the fine-tuning stage. The re-\nsults are shown in Table 4. First, by comparing the\nresults of EfficientVLM and that in Table 1, we can\nsee that modal-adaptive pruning with learned spar-\nsity for encoders of each modality substantially out-\nperforms manually tuned sparsity. We also find that\nEfficientVLM performs similarly to the KD-only\nvariant. These results confirm the effectiveness of\nmodal-adaptive pruning. We also find that pruning\nwithout distillation results in worse results, demon-\nstrating the necessity of knowledge distillation dur-\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nPruned Percentage\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78\n80\n82\n84Performance (Accuracy)\nNLVR2\nMiniVLM\n95%X-VLM\nEfficientVLM\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nPruned Percentage\n110\n112\n114\n116\n118\n120\n122\n124\n126\n128Performance (CIDEr)\nCOCO-Captioning\nDistillVLM\n95%X-VLM\nEfficientVLM\nFigure 3: Results on NLVR2 and COCO Captioning\ntasks with different sparsity ranging from 10% to 80% .\ning fine-tuning. Finally, we can see that simply\nfine-tuning the compact task-agnostic pre-trained\nEfficientVLM performs not as well. However, it\nstill outperforms existing baselines by a huge mar-\ngin. This shows that EfficientVLM can also be\nused as a good compact task-agnostic VLM.\nImpact of Pruning Sparsity We also investigate\nthe performance of our modal-adaptive pruning\nmethods with different target sparsity ranging from\n10% to 80%. The results are shown in Figure 3.\nWe can see that EfficientVLM retains over 95%\nperformance of the teacher model with a sparsity\nof 50% and 40% on NLVR2 and COCO Caption-\ning, respectively. EfficientVLM also outperforms\nthe previous best results of compact VLMs with a\nsparsity up to 70% and 60% on these tasks. This\nshows EfficientVLM also performs well with larger\nsparsity.\n5 Conclusion\nWe introduce EfficientVLM, a fast and accurate\nvision-language model trained with a distilling\nthen pruning framework. Empirical results show\nthat EfficientVLM retains 98.4% performance of\nthe base-size teacher model while only preserving\n13906\n44.3% parameters and achieving a speed-up ratio\nof 2.2×. EfficientVLM also achieves a large ab-\nsolute improvement over previous efficient VLMs,\ndemonstrating a large potential towards lightweight\nVLMs.\nLimitations\nEfficientVLM is applied on X-VLM. However,\nthere are also many recent fully Transformer\nVLMs achieving comparable or better performance.\nTherefore, applying our distilling then pruning\nframework on other state-of-the-art VLMs can be\ninteresting. Also, we do not apply quantization or\nmatrix decomposition, which are prevalent model\ncompression techniques.\nEthics Statement\nOur method is used to compress VLMs. Therefore,\nethical considerations of VLMs generally apply to\nour method. We encourage users to assess potential\nbiases before deploying EfficientVLM.\nReferences\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. Spice: Semantic propositional\nimage caption evaluation. In European conference\non computer vision, pages 382–398. Springer.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\n2018. Bottom-up and top-down attention for image\ncaptioning and visual question answering. In 2018\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018, pages 6077–6086. IEEE Computer\nSociety.\nHangbo Bao, Wenhui Wang, Li Dong, Qiang Liu,\nOwais Khan Mohammed, Kriti Aggarwal, Sub-\nhojit Som, and Furu Wei. 2022. Vlmo: Uni-\nfied vision-language pre-training with mixture-of-\nmodality-experts.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun\nChen, and Jingjing Liu. 2020. Behind the scene: Re-\nvealing the secrets of pre-trained vision-and-language\nmodels. CoRR, abs/2005.07310.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. 2015. Microsoft coco captions:\nData collection and evaluation server. arXiv preprint\narXiv:1504.00325.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European conference on\ncomputer vision, pages 104–120. Springer.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nuniversal: Language specific translation evaluation\nfor any target language. In Proceedings of the ninth\nworkshop on statistical machine translation, pages\n376–380.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. North American Chapter of the Association for\nComputational Linguistics.\nCristóbal Eyzaguirre, Felipe del Río, Vladimir Araujo,\nand Álvaro Soto. 2021. Dact-bert: Differentiable\nadaptive computation time for an efficient bert infer-\nence. arXiv preprint arXiv:2109.11745.\nZhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan\nWang, Yezhou Yang, and Zicheng Liu. 2021. Com-\npressing visual-linguistic model via knowledge dis-\ntillation. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 1428–\n1438.\nJonathan Frankle and Michael Carbin. 2018. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. arXiv preprint arXiv:1803.03635.\nZhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen,\nYu Cheng, Shuohang Wang, Jingjing Liu, Lijuan\nWang, and Zicheng Liu. 2021. Playing lottery\ntickets with vision and language. arXiv preprint\narXiv:2104.11832, 2.\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu,\nYu Cheng, and Jingjing Liu. 2020. Large-scale adver-\nsarial training for vision-and-language representation\nlearning. In Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nMitchell A Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. arXiv preprint\narXiv:2002.08307.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904–6913.\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nDemi Guo, Alexander M Rush, and Yoon Kim. 2020.\nParameter-efficient transfer learning with diff prun-\ning. arXiv preprint arXiv:2012.07463.\n13907\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nZhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei\nLiu, Dongmei Fu, and Jianlong Fu. 2021. Seeing\nout of the box: End-to-end pre-training for vision-\nlanguage representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12976–12985.\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu,\nand Jianlong Fu. 2020. Pixel-bert: Aligning image\npixels with text by deep multi-modal transformers.\narXiv preprint arXiv:2004.00849.\nHuaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik\nLearned-Miller, and Xinlei Chen. 2020. In defense\nof grid features for visual question answering. InPro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10267–10276.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2019.\nTinybert: Distilling bert for natural language under-\nstanding. arXiv preprint arXiv:1909.10351.\nAndrej Karpathy and Fei-Fei Li. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA,\nJune 7-12, 2015, pages 3128–3137. IEEE Computer\nSociety.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\nVision-and-language transformer without convolu-\ntion or region supervision. In International Con-\nference on Machine Learning , pages 5583–5594.\nPMLR.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational journal of computer vision, 123(1):32–\n73.\nFrançois Lagunas, Ella Charlaix, Victor Sanh, and\nAlexander M Rush. 2021. Block pruning for faster\ntransformers. arXiv preprint arXiv:2109.04838.\nYann LeCun, John Denker, and Sara Solla. 1989. Opti-\nmal brain damage. Advances in neural information\nprocessing systems, 2.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 34, pages 11336–\n11344.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021a. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nAdvances in neural information processing systems,\n34:9694–9705.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\narXiv preprint arXiv:1908.03557.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao\nLiu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2021b.\nUNIMO: Towards unified-modal understanding and\ngeneration via cross-modal contrastive learning. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2592–\n2607, Online. Association for Computational Lin-\nguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020b. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision,\npages 121–137. Springer.\nYehao Li, Jiahao Fan, Yingwei Pan, Ting Yao, Weiyao\nLin, and Tao Mei. 2022. Uni-eden: Universal\nencoder-decoder network by multi-granular vision-\nlanguage pre-training. ACM Trans. Multim. Comput.\nCommun. Appl., 18(2):48:1–48:16.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,\nHaotang Deng, and Qi Ju. 2020. Fastbert: a self-\ndistilling bert with adaptive inference time. arXiv\npreprint arXiv:2004.02178.\nYongfei Liu, Chenfei Wu, Shao-yen Tseng, Vasudev\nLal, Xuming He, and Nan Duan. 2021. Kd-vlp: Im-\nproving end-to-end vision-and-language pretraining\nwith object knowledge distillation. arXiv preprint\narXiv:2109.10504.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nChristos Louizos, Max Welling, and Diederik P Kingma.\n2017. Learning sparse neural networks through l_0\nregularization. arXiv preprint arXiv:1712.01312.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In\nAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada, pages 13–23.\n13908\nChris J. Maddison, Andriy Mnih, and Yee Whye Teh.\n2017. The Concrete Distribution: A Continuous\nRelaxation of Discrete Random Variables. In Pro-\nceedings of ICLR.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems, 32.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. In Advances in Neural In-\nformation Processing Systems 24: 25th Annual Con-\nference on Neural Information Processing Systems\n2011. Proceedings of a meeting held 12-14 December\n2011, Granada, Spain, pages 1143–1151.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021a. Learn-\ning transferable visual models from natural language\nsupervision. CoRR, abs/2103.00020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021b. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research , pages 8748–8763.\nPMLR.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nShaoqing Ren, Kaiming He, Ross B. Girshick, and Jian\nSun. 2015. Faster R-CNN: towards real-time ob-\nject detection with region proposal networks. In Ad-\nvances in Neural Information Processing Systems 28:\nAnnual Conference on Neural Information Process-\ning Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, pages 91–99.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nVictor Sanh, Thomas Wolf, and Alexander Rush. 2020.\nMovement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems,\n33:20378–20389.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020a. Green ai. Communications of the\nACM, 63(12):54–63.\nRoy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A Smith.\n2020b. The right tool for the job: Matching\nmodel and instance complexities. arXiv preprint\narXiv:2004.07453.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565,\nMelbourne, Australia. Association for Computational\nLinguistics.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8815–8821.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2019. Vl-bert: Pre-training\nof generic visual-linguistic representations. arXiv\npreprint arXiv:1908.08530.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2018. A corpus for\nreasoning about natural language grounded in pho-\ntographs. arXiv preprint arXiv:1811.00491.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for Com-\nputational Linguistics.\nSurat Teerapittayanon, Bradley McDanel, and Hsiang-\nTsung Kung. 2016. Branchynet: Fast inference via\nearly exiting from deep neural networks. In 2016\n23rd International Conference on Pattern Recogni-\ntion (ICPR), pages 2464–2469. IEEE.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nJianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xi-\nujun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao,\nand Zicheng Liu. 2020a. Minivlm: A smaller\n13909\nand faster vision-language model. arXiv preprint\narXiv:2012.06946.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. 2022. Unifying architectures,\ntasks, and modalities through a simple sequence-\nto-sequence learning framework. arXiv preprint\narXiv:2202.03052.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020b. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776–5788.\nZekun Wang, Wenhui Wang, Haichao Zhu, Ming Liu,\nBing Qin, and Furu Wei. 2021. Distilled dual-\nencoder model for vision-language understanding.\narXiv preprint arXiv:2112.08723.\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. 2019.\nStructured pruning of large language models. arXiv\npreprint arXiv:1910.04732.\nMengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022.\nStructured pruning learns compact and accurate mod-\nels. arXiv preprint arXiv:2204.00408.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exit-\ning for accelerating bert inference. arXiv preprint\narXiv:2004.12993.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. Bert-of-theseus: Compressing\nbert by progressive module replacing. arXiv preprint\narXiv:2002.02925.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian\nMcAuley, and Furu Wei. 2021a. Beyond preserved\naccuracy: Evaluating loyalty and robustness of BERT\ncompression. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10653–10659, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian\nMcAuley, and Furu Wei. 2021b. Beyond preserved\naccuracy: Evaluating loyalty and robustness of bert\ncompression. arXiv preprint arXiv:2109.03228.\nHaiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Song-\nfang Huang, Wenming Xiao, and Fei Huang. 2021c.\nE2E-VLP: End-to-end vision-language pre-training\nenhanced by visual learning. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 503–513, Online. Asso-\nciation for Computational Linguistics.\nJingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou,\nand Lei Li. 2021d. A survey on green deep learning.\narXiv preprint arXiv:2111.05193.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine\nLearning and Cognitive Computing-NeurIPS Edition\n(EMC2-NIPS), pages 36–39. IEEE.\nYan Zeng, Xinsong Zhang, and Hang Li. 2021.\nMulti-grained vision language pre-training: Align-\ning texts with visual concepts. arXiv preprint\narXiv:2111.08276.\nYan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang,\nJipeng Zhang, and Wangchunshu Zhou. 2022. X 2-\nvlm: All-in-one pre-trained model for vision-\nlanguage tasks.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\ntions in vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5579–5588.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cot-\nterell, and Mrinmaya Sachan. 2023. Efficient prompt-\ning via dynamic in-context learning.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses\npatience: Fast and robust inference with early exit.\nAdvances in Neural Information Processing Systems,\n33:18330–18341.\nWangchunshu Zhou, Canwen Xu, and Julian McAuley.\n2022a. Bert learns to teach: Knowledge distillation\nwith meta learning. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7037–\n7049.\nWangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xin-\nsong Zhang. 2022b. VLUE: A multi-task multi-\ndimension benchmark for evaluating vision-language\npre-training. In Proceedings of the 39th Interna-\ntional Conference on Machine Learning, volume 162\nof Proceedings of Machine Learning Research, pages\n27395–27411. PMLR.\n13910\nA Differentiable L0-Norm Regularization\nThe formulation of Equation 5 is still hard for\ngradient-based optimization by the discrete nature\nof masks, but the expectation provides some guid-\nance for empirically effective relaxations. Follow-\ning prior work (Louizos et al., 2017; Wang et al.,\n2019; Guo et al., 2020), we apply Hard-Concrete\ndistribution (Maddison et al., 2017) to relax z into\ncontinuous space [0,1]d. Specifically, z is now de-\nfined to be a deterministic and (sub)differentiable\nfunction of a sample u from a uniform distribution,\nu ∼U(0,1)\ns = sigmoid(log u −log(1 −u) + α)\n¯s = s ×(r−l) + l\nz = min(1,max(0,¯s))\nHere l <0 and r >1 are two constants used to\nstretch s into the interval(l,r)d before it is clamped\nto [0,1]d with the min(1,max(0,·)) operation. In\nthis case we have a differentiable closed-form ex-\npression for the expected L0-norm,\nE\n[\n∥˜θ∥0\n]\n=\nn∑\nj=1\nE [zj >0]\n=\nn∑\nj=1\nsigmoid\n(\nαj −log −l\nr\n)\n(7)\nTo better control the expected sparsity of the\nstudent model, we follow Wang et al. (2019) to\nreplace the vanilla l0 objective with a Lagrangian\nmultiplier. Let tbe the target model size and s(α)\nbe the constrained model size determined by the\nHard Concrete parameter α.\nThe Lagrangian method imposes an equality con-\nstraint s(α) = tby introducing a violation penalty,\nLLgr = λ1 ·(s(α) −t) + λ2 ·(s(α) −t)2\nwhere λ1,λ2 ∈R are two Lagrangian multipliers\nthat will be jointly updated during training.\nB Pre-train Datasets\nDataset # Images # Captions # Ann\nCOCO 0.11M 0.55M 0.45M\nVG 0.10M - 5.7M\nSBU 0.86M 0.86M -\nCC-3M 2.9M 2.9M -\nTable 5: Statistics of the pre-training datasets.\nC Hyperparameters\nThe hyperparameters to reproduce fine-tuning re-\nsults are in Table 6. Tasks with ∗need two-stage\nfine-tuning.\nTasks Learning Rate Batch Size Epoch\nITR-COCO 3e-5 384 10\nNLVR∗ 3e-5 80 10\nCaptioning∗ 1e-5 256 5\nVQA 5e-5 192 10\nTable 6: Hyper-parameters for fine-tuning on down-\nstream tasks.\n13911\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n6\n□\u0017 A2. Did you discuss any potential risks of your work?\nAs far as we know, our proposed method doesn’t have any risks.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n0,1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n2, 3, 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n2,3,4\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe artifacts we used are publicly available. Researchers and engineers commonly use them.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n3\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe used the data following previous works.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nThe data we used are publicly available. We used the data following previous works.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n13912\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n13913",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8108726739883423
    },
    {
      "name": "Pruning",
      "score": 0.6568018794059753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5599255561828613
    },
    {
      "name": "Language model",
      "score": 0.544257402420044
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5398288369178772
    },
    {
      "name": "Inference",
      "score": 0.46788281202316284
    },
    {
      "name": "Machine vision",
      "score": 0.4564206600189209
    },
    {
      "name": "Distillation",
      "score": 0.44624537229537964
    },
    {
      "name": "Modal",
      "score": 0.4383507966995239
    },
    {
      "name": "Machine learning",
      "score": 0.4064592719078064
    },
    {
      "name": "Natural language processing",
      "score": 0.3547181189060211
    },
    {
      "name": "Computer vision",
      "score": 0.3477655053138733
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    }
  ]
}