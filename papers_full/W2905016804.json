{
    "title": "Gaussian Transformer: A Lightweight Approach for Natural Language Inference",
    "url": "https://openalex.org/W2905016804",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2163555110",
            "name": "Maosheng Guo",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2023856972",
            "name": "Yu Zhang",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098738246",
            "name": "Ting Liu",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2163555110",
            "name": "Maosheng Guo",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2023856972",
            "name": "Yu Zhang",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098738246",
            "name": "Ting Liu",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6713134421",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2154474435",
        "https://openalex.org/W2790415926",
        "https://openalex.org/W2798524681",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W6681534285",
        "https://openalex.org/W2808308446",
        "https://openalex.org/W2607892599",
        "https://openalex.org/W2782363479",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W2612867916",
        "https://openalex.org/W2081580037",
        "https://openalex.org/W2593833795",
        "https://openalex.org/W2144614323",
        "https://openalex.org/W4302343710",
        "https://openalex.org/W2964189376",
        "https://openalex.org/W2756386045",
        "https://openalex.org/W4295253143",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2964082993",
        "https://openalex.org/W2798459010",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2523467643",
        "https://openalex.org/W4211148418",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2963077723",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2773734397",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W2798858969",
        "https://openalex.org/W4297747548",
        "https://openalex.org/W2953384591",
        "https://openalex.org/W4212904132",
        "https://openalex.org/W4386506836",
        "https://openalex.org/W2962736243",
        "https://openalex.org/W2768523431",
        "https://openalex.org/W2962739339"
    ],
    "abstract": "Natural Language Inference (NLI) is an active research area, where numerous approaches based on recurrent neural networks (RNNs), convolutional neural networks (CNNs), and self-attention networks (SANs) has been proposed. Although obtaining impressive performance, previous recurrent approaches are hard to train in parallel; convolutional models tend to cost more parameters, while self-attention networks are not good at capturing local dependency of texts. To address this problem, we introduce a Gaussian prior to selfattention mechanism, for better modeling the local structure of sentences. Then we propose an efficient RNN/CNN-free architecture named Gaussian Transformer for NLI, which consists of encoding blocks modeling both local and global dependency, high-order interaction blocks collecting the evidence of multi-step inference, and a lightweight comparison block saving lots of parameters. Experiments show that our model achieves new state-of-the-art performance on both SNLI and MultiNLI benchmarks with significantly fewer parameters and considerably less training time. Besides, evaluation using the Hard NLI datasets demonstrates that our approach is less affected by the undesirable annotation artifacts.",
    "full_text": "The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)\nGaussian Transformer: A Lightweight Approach for Natural Language Inference\nMaosheng Guo, Yu Zhang, Ting Liu∗\nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology, China\n{msguo, yzhang, tliu}@ir.hit.edu.cn\nAbstract\nNatural Language Inference (NLI) is an active research area,\nwhere numerous approaches based on recurrent neural net-\nworks (RNNs), convolutional neural networks (CNNs), and\nself-attention networks (SANs) has been proposed. Although\nobtaining impressive performance, previous recurrent ap-\nproaches are hard to train in parallel; convolutional models\ntend to cost more parameters, while self-attention networks\nare not good at capturing local dependency of texts. To ad-\ndress this problem, we introduce a Gaussian prior to self-\nattention mechanism, for better modeling the local structure\nof sentences. Then we propose an efﬁcient RNN/CNN-free\narchitecture named Gaussian Transformer for NLI, which\nconsists of encoding blocks modeling both local and global\ndependency, high-order interaction blocks collecting the ev-\nidence of multi-step inference, and a lightweight compari-\nson block saving lots of parameters. Experiments show that\nour model achieves new state-of-the-art performance on both\nSNLI and MultiNLI benchmarks with signiﬁcantly fewer pa-\nrameters and considerably less training time. Besides, evalu-\nation using the Hard NLI datasets demonstrates that our ap-\nproach is less affected by the undesirable annotation artifacts.\nIntroduction\nNatural Language Inference (NLI), also known as Recog-\nnizing Textual Entailment (RTE), is a fundamental prob-\nlem in the research ﬁeld of natural language understand-\ning, which could help tasks like questions answering, read-\ning comprehension and summarization (Dagan et al. 2013).\nIn NLI settings, the model is presented with a pair of sen-\ntences, namely premise and hypothesis, and asked to deter-\nmine the reasoning relationship between them from a set\nincluding entailment, contradiction and neutral. Numerous\nefforts have been dedicated to this task, where the dominant\ntrend is to build complex neural models using millions of pa-\nrameters which cost lots of time to train. Although achieved\nimpressive accuracy, previous works using recurrent models\nare hard to train in parallel while convolutional models often\nneed more parameters to learn.\nTo address this problem, we propose an efﬁcient\nRNN/CNN-free architecture named Gaussian Transformer,\n∗Corresponding Author\nCopyright c⃝2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n(a)\n0.0\n0.1\n0.2\n0.0\n0.1\n0.2\n(b)\n0.0\n0.1\n0.2\n-4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9\n(c)\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.0\n0.1\n0.2\nFigure 1: Probabilities of each token attending to current\ncentral word ‘book’: (a) illustrates the vanilla self-attention,\nwhere the word ‘new’ appeared in different positions obtain\nthe same importance, which is inconsistent with our expe-\nrience that adjacent words matters; (b) depicts a Gaussian\ndistribution over distance (x-axis) that encourages focusing\non neighboring tokens; (c) draws the attention corrected by\nthe Gaussian prior, where the ﬁrst ‘new’ is more important.\nwhich is adapted from the Transformer model in machine\ntranslation tasks(Vaswani et al. 2017). The original Trans-\nformer is based on the self-attention mechanism, eschewing\nthe disadvantages of recurrence and convolution. However,\nit cannot capture local structure of texts, which is enhanced\nwith a Gaussian prior probability in our approach accord-\ning to the following observation: adjacent words contribute\nmore semantically to current phrase than distant ones. For\nexample, in the sentence ‘I bought a new book yesterday\nwith a new friend in New York.’, the ﬁrst ‘new’ is more im-\nportant to the word ‘book’ than the other two. Indeed, RNNs\ntend to forget distant inputs, while CNNs exclude words out-\nside the current convolutional window. This ‘chunking’ phe-\nnomenon is naturally modeled in RNNs and CNNs but is ne-\nglected in the original Transformer. As shown in Figure 1(a),\n6489\nsame words at various distances are treated almost equally\nwhen computing self-attention. We correct this problem us-\ning a Gaussian prior probability over the distance to the\ncentral word, as depicted in Figure 1(b)&(c). Although po-\nsitional encoding might help to alleviate this problem, ex-\nperiments show that the Gaussian prior works better. Mean-\nwhile, long-term dependency is also taken into consideration\nby using multiple layers of Gaussian attention in our model.\nBesides using Gaussian self-attention networks to model\nsentences, we also introduce the high-order interaction\nblocks to collect evidence of multi-step inference and an efﬁ-\ncient comparison mechanism for NLI tasks, helping us to es-\ntablish a new state-of-the-art performance using one order of\nmagnitude fewer parameters. Thanks to the CNN/RNN-free\narchitecture, our model is also faster than previous works.\nRecently, annotation artifacts were found in existing\ndatasets, i.e., SNLI (Bowman et al. 2015) and MultiNLI\n(Williams, Nangia, and Bowman 2017), and the more chal-\nlenging Hard NLI benchmark was proposed (Gururangan et\nal. 2018). Our model outperforms previous works by about\n5 percent accuracy on this benchmark, showing that the pro-\nposed Gaussian Transformer is less affected by the undesir-\nable annotation artifacts.\nThe contributions of this paper are listed as follows:\n• We propose the novel Gaussian self-attention inspired by\nthe ‘chunking’ phenomenon, which could better capture\nboth local structure and global dependency of sequences\nwithout introducing recurrence or convolution.\n• We propose an efﬁcient NLI model, i.e, Gaussian Trans-\nformer, consisting of Gaussian encoding blocks, high-\norder interaction blocks and efﬁcient comparison blocks,\nobtaining new state-of-the-art performance with signiﬁ-\ncant fewer parameters and considerably less training time.\nGaussian Self-attention\nAs shown in Figure 1, the original transformer treats the\nsame words at various distances almost equally 1, which is\ninconsistent with our experience of natural language texts\nthat adjacent words contribute more semantically to central\nwords. While CNNs / RNNs model this ‘chunking’ phe-\nnomenon internally, the vanilla self-attention mechanism in\nTransformer could not capture the local structure of texts.\nSupposing that xi represents the central word from the\nsentence x, the vanilla dot-product self-attention works as\nFigure 2(a): It soft-aligns each token xj from x to xi, ac-\ncording to the compatibility function computed by the soft-\nmax of dot products, i.e., Compi,j = Softmaxj(xi ·xj),\nand then sums the attended values together, i.e.,\n˜xi =\n∑\nj\nCompi,jxj. (1)\nFor better modeling the local structure of texts, we in-\ncrease the importance of neighboring tokens and decay the\nweight of distant ones. We hypothesize that the semantic\n1For simplicity, we omit the positional encoding here and dis-\ncuss it later.\n(a)\nMatMul\nSoftmax\n  \nMatMul\n \n \nMatMul\nSoftmax\n  \nMatMul\n \n \n(a)\nMatMul\nSoftmax\n  \nMatMul\n \n \nMatMul\nMultiply & Norm\nGaussian\n Prior\n \nMatMul\nSoftmax\n  \n \nMatMul\nMultiply & Norm\nGaussian\n Prior\n \nMatMul\nSoftmax\n  \n \n(b)\nMatMul\nMultiply & Norm\nGaussian\n Prior\n \nMatMul\nSoftmax\n  \n \n(b) (c)\nMatMul\nSoftmax\n  \nMatMul\n \n \nGaussian\n Bias\nMatMul\nSoftmax\n  \nMatMul\n \n \nGaussian\n Bias\nFigure 2: Illustration of attentions: (a) depicts the original\ndot-product attention of Transformer, (b) & (c) are Gaussian\nprior extended self-attention.\n(a)           \n(b)                 \nFigure 3: Examples of Gaussian multiplier: (a) illustrates the\nvanilla version; (b) shows the variant introduced in Eq. (4).\ncontribution to central word from tokens at different dis-\ntance obey a normal distribution, and then use a variant of\nGaussian prior to correct the importance of tokens align-\ning to the current central word. The reason to choose nor-\nmal distribution is that it is hard to statistically measure the\nsemantic importance of a word according to another one,\nand comparison experiments with different decaying mech-\nanisms, e.g., linear decaying according to distance (Im and\nCho 2017), and other distributions such as Zipf Distribution,\ndemonstrate that the Gaussian assumption works better.\nFor simplicity, we use the (Stigler 1982) deﬁnition of a\nstandard normal distribution with variance σ2 = 1 /(2π),\nwhose probability density function is: φ(d) = e−πd2\n, where\ndis the random variable, i.e., the distance between tokens.\nThen, we insert φ(di,j) to Eq. (1) just like Figure 2(b) to\ncorrect the importance of tokens at various distances:\n˜xi =\n∑\nj\nφ(di,j)compi,j\nZ1\nxj =\n∑\nj\ne−d2\ni,j ·e(xi·xj)\nZ2\nxj\n=\n∑\nj\ne−d2\ni,j+(xi·xj)\nZ2\nxj\n=\n∑\nj\nSoftmax(−d2\ni,j + (xi ·xj))xj,\n(2)\nwhere Z1 = ∑\nkφ(di,k)compi,k, Z2 = ∑\nke−d2\ni,k+(xi·xk)\nare normalization factors. Eq. (2) converts the normal dis-\ntribution to a Gaussian bias term (Figure 2(c)), which saves\n6490\nadditional operations of multiplication. Since the Gaussian\nvariance (σ2) might not incidentally equal 1/(2π), we intro-\nduce a scalar variable wto Eq. (2) to loose the restriction:\n˜xi =\n∑\nj\nSoftmax(−wd2\ni,j + (xi ·xj))xj. (3)\nInstead of applying the vanilla Gaussian prior, we found it\nbeneﬁcial to introduce a punishment term bto the weight of\nthe central word attending itself, as depicted in Figure 3(b):\n˜xi =\n∑\nj\nSoftmax(−|wd2\ni,j + b|+ (xi ·xj))xj, (4)\nwhere |·| represents the absolute value, w > 0, b ≤ 0\nare scalar parameters. This policy is inspired by (Shen et al.\n2017), which disables the attention to itself, while we only\nreduce the weight instead to keep information of the central\nword which is also important to current chunk.\nIt is worth noting that, besides the local structures, long-\nrange dependency also matters to the semantics of sentences.\nRNNs, e.g., LSTMs, use gates and memory to capture the\nlong-term dependency, while CNNs build deep models with\nmultiple convolution layers for this phenomenon. In our\ncase, we choose the CNNs-style, that is, stacking Gaussian\nattention layers to track global dependency.\nGaussian Transformer\nIn this section, we provide a detailed description of\nthe Gaussian Transformer. The input is a pair of sen-\ntences {pi}lp\ni=1 / {hj}lh\nj=1, where pi / hj ∈ RV is\nthe one-hot vector representing i-th / j-th word of the\npremise / hypothesis with length lp / lh, and V is\nthe vocabulary size. The goal is to predict labels from\n{entailment,contradiction,neutral }. Figure 4 presents\nan overview of the architecture, including the following\ncomponents:\nEmbedding Blocks\nThe embedding blocks convert each word of a sentence to\na dense vector and combine them to construct a matrix rep-\nresentation. We employ pre-trained word vectors, random\ninitialized character n-grams embeddings and positional en-\ncoding to project sentences to high-dimensional space.\nBesides commonly used pre-trained word vectors, we uti-\nlize a lightweight character-level representation of tokens.\nUnlike using trainable character embeddings and additional\nCNNs/LSTMs in previous work, we choose static random\ninitialized character n-gram embeddings to save parameters,\nand then represent each token as the max-over-time pooling\nof its embedded character n-grams. This allows us to han-\ndle typos and out-of-vocabulary words at minimum compu-\ntational cost without any additional parameters. The sinu-\nsoidal positional encoding is used to exploit the information\nabout the order of sequences since Transformer contains no\nrecurrence and convolution (Vaswani et al. 2017).\nAll the three embeddings remain ﬁxed during training.\nThe only trainable parameter here is the projection matrix\nthat reduces the dimension of token representation todmodel,\nFeed Forward\nAdd & Norm\nFeed Forward\nAdd & Norm\nSelf-attention\nAdd & Norm\nGaussian Prior\nSelf-attention\nAdd & Norm\nGaussian Prior\nWord & Char \nEmbeddings\nFeed Forward\nWord & Char \nEmbeddings\nFeed Forward\nFeed Forward\nAdd & Norm\nFeed Forward\nAdd & Norm\nSelf-attention\nAdd & Norm\nGaussian Prior\nSelf-attention\nAdd & Norm\nGaussian Prior\nInter-attention\nAdd & Norm\nInter-attention\nAdd & Norm\nFeed Forward\nScaled Sum\nConcatenate\nFeed Forward\nScaled Sum\nConcatenate\nFeed Forward\nSoftmax\nConcatenate\nFeed Forward\nSoftmax\nConcatenate\nPositional\nEncoding\nFeed Forward\nAdd & Norm\nFeed Forward\nAdd & Norm\nSelf-attention\nAdd & Norm\nGaussian Prior\nSelf-attention\nAdd & Norm\nGaussian Prior\nWord & Char \nEmbeddings\nFeed Forward\nWord & Char \nEmbeddings\nFeed Forward\nFeed Forward\nAdd & Norm\nFeed Forward\nAdd & Norm\nSelf-attention\nAdd & Norm\nGaussian Prior\nSelf-attention\nAdd & Norm\nGaussian Prior\nInter-attention\nAdd & Norm\nInter-attention\nAdd & Norm\nFeed Forward\nScaled Sum\nConcatenate\nFeed Forward\nScaled Sum\nConcatenate\nPositional\nEncoding\nPremise Hypothesis\nOutput\nProbabilities\nEmbedding\nBlock\nM × \nEncoding\nBlock\nN × \nInteraction\nBlock\nComparison \nBlock\nEmbedding\nBlock\nM × \nEncoding\nBlock\nN × \nInteraction\nBlock\nComparison \nBlock\nFigure 4: The overall architecture of Gaussian Transformer.\nwhich is the dimension number used internally in the rest of\nmodel.\nThe process of the embedding blocks could be formally\ndescribed as follows:\nx(w)\ni = xiEw, (5)\nx(c)\ni = Maxt({x(c)\ni,tEc}\nlxi\nt=1), (6)\nx(p)\ni,2k = sin(i/100002k/dmodel), (7)\nx(p)\ni,2k+1 = cos(i/100002k/dmodel), (8)\nx(e)\ni = x(p)\ni + [x(w)\ni : x(c)\ni ]We, (9)\nwhere x is a placeholder of sentence p or h, xi ∈RV is\nthe one-hot representation of i-th word of x, which consists\nof lxi character n-grams, x(c)\ni,t ∈RVc indicates the t-th one,\nx(w), x(c), x(p) and x(e) are the word-level representation,\ncharacter-level representation, positional encoding, and the\nﬁnal representation of token xi in this block respectively,\nEw ∈RV×dw is initialized from pre-trained word vectors,\nEc ∈RVc×dc is a randomly initialized character n-gram em-\nbeddings, 2k,2k+1 ∈[0,dmodel) indicates the correspond-\n6491\ning dimension, [·: ·] represents the operation of concate-\nnation, V is the vocabulary size of tokens, Vc is the vocab-\nulary size of character n-grams, We ∈R(dw+dc)×dmodel is\nthe trainable projection matrix of encodings.\nEncoding Blocks\nIn this part, we employ a stack ofMencoding blocks to cap-\nture the local and global dependency of words in sentences,\nwhich is similar with the encoder of the original Transformer\nexcept that we introduce the Gaussian prior probability ac-\ncording to the ‘chunking’ phenomenon.\nEach encoding block consists of two sub-layers: a multi-\nhead Gaussian self-attention sub-layer and a position-wise\nfeedforward layer. Sub-layers are stacked using residual\nconnections and layer normalizations (Ba, Kiros, and Hin-\nton 2016), so that the output of a sub-layer becomes y =\nLayerNorm(x + Sublayer(x)), where x,y respectively\nrepresents the input and output, Sublayer is either the H-\nhead Gaussian self-attention or position-wise feedforward\nnetworks.\nThe multi-head attention mechanism projects representa-\ntions into H sub-spaces of dimension dh = dmodel/H and\ncomputes attention in parallel. Then a position-wise feed-\nforward network (FFN) is applied to the output of attention\nsub-layer at each position separately and identically:\nFFN (x) = Dense2(Relu(Dense1(x))), (10)\nDensek(x) = xWk + bk, k∈{1,2}, (11)\nRelu(x) = max(0,x), (12)\nwhere Wk ∈Rdmodel×dmodel, bk ∈Rdmodel.\nIn CNNs, the concept of receptive ﬁeld is deﬁned as the\nregion in the input space that affects a particular CNN’s fea-\nture, because inputs outside the ﬁlter window are excluded.\nSimilarly, we could informally deﬁne the concept of ‘focus\nﬁeld’ of Gaussian attention since most values drawn from\na normal distribution are within one or two standard devi-\nations (σ) away from the mean, so that the Gaussian atten-\ntion mainly focuses on that ﬁeld, although σis not explicitly\nprovided. Like multilayer CNNs, the bottom Gaussian atten-\ntion captures the local structure while higher-level attentions\ntrack the ‘global’ dependency. This is why we stack M en-\ncoding blocks. Thanks to residual connections, both local\nand global information could easily ﬂow to upper modules.\nInteraction Blocks\nUnlike encoding blocks extract semantics from a single se-\nquence, the interaction blocks take the alignment between\nsentences into account. Besides Gaussian self-attentions to\nmodel current sentence and the position-wise feedforward\nnets to perform non-linear projection, the interaction block\ninserts a multi-head inter-attention sub-layer between them\nto align the sentences and collect the evidence of inference.\nThe inter-attention works similarly with the self-attention.\nSupposing that vector pi represents the current central\nword from the premise p, while qj indicates the j-th to-\nken of the hypothesis, the (single-head) dot-product inter-\nattention between pi and h soft-aligns each token hj from\nh to pi, according to a compatibility function computed\nby the softmax of dot products, i.e., Comp(pi,hj) =\nSoftmaxj(pi ·hj), and then sums the attended values\ntogether, i.e., ˜pi = ∑\njComp(pi,hj)hj. Similarly, we\nhave ˜hj = ∑\niComp(hj,pi)pi, where Comp(hj,pi) =\nSoftmaxi(hj ·pi).\nThe attended ˜pi, as a representation of pi using its soft-\naligned tokens in h, contains information of the relationship\nbetween the central tokenpiand its most compatible phrases\nfrom h, which could be viewed as the evidence of local in-\nference (Parikh et al. 2016), i.e., a single-step inference.\n(Liu, Duh, and Gao 2018) point out that some sentence\npairs needs more than one-step inference to determine their\nreasoning relationship, and took the average of multiple soft-\nmax layers as their prediction. In this work, we employ a\nstack of N interaction blocks to perform high-order inter-\nattention, i.e., attention over attention, to collect evidence\nof multi-step inference. Thanks to the residual connections\nbetween sub-layers, evidence of both single-step alignment\nand multi-step inference could easily ﬂow to upper modules.\nSince the rest parts of interaction blocks, e.g., multi-head\nGaussian self-attention, are already introduced in previous\nsections, we will not repeat them here.\nComparison Block\nThe comparison block consists of aggregation layers and\nprediction layers. The aggregation layers combine the out-\nput of encoding and interaction blocks, aggregate the\nalignments of words / phrases and then compares the\ndifference between the two sentences to form a ﬁxed-\nlength feature vector for classiﬁcation. The prediction\nlayer projects that feature vector to the target space, i.e.,\n{entailment,contradiction,neutral }, and predicts the\nprobability distribution over them.\nAs depicted in Figure 4, the comparison block uses a\nSiamese architecture – two identical parameter-tied aggre-\ngation networks are applied to premise and hypothesis re-\nspectively, and then a multilayer perceptron classiﬁer is em-\nployed to determine the relationship between them:\nAggregation Networks The encoder blocks build a se-\nmantic representation of sentence x ∈ {p,h}, denoted as\n{xi}lx\ni=1, according to the local and global dependency of\ntokens modeled by multilayer Gaussian self-attention; the\ninteraction blocks, on the other hand, output the cross-\nsentence information, denoted as {˜xi}lx\ni=1 , according to\nhigh-order alignments and multi-step inferences collected\nby the stack of inter-attention. The aggregation networks\ncollect these information to extract the feature-space repre-\nsentation ¯xas follows:\nvi = Dense4(Relu(Dense3([xi : ˜xi]))), (13)\nDensek(x) = xWk + bk, k∈{3,4}, (14)\n¯x= 1√lx\nlx∑\ni=1\n(vi), (15)\nwhere W3 ∈R2dmodel×dmodel, W4 ∈Rdmodel×dmodel, and\nb3,b4,xi,˜xi,¯x∈Rdmodel.\n6492\nIt ﬁrst concatenates token xi and its attentive alignments\n˜xi, and then applies position-wise feedforward networks to\ncompare the aligned phrases, and ﬁnally aggregates the com-\nparison vectors vi by summation. We use the reciprocal\nsquare root of sentence length to alleviate the problem that\nlonger sequences have larger features vectors.\nWe also explored other aggregation mechanisms. For ex-\nample, max-pooling and average-pooling over the concate-\nnation [xi : ˜xi : xi−˜xi : xi⊙˜xi] in (Chen et al. 2017b) could\nnot further improve over our model but almost consumes as\n4 times parameters as our aggregation network.\nPrediction Layers We employ the vanilla MLP classiﬁer\nto predict the ﬁnal label of sentence pairs as follows:\ny= Softmax(Dense6(Relu(Dense5([¯p: ¯h])))), (16)\nDensek(x) = xWk + bk, k∈{5,6}, (17)\nwhere W5 ∈ R2dmodel×dmodel, W6 ∈ Rdmodel×3, b5 ∈\nRdmodel, b6 ∈R3.\nWe also tried more complex prediction layers, such as re-\nplacing [¯p: ¯h] with [¯p: ¯h: ¯p+ ¯h: ¯p−¯h] (Kim et al. 2018),\nand found it makes no signiﬁcant difference but costs more\nadditional parameters. According to the law of Occam’s ra-\nzor, we choose the simple comparison block as above.\nExperiments and Analyses\nWe conduct experiments on SNLI and MultiNLI datasets,\nwhich consists of 570k / 433k English sentence pairs, to train\nand evaluate the proposed model. We ﬁrst use the MultiNLI\nvalidation datasets to ﬁnd the optimal settings of the pro-\nposed Gaussian Transformer and conduct ablation test to\nverify the effectiveness of each component of our model.\nThen we report the performance on both the vanilla and hard\nversion of SNLI / MultiNLI, of models trained from scratch\nor by leveraging external resources, followed by the details\nof our implementation. Besides the accuracy of classiﬁca-\ntion, we also compare the number of parameters, training\ntime and inference time with previous work.\nEffectiveness of Each Component\nM\nN\nM\nN\nMatched Misatched\nM\nN\nM\nN\nFigure 5: Heatmap of accuracy (%) on MultiNLI validation\nsets. Multiple encoding / interaction blocks ( M >1 / N >\n1) performing better demonstrates the usefulness of global\ndependency and multi-step inference.\nGlobal Dependency and Multi-step InferenceFigure 5\nshows the accuracy on MultiNLI validation sets w.r.t. the\nvarious combination of numbers of encoding (M) and inter-\naction (N) blocks, i.e., the depth of model, where the label\n‘matched’ indicates the accuracy of in-domain data, while\n‘mismatched’ represents out-domain. Performance becom-\ning better when increasing M and N initially and peaking\nat M = 3 and N = 2, demonstrates the effectiveness of the\nglobal dependency, which is captured by multiple Gaussian\nself-attentions (M > 1), and the superiority of multi-step\ninference modeled by high-order interactions (N >1).\nAlthough continuing deepening the network does not re-\nsult in a signiﬁcant accuracy regression, we keep the values\n(3,2) in the rest of experiments, since the training time rises\nlinearly as the depth increases.\nModel Matched Mismatched\nVanilla Transformer (i) 79.3 79.3\n+ Zipf prior (ii) 79.7 79.5\n+ trainable distance bias (iii) 79.5 79.6\n+ linear decay bias (iv) 79.9 79.8\n+ Conv. layers (v) 80.1 80.0\n+ Gaussian prior (vi) 80.2 80.3\n+ Gaussian prior variant (vii) 80.3 80.5\nTable 1: Accuracy (%) of different variants of Gaussian\nTransformer on the MultiNLI development datasets.\nEffectiveness of Gaussian Assumption Since the sinu-\nsoidal positional encoding in the original Transformer could\nalready represent the relative position of tokens, it is neces-\nsary to evaluate the usefulness of the Gaussian assumption,\nwhich encourages self-attention to focus on locally close to-\nkens. We evaluate several models with different prior prob-\nabilities and present the results in Table 1. We could ob-\nserve that adding Gaussian prior to the vanilla Transformer\nimproves about 1 percent accuracy on MultiNLI validation\ndatasets, and using a punishment bias to reduce the impor-\ntance of central word attending to itself brings additional\nenhancement, where (vi) and (vii) indicate the model using\nEq. (3) and (4) respectively. Next, we found that depthwise\nseparable convolutions (v) which is used in QANet (Yu et\nal. 2018) is competitive with Gaussian prior but extra pa-\nrameters are introduced by convolutions. Besides Gaussian\nprior, we also evaluate the Zipf distribution (ii), since many\nlinguistics statistics, such as word / phrase frequency, obey\nthe famous Zipf’s Law. To follow the Zipf’s law, we assume\nthe semantic contribution of a token to current phrase is in-\nversely proportional to its distance to the central word, and\nuse Eq. (18) to compute self-attention:\n˜xi =\n∑\nj\n1\n|di,j|+ 1\ncompi,j\nZz\nxj; (18)\nmodel (iii) uses trainable distance bias term bdi,j to model\nthe relative distance (Parikh et al. 2016) :\nCompi,j = Softmaxj(xi ·xj + bdi,j ); (19)\n6493\nwhile model (iv) adds a linear function of the distance to the\ncompatibility function (Im and Cho 2017):\nCompi,j = Softmaxj(xi ·xj + wl|di,j|), (20)\nwhere wl(< 0) and bdi,j are scalar parameters, Zz =∑\nkcompi,k/(|di,k|+ 1) is the normalization factor.\nIn summary, it is necessary to encourage self-attention to\nfocus on local structures (model ii - vii), and the proposed\nGaussian prior (variant) performs the best.\nExperimental Results\nIn this section, we present the experimental results of Gaus-\nsian Transformer comparing with not only models trained\nfrom scratch but also approaches leveraging external re-\nsources. Then, we evaluate our model on the more challeng-\ning Hard NLI benchmarks.\nComparison with Models Training from Scratch We\nﬁrst evaluate the capability of the Gaussian Transformer it-\nself to resolve NLI problems, by training our model from\nscratch, i.e., using only NLI training sets without any exter-\nnal resources except word embeddings, and report the accu-\nracy on SNLI and MultiNLI, in Table 2 and 3 respectively.\nIt can be observed that Gaussian Transformer achieves\nstate-of-the-art performance on both SNLI and MultiNLI\nbenchmarks, improving 1 percent accuracy at most. How-\never, our Gaussian Transformer only costs 665k parame-\nters, which is one order of magnitude fewer than previ-\nous approaches. Among them, ESIM(Chen et al. 2017b),\nBiMPM(Wang, Hamza, and Florian 2017), MwAN(Tan\net al. 2018), SAN(Liu, Duh, and Gao 2018), DR-\nBiLSTM(Ghaeini et al. 2018), CAFE(Tay, Tuan, and Hui\n2017), DRCN(Kim et al. 2018) are recurrent models, while\nDIIN(Gong, Luo, and Zhang 2017) is a convolutional model\nand DecAtt(Parikh et al. 2016) is a self-attention network.\nModel |θ| Acc.(S./E.)\n200D DecAtt 580k 86.8 / -\nBiMPM 1.6m 87.5 / 88.8\n600D ESIM 4.3m 88.0 / 88.6\n448D DIIN 4.4m 88.0 / 88.9\n150D MwAN 14m 88.3 / 89.4\nSAN 3.5m 88.5 / -\n450D DR-BiLSTM 7.5m 88.5 / 89.3\n300D CAFE 4.7m 88.5 / 89.3\nDCRN 6.7m 88.9 / 90.1\n120D Gaussian Transformer 665k 89.2 / 90.3\nTable 2: Accuracy (%) on SNLI test set of approaches train-\ning from scratch by single / ensemble models, where|θ|indi-\ncates the number of parameters. Gaussian Transformer out-\nperforms previous state-of-the-arts with one order of magni-\ntude fewer parameters.\nIn addition, we compare the time cost of Gaussian Trans-\nformer with that of ESIM, a classic recurrent NLI model\nwith publicly available codes, on the same hardware. We\nevaluate them by two metrics: the time to train one epoch\non the SNLI training data and the inference time to iterate\nModel Matched Mismatched\nESIM 76.8 / - 75.8 / -\nMwAN 78.5 / 79.8 77.7 / 79.4\nDIIN 78.8 / 80.0 77.8 / 78.7\nCAFE 78.7 / 80.2 77.9 / 79.0\nDCRN 79.1 / 80.6 78.4 / 79.5\nSAN 79.3 / 80.6 78.7 / 80.1\nGaussian Transformer 80.0 / 81.6 79.4 / 80.7\nTable 3: Performance on MultiNLI test set of single / ensem-\nble approaches training from scratch. Gaussian Transformer\nimproves the state-of-the-art accuracy by 1 percent at most.\nESIM Gauss. Trans. Speedup\nTraining time 29 mins 8 mins 3.6x\nInference time 31s 4s 7.8x\nTable 4: Time consumed for training / predicting on the\nSNLI datasets for one epoch.\nthrough the test set once. Thanks to its RNN/CNN-free ar-\nchitecture, Gaussian Transformer gains 3.6x / 7.8x speedup\nversus ESIM, as shown in Table 4.\nComparison with Approaches using External Knowledge\nReasoning between sentences requires common sense,\nwhich might not be learned only using the training set. To ﬁll\nthe knowledge gap, approaches directly leveraging external\nknowledge, such as KIM (Chen et al. 2017a) using WordNet\n(Miller 1995), or indirectly transfer knowledge from other\ntasks, e.g., language modeling [ELMo (Peters et al. 2018),\nOpenAI Transformer (Radford et al. 2018)], machine trans-\nlation [CoVe (McCann et al. 2017)] and discourse marker\nprediction [DMAN (Pan et al. 2018)], were proposed. We\nexplore the feasibility of extending Gaussian Transformer\nwith a pre-trained language model, i.e., ELMo. We follow\nthe instructions (Peters et al. 2018), replacing word and char-\nacter embeddings with ELMo and concatenating the output\nof encoding blocks with ELMo, and present the results in\nTable 5.\nThe ensemble version of Gaussian Transformer achieves\nstate-of-the-art results among those approaches. However,\ncompared with pre-trained word embeddings, the improve-\nment by ELMo on SNLI (0.2) is less than that on MultiNLI\n(1.0/1.2). The reason might be that the training data in SNLI\nis larger than MultiNLI, where each genre contains only\nabout 80k sentence pairs, while SNLI could be treated as a\nsingle genre including 550k examples. The best performing\nindividual model, OpenAI Transformer, was trained from\ndatasets containing billions of words on 8 GPUs for 1 month.\nIn contrast, our single ELMo-integrated model obtains com-\npetitive results using a more lightweight architecture.\nEvaluation on Hard NLI BenchmarksAnnotation arti-\nfacts were found in the original NLI datasets and more chal-\nlenging benchmarks, i.e., Hard SNLI and MultiNLI were\nproposed. As shown in Table 6, Gaussian Transformer im-\nproves about 5 percent on average on the Hard NLI datasets,\nsuggesting that our model is less affected by the undesirable\n6494\nModel SNLI M-ma M-mis\nKIM 88.6 77.2 76.4\nCoVe 88.1 - -\nESIM(ELMo) 88.7 - -\nDMAN 88.8 78.9 78.2\nDMAN* 89.6 80.3 79.4\nOpenAI Transformer 89.9 82.1 81.4\nGaussian Transformer(ELMo) 89.4 81.0 80.6\nGaussian Transformer* 90.5 83.0 82.5\nTable 5: Experimental results of approaches using exter-\nnal resources. ‘*’ indicates ensemble models. The ensem-\nble Gaussian Transformer improves the state-of-the-art ac-\ncuracy by 0.6 / 1.0 percent on SNLI / MultiNLI.\nannotation artifacts. Integration with ELMo brings about 1.0\npercent additional enhancement, showing the usefulness of\nknowledge transferred from language models.\nModel SNLI M-ma M-mis\nDecAtt 69.4 55.8 56.2\nESIM 71.3 59.3 58.9\nDIIN 72.7 64.1 64.4\nGaussian Transformer 78.1 69.8 68.5\nGaussian Transformer(ELMo) 79.2 70.7 71.3\nGaussian Transformer* 79.9 73.0 72.8\nTable 6: Experimental results on the Hard NLI benchmarks.\n‘*’ indicates ensemble models. Gaussian Transformer gains\nan improvement of 5.0 percent accuracy on average.\nImplementation Details\nWe implement our model using Tensorﬂow (Abadi et al.\n2016), with the library tensor2tensor (Vaswani et al. 2018).\nAll experiments are conducted on a single Nvidia Titan\nXp GPU. The best performing individual model consists of\nM = 3 encoding blocks, N = 2 interaction blocks, using\nH = 4 heads attention with dmodel = 120 , dw = 300 ,\ndc = 30 . Word embeddings are initialized from the pre-\ntrained fasttext word vectors (Bojanowski et al. 2016), while\ncharacter-level 5-grams embeddings are randomly initial-\nized, and all embeddings remain ﬁxed during training. We\nshare the parameters of encoding and interaction blocks be-\ntween premise and hypothesis, where the parameters at var-\nious depth, however, are different. Dropout (Srivastava et\nal. 2014) (rate = 0 .1) is applied to all sub-layers. We em-\nploy the AdamWR algorithm (Loshchilov and Hutter 2017)\nto train our model on SNLI and MultiNLI separately, with\nbatch size 64, learning rate range [4E−5,3E−4], normal-\nized weight decay wnorm = 1/600, restarting term T = 10.\nRelated Work\nNeural models became popular in NLI ﬁeld since large\nannotated datasets, i.e., SNLI (Bowman et al. 2015) and\nMultiNLI (Williams, Nangia, and Bowman 2017), were re-\nleased. The dominant trend of NLI models is employing\ndeep complex neural models including RNNs (Chen et al.\n2017b; Wang, Hamza, and Florian 2017; Tan et al. 2018;\nLiu, Duh, and Gao 2018; Ghaeini et al. 2018; Tay, Tuan,\nand Hui 2017; Kim et al. 2018), CNNs (Gong, Luo, and\nZhang 2017), SANs (Parikh et al. 2016; Shen et al. 2017;\nIm and Cho 2017). Although obtained state-of-the-art re-\nsults, those models often cost millions of parameters and lots\nof time to train.\nTransformer was proposed by Vaswani et al. for machine\ntranslation as an encoder-decoder architecture, which based\nsolely on attention mechanisms, eschewing recurrence and\nconvolution. QANet (Yu et al. 2018) was the ﬁrst model try-\ning to improve the capability of Transformer to model lo-\ncal structure of texts, which utilizes convolution layers be-\nfore self-attention to capture local dependency. However,\na convolution layer could not model words outside cur-\nrent ﬁlter window and costs additional parameters. Besides\nQANet, Gaussian transformer is also inspired by the fol-\nlowing two works: Parikh et al. ﬁrst introduced a distance-\nsensitive bias term to attention mechanisms to capture se-\nquential information of sentences. Im and Cho further re-\nstricted the distance-based bias to a linearly decaying man-\nner. We also re-implemented those strategies as our base-\nlines and found that Gaussian prior probability performs the\nbest among these enhanced self-attention networks.\nBesides training a neural model using only NLI datasets\nfrom scratch, approaches leveraging external resources\n(Chen et al. 2017a; Peters et al. 2018; Radford et al. 2018;\nMcCann et al. 2017; Pan et al. 2018) emerged recently and\nfurther improved the performance of NLI. We also explored\nthe feasibility of extending the Gaussian Transformer via a\npre-trained language model, i.e., ELMo (Peters et al. 2018).\nGururangan et al. demonstrated that annotation artifacts\nin SNLI and MultiNLI inﬂated NLI models’ performance,\nand proposed the Hard NLI benchmarks. We employ their\ndatasets to evaluate our models besides the original ones.\nLastly, the lightweight character-level n-grams word en-\ncoding used in this work is inspired by (Tomar et al. 2017).\nThey used the sum of character n-grams to represent tokens.\nWe employ max-over-time pooling instead because in this\nway our model performs better in preliminary experiments.\nConclusion\nIn this paper, we propose a novel attention mechanism in-\nspired by the ‘chunking’ phenomenon, i.e., Gaussian self-\nattention, which could better capture both local structure and\nglobal dependency of sequences without introducing recur-\nrence or convolution.\nThen we present an efﬁcient NLI model named Gaussian\nTransformer, consisting of Gaussian encoding blocks, high-\norder interaction blocks and efﬁcient comparison blocks,\noutperforming previous state-of-the-art approaches on both\nSNLI and MultiNLI benchmarks with signiﬁcantly fewer\nparameters and considerably less training time.\nAdditional evaluation using the Hard NLI datasets\ndemonstrates that the proposed approach is less affected by\nthe undesirable annotation artifacts than previous works.\nWe also explore the feasibility of extending Gaussian\nTransformer using external resources, and obtain further im-\nprovement brought by the transferred knowledge from a pre-\ntrained language model.\n6495\nAcknowledgments\nThe paper is supported by the 973 Program (No.\n2014CB340503) and National Natural Science Foundation\nof China (No. 61472105 and No. 61502120).\nReferences\nAbadi, M.; Barham, P.; Chen, J.; Chen, Z.; Davis, A.; Dean, J.;\nDevin, M.; Ghemawat, S.; Irving, G.; and Isard, M. 2016. Ten-\nsorﬂow: a system for large-scale machine learning. In OSDI,\nvolume 16, 265–283.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer Normal-\nization. arXiv:1607.06450 [cs, stat]. arXiv: 1607.06450.\nBojanowski, P.; Grave, E.; Joulin, A.; and Mikolov, T.\n2016. Enriching Word Vectors with Subword Information.\narXiv:1607.04606 [cs]. arXiv: 1607.04606.\nBowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D. 2015.\nA large annotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). Associ-\nation for Computational Linguistics.\nChen, Q.; Zhu, X.; Ling, Z.-H.; Inkpen, D.; and Wei, S.\n2017a. Natural Language Inference with External Knowledge.\narXiv:1711.04289 [cs]. arXiv: 1711.04289.\nChen, Q.; Zhu, X.; Ling, Z.; Wei, S.; and Jiang, H. 2017b. En-\nhancing and Combining Sequential and Tree LSTM for Nat-\nural Language Inference. arXiv:1609.06038 [cs]. arXiv:\n1609.06038.\nDagan, I.; Roth, D.; Sammons, M.; and Zanzotto, F. M. 2013.\nRecognizing Textual Entailment: Models and Applications.\nSynthesis Lectures on Human Language Technologies6(4):1–\n220.\nGhaeini, R.; Hasan, S. A.; Datla, V .; Liu, J.; Lee, K.; Qadir, A.;\nLing, Y .; Prakash, A.; Fern, X. Z.; and Farri, O. 2018. DR-\nBiLSTM: Dependent Reading Bidirectional LSTM for Nat-\nural Language Inference. arXiv:1802.05577 [cs]. arXiv:\n1802.05577.\nGong, Y .; Luo, H.; and Zhang, J. 2017. Natural Language In-\nference over Interaction Space. arXiv:1709.04348 [cs]. arXiv:\n1709.04348.\nGururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.;\nBowman, S. R.; and Smith, N. A. 2018. Annotation Artifacts\nin Natural Language Inference Data.\nIm, J., and Cho, S. 2017. Distance-based Self-Attention Net-\nwork for Natural Language Inference. arXiv:1712.02047 [cs].\narXiv: 1712.02047.\nKim, S.; Hong, J.-H.; Kang, I.; and Kwak, N. 2018. Se-\nmantic Sentence Matching with Densely-connected Recurrent\nand Co-attentive Information. arXiv:1805.11360 [cs]. arXiv:\n1805.11360.\nLiu, X.; Duh, K.; and Gao, J. 2018. Stochastic Answer Net-\nworks for Natural Language Inference. arXiv:1804.07888 [cs].\narXiv: 1804.07888.\nLoshchilov, I., and Hutter, F. 2017. Fixing Weight Decay Reg-\nularization in Adam. arXiv:1711.05101 [cs, math]. arXiv:\n1711.05101.\nMcCann, B.; Bradbury, J.; Xiong, C.; and Socher, R.\n2017. Learned in Translation: Contextualized Word Vectors.\narXiv:1708.00107 [cs]. arXiv: 1708.00107.\nMiller, G. A. 1995. WordNet: a lexical database for English.\nCommunications of the ACM38(11):39–41.\nPan, B.; Yang, Y .; Zhao, Z.; Zhuang, Y .; Cai, D.; and He, X.\n2018. Discourse Marker Augmented Network with Reinforce-\nment Learning for Natural Language Inference. In the 56th An-\nnual Meeting of the Association for Computational Linguistics,\n11.\nParikh, A. P.; T¨ackstr¨om, O.; Das, D.; and Uszkoreit, J. 2016.\nA Decomposable Attention Model for Natural Language Infer-\nence. arXiv:1606.01933 [cs]. arXiv: 1606.01933.\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.;\nLee, K.; and Zettlemoyer, L. 2018. Deep contextualized word\nrepresentations. arXiv:1802.05365 [cs]. arXiv: 1802.05365.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I.\n2018. Improving Language Understanding by Generative Pre-\nTraining. Technical report.\nShen, T.; Zhou, T.; Long, G.; Jiang, J.; Pan, S.; and Zhang,\nC. 2017. DiSAN: Directional Self-Attention Network for\nRNN/CNN-Free Language Understanding. arXiv:1709.04696\n[cs]. arXiv: 1709.04696.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: A Simple Way to Prevent\nNeural Networks from Overﬁtting. Journal of Machine Learn-\ning Research15:1929–1958.\nStigler, S. M. 1982. A modest proposal: a new standard for the\nnormal. The American Statistician36(2):137–138.\nTan, C.; Wei, F.; Wang, W.; Lv, W.; and Zhou, M. 2018. Mul-\ntiway Attention Networks for Modeling Sentence Pairs. In IJ-\nCAI, 4411–4417.\nTay, Y .; Tuan, L. A.; and Hui, S. C. 2017. A Compare-\nPropagate Architecture with Alignment Factorization for Nat-\nural Language Inference. arXiv:1801.00102 [cs]. arXiv:\n1801.00102.\nTomar, G. S.; Duque, T.; T¨ackstr¨om, O.; Uszkoreit, J.; and Das,\nD. 2017. Neural Paraphrase Identiﬁcation of Questions with\nNoisy Pretraining. arXiv:1704.04565 [cs]. arXiv: 1704.04565.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention\nIs All You Need. arXiv:1706.03762 [cs]. arXiv: 1706.03762.\nVaswani, A.; Bengio, S.; Brevdo, E.; Chollet, F.; Gomez, A. N.;\nGouws, S.; Jones, L.; Kaiser, \\.; Kalchbrenner, N.; and Parmar,\nN. 2018. Tensor2tensor for neural machine translation. arXiv\npreprint arXiv:1803.07416.\nWang, Z.; Hamza, W.; and Florian, R. 2017. Bilateral\nMulti-Perspective Matching for Natural Language Sentences.\narXiv:1702.03814 [cs]. arXiv: 1702.03814.\nWilliams, A.; Nangia, N.; and Bowman, S. R. 2017. A\nBroad-Coverage Challenge Corpus for Sentence Understanding\nthrough Inference.\nYu, A. W.; Dohan, D.; Luong, M.-T.; Zhao, R.; Chen, K.;\nNorouzi, M.; and Le, Q. V . 2018. QANet: Combining Local\nConvolution with Global Self-Attention for Reading Compre-\nhension. arXiv:1804.09541 [cs]. arXiv: 1804.09541.\n6496"
}