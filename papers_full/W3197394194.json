{
    "title": "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers",
    "url": "https://openalex.org/W3197394194",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2658674003",
            "name": "Stella Frank",
            "affiliations": [
                "University of Trento",
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A3014545733",
            "name": "Emanuele Bugliarello",
            "affiliations": [
                "University of Trento",
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A2257973904",
            "name": "Desmond Elliott",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3181158454",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W2185175083",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W3157926951",
        "https://openalex.org/W3176626464",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2031489346",
        "https://openalex.org/W3201264086",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W1773149199",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3177487519",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W3159619744",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2906152891",
        "https://openalex.org/W2963430224",
        "https://openalex.org/W3163542683",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3211483028",
        "https://openalex.org/W3173681001",
        "https://openalex.org/W3034727271",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3106784008",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W3034837210"
    ],
    "abstract": "Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9847–9857\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n9847\nVision-and-Language or Vision-for-Language?\nOn Cross-Modal Inﬂuence in Multimodal Transformers\nStella Frank∗,D Emanuele Bugliarello∗,C Desmond ElliottC\nDUniversity of Trento CUniversity of Copenhagen\nstella.frank@unitn.it {emanuele, de}@di.ku.dk\nAbstract\nPretrained vision-and-language BERTs aim to\nlearn representations that combine informa-\ntion from both modalities. We propose a di-\nagnostic method based on cross-modal input\nablation to assess the extent to which these\nmodels actually integrate cross-modal infor-\nmation. This method involves ablating inputs\nfrom one modality, either entirely or selec-\ntively based on cross-modal grounding align-\nments, and evaluating the model prediction\nperformance on the other modality. Model\nperformance is measured by modality-speciﬁc\ntasks that mirror the model pretraining ob-\njectives (e.g. masked language modelling for\ntext). Models that have learned to construct\ncross-modal representations using both modal-\nities are expected to perform worse when in-\nputs are missing from a modality. We ﬁnd that\nrecently proposed models have much greater\nrelative difﬁculty predicting text when visual\ninformation is ablated, compared to predicting\nvisual object categories when text is ablated,\nindicating that these models are not symmetri-\ncally cross-modal.\n1 Introduction\nVision-and-language (V&L) BERT models extend\nthe BERT architecture (Devlin et al., 2019) to pro-\nduce cross-modal contextualised representations of\nmultimodal inputs (Lu et al., 2019; Tan and Bansal,\n2019; Chen et al., 2020; Su et al., 2020). These\nmodels have proven to be highly effective when\nﬁne-tuned for a range of downstream tasks. How-\never, in spite of their versatility, little is known\nabout how these models use cross-modal infor-\nmation: do their learned representations for lan-\nguage tasks include visual information (vision-for-\nlanguage) and vice-versa (language-for-vision)? Su\net al. (2020) claim that their model will use visual\ncontext for predicting masked words, thus aligning\n∗Equal contribution.\n[MASK] playing tennis[MASK] playing tennis[MASK] playing tennis\n[MASK] playing tennis[MASK][MASK][MASK]Girl playing tennis\nNone Object All\nNone Phrase All\nVision-for-Language Diagnostic\nLanguage-for-Vision Diagnostic\np([MASK]=girl)p(       =       )\nFigure 1: Cross-modal input ablation tests how well\nmodels predict masked data, given ablated inputs in\nthe other modality. The Vision-for-Language Diagnos-\ntic (top) measures the effect of ablation, of either the\naligned object or the full image, on masked token pre-\ndiction, while the Language-for-Vision Diagnostic (bot-\ntom) measures the effect of ablating either the aligned\nPhrase or the entire sentence when predicting the prop-\nerties of a masked image region.\nvisual and linguistic contexts, while Lu et al. (2020)\nhope to force their model to rely more heavily on\nlanguage to predict image content. The extent to\nwhich these statements hold is unknown, in part\nbecause of the difﬁculty of analysing exactly how\nthese models use information across modalities.\nIn this paper, we introduce a cross-modal input\nablation method to quantify the degree to which\na pretrained model has learned to use cross-modal\ninformation. This method, which requires no ad-\nditional training, involves ablating all or some of\nthe inputs from one modality when making a pre-\n9848\ndiction in the other modality, and measuring the\nchange in performance. See Figure 1 for an illus-\ntrative sketch. Performance is measured using the\nsame masked-target prediction tasks used during\npretraining, in which a model must predict either\na masked token given surrounding text and visual\ncontext (masked language modelling), or a masked\nvisual object given the surrounding visual context\nand accompanying text (masked region classiﬁca-\ntion). Cross-modal input ablation thus captures the\ndegree to which a model depends on cross-modal\ninputs and activations when generating predictions.\nOur use of input ablation to assess cross-modal\nrecruitment is novel. Previous analyses of multi-\nmodal models have used diagnostic classiﬁers and\nattention analyses (Li et al., 2020; Cao et al., 2020;\nParcalabescu et al., 2021). In comparison, cross-\nmodal input ablation has the following advantages:\n• It is straightforward to perform, and easy to in-\nterpret, requiring no intervention in the model\nand only minimal intervention on the data.\n• As an intrinsic diagnostic, it examines the\nmodel directly, unlike methods that add\nlearned parameters, such as classiﬁer-based\nprobing approaches (Hupkes et al., 2018).\n• It does not require interpreting activations or\nattention, which can be difﬁcult (Jain and Wal-\nlace, 2019).\nWe envision cross-modal input ablation as a use-\nful diagnostic check to be performed during model\ndevelopment, to test the effect of changes in archi-\ntecture and optimisation.\nIn this paper we perform a case study of cross-\nmodal input ablation on existing models, to demon-\nstrate its utility in understanding model behaviour.\nWe test models that have different architectures\nbut the same initialisation and training proce-\ndures (Bugliarello et al., 2021). Our cross-modal\ninput ablation results show that these models do\nlearn to use cross-modal information, resulting in\nmultimodal representations, but this is not equally\ntrue across both modalities. In particular, the repre-\nsentations of text segments are strongly inﬂuenced\nby the visual input, while the representations of\nvisual regions are much less inﬂuenced by the ac-\ncompanying textual input. This indicates that the\nlevel of cross-modal information exchange is not\nsymmetrical: the models have learned to use vision-\nfor-language more than language-for-vision.\nIn subsequent analyses, we attempt to under-\nstand the lack of recruitment of language-for-\nvision, in order to identify possible avenues for im-\nprovement. Our experiments investigate different\nloss functions, initialisation and pretraining strate-\ngies, and visual co-masking procedures. None of\nthese factors changes model behaviour signiﬁcantly.\nHowever, we ﬁnd that the visual object annota-\ntions used in pretraining, which are automatically\ngenerated by an object detector, are much noisier\nthan expected. We surmise that the ensuing mod-\nels do not recruit text because it is not useful for\npredicting these noisy object features, and discuss\nimplications for future V&L models.\n2 Related Work\nSigniﬁcant efforts have been devoted to understand-\ning what is learned by text-only pretrained lan-\nguage models (Rogers et al., 2020), which can be\ncategorised as probing based on diagnostic classi-\nﬁer, analysing attention weights, or direct evalua-\ntions (Belinkov and Glass, 2019). However, much\nless work has looked at V&L pretraining. Two\nstudies have examined learned attention weights:\nLi et al. (2020) ﬁnd that there are attention heads\nspecialising in entity phrase grounding in Visual-\nBERT, but do not investigate text-to-vision atten-\ntion. Cao et al. (2020) ﬁnd that the textual modality\ndominates in UNITER and LXMERT, both in terms\nof overall attention and more speciﬁcally during\nvisual co-reference resolution tasks. In a direct\nevaluation analysis, Parcalabescu et al. (2021) eval-\nuate VilBERT and LXMERT on a counting task\nand ﬁnd more evidence of dataset bias (predicting\nfrequent numbers) than accurate image grounding.\nThese results all indicate that cross-modal interac-\ntion within these models may not be as strong or\nas symmetric as often assumed, which is further\nconﬁrmed by the methods introduced in this paper.\nIn addition, we evaluate a larger set of models, cov-\nering both dual-stream and single-stream models.\nThe ablation method introduced here is related to\nconcurrent work in language modelling (O’Connor\nand Andreas, 2021) and machine translation (Fer-\nnandes et al., 2021), in which the effect of removing\ncontext is measured. O’Connor and Andreas (2021)\nand Fernandes et al. (2021) use ablations at both\ntraining and evaluation, whereas we only study the\neffect of ablating inputs during evaluation.\n3 Proposed Approach\nWe use ablations to determine whether pretrained\nvision-and-language models combine inputs from\n9849\nboth modalities when making predictions. In gen-\neral, ablations are used to test the hypothesis that\na certain model component (here, multimodal in-\nputs) is responsible for certain model behaviour:\nablated models are expected to perform differently\nbecause of the missing component. Importantly,\nthis hypothesis is falsiﬁable (Popper, 1935) if the\nablation leads to no change.\nMultimodal models are hypothesised to use\ncross-modal activations, triggered by multimodal\ninputs, when making predictions. If a multimodal\nmodel relies on activations from certain input data,\ne.g. object category labels, to make predictions,\nthe ablation of this input will lead to a change in\nperformance, whereas if the model has not learned\nto use the input, removing it will have no effect.\nThe predictions we evaluate are the same kinds of\npredictions that the models have learned to make\nduring pretraining, i.e. predicting the identity of\nmasked tokens or regions given (possibly ablated)\nmultimodal text and image contexts.\nInput data are in the form of an image paired\nwith a sentence describing the image; within the\nsentence, phrases can refer to particular objects\nin the image. We expect such aligned, grounded,\nphrase–object pairs to elicit especially strong cross-\nmodal activations at prediction time. By ablating\nthe alignment link, we test models’ ability to create\nand use grounded alignments. We also test whether\nnon-speciﬁc cross-modal information is used, by\nablating whole modalities.\n3.1 Vision-for-Language Diagnostic\nThe language task consists of predicting masked\ntokens, possibly using visual inputs. For visual\ninput ablation, we compare the following setups:\nNone: None of the visual features are ablated,\ni.e. the model has access to the full image.\nThis is the original multimodal setting and\nthus where a model that uses multimodal in-\nformation effectively should perform best.\nObject: Here we remove only the image regions\nthat correspond to the aligned textual phrase.\nThis tests a model’s ability to ground text to\nspeciﬁc regions of the image, by breaking any\npossible alignment. However, the model can\nstill use surrounding visual context features.\nAll: All the visual features are ablated and the\nmodel needs to predict masked textual tokens\nfrom its textual context only. Models that de-\npend on multimodal inputs should suffer.\nFigure 1 (top, middle) shows an example of Ob-\nject ablation, where the target is to predict “ girl”\nand the ablated visual input is the image region\ncorresponding to the tennis player.\n3.2 Language-for-Vision Diagnostic\nThe vision task is to predict the target object cate-\ngory within a speciﬁc region of the image, possibly\naided by the text caption. In this case, a single re-\ngion, corresponding to an object aligned to a phrase\nin the sentence, is selected as the target. The textual\ninput is ablated analogously to the visual input:\nNone: None of the text is ablated, i.e. the model\nsees the entire sentence.\nPhrase: Here we only ablate the tokens in the\nphrase aligned to the target object.\nAll: All tokens are ablated and the model is re-\nquired to predict the masked visual region\nfrom its visual context only.\nFigure 1 (bottom, middle) shows an example of\nPhrase ablation, which tests the extent to which\nthe model relies on grounding information when\npredicting the target tokens. If ablating the phrase\ndoes not change performance compared to ablating\nthe entire sentence, then the model has not learned\nto use phrase-to-object alignments to infer what is\nmissing in the image given the sentence.\n4 Experimental Setup\nIn this section, we describe our setup to evalu-\nate cross-modal inﬂuence in pretrained vision-and-\nlanguage BERTs. Our code is available online.1\n4.1 Evaluation data\nWe use the validation split of the Flickr30k En-\ntities dataset (Plummer et al., 2015), a subset of\nFlickr30k (Young et al., 2014). This dataset con-\ntains human-annotated alignments between gold\nimage regions and phrases in image captions.\nThe validation set contains 1,000 images, each\nassociated to 5 English sentences. A sentence\ncontains on average 2.89 phrases. Each phrase\nis aligned with 1.53 objects on average; 78% of\nphrases are linked to a single object. In total, this\nresults in 14,433 phrase–object(s) data points.\n4.2 Models\nWe evaluate ﬁve different architectures:\nLXMERT (Tan and Bansal, 2019) and ViL-\n1https://github.com/e-bug/\ncross-modal-ablation.\n9850\nBERT (Lu et al., 2019) (dual-stream); VL-\nBERT (Su et al., 2020), VisualBERT (Li\net al., 2019) and UNITER (Chen et al., 2020)\n(single-stream). These models extend the BERT\narchitecture to multimodal data and tasks by\nsupplementing the text input with image features.\nSingle-stream models process image and text\njointly with a single Transformer encoder, while\ndual-stream ones ﬁrst encode each modality\nseparately and then make them interact. We reuse\nthe model weights from our previous controlled\nstudy (Bugliarello et al., 2021).2\nStarting from BERT, each model was pretrained\non Conceptual Captions (CC; Sharma et al. 2018)\nfor 10 epochs. The input images were provided\nas 36 regions of interest extracted using a Faster\nR-CNN (Ren et al., 2015) pretrained on Visual\nGenome (Anderson et al., 2018). Each model was\npretrained on three objectives: masked language\nmodelling (MLM), masked region classiﬁcation\nwith KL divergence (MRC-KL), and image–text\nmatching (Chen et al., 2020). We note that since\neach model used the same visual and textual vocab-\nularies, as well as the same context length, cross-\nentropies and other entropy-based measures are\ncomparable between model architectures.\nIn addition, as a control model, we adapt BERT\nto the same multimodal distribution by continued\npretraining with MLM on Conceptual Captions for\n5 epochs.3 We denote this version as BERTCC.\nCompute infrastructure Our experiments were\nrun on a shared computing infrastructure. Ablation\nsetups were measured on CPU, with an average\nruntime of 9 minutes. When we pretrain V&L\nBERTs, we use the same hyperparameters as in our\ncontrolled study (Bugliarello et al., 2021). Pretrain-\ning takes 5 days, corresponding to 10 epochs of\nCC, on one NVIDIA Titan RTX GPU card.\n4.3 Prediction Tasks\nThe prediction tasks mirror two of the training\ntasks, MLM and MRC-KL, with a few subtle but\ncrucial differences in masking and ablation. In the\nfollowing, we denote the input textual tokens as\nw = ⟨w1,...,w T⟩and the input image regions as\nv = ⟨v1,..., vK⟩. In addition, we use m to refer\nto the set of masked indices, either in the textual or\nvisual modality, and \\m for its complement.\n2https://github.com/e-bug/volta.\n3This results in the same number of MLM updates ob-\nserved by vision-and-language models in their pretraining.\nMasked language modelling The MLM task is\nto predict the identity of a set of masked tokenswm,\ngiven unmasked tokens w\\m and visual context v:\nMLM(m,w,v; θ) =−\n∑\ni∈m\nlog2 Pθ(wi|w\\m,v),\n(1)\nwhere θdenotes a model’s parameters.\nDuring pretraining, tokens are masked at ran-\ndom, using the special [MASK] token. During eval-\nuation, only tokens corresponding to a speciﬁc\ngrounded phrase are masked for prediction.\nWhen ablating visual inputs (as opposed to mask-\ning), the ablated regions are replaced with the\naverage feature vector (calculated over the eval-\nuation dataset). This keeps the visual inputs in-\ndistribution, but renders them uninformative. Dur-\ning Object ablation, all regions that overlap with\nthe gold region are ablated, using the intersection\nover target (IoT) function deﬁned in Eq. (4) below,\nwith an overlap threshold of τ = 0.5.\nMasked region classiﬁcation The MRC task is\nto predict the object class of a masked visual re-\ngion vi given unmasked visual context v\\m and\ntokens w. The MRC-KL variant (Li et al., 2019)\nmeasures the KL-divergence of the predicted distri-\nbution rather than the cross-entropy against a single\nobject class. For each masked region vi linked to a\nphrase, MRC-KL is computed as follows:\nMRC-KL(w,vi; θ) =KL(Pg(vi)||Pθ(vi|w,v\\m)),\n(2)\nwhere Pg is the target object distribution and Pθ\nis the distribution predicted by the model. During\ntraining, the visual annotations of object classes\nPg are given by the Faster R-CNN object detec-\ntor, which has a set of 1,601 object classes. Chen\net al. (2020) showed that MRC-KL leads to better\ndownstream performance compared to MRC with\ncross-entropy (MRC-XE).\nWhen a given region is masked for prediction,\nother regions that overlap with it are also masked\nout, according to the IoT function (Eq. (4)). In prac-\ntice, masking for prediction involves replacing the\ninput region vector with a vector of zeros. When\nperforming textual ablation, the ablated tokens are\nreplaced with [MASK] tokens.4\nDuring our evaluation procedure, we only evalu-\nate region predictions for the target objects that are\n4For the All ablation setup, we also investigated using one\n[MASK] token but found no difference in model behaviour.\n9851\nmentioned in the text. As the target region, among\nthe regions masked by the IoT function, we use the\n(Faster R-CNN-generated) input region that has\nthe highest overlap with the gold one, according to\nintersection over union (IoU; Eq. (3)).\n4.4 Region Overlap Calculations\nObject detectors often output regions that overlap\nwith each other. These regions may leak visual\ninformation and hence need to be masked in our\ndiagnostic procedure. In particular, there are two\npoints where this issue arises: ﬁrst, in language-\nfor-vision, when deciding which regions overlap\nwith the target region of prediction; secondly, in\nvision-for-language Object ablation, when decid-\ning which regions overlap with the gold region.\nIntersection over union Visual region overlap is\nmost commonly calculated using intersection over\nunion (IoU; Everingham et al. 2010). Let bi and\nbj be the bounding boxes corresponding to regions\nvi and vj, respectively. IoU is computed as:\nIoU(bi,bj) =|bi ∩bj|\n|bi ∪bj|. (3)\nOur models were pretrained by co-masking any\nregion that overlapped with the randomly chosen\none by more than the threshold of τ = 0.4.\nIntersection over target A shortcoming of IoU\nis that it may not catch large regions that largely\noverlap small target regions, potentially leaking\ninformation. To avoid this, we deﬁne the more\naggressive intersection over target (IoT) measure:\nIoT(bi,bj) =|bi ∩bj|\n|bi| . (4)\nTo ﬁnd co-ablated regions during Object ablation,\nwe use IoT with a threshold τ = 0.5 (as originally\nrecommended by Everingham et al. 2010 for IoU).\n5 Cross-Modal Input Ablation Results\nVision-for-Language diagnostic Figure 2 (left)\nshows the performance of the ﬁve models in differ-\nent types of visual input ablation. The models per-\nform best when None of the visual input is ablated,\nand perform worse when the Object is ablated, in-\ndicating that the models are missing information\nabout the grounding object region. Models are\nmost affected when removing All of the visual in-\nput, which is unsurprising. However, the effect of\nObject ablation is relatively small, compared to the\nNone Object All\nVision-for-Language Ablation\n4.75\n5.00\n5.25\n5.50\n5.75\n6.00\n6.25bit\nBERTCC\nLXMERT\nViLBERT\nVL-BERT\nVisualBERT\nUNITER\n2.90\n2.95\nNone Phrase All\nLanguage-for-Vision Ablation\n0.4\n0.5\n0.6\n0.7\nFigure 2: Left: Masked language modelling loss\nfor different ablation settings: no ablation, ablation\nof the grounding object, ablation of the full image.\nRight: Masked region classiﬁcation with KL diver-\ngence loss for different ablation settings: no ablation,\nablation of the region-aligned phrase, ablation of the\nfull text. The mean performance of 10 different ini-\ntial seeds is shown for each model; variance around the\nmean is too small to be seen for most models. Mod-\nels suffer when visual input, but not textual input, is\nremoved.\neffect of All ablation, which is counter to the ex-\npectation that models should use grounding object\ninformation rather than general visual context.\nIn Figure 3, we check whether these results are\ndue to visual leakage of object information into the\ngeneral visual context. We examine the effect of\nmasking the visual object more or less aggressively,\nby changing the overlap thresholdτthat triggers co-\nmasking. Increased masking leads to worse perfor-\nmance on Object ablation, and at lower thresholds\nfor masking, the relative contribution of Object\ninformation is higher than that of the additional\ngeneral visual context in All ablation.\nComparing models, the single-stream Visual-\nBERT is least affected by ablating the visual input,\nwhile the dual-stream ViLBERT is most affected.\nThe behaviour of VL-BERT is an unexpected out-\nlier that warrants future investigation, given that it\nperforms similarly to other models in downstream\ntasks (Su et al., 2020; Bugliarello et al., 2021).\nThe language-only BERT model has a loss\nof 7.44 bits on this evaluation set, which drops\nto 5.39 bits after pretraining on the Conceptual\nCaptions dataset (BERTCC): there is a strong ef-\nfect of the language domain. Overall, when visual\ninput is provided, the V&L BERTs show better\nperformance than the language-only baselines, but\ntheir performance degrades below domain-adapted\nunimodal performance when visual input is with-\n9852\n0\n100\nMasked\n Regions [%]\nNone 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 All\nVision-for-Language Ablation\n5.0\n5.5\n6.0\n6.5bit\nBERTCC\nLXMERT\nViLBERT\nVL-BERT\nVisualBERT\nUNITER\nFigure 3: MLM performance across different visual\nmasking thresholds, leading to increased co-masking of\nregions overlapping with the ablated Object. τ = 0.5\ncorresponds to Object ablation in Figure 2.\nheld. We conclude that these models have indeed\nlearned to use vision-for-language.\nLanguage-for-Vision diagnostic On the right of\nFigure 2 are the results of ablating language when\npredicting visual object categories. Compared to\nthe effect of ablating visual inputs for language\nmodelling, where the performance drops by up\nto 25% during All ablation, we see only mini-\nmal effects of ablating text. According to paired-\ndifference t-tests, all models’ ablated performances\n(both Phrase and All ablation) differ signiﬁcantly\nfrom the standard model ( None ablation), with\np <0.05. However, the practical effects of ab-\nlation are very small. Ablating the reference phrase\nleads to a increase in KL-divergence of 0.5%–3%,\nwhile ablating All text increases the KL-divergence\nbetween 1%–10%, relative to None ablation. In\nboth cases, the least affected model is LXMERT,\nand the most affected model is ViLBERT. In sum-\nmary, this analysis shows that these V&L models\nuse language-for-vision much less than they use\nvision-for-language.\n6 Diving into Language-for-Vision\nThe previous section showed that V&L BERTs are\nmore sensitive to ablated visual inputs than lan-\nguage inputs. This behaviour could be due to sev-\neral factors, including differences in model design\nand initialisation, pretraining objectives, and issues\nwith the quality of the silver labels provided by\nFaster R-CNN. Here, we analyse how these factors\naffect language-for-vision recruitment.\n6.1 Model Design\nIn theory, particular model architectures could\nfavour the transfer and cross-modal use of features\nin one direction more than the other. Overall, our\nresults show all of the evaluated model designs\nresulting in quite similar, asymmetric, behaviour.\nInterestingly, however, the two dual-stream ar-\nchitectures, LXMERT and ViLBERT, behave very\ndifferently. LXMERT achieves the lowest MRC-\nKL value, and it is not affected by the presence of\ntextual inputs. ViLBERT, on the other hand, is the\nmodel that most relies on language when predicting\nvisual features. We believe that the main reason\nbehind the difference in performance might be due\nto their different vision streams: ViLBERT directly\nmaps visual features onto cross-modal layers, while\nLXMERT ﬁrst processes them in a 5-layer vision-\nonly stream. We hypothesise that these ﬁve layers\nare enough for LXMERT to learn visual representa-\ntions to solve the MRC-KL task, without langauge.\nSecondly, the lack of a vision-only stream in ViL-\nBERT makes it asymmetric, which might explain\nits greater recruitment of language for vision. Con-\nversely, all the other models have approximately\nthe same number of parameters for both modalities.\nFinally, VisualBERT performs much worse than\nall the other models on MRC-KL. We hypothesise\nthis is because it is the only model that does not en-\ncode spatial information from the bounding boxes\ncorresponding to the input visual features.\n6.2 Pretraining Loss Function\nWe investigate whether pretraining with a differ-\nent vision objective leads to different behaviour in\nlanguage-for-vision recruitment. In particular, pre-\ndicting object class distributions, using MRC-KL,\nmay be easier if the long-tail of object classes is\nrelatively similar between (non-masked) regions\nwithin an image, since this would make visual con-\ntextual features highly informative. Replacing the\nKL divergence objective with a cross-entropy ob-\njective forces the model to predict only the most\nlikely object class for each region, which is ex-\npected to be more dissimilar across the image.\nWe pretrain a UNITER encoder using a weighted\nmasked region classiﬁcation with cross-entropy\n(MRC-XE) objective (Tan and Bansal, 2019),\nwhere the weights are given by the probability as-\nsigned by the object detector to the most likely\nclass. We ﬁnd that pretraining with this vision\nobjective does not lead to any change in model be-\n9853\nNone Object All\nVision-for-Language Ablation\n5\n6\n7\n8\n9\n10bit\nViLBERT\nUNITER\nNone Phrase All\nLanguage-for-Vision Ablation\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nBERT  CCVL\nRnd  CCVL\nRnd  CCV  CCVL\nBERT  CCV  CCVL\nFigure 4: MLM and MRC-KL for different pretraining\ninitialisations. Pretraining affects overall performance,\nbut not ablation behaviour (lines are parallel).\nhaviour: Ablating the reference phrase leads to a\nincrease in XE of 1%, compared to the standard\nmodel (None ablation); ablating All text increases\nit by 4%, in line with the KL results in Figure 2.\n6.3 Initialisation and Pretraining Order\nAll tested models share the same pretraining\nsequence: they are ﬁrst initialised with BERT\nweights and then pretrained on CC to model vi-\nsion and language. We refer to this conﬁgura-\ntion as BERT→CCVL. Here, we ask whether the\nBERT initialisation leads to the asymmetric lack\nof language-for-vision behaviour: does the strong\nlanguage modelling capability embedded in the\nBert weights constrain the model from adapting\ntowards the vision prediction task? In addition, we\nexplore whether different pretraining regimes can\nalso lead to different behaviours in V&L BERTs.\nWe consider the following setups:\nRnd→CCVL Instead of initialising with BERT,\nthe models are randomly initialised and\ntrained on CC using all three standard losses.\nRnd→CCV→CCVL Here, randomly initialised\nmodels are ﬁrst pretrained on CC using only\nthe visual MRC-KL loss, and then pretrained\non CC as usual with all three losses. This is\nan attempt at ‘vision-ﬁrst’ models.\nBERT→CCV→CCVL Same as above, but start-\ning from a BERT initialisation.\nWe pretrain both ViLBERT and UNITER archi-\ntectures in the Rnd→CCVL setting, and UNITER\nfor the others. The results for all these setups are\ndisplayed in Figure 4, and show that (i) pretrain-\ning from scratch increases MLM loss, compared to\nmodels initialised with BERT, in both ViLBERT\nNone Object All\nVision-for-Language Ablation\n4.75\n5.00\n5.25\n5.50\n5.75\n6.00bit\nIoU\nIoT\nNone Phrase All\nLanguage-for-Vision Ablation\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.2 0.4 0.6 None\nFigure 5: MLM and MRC-KL for different pretraining\nthresholds for the UNITER model. Neither co-masking\nthreshold nor co-masking function (IoU or IoT) affect\nmodel performance nor behaviour in our diagnostics.\nand UNITER; and (ii) pretraining with vision-ﬁrst\nleads to lower MRC-KL loss for UNITER. Nev-\nertheless, our empirical ﬁndings indicate that ini-\ntialisation is not a strong factor for the observed\ncross-modal asymmetric behaviour.\n6.4 Leaking Visual Features\nThe visual contextual features may (accidentally)\nprovide sufﬁcient information for correct predic-\ntions. In particular, given that many automatically\nproposed regions in an image overlap, if these are\nnot correctly co-masked with the object region,\nthey could leak information about the object.\nWe perform an experiment where we pretrain\nUNITER with different overlapping thresholds\nτ ∈ {0.2,0.4,0.6}as well as without any co-\nmasking (None), testing both IoU and IoT masking\nfunctions. Figure 5 shows that varying the amount\nof co-masking, or the masking function, does not\naffect the recruitment of language-for-vision. Our\nresult counters the hypothesis of Lu et al. (2020)\nthat co-masking “forces the model to rely more\nheavily on language to predict image content. ”\n6.5 Silver Object Annotations\nFinally, we examine the data used for represent-\ning the visual modality. The models are trained\nand evaluated on object predictions, ‘silver distri-\nbutions,’ from Faster R-CNN, which contain noise\nbecause they are automatically predicted categories.\nDuring evaluation, noise in the target distributions\nmay lead to discounting of the role of language if\nthe evaluation set contains many examples in which\nthe target object class predictions conﬂict with the\naligned textual features.\n9854\nanimals\nbodyparts\nclothing\nother\npeople\nscene\nvehicles\nFaster R-CNN labels\nanimals\nbodyparts\nclothing\nother\npeople\nscene\nvehicles\nGold labels\n0.29 0.01 0.00 0.01 0.00 0.01 0.00\n0.01 0.27 0.02 0.05 0.01 0.01 0.00\n0.02 0.14 0.22 0.07 0.04 0.11 0.02\n0.04 0.13 0.06 0.49 0.03 0.14 0.15\n0.02 0.08 0.05 0.08 0.50 0.06 0.04\n0.02 0.00 0.06 0.15 0.02 0.22 0.07\n0.00 0.00 0.00 0.02 0.00 0.01 0.39\nError Distribution of Faster R-CNN by Category\nFigure 6: Confusion matrix showing the proportion\nof incorrect class predictions (out of all predictions),\ngrouped into categories. 60% of Faster R-CNN “peo-\nple” category label predictions are incorrect (sum of\ncolumn); 50% of all predictions are incorrect within\nthe set of “people” classes.\nTo examine this, we construct a subset of the\nFlickr30k Entities validation set, called Label-\nMatch, in which the nouns of an aligned phrase\n– extracted using Stanza (Qi et al., 2020) – perfectly\nmatch with one of the Faster R-CNN’s object labels.\n(Note that this does not mean that Faster R-CNN\npredicted that category for that example, only that\nit is possible for the model to have done so.) This\nresults in 10,159 data points out of the original14k.\nWe consider these labels, extracted from Flickr30k\nEntities, as human-generated “gold” object labels.\nThe silver distributions are noisy On the Label-\nMatch subset, the highest-probability class from\nFaster R-CNN agrees with the gold label only on\n38% of examples. The gold label is in the top-3\n55% of the time, in top-5 64%, and in top-10 75%\nof the time. Figure 6 shows the distribution of\nerrors, grouped into the higher-level categories\ndeﬁned in the Flickr30k Entities dataset. Faster\nR-CNN is mostly making mistakes within cate-\ngories, especially within the “people” category,\nwhere more than half of all predictions are wrong\naccording to the gold label. There is also a long tail\nof wrong predictions not in the Flickr30k Entities\nobject classes within the “other” predictions.\nEvaluating on gold labels Using the Label-\nMatch as gold labels for evaluation does not lead to\nany clear difference in ablated MRC performance,\nas seen in Figure 7 (right). (Figure 7 (left) conﬁrms\n2.90\n2.95\nMRC-KL\nLXMERT\nViLBERT\nVL-BERT\nVisualBERT\nUNITER\nNone Phrase All\n0.4\n0.5\n0.6\n0.7               bit\n6.85\n6.90\nMRC-XE(LM)\nNone Phrase All\n4.8\n4.9\n5.0\n5.1\nFigure 7: LabelMatch subset results: MRC-KL against\nsilver distributions and MRC-XE against gold labels.\n3.05\n3.10\n3.15\n MRC-KL\nLXMERT\nViLBERT\nVL-BERT\nVisualBERT\nUNITER\nNone Phrase All\n0.3\n0.4\n0.5\n0.6               bit\n4.25\n4.50\nMRC-XE(LM)\nNone Phrase All\n1.2\n1.3\n1.4\n1.5\nFigure 8: MRC-KL and MRC-XE in the subset of La-\nbelMatch where Faster R-CNN’s prediction match with\ngold labels. We observe similar patterns as in Label-\nMatch showing that models do not recruit text for vi-\nsion even when the predicted object classes are correct.\nthat this subset does not behave differently from\nthe full validation set on MRC-KL against the sil-\nver distribution labels.) Models still do not rely on\ntext when predicting the (gold) region class, even\nthough — in the un-ablated setting — the region\nlabel is given in the text input in this dataset.\nFinally, we also measure MRC-KL on the sub-\nset of LabelMatch in which the silver distribution\nmode and the gold labels agree. This tests whether\nmodels use language for vision when the most\nlikely class of silver distribution is correct: per-\nhaps these instances are recognisably cleaner to the\nmodel. However, we again ﬁnd no difference in\nablated performance (see Figure 8).\nIn conclusion, even when evaluated on gold la-\nbels, we still see most models making next to no\nuse of textual information for visual predictions.\nThis behaviour is consistent with models that have\nbeen pretrained against noisy silver data, where\nlanguage inputs are not useful for prediction.5\n5We believe it also explains why Chen et al. (2020) found\n9855\n7 Conclusion\nThe cross-modal input ablation diagnostic intro-\nduced in this paper demonstrated an asymmetry in\npretrained vision and language models: the predic-\ntion of masked text is strongly affected by ablated\nvisual inputs, compared to (almost) no effect of ab-\nlating textual inputs when predicting masked image\nregions. These results offer a useful check of actual\nmodel behaviour, and run counter to hypotheses\nassuming more balanced cross-modal activations.\nWe conducted several follow-up studies to better\nunderstand, and possibly ameliorate, this behaviour.\nWe explored (i) pretraining the model on vision be-\nfore cross-modal training; (ii) visual leakage from\noverlapping image regions; and (iii) problems with\nthe silver target distributions produced by Faster R-\nCNN. There were no discernible effects from (i) or\n(ii). With regards to (iii), we found the silver labels\nto be much less reliable than expected. We suspect\nthat these silver target distributions are not appro-\npriate for activating language-for-vision, and we\nrecommend that future V&L models avoid them, if\nthey aim to not only support vision-for-language.\nThis paper has focused on an intrinsic evaluation\nof pretrained models, rather than their downstream\ntask performance. We note that the models stud-\nied in this paper perform similarly on a number\nof downstream tasks (Bugliarello et al., 2021), in\nline with their similar ablation behaviour. Whether\ncross-modal ablation behaviour can predict down-\nstream performance is a question left for future\nmodels with more divergent behaviour.\nMore broadly, the fact that the tested models\nshowed an effect of vision-for-language but not\nlanguage-for-vision may be an accumulation of\nmodel engineering for multimodal tasks that are\nmore strongly vision-for-language, such as visual\nquestion answering or grounded reasoning. Some\nrecent models (Li et al., 2021; Shen et al., 2021;\nHendricks and Nematzadeh, 2021) have removed\nthe visual component from the training objective\nentirely, resulting in vision-for-language architec-\ntures. In the future, we advocate for increased work\non more language-for-vision tasks, such as text-\nmodulated object detection (Kamath et al., 2021),\nin order to push pretrained vision- and-language\nmodels to be truly, bidirectionally, cross-modal.\nan advantage of pretraining with a KL-divergence loss instead\nof a maximum-probability cross-entropy loss: predictions\nmade with KL divergence are more robust to noise.\nAcknowledgements\n⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆We are grateful to the anonymous reviewers and\nmembers of the CoAStaL NLP group for their con-\nstructive feedback. This project has received fund-\ning from the European Union’s Horizon 2020 re-\nsearch and innovation programme under the Marie\nSkłodowska-Curie grant agreement No 801199.\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\n2018 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2018, Salt Lake City, UT,\nUSA, June 18-22, 2018 , pages 6077–6086. IEEE\nComputer Society.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nEmanuele Bugliarello, Ryan Cotterell, Naoaki\nOkazaki, and Desmond Elliott. 2021. Multimodal\npretraining unmasked: A meta-analysis and a\nuniﬁed framework of vision-and-language BERTs.\nTransactions of the Association for Computational\nLinguistics.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-\nChun Chen, and Jingjing Liu. 2020. Behind the\nscene: Revealing the secrets of pre-trained vision-\nand-language models. In Computer Vision - ECCV\n2020 - 16th European Conference, Glasgow, UK,\nAugust 23-28, 2020, Proceedings, Part VI , volume\n12351 of Lecture Notes in Computer Science, pages\n565–580. Springer.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision, pages 104–120.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMark Everingham, Luc Gool, Christopher K. Williams,\nJohn Winn, and Andrew Zisserman. 2010. The pas-\ncal visual object classes (voc) challenge.Int. J. Com-\nput. Vision, 88(2):303–338.\n9856\nPatrick Fernandes, Kayo Yin, Graham Neubig, and An-\ndré F. T. Martins. 2021. Measuring and increasing\ncontext usage in context-aware machine translation.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n6467–6478, Online. Association for Computational\nLinguistics.\nLisa Anne Hendricks and Aida Nematzadeh. 2021.\nProbing image-language transformers for verb un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3635–3644, Online. Association for Computational\nLinguistics.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and ’diagnostic classiﬁers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. Journal of Artiﬁcial\nIntelligence Research, 61:907–926.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nAishwarya Kamath, Mannat Singh, Yann LeCun, Is-\nhan Misra, Gabriel Synnaeve, and Nicolas Carion.\n2021. MDETR – modulated detection for end-to-\nend multi-modal understanding. In Proceedings of\nthe IEEE/CVF International Conference on Com-\nputer Vision (ICCV).\nJunnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak\nGotmare, Shaﬁq Joty, Caiming Xiong, and Steven\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nArXiv preprint, abs/2107.07651.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2020. What does BERT\nwith vision look at? In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5265–5275, Online. Association\nfor Computational Linguistics.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada , pages\n13–23.\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 2020. 12-in-1: Multi-task vi-\nsion and language representation learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 10434–10443. IEEE.\nJoe O’Connor and Jacob Andreas. 2021. What con-\ntext features can transformer language models use?\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n851–864, Online. Association for Computational\nLinguistics.\nLetitia Parcalabescu, Albert Gatt, Anette Frank, and\nIacer Calixto. 2021. Seeing past words: Testing\nthe cross-modal capabilities of pretrained v&l mod-\nels on counting tasks. In Proceedings of the 1st\nWorkshop on Multimodal Semantic Representations\n(MMSR), pages 32–44, Groningen, Netherlands (On-\nline). Association for Computational Linguistics.\nBryan A. Plummer, Liwei Wang, Chris M. Cervantes,\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-\nto-sentence models. In 2015 IEEE International\nConference on Computer Vision, ICCV 2015, Santi-\nago, Chile, December 7-13, 2015, pages 2641–2649.\nIEEE Computer Society.\nKarl Popper. 1935. Logik der Forschung. Springer Ver-\nlag.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020. Stanza: A\npython natural language processing toolkit for many\nhuman languages. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 101–\n108, Online. Association for Computational Linguis-\ntics.\nShaoqing Ren, Kaiming He, Ross B. Girshick, and\nJian Sun. 2015. Faster R-CNN: towards real-time\nobject detection with region proposal networks. In\nAdvances in Neural Information Processing Systems\n28: Annual Conference on Neural Information Pro-\ncessing Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada, pages 91–99.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for au-\ntomatic image captioning. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2556–2565, Melbourne, Australia. Association for\nComputational Linguistics.\n9857\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit\nBansal, Anna Rohrbach, Kai-Wei Chang, Zhewei\nYao, and Kurt Keutzer. 2021. How much can clip\nbeneﬁt vision-and-language tasks? ArXiv preprint.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78."
}