{
  "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
  "url": "https://openalex.org/W4385571087",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3032751618",
      "name": "Amr Keleg",
      "affiliations": [
        "Language Science (South Korea)",
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A1990912753",
      "name": "Walid Magdy",
      "affiliations": [
        "University of Edinburgh",
        "Language Science (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3206534622",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W4285243012",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W4288804650",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W3104163040",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W2948947170"
  ],
  "abstract": "A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 6245–6266\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDLAMA: A Framework for Curating Culturally Diverse Facts for Probing\nthe Knowledge of Pretrained Language Models\nAmr Keleg and Walid Magdy\nInstitute for Language, Cognition and Computation\nSchool of Informatics, University of Edinburgh\na.keleg@sms.ed.ac.uk, wmagdy@inf.ed.ac.uk\nAbstract\nA few benchmarking datasets have been re-\nleased to evaluate the factual knowledge of pre-\ntrained language models. These benchmarks\n(e.g., LAMA, and ParaRel) are mainly devel-\noped in English and later are translated to\nform new multilingual versions (e.g., mLAMA,\nand mParaRel). Results on these multilingual\nbenchmarks suggest that using English prompts\nto recall the facts from multilingual models\nusually yields signiﬁcantly better and more\nconsistent performance than using non-English\nprompts. Our analysis shows that mLAMA\nis biased toward facts from Western countries,\nwhich might affect the fairness of probing mod-\nels. We propose a new framework for curating\nfactual triples from Wikidata that are culturally\ndiverse. A new benchmark DLAMA-v1 is built\nof factual triples from three pairs of contrasting\ncultures having a total of 78,259 triples from\n20 relation predicates. The three pairs com-\nprise facts representing the (Arab and Western),\n(Asian and Western), and (South American and\nWestern) countries respectively. Having a more\nbalanced benchmark (DLAMA-v1) supports\nthat mBERT performs better on Western facts\nthan non-Western ones, while monolingual Ara-\nbic, English, and Korean models tend to per-\nform better on their culturally proximate facts.\nMoreover, both monolingual and multilingual\nmodels tend to make a prediction that is cul-\nturally or geographically relevant to the correct\nlabel, even if the prediction is wrong.\n1 Introduction\nTransfer learning paradigms such as ﬁne-tuning,\nfew-shot learning, and zero-shot learning rely on\npretrained language models (PLMs), that require\nhaving large compilations of raw data ( Devlin et al.\n2019; Brown et al. 2020; Chowdhery et al. 2022;\nScao et al. 2022). These PLMs showed some ability\nto model different linguistic phenomena ( Goldberg\n2019; Jawahar et al. 2019) in addition to memoriz-\ning facts related to real-world knowledge. While\nthere is a drive to have multilingual models, En-\nglish is still the language that is better supported\ndue to the abundance of large English raw cor-\npora, diverse datasets, and benchmarks. Moreover,\nmonolingual non-English PLMs are still being pre-\ntrained for other high-resource languages. As a way\nto probe the non-English and multilingual PLMs,\nresearchers tend to translate English benchmarks\ninto other languages, which might degrade the qual-\nity of the samples especially if the translation is per-\nformed automatically. While translating English\nbenchmarks saves the time and money needed to\nbuild new language-speciﬁc benchmarks, it might\nintroduce unintended biases or artifacts into the\nbenchmarks.\nLAMA (Petroni et al. , 2019) and ParaRel ( Elazar\net al., 2021) are two benchmarks developed to quan-\ntify the factual knowledge of the English PLMs.\nThey used a setup in which a language model is\nsaid to know a speciﬁc fact if it can predict the\nright object for a prompt in a ﬁll-the-gap setup\n(e.g., For the prompt “The capital of England is\n[MASK]\", the model needs to ﬁll the masked gap\nwith “London\"). Multilingual versions of these\nbenchmarks namely: mLAMA ( Kassner et al. ,\n2021), and mParaRel ( Fierro and Søgaard , 2022)\nwere released to evaluate the performance of mul-\ntilingual PLMs by translating LAMA and ParaRel\ninto 53 and 46 languages respectively. The subjects\nand objects of the triples within these benchmarks\nwere translated using their multilingual labels on\nWikidata, while the templates were automatically\ntranslated from the English ones used in the origi-\nnal benchmarks. These templates transform triples\ninto textual natural language prompts for probing\nthe models. X-FACTR is another benchmark shar-\ning the same setup, and is built for 23 different\nlanguages ( Jiang et al. , 2020). All three bench-\nmarks sample factual triples in the form of (subject,\nrelation predicate, object) from T-REx, a dump\nof Wikidata triples aligned to abstracts extracted\n6245\nfrom the English Wikipedia ( Elsahar et al. , 2018).\nThe way T-REx is constructed might make it more\nrepresentative of the facts related to Western cul-\ntures, which might introduce an unnoticed bias to\nthe benchmarks based on it. We hypothesize that\nhaving a fair representation of the different cultures\nwithin a benchmark is vital for fairly probing mod-\nels pretrained for multiple languages. The main\ncontributions of our paper can be summarized as\nfollows:\n1. Investigating the impact of sampling mLAMA\ntriples from T-REx on the distribution of the\nobjects within the relation predicates.\n2. Proposing DiverseLAMA (DLAMA), a\nmethodology for curating culturally diverse\nfacts for probing the factual knowledge of\nPLMs, and building 3 sets of facts from pairs\nof contrasting cultures representing the (Arab-\nWest), (Asia-West), and (South America-\nWest) cultures, to form DLAMA-v1 1.\n3. Showing the impact of having a less skewed\nbenchmark DLAMA-v1 on the performance\nof mBERT and monolingual Arabic, English,\nKorean, and Spanish BERT models.\n4. Demonstrating the importance of having con-\ntrasting sets of facts in diagnosing the behav-\nior of the PLMs for different prompts.\n2 Related Work\nPetroni et al. (2019) investigated the possibility\nof using PLMs as potential sources of knowledge,\nwhich can later substitute manually curated knowl-\nedge graphs. To this end, they created LAMA\n(LAnguage Model Analysis), a dataset of 34,000\nrelation triples representing facts from 41 differ-\nent Wikidata relation predicates. These facts are\nextracted from a larger dataset called T-REx that\ncontains 11 million relation triples, acquired from\na large Wikidata dump of triples, that were auto-\nmatically aligned to English Wikipedia abstracts\n(Elsahar et al. , 2018). Manual English templates\nwere written to transform the triples into prompts to\nprobe the model’s factual knowledge. The triples\nwere limited to the ones whose objects are tok-\nenized into a single subtoken.\nKassner et al. (2021) constructed a multilingual\nversion of LAMA (mLAMA) having 53 different\nlanguages. They handled the limitation of using\nsingle-subtoken objects by computing the proba-\n1The DLAMA-v1 benchmark and the codebase can be\nreached through: https://github.com/AMR-KELEG/DLAMA\nbility of a multi-subtoken object as the geomet-\nric mean of the subtokens’ probabilities. They\nconcluded that the performance of mBERT when\nprobed with prompts written in 32 languages is sig-\nniﬁcantly lower than mBERT’s performance when\nprobed with English prompts. Moreover, they ob-\nserved insigniﬁcant performance improvement for\nGerman, Hindi, and Japanese when their corre-\nsponding templates were manually corrected.\nSimilarly, Jiang et al. (2020) created X-FACTR\nby sampling relation triples from T-REx for 46\ndifferent Wikidata predicates. The multilingual\nWikidata labels were used to translate the subjects\nand objects of the triples. They compared multi-\nple decoding methods. Moreover, they employed\ndifferent templates to generate prompts having the\ncorrect number/gender agreement with the subjects\nof the triples. English prompts still outperformed\nprompts written in 22 other languages.\nParaRel and its multilingual version mParaRel\nare benchmarks created by sampling triples from\nT-REx for 38 relation predicates ( Elazar et al. 2021;\nFierro and Søgaard 2022). Their aim is to measure\nthe consistency of the model in making the same\nprediction for different paraphrases of the same\ntemplate. Results on both benchmarks showed that\nthe multilingual mBERT and XLM-R models are\nless consistent than the monolingual English BERT\nmodel, especially when these multilingual models\nare prompted with non-English inputs.\nFrom a model diagnostics perspective, Cao et al.\n(2021) found that English PLMs might be biased to\nmaking speciﬁc predictions based on a predicate’s\ntemplate irrespective of the subjects used to popu-\nlate this template. Thereafter, Elazar et al. (2023)\ndesigned a causal framework for modeling multiple\nco-occurrence statistics that might cause English\nPLMs to achieve high scores on some of LAMA’s\npredicates.\nWe focus on why a non-English PLM might\nfail to recall facts and hypothesize the following\npossible reasons:\n1. The quality of the template might degrade af-\nter automatically translating it from English.\n2. Non-English or multilingual PLM are gen-\nerally pretrained on a lesser amount of non-\nEnglish data and thus might be less capable\nof recalling facts efﬁciently.\n3. Translating the underlying facts of a bench-\nmark, initially designed to probe English\nPLMs, might cause a representational bias.\n6246\nWhile the ﬁrst two factors are studied in the lit-\nerature, we believe that the third factor is a major\nquality issue that previous work has overlooked.\nRandomly sampling the triples from T-REx might\nintroduce a representation bias toward Western cul-\ntures, since only facts aligned to English Wikipedia\nabstracts are considered. We investigate the pres-\nence of such bias (§ 3). Moreover, we empirically\ndemonstrate how better model diagnostics can be\nperformed when the benchmark is formed using\ntwo diverse and contrasting sets of facts (§ 5).\n3 Cultural Bias in mLAMA\nProbing PLMs using prompts is an analysis tool at-\ntempting to understand how they behave. A biased\nprobing benchmark might be deceiving, as both a\ngood-performing model and a model sharing the\nsame bias found in the benchmark would achieve\ngood performance. In this section, we investigate if\nthe facts within mLAMA might be biased toward\nWestern cultures, which can affect the reliability of\nthe performance scores achieved by PLMs when\nprobed using mLAMA.\n3.1 Quantifying the Cultural Bias\nAs a proxy for measuring the skewness of the\ntriples of T-REx, LAMA, and X-FACTR toward\nWestern cultures, 26 relation predicates are selected\nthat have a person’s name or a place as their subject\nor object. Moreover, 21 Western countries are iden-\ntiﬁed as representative of Western cultures from\nWestern European and South Western European\ncountries2: Andorra, Austria, Belgium, France,\nGermany, Ireland, Italy, Liechtenstein, Luxem-\nbourg, Monaco, Netherlands, Portugal, San Marino,\nSpain, Switzerland, the United Kingdom, in addi-\ntion to Canada, the United States of America, Aus-\ntralia, and New Zealand. For each relation predi-\ncate out of the 26, triples with a subject or object\nthat either has a country of citizenship or is located\nin one of the 21 Western countries are counted.\n63.6% of the triples within the LAMA bench-\nmark are related to these Western countries com-\npared to 62.7% for X-FACTR, and 57.1% for T-\nREx (from which LAMA and X-FACTR are sam-\npled)3. This highlights the issue that aligning Wiki-\ndata triples to English Wikipedia abstracts in T-\nREx would skew them toward Western countries,\n2According to EuroV oc: https://eur-lex.europa.eu/\nbrowse/eurovoc.html?params=72,7206#arrow_912\n3Full percentages for each predicate are listed in Table A1.\nimpacting both LAMA and X-FACTR.\n3.2 Qualitative Analysis of the Bias and its\nImpact\nKassner et al. (2021) used mLAMA to probe\nmBERT using prompts in 41 languages. We ﬁnd\nthat all the languages in which prompts achieve the\nhighest performance 4 use the Latin script, while\nthe ones with the least performance 5 use other\nscripts. This might be attributed to the model’s\nability to share cross-lingual representations for\ncommon named entities for languages using the\nLatin script, which allows for cross-lingual knowl-\nedge sharing. Moreover, it is known that more than\n78% of mBERT’s vocabulary is Latin subwords 6.\nHowever, there are still some relation predi-\ncates for which a non-Latin scripted language out-\nperforms a Latin-scripted one. The P140 7 (reli-\ngion or worldview) predicate is a clear example of\nthese predicates. An example triple for the P140\npredicate is: (Edward I of England, religion or\nworldview [P140], Christianity) . mBERT has\nhigher performance for Arabic (23.1%), Azerbai-\njani (8.1%), Korean (30.1%), Georgian (35.1%),\nThai (13.4%), Tamil (4.0%), Russian (54.6%), and\nJapanese (30.0%) than for English (1.5%). Look-\ning at the objects for the English triples within\nmLAMA, we ﬁnd that 53.7% of the triples have\nIslam as their object.\nWhile the objects for the P140 predicate should\nbe religions, we ﬁnd that only seven triples have\nincorrect inﬂected forms of Muslim, Christian, and\nHindu instead of Islam, Christianity, and Hinduism.\nFurther investigation reveals that the English tem-\nplate used to transform the triples into prompts is\n([X] is afﬁliated with the [Y] religion . ) which\nwould suit retrieving these infrequent inﬂected la-\nbels than the frequent labels. Therefore, most pre-\ndictions for the English prompts are considered\nincorrect justifying the low performance achieved\nfor English. To overcome penalizing these pre-\ndictions, we mapped the model’s predictions and\nthe objects’ labels such that for instance Christian\nand Christianity are both considered to represent\nthe same prediction Christianity, and similarly for\nHinduism and Islam.\n4English, Indonesian, Malay, Afrikaans, Galician, Viet-\nnamese, Danish, Spanish, Catalan, Cebuano, Romanian.\n5Russian, Azerbaijani, Hebrew, Arabic, Korean, Armenian,\nGeorgian, Tamil, Thai, Japanese.\n6http://juditacs.github.io/2019/02/19/\nbert-tokenization-stats.html\n7Wikidata predicates’ identiﬁers format is P[0 − 9]+.\n6247\n0\n200\nArabic\nP@1 = 63.0%\n(245  out of 389)\nAzerbaijani\nP@1 = 53.6%\n(113  out of 211)\nHebrew\nP@1 = 27.0%\n(83  out of 307)\nArmenian\nP@1 = 22.2%\n(40  out of 180)\nGeorgian\nP@1 = 48.4%\n(103  out of 213)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\n0\n200\nRussian\nP@1 = 64.7%\n(262  out of 405)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\nThai\nP@1 = 20.1%\n(36  out of 179)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\nKorean\nP@1 = 29.9%\n(78  out of 261)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\nJapanese\nP@1 = 32.6%\n(125  out of 383)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\nT amil\nP@1 = 20.7%\n(41  out of 198)\nPrediction\n(a) Languages with the least overall performance on mLAMA.\n0\n200\nEnglish\nP@1 = 51.4%\n(243  out of 473)\nIndonesian\nP@1 = 71.2%\n(255  out of 358)\nMalay \nP@1 = 71.1%\n(145  out of 204)\nAfrikaans\nP@1 = 59.0%\n(79  out of 134)\nGalician\nP@1 = 54.3%\n(114  out of 210)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\n0\n200\nVietnamese\nP@1 = 52.0%\n(105  out of 202)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\nDanish\nP@1 = 64.8%\n(173  out of 267)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\nSpanish\nP@1 = 64.4%\n(289  out of 449)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\nCatalan\nP@1 = 69.1%\n(277  out of 401)\nIslam\nChristianity\nJudaism\nHinduism\nBuddhism\nRomanian\nP@1 = 50.9%\n(114  out of 224)\nPrediction (b) Languages with the highest overall performance on mLAMA.\nFigure 1: The distribution of the predictions of mBERT for the P140 predicate (religion or worldview) for prompts\nin 20 different languages after merging similar objects’ predictions (e.g., Muslim and Islam). The green portion of\nthe bar represents the triples for which the prediction is correct, while the red portion represents the triples for\nwhich the prediction is wrong. The P@1 (Precision at ﬁrst rank) metric is the percentage of triples for which the\nmodel’s ﬁrst prediction for a triple’s subject matches the triple’s object.\nNote: P@1 scores are not directly comparable since the number of triples in mLAMA differs between languages.\nFigure 1 shows the distribution of mBERT’s pre-\ndictions for the P140 triples for prompts in 20 dif-\nferent languages after unifying the labels. We ob-\nserve that: (1) For some languages, the predictions\nare skewed toward a speciﬁc wrong label that is\nculturally related to these languages. For exam-\nple, the mode of the predictions of prompts in Ar-\nmenian, Thai, Korean, and Tamil is Christianity,\nBuddhism, Buddhism, and Hinduism respectively.\n(2) Arabic, and Russian prompts tend to yield high\nperformance. The same holds for Indonesian and\nMalay which achieve similar performance with less\nskewness in the predictions. Since the label distri-\nbution for this predicate within mLAMA is skewed\ntoward a speciﬁc label Islam, one can not conﬁ-\ndently conclude whether the model is choosing the\nright answer for having some knowledge of the\nfacts or for making a biased guess that luckily co-\nincides with the right label. While these ﬁndings\nsignify the possibility that mLAMA is biased for\nthe P140 predicate, it on the other hand might hint\nthat mLAMA is also biased toward Western cul-\ntures for most of the remaining predicates. For\ninstance, the P103 (Native Language) predicate in\nmLAMA has Frenchas the correct label for 60.14%\nof the triples.\n4 Building DLAMA\nOur methodology aims at building a culturally di-\nverse benchmark, which would allow for a fairer\nestimation of a model’s capability of memorizing\nfacts. Within DLAMA, query parameters form un-\nderlying SPARQL queries that are used to retrieve\nWikidata triples as demonstrated in Figure 2.\nTo operationalize the concept of cultures, we\nuse countries as a proxy for the cultures of inter-\nest. For instance, countries that are members of\nthe Arab League are considered representatives of\nArab cultures. Conversely, Western countries men-\ntioned in § 3.1 represent Western cultures. Further-\nmore, China, Indonesia, Japan, Malaysia, Mongo-\nlia, Myanmar, North Korea, Philippines, Singapore,\nSouth Korea, Taiwan, Thailand, and Vietnam are\n13 countries from East Asia, and Southeast Asia 8\nrepresenting Asian cultures, while Argentina, Bo-\nlivia, Brazil, Chile, Colombia, Ecuador, Guyana,\nParaguay, Peru, Suriname, Uruguay, Venezuela rep-\nresent South American cultures.\nFor predicates in which the subject is a person,\nwe add a ﬁlter to the SPARQL query which limits\nthe country of citizenship of the person to a speciﬁc\nset of countries (i.e., a speciﬁc culture). For predi-\ncates in which the subject is a place, we limit the\nvalues of the places to those located in a country\nwithin the predeﬁned set of countries related to the\ntarget culture.\nWe implemented a Python interface to simplify\nthe process of querying Wikidata triples. Currently,\n20 relation predicates are supported. The user-\nfriendly interface allows the addition of new rela-\ntion predicates and ﬁlters, which we hope would\nencourage contributions to DLAMA.\n8Based on the UN stats classiﬁcation: https://unstats.\nun.org/unsd/methodology/m49/\n6248\nQuery parameters\n- Subject: Person \n- Object: Country \n- Predicate: P27 (Country of Citizenship) \n- Countries of interest: USA \n- Languages of labels: English\nSPARQL Query \nSELECT ?person ?country ?subject_article_en\nWHERE\n{\n    VALUES ?country {wd:Q30} . # Country is USA\n    ?person wdt:P27 ?country . \n    OPTIONAL {?subject_article_en schema:about ?person .\n                ?subject_article_en schema:inLanguage \"en\" .\n                ?subject_article_en schema:isPartOf\n<https://en.wikipedia.org/> .} .\n}\n(S2b) Sort by \narticle size\n(S2a) Query \narticles' \nsizes\n(S1a) Form \nquery\n(S3) Query all \nvalid objects\n(S4) Query labels\nInput\nOutput\nsub uri obj uri article url\nQ81324 Q30 Bret_Hart\nQ81328 Q30 Harrison_Ford\nQ65645 Q30Matthias_Pintscher\n... ... ...\n(S1b) Query \nWikidata\nsub uriobj uri article url article size\nQ81324 Q30 Bret_Hart 201353\nQ81328 Q30 Harrison_Ford 81422\nQ65645 Q30 Matthias_Pintscher6923\n... ... ... ...\nsub label obj label \nBoris JohnsonUnited States of America \nUnited Kingdom\nDonald Trump United States of America\nTom Brady United States of America\n... ...\nsub uriobj uri article url article size\nQ180589Q30\nQ145Boris_Johnson433128\nQ22686 Q30 Donal_Trump 417652\nQ313381Q30 Tom_Brady 408608\n... ... ... ...\nsub uriobj uri article url article size\nQ180589Q30 Boris_Johnson433128\nQ22686 Q30 Donal_Trump 417652\nQ313381Q30 Tom_Brady 408608\n... ... ... ...\nFigure 2: A demonstration of DLAMA’s querying framework for the predicate P27 (Country of Citizenship).\n4.1 Methodology of Querying Triples for a\nSpeciﬁc Predicate\nStep #1 - Getting an exhaustive list of triples\nfor a Wikidata predicate : A set of parameters\nneed to be speciﬁed through the Python interface\nto generate an underlying SPARQL query. These\nparameters are (1) an entity label for the subject,\nand an entity label for the object 9, (2) a set of coun-\ntries representing speciﬁc cultures, (3) a Wikidata\npredicate relating the object to the subject, (4) a\nlist of Wikipedia sites that are expected to contain\nfacts related to each speciﬁed country, and (5) a\nlist of languages for which the parallel labels of\nthe subjects and the objects are acquired and later\nused to populate the multilingual probing templates.\nIn addition to querying the Wikidata Unique Ref-\nerence Identiﬁers (URIs) of the subjects and the\nobjects, the Unique Reference Links (URLs) of the\nWikipedia articles linked to the subjects are queried\nas optional ﬁelds.\nStep #2 - Sorting the list of retrieved triples\nby their validity : Facts on Wikidata are crowd-\nsourced, and contributors are encouraged to add ref-\nerences to the facts they modify. However, lots of\nthe facts on Wikidata still have missing references.\nTherefore, we use the length of the Wikipedia arti-\ncle corresponding to the triple’s subject as a proxy\nfor the validity of the triple. The fact that contribu-\ntors and editors spent time writing a long Wikipedia\narticle implies that a group of people ﬁnds the arti-\ncle important. Therefore they will be keen on mak-\ning sure the information there is factually sound\n9The used entity labels are City, Continent, Country,\nGenre, Instrument, Language, Occupation, Original Network,\nPerson, Piece of Work, Place, Record Label.\n(Bruckman, 2022). We believe that using the size\nof the article rather than other metrics such as\nthe number of visits to the Wikipedia article, al-\nlows facts related to underrepresented groups on\nWikipedia to still be ranked high, thus making the\ntop-ranked facts more diverse and inclusive. We\nsort the retrieved triples by the size (in bytes) of the\nWikipedia article linked to their subjects. In case a\nsubject has articles on multiple Wikipedia sites, the\nsize of the largest article is used. DLAMA also al-\nlows sorting the triples by the total number of edits\n(revisions) of their subjects’ respective articles.\nStep #3 - Querying all possible objects for each\nsubject: Since a subject might be linked to multi-\nple objects for the same relation predicate, another\nquery is executed in order to ensure that all these\nobjects are retrieved. For instance, a person might\nbe a citizen of an Arab country in addition to an-\nother non-Arab country. This step ensures that the\nnon-Arab country is still considered as a valid coun-\ntry of citizenship for the person, even if the initial\nquery restricted the countries to Arab ones only.\nWhile previous benchmarks limited the object for\neach triple to a single value, we believe it is fairer\nto allow multiple valid labels instead of randomly\npicking one label out of the valid ones.\nStep #4 - Querying the labels for the triples : Till\nthis stage, the subjects and objects are represented\nby their Wikidata URIs. The Wikidata labels of all\nthe subjects and objects need to be fetched for the\nlanguages of interest. Relation triples having miss-\ning subject or object labels in any of the languages\nspeciﬁed are discarded in order to ensure that the\ntriples are the same for all the languages.\n6249\nStep #5 (optional) - Handling overlapping ob-\njects: The degree of granularity of the objects for\nWikidata’s relation predicates differs even among\ntriples of the same predicate (e.g.: The ofﬁcial\nlanguage of Australia is set to English while\nthat of The United States of America is\nset to American English which is a subclass of\nEnglish). To avoid penalizing models for picking\nan object that is a superclass of the correct object,\na graph is built, modeling the hierarchical relations\nbetween all the objects of the sampled triples of a\nrelation predicate. The graph is later used to aug-\nment the valid objects with their superclasses as\ndetailed in § B of the Appendix.\n4.2 The DLAMA-v1 Benchmark\nWe used the above method to build three sets of\nfacts as part of DLAMA-v1 to assess the perfor-\nmance of PLMs on recalling facts related to 21\nWestern countries as compared to the 22 Arab,\n13 Asian countries, and 12 South American coun-\ntries10. The sets provide examples of how the\nframework can be used to compile facts from pairs\nof contrasting cultures. We hope the community\nwill use the framework to introduce new pairs rep-\nresenting other countries and cultures. A maximum\nof 1000 triples from each predicate out of the 20\nsupported ones are independently queried for each\nset of countries within each pair. This ensures that\nthe queried triples are balanced across the two sets\nof countries within the pair.\nIn total, the (Arab-West) pair comprises 24535\ntriples with labels in Arabic and English, as com-\npared to 27076 triples with labels in Korean, and\nEnglish for the (Asia-West) pair, and 26657 triples\nwith labels in Spanish, and English for the (South\nAmerica-West) pair. Figure 3 shows an example of\na triple of DLAMA-v1’s (Arab-West) set. The un-\nderlying triples belonging to the Western cultures\nin the 3 sets are not identical. Triples in a set are\ndiscarded if their subjects or objects do not have\nlabels in the languages.\nRegarding the languages of the labels, Arabic\nand Korean are chosen as they are two of the least-\nperforming languages on mLAMA. It is expected\nthat facts related to Arab and East Asian/South\nEast Asian countries are relevant to Arabic and\nKorean PLMs respectively, and would be contrast-\ning to Western facts. Additionally, both languages\nhave non-Latin scripts, use white spaces to sepa-\n10Refer to § 3.1 and §4 for the list of countries.\n• Prompt: Egypt is located in ...\n• Subject: {Egypt}\n• Set of correct objects : {Africa, Asia}\n• Set of objects of the predicate to be ranked : {Africa, Asia, Europe,\nInsular Oceania, North America}\nFigure 3: An example of a prompt created using a rela-\ntion triple of DLAMA from the P30 (continent) relation\npredicate for the Arab-Western pair.\nrate tokens, and have an inventory of monolingual\nPLMs. On the other hand, the (South America-\nWest) pair is a trickier case since most South Amer-\nican countries use Spanish as their ofﬁcial language.\nOne can argue that sharing the same language with\nSpain introduces commonalities between the South-\nAmerican countries and the Western ones.\nOverlap between DLAMA-v1 and T-REx : For\nthe three culture sets, we measured the percentage\nof triples found in T-REx. 17.92% of Arab-related\nfacts are in T-REx compared to 39.85% of Western-\nrelated ones in the (Arab-Western) pair. Moreover,\n22.64% of Asian-related facts are found in T-REx\ncompared to 44.43% of Western-related ones in the\n(Asia-Western) pair. Lastly, the overlap percent-\nages for the (South America-West) pair are 17.68%\nand 32.22% respectively. These values demonstrate\nthat T-REx has less coverage of the Arab, Asian,\nand South American factual triples than its cov-\nerage of Western triples. Moreover, the fact that\nT-REx is tuned for higher precision means that its\nrecall is affected and a lot of the Western facts ex-\npected to be found in English Wikipedia abstracts\nare discarded. Conversely, DLAMA-v1 is a less\nskewed benchmark across different cultures.\n5 Probing PLMs via DLAMA-v1\n5.1 Experimental Setup\nWe follow mLAMA’s probing setup to evaluate\nthe PLMs’ factual knowledge. For each relation\npredicate [PREDICATE], the set {OBJECTS} of\nunique objects of the triples is ﬁrst compiled. Then,\nfor each relation triple within the [PREDICATE],\nthe PLM is asked to assign a score for each object\nwithin {OBJECTS} by computing the probability\nof having this object replacing the masked tokens.\nThis setup asks the model to choose the correct\nanswer out of a set of possible choices, instead of\ndecoding the answer as a generation task. The tem-\nplates used in DLAMA to convert triples into nat-\nural language prompts are adapted from mLAMA\nand listed in Table F9 of the Appendix.\n6250\nPrompt Model P@1 P@1\nLang. name Arab West DLAMA mLAMA\nN=10946 N=13589 N=24535 N=17128\nArabic mBERT-base 13.7 15.1* 14.5 15.2†\narBERT 33.6* 23.0 27.7† 24.4\nEnglish mBERT-base 21.2 37.7* 30.3 33.9†\nBERT-base 27.5 31.3* 29.6 37.9†\n(a) DLAMA-v1 (Arab-West)\nPrompt Model P@1 P@1\nLang. name Asia West DLAMA mLAMA\nN=13479 N=13588 N=27067 N=14217\nKorean mBERT-base 16.4 28.5* 22.5† 15.7\nKyKim 22.1* 19.5 20.8† 13.4\nEnglish mBERT-base 33.0 39.9* 36.4† 35.1\nBERT-base 38.3* 31.9 35.1 39.0†\n(b) DLAMA-v1 (Asia-West)\nPrompt Model P@1 P@1\nLang. name S. America West DLAMA mLAMA\nN=13071 N=13586 N=26657 N=28168\nSpanish mBERT-base 25.4 33.8* 29.7 30.5†\nBETO 16.0 26.5* 21.4 22.7†\nEnglish mBERT-base 27.0 37.6* 32.4 33.9†\nBERT-base 26.9 31.3* 29.2 37.1†\n(c) DLAMA-v1 (South America-West)\nTable 1: Performance of mBERT, and monolingual Ara-\nbic (arBERT), Korean (KyKim), Spanish (BETO), and\nEnglish (BERT-base) language models on the three sets\nof facts of DLAMA-v1. *: the set of cultures on which\na model performs better, †: the benchmark on which the\nmodel achieves higher P@1 score.\nModels: We evaluated the cased multilingual\nBERT-base, and the cased English BERT-base us-\ning all the sets of facts of DLAMA-v1. Moreover,\na monolingual Arabic BERT-base model arBERT\n(Abdul-Mageed et al. , 2021), a monolingual Ko-\nrean BERT-base model KyKim BERT-base (Kim,\n2020), and a monolingual cased Spanish BERT-\nbase model BETO (Cañete et al. , 2020) are evalu-\nated using the (Arab-West), the (Asia-West), and\nthe (South America-West) pairs respectively. We\nfocus on BERT models to compare our results to\nthose previously reported on mLAMA.\n5.2 Aggregated Results\nPrecision at the ﬁrst rank (P@1) is the metric\nused to evaluate the performance of the models.\nP@1 is the percentage of triples for which the\nﬁrst prediction of the model matches one of the\nobjects for this triple. In order to quantify the di-\nversity of the objects of a relation predicate for\neach culture, an entropy score is computed. For\neach triple of a relation predicate, only the most\nfrequent object among the list of valid objects\nis considered. The entropy score is computed\nas Entropy({objs}) = ∑\no∈{objs} −po ∗ log(po);\nwhere po is the probability of object o across the\nset of objects {objs}. The higher the entropy of\nthe objects is, the more diverse the objects are, and\nthus the harder the predicate would be for a model\nto randomly achieve high (P@1) scores.\nLooking at the performance of models on\nDLAMA indicated in Table 1, (1) we ﬁnd how\nthe facts’ relevance to the probed model’s lan-\nguage affects the results. For instance, arBERT\nand KyKim perform better on non-Western facts\nthan on Western ones. Conversely, the English\nBERT-base model performs better on Western facts\nfor the (Arab-West) pair. The same observation\ntends to hold for individual predicates as shown\nin Table 2. (2) Moreover, arBERT and KyKim\nachieve lower performance on mLAMA than their\nperformance on DLAMA-v1, while the English\nBERT-base and BETO models achieve higher P@1\nscores on mLAMA than on DLAMA-v1. This is\nexpected given the bias mLAMA has toward facts\nfrom Western cultures.\n5.3 Revisiting the Language bias of PLMs\nKassner et al. (2021) showed that for prompts in\nEnglish, German, Dutch, and Italian, mBERT is\nbiased toward predicting the language or the coun-\ntry name related to the language of the prompts\n(e.g., Filling the masked object with Italy if the\nprompt’s language is Italian). This phenomenon is\nnot a bias if most of the triples in the underlying\nsubset of mLAMA for a language are also biased\ntoward the same label. For DLAMA, looking at the\nP@1 scores in Table 2 in addition to checking the\nmost common predictions of arBERT and the cased\nBERT-base models in Table 3 provides a better di-\nagnostic tool for analyzing the models’ behavior 11.\nFor the P364 predicate, the models perform better\non their culturally proximate triples. This can be at-\ntributed to the Language bias phenomenon which is\nindicated by arBERT predicting Arabic for 30.8%\nof Western facts, while BERT-base predicting En-\nglish for 44.6% of Arab facts. On the other hand,\nboth models achieve high P@1 scores for P17 and\nP103. Even when the models make wrong predic-\ntions for triples of these predicates, the predictions\ncan be considered to be educated guesses, as they\nare still relevant to the culture to which the triples\nbelong. Lastly, the models perform poorly on P495\nfor being biased toward speciﬁc objects irrespective\nof the culture of the triples ( Japan for BERT-base,\nGermany and France for arBERT). These three pat-\nterns can be noticed thanks to having a contrastive\nset of facts representing two different cultures.\n11A similar analysis for the other two sets of contrasting\ncultures can be found in § E of the Appendix.\n6251\nRelation\nArabic prompts English prompts\n# facts (entropy) P@1 P@1\nArab West Arab West Arab West\nP17 (Country) 1000 (3.9) 1000 (2.8) 49.9 47.4 52.2 45.6\nP19 (Place of birth) 1000 (3.9) 1000 (2.6) 33.7 22.3 10.1 8.8\nP20 (Place of death) 1000 (3.8) 1000 (2.7) 21.3 22.7 14.2 17.2\nP27 (Country of citizenship) 1000 (3.8) 1000 (2.4) 38.1 27.9 4.1 17.5\nP30 (Continent) 22 (1.0) 19 (1.0) 45.5 26.3 86.4 84.2\nP36 (Capital) 22 (4.5) 19 (4.2) 95.5 78.9 36.4 84.2\nP37 (Ofﬁcial language) 22 (0.0) 19 (2.5) 90.9 84.2 95.5 100.0\nP47 (Shares border with) 22 (2.5) 19 (2.7) 27.3 15.8 68.2 78.9\nP103 (Native language) 1000 (1.0) 1000 (1.7) 61.8 72.8 67.7 74.4\nP106 (Occupation) 1000 (2.3) 1000 (2.0) 3.7 3.3 4.8 14.3\nP136 (Genre) 452 (2.7) 1000 (2.6) 6.6 24.3 4.0 7.6\nP190 (Sister city) 67 (4.9) 468 (7.3) 0.0 2.6 6.0 2.8\nP264 (Record label) 166 (3.0) 1000 (5.2) 0.0 0.3 4.2 7.5\nP364 (Original language of work) 1000 (0.6) 1000 (0.4) 61.2 48.5 36.1 88.9\nP449 (Original network) 127 (4.5) 1000 (5.3) 0.8 0.4 0.0 10.8\nP495 (Country of origin) 1000 (3.1) 1000 (1.3) 18.6 8.7 14.7 5.5\nP530 (Diplomatic relation) 22 (0.0) 19 (0.0) 22.7 42.1 31.8 68.4\nP1303 (Instrument) 1000 (0.9) 1000 (1.1) 0.3 0.2 1.9 27.7\nP1376 (Capital of) 24 (4.3) 26 (4.0) 91.7 84.6 79.2 76.9\nP1412 (Languages spoken or published) 1000 (0.8) 1000 (1.5) 67.4 26.1 83.4 88.7\nAggregated statistics 10946 (2.6) 13589 (2.7) 33.6 23.0 27.5 31.3\nTable 2: Detailed P@1 scores of arBERT (Arabic prompts) and cased BERT-base (English prompts) on the\nDLAMA-v1 (Arab-West) set. Note: # facts is the number of facts for each culture within the benchmark, while\n(entropy) is the entropy of the objects for the facts of each culture.\n5.4 Pilot Evaluation for a Large Language\nModel\nGiven the success of large language models (LLMs)\n(Brown et al., 2020; Scao et al. , 2022), we evaluated\nthe performance of the GPT3.5-turbo model on tu-\nples from the P30, P36, P37, P47, P103, P530, and\nP1376 predicates of DLAMA-v1 (Arab-West). To\nprobe the model, the Arabic and English templates\nfor these predicates were mapped into questions\nlisted in Table F10. While the model is instructed to\nonly respond with an entity, it sometimes provides\na full sentence. Consequently, we consider the\nmodel’s response to a question to be correct if one\nof the valid objects of the tuple used to populate the\nquestion is a substring of the response. GPT3.5’s\nprobing setup is harder than BERT’s setup in which\nan answer is chosen from a set of unique objects\nfor the predicate. Nevertheless, GPT3.5 achieves\nsuperior performance compared to the monolingual\nBERT models as per Table D3. However, GPT3.5\nseems to be hallucinating for a lot of the tuples\nwithin the P190 (Sister City) predicate (e.g.: The\ntwin city of Nice is Naples. ). Such issues might\nbe unnoticed unless benchmarks like DLAMA are\nused to systematically evaluate the LLMs.\n6 Conclusion\nPrevious work suggested that English prompts are\nmore capable of recalling facts from multilingual\npretrained language models. We show that the\nfacts within the underlying probing benchmark\n(mLAMA) are skewed toward Western countries,\nwhich makes them more relevant to English. Hence,\nwe propose a new framework (DLAMA) that per-\nmits the curation of culturally diverse facts di-\nrectly from Wikidata. Three new sets of facts\nare released as part of the DLAMA-v1 benchmark\ncontaining factual triples representing 20 relation\npredicates comprising facts from (Arab-Western),\n(Asian-Western), and (South American-Western)\ncountries, with a more balanced representation be-\ntween the countries within each pair. The results\nof probing PLMs on the DLAMA-v1 support that\nmBERT has a better performance recalling West-\nern facts than non-Western ones irrespective of the\nprompt’s language. Monolingual Arabic and Ko-\nrean models on the other hand perform better on\nculturally proximate facts. We believe the probing\nresults are more trustable and fairer when the un-\nderlying benchmark is less skewed toward speciﬁc\ncountries, languages, or cultures. Moreover, we\nﬁnd that even when the model’s prediction does not\nmatch any of the correct labels, the model might be\nmaking an educated guess relevant to the culture\nof the underlying facts. This ﬁnding augments pre-\nvious experiments which showed that models tend\nto have a language bias, by which a model tends to\novergenerate a speciﬁc prediction for each prompt-\ning language irrespective of the triple’s subject used\nto ﬁll in the prompt. Finally, our framework is open-\nsourced for the community to contribute new pairs\nto the DLAMA benchmark in the future.\n6252\nRelation predicate Common correct predictions Common wrong predictions\n(% of predictions) (% of predictions)\nProbing arBERT with Arabic promptspopulated with Arab facts\nP17: [X] is located in [Y] . Egypt (8.4%) Algeria (7.5%) Morocco(5.6%) Morocco(10.5%) Turkey (7.1%) Tunisia (6.7%)\nP19: [X] was born in [Y] . Egypt (9.0%) Algeria (4.2%) Morocco(3.9%) Algeria (18.8%) Tunisia (7.2%) Morocco(5.9%)\nP20: [X] died in [Y] . Egypt (9.8%) Baghdad(2.0%) Tunisia (1.1%) Paris (28.7%) Egypt (8.8%) Tunisia (5.7%)\nP27: [X] is [Y] citizen . Saudi Arabia(4.5%) Morocco(4.2%) Egypt (3.8%) State of Palestine(15.2%) Republic of Egypt(9.3%) Iraqi Republic(5.2%)\nP495: [X] was created in [Y] . Egypt (6.4%) Morocco(3.0%) France (2.9%) France (26.9%) Germany(19.5%) Morocco(10.2%)\nP103: The native language of [X] is [Y] . Arabic (59.7%) French (1.1%) English (0.4%) English (8.8%) Arabic (8.6%) Shilha (8.4%)\nP364: The original language of [X] is [Y] . Arabic (58.9%) French (1.1%) English (0.9%) Shilha (11.7%) English (8.5%) French (7.6%)\nP1412: [X] used to communicate in [Y] . Arabic (62.5%) French (4.7%) Spanish (0.1%) Syrian Arabic(15.1%) Arabic (9.1%) French (5.0%)\nProbing arBERT with Arabic promptspopulated with Western facts\nP17: [X] is located in [Y] . France (13.5%) United States of America(9.3%) Spain (9.1%) Germany(8.3%) South Africa(7.5%) France (6.6%)\nP19: [X] was born in [Y] . Germany(7.4%) Italy (4.6%) New York City(3.5%) New York City(26.0%) Germany(18.0%) Italy (10.8%)\nP20: [X] died in [Y] . Paris (9.0%) Germany(3.5%) Italy (3.4%) Paris (33.4%) New York City(19.7%) London (7.6%)\nP27: [X] is [Y] citizen . France (8.7%) Germany(6.5%) United States of America(6.3%) Germany(12.9%) French protectorate of Tunisia(11.0%) Republic of Ireland(6.8%)\nP495: [X] was created in [Y] . United States of America(3.9%) France (2.4%) Germany(1.7%) Germany(44.2%) France (23.4%) Algeria (4.5%)\nP103: The native language of [X] is [Y] . English (53.1%) French (12.8%) German (3.2%) French (7.7%) English (4.2%) Spanish (3.9%)\nP364: The original language of [X] is [Y] . English (47.2%) French (0.7%) German (0.2%) Arabic (30.8%) French (5.7%) Shilha (5.1%)\nP1412: [X] used to communicate in [Y] . French (12.6%) German (7.7%) Spanish (3.4%) Arabic (55.9%) German (8.3%) French (3.4%)\nRelation predicate Common correct predictions Common wrong predictions\n(% of predictions) (% of predictions)\nProbing BERT-base with English promptspopulated with Arab facts\nP17: [X] is located in [Y] . Algeria (9.0%) Egypt (8.6%) Iraq (4.6%) Bahrain (8.6%) Moscow(4.1%) Lebanon(3.8%)\nP19: [X] was born in [Y] . Cairo (3.8%) Baghdad(3.0%) Damascus(0.5%) Baghdad(31.1%) Cairo (18.1%) Paris (6.4%)\nP20: [X] died in [Y] . Cairo (10.9%) Baghdad(2.0%) Egypt (0.7%) Cairo (45.9%) Paris (19.2%) Baghdad(8.0%)\nP27: [X] is [Y] citizen . France (1.8%) Qatar (1.3%) Israel (0.4%) Qatar (73.9%) Pakistan(8.8%) Israel (2.8%)\nP495: [X] was created in [Y] . Egypt (10.0%) Algeria (1.1%) Iraq (0.7%) Japan (25.2%) India (12.2%) Egypt (9.2%)\nP103: The native language of [X] is [Y] . Arabic (66.2%) French (0.8%) English (0.4%) Arabic (12.2%) Urdu (6.5%) Kurdish (4.3%)\nP364: The original language of [X] is [Y] . Arabic (30.7%) English (3.5%) French (1.6%) English (44.6%) French (4.3%) Hindi (2.1%)\nP1412: [X] used to communicate in [Y] . Arabic (78.3%) English (2.8%) French (1.8%) Arabic (8.5%) English (4.5%) Urdu (1.2%)\nProbing BERT-base with English promptspopulated with Western facts\nP17: [X] is located in [Y] . France (15.7%) Spain (10.3%) Germany(8.2%) Georgia (10.1%) Moscow(9.1%) Canada (6.0%)\nP19: [X] was born in [Y] . Paris (3.4%) Berlin (0.9%) London (0.7%) Chicago(25.4%) London (22.1%) Paris (9.5%)\nP20: [X] died in [Y] . Paris (9.4%) London (2.7%) Rome (2.6%) Paris (29.0%) London (22.9%) Rome (8.3%)\nP27: [X] is [Y] citizen . France (11.0%) Italy (2.5%) Canada (1.1%) British America(44.5%) Austria (8.5%) Canada (6.8%)\nP495: [X] was created in [Y] . France (2.2%) Germany(1.4%) Japan (0.6%) Japan (61.3%) England(10.1%) India (4.6%)\nP103: The native language of [X] is [Y] . English (50.8%) French (13.3%) German (3.3%) Spanish (6.8%) German (3.6%) French (3.2%)\nP364: The original language of [X] is [Y] . English (85.4%) French (1.5%) German (0.9%) Latin (2.1%) English (2.0%) French (1.9%)\nP1412: [X] used to communicate in [Y] . English (59.8%) French (13.1%) German (7.3%) English (3.6%) Spanish (2.2%) Arabic (1.2%)\nTable 3: The most common predictions for monolingual arBERT and BERT-base models when probed by DLAMA-\nv1 (Arab-West) with English and Arabic prompts respectively. Purple culturally related prediction, Blue bell\nculturally proximate prediction, Light Orange culturally proximate prediction to another culture, Orange cultur-\nally related prediction to the other culture. Note: The Arabic prompts/entities are translated for clarity.\n6253\nLimitations\nWe acknowledge that the methodology used to\nbuild DLAMA-v1 still has limitations related to\nthe information within its relation triples. While\ndirectly querying Wikidata as a dynamic source of\nfacts provides the ﬂexibility needed to acquire data\nthat is relevant to different cultures (as opposed to\nusing the static T-REx dump of triples), the diver-\nsity of the triples that are compiled depends on the\navailability of a diverse set of facts on Wikidata in\nthe ﬁrst place. For instance, the smaller number\nof relation triples related to Arab countries for the\npredicates (P136 - Genre), (P190 - Sister city), and\n(P449 - Original network) in DLAMA-v1 (Arab-\nWest) demonstrates the difﬁculty of querying the\nexact number of facts for both cultures despite us-\ning exactly the same queries with the only differ-\nence being limiting the region to which the triples\nbelong. Another limitation is the inability to enu-\nmerate valid and ﬁne-grained subclasses of objects\nfor speciﬁc subjects, if these ﬁne-grained objects\nare not on Wikidata. Steps #3 and #5 of DLAMA\nexplained in § 4.1 ensure that a possible and more\ngeneral object is still valid for a speciﬁc subject.\nHowever, inferring a more speciﬁed object from a\ngeneric one is impossible. For example, the fact\nthat someone speaks “American English\" implies\nthat they speak English as well, but knowing that\nsomeone speaks “English\" is not enough to spec-\nulate about their dialect (i.e.: “American English\",\n“British English\", etc.).\nWhile the triples within DLAMA are sampled\nby picking the ones whose subjects have the largest\nWikipedia articles’ sizes, the infeasibility of man-\nually reviewing the large number of diverse facts\nwithin DLAMA-v1 makes it hard to claim that\nthe facts are free of inaccuracies or missing infor-\nmation. More broadly, DLAMA supports relations\npredicates that are already part of mLAMA to fairly\ncompare the results on DLAMA to those previously\nreported on mLAMA. Moreover, we make sure that\nthe subjects and the objects of the relation triples\nare available in the different languages of interest.\nHaving these constraints might imply that some\nculturally relevant facts might have been dropped\nout of DLAMA-v1 (e.g., Predicates that are not\npart of mLAMA, or triples having missing labels\nin one of the languages of interest).\nLastly, we used mLAMA’s probing setup in\nwhich the models rank a predeﬁned set of objects\nfor each prompt. Their prediction is correct if the\ntop-ranked object is one of the valid labels for the\ncorresponding relation triple used to populate the\nprompt. Therefore, a model’s performance is ex-\npected to be higher than that achieved by a genera-\ntive setup in which the model is asked to generate\nthe most probable completions for the masked to-\nkens.\nEthics Statement\nWe believe that using a set of countries to represent\ncultures is just a proxy for acquiring a more diverse\nset of facts that are less skewed toward a speciﬁc\nculture. More speciﬁcally, using the terms Arab\ncultures, Western cultures, and Asian cultures sim-\npliﬁes the differences between the cultures within\nthe countries that we have used to represent these\nmacro-cultures. On the other hand, we still think\nthat the differences between Asian cultures are less\nsubtle than between them and Western cultures.\nWe also acknowledge that the accuracy and valid-\nity of some relation triples queried from Wikidata\nmight be biased by the views of the people who\nadded such information to Wikidata. This might\nbe particularly vibrant for relation triples related to\nzones with political/ sectarian wars and conﬂicts.\nAcknowledgments\nThis work was supported in part by the UKRI Cen-\ntre for Doctoral Training in Natural Language Pro-\ncessing, funded by the UKRI (grant EP/S022481/1)\nand the University of Edinburgh, School of Infor-\nmatics. Amr is grateful to Matthias Lindemann\nfor recommending Wikidata, Aida Tarighat for\nthe early discussions about the benchmark, Lau-\nrie Burchell, Bálint Gyevnár, and Shangmin Guo\nfor reviewing the manual prompts, Coleman Ha-\nley for the multiple discussions about the ﬁgures,\nAnna Kapron-King and Gautier Dagan for proof-\nreading the abstract, and lastly, Dilara Keküllüo ˘glu\nand Björn Ross for their valuable reviews of the\npaper’s ﬁnal draft.\nReferences\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT &\nMARBERT: Deep bidirectional transformers for Ara-\nbic. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n7088–7105, Online. Association for Computational\nLinguistics.\n6254\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAmy S Bruckman. 2022. Should You Believe\nWikipedia?: Online Communities and the Construc-\ntion of Knowledge . Cambridge University Press.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases . In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 1860–1874, Online.\nAssociation for Computational Linguistics.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC at ICLR 2020 .\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways . arxiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Amir\nFeder, Abhilasha Ravichander, Marius Mosbach,\nYonatan Belinkov, Hinrich Schütze, and Yoav Gold-\nberg. 2023. Measuring causal effects of data statistics\non language model’s ‘factual’ predictions.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models . Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-REx: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nConstanza Fierro and Anders Søgaard. 2022. Factual\nconsistency of multilingual pretrained language mod-\nels. In Findings of the Association for Computational\nLinguistics: ACL 2022 , pages 3046–3052, Dublin,\nIreland. Association for Computational Linguistics.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. ArXiv, abs/1901.05287.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics ,\npages 3651–3657, Florence, Italy. Association for\nComputational Linguistics.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020. X-FACTR:\nMultilingual factual knowledge retrieval from pre-\ntrained language models . In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , pages 5943–5959, On-\nline. Association for Computational Linguistics.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating knowledge\nin multilingual pretrained language models . In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3250–3258, Online.\nAssociation for Computational Linguistics.\nKiyoung Kim. 2020. Pretrained language models for ko-\nrean. https://github.com/kiyoungkim1/LMkor.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) ,\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100 .\n6255\nA Detailed Bias Values within the Factual\nKnowledge Benchmarks\nTable A1 provides the ﬁne-grained percentages for\nthe distribution of the triples of T-REx, LAMA,\nand X-FACTR for 21 Western countries as com-\npared to the rest of the world. For most of the\nrelation predicates, triples related to one of the 21\nWestern countries represent more than 50% of the\ntotal triples. We ﬁnd that this skewness is even\nlarger for LAMA, and X-FACTR than for T-REx.\nTriples within LAMA are restricted to the ones\nwhose objects are tokenized into a single subword\nby monolingual language models. This ﬁltering\nmight be responsible for the increased skewness of\nLAMA toward facts from Western countries.\nB Augmenting the correct objects within\nDLAMA\nFor each relation predicate, a graph is used to\nmodel all the subclass-superclass relations between\nthe objects of the queried triples. The edges within\nthe graph are built using Wikidata’s P279 (subclass\nof) predicate. All the possible subclass/superclass\nrelations between the list of objects for each rela-\ntion predicate are queried and then used to form the\nedges of the graph. Afterward, the list of objects for\neach subject is augmented by the list of all the pos-\nsible ancestors (superclasses) of these objects (e.g.,\nThe ofﬁcial languages of The United States of\nAmerica are now set to American English and\nEnglish instead of just American English).\nSimilarly, we noticed that the level of speciﬁcity\nof places of birth (objects of P19) and places of\ndeath (objects of P20) varies between different tu-\nples. Thus, we queried all the territorial entities\nin which the places of birth and death are located.\nFor instance, Paris Hilton had the place of birth\nset to {New York City} while Donald Trump\nhad the place of birth set to {Jamaica Hospital\nMedical Center} . After querying the higher\nadministrative-territorial entities, the set of valid\nobjects for both entities became {New York City,\nNew York, United States of America} and\n{Jamaica Hospital Medical Center, Queens,\nNew York City, New York, United States of\nAmerica} respectively.\nC Results on Raw Triples before the Last\nOptional Step\nTo demonstrate the impact of the last optional step\nwithin DLAMA, we evaluate the PLMs on the\ntriples before augmenting their objects with valid\noverlapping ones (i.e.: before applying the optional\nStep #5 of the framework). It is clear that the per-\nformance of the models shown in Table C2 is worse\nthan their performance on the augmented bench-\nmark previously listed in Table 1.\nD GPT3.5 performance on a subset of\nDLAMA (Arab-West)\nAs mentioned in § 5.4, we used OpenAI’s API\nto evaluate the performance of the GPT3.5-turbo\nmodel on six predicates of DLAMA-v1 (Arab-\nWest). The accuracy scores of the model for these\npredicates are reported in Table D3. We plan to\nextend our evaluation to cover more predicates and\ninclude other LLMs.\nE Model diagnostics using the (Asia-West)\nand (South America-West) sets\nContrasting KyKim BERT to English BERT-\nbase: We replicate the analysis process done in\n§5.3 to investigate the behavior of KyKim BERT-\nbase and the English BERT-base models using Ta-\nbles E4 and E6. We ﬁnd that the English BERT-\nbase has the same patterns detailed before for P17,\nP103, P364, and P495. Moreover, since English\nBERT-base overgenerates Japan for the P495 predi-\ncate, its performance on the Asian part of DLAMA-\nv1 (Asia-West) is high. This once again shows the\nimportance of having two contrasting sets of facts\nfrom the same predicates. Despite the fact that the\nmajority of triples of P495 within the Asian part of\nDLAMA-v1 (Asia-West) has Japan as one of the\ncorrect labels, a biased model toward predicting\nJapan has a signiﬁcantly low performance on the\nopposing set of facts. Consequently, the bias can\nstill be detected.\nRegarding the KyKim BERT-base model, lan-\nguage bias toward overpredicting Korean is clear\nfor the P103 and the P364 relation predications.\nThe model also shows a bias toward the Javanese la-\nbel for P1412. This bias can be seen in the model’s\npoor performance on the Western part of the bench-\nmark. P19 is a relation predicate on which the\nmodel is generally performing well. The most fre-\nquent predictions indicate that the model leans to-\nward selecting Japan and United States of America .\nHowever, the model’s predictions change according\nto the underlying culture of the triples and hence\ndemonstrate an ability to memorize facts from both\ncultures.\n6256\nWikidata predicate T-REx LAMA X-FACTR\nWestern countries Rest of the world Western countries Rest of the world Western countries Rest of the world\nP17 (Country) 321988 (44.0%) 410512 (56.0%) 386 (41.6%) 542 (58.4%) 457 (45.7%) 543 (54.3%)\nP19 (Place of birth) 156579 (68.2%) 73069 (31.8%) 730 (77.4%) 213 (22.6%) 680 (68.0%) 320 (32.0%)\nP20 (Place of death) 63250 (76.0%) 19962 (24.0%) 734 (77.2%) 217 (22.8%) 757 (75.7%) 243 (24.3%)\nP27 (Country of citizenship) 253402 (63.8%) 143837 (36.2%) 404 (41.9%) 560 (58.1%) 623 (62.3%) 377 (37.7%)\nP36 (Capital) 4011 (44.8%) 4936 (55.2%) 436 (62.0%) 267 (38.0%) 418 (41.8%) 582 (58.2%)\nP39 (Position held) 7610 (50.1%) 7581 (49.9%) 380 (42.6%) 512 (57.4%) 504 (50.4%) 496 (49.6%)\nP47 (Shares border with) 29427 (76.6%) 9010 (23.4%) 529 (57.4%) 393 (42.6%) 762 (76.2%) 238 (23.8%)\nP101 (Field of work) 3396 (54.1%) 2885 (45.9%) 365 (52.5%) 330 (47.5%) 539 (53.9%) 461 (46.1%)\nP103 (Native language) 6983 (78.4%) 1926 (21.6%) 778 (79.7%) 198 (20.3%) 787 (78.7%) 213 (21.3%)\nP106 (Occupation) 203644 (58.7%) 143177 (41.3%) 631 (65.9%) 327 (34.1%) 578 (57.8%) 422 (42.2%)\nP108 (Employer) 27119 (91.2%) 2605 (8.8%) 371 (96.9%) 12 (3.1%) 910 (91.0%) 90 (9.0%)\nP131 (Located in the administrative territorial entity)264544 (57.7%) 194254 (42.3%) 704 (79.9%) 177 (20.1%) 552 (55.2%) 448 (44.8%)\nP136 (Genre) 16396 (17.0%) 80156 (83.0%) 547 (58.8%) 384 (41.2%) 183 (18.3%) 817 (81.7%)\nP140 (Religion) 3344 (45.5%) 4000 (54.5%) 70 (14.8%) 403 (85.2%) 480 (48.0%) 520 (52.0%)\nP159 (Headquarters location) 24841 (69.7%) 10824 (30.3%) 683 (70.7%) 283 (29.3%) 685 (68.5%) 315 (31.5%)\nP190 (Sister city) 2026 (54.1%) 1722 (45.9%) 525 (52.8%) 470 (47.2%) 542 (54.2%) 458 (45.8%)\nP276 (Location) 9239 (65.5%) 4867 (34.5%) 554 (57.9%) 403 (42.1%) 638 (63.8%) 362 (36.2%)\nP413 (Position played on team / speciality) 18307 (65.9%) 9482 (34.1%) 751 (78.9%) 201 (21.1%) 667 (66.7%) 333 (33.3%)\nP463 (Member of) 13832 (80.0%) 3452 (20.0%) 112 (49.8%) 113 (50.2%) 807 (80.7%) 193 (19.3%)\nP495 (Country of origin) 64856 (81.7%) 14518 (18.3%) 430 (47.4%) 478 (52.6%) 838 (83.8%) 162 (16.2%)\nP530 (Diplomatic relation) 888 (63.7%) 505 (36.3%) 645 (64.8%) 351 (35.2%) 633 (63.3%) 367 (36.7%)\nP740 (Location of formation) 7844 (82.5%) 1663 (17.5%) 782 (83.7%) 152 (16.3%) 836 (83.6%) 164 (16.4%)\nP937 (Work location) 6018 (79.7%) 1535 (20.3%) 813 (85.2%) 141 (14.8%) 801 (80.1%) 199 (19.9%)\nP1001 (Applies to jurisdiction) 2397 (60.9%) 1542 (39.1%) 436 (62.2%) 265 (37.8%) 612 (61.2%) 388 (38.8%)\nP1376 (Capital of) 1342 (30.4%) 3078 (69.6%) 79 (33.8%) 155 (66.2%) 297 (29.7%) 703 (70.3%)\nP1412 (Languages spoken or published) 42318 (71.2%) 17137 (28.8%) 722 (74.5%) 247 (25.5%) 728 (72.8%) 272 (27.2%)\nTotal 1555601 (57.1%) 1168235 (42.9%) 13597 (63.6%) 7794 (36.4%) 16314 (62.7%) 9686 (37.3%)\nTable A1: The number and percentage of triples belonging to one of the 21 Western countries or to other countries in the T-REx, LAMA, and X-FACTR benchmarks.\n6257\nLanguage Model nameP@1 P@1 P@1\nof Prompt Arab W est All\nN=10946N=13589N=24535\nArabic mBERT-base 11.4 12.8 12.2\narBERT 26.6 19.3 22.6\nEnglish mBERT-base 19.1 34.2 27.5\nBERT-base 24.5 29.9 27.5\n(a) DLAMA-v1 (Arab-West)\nLanguage Model nameP@1 P@1 P@1\nof Prompt Asia W est All\nN=13479N=13588N=27067\nKorean mBERT-base 15.0 22.6 18.8\nKyKim 16.0 11.8 13.9\nEnglish mBERT-base 27.1 36.2 31.7\nBERT-base 36.4 30.4 33.4\n(b) DLAMA-v1 (Asia-West)\nLanguage Model nameP@1 P@1 P@1\nof Prompt S.America W est All\nN=13071 N=13586N=26657\nSpanish mBERT-base 22.3 30.4 26.4\nBETO 15.5 25.5 20.6\nEnglish mBERT-base 24.1 34.7 29.5\nBERT-base 24.4 29.9 27.2\n(c) DLAMA-v1 (South America-West)\nTable C2: Performance of mBERT, and monolingual\nArabic (arBERT), Korean (KyKim), Spanish (BETO),\nand English (BERT-base) language models on the three\nsets of facts of DLAMA-v1 without augmenting the set\nof objects (i.e.: without applying Step #5).\nContrasting Spanish BETO to English BERT-\nbase: While similar patterns can be found in Ta-\nbles E5, and E7, a new subtle bias is that BERT-\nbase predicts Madrid for more than 50% of the\nSouth American triples in P19 (Place of Birth), and\nP20 (Place of Death) predicates. This might be\nattributed to the fact that South American names\nare hard to distinguish from Spanish ones.\nF Details of DLAMA\nWikipedia sites: For the Arab, Asian, South Amer-\nican, and Western cultures, representative countries\nfrom each region are used as a proxy. Table F8 enu-\nRelation\n# facts (entropy) Arabic prompts English prompts\nAccuracy Accuracy\nArab West Arab West Arab West\nP30 (Continent) 22 (1.0) 19 (1.0) 63.6 89.5* 100.0* 89.5\nP36 (Capital) 22 (4.5) 19 (4.2) 81.8* 63.2 95.5* 94.7\nP37 (Ofﬁcial language) 22 (0.0) 19 (2.5) 100.0* 89.5 100.0* 100.0*\nP47 (Shares border with) 22 (2.5) 19 (2.7) 100.0* 100.0* 95.5* 89.5\nP190 (Sister city) 67 (4.9) 468 (7.3) 6.0* 5.6 3.0 33.1*\nP530 (Diplomatic relation) 22 (0.0) 19 (0.0) 63.6 68.4* 50.0 84.2*\nP1376 (Capital of) 24 (4.3) 26 (4.0) 87.5 88.5* 100.0* 92.3\nTable D3: The accuracy of the GPT3.5-turbo model for\nsome predicates of the DLAMA-v1 (Arab-West) set.\nmerates the countries representing these cultures\nand their relevant respective Wikipedia sites.\nProbing templates: To probe the models’ factual\nknowledge, natural language templates are used to\ntransform the triples into prompts. The template\nhas two ﬁelds for the subject [X] and the object\n[Y ] of the triples. For each triple, the subject ﬁlls\nthe subject ﬁeld while the object ﬁeld is masked.\nModels are then fed the prompts and asked to ﬁll\nin the masked token (i.e., the object). While the\ntemplates can affect the predictions of the models,\nwe used the same ones of mLAMA listed in Ta-\nble F9 to control for the impact that changing the\ntemplates might have on the results. In addition\nto that, we mapped the templates into questions as\nshown in Table F10 to evaluate the performance\nof the GPT3.5 model on a subset of DLAMA-v1\n(Arab-West).\n6258\nRelation\nKorean prompts English prompts\n# facts (entropy) P@1 P@1\nAsia West Asia West Asia West\nP17 (Country) 1000 (2.2) 1000 (2.8) 37.8 42.1 67.1 45.3\nP19 (Place of birth) 1000 (1.7) 1000 (2.7) 63.1 55.8 24.3 11.9\nP20 (Place of death) 1000 (2.6) 1000 (2.8) 23.0 45.8 40.4 20.7\nP27 (Country of citizenship) 1000 (1.5) 1000 (2.4) 74.0 53.5 71.8 19.5\nP30 (Continent) 13 (0.0) 19 (1.0) 76.9 31.6 100.0 84.2\nP36 (Capital) 13 (3.7) 19 (4.2) 30.8 21.1 69.2 84.2\nP37 (Ofﬁcial language) 13 (2.7) 19 (2.5) 30.8 26.3 84.6 100.0\nP47 (Shares border with) 13 (1.7) 19 (2.7) 0.0 0.0 76.9 78.9\nP103 (Native language) 1000 (1.6) 1000 (1.7) 33.3 2.3 84.7 75.6\nP106 (Occupation) 1000 (0.9) 1000 (1.0) 17.0 9.4 1.4 15.9\nP136 (Genre) 1000 (1.0) 1000 (2.5) 0.2 0.5 0.8 6.3\nP190 (Sister city) 387 (7.4) 467 (7.3) 0.0 1.9 0.3 2.8\nP264 (Record label) 1000 (5.3) 1000 (4.8) 0.3 0.1 3.3 6.6\nP364 (Original language of work) 1000 (0.7) 1000 (0.3) 10.5 18.5 37.7 89.1\nP449 (Original network) 1000 (4.6) 1000 (5.0) 5.1 0.2 1.1 10.7\nP495 (Country of origin) 1000 (0.5) 1000 (1.3) 29.1 19.2 79.7 4.3\nP530 (Diplomatic relation) 13 (0.0) 19 (0.0) 7.7 5.3 46.2 68.4\nP1303 (Instrument) 1000 (0.5) 1000 (1.1) 0.4 1.1 9.0 29.5\nP1376 (Capital of) 27 (3.0) 26 (4.0) 51.9 26.9 88.9 76.9\nP1412 (Languages spoken or published) 1000 (1.3) 1000 (1.4) 1.0 13.4 87.4 86.8\nAggregated statistics 13479 (2.1) 13588 (2.6) 22.1 19.5 38.3 31.9\nTable E4: Detailed P@1 scores of KyKim (Korean prompts) and cased BERT-base (English prompts) on the\nDLAMA-v1 (Asia-West) set.\nRelation\nSpanish prompts English prompts\n# facts (entropy) P@1 P@1\nS.America West S.America West S.America West\nP17 (Country) 1000 (2.8) 1000 (2.9) 57.5 47.7 63.0 49.9\nP19 (Place of birth) 1000 (2.6) 1000 (2.5) 2.0 0.9 14.6 8.3\nP20 (Place of death) 1000 (2.8) 1000 (2.4) 0.1 0.6 0.5 10.3\nP27 (Country of citizenship) 1000 (2.5) 1000 (2.4) 19.5 4.2 28.9 14.5\nP30 (Continent) 12 (0.0) 19 (1.0) 91.7 73.7 100.0 73.7\nP36 (Capital) 12 (3.6) 19 (4.2) 83.3 68.4 66.7 84.2\nP37 (Ofﬁcial language) 12 (1.2) 19 (2.5) 75.0 84.2 75.0 100.0\nP47 (Shares border with) 12 (1.0) 19 (2.7) 83.3 68.4 91.7 78.9\nP103 (Native language) 1000 (1.1) 1000 (1.8) 34.4 78.6 58.5 74.5\nP106 (Occupation) 1000 (2.1) 1000 (2.5) 6.8 7.8 8.3 12.0\nP136 (Genre) 1000 (2.6) 1000 (2.4) 0.3 1.7 2.4 5.5\nP190 (Sister city) 144 (6.1) 465 (7.4) 4.9 1.7 3.5 3.0\nP264 (Record label) 854 (6.1) 1000 (6.0) 0.0 0.1 1.5 5.6\nP364 (Original language of work) 1000 (1.1) 1000 (0.6) 48.5 85.1 60.5 89.5\nP449 (Original network) 1000 (4.6) 1000 (4.7) 0.3 0.7 0.4 18.7\nP495 (Country of origin) 1000 (2.4) 1000 (1.8) 6.3 60.0 27.3 10.3\nP530 (Diplomatic relation) 12 (0.0) 19 (0.0) 66.7 68.4 58.3 68.4\nP1303 (Instrument) 1000 (1.2) 1000 (1.3) 6.7 11.7 17.0 26.4\nP1376 (Capital of) 13 (3.4) 26 (4.0) 84.6 73.1 84.6 76.9\nP1412 (Languages spoken or published) 1000 (1.2) 1000 (1.7) 20.2 51.6 62.9 89.2\nAggregated statistics 13071 (2.4) 13586 (2.7) 16.0 26.5 26.9 31.3\nTable E5: Detailed P@1 scores of cased BETO (Spanish prompts) and cased BERT-base (English prompts) on the\nDLAMA-v1 (South America-West) set.\n6259\nRelation predicate Common correct predictions Common wrong predictions\n(% of predictions) (% of all predictions)\nProbing KyKim with Korean promptspopulated with Asian facts\nP17: [X] is located in [Y] . Japan(30.3%)South Korea(3.0%)Thailand(0.9%) China(13.8%)United States of America(13.0%)Tonga(9.7%)\nP19: [X] was born in [Y] . Japan(60.1%)South Korea(1.9%)South Chungcheong Province(0.4%) United States of America(12.8%)South Chungcheong Province(4.0%)South Jeolla(3.2%)\nP20: [X] died in [Y] . Japan(19.2%)Tokyo(2.7%)Gyeonggi Province(0.3%) United States of America(19.8%)Gyeonggi Province(14.5%)Germany(4.7%)\nP27: [X] is [Y] citizen . Japan(70.2%)South Korea(3.2%)Singapore(0.2%) Korea(13.5%)South Korea(5.0%)China(3.0%)\nP495: [X] was created in [Y] . Japan(26.2%)South Korea(2.9%) Jordan(29.7%)South Korea(28.0%)United States of America(6.4%)\nP103: The native language of [X] is [Y] . Korean(31.5%)Japanese(1.5%)Chinese(0.2%) Korean(66.5%)Hakka(0.1%)Chinese(0.1%)\nP364: The original language of [X] is [Y] . Korean(6.0%)Japanese(4.3%)Chinese(0.1%) Korean(79.9%)English(8.1%)German(0.4%)\nP1412: [X] used to communicate in [Y] . Vietnamese(0.4%)Javanese(0.3%)Japanese(0.1%) Javanese(77.2%)Tamil(4.3%)Wu Chinese(3.9%)\nProbing KyKim with Korean promptspopulated with Western facts\nP17: [X] is located in [Y] . United States of America(28.2%)France(4.9%)Germany(4.0%) United States of America(27.4%)Korea(7.6%)China(5.7%)\nP19: [X] was born in [Y] . United States of America(42.0%)France(5.6%)Italy (4.4%) United States of America(27.0%)Italy (7.7%)Germany(6.8%)\nP20: [X] died in [Y] . United States of America(29.2%)Germany(7.0%)France(5.4%) Germany(22.2%)United States of America(17.7%)Italy (3.2%)\nP27: [X] is [Y] citizen . United States of America(37.7%)France(11.8%)Italy (1.8%) United States of America(15.3%)France(10.5%)Korea(9.3%)\nP495: [X] was created in [Y] . United States of America(17.9%)Germany(0.4%)Japan(0.4%) South Korea(36.4%)Jordan(21.4%)Japan(12.8%)\nP103: The native language of [X] is [Y] . English(2.0%)French(0.3%) Korean(97.0%)English(0.4%)Japanese(0.2%)\nP364: The original language of [X] is [Y] . English(18.0%)Korean(0.3%)French(0.1%) Korean(79.3%)English(0.8%)French(0.7%)\nP1412: [X] used to communicate in [Y] . French(7.4%)German(4.9%)Spanish(0.5%) Javanese(51.2%)German(9.3%)Burmese(9.0%)\nRelation predicate Common correct predictions Common wrong predictions\n(% of predictions) (% of all predictions)\nProbing BERT-base with English promptspopulated with Asian facts\nP17: [X] is located in [Y] . Japan(48.9%)Thailand(3.4%)Taiwan(3.2%) China(7.5%)Moscow(5.6%)Taiwan(3.2%)\nP19: [X] was born in [Y] . Tokyo(17.8%)Seoul(2.4%)Vietnam(1.0%) Tokyo(52.3%)Seoul(7.0%)Beijing(4.1%)\nP20: [X] died in [Y] . Tokyo(26.3%)Beijing(5.8%)Seoul(4.8%) Beijing(21.0%)Tokyo(16.6%)Paris (7.8%)\nP27: [X] is [Y] citizen . Japan(67.4%)Taiwan(2.4%)Vietnam(1.0%) Taiwan(12.8%)Singapore(5.4%)Korea(4.6%)\nP495: [X] was created in [Y] . Japan(79.5%)Vietnam(0.1%)Thailand(0.1%) Japan(5.3%)India (3.5%)Germany(3.0%)\nP103: The native language of [X] is [Y] . Japanese(52.4%)Korean(26.4%)Chinese(3.8%) English(4.2%)Spanish(1.6%)Wu Chinese(1.5%)\nP364: The original language of [X] is [Y] . Japanese(34.6%)English(2.3%)Chinese(0.3%) English(50.2%)French(1.8%)Latin (1.5%)\nP1412: [X] used to communicate in [Y] . Japanese(73.5%)Korean(6.8%)Chinese(2.4%) English(5.6%)Cantonese(2.2%)Japanese(1.5%)\nProbing BERT-base with English promptspopulated with Western facts\nP17: [X] is located in [Y] . France(15.5%)Germany(8.9%)Spain(8.3%) Georgia(12.6%)Moscow(7.7%)Canada(6.3%)\nP19: [X] was born in [Y] . Paris (4.5%)London(1.2%)Rome(1.0%) Chicago(24.2%)London(21.4%)Paris (10.5%)\nP20: [X] died in [Y] . Paris (11.2%)Rome(3.7%)London(2.5%) Paris (30.7%)London(19.9%)Rome(9.4%)\nP27: [X] is [Y] citizen . France(11.8%)Italy (3.4%)Canada(1.0%) British America(35.8%)Singapore(19.7%)Austria(4.9%)\nP495: [X] was created in [Y] . France(1.5%)Germany(1.0%)Japan(0.5%) Japan(60.2%)England(10.7%)Germany(5.0%)\nP103: The native language of [X] is [Y] . English(53.1%)French(12.6%)German(3.4%) Spanish(6.9%)French(3.9%)German(3.7%)\nP364: The original language of [X] is [Y] . English(87.0%)French(0.7%)German(0.5%) Latin (2.3%)English(1.9%)French(1.7%)\nP1412: [X] used to communicate in [Y] . English(61.7%)French(12.1%)German(4.3%) English(4.6%)Spanish(2.4%)Arabic(1.4%)\nTable E6: The most common predictions for monolingual Korean and English BERT models when probed by\nDLAMA-v1 (Asia-West) with English and Korean prompts, respectively. Purple culturally related prediction,\nBlue bell culturally proximate prediction, Light Orange culturally proximate prediction to another culture,\nOrange culturally related prediction to the other culture. Note: The Korean prompts/entities are translated for\nclarity.\n6260\nRelation predicate Common correct predictions Common wrong predictions\n(% of predictions) (% of all predictions)\nProbing BETO with Spanish promptspopulated with South America facts\nP17: [X] is located in [Y] . Brazil (17.6%)Argentina(14.9%)Chile (7.9%) Mexico(12.0%)Curaçao(8.1%) Venezuela(4.3%)\nP19: [X] was born in [Y] . Buenos Aires(1.5%) Lima (0.2%) Brazil (0.1%) Altötting(91.0%)Buenos Aires(5.6%) Madrid (0.3%)\nP20: [X] died in [Y] . Aripuanã(0.1%) Aripuanã(99.6%)Buenos Aires(0.1%) Caracas(0.1%)\nP27: [X] is [Y] citizen . Brazil (13.0%)Colombia(4.3%) Chile (1.4%) Colombia(39.7%)Taiwan(9.0%) Mexico(5.5%)\nP495: [X] was created in [Y] . Argentina(1.3%) Chile (1.2%) Brazil (1.1%) United States of America(29.6%)Río de la Plata(23.4%)Kingdom of Portugal(16.6%)\nP103: The native language of [X] is [Y] . Spanish(17.6%)Portuguese(15.4%)English(1.4%) English(48.2%)Spanish(14.1%)French (2.2%)\nP364: The original language of [X] is [Y] . Spanish(39.1%)Portuguese(7.5%) English(1.9%) English(36.1%)Spanish(13.7%)French (0.5%)\nP1412: [X] used to communicate in [Y] . Spanish(13.4%)English(6.5%) Portuguese(0.2%) English(70.6%)Spanish(5.8%) Latin (2.6%)\nProbing BETO with Spanish promptspopulated with Western facts\nP17: [X] is located in [Y] . France (13.1%)Spain (10.6%)United States of America(10.2%) Mexico(15.5%)United States of America(5.7%) Canada(3.2%)\nP19: [X] was born in [Y] . Paris (0.5%) Rome (0.2%) Altötting(0.1%) Altötting(95.6%)Paris (2.4%) Rome (0.4%)\nP20: [X] died in [Y] . Paris (0.3%) Rome (0.2%) Madrid (0.1%) Aripuanã(98.8%)Paris (0.2%) Rome (0.1%)\nP27: [X] is [Y] citizen . France (1.4%) Italy (1.1%) Spain (0.5%) Taiwan(25.3%)Australia(21.7%)Socialist Republic of Romania(7.3%)\nP495: [X] was created in [Y] . United States of America(54.6%)France (2.6%) Spain (2.3%) United States of America(12.5%)Kingdom of Portugal(5.1%) Río de la Plata(4.9%)\nP103: The native language of [X] is [Y] . English(63.3%)French (10.7%)German(1.7%) English(18.4%)French (1.2%) Spanish(1.2%)\nP364: The original language of [X] is [Y] . English(80.4%)Spanish(2.8%) Italian (0.7%) Spanish(10.5%)English(3.5%) French (0.3%)\nP1412: [X] used to communicate in [Y] . English(41.3%)French (6.0%) German(2.9%) English(43.6%)Spanish(3.1%) Latin (1.3%)\nRelation predicate Common correct predictions Common wrong predictions\n(% of predictions) (% of all predictions)\nProbing BERT-base with English promptspopulated with South American facts\nP17: [X] is located in [Y] . Brazil (18.1%)Argentina(16.3%)Chile (8.1%) Bolivia(6.5%) Mexico(5.1%) Spain (3.5%)\nP19: [X] was born in [Y] . Brazil (14.0%)Argentina(0.3%) Bolivia(0.1%) Madrid (54.1%)Rome (6.4%) Milan (3.9%)\nP20: [X] died in [Y] . Peru (0.2%) Brazil (0.2%) London(0.1%) Madrid (55.1%)Paris (16.8%)Rome (11.4%)\nP27: [X] is [Y] citizen . Brazil (18.0%)Argentina(9.9%) Italy (0.3%) Mexico(25.0%)Argentina(15.8%)Honduras(6.0%)\nP495: [X] was created in [Y] . Brazil (19.7%)Argentina(2.2%) Chile (1.8%) Mexico(24.4%)Japan (11.3%)Spain (8.7%)\nP103: The native language of [X] is [Y] . Portuguese(34.2%)Spanish(23.2%)English(0.6%) Spanish(20.5%)Italian (4.8%) English(3.6%)\nP364: The original language of [X] is [Y] . Spanish(40.7%)Portuguese(17.1%)English(2.6%) English(19.4%)Spanish(6.6%) Latin (5.5%)\nP1412: [X] used to communicate in [Y] . Spanish(44.7%)Portuguese(14.5%)English(2.6%) Spanish(16.4%)English(12.0%)Italian (3.0%)\nProbing BERT-base with English promptspopulated with Western facts\nP17: [X] is located in [Y] . France (16.9%)Spain (12.7%)Germany(7.9%) Georgia(9.5%) Canada(5.1%) Lebanon(2.9%)\nP19: [X] was born in [Y] . Berlin (2.3%) Paris (1.7%) London(1.7%) Berlin (36.7%)Chicago(13.0%)London(12.9%)\nP20: [X] died in [Y] . Paris (5.1%) London(1.8%) Rome (1.1%) Munich(23.0%)Paris (22.8%)Berlin (15.0%)\nP27: [X] is [Y] citizen . France (8.6%) Austria (2.0%) Italy (1.9%) Austria (36.8%)British America(26.1%)Netherlands(4.2%)\nP495: [X] was created in [Y] . France (4.8%) Spain (1.6%) Germany(1.5%) Japan (53.1%)England(10.3%)Germany(5.3%)\nP103: The native language of [X] is [Y] . English(48.8%)French (15.1%)German(3.4%) Spanish(7.4%) German(3.7%) French (3.0%)\nP364: The original language of [X] is [Y] . English(83.8%)Spanish(2.9%) German(1.3%) English(2.3%) Latin (2.0%) French (1.8%)\nP1412: [X] used to communicate in [Y] . English(38.8%)German(34.0%)French (10.1%) English(5.4%) Spanish(1.3%) German(0.8%)\nTable E7: The most common predictions for monolingual Spanish and English BERT models when probed by\nDLAMA-v1 (South America-West) with English and Spanish prompts, respectively. Purple culturally related\nprediction, Blue bell culturally proximate prediction, Light Orange culturally proximate prediction to another\nculture, Orange culturally related prediction to the other culture. Note: The Spanish prompts/entities are translated\nfor clarity.\n6261\nCultures Country Wikipedia sites used for articles\nArab Cultures 22 countries of the Arab League Arabic (ar), English (en), French (fr)\nWestern Cultures\nAustralia English (en)\nCanada English (en), French (fr)\nNew Zealand English (en), Mori (mi)\nUSA English (en)\nAndorra Catalan (ca), English (en)\nItaly Italian (it), English (en)\nLiechtenstein German (de), English (en)\nMonaco French (fr), English (en)\nPortugal Portuguese (pt), English (en)\nSan Marino Italian (it), English (en)\nSpain Spanish (es), English (en)\nAustria German (de), English (en)\nBelgium German (de), French (fr), Dutch (nl), English (en)\nFrance French (fr), English (en)\nGermany German (de), English (en)\nIreland Irish (ga), English (en)\nLuxembourg Luxembourgish (lb), French (fr), German (de), English (en)\nNetherlands Dutch (nl), English (en)\nSwitzerland German (de), French (fr), Italian (it), Romansh (rm), English (en)\nUK English (en)\nAsian Cultures\nChina English (en), Chinese (zh)\nIndonesia English (en), Indonesian (id)\nJapan English (en), Japanese (ja)\nMalaysia English (en), Malay (ms)\nMongolia English (en), Chinese (zh)\nMyanmar English (en), Burmese (my)\nNorth Korea English (en), Korean (ko)\nPhilippines English (en)\nSingapore English (en), Malay (ms)\nSouth Korea English (en), Korean (ko)\nTaiwan English (en), Chinese (zh)\nThailand English (en), Thai (th)\nVietnam English (en), Vietnamese (vi)\nSouth American Cultures\nArgentina English (en), Spanish (es)\nBolivia English (en), Spanish (es)\nBrazil English (en), Portugese (pt)\nChile English (en), Spanish (es)\nColombia English (en), Spanish (es)\nEcuador English (en), Spanish (es)\nGuyana English (en)\nParaguay English (en), Spanish (es)\nPeru English (en), Spanish (es)\nSuriname Dutch (nl), English (en)\nUruguay English (en), Spanish (es)\nVenezuela English (en), Spanish (es)\nTable F8: The list of Countries and their respective Wikipedia sites used for representing the four different cultures.\nThe English Wikipedia is used for all the countries.\n6262\nPredicate English template Arabic template Korean template Spanish template\nP17 (Country) [X] is located in [Y] .عقي ] X [يف ] Y [ . [X] 는[Y]에 있습니다. [X] se encuentra en [Y].\nP19 (Place of birth) [X] was born in [Y] .دلو ] X [يف ] Y [ . [X] 는[Y]에서 태어났습니다. [X] nació en [Y].\nP20 (Place of death) [X] died in [Y] .يفوت ] X [يف ] Y [ . [X] 는[Y]에서 사망했습니다. [X] murió en [Y].\nP27 (Country of citizenship) [X] is [Y] citizen . ] X [نطاوم ] Y [ . [X] 는[Y]시민입니다. [X] es [Y] ciudadano.\nP30 (Continent) [X] is located in [Y] .عقي ] X [يف ] Y [ . [X] 는[Y]에 있습니다. [X] se encuentra en [Y].\nP36 (Capital) The capital of [X] is [Y] .ةمصاع ] X [يه ] Y [ . [X] 의 수도는[Y]입니다. La capital de [X] es [Y].\nP37 (Ofﬁcial language) The ofﬁcial language of [X] is [Y] .ـلةيمسرلاةغللا ] X [يه ] Y [ . [X] 의 공식 언어는[Y]입니다. El idioma oﬁcial de [X] es [Y].\nP47 (Shares border with) [X] shares border with [Y] . ] X [عمدودحلايفكرتشت ] Y [ . [X] 는[Y]와 (과) 국경을 공유합니다.[X] comparte frontera con [Y].\nP103 (Native language) The native language of [X] is [Y] .ـلةيلصألاةغللا ] X [يه ] Y [ . [X] 의 모국어는[Y]입니다. El idioma nativo de [X] es [Y].\nP106 (Occupation) [X] is a [Y] by profession . ] X [يه ] Y [.ةنهملابسح [X]는 직업 별[Y]입니다. [X] es una [Y] de profesión.\nP136 (Genre) [X] plays [Y] music . ] X [ىقيسومفزعي ] Y [ . [X] 는[Y]음악을 재생합니다. [X] reproduce música [Y].\nP190 (Sister city) [X] and [Y] are twin cities . ] X [و] Y [.ناتمأوتناتنيدم [X]와[Y]는 쌍둥이 도시입니다. [X] e [Y] son ciudades gemelas.\nP264 (Record label) [X] is represented by music label [Y] . ] X [ةيقيسوملاةمالعلااهلثمي ] Y [ . [X] 는 음악 레이블[Y]로 표시됩니다.[X] está representado por el sello musical [Y].\nP364 (Original language of work) The original language of [X] is [Y] .ـلةيلصألاةغللا ] X [يه ] Y [ . [X] 의 원래 언어는[Y]입니다. El idioma original de [X] es [Y].\nP449 (Original network) [X] was originally aired on [Y] .ثبمت ] X [ىلعلصألايف ] Y [ . [X] 는 원래[Y]에 방영되었습니다. [X] se emitió originalmente en [Y].\nP495 (Country of origin) [X] was created in [Y] .ءاشنإمت ] X [يف ] Y [ . [X] 는[Y]에 작성되었습니다. [X] se creó en [Y].\nP530 (Diplomatic relation) [X] maintains diplomatic relations with [Y] . ] X [عمةيسامولبدتاقالعميقت ] Y [ . [X] 는[Y]와의 외교 관계를 유지합니다.[X] mantiene relaciones diplomáticas con [Y].\nP1303 (Instrument) [X] plays [Y] . ] X [بعلي ] Y [ . [X] 는[Y]를 재생합니다. [X] reproduce [Y].\nP1376 (Capital of) [X] is the capital of [Y] . ] X [ةمصاعيه ] Y [ . [X] 는[Y]의 수도입니다. [X] es la capital de [Y].\nP1412 (Languages spoken or published) [X] used to communicate in [Y] .مدختسي ] X [يفلصاوتلل ] Y [ . [X] 는[Y]에서 통신하는 데 사용됩니다.[X] solía comunicarse en [Y].\nTable F9: mLAMA’s templates that are also adapted in DLAMA.\n6263\nPredicate English Question Arabic Question\nP30 (Continent) Where is \"[X]\" located in? Reply with a name of a continent only.عقينيأ ] X [ \"طقفةراقمساببجأ؟ \nP36 (Capital) What is the capital of \"[X]\"? Reply with the name of the city only.ةمصاعيهام ] X [ \"طقفةنيدملامساببجأ؟ \nP37 (Ofﬁcial Language) What is the ofﬁcial language of \"[X]\"? Reply with the language name only.لةيمسرلاةغللايهام ] X [ \"طقفةغلمساببجأ؟ \nP47 (Shares border with) What is the country that shares border with \"[X]\"? Reply with a country name only.عماهدودحكرتشتيتلاةلودلايهام ] X [ \"طقفةلودمساببجأ؟ \nP190 (Sister city) What is the twin city of \"[X]\"? Reply with the name of the city only.ةنيدملمأوتلاةنيدملايهام ] X [ \"طقفةنيدملامساببجأ؟ \nP530 (Diplomatic realtion) What is the country that maintains dimplomatic relations with \"[X]\"? Reply with a country name only.عمةيسامولبدتاقالعميقتيتلاةلودلايهام ] X [ \"طقفةلودمساببجأ؟ \nP1376 (Capital of) What is the country of which the capital is \"[X]\"? Reply with a country name only.اهتمصاعيتلاةلودلايهام ] X [ \"طقفةلودمساببجأ؟ \nTable F10: The mapping of six of mLAMA’s templates to questions that can be used to evaluate the GPT3.5-turbo model.\n6264\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthical Considerations section\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSections 2, 3, 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSections 2, 3, 4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nThe benchmarks are dumps of factual triples from Wikidata.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSections 4, Appendix - Section E\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSections 4, Appendix - Section E\nC □\u0013 Did you run computational experiments?\nSection 5\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. Left blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6265\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nThe codebase is linked to in the Introduction section\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n6266",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.8231725692749023
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7955977916717529
    },
    {
      "name": "Computer science",
      "score": 0.7231424450874329
    },
    {
      "name": "Natural language processing",
      "score": 0.6145114898681641
    },
    {
      "name": "Recall",
      "score": 0.5495060682296753
    },
    {
      "name": "Arabic",
      "score": 0.5294294953346252
    },
    {
      "name": "Artificial intelligence",
      "score": 0.503717839717865
    },
    {
      "name": "Relation (database)",
      "score": 0.4843772351741791
    },
    {
      "name": "Affect (linguistics)",
      "score": 0.44450849294662476
    },
    {
      "name": "Linguistics",
      "score": 0.3824443519115448
    },
    {
      "name": "Psychology",
      "score": 0.2231065034866333
    },
    {
      "name": "Cognitive psychology",
      "score": 0.2201036810874939
    },
    {
      "name": "Geography",
      "score": 0.08133366703987122
    },
    {
      "name": "Data mining",
      "score": 0.0756324827671051
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ]
}